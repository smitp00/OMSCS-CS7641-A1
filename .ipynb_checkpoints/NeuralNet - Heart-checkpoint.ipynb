{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "222127f0-611c-4924-bed7-7165d943ac0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPainType</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>RestingECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExerciseAngina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>ST_Slope</th>\n",
       "      <th>HeartDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>140</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>172</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>F</td>\n",
       "      <td>NAP</td>\n",
       "      <td>160</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>156</td>\n",
       "      <td>N</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>130</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>ST</td>\n",
       "      <td>98</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>F</td>\n",
       "      <td>ASY</td>\n",
       "      <td>138</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>108</td>\n",
       "      <td>Y</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Flat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>M</td>\n",
       "      <td>NAP</td>\n",
       "      <td>150</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>122</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  MaxHR  \\\n",
       "0   40   M           ATA        140          289          0     Normal    172   \n",
       "1   49   F           NAP        160          180          0     Normal    156   \n",
       "2   37   M           ATA        130          283          0         ST     98   \n",
       "3   48   F           ASY        138          214          0     Normal    108   \n",
       "4   54   M           NAP        150          195          0     Normal    122   \n",
       "\n",
       "  ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n",
       "0              N      0.0       Up             0  \n",
       "1              N      1.0     Flat             1  \n",
       "2              N      0.0       Up             0  \n",
       "3              Y      1.5     Flat             1  \n",
       "4              N      0.0       Up             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "heart_df = pd.read_csv('heart.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "heart_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca4a0287-4671-4868-a4dc-ac6c2566f976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      " Age               0\n",
      "Sex               0\n",
      "ChestPainType     0\n",
      "RestingBP         0\n",
      "Cholesterol       0\n",
      "FastingBS         0\n",
      "RestingECG        0\n",
      "MaxHR             0\n",
      "ExerciseAngina    0\n",
      "Oldpeak           0\n",
      "ST_Slope          0\n",
      "HeartDisease      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = heart_df.isnull().sum()\n",
    "print(\"Missing values in each column:\\n\", missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17c8477d-cbfb-4d7d-91f0-dbb5d0fd2308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categorical and numerical columns\n",
    "categorical_features = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "numerical_features = ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak']\n",
    "\n",
    "# Define the preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Separate features and target\n",
    "X = heart_df.drop('HeartDisease', axis=1)\n",
    "y = heart_df['HeartDisease']\n",
    "\n",
    "# Apply the preprocessing pipeline to the features\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83166cd4-0a00-4860-b629-4c94528acdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the preprocessed data:\n",
      "    num__Age  num__RestingBP  num__Cholesterol  num__FastingBS  num__MaxHR  \\\n",
      "0 -1.433140        0.410909          0.825070       -0.551341    1.382928   \n",
      "1 -0.478484        1.491752         -0.171961       -0.551341    0.754157   \n",
      "2 -1.751359       -0.129513          0.770188       -0.551341   -1.525138   \n",
      "3 -0.584556        0.302825          0.139040       -0.551341   -1.132156   \n",
      "4  0.051881        0.951331         -0.034755       -0.551341   -0.581981   \n",
      "\n",
      "   num__Oldpeak  cat__Sex_F  cat__Sex_M  cat__ChestPainType_ASY  \\\n",
      "0     -0.832432         0.0         1.0                     0.0   \n",
      "1      0.105664         1.0         0.0                     0.0   \n",
      "2     -0.832432         0.0         1.0                     0.0   \n",
      "3      0.574711         1.0         0.0                     1.0   \n",
      "4     -0.832432         0.0         1.0                     0.0   \n",
      "\n",
      "   cat__ChestPainType_ATA  cat__ChestPainType_NAP  cat__ChestPainType_TA  \\\n",
      "0                     1.0                     0.0                    0.0   \n",
      "1                     0.0                     1.0                    0.0   \n",
      "2                     1.0                     0.0                    0.0   \n",
      "3                     0.0                     0.0                    0.0   \n",
      "4                     0.0                     1.0                    0.0   \n",
      "\n",
      "   cat__RestingECG_LVH  cat__RestingECG_Normal  cat__RestingECG_ST  \\\n",
      "0                  0.0                     1.0                 0.0   \n",
      "1                  0.0                     1.0                 0.0   \n",
      "2                  0.0                     0.0                 1.0   \n",
      "3                  0.0                     1.0                 0.0   \n",
      "4                  0.0                     1.0                 0.0   \n",
      "\n",
      "   cat__ExerciseAngina_N  cat__ExerciseAngina_Y  cat__ST_Slope_Down  \\\n",
      "0                    1.0                    0.0                 0.0   \n",
      "1                    1.0                    0.0                 0.0   \n",
      "2                    1.0                    0.0                 0.0   \n",
      "3                    0.0                    1.0                 0.0   \n",
      "4                    1.0                    0.0                 0.0   \n",
      "\n",
      "   cat__ST_Slope_Flat  cat__ST_Slope_Up  \n",
      "0                 0.0               1.0  \n",
      "1                 1.0               0.0  \n",
      "2                 0.0               1.0  \n",
      "3                 1.0               0.0  \n",
      "4                 0.0               1.0  \n"
     ]
    }
   ],
   "source": [
    "# Apply the preprocessing pipeline to the features and convert to DataFrame for better visualization\n",
    "X_preprocessed_df = pd.DataFrame(X_preprocessed, columns=preprocessor.get_feature_names_out())\n",
    "\n",
    "# Display the first few rows of the preprocessed data\n",
    "print(\"First few rows of the preprocessed data:\\n\", X_preprocessed_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ec77e6a-d7e8-4a6a-a97a-dc0aafdc70af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution (%):\n",
      " HeartDisease\n",
      "1    55.337691\n",
      "0    44.662309\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the class distribution of the target variable\n",
    "class_distribution = y.value_counts(normalize=True) * 100\n",
    "print(\"Class distribution (%):\\n\", class_distribution)\n",
    "\n",
    "# Visualize the class distribution\n",
    "plt.bar(class_distribution.index, class_distribution.values, tick_label=['No Disease', 'Heart Disease'])\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Class Distribution in Heart Disease Dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70e0a3b3-f9d0-4ad6-9b26-d356f96d9c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fac77b63-ad16-41f7-8c36-742945d9f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter space for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (150,), (50, 50), (100, 50), (100, 100)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'max_iter': [3000],\n",
    "}\n",
    "\n",
    "# Initialize the MLPClassifier\n",
    "mlp = MLPClassifier(random_state=42)\n",
    "\n",
    "# Set up the RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    mlp, param_distributions=param_dist, n_iter=100, cv=5, random_state=42, n_jobs=-1, verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29c7f384-25ca-47e1-80ac-cbf485acb39b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   1.0s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.6s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   1.4s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.8s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.8s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.9s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.9s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   2.2s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.1s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.1s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.1s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.9s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.9s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.8s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.8s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   1.1s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   2.0s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   2.1s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.8s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.4s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.5s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.1s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.7s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.7s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.5s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.8s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   2.0s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.3s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.7s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.7s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.7s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   2.1s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.1s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.2s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.2s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.2s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.2s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.6s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.1s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.3s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.3s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.7s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.5s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.6s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   2.1s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.4s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.6s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.4s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.5s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.6s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.3s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.7s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.7s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   2.0s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.6s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.1s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.6s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.3s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.9s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   2.0s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   2.0s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.9s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   2.0s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.5s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.1s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.6s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.4s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.1s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.1s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.8s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.5s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.5s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.5s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.5s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.8s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.1s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.1s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.1s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.1s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.4s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.5s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.5s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.1s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.6s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.4s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.4s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.5s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.5s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.6s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   1.3s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.4s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.9s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.3s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.4s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.4s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.1s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.4s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.9s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.8s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.9s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.0s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   2.4s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.9s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   2.1s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   2.7s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.8s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.8s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   2.0s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   2.1s\n",
      "[CV] END activation=tanh, alpha=0.1, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   2.5s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=constant, max_iter=3000, solver=sgd; total time=   1.1s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.8s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.9s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.9s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=constant, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.1s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.1s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.8s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.9s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.9s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   2.0s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   2.0s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.1s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.1s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.1s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.2s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(150,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.6s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.2s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.5s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=constant, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(150,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 50), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.2s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.2s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.2s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=tanh, alpha=0.0001, hidden_layer_sizes=(50,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.8s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.5s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100,), learning_rate=constant, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.4s\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.3s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.8s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.8s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.9s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   0.9s\n",
      "[CV] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 100), learning_rate=adaptive, max_iter=3000, solver=sgd; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   1.0s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.9s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.7s\n",
      "[CV] END activation=relu, alpha=0.1, hidden_layer_sizes=(100, 50), learning_rate=adaptive, max_iter=3000, solver=adam; total time=   0.8s\n",
      "Best parameters found:  {'solver': 'sgd', 'max_iter': 3000, 'learning_rate': 'constant', 'hidden_layer_sizes': (100, 50), 'alpha': 0.001, 'activation': 'relu'}\n"
     ]
    }
   ],
   "source": [
    "# Fit the random search on the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found by RandomizedSearchCV\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2660f70b-2a39-48c2-9e58-423f55c229ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Define the parameter range for alpha\n",
    "alpha_range = np.logspace(-3, 1, 30)\n",
    "\n",
    "# Create a scorer for F1 Score\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# Validation curve for alpha\n",
    "train_scores, valid_scores = validation_curve(\n",
    "    MLPClassifier(solver='sgd', max_iter=2500, learning_rate='constant', hidden_layer_sizes=(100, 50), activation='relu', random_state=42),\n",
    "    X_train, y_train,\n",
    "    param_name='alpha',\n",
    "    param_range=alpha_range,\n",
    "    cv=5,\n",
    "    scoring=f1_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate mean and standard deviation for training and validation scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "valid_mean = np.mean(valid_scores, axis=1)\n",
    "valid_std = np.std(valid_scores, axis=1)\n",
    "\n",
    "# Plot the validation curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alpha_range, train_mean, label='Training score', color='blue')\n",
    "plt.plot(alpha_range, valid_mean, label='Cross-validation score', color='green')\n",
    "plt.fill_between(alpha_range, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n",
    "plt.fill_between(alpha_range, valid_mean - valid_std, valid_mean + valid_std, alpha=0.2, color='green')\n",
    "plt.xscale('log')\n",
    "plt.title('Validation Curve for MLPClassifier (alpha)')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02680a48-8877-443e-8e17-62735e390655",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.76289642\n",
      "Iteration 2, loss = 0.75875658\n",
      "Iteration 3, loss = 0.75238698\n",
      "Iteration 4, loss = 0.74458350\n",
      "Iteration 5, loss = 0.73645331\n",
      "Iteration 6, loss = 0.72762663\n",
      "Iteration 7, loss = 0.71820518\n",
      "Iteration 8, loss = 0.70917204\n",
      "Iteration 9, loss = 0.70047644\n",
      "Iteration 10, loss = 0.69167986\n",
      "Iteration 11, loss = 0.68312632\n",
      "Iteration 12, loss = 0.67513683\n",
      "Iteration 13, loss = 0.66730446\n",
      "Iteration 14, loss = 0.66007116\n",
      "Iteration 15, loss = 0.65289908\n",
      "Iteration 16, loss = 0.64625069\n",
      "Iteration 17, loss = 0.63978759\n",
      "Iteration 18, loss = 0.63348079\n",
      "Iteration 19, loss = 0.62752057\n",
      "Iteration 20, loss = 0.62169205\n",
      "Iteration 21, loss = 0.61632419\n",
      "Iteration 22, loss = 0.61104375\n",
      "Iteration 23, loss = 0.60602502\n",
      "Iteration 24, loss = 0.60113892\n",
      "Iteration 25, loss = 0.59629820\n",
      "Iteration 26, loss = 0.59186807\n",
      "Iteration 27, loss = 0.58760024\n",
      "Iteration 28, loss = 0.58326850\n",
      "Iteration 29, loss = 0.57922555\n",
      "Iteration 30, loss = 0.57527640\n",
      "Iteration 31, loss = 0.57146784\n",
      "Iteration 32, loss = 0.56777812\n",
      "Iteration 33, loss = 0.56409337\n",
      "Iteration 34, loss = 0.56062240\n",
      "Iteration 35, loss = 0.55728364\n",
      "Iteration 36, loss = 0.55398789\n",
      "Iteration 37, loss = 0.55079595\n",
      "Iteration 38, loss = 0.54765276\n",
      "Iteration 39, loss = 0.54475755\n",
      "Iteration 40, loss = 0.54178269\n",
      "Iteration 41, loss = 0.53885852\n",
      "Iteration 42, loss = 0.53614297\n",
      "Iteration 43, loss = 0.53334337\n",
      "Iteration 44, loss = 0.53076602\n",
      "Iteration 45, loss = 0.52816694\n",
      "Iteration 46, loss = 0.52567294\n",
      "Iteration 47, loss = 0.52320232\n",
      "Iteration 48, loss = 0.52076453\n",
      "Iteration 49, loss = 0.51849220\n",
      "Iteration 50, loss = 0.51614066\n",
      "Iteration 51, loss = 0.51390089\n",
      "Iteration 52, loss = 0.51174228\n",
      "Iteration 53, loss = 0.50965868\n",
      "Iteration 54, loss = 0.50758753\n",
      "Iteration 55, loss = 0.50551539\n",
      "Iteration 56, loss = 0.50354291\n",
      "Iteration 57, loss = 0.50157430\n",
      "Iteration 58, loss = 0.49968445\n",
      "Iteration 59, loss = 0.49785804\n",
      "Iteration 60, loss = 0.49601366\n",
      "Iteration 61, loss = 0.49422751\n",
      "Iteration 62, loss = 0.49250701\n",
      "Iteration 63, loss = 0.49073765\n",
      "Iteration 64, loss = 0.48912765\n",
      "Iteration 65, loss = 0.48743454\n",
      "Iteration 66, loss = 0.48588693\n",
      "Iteration 67, loss = 0.48427567\n",
      "Iteration 68, loss = 0.48271529\n",
      "Iteration 69, loss = 0.48123638\n",
      "Iteration 70, loss = 0.47970879\n",
      "Iteration 71, loss = 0.47825995\n",
      "Iteration 72, loss = 0.47679507\n",
      "Iteration 73, loss = 0.47546028\n",
      "Iteration 74, loss = 0.47403492\n",
      "Iteration 75, loss = 0.47267348\n",
      "Iteration 76, loss = 0.47134869\n",
      "Iteration 77, loss = 0.47006182\n",
      "Iteration 78, loss = 0.46877154\n",
      "Iteration 79, loss = 0.46750132\n",
      "Iteration 80, loss = 0.46628070\n",
      "Iteration 81, loss = 0.46507592\n",
      "Iteration 1, loss = 0.69108390\n",
      "Iteration 82, loss = 0.46389203\n",
      "Iteration 83, loss = 0.46270451\n",
      "Iteration 84, loss = 0.46159791\n",
      "Iteration 85, loss = 0.46045879\n",
      "Iteration 86, loss = 0.45931792\n",
      "Iteration 87, loss = 0.45824130\n",
      "Iteration 88, loss = 0.45712171\n",
      "Iteration 89, loss = 0.45609795\n",
      "Iteration 2, loss = 0.68834655\n",
      "Iteration 90, loss = 0.45504770\n",
      "Iteration 91, loss = 0.45411663\n",
      "Iteration 3, loss = 0.68427130\n",
      "Iteration 92, loss = 0.45300752\n",
      "Iteration 93, loss = 0.45203453\n",
      "Iteration 94, loss = 0.45105131\n",
      "Iteration 95, loss = 0.45012182\n",
      "Iteration 96, loss = 0.44913932\n",
      "Iteration 97, loss = 0.44822275\n",
      "Iteration 4, loss = 0.67925270\n",
      "Iteration 98, loss = 0.44734596\n",
      "Iteration 99, loss = 0.44644174\n",
      "Iteration 100, loss = 0.44551877\n",
      "Iteration 101, loss = 0.44466489\n",
      "Iteration 102, loss = 0.44384178\n",
      "Iteration 1, loss = 0.73673972\n",
      "Iteration 103, loss = 0.44298527\n",
      "Iteration 5, loss = 0.67381879\n",
      "Iteration 104, loss = 0.44216856\n",
      "Iteration 105, loss = 0.44133199\n",
      "Iteration 106, loss = 0.44052920\n",
      "Iteration 107, loss = 0.43972437\n",
      "Iteration 6, loss = 0.66804576\n",
      "Iteration 7, loss = 0.66200591\n",
      "Iteration 108, loss = 0.43894519\n",
      "Iteration 109, loss = 0.43818810\n",
      "Iteration 8, loss = 0.65560500\n",
      "Iteration 110, loss = 0.43740249\n",
      "Iteration 111, loss = 0.43667733\n",
      "Iteration 112, loss = 0.43591403\n",
      "Iteration 9, loss = 0.64970280\n",
      "Iteration 10, loss = 0.64371171\n",
      "Iteration 113, loss = 0.43521097\n",
      "Iteration 11, loss = 0.63798611\n",
      "Iteration 114, loss = 0.43450891\n",
      "Iteration 115, loss = 0.43375647\n",
      "Iteration 116, loss = 0.43307553\n",
      "Iteration 117, loss = 0.43236382\n",
      "Iteration 12, loss = 0.63212724\n",
      "Iteration 118, loss = 0.43168348\n",
      "Iteration 1, loss = 0.77387189\n",
      "Iteration 2, loss = 0.73196335\n",
      "Iteration 13, loss = 0.62663908\n",
      "Iteration 119, loss = 0.43102638\n",
      "Iteration 3, loss = 0.72483577\n",
      "Iteration 2, loss = 0.77025964\n",
      "Iteration 120, loss = 0.43034666\n",
      "Iteration 121, loss = 0.42971209\n",
      "Iteration 14, loss = 0.62136458\n",
      "Iteration 122, loss = 0.42906807\n",
      "Iteration 123, loss = 0.42845016\n",
      "Iteration 124, loss = 0.42778300\n",
      "Iteration 3, loss = 0.76476556\n",
      "Iteration 125, loss = 0.42715017\n",
      "Iteration 4, loss = 0.71601636\n",
      "Iteration 15, loss = 0.61624742\n",
      "Iteration 126, loss = 0.42655382\n",
      "Iteration 127, loss = 0.42594643\n",
      "Iteration 16, loss = 0.61126098\n",
      "Iteration 128, loss = 0.42535741\n",
      "Iteration 129, loss = 0.42477169\n",
      "Iteration 130, loss = 0.42419948\n",
      "Iteration 5, loss = 0.70679376\n",
      "Iteration 4, loss = 0.75812543\n",
      "Iteration 17, loss = 0.60639501\n",
      "Iteration 131, loss = 0.42359806\n",
      "Iteration 132, loss = 0.42302912\n",
      "Iteration 6, loss = 0.69688182\n",
      "Iteration 18, loss = 0.60179503\n",
      "Iteration 5, loss = 0.75072313\n",
      "Iteration 133, loss = 0.42248827\n",
      "Iteration 19, loss = 0.59732965\n",
      "Iteration 134, loss = 0.42192316\n",
      "Iteration 135, loss = 0.42136072\n",
      "Iteration 20, loss = 0.59270007\n",
      "Iteration 136, loss = 0.42084048\n",
      "Iteration 137, loss = 0.42030160\n",
      "Iteration 21, loss = 0.58861702\n",
      "Iteration 7, loss = 0.68673516\n",
      "Iteration 138, loss = 0.41978105\n",
      "Iteration 22, loss = 0.58455837\n",
      "Iteration 139, loss = 0.41924654\n",
      "Iteration 6, loss = 0.74323189\n",
      "Iteration 23, loss = 0.58054373\n",
      "Iteration 24, loss = 0.57661922\n",
      "Iteration 140, loss = 0.41870167\n",
      "Iteration 25, loss = 0.57285077\n",
      "Iteration 141, loss = 0.41821998\n",
      "Iteration 8, loss = 0.67716205\n",
      "Iteration 142, loss = 0.41770577\n",
      "Iteration 26, loss = 0.56908943\n",
      "Iteration 143, loss = 0.41720551\n",
      "Iteration 27, loss = 0.56560571\n",
      "Iteration 144, loss = 0.41667449\n",
      "Iteration 7, loss = 0.73524865\n",
      "Iteration 9, loss = 0.66790240\n",
      "Iteration 145, loss = 0.41619681\n",
      "Iteration 8, loss = 0.72785668\n",
      "Iteration 146, loss = 0.41570225\n",
      "Iteration 10, loss = 0.65886338\n",
      "Iteration 147, loss = 0.41524708\n",
      "Iteration 9, loss = 0.72022838\n",
      "Iteration 28, loss = 0.56206656\n",
      "Iteration 29, loss = 0.55873822\n",
      "Iteration 148, loss = 0.41472920\n",
      "Iteration 11, loss = 0.64991317\n",
      "Iteration 149, loss = 0.41427030\n",
      "Iteration 30, loss = 0.55534518\n",
      "Iteration 150, loss = 0.41381467\n",
      "Iteration 151, loss = 0.41333087\n",
      "Iteration 152, loss = 0.41289838\n",
      "Iteration 10, loss = 0.71272919\n",
      "Iteration 153, loss = 0.41244513\n",
      "Iteration 31, loss = 0.55217495\n",
      "Iteration 154, loss = 0.41196980\n",
      "Iteration 32, loss = 0.54899826\n",
      "Iteration 11, loss = 0.70601077\n",
      "Iteration 155, loss = 0.41153778\n",
      "Iteration 12, loss = 0.64153923\n",
      "Iteration 12, loss = 0.69922155\n",
      "Iteration 33, loss = 0.54593221\n",
      "Iteration 156, loss = 0.41109641\n",
      "Iteration 157, loss = 0.41067503\n",
      "Iteration 34, loss = 0.54295551\n",
      "Iteration 158, loss = 0.41023155\n",
      "Iteration 13, loss = 0.69283915\n",
      "Iteration 159, loss = 0.40979016\n",
      "Iteration 13, loss = 0.63410923\n",
      "Iteration 35, loss = 0.54001532\n",
      "Iteration 14, loss = 0.68638856\n",
      "Iteration 160, loss = 0.40936693\n",
      "Iteration 161, loss = 0.40896250\n",
      "Iteration 15, loss = 0.68040746\n",
      "Iteration 162, loss = 0.40853738\n",
      "Iteration 36, loss = 0.53710925\n",
      "Iteration 14, loss = 0.62648029\n",
      "Iteration 163, loss = 0.40811085\n",
      "Iteration 1, loss = 0.70226065\n",
      "Iteration 37, loss = 0.53437681\n",
      "Iteration 164, loss = 0.40768185\n",
      "Iteration 16, loss = 0.67470403\n",
      "Iteration 15, loss = 0.61957112\n",
      "Iteration 38, loss = 0.53159743\n",
      "Iteration 165, loss = 0.40727205\n",
      "Iteration 39, loss = 0.52898439\n",
      "Iteration 16, loss = 0.61298803\n",
      "Iteration 166, loss = 0.40691744\n",
      "Iteration 40, loss = 0.52628328\n",
      "Iteration 17, loss = 0.66897572\n",
      "Iteration 167, loss = 0.40648713\n",
      "Iteration 17, loss = 0.60661251\n",
      "Iteration 168, loss = 0.40607972\n",
      "Iteration 41, loss = 0.52380447\n",
      "Iteration 169, loss = 0.40567737\n",
      "Iteration 18, loss = 0.66377323\n",
      "Iteration 170, loss = 0.40530553\n",
      "Iteration 42, loss = 0.52123785\n",
      "Iteration 171, loss = 0.40491120\n",
      "Iteration 43, loss = 0.51878206\n",
      "Iteration 18, loss = 0.60056671\n",
      "Iteration 2, loss = 0.70076614\n",
      "Iteration 19, loss = 0.65840584\n",
      "Iteration 172, loss = 0.40451213\n",
      "Iteration 173, loss = 0.40413567\n",
      "Iteration 19, loss = 0.59478195\n",
      "Iteration 174, loss = 0.40374800\n",
      "Iteration 44, loss = 0.51642248\n",
      "Iteration 175, loss = 0.40338886\n",
      "Iteration 176, loss = 0.40299544\n",
      "Iteration 20, loss = 0.65351790\n",
      "Iteration 177, loss = 0.40262785\n",
      "Iteration 45, loss = 0.51403702\n",
      "Iteration 178, loss = 0.40229217\n",
      "Iteration 21, loss = 0.64852788\n",
      "Iteration 179, loss = 0.40189695\n",
      "Iteration 46, loss = 0.51171730\n",
      "Iteration 3, loss = 0.69829746\n",
      "Iteration 180, loss = 0.40152195\n",
      "Iteration 47, loss = 0.50949984\n",
      "Iteration 22, loss = 0.64371358\n",
      "Iteration 181, loss = 0.40117374\n",
      "Iteration 182, loss = 0.40081718\n",
      "Iteration 23, loss = 0.63904861\n",
      "Iteration 183, loss = 0.40045007\n",
      "Iteration 48, loss = 0.50729261\n",
      "Iteration 184, loss = 0.40009597\n",
      "Iteration 24, loss = 0.63452476\n",
      "Iteration 185, loss = 0.39975664\n",
      "Iteration 20, loss = 0.58951858\n",
      "Iteration 4, loss = 0.69552805\n",
      "Iteration 49, loss = 0.50512314\n",
      "Iteration 186, loss = 0.39938182\n",
      "Iteration 25, loss = 0.63002271\n",
      "Iteration 1, loss = 0.69615400\n",
      "Iteration 187, loss = 0.39906640\n",
      "Iteration 188, loss = 0.39871679\n",
      "Iteration 26, loss = 0.62574869\n",
      "Iteration 50, loss = 0.50298992\n",
      "Iteration 189, loss = 0.39836958\n",
      "Iteration 190, loss = 0.39803307\n",
      "Iteration 51, loss = 0.50092781\n",
      "Iteration 27, loss = 0.62155284\n",
      "Iteration 191, loss = 0.39770499\n",
      "Iteration 192, loss = 0.39734806\n",
      "Iteration 52, loss = 0.49890688\n",
      "Iteration 21, loss = 0.58425717\n",
      "Iteration 28, loss = 0.61725512\n",
      "Iteration 5, loss = 0.69229822\n",
      "Iteration 193, loss = 0.39701605\n",
      "Iteration 53, loss = 0.49688494\n",
      "Iteration 22, loss = 0.57942087\n",
      "Iteration 29, loss = 0.61323415Iteration 194, loss = 0.39669756\n",
      "\n",
      "Iteration 54, loss = 0.49493108\n",
      "Iteration 6, loss = 0.68871664\n",
      "Iteration 195, loss = 0.39635314\n",
      "Iteration 23, loss = 0.57457411\n",
      "Iteration 55, loss = 0.49301697\n",
      "Iteration 30, loss = 0.60921213\n",
      "Iteration 196, loss = 0.39604642\n",
      "Iteration 2, loss = 0.69465878\n",
      "Iteration 24, loss = 0.56997773\n",
      "Iteration 197, loss = 0.39572281\n",
      "Iteration 31, loss = 0.60538410\n",
      "Iteration 56, loss = 0.49114073\n",
      "Iteration 198, loss = 0.39539550\n",
      "Iteration 57, loss = 0.48924577\n",
      "Iteration 199, loss = 0.39507450\n",
      "Iteration 32, loss = 0.60154760\n",
      "Iteration 200, loss = 0.39478206\n",
      "Iteration 58, loss = 0.48744788\n",
      "Iteration 25, loss = 0.56549223\n",
      "Iteration 7, loss = 0.68515194\n",
      "Iteration 201, loss = 0.39445978\n",
      "Iteration 59, loss = 0.48574938\n",
      "Iteration 202, loss = 0.39413457\n",
      "Iteration 33, loss = 0.59778038\n",
      "Iteration 60, loss = 0.48392188\n",
      "Iteration 8, loss = 0.68130667\n",
      "Iteration 203, loss = 0.39382057\n",
      "Iteration 61, loss = 0.48223578\n",
      "Iteration 204, loss = 0.39351449\n",
      "Iteration 3, loss = 0.69235752\n",
      "Iteration 34, loss = 0.59402777\n",
      "Iteration 62, loss = 0.48054866\n",
      "Iteration 205, loss = 0.39323178\n",
      "Iteration 26, loss = 0.56140230\n",
      "Iteration 206, loss = 0.39291939\n",
      "Iteration 35, loss = 0.59044971\n",
      "Iteration 207, loss = 0.39260937\n",
      "Iteration 63, loss = 0.47888861\n",
      "Iteration 36, loss = 0.58679391\n",
      "Iteration 9, loss = 0.67741708\n",
      "Iteration 27, loss = 0.55729009\n",
      "Iteration 208, loss = 0.39232292\n",
      "Iteration 64, loss = 0.47731015\n",
      "Iteration 209, loss = 0.39201817\n",
      "Iteration 28, loss = 0.55327330\n",
      "Iteration 65, loss = 0.47566446\n",
      "Iteration 37, loss = 0.58323022\n",
      "Iteration 210, loss = 0.39172277\n",
      "Iteration 66, loss = 0.47417778\n",
      "Iteration 211, loss = 0.39143256\n",
      "Iteration 29, loss = 0.54947583\n",
      "Iteration 38, loss = 0.57983220\n",
      "Iteration 4, loss = 0.68957595\n",
      "Iteration 212, loss = 0.39112618\n",
      "Iteration 67, loss = 0.47260843\n",
      "Iteration 213, loss = 0.39084855\n",
      "Iteration 30, loss = 0.54590371\n",
      "Iteration 68, loss = 0.47116081\n",
      "Iteration 214, loss = 0.39056614\n",
      "Iteration 39, loss = 0.57646202\n",
      "Iteration 10, loss = 0.67360048\n",
      "Iteration 69, loss = 0.46966744\n",
      "Iteration 31, loss = 0.54220609\n",
      "Iteration 215, loss = 0.39027732\n",
      "Iteration 70, loss = 0.46820095\n",
      "Iteration 216, loss = 0.38998216\n",
      "Iteration 71, loss = 0.46678018\n",
      "Iteration 11, loss = 0.66970962\n",
      "Iteration 32, loss = 0.53875565\n",
      "Iteration 217, loss = 0.38970844\n",
      "Iteration 40, loss = 0.57303068\n",
      "Iteration 72, loss = 0.46535664\n",
      "Iteration 218, loss = 0.38941839\n",
      "Iteration 5, loss = 0.68638980\n",
      "Iteration 219, loss = 0.38914626\n",
      "Iteration 33, loss = 0.53542192\n",
      "Iteration 73, loss = 0.46399542\n",
      "Iteration 41, loss = 0.56982405\n",
      "Iteration 220, loss = 0.38886706\n",
      "Iteration 221, loss = 0.38859984\n",
      "Iteration 34, loss = 0.53216479\n",
      "Iteration 74, loss = 0.46264117\n",
      "Iteration 42, loss = 0.56659278\n",
      "Iteration 222, loss = 0.38831442\n",
      "Iteration 223, loss = 0.38804728\n",
      "Iteration 75, loss = 0.46127487\n",
      "Iteration 224, loss = 0.38777923\n",
      "Iteration 12, loss = 0.66594582\n",
      "Iteration 43, loss = 0.56335605\n",
      "Iteration 76, loss = 0.46002736\n",
      "Iteration 225, loss = 0.38750549\n",
      "Iteration 35, loss = 0.52905440\n",
      "Iteration 77, loss = 0.45868506\n",
      "Iteration 226, loss = 0.38724677\n",
      "Iteration 44, loss = 0.56026125\n",
      "Iteration 227, loss = 0.38698961\n",
      "Iteration 36, loss = 0.52602560\n",
      "Iteration 78, loss = 0.45745021\n",
      "Iteration 228, loss = 0.38670613Iteration 45, loss = 0.55719664\n",
      "\n",
      "Iteration 79, loss = 0.45619845\n",
      "Iteration 229, loss = 0.38644806\n",
      "Iteration 37, loss = 0.52304653\n",
      "Iteration 6, loss = 0.68284668\n",
      "Iteration 46, loss = 0.55414761\n",
      "Iteration 80, loss = 0.45495780\n",
      "Iteration 230, loss = 0.38619858\n",
      "Iteration 38, loss = 0.52019362\n",
      "Iteration 47, loss = 0.55117672\n",
      "Iteration 231, loss = 0.38592434\n",
      "Iteration 81, loss = 0.45373894\n",
      "Iteration 48, loss = 0.54823460\n",
      "Iteration 39, loss = 0.51728171\n",
      "Iteration 13, loss = 0.66210621\n",
      "Iteration 232, loss = 0.38564738\n",
      "Iteration 82, loss = 0.45259412\n",
      "Iteration 49, loss = 0.54539957\n",
      "Iteration 233, loss = 0.38542659\n",
      "Iteration 40, loss = 0.51457775\n",
      "Iteration 83, loss = 0.45136988\n",
      "Iteration 234, loss = 0.38514315\n",
      "Iteration 50, loss = 0.54253423\n",
      "Iteration 14, loss = 0.65836699\n",
      "Iteration 7, loss = 0.67922400\n",
      "Iteration 84, loss = 0.45031023\n",
      "Iteration 235, loss = 0.38490306\n",
      "Iteration 51, loss = 0.53977038\n",
      "Iteration 85, loss = 0.44909732\n",
      "Iteration 86, loss = 0.44803802\n",
      "Iteration 52, loss = 0.53706108\n",
      "Iteration 236, loss = 0.38466080\n",
      "Iteration 1, loss = 0.74511652\n",
      "Iteration 41, loss = 0.51192753\n",
      "Iteration 87, loss = 0.44691772\n",
      "Iteration 53, loss = 0.53435972\n",
      "Iteration 237, loss = 0.38440418\n",
      "Iteration 238, loss = 0.38413165\n",
      "Iteration 54, loss = 0.53177853\n",
      "Iteration 88, loss = 0.44585910\n",
      "Iteration 15, loss = 0.65460670\n",
      "Iteration 239, loss = 0.38389797\n",
      "Iteration 240, loss = 0.38364065\n",
      "Iteration 89, loss = 0.44478767\n",
      "Iteration 55, loss = 0.52904401\n",
      "Iteration 8, loss = 0.67560427\n",
      "Iteration 241, loss = 0.38339394\n",
      "Iteration 16, loss = 0.65089594\n",
      "Iteration 242, loss = 0.38314966\n",
      "Iteration 56, loss = 0.52661025\n",
      "Iteration 243, loss = 0.38290315\n",
      "Iteration 57, loss = 0.52409091\n",
      "Iteration 244, loss = 0.38266350\n",
      "Iteration 90, loss = 0.44379720\n",
      "Iteration 245, loss = 0.38242554\n",
      "Iteration 42, loss = 0.50933390\n",
      "Iteration 9, loss = 0.67176976\n",
      "Iteration 246, loss = 0.38219786\n",
      "Iteration 58, loss = 0.52166504\n",
      "Iteration 17, loss = 0.64731806\n",
      "Iteration 91, loss = 0.44274861\n",
      "Iteration 247, loss = 0.38194338\n",
      "Iteration 248, loss = 0.38170977\n",
      "Iteration 92, loss = 0.44174804\n",
      "Iteration 43, loss = 0.50680882\n",
      "Iteration 249, loss = 0.38146433\n",
      "Iteration 59, loss = 0.51924125\n",
      "Iteration 250, loss = 0.38123348\n",
      "Iteration 18, loss = 0.64369028\n",
      "Iteration 251, loss = 0.38100520\n",
      "Iteration 60, loss = 0.51679745\n",
      "Iteration 93, loss = 0.44073805\n",
      "Iteration 44, loss = 0.50438977Iteration 252, loss = 0.38075361\n",
      "\n",
      "Iteration 61, loss = 0.51442126\n",
      "Iteration 253, loss = 0.38053693\n",
      "Iteration 45, loss = 0.50191809\n",
      "Iteration 19, loss = 0.64017152\n",
      "Iteration 254, loss = 0.38029670\n",
      "Iteration 10, loss = 0.66799918\n",
      "Iteration 255, loss = 0.38006981\n",
      "Iteration 94, loss = 0.43976703\n",
      "Iteration 256, loss = 0.37984307\n",
      "Iteration 62, loss = 0.51214605\n",
      "Iteration 20, loss = 0.63657422\n",
      "Iteration 257, loss = 0.37962579\n",
      "Iteration 95, loss = 0.43883559\n",
      "Iteration 258, loss = 0.37938283\n",
      "Iteration 63, loss = 0.50993551\n",
      "Iteration 11, loss = 0.66419225\n",
      "Iteration 96, loss = 0.43781781\n",
      "Iteration 259, loss = 0.37918747\n",
      "Iteration 2, loss = 0.74198025\n",
      "Iteration 97, loss = 0.43692724\n",
      "Iteration 260, loss = 0.37895612\n",
      "Iteration 98, loss = 0.43596281\n",
      "Iteration 21, loss = 0.63293993\n",
      "Iteration 64, loss = 0.50770981\n",
      "Iteration 261, loss = 0.37872893\n",
      "Iteration 46, loss = 0.49961497\n",
      "Iteration 262, loss = 0.37852884\n",
      "Iteration 263, loss = 0.37830085\n",
      "Iteration 65, loss = 0.50557443\n",
      "Iteration 264, loss = 0.37808201\n",
      "Iteration 47, loss = 0.49728620\n",
      "Iteration 99, loss = 0.43508793\n",
      "Iteration 66, loss = 0.50338938\n",
      "Iteration 265, loss = 0.37785723\n",
      "Iteration 48, loss = 0.49506430\n",
      "Iteration 67, loss = 0.50132860\n",
      "Iteration 266, loss = 0.37765062\n",
      "Iteration 68, loss = 0.49916543\n",
      "Iteration 100, loss = 0.43421054\n",
      "Iteration 22, loss = 0.62972489\n",
      "Iteration 69, loss = 0.49718873\n",
      "Iteration 267, loss = 0.37745285\n",
      "Iteration 49, loss = 0.49293395\n",
      "Iteration 101, loss = 0.43332263\n",
      "Iteration 70, loss = 0.49517571\n",
      "Iteration 268, loss = 0.37723431\n",
      "Iteration 50, loss = 0.49077639\n",
      "Iteration 269, loss = 0.37701666\n",
      "Iteration 12, loss = 0.66037589\n",
      "Iteration 102, loss = 0.43243202\n",
      "Iteration 71, loss = 0.49317584\n",
      "Iteration 51, loss = 0.48870769\n",
      "Iteration 270, loss = 0.37680689\n",
      "Iteration 103, loss = 0.43157943\n",
      "Iteration 72, loss = 0.49125894\n",
      "Iteration 271, loss = 0.37660657\n",
      "Iteration 104, loss = 0.43076029\n",
      "Iteration 52, loss = 0.48668929\n",
      "Iteration 105, loss = 0.42992875\n",
      "Iteration 272, loss = 0.37637086\n",
      "Iteration 273, loss = 0.37617796\n",
      "Iteration 106, loss = 0.42913297\n",
      "Iteration 23, loss = 0.62617560\n",
      "Iteration 274, loss = 0.37598339\n",
      "Iteration 53, loss = 0.48470926\n",
      "Iteration 13, loss = 0.65665764\n",
      "Iteration 107, loss = 0.42825900\n",
      "Iteration 275, loss = 0.37578027\n",
      "Iteration 73, loss = 0.48934641\n",
      "Iteration 108, loss = 0.42750337\n",
      "Iteration 276, loss = 0.37556064\n",
      "Iteration 54, loss = 0.48281991\n",
      "Iteration 1, loss = 0.76107360\n",
      "Iteration 109, loss = 0.42672710\n",
      "Iteration 74, loss = 0.48750461\n",
      "Iteration 277, loss = 0.37539113\n",
      "Iteration 55, loss = 0.48091016\n",
      "Iteration 24, loss = 0.62280858\n",
      "Iteration 110, loss = 0.42593595\n",
      "Iteration 278, loss = 0.37516794\n",
      "Iteration 56, loss = 0.47895831\n",
      "Iteration 2, loss = 0.75884791\n",
      "Iteration 279, loss = 0.37495936\n",
      "Iteration 111, loss = 0.42517149\n",
      "Iteration 57, loss = 0.47719803\n",
      "Iteration 25, loss = 0.61950796\n",
      "Iteration 280, loss = 0.37475917\n",
      "Iteration 3, loss = 0.75570943\n",
      "Iteration 58, loss = 0.47546088\n",
      "Iteration 75, loss = 0.48566372\n",
      "Iteration 59, loss = 0.47373179\n",
      "Iteration 112, loss = 0.42442862\n",
      "Iteration 113, loss = 0.42369348\n",
      "Iteration 26, loss = 0.61613277\n",
      "Iteration 76, loss = 0.48383878\n",
      "Iteration 281, loss = 0.37457057\n",
      "Iteration 60, loss = 0.47201708\n",
      "Iteration 114, loss = 0.42294222\n",
      "Iteration 282, loss = 0.37436996\n",
      "Iteration 61, loss = 0.47032132\n",
      "Iteration 283, loss = 0.37417093\n",
      "Iteration 14, loss = 0.65302447\n",
      "Iteration 115, loss = 0.42221602\n",
      "Iteration 284, loss = 0.37397152\n",
      "Iteration 77, loss = 0.48214139\n",
      "Iteration 62, loss = 0.46873299\n",
      "Iteration 285, loss = 0.37377450\n",
      "Iteration 286, loss = 0.37359059\n",
      "Iteration 3, loss = 0.73722700\n",
      "Iteration 78, loss = 0.48034402\n",
      "Iteration 116, loss = 0.42149950\n",
      "Iteration 27, loss = 0.61281216\n",
      "Iteration 287, loss = 0.37338245\n",
      "Iteration 63, loss = 0.46708840\n",
      "Iteration 79, loss = 0.47873704\n",
      "Iteration 288, loss = 0.37320824\n",
      "Iteration 28, loss = 0.60952492\n",
      "Iteration 289, loss = 0.37300123\n",
      "Iteration 117, loss = 0.42080887\n",
      "Iteration 290, loss = 0.37280308\n",
      "Iteration 4, loss = 0.75198680\n",
      "Iteration 64, loss = 0.46559634\n",
      "Iteration 291, loss = 0.37263033\n",
      "Iteration 118, loss = 0.42012544\n",
      "Iteration 15, loss = 0.64938475\n",
      "Iteration 119, loss = 0.41944225\n",
      "Iteration 80, loss = 0.47702124\n",
      "Iteration 120, loss = 0.41878026\n",
      "Iteration 65, loss = 0.46406396\n",
      "Iteration 292, loss = 0.37242636\n",
      "Iteration 29, loss = 0.60631930\n",
      "Iteration 121, loss = 0.41810073\n",
      "Iteration 293, loss = 0.37223378\n",
      "Iteration 122, loss = 0.41742396\n",
      "Iteration 294, loss = 0.37205167\n",
      "Iteration 16, loss = 0.64579467\n",
      "Iteration 66, loss = 0.46257036\n",
      "Iteration 81, loss = 0.47538067\n",
      "Iteration 295, loss = 0.37186654\n",
      "Iteration 123, loss = 0.41677726\n",
      "Iteration 296, loss = 0.37166920\n",
      "Iteration 297, loss = 0.37150449\n",
      "Iteration 67, loss = 0.46113590\n",
      "Iteration 124, loss = 0.41616417\n",
      "Iteration 30, loss = 0.60302850\n",
      "Iteration 298, loss = 0.37130179\n",
      "Iteration 82, loss = 0.47374022\n",
      "Iteration 4, loss = 0.73156386\n",
      "Iteration 5, loss = 0.74774081\n",
      "Iteration 299, loss = 0.37111958\n",
      "Iteration 125, loss = 0.41551690\n",
      "Iteration 68, loss = 0.45968294\n",
      "Iteration 83, loss = 0.47216568\n",
      "Iteration 300, loss = 0.37092270\n",
      "Iteration 126, loss = 0.41491718\n",
      "Iteration 31, loss = 0.59977154\n",
      "Iteration 84, loss = 0.47069350\n",
      "Iteration 301, loss = 0.37073749\n",
      "Iteration 127, loss = 0.41429261\n",
      "Iteration 85, loss = 0.46924050\n",
      "Iteration 302, loss = 0.37056788\n",
      "Iteration 6, loss = 0.74340428\n",
      "Iteration 303, loss = 0.37037892\n",
      "Iteration 17, loss = 0.64219147\n",
      "Iteration 69, loss = 0.45829158\n",
      "Iteration 304, loss = 0.37019768\n",
      "Iteration 305, loss = 0.37001240\n",
      "Iteration 128, loss = 0.41368663\n",
      "Iteration 32, loss = 0.59665457\n",
      "Iteration 306, loss = 0.36982249\n",
      "Iteration 86, loss = 0.46771638\n",
      "Iteration 307, loss = 0.36965971\n",
      "Iteration 129, loss = 0.41307397\n",
      "Iteration 130, loss = 0.41247956\n",
      "Iteration 308, loss = 0.36947590\n",
      "Iteration 18, loss = 0.63864870\n",
      "Iteration 87, loss = 0.46624636\n",
      "Iteration 7, loss = 0.73876727\n",
      "Iteration 5, loss = 0.72504476\n",
      "Iteration 33, loss = 0.59357304\n",
      "Iteration 70, loss = 0.45686906Iteration 131, loss = 0.41188523\n",
      "\n",
      "Iteration 309, loss = 0.36928876\n",
      "Iteration 8, loss = 0.73451665\n",
      "Iteration 310, loss = 0.36911836\n",
      "Iteration 311, loss = 0.36894589\n",
      "Iteration 132, loss = 0.41129064\n",
      "Iteration 71, loss = 0.45554676\n",
      "Iteration 88, loss = 0.46488294\n",
      "Iteration 312, loss = 0.36879048\n",
      "Iteration 34, loss = 0.59038240\n",
      "Iteration 133, loss = 0.41074965\n",
      "Iteration 89, loss = 0.46344018\n",
      "Iteration 9, loss = 0.73035309\n",
      "Iteration 313, loss = 0.36859518\n",
      "Iteration 19, loss = 0.63520406\n",
      "Iteration 314, loss = 0.36842788\n",
      "Iteration 134, loss = 0.41017503\n",
      "Iteration 35, loss = 0.58727969\n",
      "Iteration 315, loss = 0.36823866\n",
      "Iteration 90, loss = 0.46204257\n",
      "Iteration 316, loss = 0.36807198\n",
      "Iteration 72, loss = 0.45419985Iteration 135, loss = 0.40959696\n",
      "\n",
      "Iteration 6, loss = 0.71851617\n",
      "Iteration 10, loss = 0.72608550\n",
      "Iteration 317, loss = 0.36790168\n",
      "Iteration 136, loss = 0.40906358\n",
      "Iteration 73, loss = 0.45292847\n",
      "Iteration 318, loss = 0.36772620\n",
      "Iteration 91, loss = 0.46078560\n",
      "Iteration 74, loss = 0.45160906\n",
      "Iteration 319, loss = 0.36757981\n",
      "Iteration 20, loss = 0.63170569\n",
      "Iteration 137, loss = 0.40851078\n",
      "Iteration 11, loss = 0.72214075\n",
      "Iteration 92, loss = 0.45943863\n",
      "Iteration 36, loss = 0.58422936\n",
      "Iteration 320, loss = 0.36739421\n",
      "Iteration 138, loss = 0.40796301\n",
      "Iteration 321, loss = 0.36721664\n",
      "Iteration 93, loss = 0.45815232\n",
      "Iteration 7, loss = 0.71134285\n",
      "Iteration 139, loss = 0.40742609\n",
      "Iteration 75, loss = 0.45041923\n",
      "Iteration 322, loss = 0.36704697\n",
      "Iteration 140, loss = 0.40688999\n",
      "Iteration 94, loss = 0.45694259\n",
      "Iteration 323, loss = 0.36687596\n",
      "Iteration 141, loss = 0.40637246\n",
      "Iteration 324, loss = 0.36672740\n",
      "Iteration 37, loss = 0.58112976\n",
      "Iteration 76, loss = 0.44916904\n",
      "Iteration 21, loss = 0.62839637\n",
      "Iteration 12, loss = 0.71842350\n",
      "Iteration 325, loss = 0.36655942\n",
      "Iteration 8, loss = 0.70444124\n",
      "Iteration 77, loss = 0.44800051\n",
      "Iteration 142, loss = 0.40584380\n",
      "Iteration 326, loss = 0.36637669\n",
      "Iteration 95, loss = 0.45572679\n",
      "Iteration 38, loss = 0.57811451\n",
      "Iteration 327, loss = 0.36621892\n",
      "Iteration 143, loss = 0.40533922\n",
      "Iteration 328, loss = 0.36606332\n",
      "Iteration 13, loss = 0.71464079\n",
      "Iteration 329, loss = 0.36588721\n",
      "Iteration 96, loss = 0.45453304\n",
      "Iteration 39, loss = 0.57511936\n",
      "Iteration 330, loss = 0.36574242\n",
      "Iteration 144, loss = 0.40487762\n",
      "Iteration 78, loss = 0.44679641\n",
      "Iteration 331, loss = 0.36557560\n",
      "Iteration 97, loss = 0.45335072\n",
      "Iteration 9, loss = 0.69752748\n",
      "Iteration 145, loss = 0.40437294\n",
      "Iteration 332, loss = 0.36541986\n",
      "Iteration 22, loss = 0.62501414\n",
      "Iteration 14, loss = 0.71104001\n",
      "Iteration 146, loss = 0.40386360\n",
      "Iteration 333, loss = 0.36525786\n",
      "Iteration 79, loss = 0.44565500\n",
      "Iteration 40, loss = 0.57206637\n",
      "Iteration 98, loss = 0.45217411\n",
      "Iteration 334, loss = 0.36509018\n",
      "Iteration 147, loss = 0.40336448\n",
      "Iteration 15, loss = 0.70761060\n",
      "Iteration 23, loss = 0.62176796\n",
      "Iteration 10, loss = 0.69077574\n",
      "Iteration 335, loss = 0.36492965\n",
      "Iteration 80, loss = 0.44449512\n",
      "Iteration 148, loss = 0.40287166\n",
      "Iteration 336, loss = 0.36478664\n",
      "Iteration 16, loss = 0.70440270Iteration 99, loss = 0.45102857\n",
      "\n",
      "Iteration 81, loss = 0.44337424\n",
      "Iteration 24, loss = 0.61855257\n",
      "Iteration 41, loss = 0.56911736\n",
      "Iteration 149, loss = 0.40238800\n",
      "Iteration 100, loss = 0.44992681\n",
      "Iteration 337, loss = 0.36461998\n",
      "Iteration 150, loss = 0.40193766\n",
      "Iteration 101, loss = 0.44889618\n",
      "Iteration 338, loss = 0.36446123\n",
      "Iteration 82, loss = 0.44227381\n",
      "Iteration 17, loss = 0.70112760\n",
      "Iteration 151, loss = 0.40145974\n",
      "Iteration 339, loss = 0.36430580\n",
      "Iteration 83, loss = 0.44121815\n",
      "Iteration 152, loss = 0.40102283\n",
      "Iteration 102, loss = 0.44784020\n",
      "Iteration 11, loss = 0.68401067\n",
      "Iteration 42, loss = 0.56618609\n",
      "Iteration 153, loss = 0.40056079\n",
      "Iteration 25, loss = 0.61526690\n",
      "Iteration 340, loss = 0.36414873\n",
      "Iteration 84, loss = 0.44016952\n",
      "Iteration 18, loss = 0.69799492\n",
      "Iteration 154, loss = 0.40007111\n",
      "Iteration 341, loss = 0.36399074\n",
      "Iteration 103, loss = 0.44685534\n",
      "Iteration 85, loss = 0.43915123\n",
      "Iteration 342, loss = 0.36383610\n",
      "Iteration 104, loss = 0.44584461\n",
      "Iteration 155, loss = 0.39964892\n",
      "Iteration 19, loss = 0.69497734\n",
      "Iteration 26, loss = 0.61210916\n",
      "Iteration 343, loss = 0.36368593\n",
      "Iteration 43, loss = 0.56315790\n",
      "Iteration 344, loss = 0.36353079\n",
      "Iteration 156, loss = 0.39920636\n",
      "Iteration 345, loss = 0.36337435\n",
      "Iteration 12, loss = 0.67753178\n",
      "Iteration 157, loss = 0.39875605\n",
      "Iteration 105, loss = 0.44481321\n",
      "Iteration 44, loss = 0.56024062\n",
      "Iteration 86, loss = 0.43814546Iteration 346, loss = 0.36323251\n",
      "\n",
      "Iteration 106, loss = 0.44379807\n",
      "Iteration 158, loss = 0.39834036\n",
      "Iteration 347, loss = 0.36307020\n",
      "Iteration 20, loss = 0.69194560\n",
      "Iteration 87, loss = 0.43710088\n",
      "Iteration 348, loss = 0.36293882\n",
      "Iteration 159, loss = 0.39791921\n",
      "Iteration 107, loss = 0.44291746\n",
      "Iteration 349, loss = 0.36277787\n",
      "Iteration 13, loss = 0.67131677\n",
      "Iteration 45, loss = 0.55735053\n",
      "Iteration 88, loss = 0.43612638\n",
      "Iteration 350, loss = 0.36262263\n",
      "Iteration 160, loss = 0.39747786\n",
      "Iteration 27, loss = 0.60899066\n",
      "Iteration 161, loss = 0.39706605Iteration 89, loss = 0.43515565\n",
      "\n",
      "Iteration 351, loss = 0.36247719\n",
      "Iteration 21, loss = 0.68906893\n",
      "Iteration 46, loss = 0.55450078\n",
      "Iteration 90, loss = 0.43423305\n",
      "Iteration 108, loss = 0.44195464\n",
      "Iteration 352, loss = 0.36233708\n",
      "Iteration 28, loss = 0.60594682\n",
      "Iteration 162, loss = 0.39666264\n",
      "Iteration 22, loss = 0.68629715\n",
      "Iteration 91, loss = 0.43332473\n",
      "Iteration 353, loss = 0.36217774\n",
      "Iteration 109, loss = 0.44105869\n",
      "Iteration 92, loss = 0.43240060\n",
      "Iteration 163, loss = 0.39624500\n",
      "Iteration 354, loss = 0.36203834\n",
      "Iteration 23, loss = 0.68348903\n",
      "Iteration 14, loss = 0.66527446\n",
      "Iteration 355, loss = 0.36188129\n",
      "Iteration 164, loss = 0.39585551\n",
      "Iteration 93, loss = 0.43148419\n",
      "Iteration 356, loss = 0.36174117\n",
      "Iteration 47, loss = 0.55171584\n",
      "Iteration 110, loss = 0.44010879\n",
      "Iteration 29, loss = 0.60281256\n",
      "Iteration 24, loss = 0.68064004\n",
      "Iteration 357, loss = 0.36160569\n",
      "Iteration 165, loss = 0.39543292\n",
      "Iteration 94, loss = 0.43061382\n",
      "Iteration 111, loss = 0.43926428\n",
      "Iteration 358, loss = 0.36146223\n",
      "Iteration 112, loss = 0.43836007\n",
      "Iteration 166, loss = 0.39503995\n",
      "Iteration 359, loss = 0.36130971\n",
      "Iteration 95, loss = 0.42976957\n",
      "Iteration 30, loss = 0.59984532\n",
      "Iteration 25, loss = 0.67793925\n",
      "Iteration 360, loss = 0.36116617\n",
      "Iteration 48, loss = 0.54877873\n",
      "Iteration 113, loss = 0.43750556\n",
      "Iteration 361, loss = 0.36102784\n",
      "Iteration 167, loss = 0.39463889\n",
      "Iteration 15, loss = 0.65946462\n",
      "Iteration 96, loss = 0.42887604\n",
      "Iteration 362, loss = 0.36088208\n",
      "Iteration 114, loss = 0.43667791\n",
      "Iteration 168, loss = 0.39423551\n",
      "Iteration 31, loss = 0.59679917\n",
      "Iteration 49, loss = 0.54606862\n",
      "Iteration 169, loss = 0.39387661\n",
      "Iteration 26, loss = 0.67528651\n",
      "Iteration 363, loss = 0.36073859\n",
      "Iteration 170, loss = 0.39350140\n",
      "Iteration 97, loss = 0.42804194\n",
      "Iteration 50, loss = 0.54326475\n",
      "Iteration 364, loss = 0.36060569\n",
      "Iteration 115, loss = 0.43582475\n",
      "Iteration 16, loss = 0.65378909\n",
      "Iteration 171, loss = 0.39309858\n",
      "Iteration 365, loss = 0.36047108\n",
      "Iteration 116, loss = 0.43506469\n",
      "Iteration 32, loss = 0.59388030\n",
      "Iteration 366, loss = 0.36032162\n",
      "Iteration 98, loss = 0.42718713\n",
      "Iteration 27, loss = 0.67244048\n",
      "Iteration 367, loss = 0.36018137\n",
      "Iteration 172, loss = 0.39271375\n",
      "Iteration 117, loss = 0.43420210\n",
      "Iteration 99, loss = 0.42637000\n",
      "Iteration 173, loss = 0.39236236\n",
      "Iteration 51, loss = 0.54051643\n",
      "Iteration 368, loss = 0.36005961\n",
      "Iteration 118, loss = 0.43344488\n",
      "Iteration 33, loss = 0.59095293\n",
      "Iteration 174, loss = 0.39199606\n",
      "Iteration 369, loss = 0.35990619\n",
      "Iteration 28, loss = 0.66975069\n",
      "Iteration 52, loss = 0.53779899\n",
      "Iteration 175, loss = 0.39161570\n",
      "Iteration 370, loss = 0.35976624\n",
      "Iteration 119, loss = 0.43267742\n",
      "Iteration 176, loss = 0.39125165\n",
      "Iteration 100, loss = 0.42560364\n",
      "Iteration 371, loss = 0.35963072\n",
      "Iteration 177, loss = 0.39091016\n",
      "Iteration 120, loss = 0.43189116\n",
      "Iteration 372, loss = 0.35948372\n",
      "Iteration 29, loss = 0.66703575\n",
      "Iteration 101, loss = 0.42477667\n",
      "Iteration 178, loss = 0.39055704\n",
      "Iteration 373, loss = 0.35935140\n",
      "Iteration 17, loss = 0.64823616\n",
      "Iteration 34, loss = 0.58811317\n",
      "Iteration 53, loss = 0.53518375\n",
      "Iteration 102, loss = 0.42400624\n",
      "Iteration 374, loss = 0.35920465\n",
      "Iteration 179, loss = 0.39019414\n",
      "Iteration 30, loss = 0.66446021\n",
      "Iteration 121, loss = 0.43115617\n",
      "Iteration 180, loss = 0.38985799\n",
      "Iteration 375, loss = 0.35907451\n",
      "Iteration 54, loss = 0.53258121\n",
      "Iteration 35, loss = 0.58521858\n",
      "Iteration 181, loss = 0.38954454\n",
      "Iteration 103, loss = 0.42322763\n",
      "Iteration 376, loss = 0.35896924\n",
      "Iteration 377, loss = 0.35881210\n",
      "Iteration 122, loss = 0.43040839\n",
      "Iteration 182, loss = 0.38917170\n",
      "Iteration 104, loss = 0.42248637\n",
      "Iteration 378, loss = 0.35866269\n",
      "Iteration 55, loss = 0.52995758\n",
      "Iteration 379, loss = 0.35853128\n",
      "Iteration 36, loss = 0.58247485\n",
      "Iteration 380, loss = 0.35839450\n",
      "Iteration 18, loss = 0.64296409\n",
      "Iteration 105, loss = 0.42172957\n",
      "Iteration 183, loss = 0.38882700\n",
      "Iteration 31, loss = 0.66171895\n",
      "Iteration 123, loss = 0.42972547\n",
      "Iteration 381, loss = 0.35826428\n",
      "Iteration 56, loss = 0.52734207\n",
      "Iteration 382, loss = 0.35813310\n",
      "Iteration 106, loss = 0.42101349\n",
      "Iteration 383, loss = 0.35800727\n",
      "Iteration 184, loss = 0.38850106\n",
      "Iteration 107, loss = 0.42029915\n",
      "Iteration 384, loss = 0.35786224\n",
      "Iteration 57, loss = 0.52481482\n",
      "Iteration 124, loss = 0.42898322\n",
      "Iteration 185, loss = 0.38816174\n",
      "Iteration 385, loss = 0.35772940\n",
      "Iteration 32, loss = 0.65919722\n",
      "Iteration 108, loss = 0.41955730\n",
      "Iteration 186, loss = 0.38783221\n",
      "Iteration 58, loss = 0.52234503\n",
      "Iteration 109, loss = 0.41886542\n",
      "Iteration 386, loss = 0.35759900\n",
      "Iteration 187, loss = 0.38751064\n",
      "Iteration 19, loss = 0.63772298\n",
      "Iteration 125, loss = 0.42829726\n",
      "Iteration 188, loss = 0.38717531\n",
      "Iteration 33, loss = 0.65640921\n",
      "Iteration 59, loss = 0.51981406\n",
      "Iteration 387, loss = 0.35746334\n",
      "Iteration 110, loss = 0.41818468\n",
      "Iteration 126, loss = 0.42760827\n",
      "Iteration 388, loss = 0.35732678\n",
      "Iteration 37, loss = 0.57959421\n",
      "Iteration 389, loss = 0.35721015\n",
      "Iteration 127, loss = 0.42690600\n",
      "Iteration 189, loss = 0.38685815\n",
      "Iteration 34, loss = 0.65378663\n",
      "Iteration 390, loss = 0.35707026\n",
      "Iteration 391, loss = 0.35694243\n",
      "Iteration 128, loss = 0.42626185\n",
      "Iteration 111, loss = 0.41750621\n",
      "Iteration 20, loss = 0.63279242\n",
      "Iteration 392, loss = 0.35681554\n",
      "Iteration 393, loss = 0.35668023\n",
      "Iteration 190, loss = 0.38655377\n",
      "Iteration 35, loss = 0.65113277\n",
      "Iteration 129, loss = 0.42562467\n",
      "Iteration 394, loss = 0.35655177\n",
      "Iteration 60, loss = 0.51735221\n",
      "Iteration 395, loss = 0.35642179\n",
      "Iteration 112, loss = 0.41682882\n",
      "Iteration 191, loss = 0.38621594\n",
      "Iteration 396, loss = 0.35630059\n",
      "Iteration 38, loss = 0.57689634\n",
      "Iteration 130, loss = 0.42493212\n",
      "Iteration 397, loss = 0.35617422\n",
      "Iteration 398, loss = 0.35603920\n",
      "Iteration 61, loss = 0.51499413\n",
      "Iteration 131, loss = 0.42433537\n",
      "Iteration 399, loss = 0.35592308\n",
      "Iteration 36, loss = 0.64851656\n",
      "Iteration 192, loss = 0.38588307\n",
      "Iteration 21, loss = 0.62780749\n",
      "Iteration 400, loss = 0.35579299\n",
      "Iteration 113, loss = 0.41618832\n",
      "Iteration 62, loss = 0.51252491\n",
      "Iteration 193, loss = 0.38560141\n",
      "Iteration 39, loss = 0.57411356\n",
      "Iteration 401, loss = 0.35567198\n",
      "Iteration 132, loss = 0.42371476\n",
      "Iteration 194, loss = 0.38528256\n",
      "Iteration 402, loss = 0.35553741\n",
      "Iteration 37, loss = 0.64575515\n",
      "Iteration 114, loss = 0.41548319\n",
      "Iteration 133, loss = 0.42310046\n",
      "Iteration 195, loss = 0.38495462\n",
      "Iteration 403, loss = 0.35541615\n",
      "Iteration 63, loss = 0.51027232\n",
      "Iteration 22, loss = 0.62294700\n",
      "Iteration 40, loss = 0.57137664\n",
      "Iteration 115, loss = 0.41487508\n",
      "Iteration 196, loss = 0.38466876\n",
      "Iteration 404, loss = 0.35530280\n",
      "Iteration 197, loss = 0.38437723\n",
      "Iteration 64, loss = 0.50802536\n",
      "Iteration 116, loss = 0.41423435\n",
      "Iteration 38, loss = 0.64313410\n",
      "Iteration 134, loss = 0.42245728\n",
      "Iteration 198, loss = 0.38406678\n",
      "Iteration 405, loss = 0.35518429\n",
      "Iteration 199, loss = 0.38378291\n",
      "Iteration 23, loss = 0.61826413\n",
      "Iteration 39, loss = 0.64043729\n",
      "Iteration 406, loss = 0.35505212\n",
      "Iteration 117, loss = 0.41359294\n",
      "Iteration 200, loss = 0.38346093\n",
      "Iteration 65, loss = 0.50567666\n",
      "Iteration 135, loss = 0.42183685\n",
      "Iteration 201, loss = 0.38317211\n",
      "Iteration 407, loss = 0.35493962\n",
      "Iteration 136, loss = 0.42127429\n",
      "Iteration 41, loss = 0.56875947\n",
      "Iteration 202, loss = 0.38287422\n",
      "Iteration 408, loss = 0.35482878\n",
      "Iteration 66, loss = 0.50335929\n",
      "Iteration 118, loss = 0.41300392\n",
      "Iteration 137, loss = 0.42069636\n",
      "Iteration 409, loss = 0.35469937\n",
      "Iteration 40, loss = 0.63776990\n",
      "Iteration 119, loss = 0.41238386\n",
      "Iteration 203, loss = 0.38259039\n",
      "Iteration 24, loss = 0.61376228\n",
      "Iteration 204, loss = 0.38229966\n",
      "Iteration 138, loss = 0.42009744\n",
      "Iteration 410, loss = 0.35457903\n",
      "Iteration 120, loss = 0.41177081\n",
      "Iteration 42, loss = 0.56607252\n",
      "Iteration 205, loss = 0.38200898\n",
      "Iteration 411, loss = 0.35446682\n",
      "Iteration 121, loss = 0.41119507\n",
      "Iteration 67, loss = 0.50111477\n",
      "Iteration 41, loss = 0.63506835\n",
      "Iteration 206, loss = 0.38173333\n",
      "Iteration 139, loss = 0.41954277\n",
      "Iteration 412, loss = 0.35433425\n",
      "Iteration 25, loss = 0.60925571\n",
      "Iteration 207, loss = 0.38144697\n",
      "Iteration 122, loss = 0.41061695\n",
      "Iteration 140, loss = 0.41896019\n",
      "Iteration 208, loss = 0.38118314\n",
      "Iteration 413, loss = 0.35420917\n",
      "Iteration 209, loss = 0.38089842\n",
      "Iteration 68, loss = 0.49900144\n",
      "Iteration 43, loss = 0.56342824\n",
      "Iteration 141, loss = 0.41841687\n",
      "Iteration 414, loss = 0.35409462\n",
      "Iteration 123, loss = 0.41003742\n",
      "Iteration 42, loss = 0.63233434\n",
      "Iteration 210, loss = 0.38061362\n",
      "Iteration 26, loss = 0.60478652\n",
      "Iteration 69, loss = 0.49678003\n",
      "Iteration 415, loss = 0.35398100\n",
      "Iteration 124, loss = 0.40946520\n",
      "Iteration 142, loss = 0.41786382\n",
      "Iteration 211, loss = 0.38035211\n",
      "Iteration 416, loss = 0.35388454\n",
      "Iteration 43, loss = 0.62967467\n",
      "Iteration 70, loss = 0.49481492\n",
      "Iteration 212, loss = 0.38009740\n",
      "Iteration 417, loss = 0.35374819\n",
      "Iteration 418, loss = 0.35363720\n",
      "Iteration 27, loss = 0.60054289\n",
      "Iteration 143, loss = 0.41732458\n",
      "Iteration 125, loss = 0.40891063\n",
      "Iteration 44, loss = 0.62683944\n",
      "Iteration 419, loss = 0.35351814\n",
      "Iteration 44, loss = 0.56081715\n",
      "Iteration 71, loss = 0.49260387\n",
      "Iteration 420, loss = 0.35340651\n",
      "Iteration 213, loss = 0.37979664\n",
      "Iteration 421, loss = 0.35328494\n",
      "Iteration 144, loss = 0.41676218\n",
      "Iteration 45, loss = 0.62420502\n",
      "Iteration 126, loss = 0.40834373\n",
      "Iteration 422, loss = 0.35318212\n",
      "Iteration 72, loss = 0.49054437\n",
      "Iteration 214, loss = 0.37954061\n",
      "Iteration 45, loss = 0.55829883\n",
      "Iteration 423, loss = 0.35306764\n",
      "Iteration 46, loss = 0.62139124\n",
      "Iteration 73, loss = 0.48854553\n",
      "Iteration 215, loss = 0.37927291\n",
      "Iteration 145, loss = 0.41626822\n",
      "Iteration 424, loss = 0.35294857\n",
      "Iteration 127, loss = 0.40781637\n",
      "Iteration 46, loss = 0.55577107\n",
      "Iteration 28, loss = 0.59638604\n",
      "Iteration 425, loss = 0.35282987\n",
      "Iteration 146, loss = 0.41573749\n",
      "Iteration 47, loss = 0.61863791\n",
      "Iteration 216, loss = 0.37900646\n",
      "Iteration 426, loss = 0.35271945\n",
      "Iteration 427, loss = 0.35260475\n",
      "Iteration 147, loss = 0.41522739\n",
      "Iteration 74, loss = 0.48653871\n",
      "Iteration 48, loss = 0.61597511\n",
      "Iteration 217, loss = 0.37874490\n",
      "Iteration 128, loss = 0.40727402\n",
      "Iteration 148, loss = 0.41469742\n",
      "Iteration 428, loss = 0.35249446\n",
      "Iteration 218, loss = 0.37848243\n",
      "Iteration 49, loss = 0.61319713\n",
      "Iteration 29, loss = 0.59221723\n",
      "Iteration 219, loss = 0.37820836\n",
      "Iteration 149, loss = 0.41420343\n",
      "Iteration 429, loss = 0.35237939\n",
      "Iteration 47, loss = 0.55325787\n",
      "Iteration 220, loss = 0.37796566\n",
      "Iteration 129, loss = 0.40673635\n",
      "Iteration 430, loss = 0.35226455\n",
      "Iteration 75, loss = 0.48460029\n",
      "Iteration 221, loss = 0.37771954\n",
      "Iteration 431, loss = 0.35215289\n",
      "Iteration 50, loss = 0.61039380\n",
      "Iteration 432, loss = 0.35204175\n",
      "Iteration 222, loss = 0.37745817Iteration 150, loss = 0.41367837\n",
      "\n",
      "Iteration 433, loss = 0.35192342\n",
      "Iteration 130, loss = 0.40619687\n",
      "Iteration 76, loss = 0.48274001\n",
      "Iteration 434, loss = 0.35181418\n",
      "Iteration 223, loss = 0.37720760\n",
      "Iteration 151, loss = 0.41318846\n",
      "Iteration 435, loss = 0.35169749\n",
      "Iteration 224, loss = 0.37694531\n",
      "Iteration 131, loss = 0.40569002\n",
      "Iteration 51, loss = 0.60753402\n",
      "Iteration 77, loss = 0.48079501\n",
      "Iteration 30, loss = 0.58826400\n",
      "Iteration 436, loss = 0.35158236\n",
      "Iteration 225, loss = 0.37671241\n",
      "Iteration 48, loss = 0.55080864\n",
      "Iteration 132, loss = 0.40519591\n",
      "Iteration 78, loss = 0.47897752\n",
      "Iteration 437, loss = 0.35147783\n",
      "Iteration 226, loss = 0.37647042\n",
      "Iteration 152, loss = 0.41269190\n",
      "Iteration 133, loss = 0.40464451\n",
      "Iteration 438, loss = 0.35136157\n",
      "Iteration 134, loss = 0.40417229\n",
      "Iteration 227, loss = 0.37622040\n",
      "Iteration 439, loss = 0.35124888\n",
      "Iteration 79, loss = 0.47722266\n",
      "Iteration 31, loss = 0.58422678\n",
      "Iteration 440, loss = 0.35114457\n",
      "Iteration 135, loss = 0.40367206\n",
      "Iteration 52, loss = 0.60483964\n",
      "Iteration 153, loss = 0.41220374\n",
      "Iteration 441, loss = 0.35103818\n",
      "Iteration 442, loss = 0.35092334\n",
      "Iteration 80, loss = 0.47536772\n",
      "Iteration 228, loss = 0.37597684\n",
      "Iteration 136, loss = 0.40317936\n",
      "Iteration 443, loss = 0.35082179\n",
      "Iteration 444, loss = 0.35070549\n",
      "Iteration 81, loss = 0.47361699\n",
      "Iteration 137, loss = 0.40269178\n",
      "Iteration 445, loss = 0.35059787\n",
      "Iteration 154, loss = 0.41173407\n",
      "Iteration 229, loss = 0.37572424\n",
      "Iteration 446, loss = 0.35049005\n",
      "Iteration 49, loss = 0.54835525\n",
      "Iteration 447, loss = 0.35038732\n",
      "Iteration 230, loss = 0.37547979\n",
      "Iteration 155, loss = 0.41127019\n",
      "Iteration 82, loss = 0.47190368\n",
      "Iteration 53, loss = 0.60218094\n",
      "Iteration 448, loss = 0.35027702\n",
      "Iteration 231, loss = 0.37524995\n",
      "Iteration 449, loss = 0.35016826\n",
      "Iteration 156, loss = 0.41078333\n",
      "Iteration 138, loss = 0.40222490\n",
      "Iteration 450, loss = 0.35006099\n",
      "Iteration 232, loss = 0.37501683\n",
      "Iteration 83, loss = 0.47021186\n",
      "Iteration 451, loss = 0.34995737\n",
      "Iteration 157, loss = 0.41032346\n",
      "Iteration 233, loss = 0.37477816\n",
      "Iteration 452, loss = 0.34985315\n",
      "Iteration 453, loss = 0.34975379\n",
      "Iteration 32, loss = 0.58026636\n",
      "Iteration 454, loss = 0.34963659\n",
      "Iteration 139, loss = 0.40175579\n",
      "Iteration 54, loss = 0.59938682\n",
      "Iteration 234, loss = 0.37455223\n",
      "Iteration 158, loss = 0.40986103\n",
      "Iteration 455, loss = 0.34953160\n",
      "Iteration 84, loss = 0.46855141\n",
      "Iteration 235, loss = 0.37432898\n",
      "Iteration 456, loss = 0.34943858\n",
      "Iteration 140, loss = 0.40126018\n",
      "Iteration 457, loss = 0.34932221\n",
      "Iteration 55, loss = 0.59660932\n",
      "Iteration 458, loss = 0.34922556\n",
      "Iteration 141, loss = 0.40081837\n",
      "Iteration 459, loss = 0.34911979\n",
      "Iteration 85, loss = 0.46697534\n",
      "Iteration 236, loss = 0.37410072\n",
      "Iteration 50, loss = 0.54589452\n",
      "Iteration 159, loss = 0.40942455\n",
      "Iteration 460, loss = 0.34901557\n",
      "Iteration 142, loss = 0.40034154\n",
      "Iteration 56, loss = 0.59383882\n",
      "Iteration 461, loss = 0.34891118\n",
      "Iteration 462, loss = 0.34881047\n",
      "Iteration 160, loss = 0.40895789\n",
      "Iteration 463, loss = 0.34871456\n",
      "Iteration 237, loss = 0.37385811\n",
      "Iteration 464, loss = 0.34860233\n",
      "Iteration 143, loss = 0.39989581\n",
      "Iteration 86, loss = 0.46534037\n",
      "Iteration 465, loss = 0.34850441\n",
      "Iteration 238, loss = 0.37363418\n",
      "Iteration 466, loss = 0.34840209\n",
      "Iteration 144, loss = 0.39945784\n",
      "Iteration 161, loss = 0.40854870\n",
      "Iteration 239, loss = 0.37340611\n",
      "Iteration 467, loss = 0.34829705\n",
      "Iteration 57, loss = 0.59107547\n",
      "Iteration 51, loss = 0.54346196\n",
      "Iteration 468, loss = 0.34821010\n",
      "Iteration 33, loss = 0.57643724\n",
      "Iteration 87, loss = 0.46387148\n",
      "Iteration 240, loss = 0.37319631\n",
      "Iteration 145, loss = 0.39897919\n",
      "Iteration 162, loss = 0.40809154\n",
      "Iteration 469, loss = 0.34810392\n",
      "Iteration 146, loss = 0.39858934\n",
      "Iteration 470, loss = 0.34800641\n",
      "Iteration 163, loss = 0.40766465\n",
      "Iteration 241, loss = 0.37295715\n",
      "Iteration 147, loss = 0.39815183\n",
      "Iteration 164, loss = 0.40721589\n",
      "Iteration 58, loss = 0.58833554\n",
      "Iteration 471, loss = 0.34789351\n",
      "Iteration 148, loss = 0.39768370\n",
      "Iteration 52, loss = 0.54119193\n",
      "Iteration 242, loss = 0.37272907\n",
      "Iteration 88, loss = 0.46230087\n",
      "Iteration 165, loss = 0.40678347\n",
      "Iteration 59, loss = 0.58554822\n",
      "Iteration 149, loss = 0.39729204\n",
      "Iteration 243, loss = 0.37253314\n",
      "Iteration 472, loss = 0.34779864\n",
      "Iteration 150, loss = 0.39684839\n",
      "Iteration 60, loss = 0.58282082\n",
      "Iteration 166, loss = 0.40637168\n",
      "Iteration 473, loss = 0.34770319\n",
      "Iteration 474, loss = 0.34759646\n",
      "Iteration 53, loss = 0.53880738\n",
      "Iteration 167, loss = 0.40597430\n",
      "Iteration 244, loss = 0.37231563\n",
      "Iteration 475, loss = 0.34751157\n",
      "Iteration 61, loss = 0.57998012\n",
      "Iteration 34, loss = 0.57272686\n",
      "Iteration 476, loss = 0.34742190\n",
      "Iteration 168, loss = 0.40554374\n",
      "Iteration 151, loss = 0.39644775\n",
      "Iteration 89, loss = 0.46081091\n",
      "Iteration 477, loss = 0.34731297\n",
      "Iteration 245, loss = 0.37208904\n",
      "Iteration 478, loss = 0.34720827\n",
      "Iteration 54, loss = 0.53655804\n",
      "Iteration 169, loss = 0.40513791\n",
      "Iteration 152, loss = 0.39602726\n",
      "Iteration 479, loss = 0.34711734\n",
      "Iteration 480, loss = 0.34702414\n",
      "Iteration 62, loss = 0.57723382\n",
      "Iteration 481, loss = 0.34692035\n",
      "Iteration 153, loss = 0.39562358\n",
      "Iteration 170, loss = 0.40473471\n",
      "Iteration 246, loss = 0.37187124\n",
      "Iteration 482, loss = 0.34683088\n",
      "Iteration 90, loss = 0.45932976\n",
      "Iteration 171, loss = 0.40431525\n",
      "Iteration 55, loss = 0.53425095\n",
      "Iteration 483, loss = 0.34674667\n",
      "Iteration 247, loss = 0.37166489\n",
      "Iteration 35, loss = 0.56907942\n",
      "Iteration 91, loss = 0.45784648\n",
      "Iteration 484, loss = 0.34663907\n",
      "Iteration 63, loss = 0.57449718\n",
      "Iteration 56, loss = 0.53201681\n",
      "Iteration 485, loss = 0.34653773\n",
      "Iteration 172, loss = 0.40392510\n",
      "Iteration 154, loss = 0.39521358\n",
      "Iteration 486, loss = 0.34645289\n",
      "Iteration 248, loss = 0.37143343\n",
      "Iteration 92, loss = 0.45648597\n",
      "Iteration 36, loss = 0.56545600\n",
      "Iteration 64, loss = 0.57172498\n",
      "Iteration 487, loss = 0.34635398\n",
      "Iteration 249, loss = 0.37124521\n",
      "Iteration 93, loss = 0.45511651\n",
      "Iteration 488, loss = 0.34625854\n",
      "Iteration 250, loss = 0.37102522\n",
      "Iteration 173, loss = 0.40355307\n",
      "Iteration 489, loss = 0.34617381\n",
      "Iteration 57, loss = 0.52985914\n",
      "Iteration 490, loss = 0.34608361\n",
      "Iteration 94, loss = 0.45374124\n",
      "Iteration 491, loss = 0.34599733\n",
      "Iteration 251, loss = 0.37081016\n",
      "Iteration 155, loss = 0.39483647\n",
      "Iteration 174, loss = 0.40315500\n",
      "Iteration 492, loss = 0.34588943\n",
      "Iteration 493, loss = 0.34581345\n",
      "Iteration 58, loss = 0.52761949\n",
      "Iteration 95, loss = 0.45238665\n",
      "Iteration 494, loss = 0.34571280\n",
      "Iteration 175, loss = 0.40274172\n",
      "Iteration 252, loss = 0.37060558\n",
      "Iteration 495, loss = 0.34561270\n",
      "Iteration 496, loss = 0.34553823\n",
      "Iteration 176, loss = 0.40235788\n",
      "Iteration 96, loss = 0.45112063\n",
      "Iteration 497, loss = 0.34544100\n",
      "Iteration 253, loss = 0.37040240\n",
      "Iteration 59, loss = 0.52552502\n",
      "Iteration 498, loss = 0.34533986\n",
      "Iteration 65, loss = 0.56900327\n",
      "Iteration 97, loss = 0.44980646\n",
      "Iteration 177, loss = 0.40195909\n",
      "Iteration 499, loss = 0.34525369\n",
      "Iteration 156, loss = 0.39443205\n",
      "Iteration 500, loss = 0.34515806\n",
      "Iteration 501, loss = 0.34507527\n",
      "Iteration 178, loss = 0.40158539\n",
      "Iteration 98, loss = 0.44853453\n",
      "Iteration 502, loss = 0.34498504\n",
      "Iteration 66, loss = 0.56632729\n",
      "Iteration 157, loss = 0.39401625\n",
      "Iteration 179, loss = 0.40121151\n",
      "Iteration 503, loss = 0.34489823\n",
      "Iteration 504, loss = 0.34482657\n",
      "Iteration 505, loss = 0.34472231\n",
      "Iteration 158, loss = 0.39363667\n",
      "Iteration 254, loss = 0.37020896\n",
      "Iteration 99, loss = 0.44733256\n",
      "Iteration 180, loss = 0.40082220\n",
      "Iteration 255, loss = 0.36999578\n",
      "Iteration 181, loss = 0.40048208\n",
      "Iteration 60, loss = 0.52330365\n",
      "Iteration 100, loss = 0.44612552\n",
      "Iteration 182, loss = 0.40009844\n",
      "Iteration 506, loss = 0.34465098\n",
      "Iteration 67, loss = 0.56362825\n",
      "Iteration 507, loss = 0.34454033\n",
      "Iteration 101, loss = 0.44494587\n",
      "Iteration 183, loss = 0.39973583\n",
      "Iteration 37, loss = 0.56194185\n",
      "Iteration 508, loss = 0.34446477\n",
      "Iteration 509, loss = 0.34436362\n",
      "Iteration 159, loss = 0.39326065\n",
      "Iteration 68, loss = 0.56084648\n",
      "Iteration 102, loss = 0.44379024\n",
      "Iteration 184, loss = 0.39936227\n",
      "Iteration 510, loss = 0.34429974\n",
      "Iteration 256, loss = 0.36977943\n",
      "Iteration 511, loss = 0.34419398\n",
      "Iteration 185, loss = 0.39898597\n",
      "Iteration 69, loss = 0.55818547\n",
      "Iteration 512, loss = 0.34410790\n",
      "Iteration 513, loss = 0.34403305\n",
      "Iteration 38, loss = 0.55843681\n",
      "Iteration 186, loss = 0.39862752\n",
      "Iteration 257, loss = 0.36958105\n",
      "Iteration 514, loss = 0.34394041\n",
      "Iteration 160, loss = 0.39285335\n",
      "Iteration 70, loss = 0.55561288\n",
      "Iteration 515, loss = 0.34384745\n",
      "Iteration 187, loss = 0.39825649\n",
      "Iteration 516, loss = 0.34376033\n",
      "Iteration 103, loss = 0.44262181\n",
      "Iteration 517, loss = 0.34368141\n",
      "Iteration 258, loss = 0.36939076\n",
      "Iteration 161, loss = 0.39249575\n",
      "Iteration 518, loss = 0.34360141\n",
      "Iteration 71, loss = 0.55287537\n",
      "Iteration 188, loss = 0.39789327\n",
      "Iteration 519, loss = 0.34350417\n",
      "Iteration 189, loss = 0.39754478\n",
      "Iteration 72, loss = 0.55015865\n",
      "Iteration 61, loss = 0.52119825\n",
      "Iteration 520, loss = 0.34341862\n",
      "Iteration 190, loss = 0.39717913\n",
      "Iteration 521, loss = 0.34334237\n",
      "Iteration 73, loss = 0.54760638\n",
      "Iteration 522, loss = 0.34324465\n",
      "Iteration 259, loss = 0.36918206\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 191, loss = 0.39685449\n",
      "Iteration 162, loss = 0.39214299\n",
      "Iteration 260, loss = 0.36899050\n",
      "Iteration 39, loss = 0.55505909\n",
      "Iteration 163, loss = 0.39177290\n",
      "Iteration 261, loss = 0.36879280\n",
      "Iteration 62, loss = 0.51923117\n",
      "Iteration 104, loss = 0.44147168\n",
      "Iteration 192, loss = 0.39649410\n",
      "Iteration 74, loss = 0.54502710\n",
      "Iteration 193, loss = 0.39615254\n",
      "Iteration 262, loss = 0.36858675\n",
      "Iteration 194, loss = 0.39579900\n",
      "Iteration 40, loss = 0.55171260\n",
      "Iteration 164, loss = 0.39137525\n",
      "Iteration 105, loss = 0.44038660\n",
      "Iteration 263, loss = 0.36840440\n",
      "Iteration 195, loss = 0.39544658\n",
      "Iteration 106, loss = 0.43930187\n",
      "Iteration 264, loss = 0.36821357\n",
      "Iteration 63, loss = 0.51711047\n",
      "Iteration 265, loss = 0.36801814\n",
      "Iteration 165, loss = 0.39102022\n",
      "Iteration 107, loss = 0.43824984\n",
      "Iteration 266, loss = 0.36782200\n",
      "Iteration 196, loss = 0.39512070\n",
      "Iteration 108, loss = 0.43721177\n",
      "Iteration 166, loss = 0.39065739\n",
      "Iteration 267, loss = 0.36763424\n",
      "Iteration 197, loss = 0.39479843\n",
      "Iteration 64, loss = 0.51511616\n",
      "Iteration 268, loss = 0.36744670\n",
      "Iteration 109, loss = 0.43622571\n",
      "Iteration 167, loss = 0.39030382\n",
      "Iteration 41, loss = 0.54836066\n",
      "Iteration 75, loss = 0.54235964\n",
      "Iteration 65, loss = 0.51320621\n",
      "Iteration 269, loss = 0.36727366\n",
      "Iteration 198, loss = 0.39446159\n",
      "Iteration 76, loss = 0.53989501\n",
      "Iteration 270, loss = 0.36708565\n",
      "Iteration 199, loss = 0.39413130\n",
      "Iteration 77, loss = 0.53733810\n",
      "Iteration 168, loss = 0.38996006\n",
      "Iteration 110, loss = 0.43519471\n",
      "Iteration 66, loss = 0.51126460\n",
      "Iteration 200, loss = 0.39377427\n",
      "Iteration 271, loss = 0.36688881\n",
      "Iteration 78, loss = 0.53482950\n",
      "Iteration 201, loss = 0.39344834\n",
      "Iteration 169, loss = 0.38959851\n",
      "Iteration 67, loss = 0.50930871\n",
      "Iteration 272, loss = 0.36670588\n",
      "Iteration 202, loss = 0.39312241\n",
      "Iteration 42, loss = 0.54531563\n",
      "Iteration 111, loss = 0.43425441\n",
      "Iteration 79, loss = 0.53239746\n",
      "Iteration 203, loss = 0.39280328\n",
      "Iteration 273, loss = 0.36652001\n",
      "Iteration 170, loss = 0.38925562\n",
      "Iteration 204, loss = 0.39248691\n",
      "Iteration 171, loss = 0.38890405\n",
      "Iteration 68, loss = 0.50744823\n",
      "Iteration 274, loss = 0.36634480\n",
      "Iteration 205, loss = 0.39213874\n",
      "Iteration 275, loss = 0.36616081\n",
      "Iteration 172, loss = 0.38857898\n",
      "Iteration 112, loss = 0.43326843\n",
      "Iteration 80, loss = 0.53003349\n",
      "Iteration 276, loss = 0.36599019\n",
      "Iteration 43, loss = 0.54207959\n",
      "Iteration 277, loss = 0.36579855\n",
      "Iteration 113, loss = 0.43230645\n",
      "Iteration 173, loss = 0.38822181\n",
      "Iteration 206, loss = 0.39183130\n",
      "Iteration 278, loss = 0.36562050\n",
      "Iteration 81, loss = 0.52751029\n",
      "Iteration 207, loss = 0.39151364\n",
      "Iteration 174, loss = 0.38789722\n",
      "Iteration 114, loss = 0.43138652\n",
      "Iteration 208, loss = 0.39120617\n",
      "Iteration 279, loss = 0.36545471\n",
      "Iteration 44, loss = 0.53898647Iteration 209, loss = 0.39088267\n",
      "\n",
      "Iteration 69, loss = 0.50555149\n",
      "Iteration 280, loss = 0.36528693\n",
      "Iteration 175, loss = 0.38756330\n",
      "Iteration 82, loss = 0.52517920\n",
      "Iteration 1, loss = 0.72031825\n",
      "Iteration 281, loss = 0.36509716\n",
      "Iteration 210, loss = 0.39059203\n",
      "Iteration 282, loss = 0.36494924\n",
      "Iteration 211, loss = 0.39028247\n",
      "Iteration 70, loss = 0.50374487\n",
      "Iteration 283, loss = 0.36474123\n",
      "Iteration 176, loss = 0.38721531\n",
      "Iteration 115, loss = 0.43047404\n",
      "Iteration 212, loss = 0.38992928\n",
      "Iteration 284, loss = 0.36457430\n",
      "Iteration 83, loss = 0.52275801\n",
      "Iteration 71, loss = 0.50191532\n",
      "Iteration 213, loss = 0.38963534\n",
      "Iteration 285, loss = 0.36440992\n",
      "Iteration 116, loss = 0.42954812\n",
      "Iteration 45, loss = 0.53600124\n",
      "Iteration 286, loss = 0.36424102\n",
      "Iteration 214, loss = 0.38931897\n",
      "Iteration 84, loss = 0.52050346\n",
      "Iteration 287, loss = 0.36407690\n",
      "Iteration 72, loss = 0.50016190\n",
      "Iteration 177, loss = 0.38688686\n",
      "Iteration 117, loss = 0.42869242\n",
      "Iteration 215, loss = 0.38903120\n",
      "Iteration 288, loss = 0.36389800\n",
      "Iteration 85, loss = 0.51820901\n",
      "Iteration 178, loss = 0.38659177\n",
      "Iteration 46, loss = 0.53300287\n",
      "Iteration 289, loss = 0.36373350\n",
      "Iteration 2, loss = 0.71900312\n",
      "Iteration 179, loss = 0.38625472\n",
      "Iteration 86, loss = 0.51602873\n",
      "Iteration 118, loss = 0.42776911\n",
      "Iteration 216, loss = 0.38872270\n",
      "Iteration 290, loss = 0.36356763\n",
      "Iteration 73, loss = 0.49835015\n",
      "Iteration 291, loss = 0.36340164\n",
      "Iteration 217, loss = 0.38840594\n",
      "Iteration 180, loss = 0.38593320\n",
      "Iteration 87, loss = 0.51373040\n",
      "Iteration 292, loss = 0.36325032\n",
      "Iteration 181, loss = 0.38563106\n",
      "Iteration 218, loss = 0.38810764\n",
      "Iteration 74, loss = 0.49673272\n",
      "Iteration 182, loss = 0.38531134\n",
      "Iteration 119, loss = 0.42693550\n",
      "Iteration 88, loss = 0.51160926\n",
      "Iteration 293, loss = 0.36306899\n",
      "Iteration 47, loss = 0.53014705\n",
      "Iteration 294, loss = 0.36291466\n",
      "Iteration 219, loss = 0.38779708\n",
      "Iteration 75, loss = 0.49496943\n",
      "Iteration 183, loss = 0.38498785\n",
      "Iteration 120, loss = 0.42609825\n",
      "Iteration 89, loss = 0.50954046\n",
      "Iteration 184, loss = 0.38467628\n",
      "Iteration 295, loss = 0.36275728\n",
      "Iteration 3, loss = 0.71693669\n",
      "Iteration 185, loss = 0.38437571\n",
      "Iteration 220, loss = 0.38751753\n",
      "Iteration 121, loss = 0.42526690\n",
      "Iteration 90, loss = 0.50733646\n",
      "Iteration 296, loss = 0.36260913\n",
      "Iteration 186, loss = 0.38407570\n",
      "Iteration 122, loss = 0.42447741\n",
      "Iteration 91, loss = 0.50541705\n",
      "Iteration 187, loss = 0.38376966\n",
      "Iteration 221, loss = 0.38720291\n",
      "Iteration 297, loss = 0.36243251\n",
      "Iteration 48, loss = 0.52723927\n",
      "Iteration 76, loss = 0.49339965\n",
      "Iteration 188, loss = 0.38347153\n",
      "Iteration 222, loss = 0.38688985\n",
      "Iteration 4, loss = 0.71448660\n",
      "Iteration 298, loss = 0.36227251\n",
      "Iteration 189, loss = 0.38316964\n",
      "Iteration 123, loss = 0.42363417\n",
      "Iteration 92, loss = 0.50330983\n",
      "Iteration 223, loss = 0.38660170\n",
      "Iteration 299, loss = 0.36212669\n",
      "Iteration 124, loss = 0.42290021\n",
      "Iteration 190, loss = 0.38287692\n",
      "Iteration 300, loss = 0.36195528\n",
      "Iteration 77, loss = 0.49175155\n",
      "Iteration 224, loss = 0.38629864\n",
      "Iteration 49, loss = 0.52449005\n",
      "Iteration 93, loss = 0.50126733\n",
      "Iteration 301, loss = 0.36179758\n",
      "Iteration 191, loss = 0.38259744\n",
      "Iteration 5, loss = 0.71158882\n",
      "Iteration 225, loss = 0.38602802\n",
      "Iteration 302, loss = 0.36163747\n",
      "Iteration 78, loss = 0.49004242\n",
      "Iteration 125, loss = 0.42211648\n",
      "Iteration 192, loss = 0.38228918\n",
      "Iteration 94, loss = 0.49934286\n",
      "Iteration 303, loss = 0.36147965\n",
      "Iteration 304, loss = 0.36132702\n",
      "Iteration 226, loss = 0.38570845\n",
      "Iteration 79, loss = 0.48848701\n",
      "Iteration 95, loss = 0.49748458\n",
      "Iteration 193, loss = 0.38200138\n",
      "Iteration 305, loss = 0.36117292\n",
      "Iteration 126, loss = 0.42132429\n",
      "Iteration 6, loss = 0.70845485\n",
      "Iteration 306, loss = 0.36102063\n",
      "Iteration 50, loss = 0.52176778\n",
      "Iteration 227, loss = 0.38543037\n",
      "Iteration 194, loss = 0.38172806\n",
      "Iteration 307, loss = 0.36087902\n",
      "Iteration 308, loss = 0.36073136\n",
      "Iteration 127, loss = 0.42058731\n",
      "Iteration 228, loss = 0.38514015\n",
      "Iteration 80, loss = 0.48692271\n",
      "Iteration 195, loss = 0.38144955\n",
      "Iteration 96, loss = 0.49563071\n",
      "Iteration 309, loss = 0.36058061\n",
      "Iteration 229, loss = 0.38486759\n",
      "Iteration 196, loss = 0.38116836\n",
      "Iteration 51, loss = 0.51912212\n",
      "Iteration 230, loss = 0.38455819\n",
      "Iteration 197, loss = 0.38087328\n",
      "Iteration 7, loss = 0.70527251\n",
      "Iteration 310, loss = 0.36042864\n",
      "Iteration 231, loss = 0.38426255\n",
      "Iteration 198, loss = 0.38059654\n",
      "Iteration 97, loss = 0.49382433\n",
      "Iteration 232, loss = 0.38399259\n",
      "Iteration 311, loss = 0.36027592\n",
      "Iteration 199, loss = 0.38032084\n",
      "Iteration 52, loss = 0.51636142\n",
      "Iteration 128, loss = 0.41982103\n",
      "Iteration 81, loss = 0.48541628\n",
      "Iteration 233, loss = 0.38370162\n",
      "Iteration 312, loss = 0.36012422\n",
      "Iteration 8, loss = 0.70180539\n",
      "Iteration 234, loss = 0.38343444\n",
      "Iteration 200, loss = 0.38005208\n",
      "Iteration 129, loss = 0.41912603\n",
      "Iteration 313, loss = 0.35998913\n",
      "Iteration 235, loss = 0.38314316\n",
      "Iteration 130, loss = 0.41843559\n",
      "Iteration 314, loss = 0.35982590\n",
      "Iteration 82, loss = 0.48388961\n",
      "Iteration 98, loss = 0.49196876\n",
      "Iteration 9, loss = 0.69836769\n",
      "Iteration 315, loss = 0.35968182\n",
      "Iteration 201, loss = 0.37979308\n",
      "Iteration 236, loss = 0.38286280\n",
      "Iteration 131, loss = 0.41771701\n",
      "Iteration 237, loss = 0.38258698Iteration 83, loss = 0.48244074\n",
      "\n",
      "Iteration 316, loss = 0.35954633\n",
      "Iteration 53, loss = 0.51380520\n",
      "Iteration 202, loss = 0.37952172\n",
      "Iteration 132, loss = 0.41703745\n",
      "Iteration 99, loss = 0.49023847\n",
      "Iteration 10, loss = 0.69484160\n",
      "Iteration 203, loss = 0.37922784\n",
      "Iteration 317, loss = 0.35939861\n",
      "Iteration 100, loss = 0.48847745\n",
      "Iteration 238, loss = 0.38230337\n",
      "Iteration 204, loss = 0.37898603\n",
      "Iteration 318, loss = 0.35925882\n",
      "Iteration 54, loss = 0.51134526\n",
      "Iteration 101, loss = 0.48686694\n",
      "Iteration 133, loss = 0.41634377\n",
      "Iteration 84, loss = 0.48097221\n",
      "Iteration 11, loss = 0.69143430\n",
      "Iteration 239, loss = 0.38204767\n",
      "Iteration 205, loss = 0.37871390\n",
      "Iteration 102, loss = 0.48514315\n",
      "Iteration 319, loss = 0.35911610\n",
      "Iteration 134, loss = 0.41565803\n",
      "Iteration 103, loss = 0.48349426\n",
      "Iteration 135, loss = 0.41503993\n",
      "Iteration 240, loss = 0.38177262\n",
      "Iteration 320, loss = 0.35895649\n",
      "Iteration 12, loss = 0.68793843\n",
      "Iteration 206, loss = 0.37847482\n",
      "Iteration 104, loss = 0.48203624\n",
      "Iteration 241, loss = 0.38149484\n",
      "Iteration 55, loss = 0.50880958\n",
      "Iteration 321, loss = 0.35883024\n",
      "Iteration 105, loss = 0.48034744\n",
      "Iteration 136, loss = 0.41435290\n",
      "Iteration 242, loss = 0.38119909\n",
      "Iteration 85, loss = 0.47952491\n",
      "Iteration 322, loss = 0.35869359\n",
      "Iteration 207, loss = 0.37819296\n",
      "Iteration 243, loss = 0.38094004\n",
      "Iteration 13, loss = 0.68446848\n",
      "Iteration 106, loss = 0.47885874\n",
      "Iteration 137, loss = 0.41378273\n",
      "Iteration 323, loss = 0.35853855\n",
      "Iteration 208, loss = 0.37794751\n",
      "Iteration 107, loss = 0.47731200\n",
      "Iteration 138, loss = 0.41312121\n",
      "Iteration 244, loss = 0.38067045\n",
      "Iteration 324, loss = 0.35840209\n",
      "Iteration 108, loss = 0.47586639\n",
      "Iteration 139, loss = 0.41252352\n",
      "Iteration 245, loss = 0.38042848\n",
      "Iteration 209, loss = 0.37767213\n",
      "Iteration 86, loss = 0.47813110\n",
      "Iteration 325, loss = 0.35827528\n",
      "Iteration 246, loss = 0.38013022\n",
      "Iteration 14, loss = 0.68088436\n",
      "Iteration 140, loss = 0.41189304\n",
      "Iteration 56, loss = 0.50648169\n",
      "Iteration 210, loss = 0.37743564\n",
      "Iteration 326, loss = 0.35812533\n",
      "Iteration 141, loss = 0.41130362\n",
      "Iteration 247, loss = 0.37985062\n",
      "Iteration 211, loss = 0.37716911\n",
      "Iteration 327, loss = 0.35798869\n",
      "Iteration 109, loss = 0.47443695\n",
      "Iteration 212, loss = 0.37692829\n",
      "Iteration 142, loss = 0.41071065\n",
      "Iteration 87, loss = 0.47673302\n",
      "Iteration 328, loss = 0.35785450\n",
      "Iteration 213, loss = 0.37669721\n",
      "Iteration 15, loss = 0.67745912\n",
      "Iteration 248, loss = 0.37960790\n",
      "Iteration 214, loss = 0.37642271\n",
      "Iteration 88, loss = 0.47536451\n",
      "Iteration 110, loss = 0.47305291\n",
      "Iteration 57, loss = 0.50408032\n",
      "Iteration 215, loss = 0.37619768\n",
      "Iteration 329, loss = 0.35772167\n",
      "Iteration 216, loss = 0.37596178\n",
      "Iteration 143, loss = 0.41010428\n",
      "Iteration 330, loss = 0.35758380\n",
      "Iteration 249, loss = 0.37933235\n",
      "Iteration 16, loss = 0.67390574\n",
      "Iteration 217, loss = 0.37570347\n",
      "Iteration 111, loss = 0.47174034\n",
      "Iteration 218, loss = 0.37546504\n",
      "Iteration 250, loss = 0.37907782\n",
      "Iteration 331, loss = 0.35744291\n",
      "Iteration 89, loss = 0.47399828\n",
      "Iteration 219, loss = 0.37523989\n",
      "Iteration 251, loss = 0.37884207\n",
      "Iteration 58, loss = 0.50175062\n",
      "Iteration 144, loss = 0.40958336\n",
      "Iteration 220, loss = 0.37498632\n",
      "Iteration 17, loss = 0.67058740\n",
      "Iteration 252, loss = 0.37855765\n",
      "Iteration 332, loss = 0.35730144\n",
      "Iteration 112, loss = 0.47025762\n",
      "Iteration 221, loss = 0.37473189\n",
      "Iteration 253, loss = 0.37830117\n",
      "Iteration 222, loss = 0.37451576\n",
      "Iteration 254, loss = 0.37803213\n",
      "Iteration 333, loss = 0.35719846\n",
      "Iteration 113, loss = 0.46896718\n",
      "Iteration 255, loss = 0.37779703\n",
      "Iteration 18, loss = 0.66708884\n",
      "Iteration 145, loss = 0.40898934\n",
      "Iteration 223, loss = 0.37428808\n",
      "Iteration 59, loss = 0.49941699\n",
      "Iteration 334, loss = 0.35703781\n",
      "Iteration 256, loss = 0.37754477\n",
      "Iteration 90, loss = 0.47262569\n",
      "Iteration 114, loss = 0.46782497\n",
      "Iteration 257, loss = 0.37727257\n",
      "Iteration 335, loss = 0.35690970\n",
      "Iteration 258, loss = 0.37701862\n",
      "Iteration 224, loss = 0.37405493\n",
      "Iteration 336, loss = 0.35678604\n",
      "Iteration 146, loss = 0.40838966\n",
      "Iteration 19, loss = 0.66373117\n",
      "Iteration 259, loss = 0.37676026\n",
      "Iteration 337, loss = 0.35665432\n",
      "Iteration 260, loss = 0.37651614\n",
      "Iteration 115, loss = 0.46640434\n",
      "Iteration 60, loss = 0.49718015\n",
      "Iteration 91, loss = 0.47135640\n",
      "Iteration 225, loss = 0.37381927\n",
      "Iteration 261, loss = 0.37626891\n",
      "Iteration 147, loss = 0.40796317\n",
      "Iteration 226, loss = 0.37359540\n",
      "Iteration 338, loss = 0.35653044\n",
      "Iteration 339, loss = 0.35640665\n",
      "Iteration 116, loss = 0.46527390\n",
      "Iteration 61, loss = 0.49498353\n",
      "Iteration 148, loss = 0.40734420\n",
      "Iteration 20, loss = 0.66039034\n",
      "Iteration 227, loss = 0.37336002\n",
      "Iteration 262, loss = 0.37600983\n",
      "Iteration 149, loss = 0.40682861\n",
      "Iteration 340, loss = 0.35625353\n",
      "Iteration 117, loss = 0.46401290\n",
      "Iteration 228, loss = 0.37313660\n",
      "Iteration 92, loss = 0.47004748\n",
      "Iteration 150, loss = 0.40628654\n",
      "Iteration 263, loss = 0.37575718\n",
      "Iteration 341, loss = 0.35612300\n",
      "Iteration 264, loss = 0.37550840\n",
      "Iteration 118, loss = 0.46282359\n",
      "Iteration 229, loss = 0.37290818\n",
      "Iteration 265, loss = 0.37528546\n",
      "Iteration 21, loss = 0.65695529\n",
      "Iteration 230, loss = 0.37267703\n",
      "Iteration 93, loss = 0.46882882\n",
      "Iteration 151, loss = 0.40576920\n",
      "Iteration 231, loss = 0.37246546\n",
      "Iteration 62, loss = 0.49291979\n",
      "Iteration 342, loss = 0.35599391\n",
      "Iteration 232, loss = 0.37224726\n",
      "Iteration 266, loss = 0.37500142\n",
      "Iteration 119, loss = 0.46172260\n",
      "Iteration 94, loss = 0.46748048\n",
      "Iteration 343, loss = 0.35587353\n",
      "Iteration 233, loss = 0.37203336\n",
      "Iteration 152, loss = 0.40524350\n",
      "Iteration 344, loss = 0.35575266\n",
      "Iteration 22, loss = 0.65366939\n",
      "Iteration 234, loss = 0.37179834\n",
      "Iteration 345, loss = 0.35561276\n",
      "Iteration 120, loss = 0.46056164\n",
      "Iteration 346, loss = 0.35549793\n",
      "Iteration 267, loss = 0.37476191\n",
      "Iteration 63, loss = 0.49079445\n",
      "Iteration 347, loss = 0.35537503\n",
      "Iteration 235, loss = 0.37159266\n",
      "Iteration 95, loss = 0.46633276\n",
      "Iteration 121, loss = 0.45944294\n",
      "Iteration 348, loss = 0.35524264\n",
      "Iteration 153, loss = 0.40478327\n",
      "Iteration 268, loss = 0.37452099\n",
      "Iteration 236, loss = 0.37137517\n",
      "Iteration 122, loss = 0.45834957\n",
      "Iteration 349, loss = 0.35513637\n",
      "Iteration 64, loss = 0.48874121\n",
      "Iteration 237, loss = 0.37115569\n",
      "Iteration 269, loss = 0.37427195\n",
      "Iteration 96, loss = 0.46510671\n",
      "Iteration 123, loss = 0.45735108\n",
      "Iteration 350, loss = 0.35499803\n",
      "Iteration 23, loss = 0.65024238\n",
      "Iteration 154, loss = 0.40426966\n",
      "Iteration 351, loss = 0.35489222\n",
      "Iteration 124, loss = 0.45629391\n",
      "Iteration 352, loss = 0.35474769\n",
      "Iteration 270, loss = 0.37405440\n",
      "Iteration 238, loss = 0.37095161\n",
      "Iteration 353, loss = 0.35461887\n",
      "Iteration 125, loss = 0.45531634\n",
      "Iteration 155, loss = 0.40373661\n",
      "Iteration 354, loss = 0.35450462\n",
      "Iteration 271, loss = 0.37375829\n",
      "Iteration 24, loss = 0.64682577\n",
      "Iteration 97, loss = 0.46388660\n",
      "Iteration 355, loss = 0.35438923\n",
      "Iteration 126, loss = 0.45425434\n",
      "Iteration 239, loss = 0.37073811\n",
      "Iteration 356, loss = 0.35426320\n",
      "Iteration 65, loss = 0.48674974\n",
      "Iteration 156, loss = 0.40332925\n",
      "Iteration 357, loss = 0.35413936\n",
      "Iteration 272, loss = 0.37354214\n",
      "Iteration 358, loss = 0.35402602\n",
      "Iteration 240, loss = 0.37051860\n",
      "Iteration 359, loss = 0.35389794\n",
      "Iteration 157, loss = 0.40280333\n",
      "Iteration 25, loss = 0.64357215\n",
      "Iteration 360, loss = 0.35378322\n",
      "Iteration 273, loss = 0.37326856\n",
      "Iteration 158, loss = 0.40231126\n",
      "Iteration 98, loss = 0.46277346\n",
      "Iteration 127, loss = 0.45326419\n",
      "Iteration 361, loss = 0.35367628\n",
      "Iteration 362, loss = 0.35354602\n",
      "Iteration 26, loss = 0.64031785\n",
      "Iteration 363, loss = 0.35342584\n",
      "Iteration 241, loss = 0.37031631\n",
      "Iteration 159, loss = 0.40186645\n",
      "Iteration 364, loss = 0.35331086\n",
      "Iteration 274, loss = 0.37302825\n",
      "Iteration 128, loss = 0.45234726\n",
      "Iteration 365, loss = 0.35319088\n",
      "Iteration 242, loss = 0.37009511\n",
      "Iteration 366, loss = 0.35308850\n",
      "Iteration 160, loss = 0.40139014\n",
      "Iteration 275, loss = 0.37278111\n",
      "Iteration 367, loss = 0.35297155\n",
      "Iteration 27, loss = 0.63712668\n",
      "Iteration 99, loss = 0.46155934\n",
      "Iteration 66, loss = 0.48477896Iteration 368, loss = 0.35284398\n",
      "\n",
      "Iteration 243, loss = 0.36989394\n",
      "Iteration 276, loss = 0.37256302\n",
      "Iteration 129, loss = 0.45137802\n",
      "Iteration 100, loss = 0.46043657\n",
      "Iteration 277, loss = 0.37231249\n",
      "Iteration 369, loss = 0.35273677\n",
      "Iteration 161, loss = 0.40095708\n",
      "Iteration 130, loss = 0.45050757\n",
      "Iteration 278, loss = 0.37206302\n",
      "Iteration 370, loss = 0.35262401\n",
      "Iteration 244, loss = 0.36970427\n",
      "Iteration 131, loss = 0.44957128\n",
      "Iteration 371, loss = 0.35249718\n",
      "Iteration 279, loss = 0.37184441\n",
      "Iteration 101, loss = 0.45930378\n",
      "Iteration 162, loss = 0.40047436\n",
      "Iteration 372, loss = 0.35240403\n",
      "Iteration 280, loss = 0.37160838\n",
      "Iteration 132, loss = 0.44870178\n",
      "Iteration 28, loss = 0.63387128\n",
      "Iteration 67, loss = 0.48282798\n",
      "Iteration 102, loss = 0.45817525\n",
      "Iteration 373, loss = 0.35227300\n",
      "Iteration 245, loss = 0.36947991\n",
      "Iteration 281, loss = 0.37135777\n",
      "Iteration 246, loss = 0.36928477\n",
      "Iteration 163, loss = 0.40009550\n",
      "Iteration 133, loss = 0.44781591\n",
      "Iteration 374, loss = 0.35216994\n",
      "Iteration 103, loss = 0.45710234\n",
      "Iteration 247, loss = 0.36908661\n",
      "Iteration 282, loss = 0.37113040\n",
      "Iteration 68, loss = 0.48103092\n",
      "Iteration 134, loss = 0.44697670\n",
      "Iteration 248, loss = 0.36887592\n",
      "Iteration 104, loss = 0.45601654\n",
      "Iteration 375, loss = 0.35206845\n",
      "Iteration 164, loss = 0.39962142\n",
      "Iteration 283, loss = 0.37090785\n",
      "Iteration 376, loss = 0.35195586\n",
      "Iteration 29, loss = 0.63059687\n",
      "Iteration 105, loss = 0.45495412\n",
      "Iteration 284, loss = 0.37067259\n",
      "Iteration 165, loss = 0.39918950\n",
      "Iteration 377, loss = 0.35183255\n",
      "Iteration 249, loss = 0.36868708\n",
      "Iteration 135, loss = 0.44613145\n",
      "Iteration 106, loss = 0.45391506\n",
      "Iteration 30, loss = 0.62735678\n",
      "Iteration 378, loss = 0.35173632\n",
      "Iteration 379, loss = 0.35161454\n",
      "Iteration 166, loss = 0.39875221\n",
      "Iteration 285, loss = 0.37041464Iteration 69, loss = 0.47922726\n",
      "\n",
      "Iteration 136, loss = 0.44531343\n",
      "Iteration 250, loss = 0.36846730\n",
      "Iteration 167, loss = 0.39835817\n",
      "Iteration 31, loss = 0.62414226\n",
      "Iteration 380, loss = 0.35150306\n",
      "Iteration 137, loss = 0.44451143\n",
      "Iteration 251, loss = 0.36828644\n",
      "Iteration 286, loss = 0.37018953\n",
      "Iteration 381, loss = 0.35138920\n",
      "Iteration 107, loss = 0.45287713\n",
      "Iteration 382, loss = 0.35127766\n",
      "Iteration 138, loss = 0.44368924\n",
      "Iteration 168, loss = 0.39791440\n",
      "Iteration 70, loss = 0.47733224\n",
      "Iteration 287, loss = 0.36999447\n",
      "Iteration 383, loss = 0.35118984\n",
      "Iteration 288, loss = 0.36973811\n",
      "Iteration 252, loss = 0.36808186\n",
      "Iteration 139, loss = 0.44294777\n",
      "Iteration 384, loss = 0.35108014\n",
      "Iteration 289, loss = 0.36952452\n",
      "Iteration 140, loss = 0.44216946\n",
      "Iteration 169, loss = 0.39749822\n",
      "Iteration 71, loss = 0.47565284\n",
      "Iteration 385, loss = 0.35096624\n",
      "Iteration 108, loss = 0.45183561\n",
      "Iteration 386, loss = 0.35085146\n",
      "Iteration 141, loss = 0.44146773\n",
      "Iteration 290, loss = 0.36931160\n",
      "Iteration 387, loss = 0.35075213\n",
      "Iteration 253, loss = 0.36789372\n",
      "Iteration 388, loss = 0.35064323\n",
      "Iteration 142, loss = 0.44068834\n",
      "Iteration 109, loss = 0.45088347\n",
      "Iteration 291, loss = 0.36906653\n",
      "Iteration 389, loss = 0.35053244\n",
      "Iteration 170, loss = 0.39710119\n",
      "Iteration 390, loss = 0.35042989\n",
      "Iteration 292, loss = 0.36887067\n",
      "Iteration 32, loss = 0.62096648\n",
      "Iteration 143, loss = 0.43998009\n",
      "Iteration 110, loss = 0.44990782\n",
      "Iteration 254, loss = 0.36769434\n",
      "Iteration 293, loss = 0.36864208\n",
      "Iteration 171, loss = 0.39671054\n",
      "Iteration 391, loss = 0.35032113\n",
      "Iteration 111, loss = 0.44891546\n",
      "Iteration 392, loss = 0.35023191\n",
      "Iteration 255, loss = 0.36750932\n",
      "Iteration 393, loss = 0.35011464\n",
      "Iteration 144, loss = 0.43927271\n",
      "Iteration 294, loss = 0.36841250\n",
      "Iteration 394, loss = 0.35002855\n",
      "Iteration 72, loss = 0.47392025\n",
      "Iteration 112, loss = 0.44800514\n",
      "Iteration 295, loss = 0.36819632\n",
      "Iteration 395, loss = 0.34991783\n",
      "Iteration 33, loss = 0.61771242\n",
      "Iteration 172, loss = 0.39627673\n",
      "Iteration 396, loss = 0.34981460\n",
      "Iteration 397, loss = 0.34969726\n",
      "Iteration 296, loss = 0.36800846\n",
      "Iteration 256, loss = 0.36731834\n",
      "Iteration 113, loss = 0.44703682\n",
      "Iteration 145, loss = 0.43855358\n",
      "Iteration 398, loss = 0.34959462\n",
      "Iteration 173, loss = 0.39592644\n",
      "Iteration 297, loss = 0.36779174\n",
      "Iteration 399, loss = 0.34949466\n",
      "Iteration 146, loss = 0.43786780\n",
      "Iteration 257, loss = 0.36711271\n",
      "Iteration 114, loss = 0.44606288\n",
      "Iteration 400, loss = 0.34938579\n",
      "Iteration 298, loss = 0.36755398\n",
      "Iteration 73, loss = 0.47226824\n",
      "Iteration 174, loss = 0.39550807\n",
      "Iteration 299, loss = 0.36738292\n",
      "Iteration 258, loss = 0.36691885\n",
      "Iteration 401, loss = 0.34929304\n",
      "Iteration 34, loss = 0.61455096\n",
      "Iteration 175, loss = 0.39514397\n",
      "Iteration 402, loss = 0.34918137\n",
      "Iteration 147, loss = 0.43717730\n",
      "Iteration 300, loss = 0.36715239\n",
      "Iteration 259, loss = 0.36673667\n",
      "Iteration 115, loss = 0.44519545\n",
      "Iteration 403, loss = 0.34908332\n",
      "Iteration 404, loss = 0.34897963\n",
      "Iteration 176, loss = 0.39472715\n",
      "Iteration 74, loss = 0.47068024\n",
      "Iteration 148, loss = 0.43648682\n",
      "Iteration 301, loss = 0.36693920\n",
      "Iteration 35, loss = 0.61146776\n",
      "Iteration 405, loss = 0.34889027\n",
      "Iteration 260, loss = 0.36655016\n",
      "Iteration 116, loss = 0.44426780\n",
      "Iteration 177, loss = 0.39434191\n",
      "Iteration 149, loss = 0.43584609\n",
      "Iteration 302, loss = 0.36675054\n",
      "Iteration 406, loss = 0.34878835\n",
      "Iteration 36, loss = 0.60826798\n",
      "Iteration 261, loss = 0.36636961\n",
      "Iteration 407, loss = 0.34868492\n",
      "Iteration 178, loss = 0.39398349\n",
      "Iteration 303, loss = 0.36653369\n",
      "Iteration 408, loss = 0.34858350\n",
      "Iteration 150, loss = 0.43515541\n",
      "Iteration 117, loss = 0.44334046\n",
      "Iteration 304, loss = 0.36631977\n",
      "Iteration 305, loss = 0.36612516\n",
      "Iteration 262, loss = 0.36618170\n",
      "Iteration 409, loss = 0.34848514\n",
      "Iteration 37, loss = 0.60510066\n",
      "Iteration 179, loss = 0.39359523\n",
      "Iteration 151, loss = 0.43457670\n",
      "Iteration 75, loss = 0.46904459\n",
      "Iteration 118, loss = 0.44249103\n",
      "Iteration 306, loss = 0.36594332\n",
      "Iteration 410, loss = 0.34838868\n",
      "Iteration 263, loss = 0.36600166\n",
      "Iteration 307, loss = 0.36569922\n",
      "Iteration 411, loss = 0.34830603\n",
      "Iteration 180, loss = 0.39326019\n",
      "Iteration 152, loss = 0.43389203\n",
      "Iteration 264, loss = 0.36581073\n",
      "Iteration 412, loss = 0.34821035\n",
      "Iteration 413, loss = 0.34808688\n",
      "Iteration 153, loss = 0.43326981\n",
      "Iteration 265, loss = 0.36562389\n",
      "Iteration 308, loss = 0.36549638\n",
      "Iteration 38, loss = 0.60202153\n",
      "Iteration 266, loss = 0.36544414\n",
      "Iteration 154, loss = 0.43267022\n",
      "Iteration 414, loss = 0.34798975\n",
      "Iteration 119, loss = 0.44164223\n",
      "Iteration 181, loss = 0.39286864\n",
      "Iteration 309, loss = 0.36530991\n",
      "Iteration 267, loss = 0.36526414\n",
      "Iteration 76, loss = 0.46747066\n",
      "Iteration 310, loss = 0.36509882\n",
      "Iteration 268, loss = 0.36508000\n",
      "Iteration 415, loss = 0.34790341\n",
      "Iteration 155, loss = 0.43205531\n",
      "Iteration 269, loss = 0.36491108\n",
      "Iteration 270, loss = 0.36473209\n",
      "Iteration 156, loss = 0.43145326\n",
      "Iteration 182, loss = 0.39253053\n",
      "Iteration 311, loss = 0.36490008\n",
      "Iteration 120, loss = 0.44075585\n",
      "Iteration 271, loss = 0.36454369\n",
      "Iteration 272, loss = 0.36437050\n",
      "Iteration 157, loss = 0.43085859\n",
      "Iteration 273, loss = 0.36418222\n",
      "Iteration 77, loss = 0.46594506\n",
      "Iteration 416, loss = 0.34780013\n",
      "Iteration 39, loss = 0.59889661\n",
      "Iteration 183, loss = 0.39214324\n",
      "Iteration 274, loss = 0.36401294\n",
      "Iteration 312, loss = 0.36470028\n",
      "Iteration 275, loss = 0.36384892\n",
      "Iteration 417, loss = 0.34772464\n",
      "Iteration 121, loss = 0.43995491\n",
      "Iteration 158, loss = 0.43027741\n",
      "Iteration 184, loss = 0.39180530\n",
      "Iteration 78, loss = 0.46449538\n",
      "Iteration 418, loss = 0.34760686\n",
      "Iteration 276, loss = 0.36366195\n",
      "Iteration 313, loss = 0.36451068\n",
      "Iteration 185, loss = 0.39144130\n",
      "Iteration 277, loss = 0.36349065\n",
      "Iteration 122, loss = 0.43910423\n",
      "Iteration 159, loss = 0.42969833\n",
      "Iteration 314, loss = 0.36429643\n",
      "Iteration 40, loss = 0.59591148\n",
      "Iteration 186, loss = 0.39111819\n",
      "Iteration 278, loss = 0.36332151\n",
      "Iteration 419, loss = 0.34751804\n",
      "Iteration 279, loss = 0.36314575\n",
      "Iteration 187, loss = 0.39074910\n",
      "Iteration 315, loss = 0.36412917\n",
      "Iteration 79, loss = 0.46299788\n",
      "Iteration 123, loss = 0.43827413\n",
      "Iteration 420, loss = 0.34741024\n",
      "Iteration 160, loss = 0.42908998\n",
      "Iteration 421, loss = 0.34732602\n",
      "Iteration 41, loss = 0.59277692\n",
      "Iteration 316, loss = 0.36390440\n",
      "Iteration 422, loss = 0.34722856\n",
      "Iteration 80, loss = 0.46149276\n",
      "Iteration 161, loss = 0.42855000\n",
      "Iteration 280, loss = 0.36298401\n",
      "Iteration 124, loss = 0.43749856\n",
      "Iteration 188, loss = 0.39042937\n",
      "Iteration 423, loss = 0.34712789\n",
      "Iteration 81, loss = 0.46011327\n",
      "Iteration 125, loss = 0.43666307\n",
      "Iteration 424, loss = 0.34703962\n",
      "Iteration 162, loss = 0.42794164\n",
      "Iteration 281, loss = 0.36281305\n",
      "Iteration 317, loss = 0.36370720\n",
      "Iteration 425, loss = 0.34694090\n",
      "Iteration 318, loss = 0.36351919\n",
      "Iteration 189, loss = 0.39008412\n",
      "Iteration 126, loss = 0.43587976\n",
      "Iteration 319, loss = 0.36333149\n",
      "Iteration 426, loss = 0.34684973\n",
      "Iteration 320, loss = 0.36311673\n",
      "Iteration 282, loss = 0.36264285\n",
      "Iteration 82, loss = 0.45874882\n",
      "Iteration 321, loss = 0.36292525\n",
      "Iteration 127, loss = 0.43512946\n",
      "Iteration 190, loss = 0.38974819\n",
      "Iteration 427, loss = 0.34674814\n",
      "Iteration 163, loss = 0.42740762\n",
      "Iteration 322, loss = 0.36274323\n",
      "Iteration 323, loss = 0.36254460\n",
      "Iteration 191, loss = 0.38943765\n",
      "Iteration 42, loss = 0.58984673\n",
      "Iteration 428, loss = 0.34666805\n",
      "Iteration 283, loss = 0.36245991\n",
      "Iteration 128, loss = 0.43434993\n",
      "Iteration 164, loss = 0.42685618\n",
      "Iteration 429, loss = 0.34656987\n",
      "Iteration 83, loss = 0.45749406\n",
      "Iteration 324, loss = 0.36236418\n",
      "Iteration 165, loss = 0.42630874\n",
      "Iteration 192, loss = 0.38908210\n",
      "Iteration 284, loss = 0.36230921\n",
      "Iteration 430, loss = 0.34650206\n",
      "Iteration 129, loss = 0.43360055\n",
      "Iteration 325, loss = 0.36217744Iteration 431, loss = 0.34639362\n",
      "\n",
      "Iteration 84, loss = 0.45606761\n",
      "Iteration 193, loss = 0.38874655\n",
      "Iteration 285, loss = 0.36213764\n",
      "Iteration 166, loss = 0.42578446\n",
      "Iteration 432, loss = 0.34630251\n",
      "Iteration 326, loss = 0.36198811\n",
      "Iteration 194, loss = 0.38844120\n",
      "Iteration 43, loss = 0.58675107\n",
      "Iteration 327, loss = 0.36181671\n",
      "Iteration 433, loss = 0.34620647\n",
      "Iteration 286, loss = 0.36196568\n",
      "Iteration 195, loss = 0.38811798\n",
      "Iteration 167, loss = 0.42528196\n",
      "Iteration 85, loss = 0.45481948\n",
      "Iteration 130, loss = 0.43282614\n",
      "Iteration 44, loss = 0.58374780\n",
      "Iteration 287, loss = 0.36180476\n",
      "Iteration 196, loss = 0.38780801\n",
      "Iteration 288, loss = 0.36164289\n",
      "Iteration 86, loss = 0.45360757\n",
      "Iteration 197, loss = 0.38749198\n",
      "Iteration 289, loss = 0.36146211\n",
      "Iteration 45, loss = 0.58082339\n",
      "Iteration 131, loss = 0.43209799\n",
      "Iteration 87, loss = 0.45230150\n",
      "Iteration 198, loss = 0.38719403\n",
      "Iteration 132, loss = 0.43135428\n",
      "Iteration 290, loss = 0.36130430\n",
      "Iteration 46, loss = 0.57788403\n",
      "Iteration 88, loss = 0.45108218\n",
      "Iteration 199, loss = 0.38687530\n",
      "Iteration 133, loss = 0.43065569\n",
      "Iteration 291, loss = 0.36114405\n",
      "Iteration 200, loss = 0.38656970\n",
      "Iteration 292, loss = 0.36098748\n",
      "Iteration 134, loss = 0.42993857\n",
      "Iteration 47, loss = 0.57501965\n",
      "Iteration 434, loss = 0.34611905\n",
      "Iteration 328, loss = 0.36162175\n",
      "Iteration 168, loss = 0.42469157\n",
      "Iteration 293, loss = 0.36081659\n",
      "Iteration 135, loss = 0.42924146\n",
      "Iteration 201, loss = 0.38625947\n",
      "Iteration 294, loss = 0.36066652\n",
      "Iteration 435, loss = 0.34602094\n",
      "Iteration 329, loss = 0.36145150\n",
      "Iteration 202, loss = 0.38599583\n",
      "Iteration 295, loss = 0.36049814\n",
      "Iteration 136, loss = 0.42856016\n",
      "Iteration 436, loss = 0.34592137\n",
      "Iteration 48, loss = 0.57213607\n",
      "Iteration 203, loss = 0.38569148\n",
      "Iteration 330, loss = 0.36124842\n",
      "Iteration 437, loss = 0.34584020\n",
      "Iteration 137, loss = 0.42786686\n",
      "Iteration 89, loss = 0.44990147\n",
      "Iteration 296, loss = 0.36034198\n",
      "Iteration 331, loss = 0.36110898\n",
      "Iteration 297, loss = 0.36017611\n",
      "Iteration 332, loss = 0.36090692\n",
      "Iteration 138, loss = 0.42717042\n",
      "Iteration 438, loss = 0.34576311\n",
      "Iteration 204, loss = 0.38538656\n",
      "Iteration 439, loss = 0.34566066\n",
      "Iteration 205, loss = 0.38509042\n",
      "Iteration 139, loss = 0.42656204\n",
      "Iteration 333, loss = 0.36072070\n",
      "Iteration 440, loss = 0.34557004\n",
      "Iteration 298, loss = 0.36003417\n",
      "Iteration 169, loss = 0.42419059\n",
      "Iteration 49, loss = 0.56925987\n",
      "Iteration 90, loss = 0.44876795\n",
      "Iteration 299, loss = 0.35987163\n",
      "Iteration 441, loss = 0.34547747\n",
      "Iteration 170, loss = 0.42367073\n",
      "Iteration 334, loss = 0.36053756\n",
      "Iteration 206, loss = 0.38480411\n",
      "Iteration 300, loss = 0.35970910\n",
      "Iteration 442, loss = 0.34538898\n",
      "Iteration 207, loss = 0.38450904\n",
      "Iteration 50, loss = 0.56648340\n",
      "Iteration 301, loss = 0.35954499\n",
      "Iteration 335, loss = 0.36035487\n",
      "Iteration 91, loss = 0.44765988Iteration 443, loss = 0.34529597\n",
      "\n",
      "Iteration 208, loss = 0.38424167\n",
      "Iteration 302, loss = 0.35940033\n",
      "Iteration 171, loss = 0.42313574\n",
      "Iteration 209, loss = 0.38395841\n",
      "Iteration 444, loss = 0.34521263\n",
      "Iteration 336, loss = 0.36019873\n",
      "Iteration 303, loss = 0.35925033\n",
      "Iteration 51, loss = 0.56367556\n",
      "Iteration 210, loss = 0.38367805\n",
      "Iteration 445, loss = 0.34512758\n",
      "Iteration 337, loss = 0.36000894Iteration 172, loss = 0.42262055\n",
      "\n",
      "Iteration 304, loss = 0.35910188\n",
      "Iteration 211, loss = 0.38339640\n",
      "Iteration 338, loss = 0.35982027\n",
      "Iteration 446, loss = 0.34502853\n",
      "Iteration 140, loss = 0.42585622\n",
      "Iteration 92, loss = 0.44654895\n",
      "Iteration 212, loss = 0.38312907\n",
      "Iteration 52, loss = 0.56087847\n",
      "Iteration 173, loss = 0.42212800\n",
      "Iteration 339, loss = 0.35965554\n",
      "Iteration 305, loss = 0.35893966\n",
      "Iteration 447, loss = 0.34495106\n",
      "Iteration 213, loss = 0.38287676\n",
      "Iteration 340, loss = 0.35948973\n",
      "Iteration 448, loss = 0.34486530\n",
      "Iteration 214, loss = 0.38258128\n",
      "Iteration 306, loss = 0.35878648\n",
      "Iteration 174, loss = 0.42170540\n",
      "Iteration 53, loss = 0.55808759\n",
      "Iteration 307, loss = 0.35863523\n",
      "Iteration 449, loss = 0.34477167\n",
      "Iteration 308, loss = 0.35848176\n",
      "Iteration 215, loss = 0.38233384\n",
      "Iteration 175, loss = 0.42114808\n",
      "Iteration 93, loss = 0.44542663\n",
      "Iteration 309, loss = 0.35832814\n",
      "Iteration 141, loss = 0.42521265\n",
      "Iteration 341, loss = 0.35930919\n",
      "Iteration 450, loss = 0.34468799\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 310, loss = 0.35819789\n",
      "Iteration 216, loss = 0.38204996\n",
      "Iteration 311, loss = 0.35802840\n",
      "Iteration 142, loss = 0.42457464\n",
      "Iteration 342, loss = 0.35917511\n",
      "Iteration 54, loss = 0.55541614\n",
      "Iteration 176, loss = 0.42067791\n",
      "Iteration 312, loss = 0.35789258\n",
      "Iteration 94, loss = 0.44433257\n",
      "Iteration 313, loss = 0.35774483\n",
      "Iteration 217, loss = 0.38177987\n",
      "Iteration 343, loss = 0.35898049\n",
      "Iteration 177, loss = 0.42015934\n",
      "Iteration 314, loss = 0.35758843\n",
      "Iteration 344, loss = 0.35878263\n",
      "Iteration 143, loss = 0.42391848\n",
      "Iteration 55, loss = 0.55263921\n",
      "Iteration 218, loss = 0.38152200\n",
      "Iteration 315, loss = 0.35745413\n",
      "Iteration 178, loss = 0.41972660\n",
      "Iteration 345, loss = 0.35863029\n",
      "Iteration 316, loss = 0.35732008\n",
      "Iteration 95, loss = 0.44338572\n",
      "Iteration 219, loss = 0.38128067\n",
      "Iteration 56, loss = 0.54999721\n",
      "Iteration 346, loss = 0.35845513\n",
      "Iteration 144, loss = 0.42334170\n",
      "Iteration 317, loss = 0.35715942\n",
      "Iteration 220, loss = 0.38101033\n",
      "Iteration 179, loss = 0.41923757\n",
      "Iteration 221, loss = 0.38076609\n",
      "Iteration 347, loss = 0.35827068\n",
      "Iteration 318, loss = 0.35702213\n",
      "Iteration 348, loss = 0.35809805\n",
      "Iteration 222, loss = 0.38050171\n",
      "Iteration 1, loss = 0.78877322\n",
      "Iteration 145, loss = 0.42270132\n",
      "Iteration 180, loss = 0.41872343\n",
      "Iteration 57, loss = 0.54731760\n",
      "Iteration 349, loss = 0.35793527\n",
      "Iteration 223, loss = 0.38024358\n",
      "Iteration 319, loss = 0.35688172\n",
      "Iteration 96, loss = 0.44230639\n",
      "Iteration 350, loss = 0.35776809\n",
      "Iteration 224, loss = 0.38000470\n",
      "Iteration 146, loss = 0.42208016\n",
      "Iteration 320, loss = 0.35673490\n",
      "Iteration 181, loss = 0.41832060\n",
      "Iteration 351, loss = 0.35761021\n",
      "Iteration 225, loss = 0.37974482\n",
      "Iteration 321, loss = 0.35657932\n",
      "Iteration 147, loss = 0.42147456\n",
      "Iteration 352, loss = 0.35742885\n",
      "Iteration 322, loss = 0.35646432\n",
      "Iteration 226, loss = 0.37948957\n",
      "Iteration 182, loss = 0.41782215\n",
      "Iteration 353, loss = 0.35724676\n",
      "Iteration 148, loss = 0.42086007\n",
      "Iteration 323, loss = 0.35631079\n",
      "Iteration 97, loss = 0.44133327\n",
      "Iteration 354, loss = 0.35708894\n",
      "Iteration 58, loss = 0.54470155\n",
      "Iteration 355, loss = 0.35691665\n",
      "Iteration 227, loss = 0.37927506\n",
      "Iteration 149, loss = 0.42031923\n",
      "Iteration 2, loss = 0.78705353\n",
      "Iteration 324, loss = 0.35616569\n",
      "Iteration 356, loss = 0.35674524\n",
      "Iteration 183, loss = 0.41739341\n",
      "Iteration 357, loss = 0.35658095\n",
      "Iteration 150, loss = 0.41972853\n",
      "Iteration 228, loss = 0.37902659\n",
      "Iteration 98, loss = 0.44031832\n",
      "Iteration 358, loss = 0.35643351\n",
      "Iteration 325, loss = 0.35603081\n",
      "Iteration 359, loss = 0.35625695\n",
      "Iteration 184, loss = 0.41689297\n",
      "Iteration 151, loss = 0.41913114\n",
      "Iteration 229, loss = 0.37876643Iteration 360, loss = 0.35610189\n",
      "\n",
      "Iteration 326, loss = 0.35589349\n",
      "Iteration 185, loss = 0.41641548\n",
      "Iteration 361, loss = 0.35592771\n",
      "Iteration 59, loss = 0.54215936\n",
      "Iteration 99, loss = 0.43935540\n",
      "Iteration 327, loss = 0.35575498\n",
      "Iteration 186, loss = 0.41600873\n",
      "Iteration 328, loss = 0.35562061\n",
      "Iteration 187, loss = 0.41555647\n",
      "Iteration 329, loss = 0.35548307\n",
      "Iteration 230, loss = 0.37852336\n",
      "Iteration 60, loss = 0.53953705\n",
      "Iteration 330, loss = 0.35535065\n",
      "Iteration 152, loss = 0.41853550\n",
      "Iteration 188, loss = 0.41510109\n",
      "Iteration 3, loss = 0.78444835\n",
      "Iteration 331, loss = 0.35520800\n",
      "Iteration 362, loss = 0.35579922\n",
      "Iteration 189, loss = 0.41467720\n",
      "Iteration 231, loss = 0.37827114\n",
      "Iteration 100, loss = 0.43839496\n",
      "Iteration 332, loss = 0.35507690\n",
      "Iteration 363, loss = 0.35561776\n",
      "Iteration 333, loss = 0.35495005\n",
      "Iteration 364, loss = 0.35544251\n",
      "Iteration 61, loss = 0.53696091\n",
      "Iteration 190, loss = 0.41422666\n",
      "Iteration 365, loss = 0.35529317\n",
      "Iteration 334, loss = 0.35480111\n",
      "Iteration 153, loss = 0.41797842\n",
      "Iteration 232, loss = 0.37804485\n",
      "Iteration 191, loss = 0.41379786\n",
      "Iteration 335, loss = 0.35467431\n",
      "Iteration 366, loss = 0.35513115\n",
      "Iteration 336, loss = 0.35454510\n",
      "Iteration 192, loss = 0.41338182\n",
      "Iteration 101, loss = 0.43747463\n",
      "Iteration 154, loss = 0.41741506\n",
      "Iteration 337, loss = 0.35440148\n",
      "Iteration 233, loss = 0.37780694\n",
      "Iteration 193, loss = 0.41298463\n",
      "Iteration 367, loss = 0.35497399\n",
      "Iteration 338, loss = 0.35430086\n",
      "Iteration 234, loss = 0.37758202\n",
      "Iteration 155, loss = 0.41686471\n",
      "Iteration 339, loss = 0.35414961\n",
      "Iteration 102, loss = 0.43659625\n",
      "Iteration 235, loss = 0.37736692\n",
      "Iteration 340, loss = 0.35401740\n",
      "Iteration 368, loss = 0.35481856\n",
      "Iteration 62, loss = 0.53449966\n",
      "Iteration 194, loss = 0.41251285\n",
      "Iteration 156, loss = 0.41631636\n",
      "Iteration 341, loss = 0.35388249\n",
      "Iteration 4, loss = 0.78138663\n",
      "Iteration 369, loss = 0.35467396\n",
      "Iteration 236, loss = 0.37713510\n",
      "Iteration 157, loss = 0.41577015\n",
      "Iteration 370, loss = 0.35450995\n",
      "Iteration 103, loss = 0.43568618\n",
      "Iteration 342, loss = 0.35376835\n",
      "Iteration 371, loss = 0.35435043\n",
      "Iteration 63, loss = 0.53205185\n",
      "Iteration 195, loss = 0.41210322\n",
      "Iteration 237, loss = 0.37686706\n",
      "Iteration 196, loss = 0.41170201\n",
      "Iteration 104, loss = 0.43479927\n",
      "Iteration 343, loss = 0.35363135\n",
      "Iteration 158, loss = 0.41521493\n",
      "Iteration 372, loss = 0.35422202\n",
      "Iteration 238, loss = 0.37665808\n",
      "Iteration 64, loss = 0.52960610\n",
      "Iteration 197, loss = 0.41128319\n",
      "Iteration 239, loss = 0.37643271\n",
      "Iteration 159, loss = 0.41473914\n",
      "Iteration 105, loss = 0.43394775\n",
      "Iteration 344, loss = 0.35349749\n",
      "Iteration 373, loss = 0.35403685\n",
      "Iteration 65, loss = 0.52723016\n",
      "Iteration 160, loss = 0.41420385\n",
      "Iteration 374, loss = 0.35389418\n",
      "Iteration 345, loss = 0.35338749\n",
      "Iteration 198, loss = 0.41089655\n",
      "Iteration 106, loss = 0.43308770\n",
      "Iteration 5, loss = 0.77782904\n",
      "Iteration 240, loss = 0.37619013\n",
      "Iteration 161, loss = 0.41369949\n",
      "Iteration 375, loss = 0.35373978\n",
      "Iteration 346, loss = 0.35323869\n",
      "Iteration 66, loss = 0.52483232\n",
      "Iteration 376, loss = 0.35356121\n",
      "Iteration 107, loss = 0.43231177\n",
      "Iteration 199, loss = 0.41046218\n",
      "Iteration 241, loss = 0.37598400\n",
      "Iteration 347, loss = 0.35312085\n",
      "Iteration 377, loss = 0.35343227\n",
      "Iteration 162, loss = 0.41313218\n",
      "Iteration 348, loss = 0.35300090\n",
      "Iteration 200, loss = 0.41005689\n",
      "Iteration 349, loss = 0.35287472\n",
      "Iteration 108, loss = 0.43145593\n",
      "Iteration 67, loss = 0.52250957\n",
      "Iteration 378, loss = 0.35326848\n",
      "Iteration 242, loss = 0.37576079\n",
      "Iteration 201, loss = 0.40968535\n",
      "Iteration 350, loss = 0.35272847\n",
      "Iteration 163, loss = 0.41262528\n",
      "Iteration 243, loss = 0.37553223\n",
      "Iteration 379, loss = 0.35310930\n",
      "Iteration 68, loss = 0.52010471\n",
      "Iteration 244, loss = 0.37530484\n",
      "Iteration 164, loss = 0.41214243\n",
      "Iteration 202, loss = 0.40928287\n",
      "Iteration 6, loss = 0.77414885\n",
      "Iteration 109, loss = 0.43066055\n",
      "Iteration 245, loss = 0.37510319\n",
      "Iteration 380, loss = 0.35295989\n",
      "Iteration 351, loss = 0.35261957\n",
      "Iteration 165, loss = 0.41164705\n",
      "Iteration 381, loss = 0.35280636\n",
      "Iteration 69, loss = 0.51790335\n",
      "Iteration 203, loss = 0.40885727\n",
      "Iteration 352, loss = 0.35248904\n",
      "Iteration 246, loss = 0.37489034\n",
      "Iteration 110, loss = 0.42991329\n",
      "Iteration 204, loss = 0.40844711\n",
      "Iteration 247, loss = 0.37465737\n",
      "Iteration 382, loss = 0.35265110\n",
      "Iteration 353, loss = 0.35237306\n",
      "Iteration 166, loss = 0.41116232\n",
      "Iteration 248, loss = 0.37445406\n",
      "Iteration 383, loss = 0.35250722\n",
      "Iteration 205, loss = 0.40809433\n",
      "Iteration 384, loss = 0.35235668\n",
      "Iteration 70, loss = 0.51567489\n",
      "Iteration 249, loss = 0.37424196\n",
      "Iteration 354, loss = 0.35224217\n",
      "Iteration 111, loss = 0.42914771\n",
      "Iteration 385, loss = 0.35221745\n",
      "Iteration 206, loss = 0.40767626\n",
      "Iteration 386, loss = 0.35204662\n",
      "Iteration 250, loss = 0.37403654\n",
      "Iteration 207, loss = 0.40727515\n",
      "Iteration 355, loss = 0.35212025\n",
      "Iteration 7, loss = 0.77031680\n",
      "Iteration 167, loss = 0.41067349\n",
      "Iteration 387, loss = 0.35192050\n",
      "Iteration 251, loss = 0.37382916\n",
      "Iteration 356, loss = 0.35200512\n",
      "Iteration 208, loss = 0.40689454\n",
      "Iteration 388, loss = 0.35176268\n",
      "Iteration 71, loss = 0.51345442\n",
      "Iteration 209, loss = 0.40649448\n",
      "Iteration 252, loss = 0.37361884\n",
      "Iteration 357, loss = 0.35186920\n",
      "Iteration 112, loss = 0.42837384\n",
      "Iteration 389, loss = 0.35163259\n",
      "Iteration 210, loss = 0.40618262\n",
      "Iteration 390, loss = 0.35145950\n",
      "Iteration 391, loss = 0.35131201\n",
      "Iteration 358, loss = 0.35174255\n",
      "Iteration 211, loss = 0.40575142\n",
      "Iteration 72, loss = 0.51126863\n",
      "Iteration 113, loss = 0.42761599\n",
      "Iteration 392, loss = 0.35117246\n",
      "Iteration 168, loss = 0.41019228\n",
      "Iteration 253, loss = 0.37340073\n",
      "Iteration 393, loss = 0.35102326\n",
      "Iteration 254, loss = 0.37319768\n",
      "Iteration 169, loss = 0.40970641\n",
      "Iteration 73, loss = 0.50912355\n",
      "Iteration 114, loss = 0.42685438\n",
      "Iteration 359, loss = 0.35163821\n",
      "Iteration 212, loss = 0.40534168\n",
      "Iteration 394, loss = 0.35087944\n",
      "Iteration 360, loss = 0.35150426\n",
      "Iteration 255, loss = 0.37300015\n",
      "Iteration 395, loss = 0.35073938\n",
      "Iteration 213, loss = 0.40497137\n",
      "Iteration 361, loss = 0.35140226\n",
      "Iteration 214, loss = 0.40460830\n",
      "Iteration 396, loss = 0.35060869\n",
      "Iteration 362, loss = 0.35126050\n",
      "Iteration 115, loss = 0.42618427\n",
      "Iteration 397, loss = 0.35046591\n",
      "Iteration 215, loss = 0.40425197\n",
      "Iteration 256, loss = 0.37280315\n",
      "Iteration 170, loss = 0.40925272\n",
      "Iteration 398, loss = 0.35029989\n",
      "Iteration 74, loss = 0.50704201\n",
      "Iteration 216, loss = 0.40387395\n",
      "Iteration 8, loss = 0.76666296\n",
      "Iteration 399, loss = 0.35016720\n",
      "Iteration 217, loss = 0.40349403\n",
      "Iteration 257, loss = 0.37258880\n",
      "Iteration 363, loss = 0.35114051\n",
      "Iteration 400, loss = 0.35002630\n",
      "Iteration 218, loss = 0.40311946\n",
      "Iteration 364, loss = 0.35102110\n",
      "Iteration 171, loss = 0.40879976\n",
      "Iteration 401, loss = 0.34989437\n",
      "Iteration 258, loss = 0.37240685\n",
      "Iteration 116, loss = 0.42546277\n",
      "Iteration 219, loss = 0.40277222\n",
      "Iteration 402, loss = 0.34974427\n",
      "Iteration 365, loss = 0.35091578\n",
      "Iteration 172, loss = 0.40828582\n",
      "Iteration 403, loss = 0.34960218\n",
      "Iteration 75, loss = 0.50495773\n",
      "Iteration 220, loss = 0.40239277\n",
      "Iteration 404, loss = 0.34947041\n",
      "Iteration 259, loss = 0.37221279\n",
      "Iteration 221, loss = 0.40208136\n",
      "Iteration 173, loss = 0.40785960\n",
      "Iteration 405, loss = 0.34932061\n",
      "Iteration 366, loss = 0.35079271\n",
      "Iteration 260, loss = 0.37201478\n",
      "Iteration 174, loss = 0.40740397\n",
      "Iteration 261, loss = 0.37181112\n",
      "Iteration 117, loss = 0.42477017\n",
      "Iteration 222, loss = 0.40168937\n",
      "Iteration 367, loss = 0.35065867\n",
      "Iteration 406, loss = 0.34919841\n",
      "Iteration 223, loss = 0.40131838\n",
      "Iteration 262, loss = 0.37162033\n",
      "Iteration 368, loss = 0.35055269\n",
      "Iteration 118, loss = 0.42411245\n",
      "Iteration 175, loss = 0.40692338\n",
      "Iteration 369, loss = 0.35043359\n",
      "Iteration 407, loss = 0.34904397\n",
      "Iteration 370, loss = 0.35031838\n",
      "Iteration 176, loss = 0.40650525\n",
      "Iteration 119, loss = 0.42342842\n",
      "Iteration 371, loss = 0.35019672\n",
      "Iteration 408, loss = 0.34889030\n",
      "Iteration 263, loss = 0.37141933\n",
      "Iteration 224, loss = 0.40099528\n",
      "Iteration 372, loss = 0.35010821\n",
      "Iteration 9, loss = 0.76289761\n",
      "Iteration 76, loss = 0.50294099\n",
      "Iteration 373, loss = 0.34997500\n",
      "Iteration 225, loss = 0.40060119\n",
      "Iteration 374, loss = 0.34986192\n",
      "Iteration 226, loss = 0.40025092\n",
      "Iteration 409, loss = 0.34875729\n",
      "Iteration 264, loss = 0.37125430\n",
      "Iteration 177, loss = 0.40605911\n",
      "Iteration 410, loss = 0.34861515\n",
      "Iteration 227, loss = 0.39991817\n",
      "Iteration 265, loss = 0.37103579\n",
      "Iteration 411, loss = 0.34849057\n",
      "Iteration 228, loss = 0.39954730\n",
      "Iteration 375, loss = 0.34973520\n",
      "Iteration 266, loss = 0.37084892\n",
      "Iteration 120, loss = 0.42278727\n",
      "Iteration 376, loss = 0.34962060\n",
      "Iteration 412, loss = 0.34834608\n",
      "Iteration 267, loss = 0.37066091\n",
      "Iteration 377, loss = 0.34950474\n",
      "Iteration 77, loss = 0.50095490\n",
      "Iteration 178, loss = 0.40561961\n",
      "Iteration 268, loss = 0.37048296\n",
      "Iteration 378, loss = 0.34940739\n",
      "Iteration 413, loss = 0.34819457\n",
      "Iteration 269, loss = 0.37026526\n",
      "Iteration 379, loss = 0.34929396\n",
      "Iteration 10, loss = 0.75902449\n",
      "Iteration 380, loss = 0.34917150\n",
      "Iteration 270, loss = 0.37009248\n",
      "Iteration 414, loss = 0.34805404\n",
      "Iteration 381, loss = 0.34906059\n",
      "Iteration 229, loss = 0.39921810\n",
      "Iteration 415, loss = 0.34792111\n",
      "Iteration 179, loss = 0.40516814\n",
      "Iteration 121, loss = 0.42209958\n",
      "Iteration 416, loss = 0.34777772\n",
      "Iteration 78, loss = 0.49903158\n",
      "Iteration 382, loss = 0.34894720\n",
      "Iteration 417, loss = 0.34764630\n",
      "Iteration 418, loss = 0.34749185\n",
      "Iteration 419, loss = 0.34736312\n",
      "Iteration 11, loss = 0.75531323\n",
      "Iteration 383, loss = 0.34884348\n",
      "Iteration 271, loss = 0.36991684\n",
      "Iteration 230, loss = 0.39888708\n",
      "Iteration 180, loss = 0.40480109\n",
      "Iteration 420, loss = 0.34722207\n",
      "Iteration 384, loss = 0.34873217\n",
      "Iteration 122, loss = 0.42154142\n",
      "Iteration 385, loss = 0.34861844\n",
      "Iteration 181, loss = 0.40434948\n",
      "Iteration 386, loss = 0.34850259\n",
      "Iteration 421, loss = 0.34709117\n",
      "Iteration 272, loss = 0.36971059\n",
      "Iteration 387, loss = 0.34841164\n",
      "Iteration 123, loss = 0.42088303\n",
      "Iteration 231, loss = 0.39856475\n",
      "Iteration 182, loss = 0.40396381\n",
      "Iteration 388, loss = 0.34828664\n",
      "Iteration 79, loss = 0.49705382\n",
      "Iteration 422, loss = 0.34695205\n",
      "Iteration 389, loss = 0.34817968\n",
      "Iteration 232, loss = 0.39821165\n",
      "Iteration 390, loss = 0.34806635\n",
      "Iteration 183, loss = 0.40348429\n",
      "Iteration 423, loss = 0.34681877\n",
      "Iteration 391, loss = 0.34795863\n",
      "Iteration 424, loss = 0.34671415\n",
      "Iteration 273, loss = 0.36953420\n",
      "Iteration 12, loss = 0.75165376\n",
      "Iteration 233, loss = 0.39788309\n",
      "Iteration 392, loss = 0.34785834\n",
      "Iteration 184, loss = 0.40307784\n",
      "Iteration 425, loss = 0.34653073\n",
      "Iteration 234, loss = 0.39753523\n",
      "Iteration 426, loss = 0.34640087\n",
      "Iteration 427, loss = 0.34625558\n",
      "Iteration 393, loss = 0.34775906\n",
      "Iteration 274, loss = 0.36936405\n",
      "Iteration 80, loss = 0.49519507\n",
      "Iteration 185, loss = 0.40265148\n",
      "Iteration 235, loss = 0.39719484\n",
      "Iteration 394, loss = 0.34764708\n",
      "Iteration 428, loss = 0.34612912\n",
      "Iteration 395, loss = 0.34753988\n",
      "Iteration 124, loss = 0.42024627\n",
      "Iteration 429, loss = 0.34599088\n",
      "Iteration 186, loss = 0.40226428\n",
      "Iteration 275, loss = 0.36917153\n",
      "Iteration 236, loss = 0.39687729\n",
      "Iteration 81, loss = 0.49329599\n",
      "Iteration 430, loss = 0.34585375\n",
      "Iteration 276, loss = 0.36899035\n",
      "Iteration 431, loss = 0.34573502\n",
      "Iteration 277, loss = 0.36879784\n",
      "Iteration 278, loss = 0.36865161\n",
      "Iteration 396, loss = 0.34743500\n",
      "Iteration 432, loss = 0.34559551\n",
      "Iteration 397, loss = 0.34731673\n",
      "Iteration 187, loss = 0.40184880\n",
      "Iteration 13, loss = 0.74821609\n",
      "Iteration 398, loss = 0.34721197\n",
      "Iteration 433, loss = 0.34547096\n",
      "Iteration 399, loss = 0.34710234\n",
      "Iteration 434, loss = 0.34532492\n",
      "Iteration 82, loss = 0.49149028\n",
      "Iteration 237, loss = 0.39652896\n",
      "Iteration 400, loss = 0.34699593\n",
      "Iteration 435, loss = 0.34522472\n",
      "Iteration 401, loss = 0.34689817\n",
      "Iteration 436, loss = 0.34507578\n",
      "Iteration 279, loss = 0.36846205\n",
      "Iteration 238, loss = 0.39618095\n",
      "Iteration 437, loss = 0.34494518\n",
      "Iteration 402, loss = 0.34679324\n",
      "Iteration 239, loss = 0.39588068\n",
      "Iteration 438, loss = 0.34480085\n",
      "Iteration 439, loss = 0.34467324\n",
      "Iteration 188, loss = 0.40149991Iteration 403, loss = 0.34667608\n",
      "\n",
      "Iteration 440, loss = 0.34454919\n",
      "Iteration 280, loss = 0.36828491\n",
      "Iteration 240, loss = 0.39553788\n",
      "Iteration 441, loss = 0.34441319\n",
      "Iteration 404, loss = 0.34658444\n",
      "Iteration 125, loss = 0.41963454\n",
      "Iteration 241, loss = 0.39521832\n",
      "Iteration 405, loss = 0.34647912\n",
      "Iteration 442, loss = 0.34430219\n",
      "Iteration 14, loss = 0.74460049\n",
      "Iteration 83, loss = 0.48969162\n",
      "Iteration 443, loss = 0.34416473\n",
      "Iteration 406, loss = 0.34636446\n",
      "Iteration 242, loss = 0.39491013\n",
      "Iteration 281, loss = 0.36811416\n",
      "Iteration 243, loss = 0.39458987\n",
      "Iteration 282, loss = 0.36792216\n",
      "Iteration 407, loss = 0.34626462\n",
      "Iteration 244, loss = 0.39429190\n",
      "Iteration 189, loss = 0.40105629\n",
      "Iteration 408, loss = 0.34616074\n",
      "Iteration 245, loss = 0.39397574\n",
      "Iteration 126, loss = 0.41904541\n",
      "Iteration 283, loss = 0.36773874\n",
      "Iteration 409, loss = 0.34605533\n",
      "Iteration 444, loss = 0.34402584\n",
      "Iteration 410, loss = 0.34595290\n",
      "Iteration 190, loss = 0.40066878\n",
      "Iteration 84, loss = 0.48787506\n",
      "Iteration 445, loss = 0.34390481\n",
      "Iteration 246, loss = 0.39365467\n",
      "Iteration 411, loss = 0.34585831\n",
      "Iteration 412, loss = 0.34574645\n",
      "Iteration 446, loss = 0.34377651\n",
      "Iteration 15, loss = 0.74127124\n",
      "Iteration 127, loss = 0.41846586\n",
      "Iteration 247, loss = 0.39332929\n",
      "Iteration 413, loss = 0.34564482\n",
      "Iteration 447, loss = 0.34366849\n",
      "Iteration 191, loss = 0.40028242\n",
      "Iteration 448, loss = 0.34352005\n",
      "Iteration 414, loss = 0.34555059\n",
      "Iteration 85, loss = 0.48612855\n",
      "Iteration 449, loss = 0.34341737\n",
      "Iteration 415, loss = 0.34544677\n",
      "Iteration 248, loss = 0.39303646\n",
      "Iteration 284, loss = 0.36759230\n",
      "Iteration 450, loss = 0.34327617\n",
      "Iteration 416, loss = 0.34534730\n",
      "Iteration 192, loss = 0.39991046\n",
      "Iteration 451, loss = 0.34313323\n",
      "Iteration 249, loss = 0.39272342\n",
      "Iteration 128, loss = 0.41788975Iteration 452, loss = 0.34301822\n",
      "\n",
      "Iteration 193, loss = 0.39949413\n",
      "Iteration 417, loss = 0.34523710\n",
      "Iteration 86, loss = 0.48455225\n",
      "Iteration 285, loss = 0.36742194\n",
      "Iteration 286, loss = 0.36721447\n",
      "Iteration 453, loss = 0.34288161\n",
      "Iteration 418, loss = 0.34515085\n",
      "Iteration 250, loss = 0.39243117\n",
      "Iteration 287, loss = 0.36705108\n",
      "Iteration 194, loss = 0.39912417\n",
      "Iteration 288, loss = 0.36686214\n",
      "Iteration 454, loss = 0.34277897\n",
      "Iteration 251, loss = 0.39211585\n",
      "Iteration 419, loss = 0.34504200\n",
      "Iteration 129, loss = 0.41730643\n",
      "Iteration 87, loss = 0.48284971\n",
      "Iteration 420, loss = 0.34494499\n",
      "Iteration 455, loss = 0.34265289\n",
      "Iteration 289, loss = 0.36671252\n",
      "Iteration 130, loss = 0.41672324\n",
      "Iteration 252, loss = 0.39181891\n",
      "Iteration 16, loss = 0.73795811\n",
      "Iteration 195, loss = 0.39872731\n",
      "Iteration 421, loss = 0.34484037\n",
      "Iteration 456, loss = 0.34250702\n",
      "Iteration 422, loss = 0.34475327\n",
      "Iteration 131, loss = 0.41619079\n",
      "Iteration 290, loss = 0.36655958\n",
      "Iteration 457, loss = 0.34239813\n",
      "Iteration 253, loss = 0.39150826\n",
      "Iteration 423, loss = 0.34464074\n",
      "Iteration 458, loss = 0.34225823\n",
      "Iteration 88, loss = 0.48122103\n",
      "Iteration 196, loss = 0.39837363\n",
      "Iteration 424, loss = 0.34453960\n",
      "Iteration 459, loss = 0.34216424\n",
      "Iteration 132, loss = 0.41566584\n",
      "Iteration 425, loss = 0.34446060\n",
      "Iteration 291, loss = 0.36637856\n",
      "Iteration 460, loss = 0.34203241\n",
      "Iteration 426, loss = 0.34436335\n",
      "Iteration 254, loss = 0.39123718\n",
      "Iteration 427, loss = 0.34424765\n",
      "Iteration 133, loss = 0.41510102\n",
      "Iteration 461, loss = 0.34189181\n",
      "Iteration 428, loss = 0.34415836\n",
      "Iteration 255, loss = 0.39093252\n",
      "Iteration 197, loss = 0.39801335\n",
      "Iteration 292, loss = 0.36619353\n",
      "Iteration 429, loss = 0.34405220\n",
      "Iteration 256, loss = 0.39062156\n",
      "Iteration 430, loss = 0.34395864\n",
      "Iteration 462, loss = 0.34178053\n",
      "Iteration 89, loss = 0.47957267\n",
      "Iteration 198, loss = 0.39761410\n",
      "Iteration 431, loss = 0.34386091\n",
      "Iteration 257, loss = 0.39036059\n",
      "Iteration 463, loss = 0.34164955\n",
      "Iteration 134, loss = 0.41456875\n",
      "Iteration 432, loss = 0.34376926\n",
      "Iteration 293, loss = 0.36604392\n",
      "Iteration 433, loss = 0.34367534\n",
      "Iteration 258, loss = 0.39004166\n",
      "Iteration 464, loss = 0.34153822\n",
      "Iteration 199, loss = 0.39727632\n",
      "Iteration 434, loss = 0.34358840\n",
      "Iteration 259, loss = 0.38977005\n",
      "Iteration 435, loss = 0.34347445\n",
      "Iteration 294, loss = 0.36586996\n",
      "Iteration 17, loss = 0.73479781\n",
      "Iteration 465, loss = 0.34140873\n",
      "Iteration 436, loss = 0.34338863\n",
      "Iteration 200, loss = 0.39689425\n",
      "Iteration 295, loss = 0.36569266\n",
      "Iteration 437, loss = 0.34329717\n",
      "Iteration 260, loss = 0.38947403\n",
      "Iteration 438, loss = 0.34319995\n",
      "Iteration 466, loss = 0.34128101\n",
      "Iteration 135, loss = 0.41406559\n",
      "Iteration 90, loss = 0.47805737\n",
      "Iteration 296, loss = 0.36554087\n",
      "Iteration 297, loss = 0.36537276\n",
      "Iteration 439, loss = 0.34310355\n",
      "Iteration 467, loss = 0.34115852\n",
      "Iteration 136, loss = 0.41355142\n",
      "Iteration 261, loss = 0.38914911\n",
      "Iteration 298, loss = 0.36520208\n",
      "Iteration 201, loss = 0.39653269\n",
      "Iteration 440, loss = 0.34300818\n",
      "Iteration 468, loss = 0.34103836\n",
      "Iteration 299, loss = 0.36506207\n",
      "Iteration 469, loss = 0.34091934\n",
      "Iteration 300, loss = 0.36490551\n",
      "Iteration 262, loss = 0.38888303\n",
      "Iteration 470, loss = 0.34079812\n",
      "Iteration 91, loss = 0.47656361\n",
      "Iteration 301, loss = 0.36473561Iteration 202, loss = 0.39616598\n",
      "\n",
      "Iteration 471, loss = 0.34067196\n",
      "Iteration 18, loss = 0.73145468\n",
      "Iteration 441, loss = 0.34293438\n",
      "Iteration 472, loss = 0.34055522\n",
      "Iteration 263, loss = 0.38860462\n",
      "Iteration 302, loss = 0.36456628\n",
      "Iteration 137, loss = 0.41302425\n",
      "Iteration 473, loss = 0.34044786\n",
      "Iteration 442, loss = 0.34282435\n",
      "Iteration 303, loss = 0.36440904\n",
      "Iteration 474, loss = 0.34032651\n",
      "Iteration 264, loss = 0.38830753\n",
      "Iteration 475, loss = 0.34020052\n",
      "Iteration 304, loss = 0.36424728\n",
      "Iteration 443, loss = 0.34274358\n",
      "Iteration 265, loss = 0.38800145\n",
      "Iteration 476, loss = 0.34008812\n",
      "Iteration 203, loss = 0.39582396\n",
      "Iteration 92, loss = 0.47500436\n",
      "Iteration 305, loss = 0.36409215\n",
      "Iteration 477, loss = 0.33996206\n",
      "Iteration 266, loss = 0.38773168\n",
      "Iteration 444, loss = 0.34264198\n",
      "Iteration 478, loss = 0.33985998\n",
      "Iteration 306, loss = 0.36393306\n",
      "Iteration 445, loss = 0.34254571\n",
      "Iteration 138, loss = 0.41255123\n",
      "Iteration 479, loss = 0.33973118\n",
      "Iteration 446, loss = 0.34246592\n",
      "Iteration 267, loss = 0.38744927\n",
      "Iteration 307, loss = 0.36378305\n",
      "Iteration 204, loss = 0.39547430\n",
      "Iteration 480, loss = 0.33961947\n",
      "Iteration 447, loss = 0.34237537\n",
      "Iteration 139, loss = 0.41205672\n",
      "Iteration 448, loss = 0.34227537\n",
      "Iteration 19, loss = 0.72839415Iteration 205, loss = 0.39509600\n",
      "\n",
      "Iteration 268, loss = 0.38721247\n",
      "Iteration 449, loss = 0.34218455\n",
      "Iteration 481, loss = 0.33950146\n",
      "Iteration 308, loss = 0.36360340\n",
      "Iteration 93, loss = 0.47352419\n",
      "Iteration 140, loss = 0.41153135\n",
      "Iteration 450, loss = 0.34209685\n",
      "Iteration 451, loss = 0.34199361\n",
      "Iteration 482, loss = 0.33939172\n",
      "Iteration 452, loss = 0.34190844\n",
      "Iteration 206, loss = 0.39475146\n",
      "Iteration 269, loss = 0.38691373\n",
      "Iteration 309, loss = 0.36348795\n",
      "Iteration 141, loss = 0.41106910\n",
      "Iteration 453, loss = 0.34182246\n",
      "Iteration 483, loss = 0.33926852\n",
      "Iteration 270, loss = 0.38661279\n",
      "Iteration 94, loss = 0.47207438\n",
      "Iteration 454, loss = 0.34172987\n",
      "Iteration 484, loss = 0.33915783\n",
      "Iteration 142, loss = 0.41060996\n",
      "Iteration 310, loss = 0.36330012\n",
      "Iteration 271, loss = 0.38634628\n",
      "Iteration 455, loss = 0.34163821\n",
      "Iteration 485, loss = 0.33904264\n",
      "Iteration 207, loss = 0.39441614\n",
      "Iteration 311, loss = 0.36316769\n",
      "Iteration 20, loss = 0.72525024\n",
      "Iteration 486, loss = 0.33892901\n",
      "Iteration 143, loss = 0.41013826\n",
      "Iteration 456, loss = 0.34154761\n",
      "Iteration 312, loss = 0.36302525\n",
      "Iteration 487, loss = 0.33881983\n",
      "Iteration 272, loss = 0.38606510\n",
      "Iteration 488, loss = 0.33869737\n",
      "Iteration 313, loss = 0.36285113\n",
      "Iteration 457, loss = 0.34146080\n",
      "Iteration 95, loss = 0.47067407\n",
      "Iteration 144, loss = 0.40967256\n",
      "Iteration 314, loss = 0.36269459\n",
      "Iteration 458, loss = 0.34138407\n",
      "Iteration 208, loss = 0.39406341\n",
      "Iteration 489, loss = 0.33858137\n",
      "Iteration 315, loss = 0.36253207\n",
      "Iteration 273, loss = 0.38580483\n",
      "Iteration 490, loss = 0.33846646\n",
      "Iteration 459, loss = 0.34128269\n",
      "Iteration 145, loss = 0.40919123\n",
      "Iteration 96, loss = 0.46926949\n",
      "Iteration 491, loss = 0.33835198\n",
      "Iteration 492, loss = 0.33823479\n",
      "Iteration 316, loss = 0.36238838\n",
      "Iteration 209, loss = 0.39375091\n",
      "Iteration 493, loss = 0.33814135\n",
      "Iteration 460, loss = 0.34119308\n",
      "Iteration 274, loss = 0.38558999\n",
      "Iteration 21, loss = 0.72213580\n",
      "Iteration 494, loss = 0.33803333\n",
      "Iteration 97, loss = 0.46796833\n",
      "Iteration 495, loss = 0.33788125\n",
      "Iteration 461, loss = 0.34111128\n",
      "Iteration 146, loss = 0.40872846\n",
      "Iteration 317, loss = 0.36226873\n",
      "Iteration 496, loss = 0.33777765\n",
      "Iteration 210, loss = 0.39337748\n",
      "Iteration 275, loss = 0.38527861\n",
      "Iteration 497, loss = 0.33766666\n",
      "Iteration 462, loss = 0.34101918\n",
      "Iteration 498, loss = 0.33756889\n",
      "Iteration 98, loss = 0.46655134\n",
      "Iteration 499, loss = 0.33743152\n",
      "Iteration 211, loss = 0.39307969\n",
      "Iteration 318, loss = 0.36208818\n",
      "Iteration 500, loss = 0.33731821\n",
      "Iteration 463, loss = 0.34092839\n",
      "Iteration 276, loss = 0.38498187\n",
      "Iteration 501, loss = 0.33721466\n",
      "Iteration 212, loss = 0.39272604\n",
      "Iteration 147, loss = 0.40829375\n",
      "Iteration 99, loss = 0.46533477\n",
      "Iteration 502, loss = 0.33709620\n",
      "Iteration 464, loss = 0.34083266\n",
      "Iteration 319, loss = 0.36193361\n",
      "Iteration 277, loss = 0.38473327\n",
      "Iteration 503, loss = 0.33699215\n",
      "Iteration 22, loss = 0.71916051\n",
      "Iteration 213, loss = 0.39240424\n",
      "Iteration 465, loss = 0.34074909\n",
      "Iteration 504, loss = 0.33686398\n",
      "Iteration 100, loss = 0.46400170\n",
      "Iteration 320, loss = 0.36178836\n",
      "Iteration 278, loss = 0.38446079\n",
      "Iteration 214, loss = 0.39209004\n",
      "Iteration 466, loss = 0.34066964\n",
      "Iteration 505, loss = 0.33677096\n",
      "Iteration 279, loss = 0.38419895\n",
      "Iteration 215, loss = 0.39173314\n",
      "Iteration 148, loss = 0.40783896\n",
      "Iteration 101, loss = 0.46287094\n",
      "Iteration 280, loss = 0.38392404\n",
      "Iteration 321, loss = 0.36164794\n",
      "Iteration 467, loss = 0.34057351\n",
      "Iteration 506, loss = 0.33663613\n",
      "Iteration 216, loss = 0.39143201\n",
      "Iteration 281, loss = 0.38368884\n",
      "Iteration 23, loss = 0.71608838\n",
      "Iteration 468, loss = 0.34049553\n",
      "Iteration 507, loss = 0.33653017\n",
      "Iteration 282, loss = 0.38340808\n",
      "Iteration 322, loss = 0.36148350\n",
      "Iteration 217, loss = 0.39107931\n",
      "Iteration 102, loss = 0.46156156\n",
      "Iteration 283, loss = 0.38315631\n",
      "Iteration 469, loss = 0.34041245\n",
      "Iteration 508, loss = 0.33643048\n",
      "Iteration 149, loss = 0.40741782\n",
      "Iteration 218, loss = 0.39079674\n",
      "Iteration 284, loss = 0.38291072\n",
      "Iteration 509, loss = 0.33631333\n",
      "Iteration 323, loss = 0.36133759\n",
      "Iteration 285, loss = 0.38265401\n",
      "Iteration 470, loss = 0.34031891\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 219, loss = 0.39048663\n",
      "Iteration 510, loss = 0.33619197\n",
      "Iteration 286, loss = 0.38239000\n",
      "Iteration 103, loss = 0.46038766\n",
      "Iteration 511, loss = 0.33608869\n",
      "Iteration 287, loss = 0.38212198\n",
      "Iteration 324, loss = 0.36119552\n",
      "Iteration 150, loss = 0.40701368\n",
      "Iteration 512, loss = 0.33598153\n",
      "Iteration 220, loss = 0.39016296\n",
      "Iteration 288, loss = 0.38188493\n",
      "Iteration 513, loss = 0.33585731\n",
      "Iteration 24, loss = 0.71307673\n",
      "Iteration 325, loss = 0.36107930\n",
      "Iteration 289, loss = 0.38165607\n",
      "Iteration 1, loss = 0.77024581\n",
      "Iteration 514, loss = 0.33576540\n",
      "Iteration 290, loss = 0.38139256\n",
      "Iteration 2, loss = 0.76902311\n",
      "Iteration 515, loss = 0.33564870\n",
      "Iteration 151, loss = 0.40655918\n",
      "Iteration 291, loss = 0.38112453\n",
      "Iteration 3, loss = 0.76714108\n",
      "Iteration 326, loss = 0.36089638\n",
      "Iteration 516, loss = 0.33553356\n",
      "Iteration 517, loss = 0.33543692\n",
      "Iteration 221, loss = 0.38982949\n",
      "Iteration 292, loss = 0.38086366\n",
      "Iteration 4, loss = 0.76486270\n",
      "Iteration 518, loss = 0.33531432\n",
      "Iteration 293, loss = 0.38062316\n",
      "Iteration 519, loss = 0.33521078\n",
      "Iteration 327, loss = 0.36076465\n",
      "Iteration 5, loss = 0.76243369\n",
      "Iteration 294, loss = 0.38038184\n",
      "Iteration 520, loss = 0.33511696\n",
      "Iteration 295, loss = 0.38011578\n",
      "Iteration 6, loss = 0.76001649\n",
      "Iteration 25, loss = 0.71014436\n",
      "Iteration 152, loss = 0.40615385\n",
      "Iteration 222, loss = 0.38955338\n",
      "Iteration 521, loss = 0.33499027\n",
      "Iteration 328, loss = 0.36064412\n",
      "Iteration 296, loss = 0.37988606\n",
      "Iteration 7, loss = 0.75729764\n",
      "Iteration 329, loss = 0.36047170\n",
      "Iteration 297, loss = 0.37961053\n",
      "Iteration 522, loss = 0.33488829\n",
      "Iteration 8, loss = 0.75477827\n",
      "Iteration 153, loss = 0.40570288\n",
      "Iteration 298, loss = 0.37937001\n",
      "Iteration 330, loss = 0.36033789\n",
      "Iteration 523, loss = 0.33477661\n",
      "Iteration 223, loss = 0.38921507\n",
      "Iteration 9, loss = 0.75215219\n",
      "Iteration 331, loss = 0.36020923\n",
      "Iteration 524, loss = 0.33467127\n",
      "Iteration 525, loss = 0.33457774\n",
      "Iteration 299, loss = 0.37915298\n",
      "Iteration 332, loss = 0.36004363\n",
      "Iteration 10, loss = 0.74977974\n",
      "Iteration 300, loss = 0.37888024\n",
      "Iteration 26, loss = 0.70718265\n",
      "Iteration 333, loss = 0.35993119\n",
      "Iteration 526, loss = 0.33446873\n",
      "Iteration 301, loss = 0.37864629\n",
      "Iteration 224, loss = 0.38893515\n",
      "Iteration 302, loss = 0.37842487\n",
      "Iteration 154, loss = 0.40531245\n",
      "Iteration 527, loss = 0.33434422\n",
      "Iteration 11, loss = 0.74738751\n",
      "Iteration 334, loss = 0.35978697\n",
      "Iteration 528, loss = 0.33426075\n",
      "Iteration 225, loss = 0.38860951\n",
      "Iteration 12, loss = 0.74513808\n",
      "Iteration 303, loss = 0.37816945\n",
      "Iteration 529, loss = 0.33414313\n",
      "Iteration 155, loss = 0.40488084\n",
      "Iteration 335, loss = 0.35962447\n",
      "Iteration 27, loss = 0.70423356\n",
      "Iteration 530, loss = 0.33404109\n",
      "Iteration 13, loss = 0.74304597\n",
      "Iteration 531, loss = 0.33392852\n",
      "Iteration 304, loss = 0.37793058\n",
      "Iteration 532, loss = 0.33384901\n",
      "Iteration 104, loss = 0.45925807Iteration 336, loss = 0.35948487\n",
      "Iteration 533, loss = 0.33372808\n",
      "Iteration 226, loss = 0.38829454\n",
      "Iteration 14, loss = 0.74091322\n",
      "Iteration 534, loss = 0.33362570\n",
      "Iteration 337, loss = 0.35936215\n",
      "Iteration 227, loss = 0.38799742\n",
      "Iteration 15, loss = 0.73902027\n",
      "Iteration 535, loss = 0.33353294\n",
      "Iteration 156, loss = 0.40445895\n",
      "Iteration 338, loss = 0.35921586\n",
      "Iteration 305, loss = 0.37768704\n",
      "Iteration 228, loss = 0.38771891\n",
      "Iteration 339, loss = 0.35907473\n",
      "Iteration 16, loss = 0.73703525\n",
      "Iteration 536, loss = 0.33342568\n",
      "Iteration 340, loss = 0.35892052\n",
      "Iteration 157, loss = 0.40407525\n",
      "Iteration 229, loss = 0.38740028\n",
      "\n",
      "Iteration 537, loss = 0.33330700\n",
      "Iteration 306, loss = 0.37745027\n",
      "Iteration 17, loss = 0.73542504\n",
      "Iteration 538, loss = 0.33321434\n",
      "Iteration 28, loss = 0.70124836\n",
      "Iteration 341, loss = 0.35881356\n",
      "Iteration 539, loss = 0.33311044\n",
      "Iteration 307, loss = 0.37723972\n",
      "Iteration 230, loss = 0.38712194\n",
      "Iteration 342, loss = 0.35866581\n",
      "Iteration 540, loss = 0.33300878\n",
      "Iteration 158, loss = 0.40368541\n",
      "Iteration 18, loss = 0.73360254\n",
      "Iteration 308, loss = 0.37699655\n",
      "Iteration 343, loss = 0.35854061\n",
      "Iteration 231, loss = 0.38682934\n",
      "Iteration 541, loss = 0.33289759\n",
      "Iteration 309, loss = 0.37674237\n",
      "Iteration 19, loss = 0.73192253\n",
      "Iteration 159, loss = 0.40327220\n",
      "Iteration 310, loss = 0.37649936\n",
      "Iteration 542, loss = 0.33280592\n",
      "Iteration 344, loss = 0.35840008\n",
      "Iteration 232, loss = 0.38652341\n",
      "Iteration 543, loss = 0.33270649\n",
      "Iteration 160, loss = 0.40291652\n",
      "Iteration 544, loss = 0.33260872\n",
      "Iteration 311, loss = 0.37629925\n",
      "Iteration 345, loss = 0.35826684\n",
      "Iteration 545, loss = 0.33250343\n",
      "Iteration 233, loss = 0.38624869\n",
      "Iteration 105, loss = 0.45809184\n",
      "Iteration 546, loss = 0.33242652\n",
      "Iteration 29, loss = 0.69831675\n",
      "Iteration 20, loss = 0.73032701\n",
      "Iteration 346, loss = 0.35811597\n",
      "Iteration 161, loss = 0.40249954\n",
      "Iteration 547, loss = 0.33229378\n",
      "Iteration 312, loss = 0.37606127\n",
      "Iteration 347, loss = 0.35798139\n",
      "Iteration 548, loss = 0.33219264\n",
      "Iteration 21, loss = 0.72883096\n",
      "Iteration 234, loss = 0.38596510\n",
      "Iteration 348, loss = 0.35784542\n",
      "Iteration 549, loss = 0.33209236\n",
      "Iteration 22, loss = 0.72731692\n",
      "Iteration 550, loss = 0.33199888\n",
      "Iteration 313, loss = 0.37585056\n",
      "Iteration 30, loss = 0.69539445\n",
      "Iteration 349, loss = 0.35771419\n",
      "Iteration 23, loss = 0.72586529\n",
      "Iteration 551, loss = 0.33189615\n",
      "Iteration 235, loss = 0.38569836\n",
      "Iteration 162, loss = 0.40212670\n",
      "Iteration 314, loss = 0.37557621\n",
      "Iteration 552, loss = 0.33179450\n",
      "Iteration 315, loss = 0.37535703\n",
      "Iteration 106, loss = 0.45700936\n",
      "Iteration 24, loss = 0.72450217\n",
      "Iteration 350, loss = 0.35758183\n",
      "Iteration 163, loss = 0.40174385\n",
      "Iteration 553, loss = 0.33170720\n",
      "Iteration 316, loss = 0.37514616\n",
      "Iteration 236, loss = 0.38541161\n",
      "Iteration 351, loss = 0.35746230\n",
      "Iteration 554, loss = 0.33158591\n",
      "Iteration 317, loss = 0.37490468\n",
      "Iteration 237, loss = 0.38514296\n",
      "Iteration 164, loss = 0.40136584\n",
      "Iteration 25, loss = 0.72311244\n",
      "Iteration 318, loss = 0.37466877\n",
      "Iteration 555, loss = 0.33149288\n",
      "Iteration 238, loss = 0.38485016\n",
      "Iteration 352, loss = 0.35737464\n",
      "Iteration 319, loss = 0.37445099\n",
      "Iteration 556, loss = 0.33139325\n",
      "Iteration 26, loss = 0.72172103\n",
      "Iteration 239, loss = 0.38455170\n",
      "Iteration 320, loss = 0.37421793\n",
      "Iteration 557, loss = 0.33128597\n",
      "Iteration 165, loss = 0.40099853\n",
      "Iteration 353, loss = 0.35718537\n",
      "Iteration 558, loss = 0.33118556\n",
      "Iteration 31, loss = 0.69248769\n",
      "Iteration 27, loss = 0.72035069\n",
      "Iteration 240, loss = 0.38429918\n",
      "Iteration 559, loss = 0.33109746\n",
      "Iteration 321, loss = 0.37399256\n",
      "Iteration 354, loss = 0.35706391\n",
      "Iteration 560, loss = 0.33099156\n",
      "Iteration 241, loss = 0.38401504\n",
      "Iteration 355, loss = 0.35692661\n",
      "Iteration 166, loss = 0.40062495\n",
      "Iteration 28, loss = 0.71906523\n",
      "Iteration 322, loss = 0.37384992\n",
      "Iteration 561, loss = 0.33089577\n",
      "Iteration 29, loss = 0.71777875\n",
      "Iteration 242, loss = 0.38373270\n",
      "Iteration 356, loss = 0.35682030\n",
      "Iteration 562, loss = 0.33080785\n",
      "Iteration 167, loss = 0.40028515\n",
      "Iteration 357, loss = 0.35667386\n",
      "Iteration 323, loss = 0.37354337\n",
      "Iteration 243, loss = 0.38350313\n",
      "Iteration 563, loss = 0.33070576\n",
      "Iteration 30, loss = 0.71654290\n",
      "Iteration 564, loss = 0.33059436\n",
      "Iteration 244, loss = 0.38320671\n",
      "Iteration 31, loss = 0.71524064\n",
      "Iteration 168, loss = 0.39991704\n",
      "Iteration 245, loss = 0.38296890\n",
      "Iteration 32, loss = 0.71403882\n",
      "Iteration 169, loss = 0.39957820\n",
      "Iteration 358, loss = 0.35656334\n",
      "Iteration 32, loss = 0.68949496\n",
      "Iteration 565, loss = 0.33049046\n",
      "Iteration 324, loss = 0.37332980\n",
      "Iteration 566, loss = 0.33042278\n",
      "Iteration 246, loss = 0.38267992\n",
      "Iteration 359, loss = 0.35641799\n",
      "Iteration 567, loss = 0.33029287\n",
      "Iteration 325, loss = 0.37312034\n",
      "Iteration 247, loss = 0.38244268\n",
      "Iteration 170, loss = 0.39920685\n",
      "Iteration 568, loss = 0.33019854\n",
      "Iteration 33, loss = 0.71276157\n",
      "Iteration 326, loss = 0.37287689\n",
      "Iteration 569, loss = 0.33010355\n",
      "Iteration 360, loss = 0.35628432\n",
      "Iteration 34, loss = 0.71160678\n",
      "Iteration 361, loss = 0.35615642\n",
      "Iteration 248, loss = 0.38215291\n",
      "Iteration 570, loss = 0.33000467\n",
      "Iteration 171, loss = 0.39886839\n",
      "Iteration 571, loss = 0.32990659\n",
      "Iteration 249, loss = 0.38190203\n",
      "Iteration 327, loss = 0.37265866\n",
      "Iteration 572, loss = 0.32981044\n",
      "Iteration 35, loss = 0.71043749\n",
      "Iteration 362, loss = 0.35602713\n",
      "Iteration 36, loss = 0.70917109\n",
      "Iteration 172, loss = 0.39851154\n",
      "Iteration 250, loss = 0.38163921\n",
      "Iteration 573, loss = 0.32971513\n",
      "Iteration 574, loss = 0.32961889\n",
      "Iteration 328, loss = 0.37247226\n",
      "Iteration 107, loss = 0.45584871\n",
      "Iteration 575, loss = 0.32952448\n",
      "Iteration 329, loss = 0.37223111\n",
      "Iteration 37, loss = 0.70795113\n",
      "Iteration 576, loss = 0.32941540\n",
      "Iteration 577, loss = 0.32932139\n",
      "Iteration 363, loss = 0.35591199\n",
      "Iteration 330, loss = 0.37200893\n",
      "Iteration 578, loss = 0.32925883\n",
      "Iteration 173, loss = 0.39819321\n",
      "Iteration 579, loss = 0.32914800\n",
      "Iteration 331, loss = 0.37179576\n",
      "Iteration 108, loss = 0.45479895\n",
      "Iteration 251, loss = 0.38137985\n",
      "Iteration 33, loss = 0.68661241\n",
      "Iteration 364, loss = 0.35578204\n",
      "Iteration 332, loss = 0.37157217\n",
      "Iteration 580, loss = 0.32907338\n",
      "Iteration 252, loss = 0.38112517\n",
      "Iteration 365, loss = 0.35564883\n",
      "Iteration 333, loss = 0.37139600\n",
      "Iteration 174, loss = 0.39782546\n",
      "Iteration 109, loss = 0.45366651\n",
      "Iteration 38, loss = 0.70673174\n",
      "Iteration 253, loss = 0.38088429\n",
      "Iteration 581, loss = 0.32895075\n",
      "Iteration 366, loss = 0.35550250\n",
      "Iteration 334, loss = 0.37116410\n",
      "Iteration 175, loss = 0.39747917\n",
      "Iteration 39, loss = 0.70553467\n",
      "Iteration 254, loss = 0.38064268\n",
      "Iteration 582, loss = 0.32886815\n",
      "Iteration 335, loss = 0.37090767\n",
      "Iteration 367, loss = 0.35538641\n",
      "Iteration 34, loss = 0.68365264\n",
      "Iteration 176, loss = 0.39717294\n",
      "Iteration 583, loss = 0.32877110\n",
      "Iteration 40, loss = 0.70431876\n",
      "Iteration 255, loss = 0.38039009\n",
      "Iteration 336, loss = 0.37076017\n",
      "Iteration 110, loss = 0.45265747\n",
      "Iteration 584, loss = 0.32866561\n",
      "Iteration 368, loss = 0.35527802\n",
      "Iteration 256, loss = 0.38014344\n",
      "Iteration 585, loss = 0.32860510\n",
      "Iteration 337, loss = 0.37049812\n",
      "Iteration 586, loss = 0.32847740\n",
      "Iteration 587, loss = 0.32838659\n",
      "Iteration 257, loss = 0.37988115\n",
      "Iteration 369, loss = 0.35515382\n",
      "Iteration 177, loss = 0.39684588\n",
      "Iteration 111, loss = 0.45166729\n",
      "Iteration 370, loss = 0.35501526\n",
      "Iteration 338, loss = 0.37028932\n",
      "Iteration 588, loss = 0.32832056\n",
      "Iteration 258, loss = 0.37964072\n",
      "Iteration 589, loss = 0.32819826\n",
      "Iteration 35, loss = 0.68077384\n",
      "Iteration 371, loss = 0.35490054\n",
      "Iteration 178, loss = 0.39650661\n",
      "Iteration 339, loss = 0.37007320\n",
      "Iteration 41, loss = 0.70318292\n",
      "Iteration 112, loss = 0.45067376\n",
      "Iteration 590, loss = 0.32810752\n",
      "Iteration 372, loss = 0.35477986\n",
      "Iteration 591, loss = 0.32803130\n",
      "Iteration 259, loss = 0.37939648\n",
      "Iteration 373, loss = 0.35465175\n",
      "Iteration 340, loss = 0.36986695\n",
      "Iteration 592, loss = 0.32791615\n",
      "Iteration 179, loss = 0.39620186\n",
      "Iteration 36, loss = 0.67783086\n",
      "Iteration 593, loss = 0.32782448\n",
      "Iteration 341, loss = 0.36967999\n",
      "Iteration 594, loss = 0.32773332\n",
      "Iteration 113, loss = 0.44963584\n",
      "Iteration 180, loss = 0.39585236\n",
      "Iteration 114, loss = 0.44864782\n",
      "Iteration 181, loss = 0.39554510\n",
      "Iteration 182, loss = 0.39527571\n",
      "Iteration 115, loss = 0.44774577\n",
      "Iteration 183, loss = 0.39492252\n",
      "Iteration 595, loss = 0.32764674\n",
      "Iteration 342, loss = 0.36944866\n",
      "Iteration 116, loss = 0.44682729\n",
      "Iteration 596, loss = 0.32755306\n",
      "Iteration 343, loss = 0.36924671\n",
      "Iteration 37, loss = 0.67491927\n",
      "Iteration 260, loss = 0.37917359\n",
      "Iteration 374, loss = 0.35451847\n",
      "Iteration 42, loss = 0.70199514\n",
      "Iteration 344, loss = 0.36907556\n",
      "Iteration 597, loss = 0.32745252\n",
      "Iteration 261, loss = 0.37890950\n",
      "Iteration 345, loss = 0.36884068\n",
      "Iteration 598, loss = 0.32736950\n",
      "Iteration 43, loss = 0.70078733\n",
      "Iteration 375, loss = 0.35439806\n",
      "Iteration 346, loss = 0.36862252\n",
      "Iteration 262, loss = 0.37871350\n",
      "Iteration 184, loss = 0.39463468\n",
      "Iteration 44, loss = 0.69954080\n",
      "Iteration 347, loss = 0.36843328\n",
      "Iteration 117, loss = 0.44589147\n",
      "Iteration 348, loss = 0.36820735\n",
      "Iteration 376, loss = 0.35427582\n",
      "Iteration 263, loss = 0.37844404\n",
      "Iteration 45, loss = 0.69833848\n",
      "Iteration 349, loss = 0.36802624\n",
      "Iteration 599, loss = 0.32727384\n",
      "Iteration 377, loss = 0.35418002\n",
      "Iteration 185, loss = 0.39431872\n",
      "Iteration 38, loss = 0.67192068\n",
      "Iteration 46, loss = 0.69712798\n",
      "Iteration 264, loss = 0.37819945\n",
      "Iteration 378, loss = 0.35403857\n",
      "Iteration 600, loss = 0.32718130\n",
      "Iteration 118, loss = 0.44499520\n",
      "Iteration 379, loss = 0.35390384\n",
      "Iteration 47, loss = 0.69589038\n",
      "Iteration 601, loss = 0.32707845\n",
      "Iteration 380, loss = 0.35378976\n",
      "Iteration 350, loss = 0.36780570\n",
      "Iteration 602, loss = 0.32702622\n",
      "Iteration 48, loss = 0.69465040\n",
      "Iteration 265, loss = 0.37796413\n",
      "Iteration 39, loss = 0.66898614\n",
      "Iteration 381, loss = 0.35367925\n",
      "Iteration 49, loss = 0.69340883\n",
      "Iteration 603, loss = 0.32689860\n",
      "Iteration 382, loss = 0.35355438\n",
      "Iteration 119, loss = 0.44408857\n",
      "Iteration 383, loss = 0.35343298\n",
      "Iteration 604, loss = 0.32681295\n",
      "Iteration 50, loss = 0.69216824\n",
      "Iteration 266, loss = 0.37774215\n",
      "Iteration 351, loss = 0.36763121\n",
      "Iteration 384, loss = 0.35331860\n",
      "Iteration 186, loss = 0.39401875\n",
      "Iteration 605, loss = 0.32673653\n",
      "Iteration 352, loss = 0.36741378\n",
      "Iteration 385, loss = 0.35317771\n",
      "Iteration 267, loss = 0.37751699\n",
      "Iteration 606, loss = 0.32662819\n",
      "Iteration 51, loss = 0.69091601\n",
      "Iteration 187, loss = 0.39370649\n",
      "Iteration 353, loss = 0.36722176\n",
      "Iteration 52, loss = 0.68962385\n",
      "Iteration 386, loss = 0.35308453\n",
      "Iteration 268, loss = 0.37727308\n",
      "Iteration 53, loss = 0.68839653\n",
      "Iteration 607, loss = 0.32653723\n",
      "Iteration 354, loss = 0.36703125\n",
      "Iteration 387, loss = 0.35295189\n",
      "Iteration 269, loss = 0.37703954\n",
      "Iteration 40, loss = 0.66609571\n",
      "Iteration 608, loss = 0.32644009\n",
      "Iteration 54, loss = 0.68707202\n",
      "Iteration 355, loss = 0.36684641\n",
      "Iteration 388, loss = 0.35284489\n",
      "Iteration 609, loss = 0.32635462\n",
      "Iteration 55, loss = 0.68580144\n",
      "Iteration 270, loss = 0.37681149\n",
      "Iteration 610, loss = 0.32627869\n",
      "Iteration 120, loss = 0.44323417\n",
      "Iteration 389, loss = 0.35269969\n",
      "Iteration 56, loss = 0.68446052\n",
      "Iteration 390, loss = 0.35258438\n",
      "Iteration 271, loss = 0.37659022\n",
      "Iteration 611, loss = 0.32617646\n",
      "Iteration 356, loss = 0.36664970\n",
      "Iteration 272, loss = 0.37636911\n",
      "Iteration 188, loss = 0.39345069\n",
      "Iteration 357, loss = 0.36643309\n",
      "Iteration 273, loss = 0.37613866\n",
      "Iteration 358, loss = 0.36625164\n",
      "Iteration 391, loss = 0.35248065\n",
      "Iteration 612, loss = 0.32608562\n",
      "Iteration 274, loss = 0.37592287\n",
      "Iteration 359, loss = 0.36606560\n",
      "Iteration 57, loss = 0.68306483\n",
      "Iteration 41, loss = 0.66317184\n",
      "Iteration 613, loss = 0.32599804\n",
      "Iteration 392, loss = 0.35243563\n",
      "Iteration 275, loss = 0.37570322\n",
      "Iteration 614, loss = 0.32589296\n",
      "Iteration 360, loss = 0.36587058\n",
      "Iteration 58, loss = 0.68178773\n",
      "Iteration 615, loss = 0.32581711\n",
      "Iteration 393, loss = 0.35223541\n",
      "Iteration 616, loss = 0.32572185\n",
      "Iteration 276, loss = 0.37548177\n",
      "Iteration 617, loss = 0.32562350\n",
      "Iteration 121, loss = 0.44239354\n",
      "Iteration 618, loss = 0.32556420\n",
      "Iteration 619, loss = 0.32543376\n",
      "Iteration 59, loss = 0.68040410\n",
      "Iteration 394, loss = 0.35212315\n",
      "Iteration 620, loss = 0.32536871\n",
      "Iteration 621, loss = 0.32526912\n",
      "Iteration 622, loss = 0.32518420\n",
      "Iteration 623, loss = 0.32509544\n",
      "Iteration 277, loss = 0.37524674\n",
      "Iteration 624, loss = 0.32499736\n",
      "Iteration 625, loss = 0.32491627\n",
      "Iteration 626, loss = 0.32483207\n",
      "Iteration 189, loss = 0.39313509\n",
      "Iteration 627, loss = 0.32472818\n",
      "Iteration 628, loss = 0.32462548\n",
      "Iteration 122, loss = 0.44153838\n",
      "Iteration 278, loss = 0.37505405\n",
      "Iteration 395, loss = 0.35199435\n",
      "Iteration 190, loss = 0.39282752\n",
      "Iteration 60, loss = 0.67898808\n",
      "Iteration 279, loss = 0.37481026\n",
      "Iteration 396, loss = 0.35188929\n",
      "Iteration 123, loss = 0.44077873\n",
      "Iteration 397, loss = 0.35176793\n",
      "Iteration 61, loss = 0.67757927\n",
      "Iteration 361, loss = 0.36570881\n",
      "Iteration 398, loss = 0.35165198Iteration 191, loss = 0.39255863\n",
      "\n",
      "Iteration 280, loss = 0.37460710\n",
      "Iteration 124, loss = 0.43995863\n",
      "Iteration 362, loss = 0.36548320\n",
      "Iteration 42, loss = 0.66019907\n",
      "Iteration 399, loss = 0.35154668\n",
      "Iteration 192, loss = 0.39225459\n",
      "Iteration 400, loss = 0.35142841\n",
      "Iteration 363, loss = 0.36534602\n",
      "Iteration 125, loss = 0.43913858\n",
      "Iteration 364, loss = 0.36511415\n",
      "Iteration 62, loss = 0.67619028\n",
      "Iteration 193, loss = 0.39197481\n",
      "Iteration 281, loss = 0.37438565\n",
      "Iteration 126, loss = 0.43837306\n",
      "Iteration 365, loss = 0.36500849\n",
      "Iteration 63, loss = 0.67476033\n",
      "Iteration 366, loss = 0.36475291\n",
      "Iteration 401, loss = 0.35130975\n",
      "Iteration 629, loss = 0.32454543Iteration 282, loss = 0.37418286\n",
      "Iteration 367, loss = 0.36457489\n",
      "\n",
      "Iteration 368, loss = 0.36437925\n",
      "Iteration 194, loss = 0.39171116\n",
      "Iteration 402, loss = 0.35120974\n",
      "Iteration 43, loss = 0.65717393\n",
      "Iteration 369, loss = 0.36419153\n",
      "Iteration 127, loss = 0.43760162\n",
      "Iteration 630, loss = 0.32446196\n",
      "Iteration 283, loss = 0.37394775\n",
      "Iteration 64, loss = 0.67331512\n",
      "Iteration 370, loss = 0.36404743\n",
      "Iteration 403, loss = 0.35108175\n",
      "Iteration 631, loss = 0.32436517\n",
      "Iteration 371, loss = 0.36383173\n",
      "Iteration 195, loss = 0.39140342\n",
      "Iteration 372, loss = 0.36363356\n",
      "Iteration 284, loss = 0.37375390\n",
      "Iteration 632, loss = 0.32428438\n",
      "Iteration 373, loss = 0.36344963\n",
      "Iteration 65, loss = 0.67178108\n",
      "Iteration 633, loss = 0.32419075\n",
      "Iteration 374, loss = 0.36329363\n",
      "Iteration 196, loss = 0.39113322\n",
      "Iteration 66, loss = 0.67032874\n",
      "Iteration 128, loss = 0.43689333\n",
      "Iteration 44, loss = 0.65430329\n",
      "Iteration 197, loss = 0.39084910\n",
      "Iteration 67, loss = 0.66878104\n",
      "Iteration 129, loss = 0.43613648\n",
      "Iteration 375, loss = 0.36309650\n",
      "Iteration 634, loss = 0.32411617\n",
      "Iteration 198, loss = 0.39057049\n",
      "Iteration 404, loss = 0.35098351\n",
      "Iteration 376, loss = 0.36292538\n",
      "Iteration 635, loss = 0.32401220\n",
      "Iteration 285, loss = 0.37352383\n",
      "Iteration 636, loss = 0.32393081\n",
      "Iteration 377, loss = 0.36277256\n",
      "Iteration 637, loss = 0.32384240\n",
      "Iteration 378, loss = 0.36254723\n",
      "Iteration 638, loss = 0.32375413\n",
      "Iteration 286, loss = 0.37332398\n",
      "Iteration 199, loss = 0.39031946\n",
      "Iteration 130, loss = 0.43540346\n",
      "Iteration 405, loss = 0.35085737Iteration 379, loss = 0.36238410\n",
      "Iteration 639, loss = 0.32366777\n",
      "\n",
      "Iteration 640, loss = 0.32358255\n",
      "Iteration 68, loss = 0.66724199\n",
      "Iteration 380, loss = 0.36220228\n",
      "Iteration 641, loss = 0.32349948\n",
      "Iteration 287, loss = 0.37313083\n",
      "Iteration 381, loss = 0.36202719\n",
      "Iteration 69, loss = 0.66566425\n",
      "Iteration 406, loss = 0.35073587\n",
      "Iteration 45, loss = 0.65131332\n",
      "Iteration 382, loss = 0.36187493\n",
      "Iteration 642, loss = 0.32342890\n",
      "Iteration 131, loss = 0.43469734\n",
      "Iteration 70, loss = 0.66407270\n",
      "Iteration 383, loss = 0.36171545\n",
      "Iteration 288, loss = 0.37291454\n",
      "Iteration 643, loss = 0.32332367\n",
      "Iteration 384, loss = 0.36150716\n",
      "Iteration 71, loss = 0.66241679\n",
      "Iteration 407, loss = 0.35065466\n",
      "Iteration 200, loss = 0.39003928\n",
      "Iteration 408, loss = 0.35055473\n",
      "Iteration 385, loss = 0.36132685\n",
      "Iteration 644, loss = 0.32323190\n",
      "Iteration 409, loss = 0.35039794\n",
      "Iteration 386, loss = 0.36115901\n",
      "Iteration 289, loss = 0.37269927\n",
      "Iteration 410, loss = 0.35029794\n",
      "Iteration 132, loss = 0.43400832\n",
      "Iteration 387, loss = 0.36100742\n",
      "Iteration 411, loss = 0.35020338\n",
      "Iteration 133, loss = 0.43334594\n",
      "Iteration 388, loss = 0.36082959\n",
      "Iteration 72, loss = 0.66081465\n",
      "Iteration 412, loss = 0.35008723\n",
      "Iteration 389, loss = 0.36066255\n",
      "Iteration 645, loss = 0.32316020\n",
      "Iteration 73, loss = 0.65913489\n",
      "Iteration 134, loss = 0.43268131\n",
      "Iteration 390, loss = 0.36052305\n",
      "Iteration 646, loss = 0.32309837\n",
      "Iteration 290, loss = 0.37249478\n",
      "Iteration 74, loss = 0.65743232\n",
      "Iteration 647, loss = 0.32299409\n",
      "Iteration 413, loss = 0.34996096\n",
      "Iteration 391, loss = 0.36032889\n",
      "Iteration 201, loss = 0.38975731\n",
      "Iteration 648, loss = 0.32290529\n",
      "Iteration 75, loss = 0.65570918\n",
      "Iteration 414, loss = 0.34985113\n",
      "Iteration 135, loss = 0.43196741\n",
      "Iteration 291, loss = 0.37228427\n",
      "Iteration 202, loss = 0.38950207\n",
      "Iteration 649, loss = 0.32281720\n",
      "Iteration 392, loss = 0.36022307\n",
      "Iteration 46, loss = 0.64842797\n",
      "Iteration 415, loss = 0.34974726\n",
      "Iteration 650, loss = 0.32272295\n",
      "Iteration 203, loss = 0.38924498\n",
      "Iteration 76, loss = 0.65390607\n",
      "Iteration 651, loss = 0.32266622\n",
      "Iteration 393, loss = 0.35996999\n",
      "Iteration 292, loss = 0.37207802\n",
      "Iteration 204, loss = 0.38896167\n",
      "Iteration 652, loss = 0.32259085\n",
      "Iteration 416, loss = 0.34963246\n",
      "Iteration 77, loss = 0.65210634\n",
      "Iteration 394, loss = 0.35980790\n",
      "Iteration 653, loss = 0.32247653\n",
      "Iteration 136, loss = 0.43138482\n",
      "Iteration 205, loss = 0.38872413\n",
      "Iteration 654, loss = 0.32241610\n",
      "Iteration 417, loss = 0.34951854\n",
      "Iteration 293, loss = 0.37188731\n",
      "Iteration 395, loss = 0.35966266\n",
      "Iteration 655, loss = 0.32231832\n",
      "Iteration 78, loss = 0.65028915\n",
      "Iteration 206, loss = 0.38844206\n",
      "Iteration 396, loss = 0.35947736\n",
      "Iteration 656, loss = 0.32222589\n",
      "Iteration 207, loss = 0.38819359\n",
      "Iteration 294, loss = 0.37168493\n",
      "Iteration 47, loss = 0.64540407\n",
      "Iteration 397, loss = 0.35931611\n",
      "Iteration 418, loss = 0.34944248\n",
      "Iteration 657, loss = 0.32216234Iteration 208, loss = 0.38792993\n",
      "Iteration 398, loss = 0.35915576\n",
      "\n",
      "Iteration 295, loss = 0.37148003\n",
      "Iteration 209, loss = 0.38766968\n",
      "Iteration 419, loss = 0.34930168\n",
      "Iteration 399, loss = 0.35899669\n",
      "Iteration 658, loss = 0.32207408\n",
      "Iteration 79, loss = 0.64847802\n",
      "Iteration 137, loss = 0.43066581\n",
      "Iteration 420, loss = 0.34920732\n",
      "Iteration 210, loss = 0.38743355\n",
      "Iteration 659, loss = 0.32199443\n",
      "Iteration 296, loss = 0.37127162\n",
      "Iteration 400, loss = 0.35882032\n",
      "Iteration 660, loss = 0.32191928\n",
      "Iteration 80, loss = 0.64660952\n",
      "Iteration 421, loss = 0.34910077\n",
      "Iteration 661, loss = 0.32181421\n",
      "Iteration 401, loss = 0.35866851\n",
      "Iteration 297, loss = 0.37109559Iteration 138, loss = 0.43003155\n",
      "\n",
      "Iteration 81, loss = 0.64469662\n",
      "Iteration 662, loss = 0.32174453\n",
      "Iteration 422, loss = 0.34897896\n",
      "Iteration 663, loss = 0.32165294\n",
      "Iteration 402, loss = 0.35849095\n",
      "Iteration 139, loss = 0.42938951\n",
      "Iteration 664, loss = 0.32157154\n",
      "Iteration 423, loss = 0.34886798\n",
      "Iteration 48, loss = 0.64249880\n",
      "Iteration 82, loss = 0.64278501\n",
      "Iteration 665, loss = 0.32150224\n",
      "Iteration 403, loss = 0.35832960\n",
      "Iteration 666, loss = 0.32141057\n",
      "Iteration 667, loss = 0.32134082\n",
      "Iteration 424, loss = 0.34877478\n",
      "Iteration 298, loss = 0.37089034\n",
      "Iteration 668, loss = 0.32125464\n",
      "Iteration 83, loss = 0.64079998\n",
      "Iteration 669, loss = 0.32119550\n",
      "Iteration 140, loss = 0.42878678\n",
      "Iteration 670, loss = 0.32109551\n",
      "Iteration 84, loss = 0.63888993\n",
      "Iteration 671, loss = 0.32101196\n",
      "Iteration 299, loss = 0.37071344\n",
      "Iteration 85, loss = 0.63685189\n",
      "Iteration 86, loss = 0.63496187\n",
      "Iteration 404, loss = 0.35818483\n",
      "Iteration 425, loss = 0.34866458\n",
      "Iteration 405, loss = 0.35802161\n",
      "Iteration 49, loss = 0.63950879\n",
      "Iteration 87, loss = 0.63285622\n",
      "Iteration 406, loss = 0.35788563\n",
      "Iteration 426, loss = 0.34854224\n",
      "Iteration 407, loss = 0.35769084\n",
      "Iteration 408, loss = 0.35753041\n",
      "Iteration 300, loss = 0.37049897\n",
      "Iteration 211, loss = 0.38715933\n",
      "Iteration 427, loss = 0.34844257\n",
      "Iteration 409, loss = 0.35737129\n",
      "Iteration 672, loss = 0.32093438\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 428, loss = 0.34832613\n",
      "Iteration 410, loss = 0.35720542\n",
      "Iteration 301, loss = 0.37029813\n",
      "Iteration 88, loss = 0.63084485\n",
      "Iteration 50, loss = 0.63648365\n",
      "Iteration 411, loss = 0.35706083\n",
      "Iteration 429, loss = 0.34825647\n",
      "Iteration 412, loss = 0.35691613\n",
      "Iteration 141, loss = 0.42824421\n",
      "Iteration 413, loss = 0.35675150\n",
      "Iteration 430, loss = 0.34812569\n",
      "Iteration 414, loss = 0.35661429\n",
      "Iteration 302, loss = 0.37011648\n",
      "Iteration 212, loss = 0.38694232\n",
      "Iteration 89, loss = 0.62883011\n",
      "Iteration 415, loss = 0.35647872\n",
      "Iteration 431, loss = 0.34800552\n",
      "Iteration 90, loss = 0.62674194\n",
      "Iteration 303, loss = 0.36992577Iteration 142, loss = 0.42761845\n",
      "Iteration 213, loss = 0.38670465\n",
      "Iteration 432, loss = 0.34791074\n",
      "\n",
      "Iteration 91, loss = 0.62456807Iteration 1, loss = 0.78084263\n",
      "\n",
      "Iteration 214, loss = 0.38643497\n",
      "Iteration 92, loss = 0.62247680\n",
      "Iteration 433, loss = 0.34778878\n",
      "Iteration 215, loss = 0.38620184\n",
      "Iteration 2, loss = 0.77962410\n",
      "Iteration 93, loss = 0.62040932\n",
      "Iteration 304, loss = 0.36971842\n",
      "Iteration 216, loss = 0.38597241\n",
      "Iteration 434, loss = 0.34769532\n",
      "Iteration 3, loss = 0.77766947\n",
      "Iteration 217, loss = 0.38573282\n",
      "Iteration 218, loss = 0.38546602\n",
      "Iteration 305, loss = 0.36955280\n",
      "Iteration 4, loss = 0.77543137\n",
      "Iteration 94, loss = 0.61823788\n",
      "Iteration 143, loss = 0.42694410\n",
      "Iteration 435, loss = 0.34757058\n",
      "Iteration 306, loss = 0.36935452\n",
      "Iteration 219, loss = 0.38521842\n",
      "Iteration 5, loss = 0.77290353\n",
      "Iteration 307, loss = 0.36915461\n",
      "Iteration 416, loss = 0.35628928Iteration 308, loss = 0.36897960\n",
      "Iteration 309, loss = 0.36881115\n",
      "Iteration 6, loss = 0.77019389\n",
      "\n",
      "Iteration 436, loss = 0.34748259\n",
      "Iteration 51, loss = 0.63361369\n",
      "Iteration 220, loss = 0.38500138\n",
      "Iteration 144, loss = 0.42640643\n",
      "Iteration 437, loss = 0.34737403\n",
      "Iteration 95, loss = 0.61604656\n",
      "Iteration 417, loss = 0.35613735\n",
      "Iteration 418, loss = 0.35597928\n",
      "Iteration 145, loss = 0.42579797\n",
      "Iteration 221, loss = 0.38476135\n",
      "Iteration 419, loss = 0.35582262\n",
      "Iteration 96, loss = 0.61386677\n",
      "Iteration 438, loss = 0.34725565\n",
      "Iteration 310, loss = 0.36862745\n",
      "Iteration 439, loss = 0.34714564\n",
      "Iteration 97, loss = 0.61175852\n",
      "Iteration 420, loss = 0.35566683\n",
      "Iteration 311, loss = 0.36844118\n",
      "Iteration 440, loss = 0.34704944\n",
      "Iteration 146, loss = 0.42520078\n",
      "Iteration 98, loss = 0.60943277\n",
      "Iteration 421, loss = 0.35552505\n",
      "Iteration 441, loss = 0.34696751\n",
      "Iteration 222, loss = 0.38454850\n",
      "Iteration 312, loss = 0.36829816\n",
      "Iteration 442, loss = 0.34684558\n",
      "Iteration 147, loss = 0.42466490\n",
      "Iteration 99, loss = 0.60726779\n",
      "Iteration 443, loss = 0.34672812\n",
      "Iteration 52, loss = 0.63058328\n",
      "Iteration 223, loss = 0.38429578\n",
      "Iteration 422, loss = 0.35538003\n",
      "Iteration 444, loss = 0.34661795\n",
      "Iteration 100, loss = 0.60507303\n",
      "Iteration 313, loss = 0.36806498\n",
      "Iteration 445, loss = 0.34652156\n",
      "Iteration 446, loss = 0.34640728\n",
      "Iteration 101, loss = 0.60285915\n",
      "Iteration 423, loss = 0.35521360\n",
      "Iteration 148, loss = 0.42415468\n",
      "Iteration 224, loss = 0.38408723\n",
      "Iteration 447, loss = 0.34632331\n",
      "Iteration 314, loss = 0.36789938\n",
      "Iteration 102, loss = 0.60062284\n",
      "Iteration 315, loss = 0.36770959\n",
      "Iteration 424, loss = 0.35505682\n",
      "Iteration 448, loss = 0.34618594\n",
      "Iteration 103, loss = 0.59836104\n",
      "Iteration 316, loss = 0.36753483\n",
      "Iteration 149, loss = 0.42354825\n",
      "Iteration 425, loss = 0.35493733\n",
      "Iteration 104, loss = 0.59612582\n",
      "Iteration 317, loss = 0.36738161\n",
      "Iteration 449, loss = 0.34610277\n",
      "Iteration 225, loss = 0.38387029\n",
      "Iteration 105, loss = 0.59382425\n",
      "Iteration 318, loss = 0.36717733\n",
      "Iteration 426, loss = 0.35476397\n",
      "Iteration 53, loss = 0.62761275\n",
      "Iteration 450, loss = 0.34602658\n",
      "Iteration 106, loss = 0.59162891\n",
      "Iteration 226, loss = 0.38361172\n",
      "Iteration 427, loss = 0.35467785\n",
      "Iteration 319, loss = 0.36700236\n",
      "Iteration 451, loss = 0.34590329\n",
      "Iteration 150, loss = 0.42295573\n",
      "Iteration 107, loss = 0.58931921\n",
      "Iteration 452, loss = 0.34579476\n",
      "Iteration 428, loss = 0.35446771\n",
      "Iteration 320, loss = 0.36684384\n",
      "Iteration 321, loss = 0.36665375\n",
      "Iteration 429, loss = 0.35434484\n",
      "Iteration 453, loss = 0.34567995\n",
      "Iteration 108, loss = 0.58705343\n",
      "Iteration 430, loss = 0.35419357\n",
      "Iteration 454, loss = 0.34557876\n",
      "Iteration 151, loss = 0.42250296\n",
      "Iteration 227, loss = 0.38339980\n",
      "Iteration 109, loss = 0.58481312\n",
      "Iteration 455, loss = 0.34546453\n",
      "Iteration 54, loss = 0.62456713\n",
      "Iteration 152, loss = 0.42194890\n",
      "Iteration 228, loss = 0.38317166\n",
      "Iteration 110, loss = 0.58262953\n",
      "Iteration 322, loss = 0.36649938\n",
      "Iteration 456, loss = 0.34538428\n",
      "Iteration 431, loss = 0.35404067\n",
      "Iteration 457, loss = 0.34526467\n",
      "Iteration 458, loss = 0.34515746\n",
      "Iteration 229, loss = 0.38297978\n",
      "Iteration 323, loss = 0.36630975\n",
      "Iteration 459, loss = 0.34505859\n",
      "Iteration 432, loss = 0.35391554\n",
      "Iteration 111, loss = 0.58025243\n",
      "Iteration 433, loss = 0.35375285\n",
      "Iteration 230, loss = 0.38273292\n",
      "Iteration 153, loss = 0.42137367\n",
      "Iteration 434, loss = 0.35361211\n",
      "Iteration 112, loss = 0.57806390\n",
      "Iteration 231, loss = 0.38251917\n",
      "Iteration 324, loss = 0.36614568\n",
      "Iteration 460, loss = 0.34495730\n",
      "Iteration 55, loss = 0.62175443\n",
      "Iteration 113, loss = 0.57572125\n",
      "Iteration 461, loss = 0.34485548\n",
      "Iteration 154, loss = 0.42089033\n",
      "Iteration 435, loss = 0.35345909\n",
      "Iteration 462, loss = 0.34475905\n",
      "Iteration 325, loss = 0.36603691\n",
      "Iteration 463, loss = 0.34464606\n",
      "Iteration 155, loss = 0.42034651\n",
      "Iteration 464, loss = 0.34455294\n",
      "Iteration 436, loss = 0.35333323\n",
      "Iteration 232, loss = 0.38231077\n",
      "Iteration 114, loss = 0.57340414\n",
      "Iteration 7, loss = 0.76741575\n",
      "Iteration 437, loss = 0.35316780\n",
      "Iteration 326, loss = 0.36586000\n",
      "Iteration 56, loss = 0.61874186\n",
      "Iteration 156, loss = 0.41987702\n",
      "Iteration 327, loss = 0.36566230\n",
      "Iteration 438, loss = 0.35302158\n",
      "Iteration 328, loss = 0.36548070\n",
      "Iteration 465, loss = 0.34444350\n",
      "Iteration 115, loss = 0.57113463\n",
      "Iteration 57, loss = 0.61581098\n",
      "Iteration 439, loss = 0.35289510\n",
      "Iteration 157, loss = 0.41936261\n",
      "Iteration 466, loss = 0.34433817\n",
      "Iteration 329, loss = 0.36530482\n",
      "Iteration 233, loss = 0.38206001\n",
      "Iteration 116, loss = 0.56899504\n",
      "Iteration 8, loss = 0.76463818\n",
      "Iteration 117, loss = 0.56664568\n",
      "Iteration 330, loss = 0.36512639\n",
      "Iteration 331, loss = 0.36496369\n",
      "Iteration 467, loss = 0.34423373\n",
      "Iteration 9, loss = 0.76188164\n",
      "Iteration 440, loss = 0.35274176\n",
      "Iteration 332, loss = 0.36479729\n",
      "Iteration 441, loss = 0.35260088\n",
      "Iteration 118, loss = 0.56441211\n",
      "Iteration 333, loss = 0.36463703\n",
      "Iteration 58, loss = 0.61278217\n",
      "Iteration 442, loss = 0.35244933\n",
      "Iteration 468, loss = 0.34413001\n",
      "Iteration 158, loss = 0.41883397\n",
      "Iteration 234, loss = 0.38188594\n",
      "Iteration 443, loss = 0.35233040\n",
      "Iteration 119, loss = 0.56211114\n",
      "Iteration 444, loss = 0.35217201\n",
      "Iteration 334, loss = 0.36448999\n",
      "Iteration 469, loss = 0.34403804\n",
      "Iteration 445, loss = 0.35203254\n",
      "Iteration 120, loss = 0.55989045\n",
      "Iteration 470, loss = 0.34397440\n",
      "Iteration 235, loss = 0.38163369\n",
      "Iteration 446, loss = 0.35187750\n",
      "Iteration 121, loss = 0.55768604\n",
      "Iteration 159, loss = 0.41842522\n",
      "Iteration 335, loss = 0.36430910\n",
      "Iteration 10, loss = 0.75910087\n",
      "Iteration 59, loss = 0.60985718\n",
      "Iteration 471, loss = 0.34382408\n",
      "Iteration 447, loss = 0.35174393\n",
      "Iteration 472, loss = 0.34373785\n",
      "Iteration 122, loss = 0.55553680\n",
      "Iteration 448, loss = 0.35160783\n",
      "Iteration 473, loss = 0.34362185\n",
      "Iteration 123, loss = 0.55336592\n",
      "Iteration 160, loss = 0.41788636\n",
      "Iteration 474, loss = 0.34352255\n",
      "Iteration 236, loss = 0.38144085\n",
      "Iteration 336, loss = 0.36413570\n",
      "Iteration 124, loss = 0.55109908\n",
      "Iteration 475, loss = 0.34343391\n",
      "Iteration 60, loss = 0.60693604\n",
      "Iteration 476, loss = 0.34333377\n",
      "Iteration 125, loss = 0.54905867\n",
      "Iteration 477, loss = 0.34324342\n",
      "Iteration 126, loss = 0.54688532\n",
      "Iteration 449, loss = 0.35150422\n",
      "Iteration 337, loss = 0.36398244\n",
      "Iteration 237, loss = 0.38120954\n",
      "Iteration 11, loss = 0.75637243\n",
      "Iteration 338, loss = 0.36381064\n",
      "Iteration 161, loss = 0.41744306\n",
      "Iteration 450, loss = 0.35131342\n",
      "Iteration 478, loss = 0.34312229\n",
      "Iteration 127, loss = 0.54466345\n",
      "Iteration 451, loss = 0.35118562\n",
      "Iteration 339, loss = 0.36365565\n",
      "Iteration 61, loss = 0.60392066\n",
      "Iteration 238, loss = 0.38101604\n",
      "Iteration 128, loss = 0.54262283\n",
      "Iteration 340, loss = 0.36350377\n",
      "Iteration 479, loss = 0.34303168\n",
      "Iteration 12, loss = 0.75370830\n",
      "Iteration 480, loss = 0.34293173\n",
      "Iteration 239, loss = 0.38081435\n",
      "Iteration 452, loss = 0.35104734Iteration 162, loss = 0.41691981\n",
      "Iteration 341, loss = 0.36332698\n",
      "\n",
      "Iteration 481, loss = 0.34284376\n",
      "Iteration 240, loss = 0.38059003\n",
      "Iteration 62, loss = 0.60111958\n",
      "Iteration 163, loss = 0.41644040\n",
      "Iteration 129, loss = 0.54045794\n",
      "Iteration 13, loss = 0.75100732\n",
      "Iteration 241, loss = 0.38038036\n",
      "Iteration 453, loss = 0.35090060\n",
      "Iteration 482, loss = 0.34272634\n",
      "Iteration 63, loss = 0.59818945\n",
      "Iteration 14, loss = 0.74860839\n",
      "Iteration 164, loss = 0.41597925\n",
      "Iteration 242, loss = 0.38019212\n",
      "Iteration 483, loss = 0.34263183\n",
      "Iteration 454, loss = 0.35080554\n",
      "Iteration 130, loss = 0.53842771\n",
      "Iteration 484, loss = 0.34253467\n",
      "Iteration 455, loss = 0.35062269\n",
      "Iteration 342, loss = 0.36318598\n",
      "Iteration 485, loss = 0.34247069\n",
      "Iteration 15, loss = 0.74620105\n",
      "Iteration 456, loss = 0.35046956\n",
      "Iteration 131, loss = 0.53637470\n",
      "Iteration 486, loss = 0.34235999\n",
      "Iteration 165, loss = 0.41558555\n",
      "Iteration 457, loss = 0.35034853\n",
      "Iteration 487, loss = 0.34223975\n",
      "Iteration 132, loss = 0.53436491\n",
      "Iteration 343, loss = 0.36303296\n",
      "Iteration 16, loss = 0.74357386\n",
      "Iteration 458, loss = 0.35019956\n",
      "Iteration 488, loss = 0.34218313\n",
      "Iteration 344, loss = 0.36288004\n",
      "Iteration 64, loss = 0.59533420\n",
      "Iteration 133, loss = 0.53225685\n",
      "Iteration 243, loss = 0.37998201\n",
      "Iteration 17, loss = 0.74114155\n",
      "Iteration 459, loss = 0.35007639\n",
      "Iteration 489, loss = 0.34205452\n",
      "Iteration 166, loss = 0.41509031\n",
      "Iteration 460, loss = 0.34996332\n",
      "Iteration 461, loss = 0.34982310\n",
      "Iteration 134, loss = 0.53031808\n",
      "Iteration 490, loss = 0.34194356\n",
      "Iteration 345, loss = 0.36272545\n",
      "Iteration 462, loss = 0.34965304\n",
      "Iteration 244, loss = 0.37980744\n",
      "Iteration 463, loss = 0.34954991\n",
      "Iteration 491, loss = 0.34186166\n",
      "Iteration 135, loss = 0.52841419\n",
      "Iteration 464, loss = 0.34940282\n",
      "Iteration 346, loss = 0.36254883\n",
      "Iteration 167, loss = 0.41460458\n",
      "Iteration 465, loss = 0.34927284\n",
      "Iteration 492, loss = 0.34175031\n",
      "Iteration 245, loss = 0.37961421\n",
      "Iteration 466, loss = 0.34909724\n",
      "Iteration 467, loss = 0.34896348\n",
      "Iteration 347, loss = 0.36239289\n",
      "Iteration 468, loss = 0.34883527\n",
      "Iteration 493, loss = 0.34165415\n",
      "Iteration 469, loss = 0.34873788\n",
      "Iteration 470, loss = 0.34855221\n",
      "Iteration 168, loss = 0.41418635\n",
      "Iteration 471, loss = 0.34842803\n",
      "Iteration 472, loss = 0.34828439\n",
      "Iteration 169, loss = 0.41371305\n",
      "Iteration 246, loss = 0.37937808\n",
      "Iteration 473, loss = 0.34815601\n",
      "Iteration 494, loss = 0.34155806\n",
      "Iteration 348, loss = 0.36224127\n",
      "Iteration 136, loss = 0.52640138\n",
      "Iteration 474, loss = 0.34802257\n",
      "Iteration 475, loss = 0.34788439\n",
      "Iteration 495, loss = 0.34145619\n",
      "Iteration 247, loss = 0.37919925\n",
      "Iteration 349, loss = 0.36211313\n",
      "Iteration 496, loss = 0.34139199\n",
      "Iteration 65, loss = 0.59244229\n",
      "Iteration 476, loss = 0.34777431\n",
      "Iteration 350, loss = 0.36194101\n",
      "Iteration 497, loss = 0.34128177\n",
      "Iteration 137, loss = 0.52447549\n",
      "Iteration 170, loss = 0.41328710\n",
      "Iteration 477, loss = 0.34764117\n",
      "Iteration 351, loss = 0.36178930\n",
      "Iteration 248, loss = 0.37898275\n",
      "Iteration 171, loss = 0.41281388\n",
      "Iteration 138, loss = 0.52257746\n",
      "Iteration 498, loss = 0.34119983\n",
      "Iteration 478, loss = 0.34747652\n",
      "Iteration 18, loss = 0.73888081\n",
      "Iteration 352, loss = 0.36163699\n",
      "Iteration 499, loss = 0.34108884\n",
      "Iteration 172, loss = 0.41242535\n",
      "Iteration 479, loss = 0.34737683\n",
      "Iteration 249, loss = 0.37879818\n",
      "Iteration 480, loss = 0.34727396\n",
      "Iteration 19, loss = 0.73646061\n",
      "Iteration 481, loss = 0.34709670\n",
      "Iteration 139, loss = 0.52068365\n",
      "Iteration 500, loss = 0.34098593\n",
      "Iteration 353, loss = 0.36147073\n",
      "Iteration 140, loss = 0.51888388\n",
      "Iteration 173, loss = 0.41196158\n",
      "Iteration 20, loss = 0.73420395\n",
      "Iteration 501, loss = 0.34088429\n",
      "Iteration 141, loss = 0.51698371\n",
      "Iteration 66, loss = 0.58955582Iteration 250, loss = 0.37860186\n",
      "\n",
      "Iteration 142, loss = 0.51523789\n",
      "Iteration 354, loss = 0.36132390\n",
      "Iteration 502, loss = 0.34079271\n",
      "Iteration 143, loss = 0.51352316\n",
      "Iteration 21, loss = 0.73177694\n",
      "Iteration 174, loss = 0.41157004\n",
      "Iteration 251, loss = 0.37839763\n",
      "Iteration 355, loss = 0.36116645\n",
      "Iteration 144, loss = 0.51184363\n",
      "Iteration 503, loss = 0.34068867\n",
      "Iteration 356, loss = 0.36101853\n",
      "Iteration 145, loss = 0.51009084\n",
      "Iteration 482, loss = 0.34696723\n",
      "Iteration 67, loss = 0.58681827\n",
      "Iteration 22, loss = 0.72955584\n",
      "Iteration 504, loss = 0.34059867\n",
      "Iteration 146, loss = 0.50839693\n",
      "Iteration 483, loss = 0.34684363\n",
      "Iteration 357, loss = 0.36088130\n",
      "Iteration 147, loss = 0.50676396\n",
      "Iteration 484, loss = 0.34669270\n",
      "Iteration 252, loss = 0.37818932\n",
      "Iteration 505, loss = 0.34051065\n",
      "Iteration 23, loss = 0.72722277\n",
      "Iteration 485, loss = 0.34657069\n",
      "Iteration 148, loss = 0.50507636\n",
      "Iteration 253, loss = 0.37801177\n",
      "Iteration 506, loss = 0.34040468\n",
      "Iteration 24, loss = 0.72502878\n",
      "Iteration 149, loss = 0.50354882\n",
      "Iteration 254, loss = 0.37781985\n",
      "Iteration 507, loss = 0.34032054\n",
      "Iteration 175, loss = 0.41112080\n",
      "Iteration 68, loss = 0.58395060\n",
      "Iteration 25, loss = 0.72271120\n",
      "Iteration 255, loss = 0.37762354\n",
      "Iteration 486, loss = 0.34643781\n",
      "Iteration 508, loss = 0.34024458\n",
      "Iteration 176, loss = 0.41071514\n",
      "Iteration 487, loss = 0.34631221\n",
      "Iteration 26, loss = 0.72047031\n",
      "Iteration 358, loss = 0.36072121\n",
      "Iteration 256, loss = 0.37743816\n",
      "Iteration 69, loss = 0.58121442\n",
      "Iteration 488, loss = 0.34617755\n",
      "Iteration 177, loss = 0.41032243\n",
      "Iteration 150, loss = 0.50195631\n",
      "Iteration 509, loss = 0.34012860\n",
      "Iteration 359, loss = 0.36059297\n",
      "Iteration 489, loss = 0.34604996\n",
      "Iteration 510, loss = 0.34004475\n",
      "Iteration 178, loss = 0.40992261\n",
      "Iteration 70, loss = 0.57855535\n",
      "Iteration 151, loss = 0.50042371\n",
      "Iteration 360, loss = 0.36043327\n",
      "Iteration 490, loss = 0.34594513\n",
      "Iteration 511, loss = 0.33994642\n",
      "Iteration 257, loss = 0.37725071\n",
      "Iteration 27, loss = 0.71826372\n",
      "Iteration 361, loss = 0.36029483\n",
      "Iteration 152, loss = 0.49887362\n",
      "Iteration 491, loss = 0.34584098\n",
      "Iteration 179, loss = 0.40950291\n",
      "Iteration 512, loss = 0.33986379\n",
      "Iteration 71, loss = 0.57577393\n",
      "Iteration 513, loss = 0.33974960\n",
      "Iteration 492, loss = 0.34568623Iteration 362, loss = 0.36013162\n",
      "\n",
      "Iteration 153, loss = 0.49737245\n",
      "Iteration 514, loss = 0.33965824\n",
      "Iteration 180, loss = 0.40909813\n",
      "Iteration 363, loss = 0.35999306\n",
      "Iteration 493, loss = 0.34557838\n",
      "Iteration 515, loss = 0.33956794\n",
      "Iteration 154, loss = 0.49589074\n",
      "Iteration 28, loss = 0.71601119\n",
      "Iteration 516, loss = 0.33947173\n",
      "Iteration 258, loss = 0.37707393\n",
      "Iteration 494, loss = 0.34539953\n",
      "Iteration 181, loss = 0.40872735Iteration 364, loss = 0.35984444\n",
      "\n",
      "Iteration 155, loss = 0.49448973\n",
      "Iteration 517, loss = 0.33941036\n",
      "Iteration 495, loss = 0.34530928\n",
      "Iteration 518, loss = 0.33929192\n",
      "Iteration 259, loss = 0.37688510\n",
      "Iteration 156, loss = 0.49304015\n",
      "Iteration 365, loss = 0.35971066\n",
      "Iteration 519, loss = 0.33918279\n",
      "Iteration 496, loss = 0.34515087\n",
      "Iteration 29, loss = 0.71383053\n",
      "Iteration 72, loss = 0.57306721\n",
      "Iteration 520, loss = 0.33909347\n",
      "Iteration 260, loss = 0.37669709\n",
      "Iteration 157, loss = 0.49167938\n",
      "Iteration 521, loss = 0.33904564\n",
      "Iteration 366, loss = 0.35955837\n",
      "Iteration 497, loss = 0.34502593\n",
      "Iteration 182, loss = 0.40831810\n",
      "Iteration 522, loss = 0.33891610\n",
      "Iteration 498, loss = 0.34488384\n",
      "Iteration 367, loss = 0.35941521\n",
      "Iteration 261, loss = 0.37651308\n",
      "Iteration 499, loss = 0.34478427\n",
      "Iteration 523, loss = 0.33881692\n",
      "Iteration 500, loss = 0.34463832\n",
      "Iteration 30, loss = 0.71157008\n",
      "Iteration 262, loss = 0.37633193\n",
      "Iteration 501, loss = 0.34450802\n",
      "Iteration 502, loss = 0.34439872\n",
      "Iteration 524, loss = 0.33875660\n",
      "Iteration 368, loss = 0.35927078\n",
      "Iteration 31, loss = 0.70928160\n",
      "Iteration 503, loss = 0.34427721\n",
      "Iteration 158, loss = 0.49024437\n",
      "Iteration 525, loss = 0.33863035\n",
      "Iteration 369, loss = 0.35912124\n",
      "Iteration 526, loss = 0.33854247\n",
      "Iteration 263, loss = 0.37613463\n",
      "Iteration 159, loss = 0.48894889\n",
      "Iteration 370, loss = 0.35898208\n",
      "Iteration 504, loss = 0.34412382\n",
      "Iteration 32, loss = 0.70707839\n",
      "Iteration 527, loss = 0.33845765\n",
      "Iteration 264, loss = 0.37595199\n",
      "Iteration 505, loss = 0.34401057\n",
      "Iteration 183, loss = 0.40794054\n",
      "Iteration 528, loss = 0.33836762\n",
      "Iteration 371, loss = 0.35884339\n",
      "Iteration 73, loss = 0.57040001\n",
      "Iteration 506, loss = 0.34389057\n",
      "Iteration 529, loss = 0.33828080\n",
      "Iteration 372, loss = 0.35871051\n",
      "Iteration 184, loss = 0.40758309\n",
      "Iteration 373, loss = 0.35856159\n",
      "Iteration 507, loss = 0.34377085\n",
      "Iteration 530, loss = 0.33817818\n",
      "Iteration 160, loss = 0.48759743\n",
      "Iteration 265, loss = 0.37577625\n",
      "Iteration 185, loss = 0.40719885\n",
      "Iteration 508, loss = 0.34363227\n",
      "Iteration 531, loss = 0.33809343\n",
      "Iteration 186, loss = 0.40683351\n",
      "Iteration 266, loss = 0.37563009\n",
      "Iteration 509, loss = 0.34351213\n",
      "Iteration 532, loss = 0.33799156\n",
      "Iteration 374, loss = 0.35842605\n",
      "Iteration 161, loss = 0.48627176\n",
      "Iteration 510, loss = 0.34337577\n",
      "Iteration 511, loss = 0.34327766\n",
      "Iteration 533, loss = 0.33790915\n",
      "Iteration 162, loss = 0.48498296\n",
      "Iteration 187, loss = 0.40646307\n",
      "Iteration 375, loss = 0.35828690\n",
      "Iteration 267, loss = 0.37540768\n",
      "Iteration 512, loss = 0.34312222\n",
      "Iteration 33, loss = 0.70479207\n",
      "Iteration 513, loss = 0.34299719\n",
      "Iteration 376, loss = 0.35817528\n",
      "Iteration 163, loss = 0.48376715\n",
      "Iteration 74, loss = 0.56781250\n",
      "Iteration 268, loss = 0.37525405\n",
      "Iteration 188, loss = 0.40611481\n",
      "Iteration 534, loss = 0.33781134\n",
      "Iteration 164, loss = 0.48252302\n",
      "Iteration 514, loss = 0.34291762\n",
      "Iteration 535, loss = 0.33772616\n",
      "Iteration 515, loss = 0.34276226\n",
      "Iteration 536, loss = 0.33765322\n",
      "Iteration 377, loss = 0.35801173\n",
      "Iteration 516, loss = 0.34262420\n",
      "Iteration 378, loss = 0.35786078\n",
      "Iteration 189, loss = 0.40572094\n",
      "Iteration 34, loss = 0.70259414\n",
      "Iteration 537, loss = 0.33754096\n",
      "Iteration 165, loss = 0.48131988\n",
      "Iteration 517, loss = 0.34251249\n",
      "Iteration 379, loss = 0.35773990\n",
      "Iteration 269, loss = 0.37504436\n",
      "Iteration 518, loss = 0.34239494\n",
      "Iteration 519, loss = 0.34225310\n",
      "Iteration 166, loss = 0.48010128\n",
      "Iteration 380, loss = 0.35758430\n",
      "Iteration 270, loss = 0.37491035\n",
      "Iteration 520, loss = 0.34214995\n",
      "Iteration 190, loss = 0.40538024\n",
      "Iteration 381, loss = 0.35745366\n",
      "Iteration 521, loss = 0.34201062\n",
      "Iteration 35, loss = 0.70024835\n",
      "Iteration 522, loss = 0.34187534\n",
      "Iteration 167, loss = 0.47897009\n",
      "Iteration 538, loss = 0.33744399\n",
      "Iteration 271, loss = 0.37472568\n",
      "Iteration 382, loss = 0.35732034\n",
      "Iteration 523, loss = 0.34177570\n",
      "Iteration 191, loss = 0.40501431\n",
      "Iteration 168, loss = 0.47783585\n",
      "Iteration 524, loss = 0.34163850\n",
      "Iteration 169, loss = 0.47662789\n",
      "Iteration 170, loss = 0.47558704\n",
      "Iteration 192, loss = 0.40464847\n",
      "Iteration 75, loss = 0.56523585\n",
      "Iteration 383, loss = 0.35719274\n",
      "Iteration 525, loss = 0.34152067\n",
      "Iteration 36, loss = 0.69792434\n",
      "Iteration 171, loss = 0.47447873\n",
      "Iteration 272, loss = 0.37454399\n",
      "Iteration 539, loss = 0.33737599\n",
      "Iteration 526, loss = 0.34139324\n",
      "Iteration 384, loss = 0.35704629\n",
      "Iteration 540, loss = 0.33727261\n",
      "Iteration 527, loss = 0.34127218\n",
      "Iteration 193, loss = 0.40432310\n",
      "Iteration 541, loss = 0.33718680\n",
      "Iteration 385, loss = 0.35691831\n",
      "Iteration 76, loss = 0.56262678\n",
      "Iteration 172, loss = 0.47342921\n",
      "Iteration 194, loss = 0.40397409\n",
      "Iteration 528, loss = 0.34114173\n",
      "Iteration 273, loss = 0.37434763\n",
      "Iteration 386, loss = 0.35678959\n",
      "Iteration 542, loss = 0.33710049\n",
      "Iteration 173, loss = 0.47236646\n",
      "Iteration 543, loss = 0.33699946\n",
      "Iteration 387, loss = 0.35665774\n",
      "Iteration 37, loss = 0.69558768\n",
      "Iteration 529, loss = 0.34104818\n",
      "Iteration 544, loss = 0.33690908\n",
      "Iteration 77, loss = 0.56014705\n",
      "Iteration 274, loss = 0.37417243\n",
      "Iteration 545, loss = 0.33683960\n",
      "Iteration 530, loss = 0.34093089\n",
      "Iteration 546, loss = 0.33673509\n",
      "Iteration 174, loss = 0.47131963\n",
      "Iteration 195, loss = 0.40360340\n",
      "Iteration 531, loss = 0.34079336\n",
      "Iteration 547, loss = 0.33664260\n",
      "Iteration 388, loss = 0.35651891\n",
      "Iteration 175, loss = 0.47034012\n",
      "Iteration 548, loss = 0.33657061\n",
      "Iteration 389, loss = 0.35639635\n",
      "Iteration 176, loss = 0.46937727\n",
      "Iteration 549, loss = 0.33647459\n",
      "Iteration 38, loss = 0.69331425\n",
      "Iteration 532, loss = 0.34066107\n",
      "Iteration 533, loss = 0.34056441\n",
      "Iteration 275, loss = 0.37400492\n",
      "Iteration 390, loss = 0.35625689\n",
      "Iteration 550, loss = 0.33638166\n",
      "Iteration 177, loss = 0.46829189\n",
      "Iteration 551, loss = 0.33629866\n",
      "Iteration 552, loss = 0.33621711\n",
      "Iteration 178, loss = 0.46735535\n",
      "Iteration 553, loss = 0.33611215\n",
      "Iteration 196, loss = 0.40328904\n",
      "Iteration 534, loss = 0.34040359\n",
      "Iteration 554, loss = 0.33602520\n",
      "Iteration 78, loss = 0.55760214\n",
      "Iteration 391, loss = 0.35612548\n",
      "Iteration 535, loss = 0.34032959\n",
      "Iteration 555, loss = 0.33594159\n",
      "Iteration 197, loss = 0.40296736\n",
      "Iteration 536, loss = 0.34015923\n",
      "Iteration 556, loss = 0.33586201\n",
      "Iteration 179, loss = 0.46642933\n",
      "Iteration 537, loss = 0.34004677\n",
      "Iteration 392, loss = 0.35600402\n",
      "Iteration 557, loss = 0.33575943\n",
      "Iteration 538, loss = 0.33993677\n",
      "Iteration 276, loss = 0.37383945\n",
      "Iteration 558, loss = 0.33567787\n",
      "Iteration 393, loss = 0.35588061\n",
      "Iteration 559, loss = 0.33558402\n",
      "Iteration 198, loss = 0.40259519\n",
      "Iteration 539, loss = 0.33982897\n",
      "Iteration 560, loss = 0.33551378\n",
      "Iteration 180, loss = 0.46549529\n",
      "Iteration 561, loss = 0.33542702\n",
      "Iteration 199, loss = 0.40226568\n",
      "Iteration 562, loss = 0.33532476\n",
      "Iteration 540, loss = 0.33969239\n",
      "Iteration 79, loss = 0.55508253\n",
      "Iteration 563, loss = 0.33523551\n",
      "Iteration 200, loss = 0.40196193\n",
      "Iteration 564, loss = 0.33514973\n",
      "Iteration 39, loss = 0.69094664\n",
      "Iteration 565, loss = 0.33507364\n",
      "Iteration 277, loss = 0.37365141\n",
      "Iteration 181, loss = 0.46451901\n",
      "Iteration 566, loss = 0.33498886\n",
      "Iteration 201, loss = 0.40159294\n",
      "Iteration 567, loss = 0.33490763\n",
      "Iteration 541, loss = 0.33958294\n",
      "Iteration 80, loss = 0.55260185\n",
      "Iteration 182, loss = 0.46362572\n",
      "Iteration 394, loss = 0.35576271\n",
      "Iteration 568, loss = 0.33480608\n",
      "Iteration 278, loss = 0.37348794\n",
      "Iteration 395, loss = 0.35565206\n",
      "Iteration 202, loss = 0.40133726\n",
      "Iteration 542, loss = 0.33945598\n",
      "Iteration 396, loss = 0.35547920\n",
      "Iteration 183, loss = 0.46282217\n",
      "Iteration 543, loss = 0.33933668\n",
      "Iteration 397, loss = 0.35535764\n",
      "Iteration 81, loss = 0.55026653\n",
      "Iteration 40, loss = 0.68848482\n",
      "Iteration 544, loss = 0.33921811\n",
      "Iteration 398, loss = 0.35523041\n",
      "Iteration 545, loss = 0.33911582\n",
      "Iteration 279, loss = 0.37333852\n",
      "Iteration 184, loss = 0.46186996\n",
      "Iteration 569, loss = 0.33472565\n",
      "Iteration 546, loss = 0.33896635\n",
      "Iteration 399, loss = 0.35511472\n",
      "Iteration 185, loss = 0.46099579\n",
      "Iteration 41, loss = 0.68618869\n",
      "Iteration 186, loss = 0.46014882\n",
      "Iteration 570, loss = 0.33465461\n",
      "Iteration 280, loss = 0.37315423\n",
      "Iteration 82, loss = 0.54798108\n",
      "Iteration 547, loss = 0.33884163\n",
      "Iteration 187, loss = 0.45937067Iteration 203, loss = 0.40110250\n",
      "Iteration 400, loss = 0.35497289\n",
      "Iteration 571, loss = 0.33455375\n",
      "Iteration 548, loss = 0.33872425\n",
      "Iteration 549, loss = 0.33859452\n",
      "\n",
      "Iteration 401, loss = 0.35485228\n",
      "Iteration 281, loss = 0.37298178\n",
      "Iteration 188, loss = 0.45847835\n",
      "Iteration 550, loss = 0.33847858\n",
      "Iteration 572, loss = 0.33447282\n",
      "Iteration 551, loss = 0.33837844\n",
      "Iteration 552, loss = 0.33823731\n",
      "Iteration 83, loss = 0.54554709\n",
      "Iteration 42, loss = 0.68377554\n",
      "Iteration 189, loss = 0.45769079\n",
      "Iteration 204, loss = 0.40068457\n",
      "Iteration 573, loss = 0.33438123\n",
      "Iteration 553, loss = 0.33814331\n",
      "Iteration 190, loss = 0.45694669\n",
      "Iteration 282, loss = 0.37281841\n",
      "Iteration 191, loss = 0.45616243\n",
      "Iteration 574, loss = 0.33431590\n",
      "Iteration 402, loss = 0.35474971\n",
      "Iteration 192, loss = 0.45537324\n",
      "Iteration 554, loss = 0.33801160\n",
      "Iteration 403, loss = 0.35459848\n",
      "Iteration 84, loss = 0.54328895\n",
      "Iteration 205, loss = 0.40036344\n",
      "Iteration 193, loss = 0.45463644\n",
      "Iteration 555, loss = 0.33788185\n",
      "Iteration 404, loss = 0.35447663\n",
      "Iteration 556, loss = 0.33775013\n",
      "Iteration 43, loss = 0.68135651\n",
      "Iteration 405, loss = 0.35434302\n",
      "Iteration 194, loss = 0.45389620\n",
      "Iteration 575, loss = 0.33420698\n",
      "Iteration 44, loss = 0.67895686\n",
      "Iteration 85, loss = 0.54107107\n",
      "Iteration 557, loss = 0.33765864\n",
      "Iteration 206, loss = 0.40000760\n",
      "Iteration 406, loss = 0.35423943\n",
      "Iteration 283, loss = 0.37264940\n",
      "Iteration 558, loss = 0.33752306\n",
      "Iteration 207, loss = 0.39977113\n",
      "Iteration 576, loss = 0.33412915\n",
      "Iteration 559, loss = 0.33740936\n",
      "Iteration 195, loss = 0.45313951\n",
      "Iteration 407, loss = 0.35411756\n",
      "Iteration 45, loss = 0.67652981Iteration 196, loss = 0.45241383\n",
      "Iteration 577, loss = 0.33402635\n",
      "Iteration 408, loss = 0.35399013\n",
      "Iteration 284, loss = 0.37247652\n",
      "Iteration 578, loss = 0.33395244\n",
      "Iteration 197, loss = 0.45170697\n",
      "Iteration 560, loss = 0.33729682\n",
      "Iteration 208, loss = 0.39943933\n",
      "Iteration 579, loss = 0.33387232\n",
      "\n",
      "Iteration 561, loss = 0.33713798\n",
      "Iteration 580, loss = 0.33377097\n",
      "Iteration 198, loss = 0.45096861\n",
      "Iteration 562, loss = 0.33701884\n",
      "Iteration 409, loss = 0.35385955\n",
      "Iteration 581, loss = 0.33367873\n",
      "Iteration 86, loss = 0.53879735\n",
      "Iteration 582, loss = 0.33358623\n",
      "Iteration 199, loss = 0.45028424Iteration 285, loss = 0.37232948\n",
      "\n",
      "Iteration 209, loss = 0.39913455\n",
      "Iteration 563, loss = 0.33692437\n",
      "Iteration 200, loss = 0.44957883\n",
      "Iteration 201, loss = 0.44890368\n",
      "Iteration 564, loss = 0.33679193\n",
      "Iteration 202, loss = 0.44819433\n",
      "Iteration 565, loss = 0.33666484\n",
      "Iteration 410, loss = 0.35372851\n",
      "Iteration 286, loss = 0.37214567\n",
      "Iteration 583, loss = 0.33350731\n",
      "Iteration 210, loss = 0.39884112\n",
      "Iteration 203, loss = 0.44753152\n",
      "Iteration 566, loss = 0.33654447\n",
      "Iteration 46, loss = 0.67406225\n",
      "Iteration 204, loss = 0.44690747\n",
      "Iteration 287, loss = 0.37200013\n",
      "Iteration 411, loss = 0.35360884\n",
      "Iteration 584, loss = 0.33341787\n",
      "Iteration 211, loss = 0.39854026\n",
      "Iteration 567, loss = 0.33642824\n",
      "Iteration 288, loss = 0.37183293\n",
      "Iteration 47, loss = 0.67162172\n",
      "Iteration 289, loss = 0.37165810\n",
      "Iteration 212, loss = 0.39824802\n",
      "Iteration 205, loss = 0.44621967\n",
      "Iteration 585, loss = 0.33334987\n",
      "Iteration 412, loss = 0.35349794\n",
      "Iteration 586, loss = 0.33324448\n",
      "Iteration 206, loss = 0.44550931\n",
      "Iteration 290, loss = 0.37150925\n",
      "Iteration 587, loss = 0.33316761\n",
      "Iteration 568, loss = 0.33631968\n",
      "Iteration 413, loss = 0.35336506\n",
      "Iteration 588, loss = 0.33307464\n",
      "Iteration 414, loss = 0.35324825\n",
      "Iteration 569, loss = 0.33619077\n",
      "Iteration 589, loss = 0.33298902\n",
      "Iteration 291, loss = 0.37133504\n",
      "Iteration 207, loss = 0.44489939\n",
      "Iteration 590, loss = 0.33290677\n",
      "Iteration 570, loss = 0.33607889\n",
      "Iteration 87, loss = 0.53660882\n",
      "Iteration 415, loss = 0.35313819\n",
      "Iteration 591, loss = 0.33282333\n",
      "Iteration 48, loss = 0.66913956\n",
      "Iteration 213, loss = 0.39794046\n",
      "Iteration 208, loss = 0.44428114\n",
      "Iteration 292, loss = 0.37117489\n",
      "Iteration 571, loss = 0.33594072\n",
      "Iteration 49, loss = 0.66663761\n",
      "Iteration 88, loss = 0.53440962\n",
      "Iteration 209, loss = 0.44363837\n",
      "Iteration 293, loss = 0.37102140\n",
      "Iteration 592, loss = 0.33272789\n",
      "Iteration 210, loss = 0.44300748\n",
      "Iteration 593, loss = 0.33264558\n",
      "Iteration 572, loss = 0.33589077\n",
      "Iteration 594, loss = 0.33255008\n",
      "Iteration 214, loss = 0.39765590\n",
      "Iteration 211, loss = 0.44235280\n",
      "Iteration 294, loss = 0.37087813\n",
      "Iteration 595, loss = 0.33250016\n",
      "Iteration 416, loss = 0.35300042\n",
      "Iteration 573, loss = 0.33571059\n",
      "Iteration 596, loss = 0.33239056\n",
      "Iteration 50, loss = 0.66407686\n",
      "Iteration 417, loss = 0.35289288\n",
      "Iteration 212, loss = 0.44179845\n",
      "Iteration 89, loss = 0.53234025\n",
      "Iteration 574, loss = 0.33560564\n",
      "Iteration 597, loss = 0.33229321\n",
      "Iteration 215, loss = 0.39736494\n",
      "Iteration 213, loss = 0.44113213\n",
      "Iteration 575, loss = 0.33547089\n",
      "Iteration 576, loss = 0.33533634\n",
      "Iteration 90, loss = 0.53024701\n",
      "Iteration 577, loss = 0.33523709\n",
      "Iteration 598, loss = 0.33225581\n",
      "Iteration 418, loss = 0.35276111\n",
      "Iteration 599, loss = 0.33212691\n",
      "Iteration 419, loss = 0.35265818\n",
      "Iteration 214, loss = 0.44052097\n",
      "Iteration 578, loss = 0.33511463\n",
      "Iteration 215, loss = 0.43991361\n",
      "Iteration 420, loss = 0.35253716\n",
      "Iteration 579, loss = 0.33498323\n",
      "Iteration 295, loss = 0.37072062\n",
      "Iteration 600, loss = 0.33203799\n",
      "Iteration 51, loss = 0.66154538\n",
      "Iteration 601, loss = 0.33195698\n",
      "Iteration 580, loss = 0.33487996\n",
      "Iteration 216, loss = 0.39709008\n",
      "Iteration 602, loss = 0.33186858\n",
      "Iteration 296, loss = 0.37056010\n",
      "Iteration 216, loss = 0.43931637\n",
      "Iteration 581, loss = 0.33477146\n",
      "Iteration 421, loss = 0.35242341\n",
      "Iteration 217, loss = 0.39682887\n",
      "Iteration 52, loss = 0.65906459\n",
      "Iteration 582, loss = 0.33466086\n",
      "Iteration 603, loss = 0.33178890\n",
      "Iteration 217, loss = 0.43872185\n",
      "Iteration 91, loss = 0.52819432\n",
      "Iteration 422, loss = 0.35230084\n",
      "Iteration 297, loss = 0.37039890\n",
      "Iteration 604, loss = 0.33170743\n",
      "Iteration 583, loss = 0.33451455\n",
      "Iteration 218, loss = 0.39651880\n",
      "Iteration 584, loss = 0.33438365\n",
      "Iteration 605, loss = 0.33163617\n",
      "Iteration 423, loss = 0.35217857\n",
      "Iteration 218, loss = 0.43809655\n",
      "Iteration 298, loss = 0.37025166\n",
      "Iteration 606, loss = 0.33154455\n",
      "Iteration 424, loss = 0.35207295\n",
      "Iteration 219, loss = 0.39629963\n",
      "Iteration 299, loss = 0.37007411\n",
      "Iteration 607, loss = 0.33145394\n",
      "Iteration 425, loss = 0.35194970\n",
      "Iteration 219, loss = 0.43750689\n",
      "Iteration 53, loss = 0.65647004\n",
      "Iteration 608, loss = 0.33135695Iteration 220, loss = 0.39601690\n",
      "\n",
      "Iteration 585, loss = 0.33426746\n",
      "Iteration 426, loss = 0.35183029\n",
      "Iteration 609, loss = 0.33127803\n",
      "Iteration 54, loss = 0.65392256\n",
      "Iteration 427, loss = 0.35171379\n",
      "Iteration 300, loss = 0.36991929\n",
      "Iteration 586, loss = 0.33415273\n",
      "Iteration 220, loss = 0.43694822\n",
      "Iteration 610, loss = 0.33121096\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 587, loss = 0.33404938\n",
      "Iteration 428, loss = 0.35160266\n",
      "Iteration 221, loss = 0.39569713Iteration 221, loss = 0.43639460\n",
      "\n",
      "Iteration 588, loss = 0.33392137\n",
      "Iteration 92, loss = 0.52622607\n",
      "Iteration 429, loss = 0.35150278\n",
      "Iteration 301, loss = 0.36977021\n",
      "Iteration 589, loss = 0.33381509\n",
      "Iteration 222, loss = 0.43580505\n",
      "Iteration 430, loss = 0.35137195\n",
      "Iteration 222, loss = 0.39545021\n",
      "Iteration 302, loss = 0.36959908\n",
      "Iteration 223, loss = 0.43527046\n",
      "Iteration 590, loss = 0.33368589\n",
      "Iteration 223, loss = 0.39517392\n",
      "Iteration 303, loss = 0.36945572\n",
      "Iteration 224, loss = 0.43465138\n",
      "Iteration 431, loss = 0.35126412\n",
      "Iteration 591, loss = 0.33357443\n",
      "Iteration 224, loss = 0.39492961\n",
      "Iteration 225, loss = 0.43411188\n",
      "Iteration 592, loss = 0.33345723\n",
      "Iteration 55, loss = 0.65131850\n",
      "Iteration 432, loss = 0.35114794\n",
      "Iteration 93, loss = 0.52422396\n",
      "Iteration 226, loss = 0.43352891\n",
      "Iteration 593, loss = 0.33333226\n",
      "Iteration 1, loss = 0.80688092\n",
      "Iteration 227, loss = 0.43300678\n",
      "Iteration 225, loss = 0.39465302\n",
      "Iteration 94, loss = 0.52227809\n",
      "Iteration 226, loss = 0.39434626\n",
      "Iteration 433, loss = 0.35103726\n",
      "Iteration 56, loss = 0.64877316\n",
      "Iteration 228, loss = 0.43243676\n",
      "Iteration 304, loss = 0.36930924\n",
      "Iteration 227, loss = 0.39411824\n",
      "Iteration 229, loss = 0.43189106\n",
      "Iteration 594, loss = 0.33324834\n",
      "Iteration 595, loss = 0.33310350\n",
      "Iteration 95, loss = 0.52038227\n",
      "Iteration 305, loss = 0.36915886\n",
      "Iteration 434, loss = 0.35091184\n",
      "Iteration 596, loss = 0.33301404\n",
      "Iteration 57, loss = 0.64605483\n",
      "Iteration 228, loss = 0.39387661\n",
      "Iteration 597, loss = 0.33286983\n",
      "Iteration 435, loss = 0.35080749\n",
      "Iteration 96, loss = 0.51849319\n",
      "Iteration 598, loss = 0.33275015\n",
      "Iteration 230, loss = 0.43132402\n",
      "Iteration 306, loss = 0.36900680\n",
      "Iteration 599, loss = 0.33263756\n",
      "Iteration 231, loss = 0.43080806\n",
      "Iteration 436, loss = 0.35068858\n",
      "Iteration 58, loss = 0.64346684\n",
      "Iteration 232, loss = 0.43028062\n",
      "Iteration 600, loss = 0.33252934\n",
      "Iteration 437, loss = 0.35060889\n",
      "Iteration 97, loss = 0.51672673\n",
      "Iteration 601, loss = 0.33241438\n",
      "Iteration 602, loss = 0.33232276\n",
      "Iteration 438, loss = 0.35046937\n",
      "Iteration 2, loss = 0.80626806\n",
      "Iteration 307, loss = 0.36885008\n",
      "Iteration 233, loss = 0.42978687\n",
      "Iteration 59, loss = 0.64086576\n",
      "Iteration 439, loss = 0.35035168\n",
      "Iteration 98, loss = 0.51487740\n",
      "Iteration 234, loss = 0.42922566\n",
      "Iteration 60, loss = 0.63821192\n",
      "Iteration 440, loss = 0.35023933\n",
      "Iteration 235, loss = 0.42870445\n",
      "Iteration 229, loss = 0.39359864\n",
      "Iteration 308, loss = 0.36870475\n",
      "Iteration 61, loss = 0.63553456\n",
      "Iteration 441, loss = 0.35013572\n",
      "Iteration 236, loss = 0.42820607\n",
      "Iteration 230, loss = 0.39335315\n",
      "Iteration 309, loss = 0.36853813\n",
      "Iteration 99, loss = 0.51321633\n",
      "Iteration 3, loss = 0.80534246\n",
      "Iteration 442, loss = 0.35002576\n",
      "Iteration 231, loss = 0.39308593\n",
      "Iteration 237, loss = 0.42770505\n",
      "Iteration 603, loss = 0.33222026\n",
      "Iteration 310, loss = 0.36840673\n",
      "Iteration 604, loss = 0.33209883\n",
      "Iteration 443, loss = 0.34989816\n",
      "Iteration 238, loss = 0.42715055\n",
      "Iteration 444, loss = 0.34978663\n",
      "Iteration 311, loss = 0.36827600\n",
      "Iteration 232, loss = 0.39284295\n",
      "Iteration 239, loss = 0.42665393\n",
      "Iteration 445, loss = 0.34968517\n",
      "Iteration 62, loss = 0.63287161\n",
      "Iteration 240, loss = 0.42617341\n",
      "Iteration 312, loss = 0.36809472\n",
      "Iteration 100, loss = 0.51140840\n",
      "Iteration 446, loss = 0.34956018\n",
      "Iteration 241, loss = 0.42564970\n",
      "Iteration 447, loss = 0.34945403\n",
      "Iteration 63, loss = 0.63009787\n",
      "Iteration 233, loss = 0.39259496\n",
      "Iteration 242, loss = 0.42517037\n",
      "Iteration 448, loss = 0.34934160\n",
      "Iteration 313, loss = 0.36794598\n",
      "Iteration 4, loss = 0.80421739\n",
      "Iteration 243, loss = 0.42464721\n",
      "Iteration 449, loss = 0.34924076\n",
      "Iteration 101, loss = 0.50976058\n",
      "Iteration 244, loss = 0.42414980\n",
      "Iteration 234, loss = 0.39234778\n",
      "Iteration 64, loss = 0.62753582\n",
      "Iteration 450, loss = 0.34912523\n",
      "Iteration 245, loss = 0.42370340\n",
      "Iteration 314, loss = 0.36780793\n",
      "Iteration 451, loss = 0.34902577\n",
      "Iteration 235, loss = 0.39211452\n",
      "Iteration 452, loss = 0.34891835\n",
      "Iteration 246, loss = 0.42319445\n",
      "Iteration 453, loss = 0.34880858\n",
      "Iteration 102, loss = 0.50809945\n",
      "Iteration 65, loss = 0.62488588\n",
      "Iteration 247, loss = 0.42272244\n",
      "Iteration 236, loss = 0.39184080\n",
      "Iteration 454, loss = 0.34869741\n",
      "Iteration 315, loss = 0.36766182\n",
      "Iteration 5, loss = 0.80284820\n",
      "Iteration 248, loss = 0.42223523\n",
      "Iteration 237, loss = 0.39162138\n",
      "Iteration 455, loss = 0.34858399\n",
      "Iteration 249, loss = 0.42178508\n",
      "Iteration 316, loss = 0.36750548\n",
      "Iteration 66, loss = 0.62213663\n",
      "Iteration 238, loss = 0.39136318\n",
      "Iteration 456, loss = 0.34847613\n",
      "Iteration 250, loss = 0.42127707\n",
      "Iteration 103, loss = 0.50644336\n",
      "Iteration 457, loss = 0.34837610\n",
      "Iteration 317, loss = 0.36736540\n",
      "Iteration 458, loss = 0.34827819\n",
      "Iteration 251, loss = 0.42083531\n",
      "Iteration 6, loss = 0.80137735\n",
      "Iteration 67, loss = 0.61936301\n",
      "Iteration 459, loss = 0.34813859\n",
      "Iteration 104, loss = 0.50484951\n",
      "Iteration 318, loss = 0.36722110\n",
      "Iteration 239, loss = 0.39113288\n",
      "Iteration 252, loss = 0.42032432\n",
      "Iteration 460, loss = 0.34805762\n",
      "Iteration 605, loss = 0.33195508\n",
      "Iteration 253, loss = 0.41987347\n",
      "Iteration 319, loss = 0.36707379\n",
      "Iteration 68, loss = 0.61669090\n",
      "Iteration 240, loss = 0.39091047\n",
      "Iteration 461, loss = 0.34793407\n",
      "Iteration 254, loss = 0.41940873\n",
      "Iteration 255, loss = 0.41892683\n",
      "Iteration 7, loss = 0.79985859\n",
      "Iteration 105, loss = 0.50337827\n",
      "Iteration 320, loss = 0.36695806\n",
      "Iteration 256, loss = 0.41846840\n",
      "Iteration 462, loss = 0.34783244\n",
      "Iteration 321, loss = 0.36679556\n",
      "Iteration 69, loss = 0.61396943\n",
      "Iteration 257, loss = 0.41799949\n",
      "Iteration 322, loss = 0.36664870\n",
      "Iteration 241, loss = 0.39066202\n",
      "Iteration 463, loss = 0.34773377\n",
      "Iteration 464, loss = 0.34761716\n",
      "Iteration 258, loss = 0.41754034\n",
      "Iteration 465, loss = 0.34750518\n",
      "Iteration 70, loss = 0.61127518\n",
      "Iteration 323, loss = 0.36649388\n",
      "Iteration 259, loss = 0.41710657\n",
      "Iteration 8, loss = 0.79828200\n",
      "Iteration 466, loss = 0.34740599\n",
      "Iteration 260, loss = 0.41665786\n",
      "Iteration 242, loss = 0.39044095\n",
      "Iteration 106, loss = 0.50180300\n",
      "Iteration 324, loss = 0.36638199\n",
      "Iteration 467, loss = 0.34729813\n",
      "Iteration 71, loss = 0.60843256\n",
      "Iteration 261, loss = 0.41619860\n",
      "Iteration 468, loss = 0.34718683\n",
      "Iteration 243, loss = 0.39020383\n",
      "Iteration 469, loss = 0.34708886\n",
      "Iteration 470, loss = 0.34699029\n",
      "Iteration 244, loss = 0.39002403\n",
      "Iteration 262, loss = 0.41574970\n",
      "Iteration 471, loss = 0.34686940\n",
      "Iteration 9, loss = 0.79674280\n",
      "Iteration 263, loss = 0.41529401\n",
      "Iteration 72, loss = 0.60584429\n",
      "Iteration 472, loss = 0.34678514\n",
      "Iteration 325, loss = 0.36621457\n",
      "Iteration 606, loss = 0.33187690\n",
      "Iteration 473, loss = 0.34667338\n",
      "Iteration 245, loss = 0.38974126\n",
      "Iteration 264, loss = 0.41490334\n",
      "Iteration 607, loss = 0.33174699\n",
      "Iteration 107, loss = 0.50029322\n",
      "Iteration 608, loss = 0.33163520\n",
      "Iteration 326, loss = 0.36607312\n",
      "Iteration 246, loss = 0.38953325\n",
      "Iteration 265, loss = 0.41445636\n",
      "Iteration 266, loss = 0.41400945\n",
      "Iteration 327, loss = 0.36592078\n",
      "Iteration 10, loss = 0.79506584\n",
      "Iteration 474, loss = 0.34657512\n",
      "Iteration 475, loss = 0.34647306\n",
      "Iteration 267, loss = 0.41352849\n",
      "Iteration 609, loss = 0.33151192\n",
      "Iteration 73, loss = 0.60299955\n",
      "Iteration 268, loss = 0.41311150\n",
      "Iteration 328, loss = 0.36578417\n",
      "Iteration 476, loss = 0.34637315\n",
      "Iteration 610, loss = 0.33140781\n",
      "Iteration 11, loss = 0.79348300\n",
      "Iteration 247, loss = 0.38933675\n",
      "Iteration 108, loss = 0.49880282\n",
      "Iteration 611, loss = 0.33144500\n",
      "Iteration 612, loss = 0.33116678\n",
      "Iteration 248, loss = 0.38907243\n",
      "Iteration 249, loss = 0.38887423\n",
      "Iteration 613, loss = 0.33106355\n",
      "Iteration 109, loss = 0.49748722\n",
      "Iteration 269, loss = 0.41266888\n",
      "Iteration 12, loss = 0.79179435\n",
      "Iteration 250, loss = 0.38865634\n",
      "Iteration 74, loss = 0.60028474\n",
      "Iteration 614, loss = 0.33093035\n",
      "Iteration 477, loss = 0.34627110\n",
      "Iteration 329, loss = 0.36564830\n",
      "Iteration 270, loss = 0.41220459\n",
      "Iteration 615, loss = 0.33081617\n",
      "Iteration 110, loss = 0.49607550\n",
      "Iteration 616, loss = 0.33071048\n",
      "Iteration 617, loss = 0.33063559\n",
      "Iteration 271, loss = 0.41178120\n",
      "Iteration 478, loss = 0.34617634\n",
      "Iteration 272, loss = 0.41135650\n",
      "Iteration 479, loss = 0.34606610\n",
      "Iteration 618, loss = 0.33052303\n",
      "Iteration 619, loss = 0.33036688\n",
      "Iteration 251, loss = 0.38843744\n",
      "Iteration 75, loss = 0.59754379\n",
      "Iteration 111, loss = 0.49473811\n",
      "Iteration 252, loss = 0.38819854\n",
      "Iteration 620, loss = 0.33024858\n",
      "Iteration 253, loss = 0.38799852\n",
      "Iteration 112, loss = 0.49338287\n",
      "Iteration 621, loss = 0.33014192\n",
      "Iteration 254, loss = 0.38775879\n",
      "Iteration 273, loss = 0.41089792\n",
      "Iteration 480, loss = 0.34596004\n",
      "Iteration 330, loss = 0.36549790\n",
      "Iteration 76, loss = 0.59491612\n",
      "Iteration 622, loss = 0.33006290\n",
      "Iteration 274, loss = 0.41051613\n",
      "Iteration 481, loss = 0.34587061\n",
      "Iteration 623, loss = 0.32992530\n",
      "Iteration 275, loss = 0.41004850\n",
      "Iteration 482, loss = 0.34576961\n",
      "Iteration 331, loss = 0.36536408\n",
      "Iteration 276, loss = 0.40961479\n",
      "Iteration 255, loss = 0.38756738\n",
      "Iteration 624, loss = 0.32980458\n",
      "Iteration 625, loss = 0.32970209\n",
      "Iteration 483, loss = 0.34567010\n",
      "Iteration 626, loss = 0.32959095\n",
      "Iteration 332, loss = 0.36523163\n",
      "Iteration 627, loss = 0.32947746\n",
      "Iteration 333, loss = 0.36507640\n",
      "Iteration 484, loss = 0.34555068\n",
      "Iteration 277, loss = 0.40917353\n",
      "Iteration 113, loss = 0.49204653\n",
      "Iteration 485, loss = 0.34547182\n",
      "Iteration 278, loss = 0.40877664\n",
      "Iteration 486, loss = 0.34535253\n",
      "Iteration 334, loss = 0.36494662\n",
      "Iteration 256, loss = 0.38733244\n",
      "Iteration 279, loss = 0.40832502\n",
      "Iteration 487, loss = 0.34525734\n",
      "Iteration 114, loss = 0.49078725\n",
      "Iteration 335, loss = 0.36480966\n",
      "Iteration 13, loss = 0.79015915\n",
      "Iteration 628, loss = 0.32935701\n",
      "Iteration 280, loss = 0.40790428\n",
      "Iteration 629, loss = 0.32924012\n",
      "Iteration 257, loss = 0.38715150\n",
      "Iteration 281, loss = 0.40745149\n",
      "Iteration 630, loss = 0.32912818\n",
      "Iteration 282, loss = 0.40703811\n",
      "Iteration 631, loss = 0.32902283\n",
      "Iteration 488, loss = 0.34517679\n",
      "Iteration 336, loss = 0.36471440\n",
      "Iteration 283, loss = 0.40661622\n",
      "Iteration 632, loss = 0.32891362\n",
      "Iteration 258, loss = 0.38693832\n",
      "Iteration 489, loss = 0.34504620\n",
      "Iteration 284, loss = 0.40621250\n",
      "Iteration 77, loss = 0.59211266\n",
      "Iteration 285, loss = 0.40579964\n",
      "Iteration 633, loss = 0.32879779\n",
      "Iteration 286, loss = 0.40541137\n",
      "Iteration 634, loss = 0.32869323\n",
      "Iteration 287, loss = 0.40496100\n",
      "Iteration 490, loss = 0.34496765\n",
      "Iteration 14, loss = 0.78852636\n",
      "Iteration 337, loss = 0.36453295\n",
      "Iteration 635, loss = 0.32856670\n",
      "Iteration 259, loss = 0.38671048\n",
      "Iteration 491, loss = 0.34486691\n",
      "Iteration 636, loss = 0.32847273\n",
      "Iteration 78, loss = 0.58940729\n",
      "Iteration 637, loss = 0.32835512\n",
      "Iteration 492, loss = 0.34475319\n",
      "Iteration 638, loss = 0.32823848\n",
      "Iteration 493, loss = 0.34468677\n",
      "Iteration 639, loss = 0.32820015\n",
      "Iteration 115, loss = 0.48952836\n",
      "Iteration 288, loss = 0.40455896\n",
      "Iteration 494, loss = 0.34457228\n",
      "Iteration 495, loss = 0.34446083\n",
      "Iteration 289, loss = 0.40415921\n",
      "Iteration 640, loss = 0.32801512\n",
      "Iteration 641, loss = 0.32790827\n",
      "Iteration 290, loss = 0.40374347\n",
      "Iteration 338, loss = 0.36439831\n",
      "Iteration 260, loss = 0.38648992\n",
      "Iteration 642, loss = 0.32783946\n",
      "Iteration 79, loss = 0.58659734\n",
      "Iteration 643, loss = 0.32772900\n",
      "Iteration 496, loss = 0.34436234\n",
      "Iteration 644, loss = 0.32759747\n",
      "Iteration 645, loss = 0.32744832\n",
      "Iteration 339, loss = 0.36428590\n",
      "Iteration 497, loss = 0.34428747\n",
      "Iteration 15, loss = 0.78697851\n",
      "Iteration 291, loss = 0.40335002\n",
      "Iteration 646, loss = 0.32739734\n",
      "Iteration 261, loss = 0.38629297\n",
      "Iteration 292, loss = 0.40294732\n",
      "Iteration 80, loss = 0.58401227\n",
      "Iteration 498, loss = 0.34417451\n",
      "Iteration 340, loss = 0.36413599\n",
      "Iteration 499, loss = 0.34407459\n",
      "Iteration 647, loss = 0.32729556\n",
      "Iteration 116, loss = 0.48823618\n",
      "Iteration 81, loss = 0.58139381\n",
      "Iteration 341, loss = 0.36398086\n",
      "Iteration 500, loss = 0.34400891\n",
      "Iteration 648, loss = 0.32713841\n",
      "Iteration 293, loss = 0.40254790\n",
      "Iteration 501, loss = 0.34388658\n",
      "Iteration 342, loss = 0.36384667\n",
      "Iteration 262, loss = 0.38608860\n",
      "Iteration 502, loss = 0.34383185\n",
      "Iteration 343, loss = 0.36374110\n",
      "Iteration 82, loss = 0.57877133\n",
      "Iteration 649, loss = 0.32708509\n",
      "Iteration 503, loss = 0.34374057\n",
      "Iteration 294, loss = 0.40215098\n",
      "Iteration 650, loss = 0.32693986\n",
      "Iteration 263, loss = 0.38588295\n",
      "Iteration 651, loss = 0.32679579\n",
      "Iteration 504, loss = 0.34359523\n",
      "Iteration 295, loss = 0.40175917\n",
      "Iteration 344, loss = 0.36358481\n",
      "Iteration 652, loss = 0.32676331\n",
      "Iteration 117, loss = 0.48709409\n",
      "Iteration 296, loss = 0.40136222\n",
      "Iteration 505, loss = 0.34350547\n",
      "Iteration 264, loss = 0.38567860\n",
      "Iteration 345, loss = 0.36343789\n",
      "Iteration 653, loss = 0.32657507\n",
      "Iteration 83, loss = 0.57610507\n",
      "Iteration 297, loss = 0.40099375\n",
      "Iteration 506, loss = 0.34341139\n",
      "Iteration 654, loss = 0.32648454\n",
      "Iteration 298, loss = 0.40064719\n",
      "Iteration 118, loss = 0.48589846\n",
      "Iteration 346, loss = 0.36331124\n",
      "Iteration 16, loss = 0.78530688\n",
      "Iteration 265, loss = 0.38549897\n",
      "Iteration 299, loss = 0.40022064\n",
      "Iteration 655, loss = 0.32634698\n",
      "Iteration 347, loss = 0.36316905\n",
      "Iteration 507, loss = 0.34330474\n",
      "Iteration 300, loss = 0.39981693\n",
      "Iteration 84, loss = 0.57350716\n",
      "Iteration 656, loss = 0.32623856\n",
      "Iteration 301, loss = 0.39944734\n",
      "Iteration 119, loss = 0.48470677\n",
      "Iteration 266, loss = 0.38527440\n",
      "Iteration 657, loss = 0.32614757\n",
      "Iteration 508, loss = 0.34325613\n",
      "Iteration 302, loss = 0.39907706\n",
      "Iteration 348, loss = 0.36303673\n",
      "Iteration 85, loss = 0.57087789\n",
      "Iteration 267, loss = 0.38509586\n",
      "Iteration 509, loss = 0.34312924\n",
      "Iteration 658, loss = 0.32606441\n",
      "Iteration 510, loss = 0.34303082\n",
      "Iteration 303, loss = 0.39872515\n",
      "Iteration 268, loss = 0.38487697\n",
      "Iteration 659, loss = 0.32591316\n",
      "Iteration 17, loss = 0.78370797\n",
      "Iteration 349, loss = 0.36292188\n",
      "Iteration 86, loss = 0.56835111\n",
      "Iteration 511, loss = 0.34293088\n",
      "Iteration 120, loss = 0.48362914\n",
      "Iteration 660, loss = 0.32579536\n",
      "Iteration 304, loss = 0.39832586\n",
      "Iteration 350, loss = 0.36277067\n",
      "Iteration 512, loss = 0.34282058\n",
      "Iteration 305, loss = 0.39793040\n",
      "Iteration 661, loss = 0.32568086\n",
      "Iteration 269, loss = 0.38465893\n",
      "Iteration 351, loss = 0.36264657\n",
      "Iteration 18, loss = 0.78209723\n",
      "Iteration 662, loss = 0.32559568\n",
      "Iteration 306, loss = 0.39759525\n",
      "Iteration 513, loss = 0.34273953\n",
      "Iteration 352, loss = 0.36250772\n",
      "Iteration 663, loss = 0.32548560\n",
      "Iteration 87, loss = 0.56571169\n",
      "Iteration 270, loss = 0.38451922\n",
      "Iteration 664, loss = 0.32537648\n",
      "Iteration 121, loss = 0.48251816\n",
      "Iteration 353, loss = 0.36240115\n",
      "Iteration 307, loss = 0.39720078\n",
      "Iteration 271, loss = 0.38429382\n",
      "Iteration 514, loss = 0.34263259\n",
      "Iteration 354, loss = 0.36223497\n",
      "Iteration 665, loss = 0.32525532\n",
      "Iteration 308, loss = 0.39685043\n",
      "Iteration 666, loss = 0.32513315\n",
      "Iteration 272, loss = 0.38409056\n",
      "Iteration 88, loss = 0.56316663\n",
      "Iteration 309, loss = 0.39644967\n",
      "Iteration 515, loss = 0.34255423\n",
      "Iteration 667, loss = 0.32504613\n",
      "Iteration 355, loss = 0.36210313\n",
      "Iteration 516, loss = 0.34244868\n",
      "Iteration 273, loss = 0.38386640\n",
      "Iteration 668, loss = 0.32497237\n",
      "Iteration 517, loss = 0.34235334\n",
      "Iteration 274, loss = 0.38367721\n",
      "Iteration 19, loss = 0.78051391\n",
      "Iteration 310, loss = 0.39608220\n",
      "Iteration 122, loss = 0.48138942\n",
      "Iteration 89, loss = 0.56076819\n",
      "Iteration 356, loss = 0.36196891\n",
      "Iteration 275, loss = 0.38353994\n",
      "Iteration 518, loss = 0.34226438\n",
      "Iteration 311, loss = 0.39572615\n",
      "Iteration 519, loss = 0.34215976\n",
      "Iteration 357, loss = 0.36184905\n",
      "Iteration 669, loss = 0.32482423\n",
      "Iteration 520, loss = 0.34208641\n",
      "Iteration 276, loss = 0.38337829\n",
      "Iteration 312, loss = 0.39534581\n",
      "Iteration 358, loss = 0.36171526\n",
      "Iteration 521, loss = 0.34198660\n",
      "Iteration 90, loss = 0.55828052\n",
      "Iteration 670, loss = 0.32473704\n",
      "Iteration 359, loss = 0.36157919\n",
      "Iteration 313, loss = 0.39498563\n",
      "Iteration 522, loss = 0.34187417\n",
      "Iteration 123, loss = 0.48033099\n",
      "Iteration 277, loss = 0.38311129\n",
      "Iteration 671, loss = 0.32461085\n",
      "Iteration 360, loss = 0.36144170\n",
      "Iteration 314, loss = 0.39464922\n",
      "Iteration 91, loss = 0.55580192\n",
      "Iteration 20, loss = 0.77891339\n",
      "Iteration 523, loss = 0.34183739\n",
      "Iteration 672, loss = 0.32450485\n",
      "Iteration 361, loss = 0.36132833\n",
      "Iteration 315, loss = 0.39429011\n",
      "Iteration 278, loss = 0.38295411\n",
      "Iteration 92, loss = 0.55328310\n",
      "Iteration 673, loss = 0.32435977\n",
      "Iteration 674, loss = 0.32426425\n",
      "Iteration 524, loss = 0.34168385\n",
      "Iteration 124, loss = 0.47923318\n",
      "Iteration 675, loss = 0.32418289\n",
      "Iteration 362, loss = 0.36120755\n",
      "Iteration 316, loss = 0.39393741\n",
      "Iteration 525, loss = 0.34159766\n",
      "Iteration 676, loss = 0.32409444\n",
      "Iteration 677, loss = 0.32393413\n",
      "Iteration 317, loss = 0.39356787\n",
      "Iteration 279, loss = 0.38273001\n",
      "Iteration 363, loss = 0.36109001\n",
      "Iteration 678, loss = 0.32384702\n",
      "Iteration 526, loss = 0.34150804\n",
      "Iteration 21, loss = 0.77727692\n",
      "Iteration 125, loss = 0.47826116\n",
      "Iteration 93, loss = 0.55101398\n",
      "Iteration 318, loss = 0.39319239\n",
      "Iteration 679, loss = 0.32372573\n",
      "Iteration 364, loss = 0.36094266\n",
      "Iteration 319, loss = 0.39285572\n",
      "Iteration 527, loss = 0.34142420\n",
      "Iteration 680, loss = 0.32366647\n",
      "Iteration 320, loss = 0.39253163\n",
      "Iteration 280, loss = 0.38251702\n",
      "Iteration 528, loss = 0.34132984\n",
      "Iteration 321, loss = 0.39217886\n",
      "Iteration 365, loss = 0.36083323\n",
      "Iteration 529, loss = 0.34121498\n",
      "Iteration 22, loss = 0.77571727\n",
      "Iteration 126, loss = 0.47718921\n",
      "Iteration 681, loss = 0.32347911\n",
      "Iteration 322, loss = 0.39182173\n",
      "Iteration 323, loss = 0.39147097\n",
      "Iteration 530, loss = 0.34111863\n",
      "Iteration 281, loss = 0.38234865Iteration 682, loss = 0.32339601\n",
      "\n",
      "Iteration 366, loss = 0.36067117\n",
      "Iteration 683, loss = 0.32327358\n",
      "Iteration 324, loss = 0.39116921\n",
      "Iteration 282, loss = 0.38216449\n",
      "Iteration 94, loss = 0.54856913\n",
      "Iteration 531, loss = 0.34104239\n",
      "Iteration 367, loss = 0.36055327Iteration 325, loss = 0.39080146\n",
      "\n",
      "Iteration 532, loss = 0.34092723\n",
      "Iteration 684, loss = 0.32317403\n",
      "Iteration 368, loss = 0.36042380\n",
      "Iteration 23, loss = 0.77408625\n",
      "Iteration 127, loss = 0.47618722\n",
      "Iteration 326, loss = 0.39047744\n",
      "Iteration 533, loss = 0.34086669\n",
      "Iteration 685, loss = 0.32306983\n",
      "Iteration 283, loss = 0.38198793\n",
      "Iteration 369, loss = 0.36030489\n",
      "Iteration 327, loss = 0.39011522\n",
      "Iteration 686, loss = 0.32296032\n",
      "Iteration 534, loss = 0.34075722\n",
      "Iteration 370, loss = 0.36018588\n",
      "Iteration 328, loss = 0.38977579\n",
      "Iteration 95, loss = 0.54620493\n",
      "Iteration 687, loss = 0.32283992\n",
      "Iteration 329, loss = 0.38942761\n",
      "Iteration 371, loss = 0.36004492\n",
      "Iteration 688, loss = 0.32273557\n",
      "Iteration 128, loss = 0.47523438\n",
      "Iteration 535, loss = 0.34068779\n",
      "Iteration 284, loss = 0.38182075\n",
      "Iteration 330, loss = 0.38912815\n",
      "Iteration 689, loss = 0.32262739\n",
      "Iteration 372, loss = 0.35991588\n",
      "Iteration 690, loss = 0.32251915\n",
      "Iteration 373, loss = 0.35980967\n",
      "Iteration 691, loss = 0.32242398\n",
      "Iteration 331, loss = 0.38876307\n",
      "Iteration 692, loss = 0.32230024\n",
      "Iteration 536, loss = 0.34056585\n",
      "Iteration 374, loss = 0.35967696\n",
      "Iteration 96, loss = 0.54391859\n",
      "Iteration 332, loss = 0.38845688\n",
      "Iteration 537, loss = 0.34046888\n",
      "Iteration 693, loss = 0.32219158\n",
      "Iteration 375, loss = 0.35956119\n",
      "Iteration 333, loss = 0.38808765\n",
      "Iteration 285, loss = 0.38161914\n",
      "Iteration 538, loss = 0.34037961\n",
      "Iteration 334, loss = 0.38776839\n",
      "Iteration 24, loss = 0.77245057\n",
      "Iteration 694, loss = 0.32207472\n",
      "Iteration 376, loss = 0.35942907\n",
      "Iteration 129, loss = 0.47428663\n",
      "Iteration 335, loss = 0.38745128\n",
      "Iteration 97, loss = 0.54156902\n",
      "Iteration 286, loss = 0.38143847\n",
      "Iteration 377, loss = 0.35929183\n",
      "Iteration 539, loss = 0.34029720\n",
      "Iteration 695, loss = 0.32197939\n",
      "Iteration 336, loss = 0.38708989\n",
      "Iteration 378, loss = 0.35918897\n",
      "Iteration 696, loss = 0.32188553\n",
      "Iteration 287, loss = 0.38123334\n",
      "Iteration 337, loss = 0.38676346\n",
      "Iteration 540, loss = 0.34020764\n",
      "Iteration 379, loss = 0.35906670\n",
      "Iteration 98, loss = 0.53940409\n",
      "Iteration 130, loss = 0.47333658\n",
      "Iteration 288, loss = 0.38107746\n",
      "Iteration 697, loss = 0.32176054\n",
      "Iteration 338, loss = 0.38643608\n",
      "Iteration 698, loss = 0.32167081\n",
      "Iteration 339, loss = 0.38610467\n",
      "Iteration 541, loss = 0.34011912\n",
      "Iteration 289, loss = 0.38093654\n",
      "Iteration 699, loss = 0.32159813\n",
      "Iteration 99, loss = 0.53717202\n",
      "Iteration 340, loss = 0.38577846\n",
      "Iteration 25, loss = 0.77088667\n",
      "Iteration 542, loss = 0.34000443\n",
      "Iteration 131, loss = 0.47236422\n",
      "Iteration 341, loss = 0.38544251\n",
      "Iteration 380, loss = 0.35892516\n",
      "Iteration 342, loss = 0.38511071\n",
      "Iteration 100, loss = 0.53493805\n",
      "Iteration 381, loss = 0.35879816\n",
      "Iteration 343, loss = 0.38480893\n",
      "Iteration 290, loss = 0.38077646\n",
      "Iteration 543, loss = 0.33992458\n",
      "Iteration 700, loss = 0.32144125\n",
      "Iteration 344, loss = 0.38448412\n",
      "Iteration 382, loss = 0.35869499\n",
      "Iteration 383, loss = 0.35856050\n",
      "Iteration 701, loss = 0.32134516\n",
      "Iteration 101, loss = 0.53279601\n",
      "Iteration 291, loss = 0.38052269\n",
      "Iteration 345, loss = 0.38416338\n",
      "Iteration 702, loss = 0.32124674\n",
      "Iteration 544, loss = 0.33984317\n",
      "Iteration 346, loss = 0.38382927\n",
      "Iteration 703, loss = 0.32115268\n",
      "Iteration 26, loss = 0.76928862\n",
      "Iteration 347, loss = 0.38351418\n",
      "Iteration 704, loss = 0.32103669\n",
      "Iteration 348, loss = 0.38327475\n",
      "Iteration 545, loss = 0.33974152\n",
      "Iteration 132, loss = 0.47149735\n",
      "Iteration 705, loss = 0.32092150\n",
      "Iteration 349, loss = 0.38286752\n",
      "Iteration 384, loss = 0.35843671\n",
      "Iteration 350, loss = 0.38265477\n",
      "Iteration 292, loss = 0.38035010\n",
      "Iteration 706, loss = 0.32081531\n",
      "Iteration 546, loss = 0.33964624\n",
      "Iteration 102, loss = 0.53064223\n",
      "Iteration 385, loss = 0.35830367\n",
      "Iteration 547, loss = 0.33959717\n",
      "Iteration 133, loss = 0.47055554\n",
      "Iteration 548, loss = 0.33947221\n",
      "Iteration 386, loss = 0.35819532\n",
      "Iteration 707, loss = 0.32071865\n",
      "Iteration 549, loss = 0.33937752\n",
      "Iteration 293, loss = 0.38019513\n",
      "Iteration 550, loss = 0.33927950\n",
      "Iteration 708, loss = 0.32061135\n",
      "Iteration 351, loss = 0.38225852\n",
      "Iteration 103, loss = 0.52857951\n",
      "Iteration 134, loss = 0.46966201\n",
      "Iteration 27, loss = 0.76753964\n",
      "Iteration 709, loss = 0.32048873\n",
      "Iteration 294, loss = 0.38005677\n",
      "Iteration 352, loss = 0.38198977\n",
      "Iteration 387, loss = 0.35806590\n",
      "Iteration 104, loss = 0.52658046\n",
      "Iteration 710, loss = 0.32038154\n",
      "Iteration 551, loss = 0.33921141\n",
      "Iteration 295, loss = 0.37982193\n",
      "Iteration 552, loss = 0.33909686\n",
      "Iteration 388, loss = 0.35796214\n",
      "Iteration 711, loss = 0.32026942\n",
      "Iteration 353, loss = 0.38163904\n",
      "Iteration 553, loss = 0.33900064\n",
      "Iteration 105, loss = 0.52456407\n",
      "Iteration 712, loss = 0.32019119\n",
      "Iteration 554, loss = 0.33891507\n",
      "Iteration 135, loss = 0.46880872\n",
      "Iteration 354, loss = 0.38133317\n",
      "Iteration 555, loss = 0.33882013\n",
      "Iteration 713, loss = 0.32007877\n",
      "Iteration 389, loss = 0.35785023\n",
      "Iteration 28, loss = 0.76589982\n",
      "Iteration 106, loss = 0.52257393\n",
      "Iteration 296, loss = 0.37968530\n",
      "Iteration 556, loss = 0.33872869\n",
      "Iteration 714, loss = 0.31997908\n",
      "Iteration 390, loss = 0.35771376\n",
      "Iteration 715, loss = 0.31984706\n",
      "Iteration 355, loss = 0.38100706\n",
      "Iteration 716, loss = 0.31975742\n",
      "Iteration 297, loss = 0.37956585\n",
      "Iteration 391, loss = 0.35760798\n",
      "Iteration 356, loss = 0.38071434\n",
      "Iteration 557, loss = 0.33864029\n",
      "Iteration 107, loss = 0.52062175\n",
      "Iteration 357, loss = 0.38050756\n",
      "Iteration 392, loss = 0.35749494\n",
      "Iteration 558, loss = 0.33855794\n",
      "Iteration 717, loss = 0.31963727\n",
      "Iteration 298, loss = 0.37931102\n",
      "Iteration 136, loss = 0.46793985\n",
      "Iteration 108, loss = 0.51859976\n",
      "Iteration 358, loss = 0.38012031\n",
      "Iteration 559, loss = 0.33845820\n",
      "Iteration 393, loss = 0.35735254\n",
      "Iteration 29, loss = 0.76424672\n",
      "Iteration 299, loss = 0.37914895\n",
      "Iteration 560, loss = 0.33836510\n",
      "Iteration 718, loss = 0.31956472\n",
      "Iteration 359, loss = 0.37977354\n",
      "Iteration 360, loss = 0.37948089\n",
      "Iteration 561, loss = 0.33827583\n",
      "Iteration 394, loss = 0.35724075\n",
      "Iteration 137, loss = 0.46713483\n",
      "Iteration 361, loss = 0.37919244\n",
      "Iteration 719, loss = 0.31941496\n",
      "Iteration 109, loss = 0.51679786\n",
      "Iteration 362, loss = 0.37888393\n",
      "Iteration 562, loss = 0.33819615\n",
      "Iteration 720, loss = 0.31934214\n",
      "Iteration 363, loss = 0.37861173\n",
      "Iteration 300, loss = 0.37897786\n",
      "Iteration 395, loss = 0.35711080\n",
      "Iteration 30, loss = 0.76250155\n",
      "Iteration 563, loss = 0.33810208\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 364, loss = 0.37825815\n",
      "Iteration 721, loss = 0.31923418\n",
      "Iteration 722, loss = 0.31915613\n",
      "Iteration 396, loss = 0.35700926\n",
      "Iteration 138, loss = 0.46631331\n",
      "Iteration 365, loss = 0.37796532\n",
      "Iteration 397, loss = 0.35688303\n",
      "Iteration 110, loss = 0.51492932\n",
      "Iteration 366, loss = 0.37770047\n",
      "Iteration 301, loss = 0.37881048\n",
      "Iteration 723, loss = 0.31899200\n",
      "Iteration 724, loss = 0.31889901\n",
      "Iteration 398, loss = 0.35677165\n",
      "Iteration 367, loss = 0.37736740\n",
      "Iteration 139, loss = 0.46545682\n",
      "Iteration 399, loss = 0.35666296\n",
      "Iteration 302, loss = 0.37863697\n",
      "Iteration 31, loss = 0.76079647\n",
      "Iteration 368, loss = 0.37709329\n",
      "Iteration 111, loss = 0.51304839\n",
      "Iteration 725, loss = 0.31880607\n",
      "Iteration 369, loss = 0.37680148\n",
      "Iteration 726, loss = 0.31869713\n",
      "Iteration 400, loss = 0.35652413\n",
      "Iteration 727, loss = 0.31856595\n",
      "Iteration 303, loss = 0.37849430\n",
      "Iteration 140, loss = 0.46472552\n",
      "Iteration 370, loss = 0.37650954\n",
      "Iteration 728, loss = 0.31849337\n",
      "Iteration 112, loss = 0.51123178\n",
      "Iteration 729, loss = 0.31835052\n",
      "Iteration 1, loss = 0.77207392\n",
      "Iteration 401, loss = 0.35642355\n",
      "Iteration 2, loss = 0.76769166\n",
      "Iteration 371, loss = 0.37620448\n",
      "Iteration 3, loss = 0.76074015\n",
      "Iteration 4, loss = 0.75244081\n",
      "Iteration 5, loss = 0.74368172\n",
      "Iteration 6, loss = 0.73424064\n",
      "Iteration 7, loss = 0.72413601\n",
      "Iteration 372, loss = 0.37591990\n",
      "Iteration 730, loss = 0.31825302\n",
      "Iteration 8, loss = 0.71448460\n",
      "Iteration 9, loss = 0.70504569\n",
      "Iteration 10, loss = 0.69567365\n",
      "Iteration 11, loss = 0.68673916\n",
      "Iteration 373, loss = 0.37560532\n",
      "Iteration 12, loss = 0.67832684\n",
      "Iteration 13, loss = 0.67001522\n",
      "Iteration 14, loss = 0.66244890\n",
      "Iteration 731, loss = 0.31816840\n",
      "Iteration 15, loss = 0.65490423\n",
      "Iteration 374, loss = 0.37535285\n",
      "Iteration 16, loss = 0.64782111\n",
      "Iteration 304, loss = 0.37831539\n",
      "Iteration 17, loss = 0.64111170\n",
      "Iteration 141, loss = 0.46385184\n",
      "Iteration 18, loss = 0.63445551\n",
      "Iteration 19, loss = 0.62823065\n",
      "Iteration 375, loss = 0.37505904\n",
      "Iteration 20, loss = 0.62231040\n",
      "Iteration 21, loss = 0.61663233\n",
      "Iteration 732, loss = 0.31804136\n",
      "Iteration 22, loss = 0.61118648\n",
      "Iteration 23, loss = 0.60599935\n",
      "Iteration 32, loss = 0.75902618\n",
      "Iteration 113, loss = 0.50947576\n",
      "Iteration 733, loss = 0.31795690\n",
      "Iteration 734, loss = 0.31786183\n",
      "Iteration 376, loss = 0.37490742\n",
      "Iteration 24, loss = 0.60087013\n",
      "Iteration 735, loss = 0.31772615\n",
      "Iteration 25, loss = 0.59597801\n",
      "Iteration 26, loss = 0.59137802\n",
      "Iteration 142, loss = 0.46313455\n",
      "Iteration 736, loss = 0.31762490\n",
      "Iteration 27, loss = 0.58700623\n",
      "Iteration 402, loss = 0.35631559\n",
      "Iteration 28, loss = 0.58249718\n",
      "Iteration 29, loss = 0.57839649\n",
      "Iteration 30, loss = 0.57436072\n",
      "Iteration 31, loss = 0.57043638\n",
      "Iteration 377, loss = 0.37457262\n",
      "Iteration 32, loss = 0.56665387\n",
      "Iteration 737, loss = 0.31751840\n",
      "Iteration 33, loss = 0.56293521\n",
      "Iteration 403, loss = 0.35618902\n",
      "Iteration 34, loss = 0.55937873\n",
      "Iteration 35, loss = 0.55591762\n",
      "Iteration 36, loss = 0.55261214\n",
      "Iteration 37, loss = 0.54937956\n",
      "Iteration 305, loss = 0.37814131\n",
      "Iteration 378, loss = 0.37430822\n",
      "Iteration 404, loss = 0.35607273\n",
      "Iteration 114, loss = 0.50774930\n",
      "Iteration 738, loss = 0.31742576\n",
      "Iteration 739, loss = 0.31731547\n",
      "Iteration 38, loss = 0.54616209\n",
      "Iteration 740, loss = 0.31721325\n",
      "Iteration 39, loss = 0.54313359\n",
      "Iteration 40, loss = 0.54014464\n",
      "Iteration 741, loss = 0.31711141\n",
      "Iteration 115, loss = 0.50613086\n",
      "Iteration 41, loss = 0.53720478\n",
      "Iteration 742, loss = 0.31699632\n",
      "Iteration 143, loss = 0.46233356\n",
      "Iteration 42, loss = 0.53436553\n",
      "Iteration 405, loss = 0.35601376\n",
      "Iteration 743, loss = 0.31690758\n",
      "Iteration 43, loss = 0.53158085\n",
      "Iteration 44, loss = 0.52895240\n",
      "Iteration 744, loss = 0.31680761\n",
      "Iteration 379, loss = 0.37396468\n",
      "Iteration 116, loss = 0.50433018\n",
      "Iteration 45, loss = 0.52629572\n",
      "Iteration 46, loss = 0.52375375\n",
      "Iteration 33, loss = 0.75729734\n",
      "Iteration 380, loss = 0.37369778\n",
      "Iteration 745, loss = 0.31668104\n",
      "Iteration 306, loss = 0.37796950\n",
      "Iteration 381, loss = 0.37344504\n",
      "Iteration 117, loss = 0.50268738\n",
      "Iteration 746, loss = 0.31660910\n",
      "Iteration 307, loss = 0.37788817\n",
      "Iteration 382, loss = 0.37315637\n",
      "Iteration 47, loss = 0.52128586\n",
      "Iteration 747, loss = 0.31650309\n",
      "Iteration 406, loss = 0.35587453\n",
      "Iteration 48, loss = 0.51876338\n",
      "Iteration 748, loss = 0.31638353\n",
      "Iteration 49, loss = 0.51644562\n",
      "Iteration 34, loss = 0.75551177\n",
      "Iteration 118, loss = 0.50110734\n",
      "Iteration 308, loss = 0.37763325\n",
      "Iteration 50, loss = 0.51413501\n",
      "Iteration 144, loss = 0.46160826\n",
      "Iteration 749, loss = 0.31628505\n",
      "Iteration 309, loss = 0.37750344\n",
      "Iteration 383, loss = 0.37289685\n",
      "Iteration 407, loss = 0.35573440\n",
      "Iteration 51, loss = 0.51183013\n",
      "Iteration 750, loss = 0.31619091\n",
      "Iteration 52, loss = 0.50964262\n",
      "Iteration 751, loss = 0.31607559\n",
      "Iteration 384, loss = 0.37261148\n",
      "Iteration 53, loss = 0.50752796\n",
      "Iteration 54, loss = 0.50539790\n",
      "Iteration 752, loss = 0.31594854\n",
      "Iteration 310, loss = 0.37732498\n",
      "Iteration 55, loss = 0.50328140\n",
      "Iteration 56, loss = 0.50129865\n",
      "Iteration 408, loss = 0.35562385\n",
      "Iteration 57, loss = 0.49934347\n",
      "Iteration 58, loss = 0.49740899\n",
      "Iteration 385, loss = 0.37240680\n",
      "Iteration 59, loss = 0.49552014\n",
      "Iteration 60, loss = 0.49366661\n",
      "Iteration 61, loss = 0.49184293\n",
      "Iteration 62, loss = 0.49009421\n",
      "Iteration 753, loss = 0.31589669\n",
      "Iteration 63, loss = 0.48837434\n",
      "Iteration 64, loss = 0.48667072\n",
      "Iteration 65, loss = 0.48495833\n",
      "Iteration 66, loss = 0.48338779\n",
      "Iteration 311, loss = 0.37715966\n",
      "Iteration 386, loss = 0.37210224\n",
      "Iteration 67, loss = 0.48178647\n",
      "Iteration 68, loss = 0.48024247\n",
      "Iteration 754, loss = 0.31574350Iteration 409, loss = 0.35552088\n",
      "\n",
      "Iteration 145, loss = 0.46086209\n",
      "Iteration 69, loss = 0.47867656\n",
      "Iteration 70, loss = 0.47718508\n",
      "Iteration 71, loss = 0.47569118\n",
      "Iteration 119, loss = 0.49938054\n",
      "Iteration 72, loss = 0.47420933\n",
      "Iteration 73, loss = 0.47283110\n",
      "Iteration 74, loss = 0.47141021\n",
      "Iteration 387, loss = 0.37180267\n",
      "Iteration 755, loss = 0.31562926\n",
      "Iteration 75, loss = 0.47002771\n",
      "Iteration 76, loss = 0.46868388\n",
      "Iteration 410, loss = 0.35539304\n",
      "Iteration 77, loss = 0.46738659\n",
      "Iteration 756, loss = 0.31553920\n",
      "Iteration 78, loss = 0.46605514\n",
      "Iteration 388, loss = 0.37158660\n",
      "Iteration 312, loss = 0.37698787\n",
      "Iteration 79, loss = 0.46477586\n",
      "Iteration 411, loss = 0.35528015\n",
      "Iteration 35, loss = 0.75368886\n",
      "Iteration 80, loss = 0.46352858\n",
      "Iteration 146, loss = 0.46011725\n",
      "Iteration 81, loss = 0.46230489\n",
      "Iteration 757, loss = 0.31546712\n",
      "Iteration 82, loss = 0.46107983\n",
      "Iteration 412, loss = 0.35516506\n",
      "Iteration 389, loss = 0.37129567\n",
      "Iteration 120, loss = 0.49792848\n",
      "Iteration 83, loss = 0.45986965\n",
      "Iteration 758, loss = 0.31533490\n",
      "Iteration 84, loss = 0.45875304\n",
      "Iteration 85, loss = 0.45756673\n",
      "Iteration 413, loss = 0.35504642\n",
      "Iteration 86, loss = 0.45640949\n",
      "Iteration 87, loss = 0.45531833\n",
      "Iteration 88, loss = 0.45419193\n",
      "Iteration 759, loss = 0.31523159\n",
      "Iteration 390, loss = 0.37102457\n",
      "Iteration 89, loss = 0.45314034\n",
      "Iteration 760, loss = 0.31511965\n",
      "Iteration 313, loss = 0.37682752\n",
      "Iteration 90, loss = 0.45209153\n",
      "Iteration 36, loss = 0.75185125\n",
      "Iteration 761, loss = 0.31503060\n",
      "Iteration 91, loss = 0.45108876\n",
      "Iteration 92, loss = 0.45000865\n",
      "Iteration 414, loss = 0.35496121\n",
      "Iteration 93, loss = 0.44897638\n",
      "Iteration 147, loss = 0.45938007\n",
      "Iteration 94, loss = 0.44799842\n",
      "Iteration 391, loss = 0.37078653\n",
      "Iteration 95, loss = 0.44703808\n",
      "Iteration 96, loss = 0.44605655\n",
      "Iteration 121, loss = 0.49632591\n",
      "Iteration 762, loss = 0.31490393\n",
      "Iteration 97, loss = 0.44510411\n",
      "Iteration 98, loss = 0.44419259\n",
      "Iteration 99, loss = 0.44326204\n",
      "Iteration 415, loss = 0.35481946\n",
      "Iteration 100, loss = 0.44233373\n",
      "Iteration 101, loss = 0.44144623Iteration 314, loss = 0.37668701\n",
      "\n",
      "Iteration 763, loss = 0.31482926\n",
      "Iteration 392, loss = 0.37057152\n",
      "Iteration 102, loss = 0.44059514\n",
      "Iteration 148, loss = 0.45864918\n",
      "Iteration 416, loss = 0.35471669\n",
      "Iteration 764, loss = 0.31469873\n",
      "Iteration 103, loss = 0.43972130\n",
      "Iteration 104, loss = 0.43886326\n",
      "Iteration 315, loss = 0.37651676\n",
      "Iteration 393, loss = 0.37027388\n",
      "Iteration 105, loss = 0.43800961\n",
      "Iteration 765, loss = 0.31460178\n",
      "Iteration 106, loss = 0.43717388\n",
      "Iteration 417, loss = 0.35464642\n",
      "Iteration 107, loss = 0.43634898\n",
      "Iteration 316, loss = 0.37637074\n",
      "Iteration 149, loss = 0.45793160\n",
      "Iteration 122, loss = 0.49479521\n",
      "Iteration 394, loss = 0.37001170\n",
      "Iteration 108, loss = 0.43552397\n",
      "Iteration 766, loss = 0.31451172\n",
      "Iteration 109, loss = 0.43475448\n",
      "Iteration 110, loss = 0.43394618\n",
      "Iteration 317, loss = 0.37620888\n",
      "Iteration 37, loss = 0.75001244\n",
      "Iteration 767, loss = 0.31444389\n",
      "Iteration 111, loss = 0.43319765\n",
      "Iteration 395, loss = 0.36971725\n",
      "Iteration 112, loss = 0.43241070\n",
      "Iteration 418, loss = 0.35452316\n",
      "Iteration 768, loss = 0.31430709Iteration 113, loss = 0.43167606\n",
      "\n",
      "Iteration 150, loss = 0.45723404\n",
      "Iteration 114, loss = 0.43092349\n",
      "Iteration 318, loss = 0.37607403\n",
      "Iteration 396, loss = 0.36947320\n",
      "Iteration 123, loss = 0.49327340\n",
      "Iteration 115, loss = 0.43015987\n",
      "Iteration 769, loss = 0.31420712\n",
      "Iteration 116, loss = 0.42945091\n",
      "Iteration 117, loss = 0.42871619\n",
      "Iteration 397, loss = 0.36921538\n",
      "Iteration 770, loss = 0.31409632\n",
      "Iteration 419, loss = 0.35437076\n",
      "Iteration 118, loss = 0.42802089\n",
      "Iteration 771, loss = 0.31398389\n",
      "Iteration 119, loss = 0.42732503\n",
      "Iteration 124, loss = 0.49189419\n",
      "Iteration 151, loss = 0.45654043\n",
      "Iteration 120, loss = 0.42661660\n",
      "Iteration 772, loss = 0.31390324\n",
      "Iteration 121, loss = 0.42593378\n",
      "Iteration 398, loss = 0.36897244\n",
      "Iteration 122, loss = 0.42527477\n",
      "Iteration 773, loss = 0.31379714\n",
      "Iteration 319, loss = 0.37592424\n",
      "Iteration 123, loss = 0.42461238\n",
      "Iteration 124, loss = 0.42394528\n",
      "Iteration 774, loss = 0.31368071\n",
      "Iteration 125, loss = 0.42326688\n",
      "Iteration 125, loss = 0.49037395\n",
      "Iteration 420, loss = 0.35428134\n",
      "Iteration 126, loss = 0.42264634\n",
      "Iteration 775, loss = 0.31361083\n",
      "Iteration 399, loss = 0.36869815\n",
      "Iteration 127, loss = 0.42201110\n",
      "Iteration 128, loss = 0.42138002\n",
      "Iteration 776, loss = 0.31349182\n",
      "Iteration 129, loss = 0.42074897\n",
      "Iteration 130, loss = 0.42017737\n",
      "Iteration 38, loss = 0.74811543\n",
      "Iteration 131, loss = 0.41953170\n",
      "Iteration 421, loss = 0.35415040\n",
      "Iteration 777, loss = 0.31338750\n",
      "Iteration 132, loss = 0.41893980\n",
      "Iteration 133, loss = 0.41835361\n",
      "Iteration 152, loss = 0.45582051\n",
      "Iteration 134, loss = 0.41776523\n",
      "Iteration 778, loss = 0.31330581\n",
      "Iteration 135, loss = 0.41717120\n",
      "Iteration 320, loss = 0.37576948\n",
      "Iteration 400, loss = 0.36846886\n",
      "Iteration 126, loss = 0.48894277\n",
      "Iteration 136, loss = 0.41661168\n",
      "Iteration 422, loss = 0.35405514\n",
      "Iteration 779, loss = 0.31320705\n",
      "Iteration 137, loss = 0.41605883\n",
      "Iteration 401, loss = 0.36825690\n",
      "Iteration 780, loss = 0.31308103\n",
      "Iteration 138, loss = 0.41551141\n",
      "Iteration 139, loss = 0.41492377\n",
      "Iteration 402, loss = 0.36795347\n",
      "Iteration 781, loss = 0.31302199\n",
      "Iteration 140, loss = 0.41436313\n",
      "Iteration 141, loss = 0.41384031\n",
      "Iteration 423, loss = 0.35395327\n",
      "Iteration 403, loss = 0.36770819\n",
      "Iteration 321, loss = 0.37565415\n",
      "Iteration 142, loss = 0.41331533\n",
      "Iteration 782, loss = 0.31290486\n",
      "Iteration 143, loss = 0.41276759\n",
      "Iteration 127, loss = 0.48774302\n",
      "Iteration 404, loss = 0.36743080\n",
      "Iteration 144, loss = 0.41222406\n",
      "Iteration 145, loss = 0.41171540\n",
      "Iteration 783, loss = 0.31279224\n",
      "Iteration 153, loss = 0.45515228\n",
      "Iteration 146, loss = 0.41119953\n",
      "Iteration 147, loss = 0.41071182\n",
      "Iteration 148, loss = 0.41017060\n",
      "Iteration 405, loss = 0.36719167\n",
      "Iteration 424, loss = 0.35382654\n",
      "Iteration 39, loss = 0.74616993\n",
      "Iteration 149, loss = 0.40967717\n",
      "Iteration 150, loss = 0.40918867\n",
      "Iteration 784, loss = 0.31267102\n",
      "Iteration 322, loss = 0.37542984\n",
      "Iteration 151, loss = 0.40868294\n",
      "Iteration 406, loss = 0.36696629\n",
      "Iteration 152, loss = 0.40822660\n",
      "Iteration 785, loss = 0.31257010\n",
      "Iteration 153, loss = 0.40773000\n",
      "Iteration 128, loss = 0.48619276\n",
      "Iteration 786, loss = 0.31246905\n",
      "Iteration 154, loss = 0.40723454\n",
      "Iteration 425, loss = 0.35370536\n",
      "Iteration 155, loss = 0.40678169\n",
      "Iteration 787, loss = 0.31236249\n",
      "Iteration 407, loss = 0.36672706\n",
      "Iteration 156, loss = 0.40630696\n",
      "Iteration 157, loss = 0.40584928\n",
      "Iteration 158, loss = 0.40537887\n",
      "Iteration 788, loss = 0.31227485\n",
      "Iteration 159, loss = 0.40491404\n",
      "Iteration 426, loss = 0.35361739\n",
      "Iteration 129, loss = 0.48489138\n",
      "Iteration 408, loss = 0.36646772\n",
      "Iteration 160, loss = 0.40447413\n",
      "Iteration 323, loss = 0.37530526\n",
      "Iteration 789, loss = 0.31218890\n",
      "Iteration 154, loss = 0.45447658\n",
      "Iteration 161, loss = 0.40403913\n",
      "Iteration 162, loss = 0.40357104\n",
      "Iteration 409, loss = 0.36623299\n",
      "Iteration 790, loss = 0.31205624\n",
      "Iteration 427, loss = 0.35349376\n",
      "Iteration 163, loss = 0.40313370\n",
      "Iteration 164, loss = 0.40266996\n",
      "Iteration 165, loss = 0.40225617\n",
      "Iteration 410, loss = 0.36597104\n",
      "Iteration 166, loss = 0.40185467\n",
      "Iteration 167, loss = 0.40140011\n",
      "Iteration 168, loss = 0.40095546\n",
      "Iteration 169, loss = 0.40053868\n",
      "Iteration 324, loss = 0.37515403\n",
      "Iteration 130, loss = 0.48347714\n",
      "Iteration 411, loss = 0.36573055\n",
      "Iteration 170, loss = 0.40014197\n",
      "Iteration 171, loss = 0.39973124\n",
      "Iteration 40, loss = 0.74430785\n",
      "Iteration 172, loss = 0.39928861\n",
      "Iteration 173, loss = 0.39890712\n",
      "Iteration 412, loss = 0.36551287\n",
      "Iteration 174, loss = 0.39851089\n",
      "Iteration 791, loss = 0.31195338\n",
      "Iteration 175, loss = 0.39810505\n",
      "Iteration 176, loss = 0.39769696\n",
      "Iteration 177, loss = 0.39730227\n",
      "Iteration 413, loss = 0.36523584\n",
      "Iteration 428, loss = 0.35338393\n",
      "Iteration 178, loss = 0.39693962\n",
      "Iteration 792, loss = 0.31186012\n",
      "Iteration 179, loss = 0.39652633\n",
      "Iteration 155, loss = 0.45382343\n",
      "Iteration 414, loss = 0.36502098\n",
      "Iteration 131, loss = 0.48226661\n",
      "Iteration 793, loss = 0.31180150\n",
      "Iteration 325, loss = 0.37498099\n",
      "Iteration 429, loss = 0.35326857\n",
      "Iteration 180, loss = 0.39613225\n",
      "Iteration 181, loss = 0.39576843\n",
      "Iteration 415, loss = 0.36479498\n",
      "Iteration 182, loss = 0.39538442\n",
      "Iteration 132, loss = 0.48094143\n",
      "Iteration 794, loss = 0.31167269\n",
      "Iteration 183, loss = 0.39501229\n",
      "Iteration 156, loss = 0.45323007\n",
      "Iteration 41, loss = 0.74234414\n",
      "Iteration 795, loss = 0.31155050\n",
      "Iteration 184, loss = 0.39464342\n",
      "Iteration 416, loss = 0.36456086\n",
      "Iteration 185, loss = 0.39427338\n",
      "Iteration 796, loss = 0.31146944\n",
      "Iteration 186, loss = 0.39387766\n",
      "Iteration 430, loss = 0.35316186\n",
      "Iteration 797, loss = 0.31134799\n",
      "Iteration 326, loss = 0.37483137\n",
      "Iteration 187, loss = 0.39354481\n",
      "Iteration 798, loss = 0.31124760\n",
      "Iteration 188, loss = 0.39318311\n",
      "Iteration 417, loss = 0.36429077\n",
      "Iteration 431, loss = 0.35306201\n",
      "Iteration 799, loss = 0.31118364\n",
      "Iteration 189, loss = 0.39282564\n",
      "Iteration 800, loss = 0.31106500\n",
      "Iteration 190, loss = 0.39245937\n",
      "Iteration 157, loss = 0.45249662\n",
      "Iteration 191, loss = 0.39210131\n",
      "Iteration 418, loss = 0.36405371\n",
      "Iteration 192, loss = 0.39174035\n",
      "Iteration 133, loss = 0.47965263\n",
      "Iteration 327, loss = 0.37469641\n",
      "Iteration 193, loss = 0.39139754\n",
      "Iteration 432, loss = 0.35294236\n",
      "Iteration 194, loss = 0.39105743\n",
      "Iteration 195, loss = 0.39069675\n",
      "Iteration 801, loss = 0.31095427\n",
      "Iteration 196, loss = 0.39035769\n",
      "Iteration 197, loss = 0.39002800\n",
      "Iteration 198, loss = 0.38969249\n",
      "Iteration 199, loss = 0.38933876\n",
      "Iteration 200, loss = 0.38904315\n",
      "Iteration 134, loss = 0.47847394\n",
      "Iteration 201, loss = 0.38869206\n",
      "Iteration 328, loss = 0.37451652\n",
      "Iteration 202, loss = 0.38834237\n",
      "Iteration 203, loss = 0.38801320\n",
      "Iteration 802, loss = 0.31089453\n",
      "Iteration 204, loss = 0.38769267\n",
      "Iteration 205, loss = 0.38738107\n",
      "Iteration 206, loss = 0.38704976\n",
      "Iteration 42, loss = 0.74036101\n",
      "Iteration 433, loss = 0.35283699\n",
      "Iteration 207, loss = 0.38672339\n",
      "Iteration 419, loss = 0.36384126\n",
      "Iteration 803, loss = 0.31076731\n",
      "Iteration 135, loss = 0.47724020\n",
      "Iteration 208, loss = 0.38642864\n",
      "Iteration 209, loss = 0.38608966\n",
      "Iteration 158, loss = 0.45187818\n",
      "Iteration 329, loss = 0.37438860\n",
      "Iteration 210, loss = 0.38577825\n",
      "Iteration 211, loss = 0.38545850\n",
      "Iteration 804, loss = 0.31067933\n",
      "Iteration 420, loss = 0.36364694\n",
      "Iteration 212, loss = 0.38514093\n",
      "Iteration 213, loss = 0.38484144\n",
      "Iteration 805, loss = 0.31059670\n",
      "Iteration 214, loss = 0.38454806\n",
      "Iteration 434, loss = 0.35272085\n",
      "Iteration 330, loss = 0.37426673\n",
      "Iteration 215, loss = 0.38423794\n",
      "Iteration 421, loss = 0.36337581\n",
      "Iteration 806, loss = 0.31046961\n",
      "Iteration 136, loss = 0.47602114\n",
      "Iteration 216, loss = 0.38392372\n",
      "Iteration 217, loss = 0.38363832\n",
      "Iteration 218, loss = 0.38333328\n",
      "Iteration 159, loss = 0.45133725\n",
      "Iteration 331, loss = 0.37412600\n",
      "Iteration 219, loss = 0.38302366\n",
      "Iteration 422, loss = 0.36318878\n",
      "Iteration 807, loss = 0.31037114\n",
      "Iteration 220, loss = 0.38272763\n",
      "Iteration 137, loss = 0.47486831\n",
      "Iteration 808, loss = 0.31027253\n",
      "Iteration 221, loss = 0.38245106\n",
      "Iteration 435, loss = 0.35261578\n",
      "Iteration 222, loss = 0.38214886\n",
      "Iteration 423, loss = 0.36290004\n",
      "Iteration 332, loss = 0.37392979\n",
      "Iteration 223, loss = 0.38186224\n",
      "Iteration 160, loss = 0.45064874\n",
      "Iteration 224, loss = 0.38158309\n",
      "Iteration 43, loss = 0.73828130\n",
      "Iteration 138, loss = 0.47365689\n",
      "Iteration 225, loss = 0.38128266\n",
      "Iteration 333, loss = 0.37378037\n",
      "Iteration 809, loss = 0.31017818\n",
      "Iteration 424, loss = 0.36269201\n",
      "Iteration 436, loss = 0.35251033\n",
      "Iteration 226, loss = 0.38100309\n",
      "Iteration 810, loss = 0.31009025\n",
      "Iteration 227, loss = 0.38072177\n",
      "Iteration 334, loss = 0.37364638\n",
      "Iteration 811, loss = 0.31004982\n",
      "Iteration 228, loss = 0.38043716\n",
      "Iteration 437, loss = 0.35242191\n",
      "Iteration 425, loss = 0.36246422\n",
      "Iteration 229, loss = 0.38017766\n",
      "Iteration 230, loss = 0.37989595\n",
      "Iteration 231, loss = 0.37960087\n",
      "Iteration 232, loss = 0.37932184\n",
      "Iteration 812, loss = 0.30988292\n",
      "Iteration 438, loss = 0.35229532\n",
      "Iteration 233, loss = 0.37908222\n",
      "Iteration 335, loss = 0.37351761\n",
      "Iteration 161, loss = 0.45003832\n",
      "Iteration 234, loss = 0.37878344\n",
      "Iteration 439, loss = 0.35218528\n",
      "Iteration 235, loss = 0.37852539\n",
      "Iteration 813, loss = 0.30978639\n",
      "Iteration 139, loss = 0.47258898\n",
      "Iteration 236, loss = 0.37827516\n",
      "Iteration 44, loss = 0.73628522\n",
      "Iteration 440, loss = 0.35207952\n",
      "Iteration 237, loss = 0.37798784\n",
      "Iteration 426, loss = 0.36227069\n",
      "Iteration 814, loss = 0.30973363\n",
      "Iteration 238, loss = 0.37771155\n",
      "Iteration 815, loss = 0.30959110\n",
      "Iteration 239, loss = 0.37746877\n",
      "Iteration 441, loss = 0.35195891\n",
      "Iteration 816, loss = 0.30949898\n",
      "Iteration 240, loss = 0.37720160\n",
      "Iteration 336, loss = 0.37336154\n",
      "Iteration 241, loss = 0.37693671\n",
      "Iteration 427, loss = 0.36203659\n",
      "Iteration 162, loss = 0.44944581\n",
      "Iteration 140, loss = 0.47137681\n",
      "Iteration 428, loss = 0.36176023\n",
      "Iteration 817, loss = 0.30939126\n",
      "Iteration 442, loss = 0.35186478\n",
      "Iteration 337, loss = 0.37321212\n",
      "Iteration 429, loss = 0.36155780\n",
      "Iteration 818, loss = 0.30930093\n",
      "Iteration 430, loss = 0.36131902\n",
      "Iteration 242, loss = 0.37668196\n",
      "Iteration 338, loss = 0.37307035\n",
      "Iteration 243, loss = 0.37641500\n",
      "Iteration 244, loss = 0.37616306\n",
      "Iteration 819, loss = 0.30921211\n",
      "Iteration 245, loss = 0.37591836\n",
      "Iteration 246, loss = 0.37568352\n",
      "Iteration 45, loss = 0.73419330\n",
      "Iteration 247, loss = 0.37541209\n",
      "Iteration 431, loss = 0.36109991\n",
      "Iteration 248, loss = 0.37517031\n",
      "Iteration 443, loss = 0.35175055\n",
      "Iteration 249, loss = 0.37491501\n",
      "Iteration 820, loss = 0.30913588\n",
      "Iteration 250, loss = 0.37466057\n",
      "Iteration 251, loss = 0.37443027\n",
      "Iteration 141, loss = 0.47035509Iteration 252, loss = 0.37417669\n",
      "\n",
      "Iteration 253, loss = 0.37393651\n",
      "Iteration 163, loss = 0.44886194\n",
      "Iteration 339, loss = 0.37291443\n",
      "Iteration 254, loss = 0.37369788\n",
      "Iteration 255, loss = 0.37345837\n",
      "Iteration 821, loss = 0.30900598\n",
      "Iteration 432, loss = 0.36093329\n",
      "Iteration 340, loss = 0.37275929\n",
      "Iteration 433, loss = 0.36064820\n",
      "Iteration 444, loss = 0.35165931\n",
      "Iteration 434, loss = 0.36043253\n",
      "Iteration 256, loss = 0.37321758\n",
      "Iteration 822, loss = 0.30893516\n",
      "Iteration 257, loss = 0.37297351\n",
      "Iteration 258, loss = 0.37273698\n",
      "Iteration 823, loss = 0.30885725\n",
      "Iteration 142, loss = 0.46922867\n",
      "Iteration 435, loss = 0.36023633\n",
      "Iteration 445, loss = 0.35154678\n",
      "Iteration 259, loss = 0.37251759\n",
      "Iteration 46, loss = 0.73212235\n",
      "Iteration 260, loss = 0.37228364\n",
      "Iteration 824, loss = 0.30874847\n",
      "Iteration 341, loss = 0.37261901\n",
      "Iteration 261, loss = 0.37204210\n",
      "Iteration 262, loss = 0.37184814\n",
      "Iteration 263, loss = 0.37159887\n",
      "Iteration 164, loss = 0.44824069\n",
      "Iteration 446, loss = 0.35143586\n",
      "Iteration 264, loss = 0.37136374\n",
      "Iteration 436, loss = 0.36001086\n",
      "Iteration 825, loss = 0.30864334\n",
      "Iteration 265, loss = 0.37113524\n",
      "Iteration 826, loss = 0.30853275\n",
      "Iteration 447, loss = 0.35132174\n",
      "Iteration 266, loss = 0.37091529\n",
      "Iteration 827, loss = 0.30845521\n",
      "Iteration 342, loss = 0.37245310\n",
      "Iteration 267, loss = 0.37071374\n",
      "Iteration 143, loss = 0.46816449\n",
      "Iteration 437, loss = 0.35979270\n",
      "Iteration 268, loss = 0.37048869\n",
      "Iteration 448, loss = 0.35122059\n",
      "Iteration 269, loss = 0.37025864\n",
      "Iteration 343, loss = 0.37231101\n",
      "Iteration 828, loss = 0.30833122\n",
      "Iteration 270, loss = 0.37004504\n",
      "Iteration 271, loss = 0.36983160\n",
      "Iteration 272, loss = 0.36958493\n",
      "Iteration 438, loss = 0.35958530\n",
      "Iteration 273, loss = 0.36939142\n",
      "Iteration 274, loss = 0.36917110\n",
      "Iteration 275, loss = 0.36897346\n",
      "Iteration 165, loss = 0.44768508\n",
      "Iteration 829, loss = 0.30828539\n",
      "Iteration 276, loss = 0.36875214\n",
      "Iteration 439, loss = 0.35936107\n",
      "Iteration 277, loss = 0.36856065\n",
      "Iteration 278, loss = 0.36833936\n",
      "Iteration 279, loss = 0.36812083\n",
      "Iteration 144, loss = 0.46709491\n",
      "Iteration 280, loss = 0.36791716\n",
      "Iteration 449, loss = 0.35112298\n",
      "Iteration 830, loss = 0.30820184\n",
      "Iteration 47, loss = 0.72996122\n",
      "Iteration 281, loss = 0.36772089\n",
      "Iteration 440, loss = 0.35914363\n",
      "Iteration 145, loss = 0.46609814\n",
      "Iteration 450, loss = 0.35102257\n",
      "Iteration 344, loss = 0.37217008\n",
      "Iteration 282, loss = 0.36751983\n",
      "Iteration 283, loss = 0.36730071\n",
      "Iteration 284, loss = 0.36709646\n",
      "Iteration 285, loss = 0.36690030\n",
      "Iteration 831, loss = 0.30805696\n",
      "Iteration 166, loss = 0.44708095\n",
      "Iteration 286, loss = 0.36669896\n",
      "Iteration 441, loss = 0.35895042\n",
      "Iteration 287, loss = 0.36649645\n",
      "Iteration 146, loss = 0.46503514\n",
      "Iteration 288, loss = 0.36630539\n",
      "Iteration 289, loss = 0.36609389\n",
      "Iteration 290, loss = 0.36589117\n",
      "Iteration 832, loss = 0.30794594\n",
      "Iteration 442, loss = 0.35871021\n",
      "Iteration 451, loss = 0.35097979\n",
      "Iteration 291, loss = 0.36570088\n",
      "Iteration 292, loss = 0.36549786\n",
      "Iteration 443, loss = 0.35852199\n",
      "Iteration 345, loss = 0.37206173\n",
      "Iteration 293, loss = 0.36530919\n",
      "Iteration 833, loss = 0.30793687\n",
      "Iteration 294, loss = 0.36511874\n",
      "Iteration 444, loss = 0.35830565\n",
      "Iteration 295, loss = 0.36492269\n",
      "Iteration 296, loss = 0.36472461\n",
      "Iteration 297, loss = 0.36454795\n",
      "Iteration 445, loss = 0.35813136\n",
      "Iteration 452, loss = 0.35079350\n",
      "Iteration 834, loss = 0.30777270\n",
      "Iteration 298, loss = 0.36436284\n",
      "Iteration 147, loss = 0.46406343\n",
      "Iteration 299, loss = 0.36418244\n",
      "Iteration 835, loss = 0.30769542\n",
      "Iteration 346, loss = 0.37192977\n",
      "Iteration 48, loss = 0.72776163\n",
      "Iteration 300, loss = 0.36397055\n",
      "Iteration 446, loss = 0.35789026\n",
      "Iteration 836, loss = 0.30758261\n",
      "Iteration 301, loss = 0.36378307\n",
      "Iteration 453, loss = 0.35068879\n",
      "Iteration 302, loss = 0.36362535\n",
      "Iteration 347, loss = 0.37173338\n",
      "Iteration 837, loss = 0.30749192\n",
      "Iteration 303, loss = 0.36342007\n",
      "Iteration 304, loss = 0.36324063\n",
      "Iteration 838, loss = 0.30739226\n",
      "Iteration 305, loss = 0.36305399\n",
      "Iteration 454, loss = 0.35064478\n",
      "Iteration 306, loss = 0.36286378\n",
      "Iteration 348, loss = 0.37162395\n",
      "Iteration 307, loss = 0.36269889\n",
      "Iteration 148, loss = 0.46304865\n",
      "Iteration 308, loss = 0.36251100\n",
      "Iteration 309, loss = 0.36232810\n",
      "Iteration 447, loss = 0.35773368\n",
      "Iteration 839, loss = 0.30731640\n",
      "Iteration 310, loss = 0.36214708\n",
      "Iteration 311, loss = 0.36197695\n",
      "Iteration 455, loss = 0.35051596\n",
      "Iteration 312, loss = 0.36181336\n",
      "Iteration 313, loss = 0.36163121\n",
      "Iteration 167, loss = 0.44654746\n",
      "Iteration 314, loss = 0.36146624\n",
      "Iteration 840, loss = 0.30720548\n",
      "Iteration 448, loss = 0.35748867\n",
      "Iteration 315, loss = 0.36126774\n",
      "Iteration 456, loss = 0.35037728Iteration 149, loss = 0.46208095\n",
      "\n",
      "Iteration 841, loss = 0.30709064\n",
      "Iteration 316, loss = 0.36110058\n",
      "Iteration 317, loss = 0.36092759\n",
      "Iteration 842, loss = 0.30700014\n",
      "Iteration 318, loss = 0.36075623\n",
      "Iteration 349, loss = 0.37143801\n",
      "Iteration 449, loss = 0.35726608\n",
      "Iteration 49, loss = 0.72554380\n",
      "Iteration 319, loss = 0.36059958\n",
      "Iteration 843, loss = 0.30691751\n",
      "Iteration 320, loss = 0.36042650\n",
      "Iteration 150, loss = 0.46123299\n",
      "Iteration 321, loss = 0.36024418\n",
      "Iteration 322, loss = 0.36007392\n",
      "Iteration 450, loss = 0.35717623\n",
      "Iteration 844, loss = 0.30681590\n",
      "Iteration 350, loss = 0.37132546\n",
      "Iteration 323, loss = 0.35990454\n",
      "Iteration 324, loss = 0.35975205\n",
      "Iteration 457, loss = 0.35027198\n",
      "Iteration 325, loss = 0.35958439\n",
      "Iteration 845, loss = 0.30670850\n",
      "Iteration 451, loss = 0.35685720\n",
      "Iteration 326, loss = 0.35941359\n",
      "Iteration 327, loss = 0.35925349\n",
      "Iteration 168, loss = 0.44601115\n",
      "Iteration 328, loss = 0.35910407\n",
      "Iteration 846, loss = 0.30666007\n",
      "Iteration 329, loss = 0.35891185\n",
      "Iteration 351, loss = 0.37121106\n",
      "Iteration 458, loss = 0.35016959\n",
      "Iteration 330, loss = 0.35877466\n",
      "Iteration 847, loss = 0.30653058\n",
      "Iteration 331, loss = 0.35860846\n",
      "Iteration 452, loss = 0.35675458\n",
      "Iteration 332, loss = 0.35845312\n",
      "Iteration 848, loss = 0.30644721\n",
      "Iteration 169, loss = 0.44541321\n",
      "Iteration 333, loss = 0.35830289\n",
      "Iteration 849, loss = 0.30639318\n",
      "Iteration 151, loss = 0.46019872\n",
      "Iteration 334, loss = 0.35811929\n",
      "Iteration 850, loss = 0.30624510\n",
      "Iteration 335, loss = 0.35796793\n",
      "Iteration 453, loss = 0.35648587\n",
      "Iteration 336, loss = 0.35782742\n",
      "Iteration 459, loss = 0.35007125\n",
      "Iteration 337, loss = 0.35765253\n",
      "Iteration 338, loss = 0.35749535\n",
      "Iteration 352, loss = 0.37104580\n",
      "Iteration 339, loss = 0.35735110\n",
      "Iteration 340, loss = 0.35718467\n",
      "Iteration 851, loss = 0.30618613\n",
      "Iteration 454, loss = 0.35629069\n",
      "Iteration 341, loss = 0.35703786\n",
      "Iteration 342, loss = 0.35688391\n",
      "Iteration 343, loss = 0.35674120\n",
      "Iteration 50, loss = 0.72320299\n",
      "Iteration 344, loss = 0.35657797\n",
      "Iteration 345, loss = 0.35643008\n",
      "Iteration 455, loss = 0.35614054\n",
      "Iteration 346, loss = 0.35628564\n",
      "Iteration 852, loss = 0.30604808\n",
      "Iteration 460, loss = 0.34995923\n",
      "Iteration 347, loss = 0.35612432\n",
      "Iteration 152, loss = 0.45925728\n",
      "Iteration 353, loss = 0.37089270\n",
      "Iteration 853, loss = 0.30596539Iteration 348, loss = 0.35598624\n",
      "\n",
      "Iteration 349, loss = 0.35583861\n",
      "Iteration 461, loss = 0.34990060\n",
      "Iteration 350, loss = 0.35568297\n",
      "Iteration 351, loss = 0.35554427\n",
      "Iteration 456, loss = 0.35590433\n",
      "Iteration 170, loss = 0.44485386\n",
      "Iteration 352, loss = 0.35539501\n",
      "Iteration 462, loss = 0.34975659\n",
      "Iteration 353, loss = 0.35524166\n",
      "Iteration 354, loss = 0.37076114\n",
      "Iteration 354, loss = 0.35511770\n",
      "Iteration 854, loss = 0.30586490\n",
      "Iteration 463, loss = 0.34964033\n",
      "Iteration 355, loss = 0.35494675\n",
      "Iteration 457, loss = 0.35569997\n",
      "Iteration 355, loss = 0.37062437\n",
      "Iteration 356, loss = 0.35481425\n",
      "Iteration 357, loss = 0.35466136\n",
      "Iteration 855, loss = 0.30578203\n",
      "Iteration 358, loss = 0.35452427\n",
      "Iteration 359, loss = 0.35439036\n",
      "Iteration 458, loss = 0.35545980\n",
      "Iteration 464, loss = 0.34953564\n",
      "Iteration 153, loss = 0.45841736\n",
      "Iteration 356, loss = 0.37048031\n",
      "Iteration 360, loss = 0.35423835\n",
      "Iteration 856, loss = 0.30567017\n",
      "Iteration 361, loss = 0.35410805\n",
      "Iteration 459, loss = 0.35526880\n",
      "Iteration 362, loss = 0.35396832\n",
      "Iteration 363, loss = 0.35382273\n",
      "Iteration 171, loss = 0.44433529\n",
      "Iteration 51, loss = 0.72102013\n",
      "Iteration 460, loss = 0.35506416\n",
      "Iteration 857, loss = 0.30559493\n",
      "Iteration 364, loss = 0.35369446\n",
      "Iteration 357, loss = 0.37030769\n",
      "Iteration 365, loss = 0.35355118\n",
      "Iteration 858, loss = 0.30549320Iteration 465, loss = 0.34944504\n",
      "\n",
      "Iteration 366, loss = 0.35341226\n",
      "Iteration 461, loss = 0.35492933\n",
      "Iteration 367, loss = 0.35327206\n",
      "Iteration 368, loss = 0.35314171\n",
      "Iteration 154, loss = 0.45749913\n",
      "Iteration 172, loss = 0.44382557\n",
      "Iteration 859, loss = 0.30538221\n",
      "Iteration 462, loss = 0.35467268\n",
      "Iteration 369, loss = 0.35300047\n",
      "Iteration 358, loss = 0.37018406\n",
      "Iteration 370, loss = 0.35286945\n",
      "Iteration 463, loss = 0.35445470\n",
      "Iteration 371, loss = 0.35273329\n",
      "Iteration 860, loss = 0.30530644\n",
      "Iteration 466, loss = 0.34932764\n",
      "Iteration 372, loss = 0.35259653\n",
      "Iteration 464, loss = 0.35431135\n",
      "Iteration 373, loss = 0.35246450\n",
      "Iteration 359, loss = 0.37004675\n",
      "Iteration 374, loss = 0.35232539\n",
      "Iteration 465, loss = 0.35408407\n",
      "Iteration 467, loss = 0.34925025\n",
      "Iteration 861, loss = 0.30519549\n",
      "Iteration 375, loss = 0.35219841\n",
      "Iteration 376, loss = 0.35208927\n",
      "Iteration 173, loss = 0.44330367\n",
      "Iteration 377, loss = 0.35194454\n",
      "Iteration 466, loss = 0.35389189\n",
      "Iteration 378, loss = 0.35180271\n",
      "Iteration 155, loss = 0.45659934\n",
      "Iteration 379, loss = 0.35167154\n",
      "Iteration 380, loss = 0.35154853\n",
      "Iteration 381, loss = 0.35141701\n",
      "Iteration 862, loss = 0.30509599\n",
      "Iteration 467, loss = 0.35367954\n",
      "Iteration 382, loss = 0.35128991\n",
      "Iteration 383, loss = 0.35116324\n",
      "Iteration 468, loss = 0.34913480\n",
      "Iteration 384, loss = 0.35102667\n",
      "Iteration 385, loss = 0.35090232\n",
      "Iteration 52, loss = 0.71865862\n",
      "Iteration 386, loss = 0.35078065\n",
      "Iteration 387, loss = 0.35065004\n",
      "Iteration 388, loss = 0.35051333\n",
      "Iteration 360, loss = 0.36995438\n",
      "Iteration 863, loss = 0.30501064\n",
      "Iteration 389, loss = 0.35039754\n",
      "Iteration 469, loss = 0.34902797\n",
      "Iteration 468, loss = 0.35347015\n",
      "Iteration 390, loss = 0.35026930\n",
      "Iteration 174, loss = 0.44276496\n",
      "Iteration 391, loss = 0.35014522\n",
      "Iteration 156, loss = 0.45580598\n",
      "Iteration 392, loss = 0.35002947\n",
      "Iteration 393, loss = 0.34989394\n",
      "Iteration 394, loss = 0.34976369\n",
      "Iteration 470, loss = 0.34892449\n",
      "Iteration 864, loss = 0.30491394\n",
      "Iteration 395, loss = 0.34965156\n",
      "Iteration 396, loss = 0.34952918\n",
      "Iteration 469, loss = 0.35333816\n",
      "Iteration 397, loss = 0.34941109\n",
      "Iteration 471, loss = 0.34882621\n",
      "Iteration 361, loss = 0.36978294\n",
      "Iteration 865, loss = 0.30482435\n",
      "Iteration 398, loss = 0.34928861\n",
      "Iteration 399, loss = 0.34917842\n",
      "Iteration 400, loss = 0.34905207\n",
      "Iteration 401, loss = 0.34892985\n",
      "Iteration 157, loss = 0.45484589\n",
      "Iteration 470, loss = 0.35307626\n",
      "Iteration 402, loss = 0.34880956Iteration 472, loss = 0.34872468\n",
      "\n",
      "Iteration 866, loss = 0.30475030\n",
      "Iteration 362, loss = 0.36961606\n",
      "Iteration 53, loss = 0.71639625\n",
      "Iteration 403, loss = 0.34869725\n",
      "Iteration 404, loss = 0.34858280\n",
      "Iteration 867, loss = 0.30463521\n",
      "Iteration 473, loss = 0.34862779\n",
      "Iteration 405, loss = 0.34846173\n",
      "Iteration 868, loss = 0.30452092\n",
      "Iteration 406, loss = 0.34834632\n",
      "Iteration 471, loss = 0.35285927\n",
      "Iteration 363, loss = 0.36946398\n",
      "Iteration 407, loss = 0.34823160\n",
      "Iteration 869, loss = 0.30446019\n",
      "Iteration 175, loss = 0.44225066\n",
      "Iteration 474, loss = 0.34852896\n",
      "Iteration 408, loss = 0.34813064\n",
      "Iteration 870, loss = 0.30437053\n",
      "Iteration 409, loss = 0.34800722\n",
      "Iteration 472, loss = 0.35269889\n",
      "Iteration 410, loss = 0.34790632\n",
      "Iteration 411, loss = 0.34778023\n",
      "Iteration 412, loss = 0.34765241\n",
      "Iteration 364, loss = 0.36940001\n",
      "Iteration 413, loss = 0.34753852\n",
      "Iteration 871, loss = 0.30425327\n",
      "Iteration 414, loss = 0.34743136\n",
      "Iteration 473, loss = 0.35247021\n",
      "Iteration 415, loss = 0.34732672\n",
      "Iteration 158, loss = 0.45407950\n",
      "Iteration 416, loss = 0.34722216\n",
      "Iteration 365, loss = 0.36921296\n",
      "Iteration 417, loss = 0.34709603\n",
      "Iteration 475, loss = 0.34843784\n",
      "Iteration 54, loss = 0.71394615\n",
      "Iteration 418, loss = 0.34699817\n",
      "Iteration 419, loss = 0.34687706\n",
      "Iteration 474, loss = 0.35229643\n",
      "Iteration 176, loss = 0.44172910\n",
      "Iteration 420, loss = 0.34677299\n",
      "Iteration 159, loss = 0.45323430\n",
      "Iteration 872, loss = 0.30415341\n",
      "Iteration 421, loss = 0.34666123\n",
      "Iteration 422, loss = 0.34657611\n",
      "Iteration 366, loss = 0.36906420\n",
      "Iteration 423, loss = 0.34646000\n",
      "Iteration 873, loss = 0.30404405\n",
      "Iteration 424, loss = 0.34634117\n",
      "Iteration 475, loss = 0.35212050\n",
      "Iteration 874, loss = 0.30394903\n",
      "Iteration 425, loss = 0.34623490\n",
      "Iteration 426, loss = 0.34612759\n",
      "Iteration 875, loss = 0.30387197\n",
      "Iteration 476, loss = 0.34832012\n",
      "Iteration 427, loss = 0.34601641\n",
      "Iteration 160, loss = 0.45242858\n",
      "Iteration 428, loss = 0.34591920\n",
      "Iteration 876, loss = 0.30376790\n",
      "Iteration 367, loss = 0.36893964\n",
      "Iteration 429, loss = 0.34580450\n",
      "Iteration 430, loss = 0.34569784\n",
      "Iteration 431, loss = 0.34559795\n",
      "Iteration 432, loss = 0.34549466\n",
      "Iteration 877, loss = 0.30367790\n",
      "Iteration 476, loss = 0.35196627\n",
      "Iteration 433, loss = 0.34538152\n",
      "Iteration 434, loss = 0.34528363\n",
      "Iteration 368, loss = 0.36879592\n",
      "Iteration 878, loss = 0.30361047\n",
      "Iteration 477, loss = 0.34822352\n",
      "Iteration 177, loss = 0.44121567\n",
      "Iteration 435, loss = 0.34517264\n",
      "Iteration 879, loss = 0.30348004\n",
      "Iteration 477, loss = 0.35168087\n",
      "Iteration 436, loss = 0.34507075\n",
      "Iteration 880, loss = 0.30339287\n",
      "Iteration 437, loss = 0.34496962\n",
      "Iteration 881, loss = 0.30329651\n",
      "Iteration 438, loss = 0.34487102\n",
      "Iteration 478, loss = 0.35148498\n",
      "Iteration 882, loss = 0.30319616\n",
      "Iteration 439, loss = 0.34475908\n",
      "Iteration 478, loss = 0.34812724\n",
      "Iteration 178, loss = 0.44073737\n",
      "Iteration 883, loss = 0.30311557\n",
      "Iteration 161, loss = 0.45154715\n",
      "Iteration 440, loss = 0.34465689\n",
      "Iteration 479, loss = 0.35131117\n",
      "Iteration 884, loss = 0.30301345\n",
      "Iteration 441, loss = 0.34455763\n",
      "Iteration 369, loss = 0.36866498\n",
      "Iteration 442, loss = 0.34445682\n",
      "Iteration 885, loss = 0.30295732\n",
      "Iteration 443, loss = 0.34435568\n",
      "Iteration 480, loss = 0.35109112\n",
      "Iteration 55, loss = 0.71154791\n",
      "Iteration 886, loss = 0.30291508\n",
      "Iteration 444, loss = 0.34425232\n",
      "Iteration 481, loss = 0.35098145\n",
      "Iteration 445, loss = 0.34415393\n",
      "Iteration 479, loss = 0.34802617\n",
      "Iteration 446, loss = 0.34404588\n",
      "Iteration 447, loss = 0.34395250\n",
      "Iteration 887, loss = 0.30275026\n",
      "Iteration 482, loss = 0.35070315\n",
      "Iteration 448, loss = 0.34385503\n",
      "Iteration 888, loss = 0.30263307\n",
      "Iteration 449, loss = 0.34375990\n",
      "Iteration 370, loss = 0.36849873\n",
      "Iteration 889, loss = 0.30258468\n",
      "Iteration 450, loss = 0.34365014\n",
      "Iteration 483, loss = 0.35056722\n",
      "Iteration 451, loss = 0.34355076\n",
      "Iteration 890, loss = 0.30243835\n",
      "Iteration 162, loss = 0.45081454\n",
      "Iteration 452, loss = 0.34345594\n",
      "Iteration 484, loss = 0.35032919\n",
      "Iteration 891, loss = 0.30235931\n",
      "Iteration 453, loss = 0.34335767\n",
      "Iteration 480, loss = 0.34792435\n",
      "Iteration 179, loss = 0.44021547\n",
      "Iteration 892, loss = 0.30230658\n",
      "Iteration 485, loss = 0.35013703\n",
      "Iteration 454, loss = 0.34325857\n",
      "Iteration 455, loss = 0.34315990\n",
      "Iteration 481, loss = 0.34782403\n",
      "Iteration 371, loss = 0.36837294\n",
      "Iteration 486, loss = 0.34997987\n",
      "Iteration 456, loss = 0.34306180\n",
      "Iteration 893, loss = 0.30218215\n",
      "Iteration 457, loss = 0.34296389\n",
      "Iteration 458, loss = 0.34286345\n",
      "Iteration 459, loss = 0.34277321\n",
      "Iteration 460, loss = 0.34267925\n",
      "Iteration 894, loss = 0.30206281\n",
      "Iteration 461, loss = 0.34257692\n",
      "Iteration 482, loss = 0.34774720\n",
      "Iteration 487, loss = 0.34978006\n",
      "Iteration 163, loss = 0.45006594\n",
      "Iteration 462, loss = 0.34248199\n",
      "Iteration 895, loss = 0.30198329\n",
      "Iteration 463, loss = 0.34239002\n",
      "Iteration 56, loss = 0.70915695\n",
      "Iteration 896, loss = 0.30189829\n",
      "Iteration 464, loss = 0.34229121\n",
      "Iteration 372, loss = 0.36821223\n",
      "Iteration 488, loss = 0.34959486\n",
      "Iteration 180, loss = 0.43973681\n",
      "Iteration 465, loss = 0.34220286\n",
      "Iteration 897, loss = 0.30178335\n",
      "Iteration 466, loss = 0.34211004\n",
      "Iteration 483, loss = 0.34764330\n",
      "Iteration 373, loss = 0.36808958\n",
      "Iteration 467, loss = 0.34200818\n",
      "Iteration 468, loss = 0.34192666\n",
      "Iteration 469, loss = 0.34183011\n",
      "Iteration 489, loss = 0.34940966\n",
      "Iteration 470, loss = 0.34174308\n",
      "Iteration 471, loss = 0.34164444\n",
      "Iteration 898, loss = 0.30169957\n",
      "Iteration 472, loss = 0.34154206\n",
      "Iteration 490, loss = 0.34920535\n",
      "Iteration 473, loss = 0.34146870\n",
      "Iteration 374, loss = 0.36795478\n",
      "Iteration 474, loss = 0.34136568\n",
      "Iteration 475, loss = 0.34127961\n",
      "Iteration 476, loss = 0.34119220\n",
      "Iteration 164, loss = 0.44927565\n",
      "Iteration 899, loss = 0.30168980\n",
      "Iteration 477, loss = 0.34110279\n",
      "Iteration 491, loss = 0.34901529\n",
      "Iteration 484, loss = 0.34753168\n",
      "Iteration 478, loss = 0.34100867\n",
      "Iteration 479, loss = 0.34092888\n",
      "Iteration 480, loss = 0.34082364\n",
      "Iteration 481, loss = 0.34074167\n",
      "Iteration 482, loss = 0.34065767\n",
      "Iteration 900, loss = 0.30149276\n",
      "Iteration 483, loss = 0.34056736\n",
      "Iteration 484, loss = 0.34047705\n",
      "Iteration 375, loss = 0.36780309\n",
      "Iteration 485, loss = 0.34038886\n",
      "Iteration 492, loss = 0.34884305\n",
      "Iteration 486, loss = 0.34030726\n",
      "Iteration 181, loss = 0.43926072\n",
      "Iteration 901, loss = 0.30142327\n",
      "Iteration 57, loss = 0.70663782\n",
      "Iteration 485, loss = 0.34742062\n",
      "Iteration 487, loss = 0.34021191\n",
      "Iteration 902, loss = 0.30129636\n",
      "Iteration 488, loss = 0.34012676\n",
      "Iteration 489, loss = 0.34004740\n",
      "Iteration 376, loss = 0.36767095\n",
      "Iteration 490, loss = 0.33996680\n",
      "Iteration 903, loss = 0.30120437\n",
      "Iteration 165, loss = 0.44845688\n",
      "Iteration 493, loss = 0.34864764\n",
      "Iteration 491, loss = 0.33988715\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 494, loss = 0.34846112\n",
      "Iteration 904, loss = 0.30112200\n",
      "Iteration 377, loss = 0.36765225\n",
      "Iteration 486, loss = 0.34732622\n",
      "Iteration 495, loss = 0.34829588\n",
      "Iteration 905, loss = 0.30104982\n",
      "Iteration 166, loss = 0.44775244\n",
      "Iteration 496, loss = 0.34812430\n",
      "Iteration 182, loss = 0.43875119\n",
      "Iteration 906, loss = 0.30094826\n",
      "Iteration 497, loss = 0.34792404\n",
      "Iteration 487, loss = 0.34722363\n",
      "Iteration 58, loss = 0.70413008\n",
      "Iteration 378, loss = 0.36741192\n",
      "Iteration 907, loss = 0.30085701\n",
      "Iteration 498, loss = 0.34776168\n",
      "Iteration 167, loss = 0.44708719\n",
      "Iteration 1, loss = 0.68625289\n",
      "Iteration 2, loss = 0.68351100\n",
      "Iteration 908, loss = 0.30075115\n",
      "Iteration 488, loss = 0.34712029\n",
      "Iteration 3, loss = 0.67935284\n",
      "Iteration 909, loss = 0.30069728\n",
      "Iteration 4, loss = 0.67419946\n",
      "Iteration 379, loss = 0.36728101\n",
      "Iteration 910, loss = 0.30056763Iteration 5, loss = 0.66866995\n",
      "\n",
      "Iteration 183, loss = 0.43827322Iteration 168, loss = 0.44633694\n",
      "\n",
      "Iteration 6, loss = 0.66270770\n",
      "Iteration 499, loss = 0.34760352\n",
      "Iteration 7, loss = 0.65662959\n",
      "Iteration 8, loss = 0.65008414\n",
      "Iteration 9, loss = 0.64409838\n",
      "Iteration 911, loss = 0.30050182\n",
      "Iteration 10, loss = 0.63794404\n",
      "Iteration 489, loss = 0.34702628\n",
      "Iteration 380, loss = 0.36715468\n",
      "Iteration 500, loss = 0.34739792\n",
      "Iteration 912, loss = 0.30036809\n",
      "Iteration 11, loss = 0.63212893\n",
      "Iteration 59, loss = 0.70155132\n",
      "Iteration 490, loss = 0.34693846\n",
      "Iteration 12, loss = 0.62613692\n",
      "Iteration 913, loss = 0.30027104\n",
      "Iteration 501, loss = 0.34722345\n",
      "Iteration 381, loss = 0.36699881\n",
      "Iteration 13, loss = 0.62049168\n",
      "Iteration 169, loss = 0.44557701\n",
      "Iteration 14, loss = 0.61502356\n",
      "Iteration 914, loss = 0.30020762\n",
      "Iteration 382, loss = 0.36685222\n",
      "Iteration 491, loss = 0.34683412\n",
      "Iteration 15, loss = 0.60976703\n",
      "Iteration 502, loss = 0.34705486\n",
      "Iteration 16, loss = 0.60474564\n",
      "Iteration 915, loss = 0.30009983\n",
      "Iteration 383, loss = 0.36671050\n",
      "Iteration 503, loss = 0.34689540\n",
      "Iteration 492, loss = 0.34673398\n",
      "Iteration 17, loss = 0.59976107\n",
      "Iteration 18, loss = 0.59518205\n",
      "Iteration 916, loss = 0.30001366\n",
      "Iteration 170, loss = 0.44492468\n",
      "Iteration 184, loss = 0.43782862\n",
      "Iteration 19, loss = 0.59048616\n",
      "Iteration 384, loss = 0.36670462\n",
      "Iteration 60, loss = 0.69903946\n",
      "Iteration 917, loss = 0.30000510\n",
      "Iteration 20, loss = 0.58585649\n",
      "Iteration 504, loss = 0.34669111\n",
      "Iteration 918, loss = 0.29980819\n",
      "Iteration 21, loss = 0.58160194\n",
      "Iteration 493, loss = 0.34665219\n",
      "Iteration 171, loss = 0.44423312\n",
      "Iteration 919, loss = 0.29970466\n",
      "Iteration 505, loss = 0.34652337\n",
      "Iteration 22, loss = 0.57741149\n",
      "Iteration 23, loss = 0.57340307\n",
      "Iteration 494, loss = 0.34656948\n",
      "Iteration 24, loss = 0.56937761\n",
      "Iteration 920, loss = 0.29962927\n",
      "Iteration 172, loss = 0.44354733\n",
      "Iteration 385, loss = 0.36643819\n",
      "Iteration 25, loss = 0.56554646\n",
      "Iteration 921, loss = 0.29961109\n",
      "Iteration 506, loss = 0.34636661\n",
      "Iteration 26, loss = 0.56168271\n",
      "Iteration 495, loss = 0.34645035\n",
      "Iteration 922, loss = 0.29943742\n",
      "Iteration 27, loss = 0.55814033\n",
      "Iteration 386, loss = 0.36632940\n",
      "Iteration 185, loss = 0.43734511\n",
      "Iteration 923, loss = 0.29934759\n",
      "Iteration 507, loss = 0.34615883\n",
      "Iteration 496, loss = 0.34634536\n",
      "Iteration 28, loss = 0.55453260\n",
      "Iteration 29, loss = 0.55119337\n",
      "Iteration 508, loss = 0.34597780\n",
      "Iteration 924, loss = 0.29927392\n",
      "Iteration 173, loss = 0.44282658\n",
      "Iteration 61, loss = 0.69644576\n",
      "Iteration 30, loss = 0.54770403\n",
      "Iteration 509, loss = 0.34579692\n",
      "Iteration 497, loss = 0.34624050\n",
      "Iteration 31, loss = 0.54450451\n",
      "Iteration 925, loss = 0.29915905\n",
      "Iteration 387, loss = 0.36618835\n",
      "Iteration 510, loss = 0.34563181\n",
      "Iteration 32, loss = 0.54131960\n",
      "Iteration 498, loss = 0.34614611\n",
      "Iteration 33, loss = 0.53813091\n",
      "Iteration 926, loss = 0.29906820\n",
      "Iteration 511, loss = 0.34550135\n",
      "Iteration 499, loss = 0.34604434\n",
      "Iteration 34, loss = 0.53514376\n",
      "Iteration 174, loss = 0.44213886\n",
      "Iteration 186, loss = 0.43688390\n",
      "Iteration 35, loss = 0.53218194\n",
      "Iteration 927, loss = 0.29897754\n",
      "Iteration 512, loss = 0.34528267\n",
      "Iteration 388, loss = 0.36607392\n",
      "Iteration 36, loss = 0.52921253\n",
      "Iteration 928, loss = 0.29890488\n",
      "Iteration 513, loss = 0.34515931\n",
      "Iteration 929, loss = 0.29878022\n",
      "Iteration 37, loss = 0.52641659\n",
      "Iteration 38, loss = 0.52363232\n",
      "Iteration 514, loss = 0.34493557\n",
      "Iteration 500, loss = 0.34594981\n",
      "Iteration 930, loss = 0.29868958\n",
      "Iteration 62, loss = 0.69379548\n",
      "Iteration 39, loss = 0.52100097\n",
      "Iteration 515, loss = 0.34478707\n",
      "Iteration 389, loss = 0.36594271\n",
      "Iteration 175, loss = 0.44150035\n",
      "Iteration 40, loss = 0.51827721\n",
      "Iteration 931, loss = 0.29862287\n",
      "Iteration 516, loss = 0.34462470\n",
      "Iteration 187, loss = 0.43639420\n",
      "Iteration 41, loss = 0.51579412\n",
      "Iteration 932, loss = 0.29852141\n",
      "Iteration 501, loss = 0.34586446\n",
      "Iteration 517, loss = 0.34447763\n",
      "Iteration 42, loss = 0.51316468\n",
      "Iteration 43, loss = 0.51067792\n",
      "Iteration 518, loss = 0.34429766\n",
      "Iteration 933, loss = 0.29841665\n",
      "Iteration 44, loss = 0.50834703\n",
      "Iteration 390, loss = 0.36584458\n",
      "Iteration 519, loss = 0.34413324\n",
      "Iteration 45, loss = 0.50589454\n",
      "Iteration 176, loss = 0.44083077\n",
      "Iteration 502, loss = 0.34576062\n",
      "Iteration 934, loss = 0.29834874\n",
      "Iteration 520, loss = 0.34395271\n",
      "Iteration 46, loss = 0.50358147\n",
      "Iteration 47, loss = 0.50135506\n",
      "Iteration 521, loss = 0.34388102\n",
      "Iteration 48, loss = 0.49909340\n",
      "Iteration 935, loss = 0.29825683\n",
      "Iteration 49, loss = 0.49692532\n",
      "Iteration 177, loss = 0.44019474\n",
      "Iteration 391, loss = 0.36565798\n",
      "Iteration 188, loss = 0.43592952\n",
      "Iteration 503, loss = 0.34567888\n",
      "Iteration 63, loss = 0.69118974\n",
      "Iteration 504, loss = 0.34556099\n",
      "Iteration 189, loss = 0.43549102\n",
      "Iteration 522, loss = 0.34360750\n",
      "Iteration 936, loss = 0.29815506\n",
      "Iteration 505, loss = 0.34547838\n",
      "Iteration 50, loss = 0.49476751\n",
      "Iteration 523, loss = 0.34348516\n",
      "Iteration 64, loss = 0.68845030\n",
      "Iteration 51, loss = 0.49271311\n",
      "Iteration 937, loss = 0.29806819\n",
      "Iteration 392, loss = 0.36553423\n",
      "Iteration 52, loss = 0.49065213\n",
      "Iteration 178, loss = 0.43952011\n",
      "Iteration 53, loss = 0.48865939\n",
      "Iteration 524, loss = 0.34333560\n",
      "Iteration 938, loss = 0.29796155\n",
      "Iteration 54, loss = 0.48670413\n",
      "Iteration 939, loss = 0.29786613\n",
      "Iteration 55, loss = 0.48477038\n",
      "Iteration 190, loss = 0.43503761\n",
      "Iteration 525, loss = 0.34314160\n",
      "Iteration 56, loss = 0.48290028\n",
      "Iteration 506, loss = 0.34537576\n",
      "Iteration 940, loss = 0.29778867\n",
      "Iteration 526, loss = 0.34298679\n",
      "Iteration 393, loss = 0.36537080\n",
      "Iteration 57, loss = 0.48094904\n",
      "Iteration 941, loss = 0.29768470\n",
      "Iteration 58, loss = 0.47913360\n",
      "Iteration 527, loss = 0.34281735\n",
      "Iteration 179, loss = 0.43891152\n",
      "Iteration 59, loss = 0.47745055\n",
      "Iteration 942, loss = 0.29758306\n",
      "Iteration 394, loss = 0.36526091\n",
      "Iteration 528, loss = 0.34264955\n",
      "Iteration 191, loss = 0.43461304\n",
      "Iteration 507, loss = 0.34527377\n",
      "Iteration 60, loss = 0.47563487\n",
      "Iteration 529, loss = 0.34252861\n",
      "Iteration 61, loss = 0.47388738\n",
      "Iteration 395, loss = 0.36511098\n",
      "Iteration 943, loss = 0.29752217\n",
      "Iteration 62, loss = 0.47220658\n",
      "Iteration 65, loss = 0.68573107\n",
      "Iteration 508, loss = 0.34517671\n",
      "Iteration 944, loss = 0.29745738\n",
      "Iteration 63, loss = 0.47053972\n",
      "Iteration 530, loss = 0.34236953\n",
      "Iteration 64, loss = 0.46895873\n",
      "Iteration 945, loss = 0.29732415\n",
      "Iteration 396, loss = 0.36497857\n",
      "Iteration 180, loss = 0.43822287\n",
      "Iteration 531, loss = 0.34220811\n",
      "Iteration 509, loss = 0.34509784\n",
      "Iteration 65, loss = 0.46727717\n",
      "Iteration 946, loss = 0.29724167\n",
      "Iteration 66, loss = 0.46580235\n",
      "Iteration 947, loss = 0.29713005\n",
      "Iteration 67, loss = 0.46424172\n",
      "Iteration 510, loss = 0.34498710\n",
      "Iteration 192, loss = 0.43416289\n",
      "Iteration 68, loss = 0.46273276\n",
      "Iteration 532, loss = 0.34200482\n",
      "Iteration 948, loss = 0.29706907\n",
      "Iteration 397, loss = 0.36487101\n",
      "Iteration 69, loss = 0.46124555\n",
      "Iteration 533, loss = 0.34183834\n",
      "Iteration 181, loss = 0.43767710\n",
      "Iteration 70, loss = 0.45977021\n",
      "Iteration 949, loss = 0.29693674\n",
      "Iteration 71, loss = 0.45831956\n",
      "Iteration 511, loss = 0.34490757\n",
      "Iteration 950, loss = 0.29691594\n",
      "Iteration 398, loss = 0.36472323\n",
      "Iteration 72, loss = 0.45692650\n",
      "Iteration 951, loss = 0.29677496\n",
      "Iteration 534, loss = 0.34170287\n",
      "Iteration 512, loss = 0.34479350\n",
      "Iteration 73, loss = 0.45554143\n",
      "Iteration 182, loss = 0.43708509\n",
      "Iteration 952, loss = 0.29670084\n",
      "Iteration 399, loss = 0.36465228\n",
      "Iteration 74, loss = 0.45416719\n",
      "Iteration 513, loss = 0.34472588\n",
      "Iteration 535, loss = 0.34151509\n",
      "Iteration 75, loss = 0.45282466\n",
      "Iteration 193, loss = 0.43370881\n",
      "Iteration 953, loss = 0.29657537\n",
      "Iteration 400, loss = 0.36449090\n",
      "Iteration 76, loss = 0.45154363\n",
      "Iteration 514, loss = 0.34459685\n",
      "Iteration 954, loss = 0.29648882\n",
      "Iteration 77, loss = 0.45022394\n",
      "Iteration 536, loss = 0.34145407\n",
      "Iteration 66, loss = 0.68302574\n",
      "Iteration 183, loss = 0.43643866\n",
      "Iteration 955, loss = 0.29638923\n",
      "Iteration 401, loss = 0.36439289\n",
      "Iteration 78, loss = 0.44899356\n",
      "Iteration 79, loss = 0.44770571\n",
      "Iteration 515, loss = 0.34449625Iteration 80, loss = 0.44645843\n",
      "\n",
      "Iteration 956, loss = 0.29628469\n",
      "Iteration 81, loss = 0.44525913\n",
      "Iteration 537, loss = 0.34120481\n",
      "Iteration 82, loss = 0.44408576\n",
      "Iteration 402, loss = 0.36420442\n",
      "Iteration 83, loss = 0.44286551\n",
      "Iteration 84, loss = 0.44178754\n",
      "Iteration 85, loss = 0.44055105\n",
      "Iteration 957, loss = 0.29622371\n",
      "Iteration 86, loss = 0.43948090\n",
      "Iteration 516, loss = 0.34439922\n",
      "Iteration 184, loss = 0.43585295\n",
      "Iteration 87, loss = 0.43836982\n",
      "Iteration 194, loss = 0.43334224\n",
      "Iteration 538, loss = 0.34103305\n",
      "Iteration 88, loss = 0.43727107\n",
      "Iteration 958, loss = 0.29612342\n",
      "Iteration 89, loss = 0.43619743\n",
      "Iteration 517, loss = 0.34431421\n",
      "Iteration 403, loss = 0.36408931\n",
      "Iteration 539, loss = 0.34085034\n",
      "Iteration 90, loss = 0.43518573\n",
      "Iteration 185, loss = 0.43530238\n",
      "Iteration 959, loss = 0.29602456\n",
      "Iteration 91, loss = 0.43410316\n",
      "Iteration 540, loss = 0.34069882\n",
      "Iteration 92, loss = 0.43310681\n",
      "Iteration 404, loss = 0.36399147\n",
      "Iteration 93, loss = 0.43207916\n",
      "Iteration 67, loss = 0.68037998Iteration 960, loss = 0.29591855\n",
      "\n",
      "Iteration 94, loss = 0.43109396\n",
      "Iteration 961, loss = 0.29583461\n",
      "Iteration 95, loss = 0.43014043\n",
      "Iteration 518, loss = 0.34420928\n",
      "Iteration 541, loss = 0.34062302\n",
      "Iteration 405, loss = 0.36381061\n",
      "Iteration 195, loss = 0.43284986\n",
      "Iteration 96, loss = 0.42910995\n",
      "Iteration 962, loss = 0.29573750\n",
      "Iteration 963, loss = 0.29567448\n",
      "Iteration 97, loss = 0.42820120\n",
      "Iteration 542, loss = 0.34040694\n",
      "Iteration 98, loss = 0.42724442\n",
      "Iteration 519, loss = 0.34411443\n",
      "Iteration 406, loss = 0.36370211\n",
      "Iteration 964, loss = 0.29555662\n",
      "Iteration 99, loss = 0.42634860\n",
      "Iteration 543, loss = 0.34021161\n",
      "Iteration 100, loss = 0.42542677\n",
      "Iteration 186, loss = 0.43469043\n",
      "Iteration 101, loss = 0.42452891\n",
      "Iteration 965, loss = 0.29548569\n",
      "Iteration 102, loss = 0.42363444\n",
      "Iteration 407, loss = 0.36356428\n",
      "Iteration 103, loss = 0.42277221\n",
      "Iteration 104, loss = 0.42191388\n",
      "Iteration 68, loss = 0.67756290\n",
      "Iteration 544, loss = 0.34004619\n",
      "Iteration 105, loss = 0.42108770\n",
      "Iteration 520, loss = 0.34402820\n",
      "Iteration 966, loss = 0.29535307\n",
      "Iteration 106, loss = 0.42022576\n",
      "Iteration 107, loss = 0.41938384\n",
      "Iteration 408, loss = 0.36345991\n",
      "Iteration 967, loss = 0.29525510\n",
      "Iteration 108, loss = 0.41858370\n",
      "Iteration 545, loss = 0.33988645\n",
      "Iteration 187, loss = 0.43413781\n",
      "Iteration 968, loss = 0.29519567\n",
      "Iteration 109, loss = 0.41779445\n",
      "Iteration 196, loss = 0.43244751\n",
      "Iteration 409, loss = 0.36331564\n",
      "Iteration 546, loss = 0.33972877\n",
      "Iteration 110, loss = 0.41697520\n",
      "Iteration 969, loss = 0.29507059\n",
      "Iteration 521, loss = 0.34392710\n",
      "Iteration 111, loss = 0.41622502\n",
      "Iteration 112, loss = 0.41544897\n",
      "Iteration 113, loss = 0.41468313\n",
      "Iteration 547, loss = 0.33969580\n",
      "Iteration 410, loss = 0.36318807\n",
      "Iteration 970, loss = 0.29504416\n",
      "Iteration 114, loss = 0.41391927\n",
      "Iteration 188, loss = 0.43355060\n",
      "Iteration 115, loss = 0.41316886\n",
      "Iteration 116, loss = 0.41242684\n",
      "Iteration 548, loss = 0.33943790\n",
      "Iteration 117, loss = 0.41171957\n",
      "Iteration 411, loss = 0.36302904\n",
      "Iteration 971, loss = 0.29489073\n",
      "Iteration 118, loss = 0.41102916\n",
      "Iteration 522, loss = 0.34383428\n",
      "Iteration 119, loss = 0.41031147\n",
      "Iteration 120, loss = 0.40962115\n",
      "Iteration 972, loss = 0.29478770\n",
      "Iteration 189, loss = 0.43302081\n",
      "Iteration 121, loss = 0.40892118\n",
      "Iteration 973, loss = 0.29473459\n",
      "Iteration 549, loss = 0.33929358\n",
      "Iteration 122, loss = 0.40824078\n",
      "Iteration 197, loss = 0.43198970\n",
      "Iteration 974, loss = 0.29463894\n",
      "Iteration 123, loss = 0.40756804\n",
      "Iteration 550, loss = 0.33912464\n",
      "Iteration 412, loss = 0.36292998\n",
      "Iteration 124, loss = 0.40691924\n",
      "Iteration 125, loss = 0.40625322\n",
      "Iteration 126, loss = 0.40561165\n",
      "Iteration 523, loss = 0.34374138\n",
      "Iteration 127, loss = 0.40498822\n",
      "Iteration 413, loss = 0.36284957\n",
      "Iteration 551, loss = 0.33893043\n",
      "Iteration 128, loss = 0.40435411\n",
      "Iteration 524, loss = 0.34364488\n",
      "Iteration 69, loss = 0.67478776\n",
      "Iteration 129, loss = 0.40372545\n",
      "Iteration 975, loss = 0.29452920\n",
      "Iteration 414, loss = 0.36267947\n",
      "Iteration 130, loss = 0.40310398\n",
      "Iteration 976, loss = 0.29442079\n",
      "Iteration 198, loss = 0.43160878\n",
      "Iteration 190, loss = 0.43242676\n",
      "Iteration 131, loss = 0.40249738\n",
      "Iteration 525, loss = 0.34354569\n",
      "Iteration 977, loss = 0.29438261\n",
      "Iteration 132, loss = 0.40187495\n",
      "Iteration 133, loss = 0.40131365\n",
      "Iteration 552, loss = 0.33881391\n",
      "Iteration 134, loss = 0.40070956\n",
      "Iteration 415, loss = 0.36254779\n",
      "Iteration 135, loss = 0.40013406\n",
      "Iteration 136, loss = 0.39957484\n",
      "Iteration 978, loss = 0.29425020\n",
      "Iteration 137, loss = 0.39900556\n",
      "Iteration 526, loss = 0.34347652\n",
      "Iteration 138, loss = 0.39841712\n",
      "Iteration 139, loss = 0.39789189\n",
      "Iteration 140, loss = 0.39734378\n",
      "Iteration 979, loss = 0.29415049\n",
      "Iteration 527, loss = 0.34335912\n",
      "Iteration 191, loss = 0.43192971\n",
      "Iteration 553, loss = 0.33859118\n",
      "Iteration 416, loss = 0.36241517\n",
      "Iteration 554, loss = 0.33842811\n",
      "Iteration 980, loss = 0.29408189\n",
      "Iteration 199, loss = 0.43115238\n",
      "Iteration 528, loss = 0.34326366\n",
      "Iteration 141, loss = 0.39677952\n",
      "Iteration 555, loss = 0.33832460\n",
      "Iteration 981, loss = 0.29395565\n",
      "Iteration 192, loss = 0.43137918\n",
      "Iteration 142, loss = 0.39625550\n",
      "Iteration 556, loss = 0.33816876\n",
      "Iteration 143, loss = 0.39573771\n",
      "Iteration 70, loss = 0.67193441\n",
      "Iteration 557, loss = 0.33796956\n",
      "Iteration 144, loss = 0.39521840\n",
      "Iteration 417, loss = 0.36228371\n",
      "Iteration 145, loss = 0.39471974\n",
      "Iteration 558, loss = 0.33780773\n",
      "Iteration 982, loss = 0.29387393\n",
      "Iteration 529, loss = 0.34317412\n",
      "Iteration 983, loss = 0.29381310\n",
      "Iteration 193, loss = 0.43087457\n",
      "Iteration 984, loss = 0.29369526\n",
      "Iteration 146, loss = 0.39418708\n",
      "Iteration 418, loss = 0.36217382\n",
      "Iteration 559, loss = 0.33764891\n",
      "Iteration 985, loss = 0.29362087\n",
      "Iteration 986, loss = 0.29354091\n",
      "Iteration 560, loss = 0.33747742\n",
      "Iteration 987, loss = 0.29344238\n",
      "Iteration 147, loss = 0.39367908\n",
      "Iteration 530, loss = 0.34308696\n",
      "Iteration 148, loss = 0.39317191\n",
      "Iteration 71, loss = 0.66915618\n",
      "Iteration 561, loss = 0.33732385\n",
      "Iteration 988, loss = 0.29333994\n",
      "Iteration 149, loss = 0.39266842\n",
      "Iteration 150, loss = 0.39219232\n",
      "Iteration 419, loss = 0.36205193\n",
      "Iteration 151, loss = 0.39171219\n",
      "Iteration 194, loss = 0.43030527\n",
      "Iteration 200, loss = 0.43075965\n",
      "Iteration 531, loss = 0.34300072\n",
      "Iteration 562, loss = 0.33721107\n",
      "Iteration 152, loss = 0.39126194\n",
      "Iteration 153, loss = 0.39076490\n",
      "Iteration 563, loss = 0.33711756\n",
      "Iteration 989, loss = 0.29324055\n",
      "Iteration 532, loss = 0.34289727\n",
      "Iteration 154, loss = 0.39026504\n",
      "Iteration 155, loss = 0.38983988\n",
      "Iteration 420, loss = 0.36190567\n",
      "Iteration 564, loss = 0.33689044\n",
      "Iteration 990, loss = 0.29315656\n",
      "Iteration 201, loss = 0.43036332\n",
      "Iteration 156, loss = 0.38937099\n",
      "Iteration 533, loss = 0.34279870\n",
      "Iteration 157, loss = 0.38889250\n",
      "Iteration 195, loss = 0.42981332\n",
      "Iteration 991, loss = 0.29309367\n",
      "Iteration 158, loss = 0.38846861\n",
      "Iteration 534, loss = 0.34271858\n",
      "Iteration 565, loss = 0.33669782\n",
      "Iteration 159, loss = 0.38802209\n",
      "Iteration 160, loss = 0.38756948\n",
      "Iteration 421, loss = 0.36179410\n",
      "Iteration 161, loss = 0.38713160\n",
      "Iteration 992, loss = 0.29302805\n",
      "Iteration 535, loss = 0.34261786\n",
      "Iteration 566, loss = 0.33655840\n",
      "Iteration 993, loss = 0.29288853\n",
      "Iteration 162, loss = 0.38672177\n",
      "Iteration 163, loss = 0.38628120\n",
      "Iteration 536, loss = 0.34252359\n",
      "Iteration 164, loss = 0.38587138\n",
      "Iteration 994, loss = 0.29280357Iteration 72, loss = 0.66630700\n",
      "\n",
      "Iteration 567, loss = 0.33638967\n",
      "Iteration 165, loss = 0.38543242Iteration 196, loss = 0.42931763\n",
      "\n",
      "Iteration 202, loss = 0.42993202\n",
      "Iteration 166, loss = 0.38503145\n",
      "Iteration 995, loss = 0.29273062\n",
      "Iteration 537, loss = 0.34245668\n",
      "Iteration 568, loss = 0.33624268\n",
      "Iteration 422, loss = 0.36169682\n",
      "Iteration 167, loss = 0.38461766\n",
      "Iteration 996, loss = 0.29261238\n",
      "Iteration 569, loss = 0.33608009\n",
      "Iteration 168, loss = 0.38419106\n",
      "Iteration 997, loss = 0.29255277\n",
      "Iteration 169, loss = 0.38381094\n",
      "Iteration 197, loss = 0.42878610\n",
      "Iteration 998, loss = 0.29245461\n",
      "Iteration 538, loss = 0.34232586\n",
      "Iteration 170, loss = 0.38342417\n",
      "Iteration 570, loss = 0.33593093\n",
      "Iteration 999, loss = 0.29235881\n",
      "Iteration 171, loss = 0.38300432\n",
      "Iteration 1000, loss = 0.29234924\n",
      "Iteration 423, loss = 0.36155674\n",
      "Iteration 571, loss = 0.33577015\n",
      "Iteration 172, loss = 0.38260977\n",
      "Iteration 1001, loss = 0.29218970\n",
      "Iteration 203, loss = 0.42954393\n",
      "Iteration 539, loss = 0.34223249\n",
      "Iteration 173, loss = 0.38224226\n",
      "Iteration 1002, loss = 0.29210919\n",
      "Iteration 174, loss = 0.38185250\n",
      "Iteration 1003, loss = 0.29201568\n",
      "Iteration 572, loss = 0.33562055\n",
      "Iteration 175, loss = 0.38146009\n",
      "Iteration 198, loss = 0.42825941\n",
      "Iteration 1004, loss = 0.29191916\n",
      "Iteration 424, loss = 0.36141550\n",
      "Iteration 540, loss = 0.34214295\n",
      "Iteration 204, loss = 0.42912675\n",
      "Iteration 176, loss = 0.38108298\n",
      "Iteration 177, loss = 0.38071265\n",
      "Iteration 573, loss = 0.33546482\n",
      "Iteration 178, loss = 0.38034618\n",
      "Iteration 1005, loss = 0.29186093\n",
      "Iteration 73, loss = 0.66351428\n",
      "Iteration 179, loss = 0.37997493\n",
      "Iteration 180, loss = 0.37963965\n",
      "Iteration 181, loss = 0.37928811\n",
      "Iteration 574, loss = 0.33532145\n",
      "Iteration 182, loss = 0.37889854\n",
      "Iteration 1006, loss = 0.29173715\n",
      "Iteration 183, loss = 0.37853870\n",
      "Iteration 205, loss = 0.42874413\n",
      "Iteration 541, loss = 0.34206624\n",
      "Iteration 184, loss = 0.37819952\n",
      "Iteration 425, loss = 0.36129209\n",
      "Iteration 199, loss = 0.42775350\n",
      "Iteration 1007, loss = 0.29166148\n",
      "Iteration 185, loss = 0.37784710\n",
      "Iteration 575, loss = 0.33519561\n",
      "Iteration 542, loss = 0.34197218\n",
      "Iteration 186, loss = 0.37749525\n",
      "Iteration 187, loss = 0.37716383\n",
      "Iteration 200, loss = 0.42726337\n",
      "Iteration 1008, loss = 0.29155462\n",
      "Iteration 576, loss = 0.33501051\n",
      "Iteration 426, loss = 0.36117970\n",
      "Iteration 188, loss = 0.37683078\n",
      "Iteration 543, loss = 0.34188246\n",
      "Iteration 189, loss = 0.37650942\n",
      "Iteration 1009, loss = 0.29148254\n",
      "Iteration 427, loss = 0.36104636\n",
      "Iteration 201, loss = 0.42679758\n",
      "Iteration 577, loss = 0.33486533\n",
      "Iteration 544, loss = 0.34178203\n",
      "Iteration 428, loss = 0.36092733\n",
      "Iteration 545, loss = 0.34167439\n",
      "Iteration 190, loss = 0.37616082\n",
      "Iteration 74, loss = 0.66064086\n",
      "Iteration 429, loss = 0.36077788\n",
      "Iteration 578, loss = 0.33471731\n",
      "Iteration 202, loss = 0.42631719\n",
      "Iteration 1010, loss = 0.29138091\n",
      "Iteration 579, loss = 0.33454134\n",
      "Iteration 191, loss = 0.37583757\n",
      "Iteration 546, loss = 0.34159196\n",
      "Iteration 430, loss = 0.36067879\n",
      "Iteration 580, loss = 0.33440318\n",
      "Iteration 192, loss = 0.37549094\n",
      "Iteration 206, loss = 0.42832079\n",
      "Iteration 581, loss = 0.33428307\n",
      "Iteration 431, loss = 0.36054295\n",
      "Iteration 193, loss = 0.37520759\n",
      "Iteration 1011, loss = 0.29129761\n",
      "Iteration 194, loss = 0.37487483\n",
      "Iteration 195, loss = 0.37453763\n",
      "Iteration 547, loss = 0.34152039\n",
      "Iteration 196, loss = 0.37422050\n",
      "Iteration 1012, loss = 0.29123620\n",
      "Iteration 197, loss = 0.37392272\n",
      "Iteration 582, loss = 0.33409209\n",
      "Iteration 198, loss = 0.37362320\n",
      "Iteration 203, loss = 0.42582370\n",
      "Iteration 199, loss = 0.37331290\n",
      "Iteration 583, loss = 0.33392324\n",
      "Iteration 200, loss = 0.37302545\n",
      "Iteration 1013, loss = 0.29111790\n",
      "Iteration 201, loss = 0.37268828\n",
      "Iteration 432, loss = 0.36045693\n",
      "Iteration 202, loss = 0.37238962\n",
      "Iteration 584, loss = 0.33376651\n",
      "Iteration 1014, loss = 0.29102444\n",
      "Iteration 203, loss = 0.37210951\n",
      "Iteration 548, loss = 0.34141085\n",
      "Iteration 585, loss = 0.33362054\n",
      "Iteration 204, loss = 0.37181289\n",
      "Iteration 204, loss = 0.42541511\n",
      "Iteration 1015, loss = 0.29094860\n",
      "Iteration 205, loss = 0.37150784\n",
      "Iteration 586, loss = 0.33346892\n",
      "Iteration 206, loss = 0.37124000\n",
      "Iteration 433, loss = 0.36033428\n",
      "Iteration 587, loss = 0.33333710\n",
      "Iteration 207, loss = 0.37093557Iteration 207, loss = 0.42796414\n",
      "\n",
      "Iteration 1016, loss = 0.29085587\n",
      "Iteration 208, loss = 0.37066386\n",
      "Iteration 209, loss = 0.37038385\n",
      "Iteration 588, loss = 0.33315254\n",
      "Iteration 210, loss = 0.37009109\n",
      "Iteration 1017, loss = 0.29075549\n",
      "Iteration 75, loss = 0.65774835\n",
      "Iteration 211, loss = 0.36982531\n",
      "Iteration 434, loss = 0.36016548\n",
      "Iteration 208, loss = 0.42760348\n",
      "Iteration 1018, loss = 0.29073187\n",
      "Iteration 589, loss = 0.33300750\n",
      "Iteration 205, loss = 0.42489296\n",
      "Iteration 212, loss = 0.36957042\n",
      "Iteration 213, loss = 0.36926916\n",
      "Iteration 1019, loss = 0.29059363\n",
      "Iteration 214, loss = 0.36900229\n",
      "Iteration 215, loss = 0.36874486\n",
      "Iteration 1020, loss = 0.29050726\n",
      "Iteration 216, loss = 0.36847035\n",
      "Iteration 549, loss = 0.34131036\n",
      "Iteration 1021, loss = 0.29044222\n",
      "Iteration 217, loss = 0.36820433\n",
      "Iteration 435, loss = 0.36008437\n",
      "Iteration 1022, loss = 0.29036027\n",
      "Iteration 218, loss = 0.36795046\n",
      "Iteration 1023, loss = 0.29024521\n",
      "Iteration 590, loss = 0.33284554\n",
      "Iteration 219, loss = 0.36767550\n",
      "Iteration 591, loss = 0.33270078\n",
      "Iteration 550, loss = 0.34121489\n",
      "Iteration 220, loss = 0.36742265\n",
      "Iteration 206, loss = 0.42442943\n",
      "Iteration 592, loss = 0.33256964\n",
      "Iteration 1024, loss = 0.29014890Iteration 221, loss = 0.36717724\n",
      "\n",
      "Iteration 209, loss = 0.42714603\n",
      "Iteration 222, loss = 0.36692343\n",
      "Iteration 551, loss = 0.34114414\n",
      "Iteration 593, loss = 0.33240492\n",
      "Iteration 223, loss = 0.36666568\n",
      "Iteration 76, loss = 0.65486483\n",
      "Iteration 1025, loss = 0.29009612\n",
      "Iteration 224, loss = 0.36639962\n",
      "Iteration 552, loss = 0.34104087\n",
      "Iteration 594, loss = 0.33223578\n",
      "Iteration 225, loss = 0.36615388\n",
      "Iteration 226, loss = 0.36590998\n",
      "Iteration 227, loss = 0.36566020\n",
      "Iteration 1026, loss = 0.28997146\n",
      "Iteration 436, loss = 0.35992733\n",
      "Iteration 595, loss = 0.33212946\n",
      "Iteration 228, loss = 0.36541891\n",
      "Iteration 553, loss = 0.34094386\n",
      "Iteration 207, loss = 0.42399821\n",
      "Iteration 229, loss = 0.36515718\n",
      "Iteration 596, loss = 0.33196584\n",
      "Iteration 554, loss = 0.34084879\n",
      "Iteration 1027, loss = 0.28996526\n",
      "Iteration 230, loss = 0.36492307\n",
      "Iteration 597, loss = 0.33181704\n",
      "Iteration 437, loss = 0.35980682\n",
      "Iteration 231, loss = 0.36469969\n",
      "Iteration 210, loss = 0.42678017\n",
      "Iteration 232, loss = 0.36444982\n",
      "Iteration 555, loss = 0.34075921\n",
      "Iteration 1028, loss = 0.28980667\n",
      "Iteration 598, loss = 0.33160635\n",
      "Iteration 233, loss = 0.36420508\n",
      "Iteration 208, loss = 0.42353553\n",
      "Iteration 556, loss = 0.34067784\n",
      "Iteration 234, loss = 0.36397228\n",
      "Iteration 599, loss = 0.33149294\n",
      "Iteration 1029, loss = 0.28971747\n",
      "Iteration 438, loss = 0.35969642\n",
      "Iteration 235, loss = 0.36375001\n",
      "Iteration 236, loss = 0.36352041\n",
      "Iteration 557, loss = 0.34057656\n",
      "Iteration 600, loss = 0.33131600\n",
      "Iteration 1030, loss = 0.28963847\n",
      "Iteration 237, loss = 0.36327373\n",
      "Iteration 238, loss = 0.36304749\n",
      "Iteration 558, loss = 0.34049217\n",
      "Iteration 1031, loss = 0.28953354\n",
      "Iteration 239, loss = 0.36282987\n",
      "Iteration 439, loss = 0.35957642\n",
      "Iteration 240, loss = 0.36261150\n",
      "Iteration 559, loss = 0.34041931\n",
      "Iteration 209, loss = 0.42306193\n",
      "Iteration 211, loss = 0.42640605\n",
      "Iteration 1032, loss = 0.28945776\n",
      "Iteration 241, loss = 0.36237178\n",
      "Iteration 560, loss = 0.34032291\n",
      "Iteration 242, loss = 0.36213412\n",
      "Iteration 77, loss = 0.65189262\n",
      "Iteration 440, loss = 0.35942947\n",
      "Iteration 1033, loss = 0.28935013\n",
      "Iteration 210, loss = 0.42264646\n",
      "Iteration 561, loss = 0.34021885\n",
      "Iteration 243, loss = 0.36194567\n",
      "Iteration 244, loss = 0.36171684\n",
      "Iteration 245, loss = 0.36148738\n",
      "Iteration 246, loss = 0.36127946\n",
      "Iteration 1034, loss = 0.28929170\n",
      "Iteration 247, loss = 0.36106305\n",
      "Iteration 562, loss = 0.34012587\n",
      "Iteration 248, loss = 0.36084108\n",
      "Iteration 249, loss = 0.36063543\n",
      "Iteration 441, loss = 0.35931042\n",
      "Iteration 211, loss = 0.42218326\n",
      "Iteration 1035, loss = 0.28917165\n",
      "Iteration 563, loss = 0.34007589\n",
      "Iteration 250, loss = 0.36041851\n",
      "Iteration 1036, loss = 0.28912661\n",
      "Iteration 442, loss = 0.35920873\n",
      "Iteration 251, loss = 0.36021857\n",
      "Iteration 1037, loss = 0.28906494\n",
      "Iteration 564, loss = 0.33995218\n",
      "Iteration 252, loss = 0.36000172\n",
      "Iteration 1038, loss = 0.28890762\n",
      "Iteration 443, loss = 0.35918754\n",
      "Iteration 253, loss = 0.35978954\n",
      "Iteration 212, loss = 0.42605697\n",
      "Iteration 212, loss = 0.42179953\n",
      "Iteration 254, loss = 0.35958816\n",
      "Iteration 255, loss = 0.35938100\n",
      "Iteration 444, loss = 0.35896463\n",
      "Iteration 565, loss = 0.33985231\n",
      "Iteration 256, loss = 0.35918433\n",
      "Iteration 1039, loss = 0.28886566\n",
      "Iteration 257, loss = 0.35897254\n",
      "Iteration 258, loss = 0.35877255\n",
      "Iteration 78, loss = 0.64903150\n",
      "Iteration 259, loss = 0.35857016\n",
      "Iteration 566, loss = 0.33975898\n",
      "Iteration 260, loss = 0.35837522\n",
      "Iteration 1040, loss = 0.28874406\n",
      "Iteration 261, loss = 0.35817158\n",
      "Iteration 262, loss = 0.35798166\n",
      "Iteration 263, loss = 0.35779406\n",
      "Iteration 1041, loss = 0.28868446\n",
      "Iteration 445, loss = 0.35882828\n",
      "Iteration 213, loss = 0.42131619\n",
      "Iteration 264, loss = 0.35759667\n",
      "Iteration 601, loss = 0.33116823\n",
      "Iteration 265, loss = 0.35739767\n",
      "Iteration 213, loss = 0.42564052\n",
      "Iteration 266, loss = 0.35720111\n",
      "Iteration 267, loss = 0.35701300\n",
      "Iteration 567, loss = 0.33966019\n",
      "Iteration 1042, loss = 0.28857754\n",
      "Iteration 268, loss = 0.35682845\n",
      "Iteration 269, loss = 0.35665638\n",
      "Iteration 602, loss = 0.33103614Iteration 1043, loss = 0.28850212\n",
      "Iteration 446, loss = 0.35872788\n",
      "\n",
      "Iteration 270, loss = 0.35646504\n",
      "Iteration 79, loss = 0.64621322\n",
      "Iteration 1044, loss = 0.28838587\n",
      "Iteration 271, loss = 0.35626271\n",
      "Iteration 1045, loss = 0.28830465\n",
      "Iteration 214, loss = 0.42094549\n",
      "Iteration 272, loss = 0.35609128\n",
      "Iteration 568, loss = 0.33957087\n",
      "Iteration 273, loss = 0.35589925\n",
      "Iteration 274, loss = 0.35572266\n",
      "Iteration 603, loss = 0.33087130Iteration 1046, loss = 0.28821518\n",
      "Iteration 275, loss = 0.35553942\n",
      "\n",
      "Iteration 276, loss = 0.35535989\n",
      "Iteration 447, loss = 0.35858731\n",
      "Iteration 277, loss = 0.35517082\n",
      "Iteration 569, loss = 0.33949860\n",
      "Iteration 278, loss = 0.35499591\n",
      "Iteration 1047, loss = 0.28818156\n",
      "Iteration 214, loss = 0.42528936\n",
      "Iteration 279, loss = 0.35483635\n",
      "Iteration 448, loss = 0.35845662\n",
      "Iteration 604, loss = 0.33072507\n",
      "Iteration 280, loss = 0.35467486\n",
      "Iteration 215, loss = 0.42048181\n",
      "Iteration 281, loss = 0.35447603\n",
      "Iteration 1048, loss = 0.28807177\n",
      "Iteration 80, loss = 0.64325707\n",
      "Iteration 570, loss = 0.33939420\n",
      "Iteration 282, loss = 0.35432755\n",
      "Iteration 283, loss = 0.35412361\n",
      "Iteration 449, loss = 0.35836739\n",
      "Iteration 284, loss = 0.35395106\n",
      "Iteration 1049, loss = 0.28793337\n",
      "Iteration 285, loss = 0.35379913\n",
      "Iteration 571, loss = 0.33929086\n",
      "Iteration 1050, loss = 0.28782991\n",
      "Iteration 1051, loss = 0.28777372\n",
      "Iteration 216, loss = 0.42003573\n",
      "Iteration 1052, loss = 0.28768352\n",
      "Iteration 1053, loss = 0.28760061\n",
      "Iteration 286, loss = 0.35362219\n",
      "Iteration 450, loss = 0.35821295\n",
      "Iteration 1054, loss = 0.28749105\n",
      "Iteration 215, loss = 0.42491255\n",
      "Iteration 287, loss = 0.35346397\n",
      "Iteration 217, loss = 0.41960630\n",
      "Iteration 288, loss = 0.35327959\n",
      "Iteration 572, loss = 0.33921006\n",
      "Iteration 81, loss = 0.64031782\n",
      "Iteration 289, loss = 0.35312375\n",
      "Iteration 1055, loss = 0.28739380\n",
      "Iteration 290, loss = 0.35296373\n",
      "Iteration 291, loss = 0.35279646\n",
      "Iteration 292, loss = 0.35263500\n",
      "Iteration 573, loss = 0.33911660\n",
      "Iteration 293, loss = 0.35246420\n",
      "Iteration 451, loss = 0.35810012\n",
      "Iteration 1056, loss = 0.28736709\n",
      "Iteration 294, loss = 0.35231248\n",
      "Iteration 295, loss = 0.35213873\n",
      "Iteration 296, loss = 0.35201129\n",
      "Iteration 574, loss = 0.33901975\n",
      "Iteration 218, loss = 0.41925677\n",
      "Iteration 297, loss = 0.35182887\n",
      "Iteration 298, loss = 0.35167472\n",
      "Iteration 1057, loss = 0.28721118\n",
      "Iteration 452, loss = 0.35801143\n",
      "Iteration 299, loss = 0.35152864\n",
      "Iteration 575, loss = 0.33893392\n",
      "Iteration 300, loss = 0.35135901\n",
      "Iteration 1058, loss = 0.28714525\n",
      "Iteration 301, loss = 0.35119927\n",
      "Iteration 82, loss = 0.63745004\n",
      "Iteration 302, loss = 0.35104787\n",
      "Iteration 576, loss = 0.33885472\n",
      "Iteration 216, loss = 0.42454406\n",
      "Iteration 303, loss = 0.35090578\n",
      "Iteration 1059, loss = 0.28702539\n",
      "Iteration 304, loss = 0.35074449\n",
      "Iteration 453, loss = 0.35784710\n",
      "Iteration 1060, loss = 0.28700236\n",
      "Iteration 577, loss = 0.33874585\n",
      "Iteration 305, loss = 0.35059993\n",
      "Iteration 219, loss = 0.41889518\n",
      "Iteration 1061, loss = 0.28686357\n",
      "Iteration 306, loss = 0.35044880\n",
      "Iteration 578, loss = 0.33866106\n",
      "Iteration 307, loss = 0.35031404\n",
      "Iteration 1062, loss = 0.28681042\n",
      "Iteration 308, loss = 0.35015249\n",
      "Iteration 605, loss = 0.33058150Iteration 220, loss = 0.41839176\n",
      "Iteration 454, loss = 0.35775457\n",
      "Iteration 309, loss = 0.35001099\n",
      "Iteration 1063, loss = 0.28670112\n",
      "Iteration 579, loss = 0.33856947\n",
      "\n",
      "Iteration 310, loss = 0.34986537\n",
      "Iteration 311, loss = 0.34971916\n",
      "Iteration 312, loss = 0.34955627\n",
      "Iteration 217, loss = 0.42417537\n",
      "Iteration 455, loss = 0.35761521\n",
      "Iteration 313, loss = 0.34942996\n",
      "Iteration 1064, loss = 0.28659440\n",
      "Iteration 314, loss = 0.34927874\n",
      "Iteration 221, loss = 0.41799753\n",
      "Iteration 315, loss = 0.34913478\n",
      "Iteration 316, loss = 0.34899996\n",
      "Iteration 1065, loss = 0.28648735\n",
      "Iteration 83, loss = 0.63453804\n",
      "Iteration 317, loss = 0.34885610\n",
      "Iteration 456, loss = 0.35750141\n",
      "Iteration 1066, loss = 0.28642145\n",
      "Iteration 318, loss = 0.34872272\n",
      "Iteration 580, loss = 0.33847788\n",
      "Iteration 319, loss = 0.34858534\n",
      "Iteration 1067, loss = 0.28632845\n",
      "Iteration 320, loss = 0.34843344\n",
      "Iteration 606, loss = 0.33042971\n",
      "Iteration 321, loss = 0.34831406\n",
      "Iteration 322, loss = 0.34816789\n",
      "Iteration 1068, loss = 0.28623828\n",
      "Iteration 222, loss = 0.41758562\n",
      "Iteration 218, loss = 0.42382297\n",
      "Iteration 323, loss = 0.34804037\n",
      "Iteration 581, loss = 0.33838783\n",
      "Iteration 457, loss = 0.35737020\n",
      "Iteration 324, loss = 0.34788199\n",
      "Iteration 1069, loss = 0.28613265\n",
      "Iteration 84, loss = 0.63167492\n",
      "Iteration 325, loss = 0.34776030\n",
      "Iteration 223, loss = 0.41720739\n",
      "Iteration 1070, loss = 0.28603986\n",
      "Iteration 607, loss = 0.33028998\n",
      "Iteration 326, loss = 0.34762532\n",
      "Iteration 1071, loss = 0.28595524\n",
      "Iteration 327, loss = 0.34749190\n",
      "Iteration 328, loss = 0.34735779\n",
      "Iteration 1072, loss = 0.28585817\n",
      "Iteration 329, loss = 0.34721917\n",
      "Iteration 582, loss = 0.33831941\n",
      "Iteration 330, loss = 0.34709250\n",
      "Iteration 458, loss = 0.35724746\n",
      "Iteration 1073, loss = 0.28580290\n",
      "Iteration 331, loss = 0.34695392\n",
      "Iteration 332, loss = 0.34682442\n",
      "Iteration 219, loss = 0.42344999\n",
      "Iteration 1074, loss = 0.28568106\n",
      "Iteration 333, loss = 0.34671526\n",
      "Iteration 334, loss = 0.34656740\n",
      "Iteration 1075, loss = 0.28564360\n",
      "Iteration 335, loss = 0.34644303\n",
      "Iteration 608, loss = 0.33018930\n",
      "Iteration 336, loss = 0.34630976\n",
      "Iteration 1076, loss = 0.28551486\n",
      "Iteration 337, loss = 0.34619652\n",
      "Iteration 338, loss = 0.34606660\n",
      "Iteration 583, loss = 0.33820118\n",
      "Iteration 339, loss = 0.34595026\n",
      "Iteration 1077, loss = 0.28543258\n",
      "Iteration 340, loss = 0.34581455\n",
      "Iteration 224, loss = 0.41679348\n",
      "Iteration 85, loss = 0.62879047\n",
      "Iteration 459, loss = 0.35712846\n",
      "Iteration 609, loss = 0.32995838\n",
      "Iteration 341, loss = 0.34569378Iteration 1078, loss = 0.28533681\n",
      "\n",
      "Iteration 1079, loss = 0.28525351\n",
      "Iteration 584, loss = 0.33811383Iteration 342, loss = 0.34555417\n",
      "\n",
      "Iteration 460, loss = 0.35703738\n",
      "Iteration 1080, loss = 0.28520769\n",
      "Iteration 343, loss = 0.34544966\n",
      "Iteration 610, loss = 0.32981062\n",
      "Iteration 225, loss = 0.41643298\n",
      "Iteration 1081, loss = 0.28506098\n",
      "Iteration 344, loss = 0.34534478\n",
      "Iteration 220, loss = 0.42309664\n",
      "Iteration 1082, loss = 0.28500352\n",
      "Iteration 461, loss = 0.35689388\n",
      "Iteration 345, loss = 0.34519406\n",
      "Iteration 585, loss = 0.33801956\n",
      "Iteration 611, loss = 0.32964722Iteration 226, loss = 0.41605115\n",
      "\n",
      "Iteration 346, loss = 0.34507999\n",
      "Iteration 1083, loss = 0.28489793\n",
      "Iteration 347, loss = 0.34496405\n",
      "Iteration 1084, loss = 0.28479712\n",
      "Iteration 348, loss = 0.34483386\n",
      "Iteration 462, loss = 0.35680629\n",
      "Iteration 349, loss = 0.34472768\n",
      "Iteration 350, loss = 0.34461155\n",
      "Iteration 612, loss = 0.32958429\n",
      "Iteration 227, loss = 0.41573348\n",
      "Iteration 1085, loss = 0.28469696\n",
      "Iteration 351, loss = 0.34448370\n",
      "Iteration 586, loss = 0.33795699\n",
      "Iteration 1086, loss = 0.28460802\n",
      "Iteration 463, loss = 0.35665890\n",
      "Iteration 352, loss = 0.34438436\n",
      "Iteration 1087, loss = 0.28455544\n",
      "Iteration 613, loss = 0.32935596\n",
      "Iteration 353, loss = 0.34423909\n",
      "Iteration 354, loss = 0.34413701\n",
      "Iteration 228, loss = 0.41530593\n",
      "Iteration 355, loss = 0.34402349\n",
      "Iteration 356, loss = 0.34391158\n",
      "Iteration 464, loss = 0.35651157\n",
      "Iteration 357, loss = 0.34379719\n",
      "Iteration 358, loss = 0.34368587\n",
      "Iteration 221, loss = 0.42273617\n",
      "Iteration 86, loss = 0.62583044\n",
      "Iteration 465, loss = 0.35641404\n",
      "Iteration 1088, loss = 0.28453631\n",
      "Iteration 359, loss = 0.34356772\n",
      "Iteration 587, loss = 0.33785932\n",
      "Iteration 360, loss = 0.34344819\n",
      "Iteration 614, loss = 0.32919442\n",
      "Iteration 361, loss = 0.34334553\n",
      "Iteration 466, loss = 0.35638089\n",
      "Iteration 362, loss = 0.34322815\n",
      "Iteration 363, loss = 0.34311247\n",
      "Iteration 222, loss = 0.42238911\n",
      "Iteration 467, loss = 0.35615695\n",
      "Iteration 1089, loss = 0.28435544\n",
      "Iteration 364, loss = 0.34301351\n",
      "Iteration 615, loss = 0.32906471\n",
      "Iteration 468, loss = 0.35608058\n",
      "Iteration 1090, loss = 0.28427330\n",
      "Iteration 365, loss = 0.34289410\n",
      "Iteration 588, loss = 0.33776925\n",
      "Iteration 87, loss = 0.62300927\n",
      "Iteration 229, loss = 0.41505121\n",
      "Iteration 616, loss = 0.32889976\n",
      "Iteration 1091, loss = 0.28419836\n",
      "Iteration 469, loss = 0.35592075\n",
      "Iteration 223, loss = 0.42208269\n",
      "Iteration 1092, loss = 0.28407748\n",
      "Iteration 366, loss = 0.34279295\n",
      "Iteration 589, loss = 0.33770100\n",
      "Iteration 367, loss = 0.34268624Iteration 1093, loss = 0.28399036\n",
      "Iteration 470, loss = 0.35581013\n",
      "\n",
      "Iteration 368, loss = 0.34257220\n",
      "Iteration 224, loss = 0.42172159\n",
      "Iteration 369, loss = 0.34246675\n",
      "Iteration 617, loss = 0.32880059\n",
      "Iteration 88, loss = 0.62014505\n",
      "Iteration 370, loss = 0.34237022\n",
      "Iteration 1094, loss = 0.28391884\n",
      "Iteration 590, loss = 0.33759695\n",
      "Iteration 371, loss = 0.34225869\n",
      "Iteration 372, loss = 0.34215322\n",
      "Iteration 230, loss = 0.41458142\n",
      "Iteration 1095, loss = 0.28384295\n",
      "Iteration 618, loss = 0.32859188\n",
      "Iteration 373, loss = 0.34203639\n",
      "Iteration 591, loss = 0.33749650\n",
      "Iteration 374, loss = 0.34193605\n",
      "Iteration 1096, loss = 0.28376943\n",
      "Iteration 619, loss = 0.32849085\n",
      "Iteration 375, loss = 0.34182551\n",
      "Iteration 1097, loss = 0.28363359\n",
      "Iteration 376, loss = 0.34173660\n",
      "Iteration 592, loss = 0.33742084\n",
      "Iteration 620, loss = 0.32835177\n",
      "Iteration 377, loss = 0.34162535\n",
      "Iteration 89, loss = 0.61727993\n",
      "Iteration 1098, loss = 0.28358166\n",
      "Iteration 378, loss = 0.34152948\n",
      "Iteration 379, loss = 0.34142003\n",
      "Iteration 621, loss = 0.32816679\n",
      "Iteration 471, loss = 0.35570360\n",
      "Iteration 225, loss = 0.42134592\n",
      "Iteration 231, loss = 0.41420577\n",
      "Iteration 1099, loss = 0.28348899\n",
      "Iteration 380, loss = 0.34131563\n",
      "Iteration 1100, loss = 0.28338248\n",
      "Iteration 593, loss = 0.33731570\n",
      "Iteration 381, loss = 0.34122897\n",
      "Iteration 1101, loss = 0.28326867\n",
      "Iteration 622, loss = 0.32801320\n",
      "Iteration 382, loss = 0.34110805\n",
      "Iteration 1102, loss = 0.28317709\n",
      "Iteration 232, loss = 0.41385226\n",
      "Iteration 383, loss = 0.34100942\n",
      "Iteration 472, loss = 0.35559546\n",
      "Iteration 1103, loss = 0.28311633\n",
      "Iteration 384, loss = 0.34091287\n",
      "Iteration 385, loss = 0.34081887\n",
      "Iteration 1104, loss = 0.28301399\n",
      "Iteration 226, loss = 0.42099967\n",
      "Iteration 594, loss = 0.33724391\n",
      "Iteration 1105, loss = 0.28292285\n",
      "Iteration 233, loss = 0.41350587\n",
      "Iteration 1106, loss = 0.28282343\n",
      "Iteration 386, loss = 0.34070540\n",
      "Iteration 1107, loss = 0.28277101\n",
      "Iteration 623, loss = 0.32788457\n",
      "Iteration 90, loss = 0.61441528\n",
      "Iteration 234, loss = 0.41319618\n",
      "Iteration 1108, loss = 0.28264756\n",
      "Iteration 387, loss = 0.34062101\n",
      "Iteration 473, loss = 0.35546730\n",
      "Iteration 388, loss = 0.34051774\n",
      "Iteration 624, loss = 0.32770826\n",
      "Iteration 1109, loss = 0.28259362\n",
      "Iteration 595, loss = 0.33714472\n",
      "Iteration 389, loss = 0.34041732\n",
      "Iteration 625, loss = 0.32753614\n",
      "Iteration 390, loss = 0.34031816\n",
      "Iteration 235, loss = 0.41278677\n",
      "Iteration 391, loss = 0.34022419\n",
      "Iteration 1110, loss = 0.28249691\n",
      "Iteration 596, loss = 0.33706003\n",
      "Iteration 227, loss = 0.42065592\n",
      "Iteration 392, loss = 0.34014101\n",
      "Iteration 626, loss = 0.32739956\n",
      "Iteration 474, loss = 0.35532226\n",
      "Iteration 393, loss = 0.34002831\n",
      "Iteration 597, loss = 0.33697945\n",
      "Iteration 1111, loss = 0.28239281\n",
      "Iteration 394, loss = 0.33993977\n",
      "Iteration 1112, loss = 0.28230134\n",
      "Iteration 627, loss = 0.32730833\n",
      "Iteration 395, loss = 0.33984543\n",
      "Iteration 475, loss = 0.35521088\n",
      "Iteration 396, loss = 0.33975210\n",
      "Iteration 598, loss = 0.33688627\n",
      "Iteration 236, loss = 0.41243037\n",
      "Iteration 628, loss = 0.32717149\n",
      "Iteration 397, loss = 0.33964964\n",
      "Iteration 1113, loss = 0.28223041\n",
      "Iteration 629, loss = 0.32695283\n",
      "Iteration 398, loss = 0.33955043\n",
      "Iteration 599, loss = 0.33679567\n",
      "Iteration 476, loss = 0.35512745\n",
      "Iteration 228, loss = 0.42034135\n",
      "Iteration 399, loss = 0.33946489\n",
      "Iteration 630, loss = 0.32677220\n",
      "Iteration 1114, loss = 0.28212653\n",
      "Iteration 600, loss = 0.33670464\n",
      "Iteration 400, loss = 0.33936112\n",
      "Iteration 477, loss = 0.35496491\n",
      "Iteration 631, loss = 0.32667808\n",
      "Iteration 91, loss = 0.61176787Iteration 401, loss = 0.33927866\n",
      "\n",
      "Iteration 1115, loss = 0.28206831\n",
      "Iteration 402, loss = 0.33917621\n",
      "Iteration 601, loss = 0.33661680\n",
      "Iteration 403, loss = 0.33908126\n",
      "Iteration 478, loss = 0.35484667\n",
      "Iteration 404, loss = 0.33899020\n",
      "Iteration 632, loss = 0.32648997\n",
      "Iteration 602, loss = 0.33652779\n",
      "Iteration 237, loss = 0.41208591\n",
      "Iteration 405, loss = 0.33889882\n",
      "Iteration 1116, loss = 0.28193518\n",
      "Iteration 406, loss = 0.33882309\n",
      "Iteration 479, loss = 0.35474864\n",
      "Iteration 407, loss = 0.33871535\n",
      "Iteration 633, loss = 0.32632423\n",
      "Iteration 408, loss = 0.33862047\n",
      "Iteration 238, loss = 0.41172426\n",
      "Iteration 229, loss = 0.41999117\n",
      "Iteration 634, loss = 0.32627053\n",
      "Iteration 603, loss = 0.33643760\n",
      "Iteration 1117, loss = 0.28183695Iteration 409, loss = 0.33853776\n",
      "\n",
      "Iteration 480, loss = 0.35469257\n",
      "Iteration 410, loss = 0.33844400\n",
      "Iteration 604, loss = 0.33634991\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 92, loss = 0.60891607\n",
      "Iteration 411, loss = 0.33836355\n",
      "Iteration 239, loss = 0.41138884\n",
      "Iteration 635, loss = 0.32603253\n",
      "Iteration 412, loss = 0.33826709\n",
      "Iteration 1118, loss = 0.28175474\n",
      "Iteration 636, loss = 0.32592578\n",
      "Iteration 413, loss = 0.33816276\n",
      "Iteration 240, loss = 0.41106670\n",
      "Iteration 414, loss = 0.33808465\n",
      "Iteration 481, loss = 0.35449851\n",
      "Iteration 637, loss = 0.32574939\n",
      "Iteration 415, loss = 0.33799444\n",
      "Iteration 416, loss = 0.33789732\n",
      "Iteration 230, loss = 0.41970898\n",
      "Iteration 638, loss = 0.32557993\n",
      "Iteration 417, loss = 0.33783444\n",
      "Iteration 418, loss = 0.33772279\n",
      "Iteration 419, loss = 0.33763925\n",
      "Iteration 482, loss = 0.35439536\n",
      "Iteration 639, loss = 0.32546275\n",
      "Iteration 241, loss = 0.41079003\n",
      "Iteration 420, loss = 0.33754229\n",
      "Iteration 640, loss = 0.32528988\n",
      "Iteration 93, loss = 0.60619516\n",
      "Iteration 421, loss = 0.33746366\n",
      "Iteration 641, loss = 0.32518420\n",
      "Iteration 422, loss = 0.33737279\n",
      "Iteration 231, loss = 0.41932489\n",
      "Iteration 423, loss = 0.33729466\n",
      "Iteration 642, loss = 0.32500525\n",
      "Iteration 424, loss = 0.33719882\n",
      "Iteration 1, loss = 0.73862838\n",
      "Iteration 643, loss = 0.32486130\n",
      "Iteration 242, loss = 0.41034722\n",
      "Iteration 483, loss = 0.35427734\n",
      "Iteration 425, loss = 0.33711411\n",
      "Iteration 2, loss = 0.73369613\n",
      "Iteration 644, loss = 0.32473655\n",
      "Iteration 426, loss = 0.33702703\n",
      "Iteration 3, loss = 0.72627431\n",
      "Iteration 427, loss = 0.33694452\n",
      "Iteration 645, loss = 0.32459207\n",
      "Iteration 484, loss = 0.35418103\n",
      "Iteration 232, loss = 0.41903924\n",
      "Iteration 428, loss = 0.33686704\n",
      "Iteration 429, loss = 0.33677336\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 646, loss = 0.32452431\n",
      "Iteration 4, loss = 0.71722159\n",
      "Iteration 5, loss = 0.70725971\n",
      "Iteration 243, loss = 0.41003647\n",
      "Iteration 6, loss = 0.69715544\n",
      "Iteration 485, loss = 0.35404028\n",
      "Iteration 233, loss = 0.41868464\n",
      "Iteration 647, loss = 0.32429926\n",
      "Iteration 7, loss = 0.68666333\n",
      "Iteration 8, loss = 0.67643203\n",
      "Iteration 9, loss = 0.66665539\n",
      "Iteration 1, loss = 0.77703499\n",
      "Iteration 10, loss = 0.65749110\n",
      "Iteration 2, loss = 0.77307936\n",
      "Iteration 648, loss = 0.32417897\n",
      "Iteration 11, loss = 0.64813777\n",
      "Iteration 3, loss = 0.76718184\n",
      "Iteration 12, loss = 0.63935582\n",
      "Iteration 486, loss = 0.35389791\n",
      "Iteration 4, loss = 0.76020281\n",
      "Iteration 94, loss = 0.60348755\n",
      "Iteration 13, loss = 0.63148407\n",
      "Iteration 234, loss = 0.41835752\n",
      "Iteration 5, loss = 0.75224591\n",
      "Iteration 14, loss = 0.62368170\n",
      "Iteration 6, loss = 0.74425579\n",
      "Iteration 649, loss = 0.32404342\n",
      "Iteration 15, loss = 0.61655281\n",
      "Iteration 7, loss = 0.73580967\n",
      "Iteration 244, loss = 0.40972519\n",
      "Iteration 16, loss = 0.60956274\n",
      "Iteration 8, loss = 0.72793513\n",
      "Iteration 17, loss = 0.60302784\n",
      "Iteration 9, loss = 0.71971774\n",
      "Iteration 18, loss = 0.59676014\n",
      "Iteration 650, loss = 0.32386559\n",
      "Iteration 19, loss = 0.59065289\n",
      "Iteration 10, loss = 0.71188218\n",
      "Iteration 20, loss = 0.58520548\n",
      "Iteration 487, loss = 0.35380381\n",
      "Iteration 651, loss = 0.32376021\n",
      "Iteration 11, loss = 0.70484270\n",
      "Iteration 21, loss = 0.57983858\n",
      "Iteration 245, loss = 0.40936461\n",
      "Iteration 12, loss = 0.69764130\n",
      "Iteration 95, loss = 0.60079283\n",
      "Iteration 22, loss = 0.57476758\n",
      "Iteration 235, loss = 0.41808568\n",
      "Iteration 23, loss = 0.56975749\n",
      "Iteration 13, loss = 0.69088988\n",
      "Iteration 652, loss = 0.32360267\n",
      "Iteration 24, loss = 0.56494007\n",
      "Iteration 14, loss = 0.68416217\n",
      "Iteration 246, loss = 0.40906061\n",
      "Iteration 488, loss = 0.35366676\n",
      "Iteration 653, loss = 0.32343350\n",
      "Iteration 15, loss = 0.67791431\n",
      "Iteration 25, loss = 0.56029514\n",
      "Iteration 26, loss = 0.55605142\n",
      "Iteration 654, loss = 0.32330076\n",
      "Iteration 16, loss = 0.67198220\n",
      "Iteration 27, loss = 0.55180333\n",
      "Iteration 489, loss = 0.35354418\n",
      "Iteration 28, loss = 0.54765890\n",
      "Iteration 17, loss = 0.66599362\n",
      "Iteration 29, loss = 0.54370566\n",
      "Iteration 655, loss = 0.32318661\n",
      "Iteration 18, loss = 0.66057259\n",
      "Iteration 247, loss = 0.40874550\n",
      "Iteration 30, loss = 0.53995528\n",
      "Iteration 19, loss = 0.65498748\n",
      "Iteration 31, loss = 0.53622076\n",
      "Iteration 20, loss = 0.64985423\n",
      "Iteration 32, loss = 0.53265575\n",
      "Iteration 236, loss = 0.41770776\n",
      "Iteration 21, loss = 0.64467121\n",
      "Iteration 22, loss = 0.63975686\n",
      "Iteration 490, loss = 0.35345867\n",
      "Iteration 656, loss = 0.32303508\n",
      "Iteration 33, loss = 0.52912881\n",
      "Iteration 1119, loss = 0.28166510\n",
      "Iteration 34, loss = 0.52577324Iteration 23, loss = 0.63493147\n",
      "\n",
      "Iteration 491, loss = 0.35336799\n",
      "Iteration 248, loss = 0.40840119\n",
      "Iteration 657, loss = 0.32289897\n",
      "Iteration 35, loss = 0.52258635\n",
      "Iteration 24, loss = 0.63036110\n",
      "Iteration 96, loss = 0.59821754\n",
      "Iteration 36, loss = 0.51940945\n",
      "Iteration 25, loss = 0.62572548\n",
      "Iteration 237, loss = 0.41741091\n",
      "Iteration 26, loss = 0.62126871\n",
      "Iteration 27, loss = 0.61707436\n",
      "Iteration 37, loss = 0.51633136\n",
      "Iteration 492, loss = 0.35322656\n",
      "Iteration 28, loss = 0.61261792\n",
      "Iteration 658, loss = 0.32274763\n",
      "Iteration 29, loss = 0.60855577\n",
      "Iteration 38, loss = 0.51333608\n",
      "Iteration 30, loss = 0.60441864\n",
      "Iteration 39, loss = 0.51039697\n",
      "Iteration 40, loss = 0.50757501\n",
      "Iteration 31, loss = 0.60057622\n",
      "Iteration 493, loss = 0.35307685\n",
      "Iteration 41, loss = 0.50482393\n",
      "Iteration 238, loss = 0.41711222\n",
      "Iteration 659, loss = 0.32261574\n",
      "Iteration 249, loss = 0.40810895\n",
      "Iteration 42, loss = 0.50214450\n",
      "Iteration 43, loss = 0.49951769\n",
      "Iteration 32, loss = 0.59668367\n",
      "Iteration 33, loss = 0.59282204\n",
      "Iteration 44, loss = 0.49698449\n",
      "Iteration 34, loss = 0.58907788\n",
      "Iteration 1120, loss = 0.28159150\n",
      "Iteration 660, loss = 0.32250508\n",
      "Iteration 35, loss = 0.58544090\n",
      "Iteration 494, loss = 0.35302341\n",
      "Iteration 45, loss = 0.49448152\n",
      "Iteration 1121, loss = 0.28148147\n",
      "Iteration 97, loss = 0.59552102\n",
      "Iteration 36, loss = 0.58175489\n",
      "Iteration 250, loss = 0.40777069\n",
      "Iteration 46, loss = 0.49209813\n",
      "Iteration 37, loss = 0.57821031\n",
      "Iteration 495, loss = 0.35286163\n",
      "Iteration 661, loss = 0.32232413\n",
      "Iteration 38, loss = 0.57475347\n",
      "Iteration 47, loss = 0.48968623\n",
      "Iteration 39, loss = 0.57132385\n",
      "Iteration 662, loss = 0.32232019\n",
      "Iteration 239, loss = 0.41681194\n",
      "Iteration 40, loss = 0.56786854\n",
      "Iteration 41, loss = 0.56463707\n",
      "Iteration 48, loss = 0.48740004\n",
      "Iteration 663, loss = 0.32210873\n",
      "Iteration 42, loss = 0.56132914\n",
      "Iteration 1122, loss = 0.28141269Iteration 251, loss = 0.40744066\n",
      "Iteration 49, loss = 0.48511605\n",
      "Iteration 43, loss = 0.55807953\n",
      "Iteration 664, loss = 0.32194444\n",
      "\n",
      "Iteration 50, loss = 0.48291114\n",
      "Iteration 44, loss = 0.55496081\n",
      "Iteration 665, loss = 0.32177663\n",
      "Iteration 496, loss = 0.35274145\n",
      "Iteration 51, loss = 0.48079941\n",
      "Iteration 52, loss = 0.47868015\n",
      "Iteration 45, loss = 0.55186094\n",
      "Iteration 252, loss = 0.40725038\n",
      "Iteration 53, loss = 0.47663868\n",
      "Iteration 46, loss = 0.54874436\n",
      "Iteration 666, loss = 0.32168700\n",
      "Iteration 98, loss = 0.59287283\n",
      "Iteration 54, loss = 0.47469149\n",
      "Iteration 47, loss = 0.54577431\n",
      "Iteration 55, loss = 0.47270054\n",
      "Iteration 48, loss = 0.54278831\n",
      "Iteration 667, loss = 0.32152716\n",
      "Iteration 56, loss = 0.47071882\n",
      "Iteration 49, loss = 0.53993054\n",
      "Iteration 57, loss = 0.46883644\n",
      "Iteration 668, loss = 0.32134495\n",
      "Iteration 253, loss = 0.40681725\n",
      "Iteration 50, loss = 0.53697225\n",
      "Iteration 669, loss = 0.32123220\n",
      "Iteration 51, loss = 0.53418179\n",
      "Iteration 58, loss = 0.46704687\n",
      "Iteration 497, loss = 0.35260613\n",
      "Iteration 52, loss = 0.53143275\n",
      "Iteration 670, loss = 0.32107334\n",
      "Iteration 240, loss = 0.41644887\n",
      "Iteration 59, loss = 0.46526827\n",
      "Iteration 53, loss = 0.52873819\n",
      "Iteration 671, loss = 0.32094243\n",
      "Iteration 54, loss = 0.52604629\n",
      "Iteration 60, loss = 0.46347343\n",
      "Iteration 1123, loss = 0.28132577\n",
      "Iteration 254, loss = 0.40655086\n",
      "Iteration 498, loss = 0.35252286\n",
      "Iteration 61, loss = 0.46173222\n",
      "Iteration 55, loss = 0.52327587\n",
      "Iteration 99, loss = 0.59024234\n",
      "Iteration 672, loss = 0.32102762\n",
      "Iteration 62, loss = 0.46008540\n",
      "Iteration 499, loss = 0.35239806\n",
      "Iteration 56, loss = 0.52079997\n",
      "Iteration 241, loss = 0.41614466\n",
      "Iteration 1124, loss = 0.28124678\n",
      "Iteration 57, loss = 0.51827299\n",
      "Iteration 63, loss = 0.45837977\n",
      "Iteration 1125, loss = 0.28113540\n",
      "Iteration 255, loss = 0.40626096\n",
      "Iteration 58, loss = 0.51573727\n",
      "Iteration 64, loss = 0.45684350\n",
      "Iteration 673, loss = 0.32064537\n",
      "Iteration 59, loss = 0.51327977\n",
      "Iteration 1126, loss = 0.28102585\n",
      "Iteration 65, loss = 0.45521936\n",
      "Iteration 60, loss = 0.51076517\n",
      "Iteration 61, loss = 0.50834273\n",
      "Iteration 500, loss = 0.35229006\n",
      "Iteration 66, loss = 0.45363465\n",
      "Iteration 62, loss = 0.50602160\n",
      "Iteration 1127, loss = 0.28097108\n",
      "Iteration 63, loss = 0.50374156\n",
      "Iteration 674, loss = 0.32067380\n",
      "Iteration 67, loss = 0.45219023\n",
      "Iteration 256, loss = 0.40590422\n",
      "Iteration 501, loss = 0.35214611\n",
      "Iteration 675, loss = 0.32039623\n",
      "Iteration 64, loss = 0.50146174\n",
      "Iteration 68, loss = 0.45065640\n",
      "Iteration 676, loss = 0.32030487\n",
      "Iteration 502, loss = 0.35207239\n",
      "Iteration 65, loss = 0.49922684\n",
      "Iteration 677, loss = 0.32009919\n",
      "Iteration 69, loss = 0.44921106\n",
      "Iteration 1128, loss = 0.28088809\n",
      "Iteration 257, loss = 0.40560901\n",
      "Iteration 70, loss = 0.44775728\n",
      "Iteration 66, loss = 0.49701184\n",
      "Iteration 678, loss = 0.31997430\n",
      "Iteration 242, loss = 0.41587500\n",
      "Iteration 71, loss = 0.44638479\n",
      "Iteration 100, loss = 0.58777815\n",
      "Iteration 1129, loss = 0.28078096\n",
      "Iteration 503, loss = 0.35196333\n",
      "Iteration 67, loss = 0.49483890\n",
      "Iteration 68, loss = 0.49267169\n",
      "Iteration 72, loss = 0.44500807\n",
      "Iteration 258, loss = 0.40528783\n",
      "Iteration 69, loss = 0.49060855\n",
      "Iteration 1130, loss = 0.28069870\n",
      "Iteration 504, loss = 0.35178543\n",
      "Iteration 70, loss = 0.48857721\n",
      "Iteration 679, loss = 0.31983251\n",
      "Iteration 71, loss = 0.48650553\n",
      "Iteration 73, loss = 0.44364613\n",
      "Iteration 72, loss = 0.48455167\n",
      "Iteration 680, loss = 0.31973762\n",
      "Iteration 73, loss = 0.48258703\n",
      "Iteration 505, loss = 0.35175115\n",
      "Iteration 74, loss = 0.44228584\n",
      "Iteration 1131, loss = 0.28065733\n",
      "Iteration 259, loss = 0.40500982\n",
      "Iteration 74, loss = 0.48068674\n",
      "Iteration 1132, loss = 0.28051128\n",
      "Iteration 75, loss = 0.44102956\n",
      "Iteration 76, loss = 0.43975278\n",
      "Iteration 1133, loss = 0.28043326\n",
      "Iteration 681, loss = 0.31968445\n",
      "Iteration 77, loss = 0.43851505\n",
      "Iteration 243, loss = 0.41552791\n",
      "Iteration 78, loss = 0.43729880\n",
      "Iteration 1134, loss = 0.28035934\n",
      "Iteration 79, loss = 0.43605839\n",
      "Iteration 682, loss = 0.31939546\n",
      "Iteration 1135, loss = 0.28024513\n",
      "Iteration 506, loss = 0.35156224\n",
      "Iteration 80, loss = 0.43488159\n",
      "Iteration 1136, loss = 0.28014754\n",
      "Iteration 81, loss = 0.43371867\n",
      "Iteration 683, loss = 0.31928275\n",
      "Iteration 75, loss = 0.47877572\n",
      "Iteration 82, loss = 0.43257041\n",
      "Iteration 260, loss = 0.40474496\n",
      "Iteration 1137, loss = 0.28005583\n",
      "Iteration 83, loss = 0.43145391\n",
      "Iteration 1138, loss = 0.27997762\n",
      "Iteration 244, loss = 0.41526071\n",
      "Iteration 84, loss = 0.43037334\n",
      "Iteration 76, loss = 0.47694105\n",
      "Iteration 1139, loss = 0.27991143\n",
      "Iteration 85, loss = 0.42927524\n",
      "Iteration 101, loss = 0.58532342\n",
      "Iteration 77, loss = 0.47518021\n",
      "Iteration 684, loss = 0.31913952\n",
      "Iteration 507, loss = 0.35146945\n",
      "Iteration 86, loss = 0.42820732\n",
      "Iteration 78, loss = 0.47341680\n",
      "Iteration 261, loss = 0.40445253\n",
      "Iteration 87, loss = 0.42715267\n",
      "Iteration 685, loss = 0.31906479\n",
      "Iteration 88, loss = 0.42613978\n",
      "Iteration 79, loss = 0.47170588\n",
      "Iteration 89, loss = 0.42513641\n",
      "Iteration 1140, loss = 0.27980974\n",
      "Iteration 80, loss = 0.46998131\n",
      "Iteration 508, loss = 0.35134249\n",
      "Iteration 81, loss = 0.46829527\n",
      "Iteration 90, loss = 0.42416349\n",
      "Iteration 686, loss = 0.31885587\n",
      "Iteration 1141, loss = 0.27971792\n",
      "Iteration 1142, loss = 0.27966107\n",
      "Iteration 245, loss = 0.41492869\n",
      "Iteration 91, loss = 0.42318487\n",
      "Iteration 82, loss = 0.46665422\n",
      "Iteration 1143, loss = 0.27953695\n",
      "Iteration 92, loss = 0.42222888\n",
      "Iteration 93, loss = 0.42126853\n",
      "Iteration 1144, loss = 0.27944587\n",
      "Iteration 83, loss = 0.46503633\n",
      "Iteration 687, loss = 0.31873909\n",
      "Iteration 262, loss = 0.40413323\n",
      "Iteration 102, loss = 0.58288365\n",
      "Iteration 1145, loss = 0.27937366\n",
      "Iteration 94, loss = 0.42035088\n",
      "Iteration 84, loss = 0.46349812\n",
      "Iteration 688, loss = 0.31858533\n",
      "Iteration 1146, loss = 0.27928001\n",
      "Iteration 509, loss = 0.35121211\n",
      "Iteration 95, loss = 0.41946383\n",
      "Iteration 1147, loss = 0.27916093Iteration 85, loss = 0.46198289\n",
      "\n",
      "Iteration 86, loss = 0.46043459\n",
      "Iteration 510, loss = 0.35112301\n",
      "Iteration 689, loss = 0.31851625\n",
      "Iteration 87, loss = 0.45897035\n",
      "Iteration 263, loss = 0.40387567\n",
      "Iteration 88, loss = 0.45751777\n",
      "Iteration 1148, loss = 0.27907957\n",
      "Iteration 89, loss = 0.45607819\n",
      "Iteration 511, loss = 0.35098162\n",
      "Iteration 90, loss = 0.45466237\n",
      "Iteration 91, loss = 0.45337760\n",
      "Iteration 690, loss = 0.31829918\n",
      "Iteration 1149, loss = 0.27900423\n",
      "Iteration 96, loss = 0.41851480\n",
      "Iteration 103, loss = 0.58046499\n",
      "Iteration 246, loss = 0.41462767\n",
      "Iteration 512, loss = 0.35089355\n",
      "Iteration 92, loss = 0.45197522\n",
      "Iteration 264, loss = 0.40355198\n",
      "Iteration 1150, loss = 0.27895564\n",
      "Iteration 691, loss = 0.31823574\n",
      "Iteration 97, loss = 0.41766278\n",
      "Iteration 93, loss = 0.45065104\n",
      "Iteration 513, loss = 0.35074025\n",
      "Iteration 692, loss = 0.31806435\n",
      "Iteration 98, loss = 0.41676389\n",
      "Iteration 1151, loss = 0.27882290\n",
      "Iteration 94, loss = 0.44940989\n",
      "Iteration 693, loss = 0.31790303\n",
      "Iteration 514, loss = 0.35064293\n",
      "Iteration 99, loss = 0.41593340\n",
      "Iteration 95, loss = 0.44811212\n",
      "Iteration 694, loss = 0.31775108\n",
      "Iteration 1152, loss = 0.27876290\n",
      "Iteration 100, loss = 0.41510369\n",
      "Iteration 96, loss = 0.44688326\n",
      "Iteration 104, loss = 0.57805319\n",
      "Iteration 695, loss = 0.31762573\n",
      "Iteration 97, loss = 0.44565652\n",
      "Iteration 515, loss = 0.35050767\n",
      "Iteration 1153, loss = 0.27864671\n",
      "Iteration 101, loss = 0.41425514\n",
      "Iteration 247, loss = 0.41431231\n",
      "Iteration 102, loss = 0.41344914\n",
      "Iteration 1154, loss = 0.27854492\n",
      "Iteration 98, loss = 0.44442968\n",
      "Iteration 103, loss = 0.41262265\n",
      "Iteration 1155, loss = 0.27846644\n",
      "Iteration 99, loss = 0.44327495\n",
      "Iteration 696, loss = 0.31754322\n",
      "Iteration 100, loss = 0.44209098\n",
      "Iteration 1156, loss = 0.27839732\n",
      "Iteration 104, loss = 0.41184337\n",
      "Iteration 101, loss = 0.44097472\n",
      "Iteration 1157, loss = 0.27827834\n",
      "Iteration 265, loss = 0.40327685\n",
      "Iteration 102, loss = 0.43985343\n",
      "Iteration 516, loss = 0.35060976\n",
      "Iteration 105, loss = 0.41104916\n",
      "Iteration 1158, loss = 0.27816953\n",
      "Iteration 103, loss = 0.43877529\n",
      "Iteration 697, loss = 0.31734709\n",
      "Iteration 106, loss = 0.41031123\n",
      "Iteration 1159, loss = 0.27813189\n",
      "Iteration 104, loss = 0.43769539\n",
      "Iteration 248, loss = 0.41403715\n",
      "Iteration 105, loss = 0.43661978\n",
      "Iteration 698, loss = 0.31718304\n",
      "Iteration 107, loss = 0.40954982\n",
      "Iteration 106, loss = 0.43556430\n",
      "Iteration 108, loss = 0.40880755\n",
      "Iteration 699, loss = 0.31704367\n",
      "Iteration 107, loss = 0.43458887\n",
      "Iteration 1160, loss = 0.27801921\n",
      "Iteration 108, loss = 0.43354677\n",
      "Iteration 109, loss = 0.43259320\n",
      "Iteration 109, loss = 0.40804639\n",
      "Iteration 110, loss = 0.43160621\n",
      "Iteration 266, loss = 0.40296692\n",
      "Iteration 105, loss = 0.57573428\n",
      "Iteration 110, loss = 0.40733137\n",
      "Iteration 111, loss = 0.43072735\n",
      "Iteration 700, loss = 0.31695718\n",
      "Iteration 111, loss = 0.40664597\n",
      "Iteration 112, loss = 0.42975233\n",
      "Iteration 1161, loss = 0.27792703\n",
      "Iteration 517, loss = 0.35027437\n",
      "Iteration 112, loss = 0.40593009\n",
      "Iteration 113, loss = 0.42884880\n",
      "Iteration 1162, loss = 0.27782779\n",
      "Iteration 114, loss = 0.42797491\n",
      "Iteration 115, loss = 0.42708032\n",
      "Iteration 701, loss = 0.31676013\n",
      "Iteration 116, loss = 0.42626665\n",
      "Iteration 113, loss = 0.40523255\n",
      "Iteration 267, loss = 0.40269408\n",
      "Iteration 1163, loss = 0.27775857\n",
      "Iteration 249, loss = 0.41371870\n",
      "Iteration 114, loss = 0.40452232\n",
      "Iteration 518, loss = 0.35016230\n",
      "Iteration 1164, loss = 0.27762565\n",
      "Iteration 117, loss = 0.42537165\n",
      "Iteration 1165, loss = 0.27754578\n",
      "Iteration 702, loss = 0.31664049\n",
      "Iteration 115, loss = 0.40387111\n",
      "Iteration 519, loss = 0.35004399\n",
      "Iteration 1166, loss = 0.27746397\n",
      "Iteration 118, loss = 0.42456910\n",
      "Iteration 703, loss = 0.31653091\n",
      "Iteration 1167, loss = 0.27738141\n",
      "Iteration 116, loss = 0.40319999\n",
      "Iteration 119, loss = 0.42373374\n",
      "Iteration 704, loss = 0.31635685\n",
      "Iteration 1168, loss = 0.27727565\n",
      "Iteration 120, loss = 0.42292065\n",
      "Iteration 117, loss = 0.40252393\n",
      "Iteration 1169, loss = 0.27721231\n",
      "Iteration 121, loss = 0.42216057\n",
      "Iteration 520, loss = 0.34991294\n",
      "Iteration 250, loss = 0.41343029\n",
      "Iteration 705, loss = 0.31621750\n",
      "Iteration 118, loss = 0.40189282\n",
      "Iteration 1170, loss = 0.27709858\n",
      "Iteration 268, loss = 0.40239876\n",
      "Iteration 106, loss = 0.57340977\n",
      "Iteration 122, loss = 0.42135195\n",
      "Iteration 119, loss = 0.40125532\n",
      "Iteration 1171, loss = 0.27700819\n",
      "Iteration 123, loss = 0.42056458\n",
      "Iteration 706, loss = 0.31608607\n",
      "Iteration 1172, loss = 0.27695667Iteration 124, loss = 0.41981844\n",
      "\n",
      "Iteration 707, loss = 0.31595851\n",
      "Iteration 1173, loss = 0.27685948\n",
      "Iteration 120, loss = 0.40060862\n",
      "Iteration 125, loss = 0.41906033\n",
      "Iteration 708, loss = 0.31579847\n",
      "Iteration 126, loss = 0.41836111\n",
      "Iteration 1174, loss = 0.27674050\n",
      "Iteration 521, loss = 0.34983271\n",
      "Iteration 127, loss = 0.41761767\n",
      "Iteration 1175, loss = 0.27668236\n",
      "Iteration 128, loss = 0.41691339\n",
      "Iteration 121, loss = 0.40000956\n",
      "Iteration 709, loss = 0.31566315\n",
      "Iteration 269, loss = 0.40212521\n",
      "Iteration 129, loss = 0.41621720\n",
      "Iteration 122, loss = 0.39939385\n",
      "Iteration 270, loss = 0.40183790\n",
      "Iteration 123, loss = 0.39877655\n",
      "Iteration 130, loss = 0.41549171\n",
      "Iteration 1176, loss = 0.27657952\n",
      "Iteration 710, loss = 0.31550441\n",
      "Iteration 124, loss = 0.39817368\n",
      "Iteration 125, loss = 0.39759867\n",
      "Iteration 271, loss = 0.40156428\n",
      "Iteration 711, loss = 0.31538876\n",
      "Iteration 712, loss = 0.31523371\n",
      "Iteration 251, loss = 0.41311634\n",
      "Iteration 522, loss = 0.34968303\n",
      "Iteration 272, loss = 0.40127012\n",
      "Iteration 713, loss = 0.31509533\n",
      "Iteration 126, loss = 0.39699193\n",
      "Iteration 523, loss = 0.34956325\n",
      "Iteration 127, loss = 0.39642806\n",
      "Iteration 128, loss = 0.39584485\n",
      "Iteration 131, loss = 0.41480841\n",
      "Iteration 132, loss = 0.41415763\n",
      "Iteration 1177, loss = 0.27646185\n",
      "Iteration 133, loss = 0.41348703\n",
      "Iteration 1178, loss = 0.27639442\n",
      "Iteration 714, loss = 0.31493476\n",
      "Iteration 252, loss = 0.41286213\n",
      "Iteration 134, loss = 0.41281909\n",
      "Iteration 129, loss = 0.39530112\n",
      "Iteration 135, loss = 0.41217554\n",
      "Iteration 130, loss = 0.39473874\n",
      "Iteration 524, loss = 0.34945442\n",
      "Iteration 1179, loss = 0.27631329\n",
      "Iteration 107, loss = 0.57125889\n",
      "Iteration 136, loss = 0.41151743\n",
      "Iteration 137, loss = 0.41093294\n",
      "Iteration 131, loss = 0.39420008\n",
      "Iteration 132, loss = 0.39365484\n",
      "Iteration 715, loss = 0.31477481\n",
      "Iteration 138, loss = 0.41028310\n",
      "Iteration 1180, loss = 0.27622610\n",
      "Iteration 273, loss = 0.40105480\n",
      "Iteration 1181, loss = 0.27612257\n",
      "Iteration 133, loss = 0.39309211\n",
      "Iteration 716, loss = 0.31467588\n",
      "Iteration 139, loss = 0.40968490\n",
      "Iteration 140, loss = 0.40909946\n",
      "Iteration 253, loss = 0.41252759\n",
      "Iteration 141, loss = 0.40847570\n",
      "Iteration 1182, loss = 0.27602425\n",
      "Iteration 134, loss = 0.39259080\n",
      "Iteration 525, loss = 0.34939248\n",
      "Iteration 135, loss = 0.39206478\n",
      "Iteration 142, loss = 0.40788962\n",
      "Iteration 717, loss = 0.31450942\n",
      "Iteration 108, loss = 0.56896700\n",
      "Iteration 136, loss = 0.39152695\n",
      "Iteration 137, loss = 0.39103281\n",
      "Iteration 1183, loss = 0.27598527\n",
      "Iteration 138, loss = 0.39054288\n",
      "Iteration 718, loss = 0.31442857\n",
      "Iteration 143, loss = 0.40732554\n",
      "Iteration 144, loss = 0.40670339\n",
      "Iteration 139, loss = 0.39002604\n",
      "Iteration 1184, loss = 0.27584949\n",
      "Iteration 145, loss = 0.40621283\n",
      "Iteration 719, loss = 0.31422602\n",
      "Iteration 140, loss = 0.38953698\n",
      "Iteration 274, loss = 0.40072419\n",
      "Iteration 526, loss = 0.34925697\n",
      "Iteration 254, loss = 0.41224442\n",
      "Iteration 146, loss = 0.40561532\n",
      "Iteration 1185, loss = 0.27577988\n",
      "Iteration 141, loss = 0.38902852\n",
      "Iteration 147, loss = 0.40505415\n",
      "Iteration 720, loss = 0.31407617\n",
      "Iteration 142, loss = 0.38855274\n",
      "Iteration 148, loss = 0.40453383\n",
      "Iteration 1186, loss = 0.27567734\n",
      "Iteration 143, loss = 0.38810364\n",
      "Iteration 721, loss = 0.31396632\n",
      "Iteration 1187, loss = 0.27557416\n",
      "Iteration 527, loss = 0.34912720\n",
      "Iteration 722, loss = 0.31379174\n",
      "Iteration 149, loss = 0.40398183\n",
      "Iteration 144, loss = 0.38761393\n",
      "Iteration 150, loss = 0.40346107\n",
      "Iteration 1188, loss = 0.27547197\n",
      "Iteration 145, loss = 0.38712473\n",
      "Iteration 109, loss = 0.56677169\n",
      "Iteration 151, loss = 0.40291057\n",
      "Iteration 275, loss = 0.40046507\n",
      "Iteration 152, loss = 0.40238814\n",
      "Iteration 1189, loss = 0.27539811\n",
      "Iteration 146, loss = 0.38668813\n",
      "Iteration 723, loss = 0.31367187\n",
      "Iteration 1190, loss = 0.27528896\n",
      "Iteration 153, loss = 0.40189504\n",
      "Iteration 528, loss = 0.34900700\n",
      "Iteration 724, loss = 0.31353654\n",
      "Iteration 154, loss = 0.40136593\n",
      "Iteration 1191, loss = 0.27520883\n",
      "Iteration 725, loss = 0.31342632\n",
      "Iteration 276, loss = 0.40018707\n",
      "Iteration 255, loss = 0.41201544\n",
      "Iteration 147, loss = 0.38624852\n",
      "Iteration 155, loss = 0.40085951\n",
      "Iteration 529, loss = 0.34889639\n",
      "Iteration 1192, loss = 0.27515015\n",
      "Iteration 726, loss = 0.31327351\n",
      "Iteration 148, loss = 0.38578467\n",
      "Iteration 727, loss = 0.31316293\n",
      "Iteration 1193, loss = 0.27505954\n",
      "Iteration 149, loss = 0.38533022\n",
      "Iteration 156, loss = 0.40037495\n",
      "Iteration 728, loss = 0.31306821\n",
      "Iteration 1194, loss = 0.27494311\n",
      "Iteration 157, loss = 0.39987669\n",
      "Iteration 150, loss = 0.38488169\n",
      "Iteration 729, loss = 0.31283323\n",
      "Iteration 1195, loss = 0.27485474\n",
      "Iteration 277, loss = 0.39994146\n",
      "Iteration 530, loss = 0.34876632\n",
      "Iteration 158, loss = 0.39938572\n",
      "Iteration 730, loss = 0.31267786\n",
      "Iteration 110, loss = 0.56467917\n",
      "Iteration 151, loss = 0.38444222\n",
      "Iteration 1196, loss = 0.27476745\n",
      "Iteration 256, loss = 0.41167921\n",
      "Iteration 731, loss = 0.31253296\n",
      "Iteration 159, loss = 0.39891126\n",
      "Iteration 160, loss = 0.39840641\n",
      "Iteration 1197, loss = 0.27468374\n",
      "Iteration 152, loss = 0.38401995\n",
      "Iteration 161, loss = 0.39797007\n",
      "Iteration 153, loss = 0.38359371\n",
      "Iteration 162, loss = 0.39747967\n",
      "Iteration 732, loss = 0.31246531\n",
      "Iteration 531, loss = 0.34864109\n",
      "Iteration 154, loss = 0.38317848\n",
      "Iteration 163, loss = 0.39704083\n",
      "Iteration 278, loss = 0.39963890\n",
      "Iteration 155, loss = 0.38276589\n",
      "Iteration 164, loss = 0.39656927\n",
      "Iteration 156, loss = 0.38234223\n",
      "Iteration 165, loss = 0.39609163\n",
      "Iteration 157, loss = 0.38192964\n",
      "Iteration 166, loss = 0.39565118\n",
      "Iteration 1198, loss = 0.27458734\n",
      "Iteration 158, loss = 0.38152165\n",
      "Iteration 167, loss = 0.39522808\n",
      "Iteration 733, loss = 0.31227698\n",
      "Iteration 257, loss = 0.41138163\n",
      "Iteration 532, loss = 0.34851155\n",
      "Iteration 168, loss = 0.39477575\n",
      "Iteration 159, loss = 0.38112934\n",
      "Iteration 111, loss = 0.56262695\n",
      "Iteration 169, loss = 0.39432179\n",
      "Iteration 1199, loss = 0.27447015\n",
      "Iteration 160, loss = 0.38071222\n",
      "Iteration 533, loss = 0.34839593\n",
      "Iteration 1200, loss = 0.27437951\n",
      "Iteration 170, loss = 0.39388778\n",
      "Iteration 734, loss = 0.31213718\n",
      "Iteration 279, loss = 0.39938218\n",
      "Iteration 161, loss = 0.38033938\n",
      "Iteration 534, loss = 0.34829076\n",
      "Iteration 162, loss = 0.37996780\n",
      "Iteration 735, loss = 0.31205469\n",
      "Iteration 1201, loss = 0.27432611\n",
      "Iteration 163, loss = 0.37957341\n",
      "Iteration 171, loss = 0.39345231\n",
      "Iteration 280, loss = 0.39913998\n",
      "Iteration 535, loss = 0.34816335\n",
      "Iteration 164, loss = 0.37917129\n",
      "Iteration 736, loss = 0.31196253\n",
      "Iteration 1202, loss = 0.27419732\n",
      "Iteration 165, loss = 0.37880672\n",
      "Iteration 172, loss = 0.39303700\n",
      "Iteration 258, loss = 0.41111078\n",
      "Iteration 166, loss = 0.37842336\n",
      "Iteration 167, loss = 0.37805674\n",
      "Iteration 1203, loss = 0.27411245\n",
      "Iteration 168, loss = 0.37768957\n",
      "Iteration 173, loss = 0.39263339\n",
      "Iteration 536, loss = 0.34812712\n",
      "Iteration 169, loss = 0.37732979\n",
      "Iteration 737, loss = 0.31173994\n",
      "Iteration 174, loss = 0.39219250\n",
      "Iteration 1204, loss = 0.27410545\n",
      "Iteration 170, loss = 0.37696567\n",
      "Iteration 175, loss = 0.39178402\n",
      "Iteration 738, loss = 0.31155465\n",
      "Iteration 176, loss = 0.39137352\n",
      "Iteration 171, loss = 0.37660255\n",
      "Iteration 1205, loss = 0.27393459\n",
      "Iteration 177, loss = 0.39093618\n",
      "Iteration 172, loss = 0.37626523\n",
      "Iteration 537, loss = 0.34791145\n",
      "Iteration 259, loss = 0.41085154\n",
      "Iteration 173, loss = 0.37589654\n",
      "Iteration 281, loss = 0.39887733\n",
      "Iteration 112, loss = 0.56056140\n",
      "Iteration 174, loss = 0.37554499\n",
      "Iteration 178, loss = 0.39055042\n",
      "Iteration 1206, loss = 0.27385778\n",
      "Iteration 179, loss = 0.39013743\n",
      "Iteration 739, loss = 0.31145120\n",
      "Iteration 538, loss = 0.34781186\n",
      "Iteration 740, loss = 0.31127213\n",
      "Iteration 180, loss = 0.38973225\n",
      "Iteration 1207, loss = 0.27375377\n",
      "Iteration 539, loss = 0.34767418\n",
      "Iteration 260, loss = 0.41054169\n",
      "Iteration 181, loss = 0.38937611\n",
      "Iteration 175, loss = 0.37522382\n",
      "Iteration 282, loss = 0.39860196\n",
      "Iteration 540, loss = 0.34761203\n",
      "Iteration 1208, loss = 0.27369256\n",
      "Iteration 541, loss = 0.34743416\n",
      "Iteration 113, loss = 0.55867981\n",
      "Iteration 176, loss = 0.37486798\n",
      "Iteration 261, loss = 0.41023981\n",
      "Iteration 177, loss = 0.37453035\n",
      "Iteration 542, loss = 0.34732873\n",
      "Iteration 178, loss = 0.37421169\n",
      "Iteration 179, loss = 0.37387943\n",
      "Iteration 1209, loss = 0.27355995\n",
      "Iteration 543, loss = 0.34721523\n",
      "Iteration 283, loss = 0.39833340\n",
      "Iteration 1210, loss = 0.27348254\n",
      "Iteration 741, loss = 0.31129125\n",
      "Iteration 544, loss = 0.34710411\n",
      "Iteration 180, loss = 0.37354873\n",
      "Iteration 1211, loss = 0.27344119\n",
      "Iteration 181, loss = 0.37321216\n",
      "Iteration 182, loss = 0.37290335\n",
      "Iteration 742, loss = 0.31104901\n",
      "Iteration 182, loss = 0.38899416\n",
      "Iteration 284, loss = 0.39806491\n",
      "Iteration 1212, loss = 0.27332654\n",
      "Iteration 183, loss = 0.38857186\n",
      "Iteration 1213, loss = 0.27320755\n",
      "Iteration 183, loss = 0.37258444\n",
      "Iteration 114, loss = 0.55664688\n",
      "Iteration 184, loss = 0.38819784\n",
      "Iteration 1214, loss = 0.27311763\n",
      "Iteration 185, loss = 0.38780607\n",
      "Iteration 743, loss = 0.31089244\n",
      "Iteration 186, loss = 0.38744490\n",
      "Iteration 1215, loss = 0.27300830\n",
      "Iteration 184, loss = 0.37226489\n",
      "Iteration 1216, loss = 0.27293949\n",
      "Iteration 187, loss = 0.38704831\n",
      "Iteration 1217, loss = 0.27283675\n",
      "Iteration 185, loss = 0.37194591\n",
      "Iteration 188, loss = 0.38667033\n",
      "Iteration 744, loss = 0.31071167\n",
      "Iteration 262, loss = 0.40998091\n",
      "Iteration 1218, loss = 0.27276870Iteration 186, loss = 0.37163517\n",
      "\n",
      "Iteration 189, loss = 0.38631409\n",
      "Iteration 190, loss = 0.38593567\n",
      "Iteration 1219, loss = 0.27266236\n",
      "Iteration 745, loss = 0.31056362\n",
      "Iteration 191, loss = 0.38559068\n",
      "Iteration 1220, loss = 0.27255526\n",
      "Iteration 192, loss = 0.38521442\n",
      "Iteration 187, loss = 0.37131992\n",
      "Iteration 1221, loss = 0.27248246\n",
      "Iteration 193, loss = 0.38486996\n",
      "Iteration 188, loss = 0.37100745\n",
      "Iteration 194, loss = 0.38449059\n",
      "Iteration 746, loss = 0.31043999\n",
      "Iteration 195, loss = 0.38414313\n",
      "Iteration 196, loss = 0.38379291\n",
      "Iteration 545, loss = 0.34697648\n",
      "Iteration 197, loss = 0.38344278\n",
      "Iteration 1222, loss = 0.27237495\n",
      "Iteration 198, loss = 0.38311173\n",
      "Iteration 263, loss = 0.40969220\n",
      "Iteration 189, loss = 0.37070446\n",
      "Iteration 285, loss = 0.39779538\n",
      "Iteration 190, loss = 0.37040402\n",
      "Iteration 199, loss = 0.38276809\n",
      "Iteration 1223, loss = 0.27230662\n",
      "Iteration 191, loss = 0.37010552\n",
      "Iteration 1224, loss = 0.27223023\n",
      "Iteration 192, loss = 0.36980587\n",
      "Iteration 200, loss = 0.38241410\n",
      "Iteration 747, loss = 0.31029353\n",
      "Iteration 193, loss = 0.36949643\n",
      "Iteration 201, loss = 0.38209105\n",
      "Iteration 194, loss = 0.36923130\n",
      "Iteration 748, loss = 0.31024547\n",
      "Iteration 1225, loss = 0.27215475\n",
      "Iteration 202, loss = 0.38173621\n",
      "Iteration 546, loss = 0.34687537\n",
      "Iteration 749, loss = 0.30999104\n",
      "Iteration 286, loss = 0.39753656\n",
      "Iteration 203, loss = 0.38142458\n",
      "Iteration 115, loss = 0.55466665\n",
      "Iteration 547, loss = 0.34674042\n",
      "Iteration 204, loss = 0.38109376\n",
      "Iteration 195, loss = 0.36893685\n",
      "Iteration 750, loss = 0.30994002\n",
      "Iteration 205, loss = 0.38074213\n",
      "Iteration 1226, loss = 0.27204933\n",
      "Iteration 206, loss = 0.38041799\n",
      "Iteration 207, loss = 0.38009048\n",
      "Iteration 208, loss = 0.37978488\n",
      "Iteration 751, loss = 0.30971162\n",
      "Iteration 196, loss = 0.36863268\n",
      "Iteration 209, loss = 0.37944682\n",
      "Iteration 210, loss = 0.37915008\n",
      "Iteration 197, loss = 0.36833376\n",
      "Iteration 548, loss = 0.34672513\n",
      "Iteration 211, loss = 0.37882569\n",
      "Iteration 1227, loss = 0.27193554\n",
      "Iteration 212, loss = 0.37848589\n",
      "Iteration 198, loss = 0.36807177\n",
      "Iteration 752, loss = 0.30960579\n",
      "Iteration 213, loss = 0.37819177\n",
      "Iteration 214, loss = 0.37786920\n",
      "Iteration 215, loss = 0.37755931\n",
      "Iteration 216, loss = 0.37727103\n",
      "Iteration 199, loss = 0.36777219\n",
      "Iteration 1228, loss = 0.27185589\n",
      "Iteration 264, loss = 0.40942715\n",
      "Iteration 217, loss = 0.37694511\n",
      "Iteration 753, loss = 0.30945078\n",
      "Iteration 218, loss = 0.37664275\n",
      "Iteration 1229, loss = 0.27176876\n",
      "Iteration 219, loss = 0.37633593\n",
      "Iteration 200, loss = 0.36748970\n",
      "Iteration 220, loss = 0.37605688\n",
      "Iteration 1230, loss = 0.27166673\n",
      "Iteration 221, loss = 0.37575974\n",
      "Iteration 754, loss = 0.30927779\n",
      "Iteration 287, loss = 0.39726618\n",
      "Iteration 201, loss = 0.36721945\n",
      "Iteration 1231, loss = 0.27156269\n",
      "Iteration 222, loss = 0.37543383\n",
      "Iteration 265, loss = 0.40914571\n",
      "Iteration 202, loss = 0.36694314\n",
      "Iteration 1232, loss = 0.27148366\n",
      "Iteration 755, loss = 0.30917350\n",
      "Iteration 116, loss = 0.55278487\n",
      "Iteration 223, loss = 0.37514217\n",
      "Iteration 1233, loss = 0.27146080\n",
      "Iteration 288, loss = 0.39702168\n",
      "Iteration 203, loss = 0.36664417\n",
      "Iteration 224, loss = 0.37484275\n",
      "Iteration 1234, loss = 0.27133483\n",
      "Iteration 756, loss = 0.30900032\n",
      "Iteration 225, loss = 0.37456616\n",
      "Iteration 204, loss = 0.36638669\n",
      "Iteration 1235, loss = 0.27122965\n",
      "Iteration 205, loss = 0.36611872\n",
      "Iteration 757, loss = 0.30889137\n",
      "Iteration 226, loss = 0.37427890\n",
      "Iteration 549, loss = 0.34649974\n",
      "Iteration 206, loss = 0.36584421\n",
      "Iteration 266, loss = 0.40890506\n",
      "Iteration 207, loss = 0.36557798\n",
      "Iteration 227, loss = 0.37398565\n",
      "Iteration 208, loss = 0.36531952\n",
      "Iteration 758, loss = 0.30871175\n",
      "Iteration 209, loss = 0.36502593\n",
      "Iteration 759, loss = 0.30865550\n",
      "Iteration 228, loss = 0.37369115\n",
      "Iteration 1236, loss = 0.27111054\n",
      "Iteration 210, loss = 0.36478373\n",
      "Iteration 1237, loss = 0.27102695\n",
      "Iteration 550, loss = 0.34637885\n",
      "Iteration 211, loss = 0.36452078\n",
      "Iteration 289, loss = 0.39676327\n",
      "Iteration 760, loss = 0.30844659\n",
      "Iteration 267, loss = 0.40862040\n",
      "Iteration 1238, loss = 0.27095556\n",
      "Iteration 212, loss = 0.36427407\n",
      "Iteration 117, loss = 0.55101818\n",
      "Iteration 761, loss = 0.30829303\n",
      "Iteration 1239, loss = 0.27084826\n",
      "Iteration 213, loss = 0.36402812\n",
      "Iteration 551, loss = 0.34627670\n",
      "Iteration 229, loss = 0.37341865\n",
      "Iteration 214, loss = 0.36374356\n",
      "Iteration 230, loss = 0.37313320\n",
      "Iteration 1240, loss = 0.27079322\n",
      "Iteration 231, loss = 0.37285785\n",
      "Iteration 290, loss = 0.39649675\n",
      "Iteration 232, loss = 0.37257890\n",
      "Iteration 215, loss = 0.36351234\n",
      "Iteration 233, loss = 0.37230679\n",
      "Iteration 1241, loss = 0.27066649\n",
      "Iteration 762, loss = 0.30813614\n",
      "Iteration 234, loss = 0.37204468\n",
      "Iteration 235, loss = 0.37173832\n",
      "Iteration 1242, loss = 0.27058468\n",
      "Iteration 552, loss = 0.34615628\n",
      "Iteration 268, loss = 0.40834469\n",
      "Iteration 236, loss = 0.37148281\n",
      "Iteration 237, loss = 0.37120456\n",
      "Iteration 216, loss = 0.36324185\n",
      "Iteration 291, loss = 0.39624670\n",
      "Iteration 763, loss = 0.30805952\n",
      "Iteration 217, loss = 0.36299923\n",
      "Iteration 218, loss = 0.36275778\n",
      "Iteration 118, loss = 0.54913117\n",
      "Iteration 219, loss = 0.36252533\n",
      "Iteration 292, loss = 0.39597945\n",
      "Iteration 238, loss = 0.37091849\n",
      "Iteration 1243, loss = 0.27052935\n",
      "Iteration 553, loss = 0.34603385\n",
      "Iteration 220, loss = 0.36224742\n",
      "Iteration 1244, loss = 0.27040025\n",
      "Iteration 764, loss = 0.30785433\n",
      "Iteration 239, loss = 0.37066188\n",
      "Iteration 221, loss = 0.36199679\n",
      "Iteration 240, loss = 0.37038712\n",
      "Iteration 222, loss = 0.36176440\n",
      "Iteration 1245, loss = 0.27030344\n",
      "Iteration 223, loss = 0.36154460\n",
      "Iteration 241, loss = 0.37012838\n",
      "Iteration 765, loss = 0.30772491\n",
      "Iteration 1246, loss = 0.27024390\n",
      "Iteration 269, loss = 0.40805674\n",
      "Iteration 224, loss = 0.36129896\n",
      "Iteration 554, loss = 0.34591791\n",
      "Iteration 225, loss = 0.36106703\n",
      "Iteration 1247, loss = 0.27015741\n",
      "Iteration 293, loss = 0.39573278\n",
      "Iteration 119, loss = 0.54742037\n",
      "Iteration 226, loss = 0.36084288\n",
      "Iteration 227, loss = 0.36059603\n",
      "Iteration 1248, loss = 0.27004679\n",
      "Iteration 766, loss = 0.30757869\n",
      "Iteration 242, loss = 0.36987049\n",
      "Iteration 228, loss = 0.36035841\n",
      "Iteration 1249, loss = 0.26998601\n",
      "Iteration 243, loss = 0.36960135\n",
      "Iteration 229, loss = 0.36013431\n",
      "Iteration 244, loss = 0.36932040\n",
      "Iteration 767, loss = 0.30745011\n",
      "Iteration 1250, loss = 0.26987579\n",
      "Iteration 230, loss = 0.35990296\n",
      "Iteration 1251, loss = 0.26980150\n",
      "Iteration 245, loss = 0.36909305\n",
      "Iteration 768, loss = 0.30730837\n",
      "Iteration 246, loss = 0.36883038\n",
      "Iteration 555, loss = 0.34578335\n",
      "Iteration 294, loss = 0.39550822\n",
      "Iteration 270, loss = 0.40781919\n",
      "Iteration 231, loss = 0.35968310\n",
      "Iteration 1252, loss = 0.26968779\n",
      "Iteration 247, loss = 0.36854057\n",
      "Iteration 232, loss = 0.35946547\n",
      "Iteration 248, loss = 0.36829660\n",
      "Iteration 233, loss = 0.35925883\n",
      "Iteration 769, loss = 0.30714476\n",
      "Iteration 249, loss = 0.36802020\n",
      "Iteration 1253, loss = 0.26959783\n",
      "Iteration 234, loss = 0.35901161\n",
      "Iteration 250, loss = 0.36776788\n",
      "Iteration 556, loss = 0.34565855\n",
      "Iteration 251, loss = 0.36756952\n",
      "Iteration 1254, loss = 0.26953657\n",
      "Iteration 235, loss = 0.35880454\n",
      "Iteration 252, loss = 0.36726361\n",
      "Iteration 236, loss = 0.35858882\n",
      "Iteration 1255, loss = 0.26942148\n",
      "Iteration 253, loss = 0.36701999\n",
      "Iteration 237, loss = 0.35836071\n",
      "Iteration 295, loss = 0.39522218\n",
      "Iteration 254, loss = 0.36674668\n",
      "Iteration 1256, loss = 0.26935366\n",
      "Iteration 255, loss = 0.36653229\n",
      "Iteration 271, loss = 0.40753449\n",
      "Iteration 120, loss = 0.54559092\n",
      "Iteration 238, loss = 0.35815917\n",
      "Iteration 770, loss = 0.30703976\n",
      "Iteration 256, loss = 0.36625836\n",
      "Iteration 1257, loss = 0.26925465\n",
      "Iteration 257, loss = 0.36602063\n",
      "Iteration 258, loss = 0.36577861\n",
      "Iteration 557, loss = 0.34555923\n",
      "Iteration 259, loss = 0.36554145\n",
      "Iteration 771, loss = 0.30691356\n",
      "Iteration 1258, loss = 0.26916613\n",
      "Iteration 239, loss = 0.35794509\n",
      "Iteration 296, loss = 0.39494681\n",
      "Iteration 260, loss = 0.36529789\n",
      "Iteration 558, loss = 0.34546042\n",
      "Iteration 772, loss = 0.30676172\n",
      "Iteration 240, loss = 0.35772410\n",
      "Iteration 1259, loss = 0.26913646\n",
      "Iteration 261, loss = 0.36502721\n",
      "Iteration 241, loss = 0.35753298\n",
      "Iteration 773, loss = 0.30662628\n",
      "Iteration 272, loss = 0.40727425\n",
      "Iteration 242, loss = 0.35730788\n",
      "Iteration 559, loss = 0.34530443\n",
      "Iteration 1260, loss = 0.26898217\n",
      "Iteration 262, loss = 0.36479625\n",
      "Iteration 243, loss = 0.35710302\n",
      "Iteration 263, loss = 0.36456138\n",
      "Iteration 1261, loss = 0.26891409\n",
      "Iteration 264, loss = 0.36433733\n",
      "Iteration 774, loss = 0.30645091\n",
      "Iteration 265, loss = 0.36412734\n",
      "Iteration 244, loss = 0.35692161\n",
      "Iteration 297, loss = 0.39470968\n",
      "Iteration 560, loss = 0.34527847\n",
      "Iteration 775, loss = 0.30628069\n",
      "Iteration 1262, loss = 0.26881792\n",
      "Iteration 266, loss = 0.36385079\n",
      "Iteration 245, loss = 0.35669707\n",
      "Iteration 1263, loss = 0.26871512\n",
      "Iteration 246, loss = 0.35650003\n",
      "Iteration 267, loss = 0.36362939\n",
      "Iteration 121, loss = 0.54400781\n",
      "Iteration 247, loss = 0.35629785\n",
      "Iteration 561, loss = 0.34509330\n",
      "Iteration 1264, loss = 0.26866039\n",
      "Iteration 776, loss = 0.30614474\n",
      "Iteration 268, loss = 0.36339450\n",
      "Iteration 248, loss = 0.35608763\n",
      "Iteration 777, loss = 0.30603208\n",
      "Iteration 273, loss = 0.40704120\n",
      "Iteration 1265, loss = 0.26855924\n",
      "Iteration 249, loss = 0.35590695\n",
      "Iteration 269, loss = 0.36317602\n",
      "Iteration 250, loss = 0.35570563\n",
      "Iteration 298, loss = 0.39450361\n",
      "Iteration 251, loss = 0.35550558\n",
      "Iteration 270, loss = 0.36299814\n",
      "Iteration 562, loss = 0.34498585\n",
      "Iteration 778, loss = 0.30584307\n",
      "Iteration 271, loss = 0.36269377Iteration 252, loss = 0.35529695\n",
      "\n",
      "Iteration 1266, loss = 0.26850635\n",
      "Iteration 563, loss = 0.34485017\n",
      "Iteration 274, loss = 0.40676028\n",
      "Iteration 253, loss = 0.35511828\n",
      "Iteration 779, loss = 0.30570085\n",
      "Iteration 1267, loss = 0.26838351\n",
      "Iteration 272, loss = 0.36250326\n",
      "Iteration 780, loss = 0.30559007\n",
      "Iteration 1268, loss = 0.26833174\n",
      "Iteration 273, loss = 0.36225692\n",
      "Iteration 254, loss = 0.35493018\n",
      "Iteration 274, loss = 0.36202489\n",
      "Iteration 1269, loss = 0.26828188\n",
      "Iteration 299, loss = 0.39417798\n",
      "Iteration 255, loss = 0.35473120\n",
      "Iteration 122, loss = 0.54230351\n",
      "Iteration 1270, loss = 0.26817486\n",
      "Iteration 275, loss = 0.36181507\n",
      "Iteration 781, loss = 0.30543198\n",
      "Iteration 256, loss = 0.35454976\n",
      "Iteration 1271, loss = 0.26802085\n",
      "Iteration 276, loss = 0.36159860\n",
      "Iteration 1272, loss = 0.26792953\n",
      "Iteration 257, loss = 0.35435799\n",
      "Iteration 564, loss = 0.34476697\n",
      "Iteration 277, loss = 0.36137783\n",
      "Iteration 300, loss = 0.39402019\n",
      "Iteration 275, loss = 0.40651664\n",
      "Iteration 258, loss = 0.35416243\n",
      "Iteration 259, loss = 0.35397725\n",
      "Iteration 1273, loss = 0.26788325\n",
      "Iteration 782, loss = 0.30526475\n",
      "Iteration 260, loss = 0.35378918\n",
      "Iteration 278, loss = 0.36117084\n",
      "Iteration 261, loss = 0.35360694\n",
      "Iteration 783, loss = 0.30515625\n",
      "Iteration 262, loss = 0.35342277\n",
      "Iteration 1274, loss = 0.26775926\n",
      "Iteration 565, loss = 0.34464119\n",
      "Iteration 279, loss = 0.36095345\n",
      "Iteration 263, loss = 0.35324472\n",
      "Iteration 123, loss = 0.54067844\n",
      "Iteration 264, loss = 0.35306860\n",
      "Iteration 280, loss = 0.36074746\n",
      "Iteration 1275, loss = 0.26765516\n",
      "Iteration 265, loss = 0.35286969\n",
      "Iteration 784, loss = 0.30501146\n",
      "Iteration 281, loss = 0.36051671\n",
      "Iteration 276, loss = 0.40624842\n",
      "Iteration 266, loss = 0.35269477\n",
      "Iteration 1276, loss = 0.26759885\n",
      "Iteration 301, loss = 0.39375768\n",
      "Iteration 282, loss = 0.36031693\n",
      "Iteration 267, loss = 0.35252283\n",
      "Iteration 785, loss = 0.30486634\n",
      "Iteration 283, loss = 0.36011325\n",
      "Iteration 1277, loss = 0.26749304\n",
      "Iteration 284, loss = 0.35991443\n",
      "Iteration 566, loss = 0.34451836\n",
      "Iteration 1278, loss = 0.26739986\n",
      "Iteration 285, loss = 0.35968377\n",
      "Iteration 786, loss = 0.30481463\n",
      "Iteration 1279, loss = 0.26728723\n",
      "Iteration 1280, loss = 0.26719685\n",
      "Iteration 286, loss = 0.35948394\n",
      "Iteration 302, loss = 0.39350782\n",
      "Iteration 268, loss = 0.35233520\n",
      "Iteration 269, loss = 0.35216989\n",
      "Iteration 787, loss = 0.30455378\n",
      "Iteration 1281, loss = 0.26712162\n",
      "Iteration 270, loss = 0.35198954\n",
      "Iteration 788, loss = 0.30451134\n",
      "Iteration 271, loss = 0.35181964\n",
      "Iteration 124, loss = 0.53911891\n",
      "Iteration 1282, loss = 0.26701237\n",
      "Iteration 567, loss = 0.34439111\n",
      "Iteration 272, loss = 0.35163581\n",
      "Iteration 287, loss = 0.35927578\n",
      "Iteration 273, loss = 0.35146129\n",
      "Iteration 277, loss = 0.40602648\n",
      "Iteration 274, loss = 0.35129832\n",
      "Iteration 275, loss = 0.35113076\n",
      "Iteration 1283, loss = 0.26693352\n",
      "Iteration 288, loss = 0.35905937\n",
      "Iteration 789, loss = 0.30428954\n",
      "Iteration 289, loss = 0.35887540\n",
      "Iteration 276, loss = 0.35094847\n",
      "Iteration 290, loss = 0.35867699\n",
      "Iteration 790, loss = 0.30427773\n",
      "Iteration 291, loss = 0.35845374\n",
      "Iteration 1284, loss = 0.26684823\n",
      "Iteration 303, loss = 0.39322447\n",
      "Iteration 791, loss = 0.30402234\n",
      "Iteration 568, loss = 0.34426816\n",
      "Iteration 277, loss = 0.35078426\n",
      "Iteration 1285, loss = 0.26681219\n",
      "Iteration 792, loss = 0.30381946\n",
      "Iteration 1286, loss = 0.26667148\n",
      "Iteration 278, loss = 0.35062125\n",
      "Iteration 793, loss = 0.30371537\n",
      "Iteration 569, loss = 0.34417956\n",
      "Iteration 1287, loss = 0.26655876\n",
      "Iteration 304, loss = 0.39301417\n",
      "Iteration 292, loss = 0.35828011\n",
      "Iteration 1288, loss = 0.26646475\n",
      "Iteration 279, loss = 0.35045650\n",
      "Iteration 280, loss = 0.35029014\n",
      "Iteration 794, loss = 0.30359171\n",
      "Iteration 1289, loss = 0.26637184\n",
      "Iteration 293, loss = 0.35807933\n",
      "Iteration 278, loss = 0.40575818\n",
      "Iteration 294, loss = 0.35785534\n",
      "Iteration 305, loss = 0.39272818\n",
      "Iteration 125, loss = 0.53756706\n",
      "Iteration 295, loss = 0.35766523\n",
      "Iteration 795, loss = 0.30340126\n",
      "Iteration 570, loss = 0.34416506\n",
      "Iteration 281, loss = 0.35011977\n",
      "Iteration 296, loss = 0.35749670\n",
      "Iteration 282, loss = 0.34996413\n",
      "Iteration 306, loss = 0.39245998\n",
      "Iteration 297, loss = 0.35728312\n",
      "Iteration 298, loss = 0.35709074\n",
      "Iteration 279, loss = 0.40551477\n",
      "Iteration 283, loss = 0.34979179\n",
      "Iteration 571, loss = 0.34390921\n",
      "Iteration 284, loss = 0.34964634\n",
      "Iteration 796, loss = 0.30331210\n",
      "Iteration 299, loss = 0.35689791\n",
      "Iteration 300, loss = 0.35670281\n",
      "Iteration 126, loss = 0.53600456\n",
      "Iteration 797, loss = 0.30321411\n",
      "Iteration 285, loss = 0.34948051\n",
      "Iteration 572, loss = 0.34379525\n",
      "Iteration 1290, loss = 0.26644697\n",
      "Iteration 301, loss = 0.35651394\n",
      "Iteration 302, loss = 0.35633857\n",
      "Iteration 307, loss = 0.39221117\n",
      "Iteration 303, loss = 0.35614492\n",
      "Iteration 286, loss = 0.34930296\n",
      "Iteration 1291, loss = 0.26620648\n",
      "Iteration 304, loss = 0.35596371\n",
      "Iteration 287, loss = 0.34914030\n",
      "Iteration 1292, loss = 0.26616004\n",
      "Iteration 573, loss = 0.34367133\n",
      "Iteration 127, loss = 0.53454015\n",
      "Iteration 305, loss = 0.35577117\n",
      "Iteration 1293, loss = 0.26601476\n",
      "Iteration 288, loss = 0.34898334\n",
      "Iteration 798, loss = 0.30299057\n",
      "Iteration 306, loss = 0.35559694\n",
      "Iteration 799, loss = 0.30279981\n",
      "Iteration 1294, loss = 0.26590804\n",
      "Iteration 289, loss = 0.34883096\n",
      "Iteration 574, loss = 0.34354479\n",
      "Iteration 308, loss = 0.39196868\n",
      "Iteration 800, loss = 0.30273288\n",
      "Iteration 307, loss = 0.35538924\n",
      "Iteration 290, loss = 0.34867713\n",
      "Iteration 1295, loss = 0.26582737\n",
      "Iteration 280, loss = 0.40523116\n",
      "Iteration 291, loss = 0.34852032\n",
      "Iteration 575, loss = 0.34352337\n",
      "Iteration 801, loss = 0.30252848\n",
      "Iteration 292, loss = 0.34835849\n",
      "Iteration 128, loss = 0.53303009\n",
      "Iteration 576, loss = 0.34332901\n",
      "Iteration 1296, loss = 0.26574985\n",
      "Iteration 293, loss = 0.34820618\n",
      "Iteration 294, loss = 0.34805557\n",
      "Iteration 1297, loss = 0.26566709\n",
      "Iteration 309, loss = 0.39174586\n",
      "Iteration 295, loss = 0.34790689\n",
      "Iteration 802, loss = 0.30237974\n",
      "Iteration 296, loss = 0.34774166\n",
      "Iteration 577, loss = 0.34321477\n",
      "Iteration 308, loss = 0.35523782\n",
      "Iteration 803, loss = 0.30226814\n",
      "Iteration 1298, loss = 0.26556762\n",
      "Iteration 309, loss = 0.35504759\n",
      "Iteration 310, loss = 0.39151386\n",
      "Iteration 297, loss = 0.34759011\n",
      "Iteration 310, loss = 0.35484320\n",
      "Iteration 804, loss = 0.30214980\n",
      "Iteration 1299, loss = 0.26548247\n",
      "Iteration 311, loss = 0.35466518\n",
      "Iteration 312, loss = 0.35448242\n",
      "Iteration 805, loss = 0.30197840\n",
      "Iteration 298, loss = 0.34743836\n",
      "Iteration 299, loss = 0.34730151\n",
      "Iteration 578, loss = 0.34309231\n",
      "Iteration 1300, loss = 0.26538712\n",
      "Iteration 129, loss = 0.53163045\n",
      "Iteration 313, loss = 0.35432768\n",
      "Iteration 311, loss = 0.39128681\n",
      "Iteration 806, loss = 0.30179061\n",
      "Iteration 281, loss = 0.40504311\n",
      "Iteration 314, loss = 0.35412035\n",
      "Iteration 300, loss = 0.34713964\n",
      "Iteration 1301, loss = 0.26530119\n",
      "Iteration 315, loss = 0.35396138\n",
      "Iteration 579, loss = 0.34303053\n",
      "Iteration 1302, loss = 0.26519170\n",
      "Iteration 316, loss = 0.35377695\n",
      "Iteration 807, loss = 0.30170939\n",
      "Iteration 301, loss = 0.34698672\n",
      "Iteration 317, loss = 0.35360855\n",
      "Iteration 1303, loss = 0.26510306\n",
      "Iteration 312, loss = 0.39104248\n",
      "Iteration 1304, loss = 0.26503937\n",
      "Iteration 302, loss = 0.34684408\n",
      "Iteration 318, loss = 0.35342217\n",
      "Iteration 1305, loss = 0.26495162\n",
      "Iteration 282, loss = 0.40476549\n",
      "Iteration 303, loss = 0.34669675\n",
      "Iteration 319, loss = 0.35325419\n",
      "Iteration 320, loss = 0.35307111\n",
      "Iteration 304, loss = 0.34656440\n",
      "Iteration 580, loss = 0.34292800Iteration 1306, loss = 0.26483694\n",
      "Iteration 808, loss = 0.30151508Iteration 305, loss = 0.34640877\n",
      "\n",
      "\n",
      "Iteration 1307, loss = 0.26472509\n",
      "Iteration 321, loss = 0.35291310\n",
      "Iteration 306, loss = 0.34625929\n",
      "Iteration 1308, loss = 0.26467895\n",
      "Iteration 307, loss = 0.34611381\n",
      "Iteration 313, loss = 0.39074867\n",
      "Iteration 322, loss = 0.35274219\n",
      "Iteration 308, loss = 0.34597845\n",
      "Iteration 323, loss = 0.35257431\n",
      "Iteration 309, loss = 0.34582717\n",
      "Iteration 324, loss = 0.35241005\n",
      "Iteration 1309, loss = 0.26457126\n",
      "Iteration 310, loss = 0.34570312\n",
      "Iteration 325, loss = 0.35224044\n",
      "Iteration 130, loss = 0.53019979\n",
      "Iteration 809, loss = 0.30138650\n",
      "Iteration 311, loss = 0.34555541\n",
      "Iteration 312, loss = 0.34541736\n",
      "Iteration 1310, loss = 0.26445995\n",
      "Iteration 326, loss = 0.35205522\n",
      "Iteration 313, loss = 0.34527411\n",
      "Iteration 1311, loss = 0.26436453\n",
      "Iteration 314, loss = 0.34513565\n",
      "Iteration 314, loss = 0.39058321\n",
      "Iteration 327, loss = 0.35190373\n",
      "Iteration 315, loss = 0.34499965\n",
      "Iteration 810, loss = 0.30122746\n",
      "Iteration 283, loss = 0.40457665\n",
      "Iteration 1312, loss = 0.26429623\n",
      "Iteration 316, loss = 0.34486751\n",
      "Iteration 581, loss = 0.34277351\n",
      "Iteration 328, loss = 0.35173531\n",
      "Iteration 317, loss = 0.34472134\n",
      "Iteration 811, loss = 0.30104728\n",
      "Iteration 318, loss = 0.34459060\n",
      "Iteration 329, loss = 0.35156681\n",
      "Iteration 330, loss = 0.35139487\n",
      "Iteration 812, loss = 0.30094564\n",
      "Iteration 331, loss = 0.35126411\n",
      "Iteration 319, loss = 0.34446630\n",
      "Iteration 1313, loss = 0.26419628\n",
      "Iteration 332, loss = 0.35109566\n",
      "Iteration 813, loss = 0.30079705\n",
      "Iteration 315, loss = 0.39029270\n",
      "Iteration 333, loss = 0.35093056\n",
      "Iteration 320, loss = 0.34432266\n",
      "Iteration 582, loss = 0.34261343\n",
      "Iteration 1314, loss = 0.26410588\n",
      "Iteration 334, loss = 0.35075694\n",
      "Iteration 1315, loss = 0.26400831\n",
      "Iteration 814, loss = 0.30062537\n",
      "Iteration 335, loss = 0.35059585\n",
      "Iteration 1316, loss = 0.26392446\n",
      "Iteration 321, loss = 0.34417674\n",
      "Iteration 815, loss = 0.30047535\n",
      "Iteration 583, loss = 0.34251062\n",
      "Iteration 336, loss = 0.35045791\n",
      "Iteration 131, loss = 0.52885088\n",
      "Iteration 337, loss = 0.35029838\n",
      "Iteration 322, loss = 0.34406358\n",
      "Iteration 338, loss = 0.35013053\n",
      "Iteration 816, loss = 0.30037265\n",
      "Iteration 284, loss = 0.40426594\n",
      "Iteration 584, loss = 0.34239439\n",
      "Iteration 323, loss = 0.34391936\n",
      "Iteration 339, loss = 0.34998118\n",
      "Iteration 1317, loss = 0.26382099\n",
      "Iteration 324, loss = 0.34378991\n",
      "Iteration 340, loss = 0.34983894\n",
      "Iteration 817, loss = 0.30025908\n",
      "Iteration 1318, loss = 0.26372102\n",
      "Iteration 325, loss = 0.34366068\n",
      "Iteration 818, loss = 0.30002571\n",
      "Iteration 1319, loss = 0.26365087\n",
      "Iteration 341, loss = 0.34968119\n",
      "Iteration 585, loss = 0.34228875\n",
      "Iteration 326, loss = 0.34352855Iteration 342, loss = 0.34953228\n",
      "Iteration 1320, loss = 0.26357230\n",
      "\n",
      "Iteration 819, loss = 0.29991617\n",
      "Iteration 1321, loss = 0.26345745\n",
      "Iteration 316, loss = 0.39008123\n",
      "Iteration 327, loss = 0.34339667\n",
      "Iteration 586, loss = 0.34216334\n",
      "Iteration 343, loss = 0.34938087\n",
      "Iteration 344, loss = 0.34922203\n",
      "Iteration 1322, loss = 0.26336932\n",
      "Iteration 820, loss = 0.29978475\n",
      "Iteration 587, loss = 0.34205759\n",
      "Iteration 132, loss = 0.52752708\n",
      "Iteration 328, loss = 0.34327800\n",
      "Iteration 345, loss = 0.34907669\n",
      "Iteration 1323, loss = 0.26325758\n",
      "Iteration 329, loss = 0.34314576\n",
      "Iteration 588, loss = 0.34193859\n",
      "Iteration 346, loss = 0.34893041\n",
      "Iteration 1324, loss = 0.26315318\n",
      "Iteration 330, loss = 0.34302693\n",
      "Iteration 821, loss = 0.29960792\n",
      "Iteration 285, loss = 0.40404861\n",
      "Iteration 331, loss = 0.34289076\n",
      "Iteration 317, loss = 0.38983528\n",
      "Iteration 332, loss = 0.34276902\n",
      "Iteration 347, loss = 0.34877772\n",
      "Iteration 333, loss = 0.34265939\n",
      "Iteration 1325, loss = 0.26305951\n",
      "Iteration 334, loss = 0.34251494\n",
      "Iteration 822, loss = 0.29951192\n",
      "Iteration 348, loss = 0.34862264\n",
      "Iteration 349, loss = 0.34848302\n",
      "Iteration 1326, loss = 0.26299187\n",
      "Iteration 335, loss = 0.34239118\n",
      "Iteration 823, loss = 0.29934185\n",
      "Iteration 350, loss = 0.34834074\n",
      "Iteration 589, loss = 0.34190785\n",
      "Iteration 336, loss = 0.34226839\n",
      "Iteration 351, loss = 0.34821323\n",
      "Iteration 824, loss = 0.29918334\n",
      "Iteration 286, loss = 0.40380151\n",
      "Iteration 337, loss = 0.34214285\n",
      "Iteration 1327, loss = 0.26286714\n",
      "Iteration 352, loss = 0.34804913\n",
      "Iteration 338, loss = 0.34202882\n",
      "Iteration 353, loss = 0.34790549\n",
      "Iteration 339, loss = 0.34191142\n",
      "Iteration 1328, loss = 0.26279163\n",
      "Iteration 354, loss = 0.34776477\n",
      "Iteration 825, loss = 0.29905425\n",
      "Iteration 590, loss = 0.34172831\n",
      "Iteration 1329, loss = 0.26268253\n",
      "Iteration 355, loss = 0.34762860\n",
      "Iteration 340, loss = 0.34177907\n",
      "Iteration 1330, loss = 0.26256795\n",
      "Iteration 341, loss = 0.34166790\n",
      "Iteration 356, loss = 0.34747404\n",
      "Iteration 342, loss = 0.34153773\n",
      "Iteration 826, loss = 0.29888245\n",
      "Iteration 287, loss = 0.40358030\n",
      "Iteration 1331, loss = 0.26247920\n",
      "Iteration 343, loss = 0.34141937\n",
      "Iteration 133, loss = 0.52618876\n",
      "Iteration 357, loss = 0.34734025\n",
      "Iteration 827, loss = 0.29872214\n",
      "Iteration 1332, loss = 0.26242486\n",
      "Iteration 358, loss = 0.34720298\n",
      "Iteration 1333, loss = 0.26236195\n",
      "Iteration 344, loss = 0.34130985\n",
      "Iteration 359, loss = 0.34704204\n",
      "Iteration 828, loss = 0.29868375\n",
      "Iteration 591, loss = 0.34159823\n",
      "Iteration 1334, loss = 0.26231310\n",
      "Iteration 829, loss = 0.29844212\n",
      "Iteration 345, loss = 0.34119196\n",
      "Iteration 360, loss = 0.34691281\n",
      "Iteration 361, loss = 0.34677241\n",
      "Iteration 1335, loss = 0.26209396\n",
      "Iteration 592, loss = 0.34145806\n",
      "Iteration 830, loss = 0.29829807\n",
      "Iteration 362, loss = 0.34665139\n",
      "Iteration 1336, loss = 0.26200839\n",
      "Iteration 346, loss = 0.34106845\n",
      "Iteration 288, loss = 0.40332259\n",
      "Iteration 1337, loss = 0.26190543\n",
      "Iteration 363, loss = 0.34650662\n",
      "Iteration 347, loss = 0.34094765\n",
      "Iteration 831, loss = 0.29817098\n",
      "Iteration 364, loss = 0.34635089\n",
      "Iteration 365, loss = 0.34622452\n",
      "Iteration 348, loss = 0.34084862\n",
      "Iteration 366, loss = 0.34609365\n",
      "Iteration 832, loss = 0.29800315\n",
      "Iteration 1338, loss = 0.26186409\n",
      "Iteration 593, loss = 0.34135156\n",
      "Iteration 367, loss = 0.34594398\n",
      "Iteration 349, loss = 0.34072134\n",
      "Iteration 833, loss = 0.29789133\n",
      "Iteration 368, loss = 0.34581622\n",
      "Iteration 350, loss = 0.34059615\n",
      "Iteration 369, loss = 0.34569697\n",
      "Iteration 351, loss = 0.34049880\n",
      "Iteration 1339, loss = 0.26179878\n",
      "Iteration 834, loss = 0.29772761\n",
      "Iteration 370, loss = 0.34555053\n",
      "Iteration 594, loss = 0.34123714\n",
      "Iteration 352, loss = 0.34037370\n",
      "Iteration 371, loss = 0.34542606\n",
      "Iteration 134, loss = 0.52484551\n",
      "Iteration 1340, loss = 0.26162435\n",
      "Iteration 372, loss = 0.34530187\n",
      "Iteration 289, loss = 0.40307823\n",
      "Iteration 1341, loss = 0.26157258\n",
      "Iteration 373, loss = 0.34515247\n",
      "Iteration 353, loss = 0.34029035\n",
      "Iteration 374, loss = 0.34502772\n",
      "Iteration 595, loss = 0.34110302\n",
      "Iteration 1342, loss = 0.26144460\n",
      "Iteration 835, loss = 0.29762515\n",
      "Iteration 375, loss = 0.34489047\n",
      "Iteration 354, loss = 0.34015266\n",
      "Iteration 1343, loss = 0.26133716\n",
      "Iteration 376, loss = 0.34475040\n",
      "Iteration 377, loss = 0.34463102\n",
      "Iteration 596, loss = 0.34099578\n",
      "Iteration 836, loss = 0.29744619\n",
      "Iteration 355, loss = 0.34004846\n",
      "Iteration 378, loss = 0.34451120\n",
      "Iteration 290, loss = 0.40287790\n",
      "Iteration 1344, loss = 0.26130169\n",
      "Iteration 837, loss = 0.29726487\n",
      "Iteration 379, loss = 0.34435586\n",
      "Iteration 356, loss = 0.33992383\n",
      "Iteration 597, loss = 0.34090845\n",
      "Iteration 1345, loss = 0.26115662\n",
      "Iteration 380, loss = 0.34423857\n",
      "Iteration 838, loss = 0.29715426\n",
      "Iteration 357, loss = 0.33980875\n",
      "Iteration 381, loss = 0.34410044\n",
      "Iteration 358, loss = 0.33970681\n",
      "Iteration 382, loss = 0.34396823\n",
      "Iteration 839, loss = 0.29698567\n",
      "Iteration 359, loss = 0.33961502\n",
      "Iteration 383, loss = 0.34384419\n",
      "Iteration 1346, loss = 0.26107326\n",
      "Iteration 360, loss = 0.33948325\n",
      "Iteration 384, loss = 0.34371351\n",
      "Iteration 840, loss = 0.29683972\n",
      "Iteration 291, loss = 0.40260231\n",
      "Iteration 385, loss = 0.34358105\n",
      "Iteration 361, loss = 0.33942864\n",
      "Iteration 386, loss = 0.34345432\n",
      "Iteration 841, loss = 0.29680274\n",
      "Iteration 387, loss = 0.34334520\n",
      "Iteration 1347, loss = 0.26107218\n",
      "Iteration 598, loss = 0.34080587\n",
      "Iteration 362, loss = 0.33926405\n",
      "Iteration 135, loss = 0.52364222\n",
      "Iteration 842, loss = 0.29655573\n",
      "Iteration 388, loss = 0.34320839\n",
      "Iteration 843, loss = 0.29648881\n",
      "Iteration 599, loss = 0.34063109\n",
      "Iteration 1348, loss = 0.26089195\n",
      "Iteration 844, loss = 0.29637529\n",
      "Iteration 1349, loss = 0.26080423\n",
      "Iteration 363, loss = 0.33915908\n",
      "Iteration 389, loss = 0.34309516\n",
      "Iteration 1350, loss = 0.26071085\n",
      "Iteration 600, loss = 0.34053271\n",
      "Iteration 390, loss = 0.34295220\n",
      "Iteration 364, loss = 0.33904421\n",
      "Iteration 391, loss = 0.34283033\n",
      "Iteration 1351, loss = 0.26065723\n",
      "Iteration 392, loss = 0.34270690\n",
      "Iteration 365, loss = 0.33894366\n",
      "Iteration 1352, loss = 0.26052483\n",
      "Iteration 393, loss = 0.34258988\n",
      "Iteration 845, loss = 0.29612519\n",
      "Iteration 394, loss = 0.34246242\n",
      "Iteration 1353, loss = 0.26041866\n",
      "Iteration 366, loss = 0.33884601\n",
      "Iteration 601, loss = 0.34041583\n",
      "Iteration 395, loss = 0.34235365\n",
      "Iteration 1354, loss = 0.26033437\n",
      "Iteration 396, loss = 0.34222997\n",
      "Iteration 367, loss = 0.33871913\n",
      "Iteration 397, loss = 0.34211169\n",
      "Iteration 292, loss = 0.40237915\n",
      "Iteration 398, loss = 0.34197261\n",
      "Iteration 602, loss = 0.34028986\n",
      "Iteration 318, loss = 0.38963584\n",
      "Iteration 399, loss = 0.34187645\n",
      "Iteration 368, loss = 0.33862019\n",
      "Iteration 400, loss = 0.34174163\n",
      "Iteration 603, loss = 0.34021054\n",
      "Iteration 846, loss = 0.29603051\n",
      "Iteration 1355, loss = 0.26023844\n",
      "Iteration 369, loss = 0.33852199\n",
      "Iteration 401, loss = 0.34165339\n",
      "Iteration 136, loss = 0.52237851\n",
      "Iteration 293, loss = 0.40215564\n",
      "Iteration 1356, loss = 0.26015544\n",
      "Iteration 847, loss = 0.29592196\n",
      "Iteration 402, loss = 0.34149349\n",
      "Iteration 403, loss = 0.34137524\n",
      "Iteration 1357, loss = 0.26004988\n",
      "Iteration 370, loss = 0.33841070\n",
      "Iteration 404, loss = 0.34127056\n",
      "Iteration 1358, loss = 0.25996996\n",
      "Iteration 405, loss = 0.34115038\n",
      "Iteration 371, loss = 0.33830268\n",
      "Iteration 406, loss = 0.34103045\n",
      "Iteration 1359, loss = 0.25990012\n",
      "Iteration 407, loss = 0.34090812\n",
      "Iteration 1360, loss = 0.25983244\n",
      "Iteration 848, loss = 0.29569261\n",
      "Iteration 408, loss = 0.34078115\n",
      "Iteration 604, loss = 0.34006667\n",
      "Iteration 372, loss = 0.33820907\n",
      "Iteration 409, loss = 0.34067089\n",
      "Iteration 1361, loss = 0.25970948\n",
      "Iteration 410, loss = 0.34054732\n",
      "Iteration 1362, loss = 0.25966584\n",
      "Iteration 373, loss = 0.33810098Iteration 411, loss = 0.34047711\n",
      "\n",
      "Iteration 605, loss = 0.33996730\n",
      "Iteration 412, loss = 0.34031620\n",
      "Iteration 1363, loss = 0.25952133\n",
      "Iteration 374, loss = 0.33800931\n",
      "Iteration 413, loss = 0.34020060\n",
      "Iteration 375, loss = 0.33789456\n",
      "Iteration 1364, loss = 0.25940131\n",
      "Iteration 849, loss = 0.29553275\n",
      "Iteration 137, loss = 0.52114242\n",
      "Iteration 414, loss = 0.34008628\n",
      "Iteration 376, loss = 0.33778264\n",
      "Iteration 415, loss = 0.33997563\n",
      "Iteration 1365, loss = 0.25932065\n",
      "Iteration 377, loss = 0.33767549\n",
      "Iteration 606, loss = 0.33984130\n",
      "Iteration 416, loss = 0.33985110\n",
      "Iteration 378, loss = 0.33759104\n",
      "Iteration 379, loss = 0.33748149\n",
      "Iteration 850, loss = 0.29542665\n",
      "Iteration 380, loss = 0.33738465\n",
      "Iteration 417, loss = 0.33974226\n",
      "Iteration 381, loss = 0.33728081\n",
      "Iteration 1366, loss = 0.25921290\n",
      "Iteration 851, loss = 0.29524813\n",
      "Iteration 294, loss = 0.40195576\n",
      "Iteration 382, loss = 0.33718161\n",
      "Iteration 607, loss = 0.33973318\n",
      "Iteration 418, loss = 0.33965755\n",
      "Iteration 383, loss = 0.33708441\n",
      "Iteration 852, loss = 0.29510404\n",
      "Iteration 1367, loss = 0.25912016\n",
      "Iteration 384, loss = 0.33697876\n",
      "Iteration 1368, loss = 0.25904619\n",
      "Iteration 608, loss = 0.33960029\n",
      "Iteration 385, loss = 0.33689187\n",
      "Iteration 1369, loss = 0.25894774\n",
      "Iteration 386, loss = 0.33678719\n",
      "Iteration 387, loss = 0.33668852\n",
      "Iteration 388, loss = 0.33659855\n",
      "Iteration 609, loss = 0.33958811\n",
      "Iteration 419, loss = 0.33950838\n",
      "Iteration 853, loss = 0.29496866\n",
      "Iteration 420, loss = 0.33940257\n",
      "Iteration 389, loss = 0.33649380\n",
      "Iteration 854, loss = 0.29487482\n",
      "Iteration 1370, loss = 0.25886907\n",
      "Iteration 421, loss = 0.33928059\n",
      "Iteration 319, loss = 0.38942592\n",
      "Iteration 855, loss = 0.29481493\n",
      "Iteration 1371, loss = 0.25876059\n",
      "Iteration 610, loss = 0.33939825\n",
      "Iteration 1372, loss = 0.25867727\n",
      "Iteration 422, loss = 0.33917334\n",
      "Iteration 390, loss = 0.33639792\n",
      "Iteration 856, loss = 0.29450013\n",
      "Iteration 1373, loss = 0.25858460\n",
      "Iteration 391, loss = 0.33629685\n",
      "Iteration 857, loss = 0.29438461\n",
      "Iteration 423, loss = 0.33906727\n",
      "Iteration 611, loss = 0.33926696\n",
      "Iteration 392, loss = 0.33620259\n",
      "Iteration 1374, loss = 0.25847551\n",
      "Iteration 295, loss = 0.40169850\n",
      "Iteration 393, loss = 0.33611882\n",
      "Iteration 858, loss = 0.29424632\n",
      "Iteration 1375, loss = 0.25841268\n",
      "Iteration 424, loss = 0.33895188\n",
      "Iteration 394, loss = 0.33602332\n",
      "Iteration 138, loss = 0.51990970\n",
      "Iteration 1376, loss = 0.25828655\n",
      "Iteration 612, loss = 0.33913875\n",
      "Iteration 395, loss = 0.33592623\n",
      "Iteration 396, loss = 0.33583574\n",
      "Iteration 1377, loss = 0.25820503\n",
      "Iteration 859, loss = 0.29409513\n",
      "Iteration 397, loss = 0.33573643\n",
      "Iteration 398, loss = 0.33563420\n",
      "Iteration 425, loss = 0.33883539\n",
      "Iteration 860, loss = 0.29392660\n",
      "Iteration 1378, loss = 0.25815106\n",
      "Iteration 399, loss = 0.33554113\n",
      "Iteration 1379, loss = 0.25801579\n",
      "Iteration 861, loss = 0.29382630\n",
      "Iteration 613, loss = 0.33903030\n",
      "Iteration 426, loss = 0.33871831\n",
      "Iteration 1380, loss = 0.25792396\n",
      "Iteration 862, loss = 0.29363455\n",
      "Iteration 1381, loss = 0.25784476\n",
      "Iteration 427, loss = 0.33859988\n",
      "Iteration 863, loss = 0.29352075\n",
      "Iteration 614, loss = 0.33891614\n",
      "Iteration 1382, loss = 0.25776753\n",
      "Iteration 296, loss = 0.40143370\n",
      "Iteration 428, loss = 0.33850509\n",
      "Iteration 1383, loss = 0.25762967\n",
      "Iteration 400, loss = 0.33545461\n",
      "Iteration 1384, loss = 0.25758117\n",
      "Iteration 429, loss = 0.33838464\n",
      "Iteration 1385, loss = 0.25747858\n",
      "Iteration 401, loss = 0.33538161\n",
      "Iteration 430, loss = 0.33826874\n",
      "Iteration 864, loss = 0.29338988\n",
      "Iteration 615, loss = 0.33881792\n",
      "Iteration 1386, loss = 0.25737341\n",
      "Iteration 431, loss = 0.33817059\n",
      "Iteration 139, loss = 0.51875148\n",
      "Iteration 432, loss = 0.33805784\n",
      "Iteration 402, loss = 0.33526633\n",
      "Iteration 433, loss = 0.33794701\n",
      "Iteration 297, loss = 0.40121395\n",
      "Iteration 434, loss = 0.33782967\n",
      "Iteration 1387, loss = 0.25737713\n",
      "Iteration 403, loss = 0.33517144\n",
      "Iteration 435, loss = 0.33774921\n",
      "Iteration 436, loss = 0.33762407\n",
      "Iteration 437, loss = 0.33751198\n",
      "Iteration 438, loss = 0.33739721\n",
      "Iteration 404, loss = 0.33508276\n",
      "Iteration 439, loss = 0.33728647\n",
      "Iteration 616, loss = 0.33866175\n",
      "Iteration 405, loss = 0.33500636\n",
      "Iteration 1388, loss = 0.25720815\n",
      "Iteration 298, loss = 0.40100440\n",
      "Iteration 406, loss = 0.33491029\n",
      "Iteration 140, loss = 0.51759251\n",
      "Iteration 865, loss = 0.29321395\n",
      "Iteration 407, loss = 0.33481062\n",
      "Iteration 440, loss = 0.33719269\n",
      "Iteration 408, loss = 0.33472179\n",
      "Iteration 441, loss = 0.33707864\n",
      "Iteration 866, loss = 0.29311051\n",
      "Iteration 1389, loss = 0.25715996\n",
      "Iteration 442, loss = 0.33696826\n",
      "Iteration 409, loss = 0.33463712\n",
      "Iteration 867, loss = 0.29294029\n",
      "Iteration 443, loss = 0.33685760\n",
      "Iteration 410, loss = 0.33454871\n",
      "Iteration 320, loss = 0.38915799\n",
      "Iteration 411, loss = 0.33446965\n",
      "Iteration 617, loss = 0.33860657\n",
      "Iteration 412, loss = 0.33436808\n",
      "Iteration 1390, loss = 0.25699448\n",
      "Iteration 444, loss = 0.33674962\n",
      "Iteration 868, loss = 0.29278230\n",
      "Iteration 413, loss = 0.33427546\n",
      "Iteration 299, loss = 0.40077772\n",
      "Iteration 414, loss = 0.33421500\n",
      "Iteration 415, loss = 0.33411277\n",
      "Iteration 445, loss = 0.33664287\n",
      "Iteration 618, loss = 0.33843761\n",
      "Iteration 1391, loss = 0.25690873\n",
      "Iteration 869, loss = 0.29264523\n",
      "Iteration 141, loss = 0.51643333\n",
      "Iteration 1392, loss = 0.25683607\n",
      "Iteration 870, loss = 0.29252453\n",
      "Iteration 619, loss = 0.33832377\n",
      "Iteration 321, loss = 0.38886423\n",
      "Iteration 300, loss = 0.40055705\n",
      "Iteration 1393, loss = 0.25682541\n",
      "Iteration 416, loss = 0.33402692\n",
      "Iteration 446, loss = 0.33653640\n",
      "Iteration 1394, loss = 0.25663978\n",
      "Iteration 871, loss = 0.29233475\n",
      "Iteration 417, loss = 0.33392902\n",
      "Iteration 1395, loss = 0.25655883\n",
      "Iteration 447, loss = 0.33644362\n",
      "Iteration 418, loss = 0.33384449\n",
      "Iteration 1396, loss = 0.25656715\n",
      "Iteration 620, loss = 0.33821885\n",
      "Iteration 448, loss = 0.33632370\n",
      "Iteration 322, loss = 0.38864849\n",
      "Iteration 419, loss = 0.33376255\n",
      "Iteration 1397, loss = 0.25635937\n",
      "Iteration 872, loss = 0.29218235\n",
      "Iteration 420, loss = 0.33368027\n",
      "Iteration 1398, loss = 0.25625551\n",
      "Iteration 449, loss = 0.33621336\n",
      "Iteration 142, loss = 0.51527484Iteration 1399, loss = 0.25617606\n",
      "Iteration 421, loss = 0.33359265\n",
      "Iteration 621, loss = 0.33810658\n",
      "Iteration 323, loss = 0.38841134\n",
      "Iteration 1400, loss = 0.25611055\n",
      "\n",
      "Iteration 450, loss = 0.33611836\n",
      "Iteration 873, loss = 0.29213782\n",
      "Iteration 1401, loss = 0.25598868\n",
      "Iteration 451, loss = 0.33599815\n",
      "Iteration 874, loss = 0.29197662\n",
      "Iteration 422, loss = 0.33350895\n",
      "Iteration 324, loss = 0.38819260\n",
      "Iteration 452, loss = 0.33590613\n",
      "Iteration 301, loss = 0.40030991\n",
      "Iteration 1402, loss = 0.25589592\n",
      "Iteration 875, loss = 0.29185685\n",
      "Iteration 423, loss = 0.33342061\n",
      "Iteration 622, loss = 0.33798455\n",
      "Iteration 453, loss = 0.33579414\n",
      "Iteration 424, loss = 0.33333159\n",
      "Iteration 1403, loss = 0.25578330\n",
      "Iteration 425, loss = 0.33326580\n",
      "Iteration 454, loss = 0.33567829\n",
      "Iteration 876, loss = 0.29162238\n",
      "Iteration 1404, loss = 0.25574326\n",
      "Iteration 623, loss = 0.33789104\n",
      "Iteration 455, loss = 0.33558145\n",
      "Iteration 426, loss = 0.33317610\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 302, loss = 0.40008308\n",
      "Iteration 456, loss = 0.33546910\n",
      "Iteration 1405, loss = 0.25581471\n",
      "Iteration 143, loss = 0.51423449\n",
      "Iteration 1406, loss = 0.25551156\n",
      "Iteration 624, loss = 0.33774498\n",
      "Iteration 1407, loss = 0.25546355\n",
      "Iteration 303, loss = 0.39985971\n",
      "Iteration 1408, loss = 0.25533431\n",
      "Iteration 625, loss = 0.33764938\n",
      "Iteration 457, loss = 0.33536511\n",
      "Iteration 1409, loss = 0.25524348\n",
      "Iteration 458, loss = 0.33524702\n",
      "Iteration 459, loss = 0.33520274\n",
      "Iteration 626, loss = 0.33752562\n",
      "Iteration 460, loss = 0.33505282\n",
      "Iteration 144, loss = 0.51311606\n",
      "Iteration 877, loss = 0.29153078\n",
      "Iteration 1410, loss = 0.25516387\n",
      "Iteration 461, loss = 0.33493632\n",
      "Iteration 325, loss = 0.38793521Iteration 462, loss = 0.33484566\n",
      "\n",
      "Iteration 1411, loss = 0.25508260\n",
      "Iteration 1, loss = 0.70334932\n",
      "Iteration 1412, loss = 0.25501305\n",
      "Iteration 463, loss = 0.33473201\n",
      "Iteration 878, loss = 0.29133856\n",
      "Iteration 2, loss = 0.70177640\n",
      "Iteration 464, loss = 0.33462858\n",
      "Iteration 3, loss = 0.69918269\n",
      "Iteration 326, loss = 0.38776149\n",
      "Iteration 304, loss = 0.39967280\n",
      "Iteration 1413, loss = 0.25487834\n",
      "Iteration 4, loss = 0.69617384\n",
      "Iteration 465, loss = 0.33454072\n",
      "Iteration 879, loss = 0.29121856\n",
      "Iteration 1414, loss = 0.25476069\n",
      "Iteration 5, loss = 0.69279074\n",
      "Iteration 627, loss = 0.33744316\n",
      "Iteration 466, loss = 0.33441939\n",
      "Iteration 6, loss = 0.68899440\n",
      "Iteration 880, loss = 0.29114369\n",
      "Iteration 1415, loss = 0.25475130\n",
      "Iteration 327, loss = 0.38751815\n",
      "Iteration 7, loss = 0.68515050\n",
      "Iteration 467, loss = 0.33433648\n",
      "Iteration 881, loss = 0.29091325\n",
      "Iteration 8, loss = 0.68104711\n",
      "Iteration 628, loss = 0.33730929\n",
      "Iteration 468, loss = 0.33423746\n",
      "Iteration 1416, loss = 0.25455851\n",
      "Iteration 882, loss = 0.29078810\n",
      "Iteration 9, loss = 0.67696323\n",
      "Iteration 328, loss = 0.38724308\n",
      "Iteration 469, loss = 0.33410814\n",
      "Iteration 10, loss = 0.67288354\n",
      "Iteration 470, loss = 0.33401022\n",
      "Iteration 1417, loss = 0.25449569\n",
      "Iteration 471, loss = 0.33389952\n",
      "Iteration 883, loss = 0.29066315\n",
      "Iteration 11, loss = 0.66880269\n",
      "Iteration 472, loss = 0.33380218\n",
      "Iteration 329, loss = 0.38709136\n",
      "Iteration 1418, loss = 0.25439887\n",
      "Iteration 305, loss = 0.39942138\n",
      "Iteration 884, loss = 0.29047313\n",
      "Iteration 12, loss = 0.66478294\n",
      "Iteration 629, loss = 0.33719469\n",
      "Iteration 473, loss = 0.33370778\n",
      "Iteration 885, loss = 0.29038285\n",
      "Iteration 474, loss = 0.33359978\n",
      "Iteration 1419, loss = 0.25430896\n",
      "Iteration 886, loss = 0.29022852\n",
      "Iteration 475, loss = 0.33349974\n",
      "Iteration 330, loss = 0.38681097\n",
      "Iteration 13, loss = 0.66070415\n",
      "Iteration 476, loss = 0.33341267\n",
      "Iteration 887, loss = 0.29007097\n",
      "Iteration 14, loss = 0.65673899\n",
      "Iteration 477, loss = 0.33329683\n",
      "Iteration 1420, loss = 0.25423051\n",
      "Iteration 478, loss = 0.33320958\n",
      "Iteration 888, loss = 0.28991945\n",
      "Iteration 15, loss = 0.65284646\n",
      "Iteration 630, loss = 0.33706977\n",
      "Iteration 889, loss = 0.28977827\n",
      "Iteration 479, loss = 0.33309189\n",
      "Iteration 145, loss = 0.51209644\n",
      "Iteration 480, loss = 0.33299467\n",
      "Iteration 481, loss = 0.33290387\n",
      "Iteration 16, loss = 0.64891265\n",
      "Iteration 482, loss = 0.33280552\n",
      "Iteration 890, loss = 0.28964075\n",
      "Iteration 306, loss = 0.39920067\n",
      "Iteration 1421, loss = 0.25407026\n",
      "Iteration 331, loss = 0.38661937\n",
      "Iteration 631, loss = 0.33697465\n",
      "Iteration 483, loss = 0.33270045\n",
      "Iteration 891, loss = 0.28949973\n",
      "Iteration 1422, loss = 0.25399895\n",
      "Iteration 17, loss = 0.64512735\n",
      "Iteration 1423, loss = 0.25392011\n",
      "Iteration 484, loss = 0.33260099\n",
      "Iteration 18, loss = 0.64127097\n",
      "Iteration 1424, loss = 0.25379104\n",
      "Iteration 485, loss = 0.33251719\n",
      "Iteration 892, loss = 0.28933370\n",
      "Iteration 19, loss = 0.63753730\n",
      "Iteration 1425, loss = 0.25370082\n",
      "Iteration 486, loss = 0.33241563\n",
      "Iteration 20, loss = 0.63372626\n",
      "Iteration 893, loss = 0.28921344\n",
      "Iteration 1426, loss = 0.25360298\n",
      "Iteration 487, loss = 0.33232568\n",
      "Iteration 21, loss = 0.62994121\n",
      "Iteration 307, loss = 0.39898490\n",
      "Iteration 488, loss = 0.33222430\n",
      "Iteration 894, loss = 0.28908514\n",
      "Iteration 632, loss = 0.33683459\n",
      "Iteration 332, loss = 0.38635376\n",
      "Iteration 1427, loss = 0.25356069\n",
      "Iteration 489, loss = 0.33212595\n",
      "Iteration 22, loss = 0.62651237\n",
      "Iteration 490, loss = 0.33202524\n",
      "Iteration 1428, loss = 0.25339661\n",
      "Iteration 895, loss = 0.28899500\n",
      "Iteration 491, loss = 0.33192738\n",
      "Iteration 146, loss = 0.51095866\n",
      "Iteration 492, loss = 0.33183988\n",
      "Iteration 23, loss = 0.62288788\n",
      "Iteration 633, loss = 0.33676764\n",
      "Iteration 24, loss = 0.61929911\n",
      "Iteration 1429, loss = 0.25330991\n",
      "Iteration 493, loss = 0.33174467\n",
      "Iteration 25, loss = 0.61580611\n",
      "Iteration 333, loss = 0.38611250\n",
      "Iteration 494, loss = 0.33166846Iteration 634, loss = 0.33661276\n",
      "\n",
      "Iteration 26, loss = 0.61222647\n",
      "Iteration 896, loss = 0.28882072\n",
      "Iteration 308, loss = 0.39874719\n",
      "Iteration 1430, loss = 0.25331686\n",
      "Iteration 897, loss = 0.28862243\n",
      "Iteration 495, loss = 0.33154116\n",
      "Iteration 1431, loss = 0.25314182\n",
      "Iteration 898, loss = 0.28846745\n",
      "Iteration 1432, loss = 0.25299738\n",
      "Iteration 334, loss = 0.38589162\n",
      "Iteration 496, loss = 0.33144834\n",
      "Iteration 27, loss = 0.60875457\n",
      "Iteration 899, loss = 0.28839429\n",
      "Iteration 1433, loss = 0.25290562\n",
      "Iteration 28, loss = 0.60529370\n",
      "Iteration 497, loss = 0.33135760\n",
      "Iteration 900, loss = 0.28819581\n",
      "Iteration 29, loss = 0.60190877\n",
      "Iteration 498, loss = 0.33127446\n",
      "Iteration 499, loss = 0.33116245\n",
      "Iteration 901, loss = 0.28813422\n",
      "Iteration 30, loss = 0.59847406\n",
      "Iteration 500, loss = 0.33107197\n",
      "Iteration 309, loss = 0.39855789\n",
      "Iteration 31, loss = 0.59505365\n",
      "Iteration 902, loss = 0.28793508\n",
      "Iteration 501, loss = 0.33098854\n",
      "Iteration 502, loss = 0.33089049\n",
      "Iteration 32, loss = 0.59177706\n",
      "Iteration 903, loss = 0.28778476\n",
      "Iteration 503, loss = 0.33079394\n",
      "Iteration 635, loss = 0.33653115\n",
      "Iteration 1434, loss = 0.25282999\n",
      "Iteration 504, loss = 0.33069720\n",
      "Iteration 335, loss = 0.38566922\n",
      "Iteration 904, loss = 0.28767937\n",
      "Iteration 505, loss = 0.33062019\n",
      "Iteration 147, loss = 0.50991464\n",
      "Iteration 33, loss = 0.58852272\n",
      "Iteration 1435, loss = 0.25275115\n",
      "Iteration 905, loss = 0.28755184\n",
      "Iteration 34, loss = 0.58516192\n",
      "Iteration 506, loss = 0.33050800\n",
      "Iteration 1436, loss = 0.25260404\n",
      "Iteration 906, loss = 0.28734644\n",
      "Iteration 35, loss = 0.58188788\n",
      "Iteration 636, loss = 0.33641467\n",
      "Iteration 36, loss = 0.57867759\n",
      "Iteration 907, loss = 0.28719227\n",
      "Iteration 37, loss = 0.57541869\n",
      "Iteration 1437, loss = 0.25251009\n",
      "Iteration 908, loss = 0.28713899\n",
      "Iteration 336, loss = 0.38562043\n",
      "Iteration 637, loss = 0.33626422\n",
      "Iteration 1438, loss = 0.25243706\n",
      "Iteration 507, loss = 0.33042682\n",
      "Iteration 909, loss = 0.28695759\n",
      "Iteration 310, loss = 0.39831535\n",
      "Iteration 1439, loss = 0.25241906\n",
      "Iteration 38, loss = 0.57229926\n",
      "Iteration 508, loss = 0.33032961\n",
      "Iteration 1440, loss = 0.25220936\n",
      "Iteration 638, loss = 0.33619848\n",
      "Iteration 910, loss = 0.28676625\n",
      "Iteration 1441, loss = 0.25217651\n",
      "Iteration 39, loss = 0.56915262\n",
      "Iteration 509, loss = 0.33024036\n",
      "Iteration 337, loss = 0.38526073\n",
      "Iteration 1442, loss = 0.25203349\n",
      "Iteration 40, loss = 0.56597304\n",
      "Iteration 1443, loss = 0.25193335\n",
      "Iteration 911, loss = 0.28667881\n",
      "Iteration 639, loss = 0.33606259\n",
      "Iteration 1444, loss = 0.25180484\n",
      "Iteration 510, loss = 0.33015630\n",
      "Iteration 41, loss = 0.56293563\n",
      "Iteration 912, loss = 0.28651042\n",
      "Iteration 1445, loss = 0.25171837\n",
      "Iteration 311, loss = 0.39811781\n",
      "Iteration 511, loss = 0.33006187\n",
      "Iteration 1446, loss = 0.25159372\n",
      "Iteration 512, loss = 0.32996811\n",
      "Iteration 913, loss = 0.28633820\n",
      "Iteration 513, loss = 0.32986588\n",
      "Iteration 42, loss = 0.55988960\n",
      "Iteration 1447, loss = 0.25150278\n",
      "Iteration 338, loss = 0.38500133\n",
      "Iteration 514, loss = 0.32978969\n",
      "Iteration 914, loss = 0.28621833\n",
      "Iteration 1448, loss = 0.25139140\n",
      "Iteration 515, loss = 0.32970410\n",
      "Iteration 640, loss = 0.33597384\n",
      "Iteration 43, loss = 0.55679168\n",
      "Iteration 516, loss = 0.32961088\n",
      "Iteration 1449, loss = 0.25131331\n",
      "Iteration 44, loss = 0.55373717\n",
      "Iteration 1450, loss = 0.25120465\n",
      "Iteration 517, loss = 0.32954396\n",
      "Iteration 915, loss = 0.28612597\n",
      "Iteration 45, loss = 0.55077189\n",
      "Iteration 1451, loss = 0.25108031\n",
      "Iteration 148, loss = 0.50898469\n",
      "Iteration 46, loss = 0.54781454\n",
      "Iteration 518, loss = 0.32942206\n",
      "Iteration 1452, loss = 0.25104780\n",
      "Iteration 339, loss = 0.38477808\n",
      "Iteration 47, loss = 0.54495213\n",
      "Iteration 519, loss = 0.32933479\n",
      "Iteration 641, loss = 0.33582405\n",
      "Iteration 520, loss = 0.32925883\n",
      "Iteration 916, loss = 0.28593483\n",
      "Iteration 48, loss = 0.54194424\n",
      "Iteration 521, loss = 0.32917297\n",
      "Iteration 312, loss = 0.39789986\n",
      "Iteration 1453, loss = 0.25087148\n",
      "Iteration 49, loss = 0.53911111\n",
      "Iteration 522, loss = 0.32907864\n",
      "Iteration 340, loss = 0.38459142\n",
      "Iteration 1454, loss = 0.25081267\n",
      "Iteration 50, loss = 0.53628901\n",
      "Iteration 523, loss = 0.32900384\n",
      "Iteration 524, loss = 0.32890683\n",
      "Iteration 1455, loss = 0.25067885\n",
      "Iteration 51, loss = 0.53336663\n",
      "Iteration 1456, loss = 0.25063294\n",
      "Iteration 917, loss = 0.28575374\n",
      "Iteration 642, loss = 0.33570797\n",
      "Iteration 341, loss = 0.38445802\n",
      "Iteration 313, loss = 0.39777698\n",
      "Iteration 918, loss = 0.28560529\n",
      "Iteration 52, loss = 0.53061777\n",
      "Iteration 1457, loss = 0.25049152\n",
      "Iteration 525, loss = 0.32881138\n",
      "Iteration 919, loss = 0.28549909\n",
      "Iteration 526, loss = 0.32872831\n",
      "Iteration 1458, loss = 0.25039589\n",
      "Iteration 53, loss = 0.52795527\n",
      "Iteration 920, loss = 0.28533263\n",
      "Iteration 527, loss = 0.32862692\n",
      "Iteration 54, loss = 0.52520210\n",
      "Iteration 528, loss = 0.32855431\n",
      "Iteration 342, loss = 0.38411408\n",
      "Iteration 921, loss = 0.28518546\n",
      "Iteration 1459, loss = 0.25024012\n",
      "Iteration 55, loss = 0.52254145\n",
      "Iteration 529, loss = 0.32847715\n",
      "Iteration 56, loss = 0.51983083\n",
      "Iteration 530, loss = 0.32838633\n",
      "Iteration 922, loss = 0.28507225\n",
      "Iteration 531, loss = 0.32828264\n",
      "Iteration 57, loss = 0.51725247\n",
      "Iteration 532, loss = 0.32825224\n",
      "Iteration 923, loss = 0.28496137\n",
      "Iteration 149, loss = 0.50794020\n",
      "Iteration 58, loss = 0.51466720\n",
      "Iteration 533, loss = 0.32812433\n",
      "Iteration 314, loss = 0.39751098\n",
      "Iteration 643, loss = 0.33559877\n",
      "Iteration 534, loss = 0.32804027\n",
      "Iteration 924, loss = 0.28486467\n",
      "Iteration 59, loss = 0.51205267\n",
      "Iteration 535, loss = 0.32795354\n",
      "Iteration 1460, loss = 0.25016409\n",
      "Iteration 343, loss = 0.38395563\n",
      "Iteration 60, loss = 0.50949416\n",
      "Iteration 536, loss = 0.32786253\n",
      "Iteration 925, loss = 0.28479076\n",
      "Iteration 537, loss = 0.32777061\n",
      "Iteration 1461, loss = 0.25002023\n",
      "Iteration 538, loss = 0.32768136\n",
      "Iteration 926, loss = 0.28451635\n",
      "Iteration 1462, loss = 0.24997322\n",
      "Iteration 927, loss = 0.28431226\n",
      "Iteration 1463, loss = 0.24982135\n",
      "Iteration 539, loss = 0.32760358\n",
      "Iteration 644, loss = 0.33549511\n",
      "Iteration 928, loss = 0.28416652\n",
      "Iteration 540, loss = 0.32752273\n",
      "Iteration 61, loss = 0.50707353\n",
      "Iteration 541, loss = 0.32743188\n",
      "Iteration 929, loss = 0.28405632\n",
      "Iteration 1464, loss = 0.24973625\n",
      "Iteration 542, loss = 0.32735303\n",
      "Iteration 543, loss = 0.32725290\n",
      "Iteration 1465, loss = 0.24962190\n",
      "Iteration 930, loss = 0.28395389\n",
      "Iteration 62, loss = 0.50448779\n",
      "Iteration 544, loss = 0.32718190\n",
      "Iteration 315, loss = 0.39725566\n",
      "Iteration 1466, loss = 0.24952828\n",
      "Iteration 545, loss = 0.32708812\n",
      "Iteration 645, loss = 0.33538293Iteration 344, loss = 0.38366883\n",
      "\n",
      "Iteration 150, loss = 0.50697770\n",
      "Iteration 931, loss = 0.28376873\n",
      "Iteration 546, loss = 0.32700993\n",
      "Iteration 1467, loss = 0.24949200\n",
      "Iteration 547, loss = 0.32691544\n",
      "Iteration 63, loss = 0.50217314\n",
      "Iteration 1468, loss = 0.24931167\n",
      "Iteration 548, loss = 0.32684493\n",
      "Iteration 932, loss = 0.28359750\n",
      "Iteration 64, loss = 0.49979501\n",
      "Iteration 549, loss = 0.32674609\n",
      "Iteration 1469, loss = 0.24922359\n",
      "Iteration 933, loss = 0.28349102\n",
      "Iteration 65, loss = 0.49744430\n",
      "Iteration 646, loss = 0.33526624\n",
      "Iteration 1470, loss = 0.24909834\n",
      "Iteration 934, loss = 0.28338514\n",
      "Iteration 550, loss = 0.32666278\n",
      "Iteration 1471, loss = 0.24901940\n",
      "Iteration 151, loss = 0.50593854\n",
      "Iteration 66, loss = 0.49504544\n",
      "Iteration 935, loss = 0.28336078\n",
      "Iteration 1472, loss = 0.24889317\n",
      "Iteration 647, loss = 0.33517635\n",
      "Iteration 551, loss = 0.32661034\n",
      "Iteration 1473, loss = 0.24879200\n",
      "Iteration 345, loss = 0.38347424\n",
      "Iteration 936, loss = 0.28302685\n",
      "Iteration 552, loss = 0.32649125\n",
      "Iteration 1474, loss = 0.24870845\n",
      "Iteration 648, loss = 0.33502960\n",
      "Iteration 67, loss = 0.49275015\n",
      "Iteration 1475, loss = 0.24857572Iteration 553, loss = 0.32641033\n",
      "Iteration 68, loss = 0.49054350\n",
      "\n",
      "Iteration 937, loss = 0.28290537\n",
      "Iteration 69, loss = 0.48827130\n",
      "Iteration 554, loss = 0.32631689\n",
      "Iteration 1476, loss = 0.24851960\n",
      "Iteration 346, loss = 0.38333449\n",
      "Iteration 555, loss = 0.32626418\n",
      "Iteration 70, loss = 0.48623156\n",
      "Iteration 316, loss = 0.39705010\n",
      "Iteration 556, loss = 0.32615109\n",
      "Iteration 1477, loss = 0.24838625\n",
      "Iteration 71, loss = 0.48394878\n",
      "Iteration 557, loss = 0.32607434\n",
      "Iteration 1478, loss = 0.24829310\n",
      "Iteration 649, loss = 0.33495519\n",
      "Iteration 558, loss = 0.32598827\n",
      "Iteration 938, loss = 0.28276929\n",
      "Iteration 72, loss = 0.48183441\n",
      "Iteration 559, loss = 0.32590754\n",
      "Iteration 1479, loss = 0.24816437\n",
      "Iteration 73, loss = 0.47979257\n",
      "Iteration 560, loss = 0.32582206\n",
      "Iteration 561, loss = 0.32574759\n",
      "Iteration 74, loss = 0.47775259\n",
      "Iteration 152, loss = 0.50502451\n",
      "Iteration 562, loss = 0.32566295\n",
      "Iteration 1480, loss = 0.24804894\n",
      "Iteration 563, loss = 0.32559355\n",
      "Iteration 939, loss = 0.28294165\n",
      "Iteration 347, loss = 0.38302720\n",
      "Iteration 564, loss = 0.32548655\n",
      "Iteration 1481, loss = 0.24801813\n",
      "Iteration 565, loss = 0.32540726\n",
      "Iteration 1482, loss = 0.24784860\n",
      "Iteration 566, loss = 0.32533276\n",
      "Iteration 75, loss = 0.47573120\n",
      "Iteration 567, loss = 0.32523539\n",
      "Iteration 1483, loss = 0.24771130\n",
      "Iteration 650, loss = 0.33487493\n",
      "Iteration 940, loss = 0.28251053\n",
      "Iteration 568, loss = 0.32516014\n",
      "Iteration 1484, loss = 0.24761938\n",
      "Iteration 569, loss = 0.32507533\n",
      "Iteration 317, loss = 0.39683996\n",
      "Iteration 153, loss = 0.50407789\n",
      "Iteration 570, loss = 0.32500090\n",
      "Iteration 941, loss = 0.28230845\n",
      "Iteration 1485, loss = 0.24751469\n",
      "Iteration 76, loss = 0.47382796\n",
      "Iteration 571, loss = 0.32492454\n",
      "Iteration 1486, loss = 0.24744651\n",
      "Iteration 651, loss = 0.33470354\n",
      "Iteration 572, loss = 0.32484662\n",
      "Iteration 348, loss = 0.38282353\n",
      "Iteration 77, loss = 0.47185879\n",
      "Iteration 942, loss = 0.28220259\n",
      "Iteration 573, loss = 0.32474914\n",
      "Iteration 1487, loss = 0.24731053\n",
      "Iteration 78, loss = 0.46996283\n",
      "Iteration 574, loss = 0.32466736\n",
      "Iteration 1488, loss = 0.24724002\n",
      "Iteration 79, loss = 0.46820790\n",
      "Iteration 1489, loss = 0.24715868\n",
      "Iteration 575, loss = 0.32460213\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 80, loss = 0.46628813\n",
      "Iteration 652, loss = 0.33462047\n",
      "Iteration 349, loss = 0.38263508\n",
      "Iteration 943, loss = 0.28203815\n",
      "Iteration 81, loss = 0.46451873\n",
      "Iteration 82, loss = 0.46276085\n",
      "Iteration 944, loss = 0.28191595\n",
      "Iteration 653, loss = 0.33447508\n",
      "Iteration 83, loss = 0.46099961\n",
      "Iteration 1490, loss = 0.24698401\n",
      "Iteration 318, loss = 0.39663314\n",
      "Iteration 84, loss = 0.45931080\n",
      "Iteration 350, loss = 0.38243688\n",
      "Iteration 154, loss = 0.50317039\n",
      "Iteration 85, loss = 0.45768657\n",
      "Iteration 654, loss = 0.33437312\n",
      "Iteration 1491, loss = 0.24687769\n",
      "Iteration 945, loss = 0.28175627\n",
      "Iteration 86, loss = 0.45599200\n",
      "Iteration 1492, loss = 0.24677068\n",
      "Iteration 1, loss = 0.69397035\n",
      "Iteration 946, loss = 0.28175335\n",
      "Iteration 2, loss = 0.69242942\n",
      "Iteration 1493, loss = 0.24670047\n",
      "Iteration 351, loss = 0.38216387\n",
      "Iteration 319, loss = 0.39642406\n",
      "Iteration 3, loss = 0.69002351\n",
      "Iteration 87, loss = 0.45446672\n",
      "Iteration 4, loss = 0.68713986\n",
      "Iteration 88, loss = 0.45284997\n",
      "Iteration 352, loss = 0.38202657\n",
      "Iteration 89, loss = 0.45134621\n",
      "Iteration 5, loss = 0.68381006\n",
      "Iteration 1494, loss = 0.24658690\n",
      "Iteration 655, loss = 0.33425551\n",
      "Iteration 6, loss = 0.68014052\n",
      "Iteration 947, loss = 0.28146662\n",
      "Iteration 90, loss = 0.44978357\n",
      "Iteration 320, loss = 0.39622314\n",
      "Iteration 91, loss = 0.44826342\n",
      "Iteration 353, loss = 0.38170362\n",
      "Iteration 7, loss = 0.67639674\n",
      "Iteration 1495, loss = 0.24647344\n",
      "Iteration 948, loss = 0.28131086\n",
      "Iteration 92, loss = 0.44682633\n",
      "Iteration 1496, loss = 0.24639255\n",
      "Iteration 8, loss = 0.67260644\n",
      "Iteration 949, loss = 0.28124728\n",
      "Iteration 1497, loss = 0.24629878\n",
      "Iteration 9, loss = 0.66872868\n",
      "Iteration 656, loss = 0.33413678\n",
      "Iteration 950, loss = 0.28101527\n",
      "Iteration 1498, loss = 0.24616613\n",
      "Iteration 93, loss = 0.44541669\n",
      "Iteration 10, loss = 0.66480971\n",
      "Iteration 1499, loss = 0.24606476\n",
      "Iteration 94, loss = 0.44399425\n",
      "Iteration 95, loss = 0.44258431\n",
      "Iteration 11, loss = 0.66086587\n",
      "Iteration 354, loss = 0.38154456\n",
      "Iteration 96, loss = 0.44125085\n",
      "Iteration 1500, loss = 0.24597615\n",
      "Iteration 951, loss = 0.28100075\n",
      "Iteration 12, loss = 0.65688424\n",
      "Iteration 952, loss = 0.28076383\n",
      "Iteration 13, loss = 0.65306638\n",
      "Iteration 97, loss = 0.43991009\n",
      "Iteration 657, loss = 0.33405995\n",
      "Iteration 1501, loss = 0.24586645\n",
      "Iteration 953, loss = 0.28071908\n",
      "Iteration 14, loss = 0.64927439\n",
      "Iteration 1502, loss = 0.24582006\n",
      "Iteration 954, loss = 0.28046455\n",
      "Iteration 98, loss = 0.43859505\n",
      "Iteration 15, loss = 0.64552548\n",
      "Iteration 99, loss = 0.43732225\n",
      "Iteration 155, loss = 0.50222370\n",
      "Iteration 658, loss = 0.33393753\n",
      "Iteration 100, loss = 0.43605388\n",
      "Iteration 16, loss = 0.64188500\n",
      "Iteration 101, loss = 0.43484947\n",
      "Iteration 355, loss = 0.38128699\n",
      "Iteration 17, loss = 0.63811538\n",
      "Iteration 1503, loss = 0.24568510\n",
      "Iteration 18, loss = 0.63453227\n",
      "Iteration 955, loss = 0.28042805\n",
      "Iteration 102, loss = 0.43365003\n",
      "Iteration 956, loss = 0.28022788\n",
      "Iteration 321, loss = 0.39603956\n",
      "Iteration 659, loss = 0.33380642\n",
      "Iteration 103, loss = 0.43239680\n",
      "Iteration 356, loss = 0.38111951\n",
      "Iteration 1504, loss = 0.24558486\n",
      "Iteration 957, loss = 0.28002661\n",
      "Iteration 1505, loss = 0.24550701\n",
      "Iteration 104, loss = 0.43121080\n",
      "Iteration 1506, loss = 0.24537526\n",
      "Iteration 660, loss = 0.33371495\n",
      "Iteration 1507, loss = 0.24532856\n",
      "Iteration 322, loss = 0.39582039\n",
      "Iteration 156, loss = 0.50138908\n",
      "Iteration 105, loss = 0.43009301\n",
      "Iteration 1508, loss = 0.24516320\n",
      "Iteration 106, loss = 0.42895407\n",
      "Iteration 958, loss = 0.27991140\n",
      "Iteration 107, loss = 0.42783060\n",
      "Iteration 19, loss = 0.63095595\n",
      "Iteration 108, loss = 0.42674992\n",
      "Iteration 959, loss = 0.27972316\n",
      "Iteration 109, loss = 0.42570398\n",
      "Iteration 110, loss = 0.42463883\n",
      "Iteration 157, loss = 0.50048712\n",
      "Iteration 960, loss = 0.27958769\n",
      "Iteration 1509, loss = 0.24515347\n",
      "Iteration 111, loss = 0.42360814\n",
      "Iteration 961, loss = 0.27942021\n",
      "Iteration 112, loss = 0.42261286\n",
      "Iteration 113, loss = 0.42160202\n",
      "Iteration 661, loss = 0.33364447\n",
      "Iteration 323, loss = 0.39559712\n",
      "Iteration 114, loss = 0.42061026\n",
      "Iteration 1510, loss = 0.24495835\n",
      "Iteration 962, loss = 0.27938838\n",
      "Iteration 115, loss = 0.41966897\n",
      "Iteration 1511, loss = 0.24483850\n",
      "Iteration 116, loss = 0.41870662\n",
      "Iteration 662, loss = 0.33356200\n",
      "Iteration 117, loss = 0.41782170\n",
      "Iteration 118, loss = 0.41683737\n",
      "Iteration 1512, loss = 0.24473654\n",
      "Iteration 963, loss = 0.27916174\n",
      "Iteration 20, loss = 0.62737267\n",
      "Iteration 119, loss = 0.41597816\n",
      "Iteration 357, loss = 0.38087810\n",
      "Iteration 1513, loss = 0.24465398\n",
      "Iteration 120, loss = 0.41511241\n",
      "Iteration 158, loss = 0.49960962\n",
      "Iteration 663, loss = 0.33338577\n",
      "Iteration 121, loss = 0.41424070\n",
      "Iteration 1514, loss = 0.24453180\n",
      "Iteration 21, loss = 0.62400632\n",
      "Iteration 122, loss = 0.41337702\n",
      "Iteration 1515, loss = 0.24442949\n",
      "Iteration 123, loss = 0.41252857\n",
      "Iteration 22, loss = 0.62046597\n",
      "Iteration 964, loss = 0.27898712\n",
      "Iteration 1516, loss = 0.24439476\n",
      "Iteration 124, loss = 0.41176674\n",
      "Iteration 125, loss = 0.41093286\n",
      "Iteration 358, loss = 0.38067203\n",
      "Iteration 965, loss = 0.27902700\n",
      "Iteration 23, loss = 0.61719084\n",
      "Iteration 664, loss = 0.33332755\n",
      "Iteration 1517, loss = 0.24426232\n",
      "Iteration 324, loss = 0.39542460\n",
      "Iteration 359, loss = 0.38046231\n",
      "Iteration 1518, loss = 0.24409880\n",
      "Iteration 966, loss = 0.27872534\n",
      "Iteration 967, loss = 0.27863593\n",
      "Iteration 24, loss = 0.61389494\n",
      "Iteration 1519, loss = 0.24404116\n",
      "Iteration 665, loss = 0.33312883\n",
      "Iteration 1520, loss = 0.24390470\n",
      "Iteration 968, loss = 0.27846985\n",
      "Iteration 25, loss = 0.61054482\n",
      "Iteration 159, loss = 0.49874209\n",
      "Iteration 666, loss = 0.33303403\n",
      "Iteration 969, loss = 0.27826438\n",
      "Iteration 360, loss = 0.38021217\n",
      "Iteration 1521, loss = 0.24385429\n",
      "Iteration 325, loss = 0.39522078\n",
      "Iteration 26, loss = 0.60727552\n",
      "Iteration 970, loss = 0.27815629\n",
      "Iteration 1522, loss = 0.24371746\n",
      "Iteration 667, loss = 0.33292972\n",
      "Iteration 971, loss = 0.27806542\n",
      "Iteration 1523, loss = 0.24361522\n",
      "Iteration 972, loss = 0.27792889\n",
      "Iteration 326, loss = 0.39502284\n",
      "Iteration 160, loss = 0.49788839\n",
      "Iteration 361, loss = 0.37999570\n",
      "Iteration 1524, loss = 0.24351781\n",
      "Iteration 1525, loss = 0.24342918\n",
      "Iteration 27, loss = 0.60408405\n",
      "Iteration 973, loss = 0.27769853\n",
      "Iteration 362, loss = 0.37980788\n",
      "Iteration 974, loss = 0.27755455\n",
      "Iteration 1526, loss = 0.24329510\n",
      "Iteration 161, loss = 0.49704265\n",
      "Iteration 327, loss = 0.39482302\n",
      "Iteration 1527, loss = 0.24325535\n",
      "Iteration 363, loss = 0.37961547\n",
      "Iteration 28, loss = 0.60091875\n",
      "Iteration 1528, loss = 0.24309769\n",
      "Iteration 1529, loss = 0.24298216\n",
      "Iteration 364, loss = 0.37936907\n",
      "Iteration 975, loss = 0.27746486\n",
      "Iteration 29, loss = 0.59771120\n",
      "Iteration 1530, loss = 0.24287995Iteration 126, loss = 0.41013991\n",
      "\n",
      "Iteration 30, loss = 0.59465806\n",
      "Iteration 365, loss = 0.37918102\n",
      "Iteration 31, loss = 0.59151963\n",
      "Iteration 1531, loss = 0.24280863\n",
      "Iteration 976, loss = 0.27736681Iteration 162, loss = 0.49629320\n",
      "Iteration 668, loss = 0.33282860\n",
      "Iteration 127, loss = 0.40935143\n",
      "\n",
      "Iteration 1532, loss = 0.24267338\n",
      "Iteration 1533, loss = 0.24255286\n",
      "Iteration 669, loss = 0.33270436\n",
      "Iteration 128, loss = 0.40858560\n",
      "Iteration 32, loss = 0.58852383\n",
      "Iteration 977, loss = 0.27719614\n",
      "Iteration 129, loss = 0.40783021\n",
      "Iteration 1534, loss = 0.24249500\n",
      "Iteration 978, loss = 0.27700297\n",
      "Iteration 670, loss = 0.33257770\n",
      "Iteration 1535, loss = 0.24245760\n",
      "Iteration 33, loss = 0.58552154\n",
      "Iteration 1536, loss = 0.24225012\n",
      "Iteration 979, loss = 0.27685349\n",
      "Iteration 130, loss = 0.40708978\n",
      "Iteration 671, loss = 0.33249319\n",
      "Iteration 328, loss = 0.39460812\n",
      "Iteration 131, loss = 0.40637787\n",
      "Iteration 132, loss = 0.40567889\n",
      "Iteration 34, loss = 0.58259791\n",
      "Iteration 133, loss = 0.40497429\n",
      "Iteration 672, loss = 0.33237463\n",
      "Iteration 980, loss = 0.27672133\n",
      "Iteration 134, loss = 0.40421552\n",
      "Iteration 135, loss = 0.40358947\n",
      "Iteration 1537, loss = 0.24217457\n",
      "Iteration 366, loss = 0.37890182\n",
      "Iteration 136, loss = 0.40288453\n",
      "Iteration 137, loss = 0.40229249\n",
      "Iteration 673, loss = 0.33229282\n",
      "Iteration 981, loss = 0.27656599\n",
      "Iteration 138, loss = 0.40160135\n",
      "Iteration 1538, loss = 0.24205381\n",
      "Iteration 139, loss = 0.40095826\n",
      "Iteration 140, loss = 0.40031061\n",
      "Iteration 674, loss = 0.33218665\n",
      "Iteration 35, loss = 0.57962744\n",
      "Iteration 141, loss = 0.39971158\n",
      "Iteration 1539, loss = 0.24194921\n",
      "Iteration 982, loss = 0.27645085\n",
      "Iteration 329, loss = 0.39441315\n",
      "Iteration 1540, loss = 0.24186835\n",
      "Iteration 142, loss = 0.39908506\n",
      "Iteration 143, loss = 0.39846367\n",
      "Iteration 367, loss = 0.37870612\n",
      "Iteration 1541, loss = 0.24173336\n",
      "Iteration 983, loss = 0.27622890\n",
      "Iteration 144, loss = 0.39792332\n",
      "Iteration 145, loss = 0.39731044\n",
      "Iteration 36, loss = 0.57673969\n",
      "Iteration 146, loss = 0.39671072\n",
      "Iteration 37, loss = 0.57384193\n",
      "Iteration 147, loss = 0.39623217\n",
      "Iteration 163, loss = 0.49546838\n",
      "Iteration 1542, loss = 0.24162657\n",
      "Iteration 148, loss = 0.39562616\n",
      "Iteration 38, loss = 0.57105280\n",
      "Iteration 149, loss = 0.39508129\n",
      "Iteration 675, loss = 0.33205023\n",
      "Iteration 150, loss = 0.39452737\n",
      "Iteration 1543, loss = 0.24154938\n",
      "Iteration 330, loss = 0.39422419\n",
      "Iteration 984, loss = 0.27609390\n",
      "Iteration 151, loss = 0.39397477\n",
      "Iteration 152, loss = 0.39344772Iteration 1544, loss = 0.24141120\n",
      "\n",
      "Iteration 39, loss = 0.56818593\n",
      "Iteration 153, loss = 0.39296943\n",
      "Iteration 985, loss = 0.27601187\n",
      "Iteration 154, loss = 0.39242916\n",
      "Iteration 40, loss = 0.56539381\n",
      "Iteration 155, loss = 0.39192230\n",
      "Iteration 676, loss = 0.33196637\n",
      "Iteration 368, loss = 0.37849592\n",
      "Iteration 156, loss = 0.39145551\n",
      "Iteration 986, loss = 0.27579837\n",
      "Iteration 1545, loss = 0.24136635\n",
      "Iteration 41, loss = 0.56268275\n",
      "Iteration 164, loss = 0.49467481\n",
      "Iteration 157, loss = 0.39094302\n",
      "Iteration 42, loss = 0.55992424\n",
      "Iteration 677, loss = 0.33184002\n",
      "Iteration 987, loss = 0.27566173\n",
      "Iteration 43, loss = 0.55724136\n",
      "Iteration 331, loss = 0.39411814\n",
      "Iteration 158, loss = 0.39044678\n",
      "Iteration 988, loss = 0.27554379\n",
      "Iteration 44, loss = 0.55459031\n",
      "Iteration 1546, loss = 0.24126658\n",
      "Iteration 678, loss = 0.33170211\n",
      "Iteration 45, loss = 0.55199276\n",
      "Iteration 1547, loss = 0.24108988\n",
      "Iteration 159, loss = 0.38997662\n",
      "Iteration 989, loss = 0.27541772\n",
      "Iteration 165, loss = 0.49385159Iteration 679, loss = 0.33163590\n",
      "\n",
      "Iteration 369, loss = 0.37828468\n",
      "Iteration 1548, loss = 0.24105817\n",
      "Iteration 160, loss = 0.38948491\n",
      "Iteration 46, loss = 0.54942401\n",
      "Iteration 161, loss = 0.38902900\n",
      "Iteration 990, loss = 0.27525587\n",
      "Iteration 162, loss = 0.38853819\n",
      "Iteration 332, loss = 0.39383782\n",
      "Iteration 1549, loss = 0.24086898\n",
      "Iteration 163, loss = 0.38812584\n",
      "Iteration 47, loss = 0.54689183\n",
      "Iteration 991, loss = 0.27512261\n",
      "Iteration 680, loss = 0.33147959\n",
      "Iteration 164, loss = 0.38767040\n",
      "Iteration 48, loss = 0.54435127\n",
      "Iteration 165, loss = 0.38723748\n",
      "Iteration 1550, loss = 0.24080920\n",
      "Iteration 166, loss = 0.38678363\n",
      "Iteration 370, loss = 0.37816263\n",
      "Iteration 49, loss = 0.54183020\n",
      "Iteration 167, loss = 0.38638696\n",
      "Iteration 992, loss = 0.27495543\n",
      "Iteration 168, loss = 0.38589626\n",
      "Iteration 681, loss = 0.33142273\n",
      "Iteration 1551, loss = 0.24067525\n",
      "Iteration 169, loss = 0.38549828\n",
      "Iteration 50, loss = 0.53933725\n",
      "Iteration 1552, loss = 0.24056870\n",
      "Iteration 170, loss = 0.38508540\n",
      "Iteration 993, loss = 0.27481341\n",
      "Iteration 333, loss = 0.39366939\n",
      "Iteration 51, loss = 0.53684925\n",
      "Iteration 682, loss = 0.33126513\n",
      "Iteration 994, loss = 0.27469083\n",
      "Iteration 371, loss = 0.37786266\n",
      "Iteration 1553, loss = 0.24045964\n",
      "Iteration 166, loss = 0.49306941\n",
      "Iteration 52, loss = 0.53451550\n",
      "Iteration 683, loss = 0.33116579\n",
      "Iteration 171, loss = 0.38468210\n",
      "Iteration 995, loss = 0.27450737\n",
      "Iteration 1554, loss = 0.24039130\n",
      "Iteration 53, loss = 0.53212626\n",
      "Iteration 684, loss = 0.33106259\n",
      "Iteration 334, loss = 0.39343561\n",
      "Iteration 172, loss = 0.38426888\n",
      "Iteration 996, loss = 0.27442081\n",
      "Iteration 685, loss = 0.33095670\n",
      "Iteration 1555, loss = 0.24026651\n",
      "Iteration 173, loss = 0.38387325\n",
      "Iteration 997, loss = 0.27434366\n",
      "Iteration 54, loss = 0.52978822\n",
      "Iteration 1556, loss = 0.24016831\n",
      "Iteration 174, loss = 0.38347046\n",
      "Iteration 686, loss = 0.33085430\n",
      "Iteration 175, loss = 0.38312306\n",
      "Iteration 1557, loss = 0.24003386\n",
      "Iteration 372, loss = 0.37766614\n",
      "Iteration 1558, loss = 0.23991753\n",
      "Iteration 176, loss = 0.38267178\n",
      "Iteration 998, loss = 0.27411043\n",
      "Iteration 167, loss = 0.49231882\n",
      "Iteration 55, loss = 0.52745726\n",
      "Iteration 177, loss = 0.38229610\n",
      "Iteration 335, loss = 0.39326343\n",
      "Iteration 178, loss = 0.38193033\n",
      "Iteration 1559, loss = 0.23984359\n",
      "Iteration 999, loss = 0.27396529\n",
      "Iteration 373, loss = 0.37751009\n",
      "Iteration 179, loss = 0.38152474\n",
      "Iteration 1560, loss = 0.23973517\n",
      "Iteration 56, loss = 0.52516613\n",
      "Iteration 1000, loss = 0.27381439\n",
      "Iteration 180, loss = 0.38117934\n",
      "Iteration 57, loss = 0.52295941\n",
      "Iteration 1001, loss = 0.27369797\n",
      "Iteration 181, loss = 0.38078350\n",
      "Iteration 1561, loss = 0.23968808\n",
      "Iteration 58, loss = 0.52071759\n",
      "Iteration 687, loss = 0.33072339\n",
      "Iteration 1002, loss = 0.27352009\n",
      "Iteration 1562, loss = 0.23962518\n",
      "Iteration 182, loss = 0.38047177\n",
      "Iteration 59, loss = 0.51858172\n",
      "Iteration 1563, loss = 0.23937569\n",
      "Iteration 1003, loss = 0.27346899\n",
      "Iteration 374, loss = 0.37731231\n",
      "Iteration 1564, loss = 0.23923744\n",
      "Iteration 60, loss = 0.51634689\n",
      "Iteration 183, loss = 0.38005803\n",
      "Iteration 1004, loss = 0.27324496\n",
      "Iteration 1565, loss = 0.23915548\n",
      "Iteration 184, loss = 0.37971112\n",
      "Iteration 61, loss = 0.51420817\n",
      "Iteration 336, loss = 0.39306581\n",
      "Iteration 688, loss = 0.33060530\n",
      "Iteration 1005, loss = 0.27312506\n",
      "Iteration 62, loss = 0.51220386\n",
      "Iteration 185, loss = 0.37933660\n",
      "Iteration 1566, loss = 0.23907568\n",
      "Iteration 186, loss = 0.37900893\n",
      "Iteration 1567, loss = 0.23892329\n",
      "Iteration 375, loss = 0.37705103\n",
      "Iteration 689, loss = 0.33049516\n",
      "Iteration 187, loss = 0.37865965\n",
      "Iteration 1568, loss = 0.23889037\n",
      "Iteration 1006, loss = 0.27301020\n",
      "Iteration 188, loss = 0.37830918\n",
      "Iteration 1569, loss = 0.23872266\n",
      "Iteration 63, loss = 0.51006745\n",
      "Iteration 189, loss = 0.37796826\n",
      "Iteration 690, loss = 0.33049697\n",
      "Iteration 168, loss = 0.49152774\n",
      "Iteration 1007, loss = 0.27283843\n",
      "Iteration 337, loss = 0.39285044\n",
      "Iteration 64, loss = 0.50804416\n",
      "Iteration 1570, loss = 0.23859488\n",
      "Iteration 190, loss = 0.37764098\n",
      "Iteration 376, loss = 0.37682425\n",
      "Iteration 1571, loss = 0.23851542\n",
      "Iteration 65, loss = 0.50611163\n",
      "Iteration 1008, loss = 0.27267575\n",
      "Iteration 691, loss = 0.33029396\n",
      "Iteration 1572, loss = 0.23838384\n",
      "Iteration 191, loss = 0.37729832\n",
      "Iteration 66, loss = 0.50412821\n",
      "Iteration 192, loss = 0.37696808\n",
      "Iteration 1573, loss = 0.23827360\n",
      "Iteration 338, loss = 0.39266962\n",
      "Iteration 67, loss = 0.50215683\n",
      "Iteration 193, loss = 0.37663577\n",
      "Iteration 1574, loss = 0.23816743\n",
      "Iteration 1575, loss = 0.23810243\n",
      "Iteration 68, loss = 0.50029532\n",
      "Iteration 1009, loss = 0.27257341\n",
      "Iteration 1576, loss = 0.23794304\n",
      "Iteration 194, loss = 0.37631445\n",
      "Iteration 1010, loss = 0.27242970\n",
      "Iteration 69, loss = 0.49833228\n",
      "Iteration 377, loss = 0.37664099\n",
      "Iteration 1011, loss = 0.27232736\n",
      "Iteration 70, loss = 0.49657513\n",
      "Iteration 1577, loss = 0.23787445\n",
      "Iteration 195, loss = 0.37600764\n",
      "Iteration 339, loss = 0.39247215\n",
      "Iteration 196, loss = 0.37569284\n",
      "Iteration 1578, loss = 0.23776225\n",
      "Iteration 692, loss = 0.33020081\n",
      "Iteration 197, loss = 0.37537206\n",
      "Iteration 1579, loss = 0.23765918\n",
      "Iteration 198, loss = 0.37508476\n",
      "Iteration 1580, loss = 0.23753701\n",
      "Iteration 693, loss = 0.33006828\n",
      "Iteration 1012, loss = 0.27211355\n",
      "Iteration 1581, loss = 0.23741903\n",
      "Iteration 71, loss = 0.49468991\n",
      "Iteration 340, loss = 0.39228253\n",
      "Iteration 1582, loss = 0.23735193\n",
      "Iteration 1013, loss = 0.27194536\n",
      "Iteration 694, loss = 0.32995147\n",
      "Iteration 169, loss = 0.49075147\n",
      "Iteration 1583, loss = 0.23720604\n",
      "Iteration 1014, loss = 0.27181168\n",
      "Iteration 378, loss = 0.37646931\n",
      "Iteration 72, loss = 0.49291932\n",
      "Iteration 1584, loss = 0.23718979\n",
      "Iteration 199, loss = 0.37475993\n",
      "Iteration 1015, loss = 0.27174745\n",
      "Iteration 695, loss = 0.32988341\n",
      "Iteration 200, loss = 0.37448272\n",
      "Iteration 1585, loss = 0.23701808\n",
      "Iteration 1016, loss = 0.27154511\n",
      "Iteration 201, loss = 0.37415802\n",
      "Iteration 73, loss = 0.49108704\n",
      "Iteration 1586, loss = 0.23690488\n",
      "Iteration 1587, loss = 0.23692118\n",
      "Iteration 1017, loss = 0.27138376\n",
      "Iteration 74, loss = 0.48940382\n",
      "Iteration 202, loss = 0.37388142\n",
      "Iteration 1018, loss = 0.27125749\n",
      "Iteration 341, loss = 0.39209741\n",
      "Iteration 170, loss = 0.49008039\n",
      "Iteration 75, loss = 0.48763914\n",
      "Iteration 1588, loss = 0.23666923\n",
      "Iteration 1019, loss = 0.27114334\n",
      "Iteration 76, loss = 0.48605007\n",
      "Iteration 696, loss = 0.32973586\n",
      "Iteration 1020, loss = 0.27098214\n",
      "Iteration 1589, loss = 0.23658075\n",
      "Iteration 203, loss = 0.37360358\n",
      "Iteration 1590, loss = 0.23648707\n",
      "Iteration 204, loss = 0.37327906\n",
      "Iteration 77, loss = 0.48437970\n",
      "Iteration 379, loss = 0.37621327\n",
      "Iteration 1591, loss = 0.23644185\n",
      "Iteration 205, loss = 0.37301176\n",
      "Iteration 1021, loss = 0.27079027\n",
      "Iteration 697, loss = 0.32964456\n",
      "Iteration 1592, loss = 0.23625561\n",
      "Iteration 1022, loss = 0.27071067\n",
      "Iteration 78, loss = 0.48267640\n",
      "Iteration 206, loss = 0.37273589\n",
      "Iteration 342, loss = 0.39192019\n",
      "Iteration 380, loss = 0.37602388\n",
      "Iteration 1023, loss = 0.27050188\n",
      "Iteration 79, loss = 0.48109663\n",
      "Iteration 698, loss = 0.32958782\n",
      "Iteration 1024, loss = 0.27042633\n",
      "Iteration 80, loss = 0.47951382\n",
      "Iteration 207, loss = 0.37245488\n",
      "Iteration 208, loss = 0.37216546\n",
      "Iteration 81, loss = 0.47799033\n",
      "Iteration 171, loss = 0.48935814\n",
      "Iteration 1593, loss = 0.23619021\n",
      "Iteration 381, loss = 0.37584706\n",
      "Iteration 209, loss = 0.37189475Iteration 1025, loss = 0.27027603\n",
      "Iteration 343, loss = 0.39180195\n",
      "\n",
      "Iteration 82, loss = 0.47641516\n",
      "Iteration 1026, loss = 0.27005947\n",
      "Iteration 210, loss = 0.37162720\n",
      "Iteration 83, loss = 0.47494426\n",
      "Iteration 382, loss = 0.37561373\n",
      "Iteration 211, loss = 0.37134199\n",
      "Iteration 1027, loss = 0.26995846\n",
      "Iteration 699, loss = 0.32947245\n",
      "Iteration 84, loss = 0.47348354\n",
      "Iteration 212, loss = 0.37108817\n",
      "Iteration 1594, loss = 0.23609654\n",
      "Iteration 344, loss = 0.39153988\n",
      "Iteration 1028, loss = 0.26986082\n",
      "Iteration 1595, loss = 0.23595068\n",
      "Iteration 213, loss = 0.37083190\n",
      "Iteration 1029, loss = 0.26979193\n",
      "Iteration 85, loss = 0.47200273\n",
      "Iteration 214, loss = 0.37055974\n",
      "Iteration 1596, loss = 0.23583993\n",
      "Iteration 215, loss = 0.37030819\n",
      "Iteration 86, loss = 0.47058789\n",
      "Iteration 172, loss = 0.48857001\n",
      "Iteration 383, loss = 0.37543034\n",
      "Iteration 216, loss = 0.37004646\n",
      "Iteration 1030, loss = 0.26947911\n",
      "Iteration 1597, loss = 0.23567811\n",
      "Iteration 87, loss = 0.46914621\n",
      "Iteration 217, loss = 0.36978119\n",
      "Iteration 345, loss = 0.39134562\n",
      "Iteration 218, loss = 0.36955457\n",
      "Iteration 88, loss = 0.46776905\n",
      "Iteration 1031, loss = 0.26932851\n",
      "Iteration 219, loss = 0.36929559\n",
      "Iteration 89, loss = 0.46641052\n",
      "Iteration 1032, loss = 0.26918337\n",
      "Iteration 700, loss = 0.32933597\n",
      "Iteration 1598, loss = 0.23563292\n",
      "Iteration 90, loss = 0.46503977\n",
      "Iteration 1599, loss = 0.23548543\n",
      "Iteration 220, loss = 0.36902066\n",
      "Iteration 1600, loss = 0.23546709\n",
      "Iteration 346, loss = 0.39128428\n",
      "Iteration 1033, loss = 0.26908193\n",
      "Iteration 91, loss = 0.46371728\n",
      "Iteration 1601, loss = 0.23530994\n",
      "Iteration 1034, loss = 0.26899709\n",
      "Iteration 221, loss = 0.36879498\n",
      "Iteration 1602, loss = 0.23518610\n",
      "Iteration 384, loss = 0.37521758\n",
      "Iteration 1603, loss = 0.23512603\n",
      "Iteration 173, loss = 0.48787017\n",
      "Iteration 222, loss = 0.36855311\n",
      "Iteration 701, loss = 0.32921346\n",
      "Iteration 1604, loss = 0.23494083\n",
      "Iteration 223, loss = 0.36830634\n",
      "Iteration 92, loss = 0.46237484\n",
      "Iteration 1035, loss = 0.26879842\n",
      "Iteration 224, loss = 0.36805441\n",
      "Iteration 1605, loss = 0.23493101\n",
      "Iteration 385, loss = 0.37501961\n",
      "Iteration 702, loss = 0.32908899\n",
      "Iteration 1606, loss = 0.23488100\n",
      "Iteration 1607, loss = 0.23466900\n",
      "Iteration 347, loss = 0.39099554\n",
      "Iteration 225, loss = 0.36781869\n",
      "Iteration 1036, loss = 0.26860785\n",
      "Iteration 93, loss = 0.46117593\n",
      "Iteration 1608, loss = 0.23457278\n",
      "Iteration 226, loss = 0.36758500\n",
      "Iteration 1037, loss = 0.26852310\n",
      "Iteration 94, loss = 0.45981547\n",
      "Iteration 227, loss = 0.36737219\n",
      "Iteration 386, loss = 0.37484925\n",
      "Iteration 703, loss = 0.32902133\n",
      "Iteration 1609, loss = 0.23444138\n",
      "Iteration 1038, loss = 0.26831712\n",
      "Iteration 95, loss = 0.45859100\n",
      "Iteration 228, loss = 0.36711371\n",
      "Iteration 229, loss = 0.36689859\n",
      "Iteration 1039, loss = 0.26826845\n",
      "Iteration 96, loss = 0.45740043\n",
      "Iteration 230, loss = 0.36665934\n",
      "Iteration 1610, loss = 0.23434033\n",
      "Iteration 1040, loss = 0.26806127\n",
      "Iteration 231, loss = 0.36642269\n",
      "Iteration 97, loss = 0.45614608\n",
      "Iteration 232, loss = 0.36619812\n",
      "Iteration 1611, loss = 0.23420619\n",
      "Iteration 1041, loss = 0.26796890\n",
      "Iteration 233, loss = 0.36599719\n",
      "Iteration 174, loss = 0.48717015\n",
      "Iteration 704, loss = 0.32889224\n",
      "Iteration 387, loss = 0.37469236\n",
      "Iteration 234, loss = 0.36578013\n",
      "Iteration 1042, loss = 0.26773999\n",
      "Iteration 1612, loss = 0.23413591\n",
      "Iteration 98, loss = 0.45496940\n",
      "Iteration 1043, loss = 0.26757086\n",
      "Iteration 1613, loss = 0.23404694\n",
      "Iteration 235, loss = 0.36556304\n",
      "Iteration 1614, loss = 0.23390423\n",
      "Iteration 348, loss = 0.39090829\n",
      "Iteration 388, loss = 0.37446759\n",
      "Iteration 99, loss = 0.45377285\n",
      "Iteration 1615, loss = 0.23381233\n",
      "Iteration 1044, loss = 0.26746275\n",
      "Iteration 1616, loss = 0.23367807\n",
      "Iteration 100, loss = 0.45262196\n",
      "Iteration 705, loss = 0.32878463\n",
      "Iteration 236, loss = 0.36533938\n",
      "Iteration 1617, loss = 0.23361052\n",
      "Iteration 175, loss = 0.48650538\n",
      "Iteration 1618, loss = 0.23349526\n",
      "Iteration 101, loss = 0.45147467\n",
      "Iteration 1619, loss = 0.23337407\n",
      "Iteration 1045, loss = 0.26730928\n",
      "Iteration 102, loss = 0.45033047\n",
      "Iteration 1620, loss = 0.23326493\n",
      "Iteration 237, loss = 0.36509876\n",
      "Iteration 706, loss = 0.32868361\n",
      "Iteration 103, loss = 0.44924598\n",
      "Iteration 238, loss = 0.36490251\n",
      "Iteration 1621, loss = 0.23316682\n",
      "Iteration 349, loss = 0.39063795\n",
      "Iteration 1622, loss = 0.23307585\n",
      "Iteration 1046, loss = 0.26716742\n",
      "Iteration 104, loss = 0.44814389\n",
      "Iteration 239, loss = 0.36469617\n",
      "Iteration 1623, loss = 0.23299275\n",
      "Iteration 707, loss = 0.32860530\n",
      "Iteration 240, loss = 0.36447346\n",
      "Iteration 1624, loss = 0.23289292\n",
      "Iteration 389, loss = 0.37430485\n",
      "Iteration 708, loss = 0.32853863\n",
      "Iteration 1625, loss = 0.23280212\n",
      "Iteration 350, loss = 0.39047000\n",
      "Iteration 241, loss = 0.36428088\n",
      "Iteration 1626, loss = 0.23267285\n",
      "Iteration 1047, loss = 0.26701743\n",
      "Iteration 1627, loss = 0.23253930\n",
      "Iteration 105, loss = 0.44706835\n",
      "Iteration 709, loss = 0.32837232\n",
      "Iteration 242, loss = 0.36409758\n",
      "Iteration 1048, loss = 0.26690031\n",
      "Iteration 106, loss = 0.44602958\n",
      "Iteration 1049, loss = 0.26677476\n",
      "Iteration 107, loss = 0.44494963\n",
      "Iteration 243, loss = 0.36387436\n",
      "Iteration 1628, loss = 0.23242784\n",
      "Iteration 710, loss = 0.32826501\n",
      "Iteration 351, loss = 0.39025633\n",
      "Iteration 390, loss = 0.37398987\n",
      "Iteration 1050, loss = 0.26655467\n",
      "Iteration 176, loss = 0.48581456\n",
      "Iteration 244, loss = 0.36366201\n",
      "Iteration 108, loss = 0.44390702\n",
      "Iteration 1629, loss = 0.23232587\n",
      "Iteration 245, loss = 0.36345964\n",
      "Iteration 1630, loss = 0.23220420\n",
      "Iteration 1051, loss = 0.26642854\n",
      "Iteration 352, loss = 0.39008953\n",
      "Iteration 246, loss = 0.36326655\n",
      "Iteration 109, loss = 0.44292523\n",
      "Iteration 1631, loss = 0.23210119\n",
      "Iteration 711, loss = 0.32819682\n",
      "Iteration 110, loss = 0.44190739\n",
      "Iteration 391, loss = 0.37380500\n",
      "Iteration 1052, loss = 0.26631266\n",
      "Iteration 247, loss = 0.36305462\n",
      "Iteration 1632, loss = 0.23200239\n",
      "Iteration 111, loss = 0.44094058\n",
      "Iteration 712, loss = 0.32807047\n",
      "Iteration 1053, loss = 0.26617942\n",
      "Iteration 392, loss = 0.37361436\n",
      "Iteration 248, loss = 0.36287777\n",
      "Iteration 177, loss = 0.48510197\n",
      "Iteration 1054, loss = 0.26595929\n",
      "Iteration 1633, loss = 0.23189005\n",
      "Iteration 112, loss = 0.43999993\n",
      "Iteration 249, loss = 0.36267966\n",
      "Iteration 1634, loss = 0.23185214\n",
      "Iteration 250, loss = 0.36247401\n",
      "Iteration 353, loss = 0.38990914\n",
      "Iteration 1635, loss = 0.23170002\n",
      "Iteration 113, loss = 0.43898886\n",
      "Iteration 713, loss = 0.32793539\n",
      "Iteration 1636, loss = 0.23158839\n",
      "Iteration 251, loss = 0.36229163\n",
      "Iteration 1055, loss = 0.26581223\n",
      "Iteration 1637, loss = 0.23147388\n",
      "Iteration 114, loss = 0.43802763\n",
      "Iteration 1056, loss = 0.26567478\n",
      "Iteration 252, loss = 0.36211888\n",
      "Iteration 393, loss = 0.37340146\n",
      "Iteration 1638, loss = 0.23141984\n",
      "Iteration 115, loss = 0.43711099\n",
      "Iteration 714, loss = 0.32784162\n",
      "Iteration 1639, loss = 0.23133218\n",
      "Iteration 178, loss = 0.48447058\n",
      "Iteration 715, loss = 0.32774785\n",
      "Iteration 1057, loss = 0.26552977\n",
      "Iteration 354, loss = 0.38973413\n",
      "Iteration 1640, loss = 0.23115917\n",
      "Iteration 116, loss = 0.43616572\n",
      "Iteration 253, loss = 0.36190936\n",
      "Iteration 394, loss = 0.37330276\n",
      "Iteration 117, loss = 0.43523466\n",
      "Iteration 1641, loss = 0.23111705\n",
      "Iteration 1058, loss = 0.26538548\n",
      "Iteration 254, loss = 0.36173032\n",
      "Iteration 1642, loss = 0.23100849\n",
      "Iteration 716, loss = 0.32762251\n",
      "Iteration 1643, loss = 0.23087326\n",
      "Iteration 118, loss = 0.43438929\n",
      "Iteration 355, loss = 0.38955808\n",
      "Iteration 255, loss = 0.36153637\n",
      "Iteration 1059, loss = 0.26530372\n",
      "Iteration 395, loss = 0.37304242\n",
      "Iteration 1644, loss = 0.23076803\n",
      "Iteration 717, loss = 0.32749101\n",
      "Iteration 119, loss = 0.43348177\n",
      "Iteration 256, loss = 0.36136142\n",
      "Iteration 1060, loss = 0.26510997\n",
      "Iteration 356, loss = 0.38936604\n",
      "Iteration 120, loss = 0.43261280\n",
      "Iteration 1645, loss = 0.23074828\n",
      "Iteration 257, loss = 0.36115898\n",
      "Iteration 718, loss = 0.32739571\n",
      "Iteration 121, loss = 0.43176410\n",
      "Iteration 396, loss = 0.37288654\n",
      "Iteration 1061, loss = 0.26491922\n",
      "Iteration 258, loss = 0.36098577\n",
      "Iteration 1646, loss = 0.23057975\n",
      "Iteration 122, loss = 0.43090003\n",
      "Iteration 179, loss = 0.48380497\n",
      "Iteration 719, loss = 0.32734441\n",
      "Iteration 123, loss = 0.43002569\n",
      "Iteration 1647, loss = 0.23047416\n",
      "Iteration 259, loss = 0.36081689\n",
      "Iteration 357, loss = 0.38917777\n",
      "Iteration 397, loss = 0.37260684\n",
      "Iteration 720, loss = 0.32719458\n",
      "Iteration 1062, loss = 0.26475770\n",
      "Iteration 124, loss = 0.42928956\n",
      "Iteration 260, loss = 0.36064558\n",
      "Iteration 1648, loss = 0.23037551\n",
      "Iteration 261, loss = 0.36043950\n",
      "Iteration 125, loss = 0.42840559\n",
      "Iteration 262, loss = 0.36028585\n",
      "Iteration 1649, loss = 0.23023503\n",
      "Iteration 1063, loss = 0.26462842\n",
      "Iteration 126, loss = 0.42759309\n",
      "Iteration 721, loss = 0.32709167\n",
      "Iteration 1064, loss = 0.26468935\n",
      "Iteration 263, loss = 0.36010170\n",
      "Iteration 1650, loss = 0.23011005\n",
      "Iteration 1065, loss = 0.26430882\n",
      "Iteration 264, loss = 0.35994292\n",
      "Iteration 398, loss = 0.37250497\n",
      "Iteration 1651, loss = 0.23001580\n",
      "Iteration 180, loss = 0.48311207\n",
      "Iteration 127, loss = 0.42682336\n",
      "Iteration 1066, loss = 0.26432500\n",
      "Iteration 265, loss = 0.35976866\n",
      "Iteration 1652, loss = 0.22996682\n",
      "Iteration 358, loss = 0.38903921\n",
      "Iteration 722, loss = 0.32696388\n",
      "Iteration 128, loss = 0.42601346\n",
      "Iteration 266, loss = 0.35960307\n",
      "Iteration 1067, loss = 0.26404010\n",
      "Iteration 129, loss = 0.42524036\n",
      "Iteration 267, loss = 0.35942531\n",
      "Iteration 1653, loss = 0.22983522\n",
      "Iteration 130, loss = 0.42446953\n",
      "Iteration 1068, loss = 0.26385192\n",
      "Iteration 359, loss = 0.38880954\n",
      "Iteration 399, loss = 0.37217901\n",
      "Iteration 1654, loss = 0.22970078\n",
      "Iteration 723, loss = 0.32686484\n",
      "Iteration 268, loss = 0.35925575\n",
      "Iteration 131, loss = 0.42370997\n",
      "Iteration 1655, loss = 0.22966621\n",
      "Iteration 1069, loss = 0.26371870\n",
      "Iteration 1656, loss = 0.22954499\n",
      "Iteration 1657, loss = 0.22941565\n",
      "Iteration 1070, loss = 0.26363932\n",
      "Iteration 269, loss = 0.35907807\n",
      "Iteration 181, loss = 0.48248737\n",
      "Iteration 360, loss = 0.38864542\n",
      "Iteration 724, loss = 0.32678562\n",
      "Iteration 1658, loss = 0.22940440\n",
      "Iteration 132, loss = 0.42293530\n",
      "Iteration 270, loss = 0.35890041\n",
      "Iteration 1071, loss = 0.26348328\n",
      "Iteration 1659, loss = 0.22925779\n",
      "Iteration 400, loss = 0.37200197\n",
      "Iteration 271, loss = 0.35875164\n",
      "Iteration 1660, loss = 0.22909244\n",
      "Iteration 1661, loss = 0.22899284\n",
      "Iteration 725, loss = 0.32665268\n",
      "Iteration 1072, loss = 0.26327111\n",
      "Iteration 272, loss = 0.35857183\n",
      "Iteration 133, loss = 0.42222921\n",
      "Iteration 134, loss = 0.42148647\n",
      "Iteration 273, loss = 0.35840337\n",
      "Iteration 401, loss = 0.37180511\n",
      "Iteration 274, loss = 0.35826765\n",
      "Iteration 1073, loss = 0.26316421\n",
      "Iteration 1662, loss = 0.22888120\n",
      "Iteration 275, loss = 0.35809024\n",
      "Iteration 135, loss = 0.42077149\n",
      "Iteration 276, loss = 0.35791832\n",
      "Iteration 726, loss = 0.32666525\n",
      "Iteration 277, loss = 0.35775099\n",
      "Iteration 1663, loss = 0.22878137\n",
      "Iteration 136, loss = 0.42009328\n",
      "Iteration 278, loss = 0.35769054\n",
      "Iteration 361, loss = 0.38848716\n",
      "Iteration 1074, loss = 0.26305197\n",
      "Iteration 402, loss = 0.37162499\n",
      "Iteration 182, loss = 0.48184970\n",
      "Iteration 279, loss = 0.35745143\n",
      "Iteration 727, loss = 0.32645315\n",
      "Iteration 1664, loss = 0.22864813\n",
      "Iteration 1075, loss = 0.26280545\n",
      "Iteration 1076, loss = 0.26273127\n",
      "Iteration 280, loss = 0.35729839\n",
      "Iteration 137, loss = 0.41932809\n",
      "Iteration 728, loss = 0.32631271\n",
      "Iteration 281, loss = 0.35714301\n",
      "Iteration 138, loss = 0.41862983\n",
      "Iteration 1665, loss = 0.22854065\n",
      "Iteration 1077, loss = 0.26251825\n",
      "Iteration 1666, loss = 0.22846178\n",
      "Iteration 139, loss = 0.41798968\n",
      "Iteration 1667, loss = 0.22840176\n",
      "Iteration 282, loss = 0.35699495\n",
      "Iteration 1668, loss = 0.22828093\n",
      "Iteration 140, loss = 0.41727051\n",
      "Iteration 403, loss = 0.37139460\n",
      "Iteration 183, loss = 0.48128716\n",
      "Iteration 1078, loss = 0.26237625\n",
      "Iteration 283, loss = 0.35683632\n",
      "Iteration 729, loss = 0.32622187\n",
      "Iteration 1079, loss = 0.26218467\n",
      "Iteration 1669, loss = 0.22814876\n",
      "Iteration 1080, loss = 0.26212130\n",
      "Iteration 284, loss = 0.35669678\n",
      "Iteration 362, loss = 0.38829432\n",
      "Iteration 1670, loss = 0.22802392\n",
      "Iteration 404, loss = 0.37127225\n",
      "Iteration 285, loss = 0.35653360\n",
      "Iteration 730, loss = 0.32612397\n",
      "Iteration 1671, loss = 0.22795016\n",
      "Iteration 1081, loss = 0.26190745\n",
      "Iteration 731, loss = 0.32600663\n",
      "Iteration 1672, loss = 0.22783862\n",
      "Iteration 286, loss = 0.35637687\n",
      "Iteration 1082, loss = 0.26179763\n",
      "Iteration 1673, loss = 0.22778797\n",
      "Iteration 287, loss = 0.35622834\n",
      "Iteration 141, loss = 0.41659267\n",
      "Iteration 1083, loss = 0.26169508\n",
      "Iteration 288, loss = 0.35606638\n",
      "Iteration 1674, loss = 0.22761087\n",
      "Iteration 1084, loss = 0.26144180\n",
      "Iteration 142, loss = 0.41591954\n",
      "Iteration 289, loss = 0.35594003\n",
      "Iteration 184, loss = 0.48070211\n",
      "Iteration 1085, loss = 0.26130292\n",
      "Iteration 1675, loss = 0.22751643\n",
      "Iteration 363, loss = 0.38815345\n",
      "Iteration 143, loss = 0.41525442\n",
      "Iteration 405, loss = 0.37099932\n",
      "Iteration 1086, loss = 0.26126459\n",
      "Iteration 1676, loss = 0.22744911\n",
      "Iteration 290, loss = 0.35580328\n",
      "Iteration 1087, loss = 0.26103846\n",
      "Iteration 144, loss = 0.41461883\n",
      "Iteration 1677, loss = 0.22734233\n",
      "Iteration 732, loss = 0.32588931\n",
      "Iteration 406, loss = 0.37084766\n",
      "Iteration 1678, loss = 0.22720480\n",
      "Iteration 291, loss = 0.35564677\n",
      "Iteration 1679, loss = 0.22709423\n",
      "Iteration 145, loss = 0.41401521\n",
      "Iteration 733, loss = 0.32578805\n",
      "Iteration 292, loss = 0.35549320\n",
      "Iteration 1088, loss = 0.26081691\n",
      "Iteration 407, loss = 0.37062177\n",
      "Iteration 364, loss = 0.38807408\n",
      "Iteration 185, loss = 0.48002410\n",
      "Iteration 146, loss = 0.41335712\n",
      "Iteration 293, loss = 0.35535015\n",
      "Iteration 408, loss = 0.37042164\n",
      "Iteration 734, loss = 0.32572736\n",
      "Iteration 147, loss = 0.41272724\n",
      "Iteration 1089, loss = 0.26077734\n",
      "Iteration 1680, loss = 0.22698119\n",
      "Iteration 1090, loss = 0.26052348\n",
      "Iteration 294, loss = 0.35521818\n",
      "Iteration 735, loss = 0.32558277\n",
      "Iteration 409, loss = 0.37022386\n",
      "Iteration 1681, loss = 0.22690536\n",
      "Iteration 148, loss = 0.41205314\n",
      "Iteration 1091, loss = 0.26035537\n",
      "Iteration 295, loss = 0.35505013\n",
      "Iteration 1682, loss = 0.22676950\n",
      "Iteration 736, loss = 0.32553237\n",
      "Iteration 149, loss = 0.41150951\n",
      "Iteration 410, loss = 0.37006610\n",
      "Iteration 296, loss = 0.35490998\n",
      "Iteration 1092, loss = 0.26021590\n",
      "Iteration 365, loss = 0.38776073\n",
      "Iteration 297, loss = 0.35478918\n",
      "Iteration 1683, loss = 0.22669025\n",
      "Iteration 150, loss = 0.41089832\n",
      "Iteration 298, loss = 0.35463062\n",
      "Iteration 411, loss = 0.36987299\n",
      "Iteration 186, loss = 0.47940923\n",
      "Iteration 1093, loss = 0.26019512\n",
      "Iteration 737, loss = 0.32539133\n",
      "Iteration 299, loss = 0.35451349\n",
      "Iteration 1684, loss = 0.22656698\n",
      "Iteration 1094, loss = 0.25989813\n",
      "Iteration 151, loss = 0.41027005\n",
      "Iteration 1685, loss = 0.22653871\n",
      "Iteration 738, loss = 0.32530435\n",
      "Iteration 1686, loss = 0.22636813\n",
      "Iteration 300, loss = 0.35436443\n",
      "Iteration 301, loss = 0.35422986\n",
      "Iteration 1095, loss = 0.25989221\n",
      "Iteration 152, loss = 0.40965945\n",
      "Iteration 1687, loss = 0.22629418\n",
      "Iteration 366, loss = 0.38762870\n",
      "Iteration 1096, loss = 0.25961240\n",
      "Iteration 412, loss = 0.36964663\n",
      "Iteration 739, loss = 0.32516217\n",
      "Iteration 302, loss = 0.35407784\n",
      "Iteration 153, loss = 0.40909189\n",
      "Iteration 1097, loss = 0.25949895\n",
      "Iteration 303, loss = 0.35395075\n",
      "Iteration 1688, loss = 0.22619424\n",
      "Iteration 1098, loss = 0.25946692\n",
      "Iteration 304, loss = 0.35380566\n",
      "Iteration 1099, loss = 0.25921252\n",
      "Iteration 367, loss = 0.38745781\n",
      "Iteration 1100, loss = 0.25904546\n",
      "Iteration 740, loss = 0.32502560\n",
      "Iteration 154, loss = 0.40850609\n",
      "Iteration 1101, loss = 0.25895962\n",
      "Iteration 413, loss = 0.36947820\n",
      "Iteration 1689, loss = 0.22607239\n",
      "Iteration 741, loss = 0.32496400\n",
      "Iteration 1690, loss = 0.22597685\n",
      "Iteration 414, loss = 0.36926049\n",
      "Iteration 1691, loss = 0.22591191\n",
      "Iteration 742, loss = 0.32482306\n",
      "Iteration 155, loss = 0.40794111\n",
      "Iteration 368, loss = 0.38726325\n",
      "Iteration 187, loss = 0.47880893\n",
      "Iteration 1692, loss = 0.22578237\n",
      "Iteration 743, loss = 0.32472915\n",
      "Iteration 156, loss = 0.40736564\n",
      "Iteration 1693, loss = 0.22561195\n",
      "Iteration 415, loss = 0.36904897\n",
      "Iteration 744, loss = 0.32471798\n",
      "Iteration 305, loss = 0.35367052\n",
      "Iteration 1694, loss = 0.22553525\n",
      "Iteration 157, loss = 0.40680364\n",
      "Iteration 306, loss = 0.35353552\n",
      "Iteration 1102, loss = 0.25869218\n",
      "Iteration 416, loss = 0.36885449\n",
      "Iteration 158, loss = 0.40623632\n",
      "Iteration 307, loss = 0.35339574\n",
      "Iteration 1695, loss = 0.22544776\n",
      "Iteration 1103, loss = 0.25864639\n",
      "Iteration 308, loss = 0.35324985\n",
      "Iteration 1696, loss = 0.22536621\n",
      "Iteration 159, loss = 0.40573305\n",
      "Iteration 188, loss = 0.47827805\n",
      "Iteration 1104, loss = 0.25845781\n",
      "Iteration 1697, loss = 0.22526550\n",
      "Iteration 160, loss = 0.40515145\n",
      "Iteration 309, loss = 0.35313547\n",
      "Iteration 745, loss = 0.32452905\n",
      "Iteration 1698, loss = 0.22511328Iteration 310, loss = 0.35299215\n",
      "Iteration 1105, loss = 0.25837082\n",
      "Iteration 311, loss = 0.35287821\n",
      "Iteration 369, loss = 0.38707988\n",
      "\n",
      "Iteration 312, loss = 0.35276899\n",
      "Iteration 161, loss = 0.40464839\n",
      "Iteration 313, loss = 0.35261484\n",
      "Iteration 1106, loss = 0.25807750\n",
      "Iteration 746, loss = 0.32440145\n",
      "Iteration 189, loss = 0.47764960\n",
      "Iteration 1699, loss = 0.22502115\n",
      "Iteration 314, loss = 0.35248285\n",
      "Iteration 417, loss = 0.36865882\n",
      "Iteration 747, loss = 0.32426607\n",
      "Iteration 1700, loss = 0.22490217\n",
      "Iteration 1107, loss = 0.25813544\n",
      "Iteration 1701, loss = 0.22481578\n",
      "Iteration 1702, loss = 0.22472139\n",
      "Iteration 162, loss = 0.40405073\n",
      "Iteration 748, loss = 0.32415784\n",
      "Iteration 1703, loss = 0.22459299\n",
      "Iteration 1704, loss = 0.22448118\n",
      "Iteration 315, loss = 0.35234084\n",
      "Iteration 163, loss = 0.40353666\n",
      "Iteration 316, loss = 0.35221209\n",
      "Iteration 749, loss = 0.32407040\n",
      "Iteration 1108, loss = 0.25779902\n",
      "Iteration 370, loss = 0.38691191\n",
      "Iteration 1109, loss = 0.25772116\n",
      "Iteration 1705, loss = 0.22442251\n",
      "Iteration 317, loss = 0.35211780\n",
      "Iteration 190, loss = 0.47708749\n",
      "Iteration 1706, loss = 0.22427356\n",
      "Iteration 318, loss = 0.35196506\n",
      "Iteration 418, loss = 0.36845274\n",
      "Iteration 1110, loss = 0.25753792\n",
      "Iteration 1707, loss = 0.22424575\n",
      "Iteration 319, loss = 0.35185698\n",
      "Iteration 1708, loss = 0.22406511\n",
      "Iteration 750, loss = 0.32397042\n",
      "Iteration 320, loss = 0.35172451\n",
      "Iteration 1709, loss = 0.22395744\n",
      "Iteration 1111, loss = 0.25734942\n",
      "Iteration 419, loss = 0.36825503\n",
      "Iteration 321, loss = 0.35158999\n",
      "Iteration 164, loss = 0.40302362\n",
      "Iteration 751, loss = 0.32388107\n",
      "Iteration 322, loss = 0.35145603\n",
      "Iteration 371, loss = 0.38673835\n",
      "Iteration 1710, loss = 0.22386007\n",
      "Iteration 752, loss = 0.32380067\n",
      "Iteration 191, loss = 0.47655340\n",
      "Iteration 1112, loss = 0.25717110\n",
      "Iteration 1711, loss = 0.22375351\n",
      "Iteration 420, loss = 0.36811786\n",
      "Iteration 165, loss = 0.40250276\n",
      "Iteration 323, loss = 0.35133248\n",
      "Iteration 324, loss = 0.35123837\n",
      "Iteration 166, loss = 0.40197771\n",
      "Iteration 1712, loss = 0.22365793\n",
      "Iteration 1113, loss = 0.25708357\n",
      "Iteration 325, loss = 0.35110288\n",
      "Iteration 167, loss = 0.40148435\n",
      "Iteration 421, loss = 0.36797950\n",
      "Iteration 326, loss = 0.35097135\n",
      "Iteration 372, loss = 0.38658231\n",
      "Iteration 753, loss = 0.32364210\n",
      "Iteration 1114, loss = 0.25689927\n",
      "Iteration 1713, loss = 0.22359182\n",
      "Iteration 168, loss = 0.40099915\n",
      "Iteration 327, loss = 0.35085266\n",
      "Iteration 1714, loss = 0.22342579\n",
      "Iteration 1115, loss = 0.25672037\n",
      "Iteration 1715, loss = 0.22343198\n",
      "Iteration 422, loss = 0.36780877\n",
      "Iteration 1716, loss = 0.22332266\n",
      "Iteration 328, loss = 0.35078042\n",
      "Iteration 1116, loss = 0.25659843\n",
      "Iteration 169, loss = 0.40047877\n",
      "Iteration 754, loss = 0.32354442\n",
      "Iteration 1717, loss = 0.22314758\n",
      "Iteration 170, loss = 0.40002341\n",
      "Iteration 1117, loss = 0.25647427\n",
      "Iteration 423, loss = 0.36746959\n",
      "Iteration 171, loss = 0.39950643\n",
      "Iteration 329, loss = 0.35060870\n",
      "Iteration 1718, loss = 0.22302101\n",
      "Iteration 330, loss = 0.35050357\n",
      "Iteration 1118, loss = 0.25626803\n",
      "Iteration 331, loss = 0.35037117\n",
      "Iteration 1119, loss = 0.25610703\n",
      "Iteration 332, loss = 0.35026218\n",
      "Iteration 1719, loss = 0.22294746\n",
      "Iteration 333, loss = 0.35013744\n",
      "Iteration 172, loss = 0.39903766\n",
      "Iteration 1720, loss = 0.22280271\n",
      "Iteration 334, loss = 0.35003321\n",
      "Iteration 424, loss = 0.36729870\n",
      "Iteration 755, loss = 0.32340085\n",
      "Iteration 1120, loss = 0.25595632\n",
      "Iteration 173, loss = 0.39854657\n",
      "Iteration 192, loss = 0.47601035\n",
      "Iteration 335, loss = 0.34989840\n",
      "Iteration 174, loss = 0.39807930\n",
      "Iteration 425, loss = 0.36715147\n",
      "Iteration 373, loss = 0.38642858\n",
      "Iteration 1721, loss = 0.22276942\n",
      "Iteration 1121, loss = 0.25583642\n",
      "Iteration 756, loss = 0.32330557\n",
      "Iteration 336, loss = 0.34979598\n",
      "Iteration 1122, loss = 0.25568785\n",
      "Iteration 426, loss = 0.36694405\n",
      "Iteration 337, loss = 0.34970623\n",
      "Iteration 757, loss = 0.32320030\n",
      "Iteration 1123, loss = 0.25548086\n",
      "Iteration 1722, loss = 0.22263730\n",
      "Iteration 175, loss = 0.39761477\n",
      "Iteration 193, loss = 0.47542977\n",
      "Iteration 338, loss = 0.34956545\n",
      "Iteration 339, loss = 0.34944307\n",
      "Iteration 1124, loss = 0.25537866\n",
      "Iteration 374, loss = 0.38623623\n",
      "Iteration 1723, loss = 0.22247788\n",
      "Iteration 176, loss = 0.39714473\n",
      "Iteration 758, loss = 0.32309990\n",
      "Iteration 1125, loss = 0.25529003\n",
      "Iteration 1724, loss = 0.22241667\n",
      "Iteration 1126, loss = 0.25507506\n",
      "Iteration 759, loss = 0.32299031\n",
      "Iteration 177, loss = 0.39669533\n",
      "Iteration 178, loss = 0.39624549\n",
      "Iteration 194, loss = 0.47497410\n",
      "Iteration 760, loss = 0.32288767\n",
      "Iteration 1725, loss = 0.22225524\n",
      "Iteration 340, loss = 0.34934933\n",
      "Iteration 1127, loss = 0.25494786\n",
      "Iteration 179, loss = 0.39575379\n",
      "Iteration 427, loss = 0.36675913\n",
      "Iteration 761, loss = 0.32275861\n",
      "Iteration 1726, loss = 0.22219984\n",
      "Iteration 180, loss = 0.39536012\n",
      "Iteration 341, loss = 0.34923645\n",
      "Iteration 1128, loss = 0.25474609\n",
      "Iteration 1727, loss = 0.22206631\n",
      "Iteration 342, loss = 0.34910159\n",
      "Iteration 1129, loss = 0.25466123\n",
      "Iteration 1728, loss = 0.22198437\n",
      "Iteration 181, loss = 0.39491309\n",
      "Iteration 762, loss = 0.32268058\n",
      "Iteration 1130, loss = 0.25446052\n",
      "Iteration 1729, loss = 0.22186873\n",
      "Iteration 375, loss = 0.38605160\n",
      "Iteration 343, loss = 0.34900033\n",
      "Iteration 763, loss = 0.32254338\n",
      "Iteration 1730, loss = 0.22179040\n",
      "Iteration 182, loss = 0.39449963\n",
      "Iteration 1131, loss = 0.25440702\n",
      "Iteration 344, loss = 0.34889777\n",
      "Iteration 345, loss = 0.34878318\n",
      "Iteration 183, loss = 0.39401502\n",
      "Iteration 764, loss = 0.32247090\n",
      "Iteration 346, loss = 0.34866223\n",
      "Iteration 1731, loss = 0.22171132\n",
      "Iteration 184, loss = 0.39360340\n",
      "Iteration 347, loss = 0.34855935\n",
      "Iteration 1132, loss = 0.25416584\n",
      "Iteration 185, loss = 0.39316017\n",
      "Iteration 428, loss = 0.36648764\n",
      "Iteration 1732, loss = 0.22152407\n",
      "Iteration 1133, loss = 0.25397786\n",
      "Iteration 1733, loss = 0.22146395\n",
      "Iteration 186, loss = 0.39276334\n",
      "Iteration 1134, loss = 0.25386919\n",
      "Iteration 195, loss = 0.47436689\n",
      "Iteration 1135, loss = 0.25368779\n",
      "Iteration 1734, loss = 0.22133487\n",
      "Iteration 187, loss = 0.39232828\n",
      "Iteration 348, loss = 0.34843723\n",
      "Iteration 765, loss = 0.32234989\n",
      "Iteration 376, loss = 0.38592131\n",
      "Iteration 1136, loss = 0.25352735\n",
      "Iteration 1735, loss = 0.22122392\n",
      "Iteration 188, loss = 0.39191833\n",
      "Iteration 349, loss = 0.34832158\n",
      "Iteration 429, loss = 0.36629785\n",
      "Iteration 1736, loss = 0.22112803\n",
      "Iteration 189, loss = 0.39152628\n",
      "Iteration 1737, loss = 0.22101223\n",
      "Iteration 766, loss = 0.32222779\n",
      "Iteration 1738, loss = 0.22093680\n",
      "Iteration 190, loss = 0.39112403Iteration 1137, loss = 0.25338548\n",
      "\n",
      "Iteration 350, loss = 0.34822194\n",
      "Iteration 1739, loss = 0.22085733\n",
      "Iteration 191, loss = 0.39072487\n",
      "Iteration 1138, loss = 0.25330308\n",
      "Iteration 377, loss = 0.38571635\n",
      "Iteration 430, loss = 0.36612627\n",
      "Iteration 192, loss = 0.39031883\n",
      "Iteration 1740, loss = 0.22073626\n",
      "Iteration 767, loss = 0.32215484\n",
      "Iteration 1741, loss = 0.22065823\n",
      "Iteration 1139, loss = 0.25304274\n",
      "Iteration 1742, loss = 0.22050005\n",
      "Iteration 351, loss = 0.34811933\n",
      "Iteration 1743, loss = 0.22039041\n",
      "Iteration 193, loss = 0.38991717\n",
      "Iteration 352, loss = 0.34803926\n",
      "Iteration 768, loss = 0.32201892\n",
      "Iteration 196, loss = 0.47382967\n",
      "Iteration 353, loss = 0.34790409\n",
      "Iteration 378, loss = 0.38555209\n",
      "Iteration 354, loss = 0.34778317\n",
      "Iteration 1140, loss = 0.25287698\n",
      "Iteration 1744, loss = 0.22028350\n",
      "Iteration 1141, loss = 0.25272864\n",
      "Iteration 194, loss = 0.38953938Iteration 1745, loss = 0.22022227\n",
      "\n",
      "Iteration 1746, loss = 0.22010013\n",
      "Iteration 1142, loss = 0.25258943\n",
      "Iteration 355, loss = 0.34768487\n",
      "Iteration 1143, loss = 0.25247235\n",
      "Iteration 769, loss = 0.32192175\n",
      "Iteration 195, loss = 0.38914419\n",
      "Iteration 1144, loss = 0.25230582\n",
      "Iteration 1747, loss = 0.22000911\n",
      "Iteration 356, loss = 0.34758786\n",
      "Iteration 196, loss = 0.38876214\n",
      "Iteration 1748, loss = 0.21986572\n",
      "Iteration 357, loss = 0.34746439\n",
      "Iteration 1749, loss = 0.21975158\n",
      "Iteration 379, loss = 0.38539833\n",
      "Iteration 197, loss = 0.38842187\n",
      "Iteration 1750, loss = 0.21964149\n",
      "Iteration 1145, loss = 0.25211969\n",
      "Iteration 1751, loss = 0.21956363\n",
      "Iteration 358, loss = 0.34738607\n",
      "Iteration 198, loss = 0.38801469\n",
      "Iteration 431, loss = 0.36591538\n",
      "Iteration 359, loss = 0.34725521\n",
      "Iteration 199, loss = 0.38768147\n",
      "Iteration 360, loss = 0.34716385\n",
      "Iteration 1752, loss = 0.21943507\n",
      "Iteration 200, loss = 0.38728454\n",
      "Iteration 361, loss = 0.34704249\n",
      "Iteration 1146, loss = 0.25195529\n",
      "Iteration 770, loss = 0.32181949\n",
      "Iteration 1753, loss = 0.21939051\n",
      "Iteration 1147, loss = 0.25183319\n",
      "Iteration 362, loss = 0.34694736\n",
      "Iteration 1148, loss = 0.25185280\n",
      "Iteration 771, loss = 0.32170562\n",
      "Iteration 1754, loss = 0.21925137\n",
      "Iteration 197, loss = 0.47328023\n",
      "Iteration 363, loss = 0.34684680\n",
      "Iteration 380, loss = 0.38521879\n",
      "Iteration 432, loss = 0.36576026\n",
      "Iteration 1755, loss = 0.21913420\n",
      "Iteration 772, loss = 0.32163221\n",
      "Iteration 201, loss = 0.38691912\n",
      "Iteration 1756, loss = 0.21903556\n",
      "Iteration 1149, loss = 0.25158656\n",
      "Iteration 364, loss = 0.34675784\n",
      "Iteration 1757, loss = 0.21895168\n",
      "Iteration 202, loss = 0.38654138\n",
      "Iteration 773, loss = 0.32148069\n",
      "Iteration 1758, loss = 0.21881155\n",
      "Iteration 433, loss = 0.36556746\n",
      "Iteration 365, loss = 0.34663602\n",
      "Iteration 1150, loss = 0.25133000\n",
      "Iteration 203, loss = 0.38619968\n",
      "Iteration 1759, loss = 0.21873913\n",
      "Iteration 1151, loss = 0.25115978\n",
      "Iteration 366, loss = 0.34651400\n",
      "Iteration 1760, loss = 0.21858243\n",
      "Iteration 1152, loss = 0.25106263\n",
      "Iteration 434, loss = 0.36537117\n",
      "Iteration 774, loss = 0.32138847\n",
      "Iteration 204, loss = 0.38584433\n",
      "Iteration 1153, loss = 0.25083614\n",
      "Iteration 198, loss = 0.47281254\n",
      "Iteration 205, loss = 0.38548134\n",
      "Iteration 775, loss = 0.32127410\n",
      "Iteration 1761, loss = 0.21849954\n",
      "Iteration 206, loss = 0.38514332\n",
      "Iteration 367, loss = 0.34642209\n",
      "Iteration 381, loss = 0.38509633\n",
      "Iteration 1154, loss = 0.25068651\n",
      "Iteration 207, loss = 0.38481467\n",
      "Iteration 435, loss = 0.36514400\n",
      "Iteration 1762, loss = 0.21841848\n",
      "Iteration 368, loss = 0.34634939\n",
      "Iteration 1763, loss = 0.21826093\n",
      "Iteration 208, loss = 0.38444325\n",
      "Iteration 1155, loss = 0.25062308\n",
      "Iteration 1764, loss = 0.21819939\n",
      "Iteration 369, loss = 0.34622888\n",
      "Iteration 209, loss = 0.38411548\n",
      "Iteration 1765, loss = 0.21810403\n",
      "Iteration 436, loss = 0.36497962\n",
      "Iteration 1766, loss = 0.21815280\n",
      "Iteration 210, loss = 0.38375830\n",
      "Iteration 1767, loss = 0.21785041\n",
      "Iteration 776, loss = 0.32115332\n",
      "Iteration 1156, loss = 0.25041833\n",
      "Iteration 370, loss = 0.34612531\n",
      "Iteration 1768, loss = 0.21780436\n",
      "Iteration 211, loss = 0.38343609\n",
      "Iteration 371, loss = 0.34602414\n",
      "Iteration 1769, loss = 0.21767893\n",
      "Iteration 1157, loss = 0.25024905\n",
      "Iteration 1770, loss = 0.21753101\n",
      "Iteration 212, loss = 0.38309288\n",
      "Iteration 372, loss = 0.34592563\n",
      "Iteration 1771, loss = 0.21743243\n",
      "Iteration 382, loss = 0.38489104\n",
      "Iteration 437, loss = 0.36487879\n",
      "Iteration 1158, loss = 0.25016724\n",
      "Iteration 373, loss = 0.34582040\n",
      "Iteration 777, loss = 0.32105528\n",
      "Iteration 1159, loss = 0.24993878\n",
      "Iteration 1772, loss = 0.21731482\n",
      "Iteration 213, loss = 0.38276971\n",
      "Iteration 374, loss = 0.34571552\n",
      "Iteration 199, loss = 0.47222570\n",
      "Iteration 214, loss = 0.38246122\n",
      "Iteration 1773, loss = 0.21731864\n",
      "Iteration 375, loss = 0.34562921\n",
      "Iteration 1160, loss = 0.24982324\n",
      "Iteration 438, loss = 0.36458310\n",
      "Iteration 778, loss = 0.32099872\n",
      "Iteration 215, loss = 0.38212210\n",
      "Iteration 1161, loss = 0.24955764\n",
      "Iteration 1774, loss = 0.21712479\n",
      "Iteration 216, loss = 0.38180327\n",
      "Iteration 779, loss = 0.32087953\n",
      "Iteration 376, loss = 0.34551234Iteration 383, loss = 0.38475604\n",
      "\n",
      "Iteration 1162, loss = 0.24953618\n",
      "Iteration 377, loss = 0.34542462\n",
      "Iteration 1163, loss = 0.24941258\n",
      "Iteration 780, loss = 0.32074994\n",
      "Iteration 217, loss = 0.38145653\n",
      "Iteration 439, loss = 0.36441653\n",
      "Iteration 1164, loss = 0.24908746\n",
      "Iteration 1775, loss = 0.21705821\n",
      "Iteration 378, loss = 0.34532121\n",
      "Iteration 781, loss = 0.32062152\n",
      "Iteration 1165, loss = 0.24899648\n",
      "Iteration 200, loss = 0.47181829\n",
      "Iteration 379, loss = 0.34521283\n",
      "Iteration 1776, loss = 0.21685671\n",
      "Iteration 1166, loss = 0.24881034\n",
      "Iteration 380, loss = 0.34512673\n",
      "Iteration 384, loss = 0.38456242\n",
      "Iteration 218, loss = 0.38115957\n",
      "Iteration 440, loss = 0.36418827\n",
      "Iteration 782, loss = 0.32055390\n",
      "Iteration 381, loss = 0.34503219\n",
      "Iteration 1777, loss = 0.21686673\n",
      "Iteration 382, loss = 0.34492338\n",
      "Iteration 1167, loss = 0.24860599\n",
      "Iteration 219, loss = 0.38084900\n",
      "Iteration 383, loss = 0.34483530\n",
      "Iteration 385, loss = 0.38439865\n",
      "Iteration 441, loss = 0.36402643\n",
      "Iteration 384, loss = 0.34475301\n",
      "Iteration 1168, loss = 0.24842524\n",
      "Iteration 783, loss = 0.32041647\n",
      "Iteration 385, loss = 0.34465690\n",
      "Iteration 1778, loss = 0.21667407\n",
      "Iteration 386, loss = 0.34453690\n",
      "Iteration 220, loss = 0.38054510\n",
      "Iteration 1779, loss = 0.21658466\n",
      "Iteration 387, loss = 0.34445149\n",
      "Iteration 1169, loss = 0.24829376\n",
      "Iteration 442, loss = 0.36386329\n",
      "Iteration 201, loss = 0.47121495\n",
      "Iteration 784, loss = 0.32033424\n",
      "Iteration 221, loss = 0.38020678\n",
      "Iteration 1170, loss = 0.24809923\n",
      "Iteration 1780, loss = 0.21645130\n",
      "Iteration 388, loss = 0.34438348\n",
      "Iteration 1781, loss = 0.21638032\n",
      "Iteration 1171, loss = 0.24795598\n",
      "Iteration 785, loss = 0.32025733\n",
      "Iteration 222, loss = 0.37993960\n",
      "Iteration 389, loss = 0.34425187\n",
      "Iteration 1782, loss = 0.21628470\n",
      "Iteration 386, loss = 0.38425140\n",
      "Iteration 786, loss = 0.32011413\n",
      "Iteration 1172, loss = 0.24775588\n",
      "Iteration 223, loss = 0.37960598\n",
      "Iteration 390, loss = 0.34415857Iteration 1783, loss = 0.21611921\n",
      "\n",
      "Iteration 224, loss = 0.37934816\n",
      "Iteration 1173, loss = 0.24764799\n",
      "Iteration 391, loss = 0.34406174\n",
      "Iteration 443, loss = 0.36363143\n",
      "Iteration 225, loss = 0.37902325\n",
      "Iteration 202, loss = 0.47073960\n",
      "Iteration 1784, loss = 0.21605283\n",
      "Iteration 226, loss = 0.37871505\n",
      "Iteration 392, loss = 0.34404449\n",
      "Iteration 1174, loss = 0.24743559\n",
      "Iteration 787, loss = 0.31998361\n",
      "Iteration 444, loss = 0.36342415\n",
      "Iteration 393, loss = 0.34387359\n",
      "Iteration 227, loss = 0.37843821\n",
      "Iteration 387, loss = 0.38407746\n",
      "Iteration 1785, loss = 0.21596591\n",
      "Iteration 1175, loss = 0.24729820\n",
      "Iteration 1786, loss = 0.21585378\n",
      "Iteration 394, loss = 0.34378360\n",
      "Iteration 1787, loss = 0.21569827\n",
      "Iteration 788, loss = 0.31988241\n",
      "Iteration 228, loss = 0.37814930\n",
      "Iteration 395, loss = 0.34368872\n",
      "Iteration 1176, loss = 0.24714956\n",
      "Iteration 445, loss = 0.36328859\n",
      "Iteration 1788, loss = 0.21567726\n",
      "Iteration 396, loss = 0.34361040\n",
      "Iteration 1789, loss = 0.21549282\n",
      "Iteration 1177, loss = 0.24698433\n",
      "Iteration 1790, loss = 0.21536328\n",
      "Iteration 789, loss = 0.31976249\n",
      "Iteration 1791, loss = 0.21524263\n",
      "Iteration 397, loss = 0.34349531\n",
      "Iteration 1178, loss = 0.24684125\n",
      "Iteration 1792, loss = 0.21513542\n",
      "Iteration 229, loss = 0.37784853\n",
      "Iteration 790, loss = 0.31968546\n",
      "Iteration 398, loss = 0.34341492\n",
      "Iteration 1793, loss = 0.21506387\n",
      "Iteration 388, loss = 0.38392331\n",
      "Iteration 1179, loss = 0.24668481\n",
      "Iteration 230, loss = 0.37757047\n",
      "Iteration 1180, loss = 0.24653094\n",
      "Iteration 791, loss = 0.31955027\n",
      "Iteration 231, loss = 0.37728645\n",
      "Iteration 1794, loss = 0.21497604\n",
      "Iteration 399, loss = 0.34332348\n",
      "Iteration 446, loss = 0.36309044\n",
      "Iteration 232, loss = 0.37700100\n",
      "Iteration 203, loss = 0.47024050\n",
      "Iteration 233, loss = 0.37672190\n",
      "Iteration 389, loss = 0.38374205\n",
      "Iteration 400, loss = 0.34322795\n",
      "Iteration 1795, loss = 0.21488401\n",
      "Iteration 234, loss = 0.37645571\n",
      "Iteration 1181, loss = 0.24637181\n",
      "Iteration 401, loss = 0.34313503\n",
      "Iteration 1796, loss = 0.21476630\n",
      "Iteration 402, loss = 0.34304087\n",
      "Iteration 792, loss = 0.31943927\n",
      "Iteration 1182, loss = 0.24612383\n",
      "Iteration 235, loss = 0.37618852\n",
      "Iteration 1797, loss = 0.21461512\n",
      "Iteration 1183, loss = 0.24596505\n",
      "Iteration 403, loss = 0.34295362\n",
      "Iteration 447, loss = 0.36285561\n",
      "Iteration 1184, loss = 0.24580726\n",
      "Iteration 1798, loss = 0.21447855\n",
      "Iteration 236, loss = 0.37590262\n",
      "Iteration 1799, loss = 0.21436506\n",
      "Iteration 1185, loss = 0.24568000\n",
      "Iteration 404, loss = 0.34287185\n",
      "Iteration 405, loss = 0.34276491\n",
      "Iteration 1186, loss = 0.24552360\n",
      "Iteration 390, loss = 0.38359982\n",
      "Iteration 793, loss = 0.31936206\n",
      "Iteration 204, loss = 0.46976396\n",
      "Iteration 406, loss = 0.34269358\n",
      "Iteration 1800, loss = 0.21441913\n",
      "Iteration 237, loss = 0.37563409\n",
      "Iteration 448, loss = 0.36269122\n",
      "Iteration 1187, loss = 0.24547652\n",
      "Iteration 1801, loss = 0.21421811\n",
      "Iteration 407, loss = 0.34259419\n",
      "Iteration 794, loss = 0.31923707\n",
      "Iteration 238, loss = 0.37537501\n",
      "Iteration 1802, loss = 0.21405222\n",
      "Iteration 391, loss = 0.38343799\n",
      "Iteration 449, loss = 0.36248784\n",
      "Iteration 1188, loss = 0.24514614\n",
      "Iteration 1803, loss = 0.21396017\n",
      "Iteration 408, loss = 0.34250673\n",
      "Iteration 1189, loss = 0.24507191\n",
      "Iteration 239, loss = 0.37507485\n",
      "Iteration 409, loss = 0.34240365\n",
      "Iteration 795, loss = 0.31918152\n",
      "Iteration 450, loss = 0.36225743\n",
      "Iteration 410, loss = 0.34232147\n",
      "Iteration 1804, loss = 0.21384089\n",
      "Iteration 411, loss = 0.34222937\n",
      "Iteration 392, loss = 0.38327801\n",
      "Iteration 205, loss = 0.46921921\n",
      "Iteration 412, loss = 0.34213628\n",
      "Iteration 796, loss = 0.31902651\n",
      "Iteration 1190, loss = 0.24487116\n",
      "Iteration 1805, loss = 0.21373368\n",
      "Iteration 240, loss = 0.37482203\n",
      "Iteration 451, loss = 0.36216681\n",
      "Iteration 241, loss = 0.37457310\n",
      "Iteration 413, loss = 0.34204444\n",
      "Iteration 797, loss = 0.31889586\n",
      "Iteration 1806, loss = 0.21364494\n",
      "Iteration 1191, loss = 0.24479995\n",
      "Iteration 242, loss = 0.37429601\n",
      "Iteration 393, loss = 0.38316922\n",
      "Iteration 798, loss = 0.31883636\n",
      "Iteration 1807, loss = 0.21353156\n",
      "Iteration 414, loss = 0.34196520\n",
      "Iteration 1808, loss = 0.21339201\n",
      "Iteration 799, loss = 0.31870500\n",
      "Iteration 452, loss = 0.36189989\n",
      "Iteration 243, loss = 0.37407695\n",
      "Iteration 1809, loss = 0.21331211\n",
      "Iteration 206, loss = 0.46877971\n",
      "Iteration 1192, loss = 0.24452847\n",
      "Iteration 415, loss = 0.34187135\n",
      "Iteration 244, loss = 0.37378279\n",
      "Iteration 800, loss = 0.31859471\n",
      "Iteration 1810, loss = 0.21321402\n",
      "Iteration 453, loss = 0.36173662\n",
      "Iteration 416, loss = 0.34180741\n",
      "Iteration 245, loss = 0.37355538\n",
      "Iteration 801, loss = 0.31848123\n",
      "Iteration 1811, loss = 0.21305181\n",
      "Iteration 417, loss = 0.34169366\n",
      "Iteration 246, loss = 0.37328329\n",
      "Iteration 418, loss = 0.34164104\n",
      "Iteration 454, loss = 0.36163345\n",
      "Iteration 247, loss = 0.37303453\n",
      "Iteration 1812, loss = 0.21297462\n",
      "Iteration 1193, loss = 0.24433758\n",
      "Iteration 419, loss = 0.34152041\n",
      "Iteration 394, loss = 0.38297801\n",
      "Iteration 1813, loss = 0.21287506\n",
      "Iteration 420, loss = 0.34143309\n",
      "Iteration 1814, loss = 0.21273000\n",
      "Iteration 455, loss = 0.36135967\n",
      "Iteration 421, loss = 0.34133937\n",
      "Iteration 1194, loss = 0.24416834\n",
      "Iteration 248, loss = 0.37279151\n",
      "Iteration 422, loss = 0.34126095\n",
      "Iteration 1815, loss = 0.21264528\n",
      "Iteration 802, loss = 0.31836789\n",
      "Iteration 1195, loss = 0.24409060\n",
      "Iteration 249, loss = 0.37254221\n",
      "Iteration 1816, loss = 0.21256807\n",
      "Iteration 207, loss = 0.46826262\n",
      "Iteration 423, loss = 0.34116787\n",
      "Iteration 1817, loss = 0.21241778\n",
      "Iteration 1196, loss = 0.24381858\n",
      "Iteration 250, loss = 0.37231213\n",
      "Iteration 1818, loss = 0.21230916\n",
      "Iteration 424, loss = 0.34108595\n",
      "Iteration 1197, loss = 0.24367531\n",
      "Iteration 1819, loss = 0.21218992\n",
      "Iteration 456, loss = 0.36118115\n",
      "Iteration 425, loss = 0.34099492\n",
      "Iteration 251, loss = 0.37206369\n",
      "Iteration 1198, loss = 0.24357381\n",
      "Iteration 1820, loss = 0.21206853\n",
      "Iteration 426, loss = 0.34090689\n",
      "Iteration 1821, loss = 0.21204801\n",
      "Iteration 427, loss = 0.34083999\n",
      "Iteration 395, loss = 0.38278198\n",
      "Iteration 1199, loss = 0.24338888\n",
      "Iteration 803, loss = 0.31826453\n",
      "Iteration 1822, loss = 0.21190489\n",
      "Iteration 428, loss = 0.34074223\n",
      "Iteration 252, loss = 0.37180968\n",
      "Iteration 429, loss = 0.34070545\n",
      "Iteration 457, loss = 0.36096094\n",
      "Iteration 804, loss = 0.31816514\n",
      "Iteration 1823, loss = 0.21178753\n",
      "Iteration 1200, loss = 0.24324534\n",
      "Iteration 1824, loss = 0.21167325\n",
      "Iteration 253, loss = 0.37160843\n",
      "Iteration 805, loss = 0.31805486\n",
      "Iteration 430, loss = 0.34057949\n",
      "Iteration 208, loss = 0.46780680\n",
      "Iteration 1201, loss = 0.24316379\n",
      "Iteration 1825, loss = 0.21157480\n",
      "Iteration 396, loss = 0.38260400\n",
      "Iteration 1826, loss = 0.21148571\n",
      "Iteration 431, loss = 0.34049113\n",
      "Iteration 1827, loss = 0.21133831\n",
      "Iteration 806, loss = 0.31794534\n",
      "Iteration 254, loss = 0.37134225\n",
      "Iteration 458, loss = 0.36081169\n",
      "Iteration 1202, loss = 0.24298304\n",
      "Iteration 1828, loss = 0.21122221\n",
      "Iteration 1829, loss = 0.21113605\n",
      "Iteration 255, loss = 0.37110888\n",
      "Iteration 209, loss = 0.46732248\n",
      "Iteration 1203, loss = 0.24275426\n",
      "Iteration 459, loss = 0.36064196\n",
      "Iteration 1830, loss = 0.21104084\n",
      "Iteration 807, loss = 0.31784465\n",
      "Iteration 1204, loss = 0.24260293\n",
      "Iteration 432, loss = 0.34038521\n",
      "Iteration 256, loss = 0.37088662\n",
      "Iteration 397, loss = 0.38244225\n",
      "Iteration 1831, loss = 0.21095059\n",
      "Iteration 1205, loss = 0.24241619\n",
      "Iteration 460, loss = 0.36041645\n",
      "Iteration 1206, loss = 0.24224372\n",
      "Iteration 808, loss = 0.31773141\n",
      "Iteration 433, loss = 0.34030710\n",
      "Iteration 1832, loss = 0.21082590\n",
      "Iteration 1207, loss = 0.24205903\n",
      "Iteration 257, loss = 0.37062537\n",
      "Iteration 809, loss = 0.31762532\n",
      "Iteration 434, loss = 0.34022610\n",
      "Iteration 1833, loss = 0.21073779\n",
      "Iteration 461, loss = 0.36020406\n",
      "Iteration 258, loss = 0.37041467\n",
      "Iteration 1208, loss = 0.24181893\n",
      "Iteration 1834, loss = 0.21067113\n",
      "Iteration 810, loss = 0.31752661\n",
      "Iteration 398, loss = 0.38230855\n",
      "Iteration 435, loss = 0.34013603\n",
      "Iteration 436, loss = 0.34006941\n",
      "Iteration 210, loss = 0.46685020\n",
      "Iteration 437, loss = 0.33997735\n",
      "Iteration 811, loss = 0.31740293\n",
      "Iteration 1835, loss = 0.21051804\n",
      "Iteration 1209, loss = 0.24175865\n",
      "Iteration 259, loss = 0.37016900\n",
      "Iteration 1836, loss = 0.21044019\n",
      "Iteration 462, loss = 0.36000607\n",
      "Iteration 1837, loss = 0.21031911\n",
      "Iteration 812, loss = 0.31728235\n",
      "Iteration 1838, loss = 0.21020183\n",
      "Iteration 438, loss = 0.33990300\n",
      "Iteration 1210, loss = 0.24155120\n",
      "Iteration 260, loss = 0.36997786\n",
      "Iteration 813, loss = 0.31717414\n",
      "Iteration 1839, loss = 0.21009211\n",
      "Iteration 439, loss = 0.33979285\n",
      "Iteration 1211, loss = 0.24132478\n",
      "Iteration 1840, loss = 0.21003313\n",
      "Iteration 814, loss = 0.31707136\n",
      "Iteration 1212, loss = 0.24132588\n",
      "Iteration 463, loss = 0.35988333\n",
      "Iteration 399, loss = 0.38212847\n",
      "Iteration 261, loss = 0.36973481\n",
      "Iteration 440, loss = 0.33970915\n",
      "Iteration 441, loss = 0.33964993\n",
      "Iteration 1841, loss = 0.20983074\n",
      "Iteration 1213, loss = 0.24109773\n",
      "Iteration 1842, loss = 0.20974930\n",
      "Iteration 464, loss = 0.35966567\n",
      "Iteration 1214, loss = 0.24087835\n",
      "Iteration 262, loss = 0.36950199\n",
      "Iteration 815, loss = 0.31696230\n",
      "Iteration 442, loss = 0.33956746\n",
      "Iteration 211, loss = 0.46648238\n",
      "Iteration 1843, loss = 0.20969518\n",
      "Iteration 1215, loss = 0.24072217\n",
      "Iteration 263, loss = 0.36929628\n",
      "Iteration 1216, loss = 0.24054865\n",
      "Iteration 443, loss = 0.33945554\n",
      "Iteration 264, loss = 0.36906145\n",
      "Iteration 1844, loss = 0.20955714Iteration 1217, loss = 0.24050017\n",
      "\n",
      "Iteration 444, loss = 0.33937640\n",
      "Iteration 400, loss = 0.38201929\n",
      "Iteration 265, loss = 0.36883425\n",
      "Iteration 1845, loss = 0.20941683\n",
      "Iteration 445, loss = 0.33930351\n",
      "Iteration 1218, loss = 0.24021416\n",
      "Iteration 816, loss = 0.31683875\n",
      "Iteration 266, loss = 0.36862230\n",
      "Iteration 1846, loss = 0.20933465\n",
      "Iteration 1219, loss = 0.24006141\n",
      "Iteration 817, loss = 0.31674895\n",
      "Iteration 446, loss = 0.33921014\n",
      "Iteration 1220, loss = 0.23986977\n",
      "Iteration 1847, loss = 0.20921752\n",
      "Iteration 465, loss = 0.35945487\n",
      "Iteration 1848, loss = 0.20912155\n",
      "Iteration 1221, loss = 0.23965965\n",
      "Iteration 267, loss = 0.36842455\n",
      "Iteration 447, loss = 0.33914533\n",
      "Iteration 212, loss = 0.46595968\n",
      "Iteration 1849, loss = 0.20901006\n",
      "Iteration 1222, loss = 0.23949851\n",
      "Iteration 1850, loss = 0.20900691\n",
      "Iteration 818, loss = 0.31662593\n",
      "Iteration 1223, loss = 0.23943570\n",
      "Iteration 1851, loss = 0.20885848\n",
      "Iteration 448, loss = 0.33903259\n",
      "Iteration 268, loss = 0.36817225\n",
      "Iteration 1852, loss = 0.20872637\n",
      "Iteration 401, loss = 0.38180958\n",
      "Iteration 449, loss = 0.33897165\n",
      "Iteration 1224, loss = 0.23925549\n",
      "Iteration 450, loss = 0.33890381\n",
      "Iteration 466, loss = 0.35934135\n",
      "Iteration 451, loss = 0.33880703\n",
      "Iteration 1225, loss = 0.23900647\n",
      "Iteration 819, loss = 0.31652668\n",
      "Iteration 269, loss = 0.36795767\n",
      "Iteration 452, loss = 0.33872695\n",
      "Iteration 1853, loss = 0.20858724\n",
      "Iteration 453, loss = 0.33863720\n",
      "Iteration 402, loss = 0.38164103\n",
      "Iteration 1226, loss = 0.23887697\n",
      "Iteration 1854, loss = 0.20847992Iteration 820, loss = 0.31641875\n",
      "\n",
      "Iteration 270, loss = 0.36774982\n",
      "Iteration 454, loss = 0.33855514\n",
      "Iteration 467, loss = 0.35908911\n",
      "Iteration 1227, loss = 0.23874878\n",
      "Iteration 1855, loss = 0.20839140\n",
      "Iteration 455, loss = 0.33846557\n",
      "Iteration 271, loss = 0.36753748\n",
      "Iteration 1228, loss = 0.23856952\n",
      "Iteration 403, loss = 0.38150956\n",
      "Iteration 213, loss = 0.46555733\n",
      "Iteration 1856, loss = 0.20830580\n",
      "Iteration 1229, loss = 0.23836957\n",
      "Iteration 456, loss = 0.33840073\n",
      "Iteration 272, loss = 0.36732409\n",
      "Iteration 821, loss = 0.31632112\n",
      "Iteration 1857, loss = 0.20818261\n",
      "Iteration 1230, loss = 0.23815090\n",
      "Iteration 468, loss = 0.35894080\n",
      "Iteration 273, loss = 0.36712673\n",
      "Iteration 457, loss = 0.33830337\n",
      "Iteration 1231, loss = 0.23807146\n",
      "Iteration 1858, loss = 0.20806574\n",
      "Iteration 1232, loss = 0.23795643\n",
      "Iteration 274, loss = 0.36692945\n",
      "Iteration 458, loss = 0.33822674\n",
      "Iteration 1233, loss = 0.23768294\n",
      "Iteration 822, loss = 0.31625345\n",
      "Iteration 1859, loss = 0.20807125\n",
      "Iteration 404, loss = 0.38134106\n",
      "Iteration 823, loss = 0.31610020\n",
      "Iteration 275, loss = 0.36669685\n",
      "Iteration 459, loss = 0.33815486\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1234, loss = 0.23751601\n",
      "Iteration 1860, loss = 0.20784409\n",
      "Iteration 824, loss = 0.31598315\n",
      "Iteration 214, loss = 0.46511438\n",
      "Iteration 469, loss = 0.35873665\n",
      "Iteration 1861, loss = 0.20776712\n",
      "Iteration 1235, loss = 0.23733600\n",
      "Iteration 825, loss = 0.31592573\n",
      "Iteration 276, loss = 0.36653758\n",
      "Iteration 1862, loss = 0.20771832\n",
      "Iteration 1863, loss = 0.20752953\n",
      "Iteration 470, loss = 0.35854826\n",
      "Iteration 1864, loss = 0.20744505\n",
      "Iteration 405, loss = 0.38116991\n",
      "Iteration 826, loss = 0.31577413\n",
      "Iteration 277, loss = 0.36629600\n",
      "Iteration 1, loss = 0.74811240\n",
      "Iteration 1865, loss = 0.20736075\n",
      "Iteration 1236, loss = 0.23720912\n",
      "Iteration 1866, loss = 0.20721409\n",
      "Iteration 471, loss = 0.35840635\n",
      "Iteration 215, loss = 0.46459425\n",
      "Iteration 827, loss = 0.31565512\n",
      "Iteration 1867, loss = 0.20710312\n",
      "Iteration 1237, loss = 0.23711926\n",
      "Iteration 278, loss = 0.36611297\n",
      "Iteration 1868, loss = 0.20698106\n",
      "Iteration 1869, loss = 0.20685125\n",
      "Iteration 2, loss = 0.74477713\n",
      "Iteration 279, loss = 0.36589088\n",
      "Iteration 1870, loss = 0.20676628\n",
      "Iteration 1238, loss = 0.23686586\n",
      "Iteration 1871, loss = 0.20668710\n",
      "Iteration 280, loss = 0.36570792\n",
      "Iteration 1239, loss = 0.23668406\n",
      "Iteration 1872, loss = 0.20651872\n",
      "Iteration 472, loss = 0.35818163\n",
      "Iteration 1873, loss = 0.20641988\n",
      "Iteration 281, loss = 0.36550425\n",
      "Iteration 406, loss = 0.38101125\n",
      "Iteration 828, loss = 0.31560956\n",
      "Iteration 1874, loss = 0.20632674\n",
      "Iteration 1240, loss = 0.23654540\n",
      "Iteration 3, loss = 0.73967944\n",
      "Iteration 1875, loss = 0.20622905\n",
      "Iteration 282, loss = 0.36531981\n",
      "Iteration 473, loss = 0.35799383\n",
      "Iteration 1241, loss = 0.23637465\n",
      "Iteration 407, loss = 0.38087312\n",
      "Iteration 829, loss = 0.31543607\n",
      "Iteration 216, loss = 0.46417143\n",
      "Iteration 1876, loss = 0.20614717\n",
      "Iteration 283, loss = 0.36511389\n",
      "Iteration 1877, loss = 0.20599055\n",
      "Iteration 1242, loss = 0.23625291\n",
      "Iteration 284, loss = 0.36492574\n",
      "Iteration 4, loss = 0.73360776\n",
      "Iteration 1878, loss = 0.20587248\n",
      "Iteration 1243, loss = 0.23615782\n",
      "Iteration 1879, loss = 0.20580282\n",
      "Iteration 474, loss = 0.35785458\n",
      "Iteration 285, loss = 0.36471676\n",
      "Iteration 5, loss = 0.72662227\n",
      "Iteration 1880, loss = 0.20564335\n",
      "Iteration 1881, loss = 0.20557954\n",
      "Iteration 6, loss = 0.71961421\n",
      "Iteration 217, loss = 0.46381028\n",
      "Iteration 830, loss = 0.31533420\n",
      "Iteration 1244, loss = 0.23581013\n",
      "Iteration 1882, loss = 0.20543708\n",
      "Iteration 286, loss = 0.36457594\n",
      "Iteration 1245, loss = 0.23564931\n",
      "Iteration 7, loss = 0.71221803\n",
      "Iteration 408, loss = 0.38080104\n",
      "Iteration 1883, loss = 0.20538467\n",
      "Iteration 8, loss = 0.70474414\n",
      "Iteration 475, loss = 0.35759822\n",
      "Iteration 287, loss = 0.36436131\n",
      "Iteration 1884, loss = 0.20524819\n",
      "Iteration 218, loss = 0.46329261\n",
      "Iteration 831, loss = 0.31542269\n",
      "Iteration 1885, loss = 0.20514598\n",
      "Iteration 1246, loss = 0.23552195\n",
      "Iteration 9, loss = 0.69728177\n",
      "Iteration 1886, loss = 0.20501260\n",
      "Iteration 1887, loss = 0.20487895\n",
      "Iteration 288, loss = 0.36415258\n",
      "Iteration 409, loss = 0.38056904\n",
      "Iteration 1247, loss = 0.23537020\n",
      "Iteration 1248, loss = 0.23518890Iteration 476, loss = 0.35741382\n",
      "\n",
      "Iteration 1888, loss = 0.20481180\n",
      "Iteration 832, loss = 0.31510795\n",
      "Iteration 1249, loss = 0.23499755\n",
      "Iteration 10, loss = 0.69017524\n",
      "Iteration 289, loss = 0.36399590\n",
      "Iteration 410, loss = 0.38038389\n",
      "Iteration 11, loss = 0.68311707\n",
      "Iteration 477, loss = 0.35722956\n",
      "Iteration 1250, loss = 0.23483132\n",
      "Iteration 1889, loss = 0.20468612\n",
      "Iteration 833, loss = 0.31501279\n",
      "Iteration 1890, loss = 0.20469098\n",
      "Iteration 290, loss = 0.36377858\n",
      "Iteration 12, loss = 0.67634571\n",
      "Iteration 219, loss = 0.46287956\n",
      "Iteration 291, loss = 0.36361250\n",
      "Iteration 411, loss = 0.38024374\n",
      "Iteration 1251, loss = 0.23471238\n",
      "Iteration 13, loss = 0.66971418\n",
      "Iteration 1252, loss = 0.23449530\n",
      "Iteration 14, loss = 0.66346420\n",
      "Iteration 1891, loss = 0.20449326\n",
      "Iteration 292, loss = 0.36342202\n",
      "Iteration 478, loss = 0.35703722\n",
      "Iteration 1253, loss = 0.23428498\n",
      "Iteration 220, loss = 0.46250597\n",
      "Iteration 1892, loss = 0.20437047\n",
      "Iteration 293, loss = 0.36323483\n",
      "Iteration 1893, loss = 0.20425964\n",
      "Iteration 834, loss = 0.31492296\n",
      "Iteration 294, loss = 0.36305780\n",
      "Iteration 412, loss = 0.38011441\n",
      "Iteration 1254, loss = 0.23417258\n",
      "Iteration 295, loss = 0.36289322\n",
      "Iteration 1894, loss = 0.20412394\n",
      "Iteration 835, loss = 0.31479204\n",
      "Iteration 296, loss = 0.36270572\n",
      "Iteration 1895, loss = 0.20407129\n",
      "Iteration 15, loss = 0.65727722\n",
      "Iteration 1255, loss = 0.23393489\n",
      "Iteration 1896, loss = 0.20394196\n",
      "Iteration 479, loss = 0.35703173\n",
      "Iteration 1256, loss = 0.23383765\n",
      "Iteration 16, loss = 0.65137703\n",
      "Iteration 297, loss = 0.36254486\n",
      "Iteration 1897, loss = 0.20382327\n",
      "Iteration 1257, loss = 0.23369635\n",
      "Iteration 1898, loss = 0.20372642\n",
      "Iteration 298, loss = 0.36235860\n",
      "Iteration 1258, loss = 0.23354389\n",
      "Iteration 1899, loss = 0.20372869\n",
      "Iteration 836, loss = 0.31471445\n",
      "Iteration 413, loss = 0.37991469\n",
      "Iteration 221, loss = 0.46200797\n",
      "Iteration 1900, loss = 0.20349096\n",
      "Iteration 1259, loss = 0.23333100\n",
      "Iteration 299, loss = 0.36218224\n",
      "Iteration 17, loss = 0.64544095\n",
      "Iteration 1260, loss = 0.23319488\n",
      "Iteration 480, loss = 0.35668357\n",
      "Iteration 1901, loss = 0.20343664\n",
      "Iteration 300, loss = 0.36199762\n",
      "Iteration 1902, loss = 0.20327959\n",
      "Iteration 1903, loss = 0.20318806\n",
      "Iteration 18, loss = 0.63981150\n",
      "Iteration 1904, loss = 0.20307629\n",
      "Iteration 1905, loss = 0.20314049\n",
      "Iteration 222, loss = 0.46160340\n",
      "Iteration 837, loss = 0.31461811\n",
      "Iteration 19, loss = 0.63442920\n",
      "Iteration 301, loss = 0.36181253\n",
      "Iteration 1261, loss = 0.23296604\n",
      "Iteration 1906, loss = 0.20283851\n",
      "Iteration 414, loss = 0.37977581\n",
      "Iteration 20, loss = 0.62924277\n",
      "Iteration 481, loss = 0.35655535\n",
      "Iteration 302, loss = 0.36166103\n",
      "Iteration 1907, loss = 0.20272302\n",
      "Iteration 1262, loss = 0.23283023\n",
      "Iteration 303, loss = 0.36148440\n",
      "Iteration 21, loss = 0.62399155\n",
      "Iteration 1908, loss = 0.20263488\n",
      "Iteration 838, loss = 0.31446507\n",
      "Iteration 482, loss = 0.35626024\n",
      "Iteration 223, loss = 0.46117561\n",
      "Iteration 1263, loss = 0.23260206\n",
      "Iteration 1909, loss = 0.20252676\n",
      "Iteration 1264, loss = 0.23253830\n",
      "Iteration 1910, loss = 0.20244819\n",
      "Iteration 22, loss = 0.61890080\n",
      "Iteration 1265, loss = 0.23243806\n",
      "Iteration 839, loss = 0.31438256\n",
      "Iteration 23, loss = 0.61399222\n",
      "Iteration 1266, loss = 0.23210446\n",
      "Iteration 1267, loss = 0.23192922\n",
      "Iteration 840, loss = 0.31426451\n",
      "Iteration 304, loss = 0.36131105\n",
      "Iteration 24, loss = 0.60928766\n",
      "Iteration 1911, loss = 0.20228410\n",
      "Iteration 1268, loss = 0.23177203\n",
      "Iteration 841, loss = 0.31414064\n",
      "Iteration 1912, loss = 0.20222514\n",
      "Iteration 415, loss = 0.37962912\n",
      "Iteration 305, loss = 0.36115669\n",
      "Iteration 1913, loss = 0.20217897\n",
      "Iteration 25, loss = 0.60449690\n",
      "Iteration 1914, loss = 0.20203478\n",
      "Iteration 842, loss = 0.31405876\n",
      "Iteration 1915, loss = 0.20186977\n",
      "Iteration 224, loss = 0.46074679\n",
      "Iteration 1269, loss = 0.23164089\n",
      "Iteration 483, loss = 0.35610720\n",
      "Iteration 1916, loss = 0.20180947\n",
      "Iteration 26, loss = 0.59985161\n",
      "Iteration 843, loss = 0.31391471\n",
      "Iteration 306, loss = 0.36097361\n",
      "Iteration 484, loss = 0.35589802\n",
      "Iteration 27, loss = 0.59541294\n",
      "Iteration 1917, loss = 0.20168329\n",
      "Iteration 844, loss = 0.31382003\n",
      "Iteration 28, loss = 0.59103293\n",
      "Iteration 307, loss = 0.36079804\n",
      "Iteration 1918, loss = 0.20163616\n",
      "Iteration 1270, loss = 0.23143653\n",
      "Iteration 845, loss = 0.31376864\n",
      "Iteration 485, loss = 0.35572623\n",
      "Iteration 1919, loss = 0.20145103\n",
      "Iteration 308, loss = 0.36065119\n",
      "Iteration 1920, loss = 0.20136631\n",
      "Iteration 1271, loss = 0.23125342\n",
      "Iteration 1272, loss = 0.23110725\n",
      "Iteration 1921, loss = 0.20124423\n",
      "Iteration 486, loss = 0.35556244\n",
      "Iteration 225, loss = 0.46043548\n",
      "Iteration 1273, loss = 0.23101596\n",
      "Iteration 29, loss = 0.58677794\n",
      "Iteration 1274, loss = 0.23077324\n",
      "Iteration 1922, loss = 0.20114783\n",
      "Iteration 309, loss = 0.36047336\n",
      "Iteration 1275, loss = 0.23057242\n",
      "Iteration 416, loss = 0.37950102\n",
      "Iteration 487, loss = 0.35532887\n",
      "Iteration 1923, loss = 0.20101468\n",
      "Iteration 846, loss = 0.31361326\n",
      "Iteration 30, loss = 0.58260877\n",
      "Iteration 1924, loss = 0.20090939\n",
      "Iteration 31, loss = 0.57839634\n",
      "Iteration 847, loss = 0.31358931\n",
      "Iteration 1276, loss = 0.23039339\n",
      "Iteration 226, loss = 0.45992337\n",
      "Iteration 1925, loss = 0.20081172\n",
      "Iteration 1277, loss = 0.23032422\n",
      "Iteration 488, loss = 0.35524959\n",
      "Iteration 310, loss = 0.36032777\n",
      "Iteration 1278, loss = 0.23009178\n",
      "Iteration 1926, loss = 0.20070548\n",
      "Iteration 311, loss = 0.36015107\n",
      "Iteration 848, loss = 0.31342077\n",
      "Iteration 312, loss = 0.36000046\n",
      "Iteration 417, loss = 0.37932386\n",
      "Iteration 32, loss = 0.57430772\n",
      "Iteration 1279, loss = 0.23008783\n",
      "Iteration 313, loss = 0.35981434\n",
      "Iteration 1927, loss = 0.20065955\n",
      "Iteration 489, loss = 0.35496907\n",
      "Iteration 314, loss = 0.35965298\n",
      "Iteration 1928, loss = 0.20051151\n",
      "Iteration 849, loss = 0.31328653\n",
      "Iteration 1929, loss = 0.20038220\n",
      "Iteration 315, loss = 0.35950038\n",
      "Iteration 33, loss = 0.57036826\n",
      "Iteration 1930, loss = 0.20027613\n",
      "Iteration 1280, loss = 0.22973485\n",
      "Iteration 1931, loss = 0.20031723\n",
      "Iteration 34, loss = 0.56643634\n",
      "Iteration 1932, loss = 0.20007220\n",
      "Iteration 850, loss = 0.31318086\n",
      "Iteration 1933, loss = 0.19996786\n",
      "Iteration 418, loss = 0.37913477\n",
      "Iteration 316, loss = 0.35932928\n",
      "Iteration 1281, loss = 0.22960103\n",
      "Iteration 851, loss = 0.31304110\n",
      "Iteration 490, loss = 0.35482978\n",
      "Iteration 35, loss = 0.56269862\n",
      "Iteration 317, loss = 0.35920235\n",
      "Iteration 419, loss = 0.37901193\n",
      "Iteration 1934, loss = 0.19986085\n",
      "Iteration 1282, loss = 0.22938247\n",
      "Iteration 36, loss = 0.55891708\n",
      "Iteration 491, loss = 0.35461695\n",
      "Iteration 318, loss = 0.35902571\n",
      "Iteration 227, loss = 0.45957609\n",
      "Iteration 1935, loss = 0.19978339\n",
      "Iteration 319, loss = 0.35887392\n",
      "Iteration 1283, loss = 0.22920830Iteration 852, loss = 0.31291311\n",
      "\n",
      "Iteration 1936, loss = 0.19965209\n",
      "Iteration 1284, loss = 0.22902173\n",
      "Iteration 37, loss = 0.55529588\n",
      "Iteration 1937, loss = 0.19953534\n",
      "Iteration 1285, loss = 0.22880564\n",
      "Iteration 492, loss = 0.35442942\n",
      "Iteration 420, loss = 0.37883768\n",
      "Iteration 853, loss = 0.31283715\n",
      "Iteration 38, loss = 0.55164661\n",
      "Iteration 1938, loss = 0.19946982\n",
      "Iteration 228, loss = 0.45920914\n",
      "Iteration 493, loss = 0.35425249\n",
      "Iteration 320, loss = 0.35871577\n",
      "Iteration 1939, loss = 0.19929654\n",
      "Iteration 39, loss = 0.54810958\n",
      "Iteration 1286, loss = 0.22866486\n",
      "Iteration 854, loss = 0.31275108\n",
      "Iteration 321, loss = 0.35854926\n",
      "Iteration 1287, loss = 0.22846188\n",
      "Iteration 494, loss = 0.35406834\n",
      "Iteration 1940, loss = 0.19924931\n",
      "Iteration 1941, loss = 0.19924564\n",
      "Iteration 1288, loss = 0.22829777\n",
      "Iteration 40, loss = 0.54471811\n",
      "Iteration 322, loss = 0.35842373\n",
      "Iteration 1942, loss = 0.19898780\n",
      "Iteration 1289, loss = 0.22817372\n",
      "Iteration 1943, loss = 0.19893913\n",
      "Iteration 323, loss = 0.35824658\n",
      "Iteration 41, loss = 0.54129828\n",
      "Iteration 421, loss = 0.37867476\n",
      "Iteration 1290, loss = 0.22795138\n",
      "Iteration 324, loss = 0.35811734\n",
      "Iteration 855, loss = 0.31260354\n",
      "Iteration 1291, loss = 0.22784245\n",
      "Iteration 325, loss = 0.35797406\n",
      "Iteration 495, loss = 0.35388601\n",
      "Iteration 42, loss = 0.53798429\n",
      "Iteration 1292, loss = 0.22776054\n",
      "Iteration 1944, loss = 0.19880942\n",
      "Iteration 326, loss = 0.35782668\n",
      "Iteration 229, loss = 0.45874977\n",
      "Iteration 856, loss = 0.31252337\n",
      "Iteration 327, loss = 0.35765188\n",
      "Iteration 1293, loss = 0.22745220\n",
      "Iteration 43, loss = 0.53471540\n",
      "Iteration 1294, loss = 0.22739162\n",
      "Iteration 328, loss = 0.35752941\n",
      "Iteration 44, loss = 0.53145294\n",
      "Iteration 329, loss = 0.35735851\n",
      "Iteration 1295, loss = 0.22713327\n",
      "Iteration 330, loss = 0.35720096\n",
      "Iteration 45, loss = 0.52844622\n",
      "Iteration 1945, loss = 0.19877986\n",
      "Iteration 46, loss = 0.52531301\n",
      "Iteration 422, loss = 0.37851105\n",
      "Iteration 1946, loss = 0.19860636\n",
      "Iteration 230, loss = 0.45833419\n",
      "Iteration 1947, loss = 0.19844371\n",
      "Iteration 1948, loss = 0.19836690\n",
      "Iteration 496, loss = 0.35366077\n",
      "Iteration 1296, loss = 0.22693991\n",
      "Iteration 1949, loss = 0.19828532\n",
      "Iteration 331, loss = 0.35706186\n",
      "Iteration 857, loss = 0.31239146\n",
      "Iteration 1950, loss = 0.19816628\n",
      "Iteration 1951, loss = 0.19806409\n",
      "Iteration 423, loss = 0.37842105\n",
      "Iteration 1297, loss = 0.22678326\n",
      "Iteration 1952, loss = 0.19795789\n",
      "Iteration 858, loss = 0.31232082\n",
      "Iteration 1298, loss = 0.22677527\n",
      "Iteration 1953, loss = 0.19783842\n",
      "Iteration 1954, loss = 0.19774580\n",
      "Iteration 859, loss = 0.31219016\n",
      "Iteration 332, loss = 0.35691806\n",
      "Iteration 424, loss = 0.37822668\n",
      "Iteration 333, loss = 0.35677675\n",
      "Iteration 1299, loss = 0.22646291\n",
      "Iteration 497, loss = 0.35350395\n",
      "Iteration 231, loss = 0.45795497\n",
      "Iteration 1300, loss = 0.22631386\n",
      "Iteration 860, loss = 0.31209169\n",
      "Iteration 1301, loss = 0.22611582\n",
      "Iteration 334, loss = 0.35665059\n",
      "Iteration 1955, loss = 0.19766523\n",
      "Iteration 498, loss = 0.35329595\n",
      "Iteration 1956, loss = 0.19760774\n",
      "Iteration 1957, loss = 0.19745359\n",
      "Iteration 47, loss = 0.52233208\n",
      "Iteration 861, loss = 0.31195313\n",
      "Iteration 1302, loss = 0.22596872\n",
      "Iteration 1958, loss = 0.19731502\n",
      "Iteration 335, loss = 0.35649730\n",
      "Iteration 1959, loss = 0.19723845\n",
      "Iteration 1960, loss = 0.19709316\n",
      "Iteration 499, loss = 0.35312931\n",
      "Iteration 425, loss = 0.37804327\n",
      "Iteration 1961, loss = 0.19700333\n",
      "Iteration 336, loss = 0.35634339\n",
      "Iteration 1303, loss = 0.22585697\n",
      "Iteration 232, loss = 0.45756952\n",
      "Iteration 1962, loss = 0.19687467\n",
      "Iteration 48, loss = 0.51931269\n",
      "Iteration 337, loss = 0.35621916\n",
      "Iteration 1963, loss = 0.19682319\n",
      "Iteration 1964, loss = 0.19669513\n",
      "Iteration 1304, loss = 0.22563718\n",
      "Iteration 338, loss = 0.35606779\n",
      "Iteration 1965, loss = 0.19661518\n",
      "Iteration 339, loss = 0.35591372\n",
      "Iteration 1966, loss = 0.19648721\n",
      "Iteration 49, loss = 0.51646287\n",
      "Iteration 500, loss = 0.35291414\n",
      "Iteration 340, loss = 0.35578595\n",
      "Iteration 1967, loss = 0.19641778\n",
      "Iteration 426, loss = 0.37790881\n",
      "Iteration 1968, loss = 0.19629974\n",
      "Iteration 341, loss = 0.35563316\n",
      "Iteration 862, loss = 0.31186280\n",
      "Iteration 1969, loss = 0.19616162\n",
      "Iteration 342, loss = 0.35550957\n",
      "Iteration 501, loss = 0.35279866\n",
      "Iteration 1970, loss = 0.19607527\n",
      "Iteration 1971, loss = 0.19594556\n",
      "Iteration 343, loss = 0.35536131\n",
      "Iteration 50, loss = 0.51367293\n",
      "Iteration 863, loss = 0.31174738\n",
      "Iteration 1305, loss = 0.22552542\n",
      "Iteration 344, loss = 0.35524682\n",
      "Iteration 1306, loss = 0.22529551\n",
      "Iteration 502, loss = 0.35250896\n",
      "Iteration 233, loss = 0.45715587\n",
      "Iteration 427, loss = 0.37777628\n",
      "Iteration 51, loss = 0.51090255\n",
      "Iteration 1307, loss = 0.22521524\n",
      "Iteration 1308, loss = 0.22506698\n",
      "Iteration 52, loss = 0.50805694\n",
      "Iteration 345, loss = 0.35510162\n",
      "Iteration 503, loss = 0.35234824\n",
      "Iteration 1309, loss = 0.22472680\n",
      "Iteration 864, loss = 0.31164856\n",
      "Iteration 53, loss = 0.50543565\n",
      "Iteration 1972, loss = 0.19587445\n",
      "Iteration 346, loss = 0.35496127\n",
      "Iteration 1310, loss = 0.22460211\n",
      "Iteration 1973, loss = 0.19575458\n",
      "Iteration 504, loss = 0.35217289\n",
      "Iteration 865, loss = 0.31152283\n",
      "Iteration 54, loss = 0.50284971\n",
      "Iteration 234, loss = 0.45680084\n",
      "Iteration 347, loss = 0.35482913\n",
      "Iteration 428, loss = 0.37760476\n",
      "Iteration 1311, loss = 0.22441036\n",
      "Iteration 55, loss = 0.50028103\n",
      "Iteration 866, loss = 0.31143948\n",
      "Iteration 505, loss = 0.35208087\n",
      "Iteration 1974, loss = 0.19570701\n",
      "Iteration 348, loss = 0.35469527\n",
      "Iteration 56, loss = 0.49782557\n",
      "Iteration 429, loss = 0.37746561\n",
      "Iteration 349, loss = 0.35457820\n",
      "Iteration 506, loss = 0.35181868\n",
      "Iteration 57, loss = 0.49538999\n",
      "Iteration 1975, loss = 0.19553874\n",
      "Iteration 1312, loss = 0.22437538\n",
      "Iteration 1976, loss = 0.19546608\n",
      "Iteration 58, loss = 0.49294269\n",
      "Iteration 867, loss = 0.31134893\n",
      "Iteration 1977, loss = 0.19531392\n",
      "Iteration 350, loss = 0.35442811\n",
      "Iteration 1978, loss = 0.19522543\n",
      "Iteration 59, loss = 0.49059333\n",
      "Iteration 351, loss = 0.35430301\n",
      "Iteration 1979, loss = 0.19517733\n",
      "Iteration 235, loss = 0.45643337\n",
      "Iteration 1980, loss = 0.19500494\n",
      "Iteration 507, loss = 0.35171325\n",
      "Iteration 352, loss = 0.35416205\n",
      "Iteration 60, loss = 0.48828874\n",
      "Iteration 1981, loss = 0.19492262\n",
      "Iteration 353, loss = 0.35402626\n",
      "Iteration 1313, loss = 0.22409500\n",
      "Iteration 1982, loss = 0.19491251\n",
      "Iteration 1983, loss = 0.19471324\n",
      "Iteration 354, loss = 0.35390248\n",
      "Iteration 430, loss = 0.37734353\n",
      "Iteration 868, loss = 0.31120153\n",
      "Iteration 355, loss = 0.35376187\n",
      "Iteration 508, loss = 0.35141022\n",
      "Iteration 1314, loss = 0.22396921\n",
      "Iteration 1984, loss = 0.19461140\n",
      "Iteration 61, loss = 0.48606275\n",
      "Iteration 356, loss = 0.35363183\n",
      "Iteration 869, loss = 0.31112075\n",
      "Iteration 357, loss = 0.35351220\n",
      "Iteration 62, loss = 0.48390460\n",
      "Iteration 431, loss = 0.37718754\n",
      "Iteration 1985, loss = 0.19455755\n",
      "Iteration 870, loss = 0.31103810Iteration 358, loss = 0.35338299\n",
      "\n",
      "Iteration 63, loss = 0.48176098\n",
      "Iteration 1315, loss = 0.22376650\n",
      "Iteration 1986, loss = 0.19437626\n",
      "Iteration 236, loss = 0.45601587\n",
      "Iteration 64, loss = 0.47963163\n",
      "Iteration 509, loss = 0.35139794\n",
      "Iteration 359, loss = 0.35326421\n",
      "Iteration 871, loss = 0.31089458\n",
      "Iteration 1316, loss = 0.22365287\n",
      "Iteration 360, loss = 0.35312444\n",
      "Iteration 65, loss = 0.47760812\n",
      "Iteration 361, loss = 0.35301317\n",
      "Iteration 1987, loss = 0.19431153\n",
      "Iteration 66, loss = 0.47556545\n",
      "Iteration 1317, loss = 0.22349690\n",
      "Iteration 1988, loss = 0.19419599\n",
      "Iteration 362, loss = 0.35287388\n",
      "Iteration 432, loss = 0.37699548\n",
      "Iteration 1989, loss = 0.19407999\n",
      "Iteration 67, loss = 0.47360881\n",
      "Iteration 510, loss = 0.35105188\n",
      "Iteration 363, loss = 0.35275245\n",
      "Iteration 1990, loss = 0.19402909\n",
      "Iteration 364, loss = 0.35262748\n",
      "Iteration 1318, loss = 0.22324196\n",
      "Iteration 1991, loss = 0.19387619\n",
      "Iteration 872, loss = 0.31080168\n",
      "Iteration 68, loss = 0.47173058\n",
      "Iteration 237, loss = 0.45568689\n",
      "Iteration 365, loss = 0.35250841\n",
      "Iteration 1992, loss = 0.19379231\n",
      "Iteration 873, loss = 0.31066184\n",
      "Iteration 366, loss = 0.35238030\n",
      "Iteration 69, loss = 0.46990661\n",
      "Iteration 1319, loss = 0.22308786\n",
      "Iteration 1993, loss = 0.19363933\n",
      "Iteration 1994, loss = 0.19356590\n",
      "Iteration 367, loss = 0.35228331\n",
      "Iteration 70, loss = 0.46799695\n",
      "Iteration 1320, loss = 0.22294118\n",
      "Iteration 1995, loss = 0.19351886\n",
      "Iteration 1321, loss = 0.22283446\n",
      "Iteration 1996, loss = 0.19348969\n",
      "Iteration 71, loss = 0.46621692\n",
      "Iteration 511, loss = 0.35089014\n",
      "Iteration 1997, loss = 0.19330747\n",
      "Iteration 874, loss = 0.31055636\n",
      "Iteration 368, loss = 0.35212528\n",
      "Iteration 238, loss = 0.45528100\n",
      "Iteration 1998, loss = 0.19317726\n",
      "Iteration 369, loss = 0.35200431\n",
      "Iteration 1322, loss = 0.22254825\n",
      "Iteration 1999, loss = 0.19307315\n",
      "Iteration 875, loss = 0.31048562\n",
      "Iteration 370, loss = 0.35189266\n",
      "Iteration 433, loss = 0.37685790\n",
      "Iteration 2000, loss = 0.19293296\n",
      "Iteration 371, loss = 0.35176772\n",
      "Iteration 1323, loss = 0.22235088\n",
      "Iteration 72, loss = 0.46448067\n",
      "Iteration 876, loss = 0.31038239\n",
      "Iteration 2001, loss = 0.19287677\n",
      "Iteration 372, loss = 0.35167155\n",
      "Iteration 239, loss = 0.45492945\n",
      "Iteration 877, loss = 0.31021858\n",
      "Iteration 373, loss = 0.35152409\n",
      "Iteration 434, loss = 0.37670005\n",
      "Iteration 1324, loss = 0.22228129\n",
      "Iteration 374, loss = 0.35141426\n",
      "Iteration 2002, loss = 0.19277242\n",
      "Iteration 73, loss = 0.46275632\n",
      "Iteration 1325, loss = 0.22203065\n",
      "Iteration 375, loss = 0.35129033\n",
      "Iteration 1326, loss = 0.22193460\n",
      "Iteration 878, loss = 0.31014561Iteration 376, loss = 0.35117904\n",
      "\n",
      "Iteration 2003, loss = 0.19270007\n",
      "Iteration 512, loss = 0.35065736\n",
      "Iteration 1327, loss = 0.22182096\n",
      "Iteration 2004, loss = 0.19255080\n",
      "Iteration 377, loss = 0.35106306\n",
      "Iteration 74, loss = 0.46112835\n",
      "Iteration 2005, loss = 0.19242169Iteration 1328, loss = 0.22153029\n",
      "\n",
      "Iteration 435, loss = 0.37656913\n",
      "Iteration 1329, loss = 0.22137462\n",
      "Iteration 879, loss = 0.31003918\n",
      "Iteration 1330, loss = 0.22124857\n",
      "Iteration 75, loss = 0.45948154\n",
      "Iteration 513, loss = 0.35047049\n",
      "Iteration 2006, loss = 0.19234295\n",
      "Iteration 378, loss = 0.35092926\n",
      "Iteration 880, loss = 0.30997544\n",
      "Iteration 1331, loss = 0.22103474\n",
      "Iteration 379, loss = 0.35085419\n",
      "Iteration 436, loss = 0.37642682\n",
      "Iteration 2007, loss = 0.19225701\n",
      "Iteration 240, loss = 0.45455935\n",
      "Iteration 881, loss = 0.30981959\n",
      "Iteration 2008, loss = 0.19210686\n",
      "Iteration 76, loss = 0.45784042\n",
      "Iteration 514, loss = 0.35028738\n",
      "Iteration 1332, loss = 0.22085553\n",
      "Iteration 2009, loss = 0.19200440\n",
      "Iteration 380, loss = 0.35070170\n",
      "Iteration 77, loss = 0.45630910\n",
      "Iteration 78, loss = 0.45483338\n",
      "Iteration 515, loss = 0.35012061\n",
      "Iteration 2010, loss = 0.19197194\n",
      "Iteration 1333, loss = 0.22068042\n",
      "Iteration 241, loss = 0.45419380\n",
      "Iteration 381, loss = 0.35059765\n",
      "Iteration 2011, loss = 0.19187011\n",
      "Iteration 882, loss = 0.30975474\n",
      "Iteration 382, loss = 0.35048111\n",
      "Iteration 1334, loss = 0.22050961\n",
      "Iteration 2012, loss = 0.19172894\n",
      "Iteration 383, loss = 0.35038651\n",
      "Iteration 79, loss = 0.45327574\n",
      "Iteration 2013, loss = 0.19156770\n",
      "Iteration 384, loss = 0.35026989\n",
      "Iteration 2014, loss = 0.19148412\n",
      "Iteration 883, loss = 0.30962054\n",
      "Iteration 1335, loss = 0.22056051\n",
      "Iteration 80, loss = 0.45174575\n",
      "Iteration 2015, loss = 0.19137557\n",
      "Iteration 385, loss = 0.35017638\n",
      "Iteration 2016, loss = 0.19133232\n",
      "Iteration 437, loss = 0.37626678\n",
      "Iteration 81, loss = 0.45033112\n",
      "Iteration 2017, loss = 0.19116949\n",
      "Iteration 516, loss = 0.34990316\n",
      "Iteration 1336, loss = 0.22019393\n",
      "Iteration 2018, loss = 0.19131627\n",
      "Iteration 386, loss = 0.35002091\n",
      "Iteration 82, loss = 0.44897046\n",
      "Iteration 2019, loss = 0.19101929\n",
      "Iteration 884, loss = 0.30952232\n",
      "Iteration 83, loss = 0.44762362\n",
      "Iteration 438, loss = 0.37611373\n",
      "Iteration 1337, loss = 0.22000526\n",
      "Iteration 2020, loss = 0.19091680\n",
      "Iteration 242, loss = 0.45377427\n",
      "Iteration 387, loss = 0.34990934\n",
      "Iteration 1338, loss = 0.22001387\n",
      "Iteration 517, loss = 0.34982633\n",
      "Iteration 388, loss = 0.34980379\n",
      "Iteration 2021, loss = 0.19076139\n",
      "Iteration 84, loss = 0.44618748\n",
      "Iteration 1339, loss = 0.21971524\n",
      "Iteration 2022, loss = 0.19068936\n",
      "Iteration 439, loss = 0.37596933\n",
      "Iteration 885, loss = 0.30940053\n",
      "Iteration 1340, loss = 0.21946829\n",
      "Iteration 2023, loss = 0.19056348\n",
      "Iteration 1341, loss = 0.21942963\n",
      "Iteration 2024, loss = 0.19047809\n",
      "Iteration 389, loss = 0.34969067\n",
      "Iteration 886, loss = 0.30925940\n",
      "Iteration 85, loss = 0.44490798\n",
      "Iteration 440, loss = 0.37579580\n",
      "Iteration 1342, loss = 0.21915749\n",
      "Iteration 518, loss = 0.34953192\n",
      "Iteration 2025, loss = 0.19040177\n",
      "Iteration 390, loss = 0.34957401\n",
      "Iteration 86, loss = 0.44369530\n",
      "Iteration 1343, loss = 0.21898999\n",
      "Iteration 391, loss = 0.34947181\n",
      "Iteration 2026, loss = 0.19032702\n",
      "Iteration 87, loss = 0.44233437\n",
      "Iteration 887, loss = 0.30917540\n",
      "Iteration 2027, loss = 0.19017932\n",
      "Iteration 1344, loss = 0.21885377\n",
      "Iteration 2028, loss = 0.19002223\n",
      "Iteration 88, loss = 0.44110198\n",
      "Iteration 519, loss = 0.34943314\n",
      "Iteration 392, loss = 0.34934599\n",
      "Iteration 1345, loss = 0.21875204\n",
      "Iteration 89, loss = 0.43991360\n",
      "Iteration 2029, loss = 0.18997516\n",
      "Iteration 888, loss = 0.30905722\n",
      "Iteration 441, loss = 0.37567359\n",
      "Iteration 1346, loss = 0.21853202\n",
      "Iteration 243, loss = 0.45348921\n",
      "Iteration 393, loss = 0.34927336\n",
      "Iteration 1347, loss = 0.21841778\n",
      "Iteration 889, loss = 0.30895297\n",
      "Iteration 2030, loss = 0.18983373\n",
      "Iteration 1348, loss = 0.21823260\n",
      "Iteration 520, loss = 0.34919925\n",
      "Iteration 2031, loss = 0.18977704\n",
      "Iteration 890, loss = 0.30886955\n",
      "Iteration 1349, loss = 0.21795322\n",
      "Iteration 394, loss = 0.34915389\n",
      "Iteration 90, loss = 0.43874626\n",
      "Iteration 1350, loss = 0.21790463\n",
      "Iteration 891, loss = 0.30879882\n",
      "Iteration 2032, loss = 0.18974469\n",
      "Iteration 521, loss = 0.34904154\n",
      "Iteration 2033, loss = 0.18955602\n",
      "Iteration 892, loss = 0.30863718\n",
      "Iteration 91, loss = 0.43756159\n",
      "Iteration 2034, loss = 0.18940286\n",
      "Iteration 1351, loss = 0.21789558\n",
      "Iteration 395, loss = 0.34902298\n",
      "Iteration 442, loss = 0.37554234\n",
      "Iteration 92, loss = 0.43643569\n",
      "Iteration 522, loss = 0.34886551\n",
      "Iteration 244, loss = 0.45308186\n",
      "Iteration 93, loss = 0.43527768\n",
      "Iteration 94, loss = 0.43420167\n",
      "Iteration 443, loss = 0.37539193\n",
      "Iteration 396, loss = 0.34891622\n",
      "Iteration 95, loss = 0.43316021\n",
      "Iteration 523, loss = 0.34875836\n",
      "Iteration 1352, loss = 0.21751323\n",
      "Iteration 96, loss = 0.43209155\n",
      "Iteration 444, loss = 0.37520073\n",
      "Iteration 397, loss = 0.34881449\n",
      "Iteration 245, loss = 0.45270030\n",
      "Iteration 97, loss = 0.43108151\n",
      "Iteration 2035, loss = 0.18930330\n",
      "Iteration 893, loss = 0.30851342\n",
      "Iteration 398, loss = 0.34869320\n",
      "Iteration 2036, loss = 0.18933457\n",
      "Iteration 2037, loss = 0.18911140\n",
      "Iteration 399, loss = 0.34859642\n",
      "Iteration 445, loss = 0.37506738\n",
      "Iteration 2038, loss = 0.18903163\n",
      "Iteration 894, loss = 0.30842336\n",
      "Iteration 400, loss = 0.34847980\n",
      "Iteration 2039, loss = 0.18889546\n",
      "Iteration 2040, loss = 0.18880261\n",
      "Iteration 98, loss = 0.43007162\n",
      "Iteration 401, loss = 0.34836513\n",
      "Iteration 895, loss = 0.30848329\n",
      "Iteration 524, loss = 0.34844699\n",
      "Iteration 1353, loss = 0.21737606\n",
      "Iteration 2041, loss = 0.18870475\n",
      "Iteration 1354, loss = 0.21713397\n",
      "Iteration 402, loss = 0.34825860\n",
      "Iteration 2042, loss = 0.18861752\n",
      "Iteration 896, loss = 0.30820487\n",
      "Iteration 1355, loss = 0.21694865\n",
      "Iteration 2043, loss = 0.18851735\n",
      "Iteration 403, loss = 0.34815452\n",
      "Iteration 2044, loss = 0.18838172\n",
      "Iteration 1356, loss = 0.21681701\n",
      "Iteration 99, loss = 0.42902511\n",
      "Iteration 1357, loss = 0.21662195\n",
      "Iteration 2045, loss = 0.18841093\n",
      "Iteration 1358, loss = 0.21659669\n",
      "Iteration 2046, loss = 0.18826246\n",
      "Iteration 897, loss = 0.30808496\n",
      "Iteration 2047, loss = 0.18813877\n",
      "Iteration 1359, loss = 0.21637376\n",
      "Iteration 404, loss = 0.34806029\n",
      "Iteration 100, loss = 0.42804686\n",
      "Iteration 1360, loss = 0.21619134\n",
      "Iteration 898, loss = 0.30796612\n",
      "Iteration 1361, loss = 0.21595606\n",
      "Iteration 2048, loss = 0.18798030\n",
      "Iteration 446, loss = 0.37491198\n",
      "Iteration 101, loss = 0.42708753\n",
      "Iteration 1362, loss = 0.21586880\n",
      "Iteration 2049, loss = 0.18788645\n",
      "Iteration 525, loss = 0.34828730\n",
      "Iteration 1363, loss = 0.21559206\n",
      "Iteration 2050, loss = 0.18782788\n",
      "Iteration 246, loss = 0.45234387\n",
      "Iteration 102, loss = 0.42620090\n",
      "Iteration 1364, loss = 0.21543632\n",
      "Iteration 2051, loss = 0.18770792\n",
      "Iteration 405, loss = 0.34793981\n",
      "Iteration 447, loss = 0.37476711\n",
      "Iteration 1365, loss = 0.21528316\n",
      "Iteration 2052, loss = 0.18757234\n",
      "Iteration 406, loss = 0.34783741\n",
      "Iteration 2053, loss = 0.18746465\n",
      "Iteration 1366, loss = 0.21553492\n",
      "Iteration 103, loss = 0.42524397\n",
      "Iteration 1367, loss = 0.21495047\n",
      "Iteration 899, loss = 0.30788775\n",
      "Iteration 2054, loss = 0.18740866\n",
      "Iteration 526, loss = 0.34814789\n",
      "Iteration 1368, loss = 0.21482681\n",
      "Iteration 448, loss = 0.37462724\n",
      "Iteration 2055, loss = 0.18736429\n",
      "Iteration 407, loss = 0.34774309\n",
      "Iteration 527, loss = 0.34793150\n",
      "Iteration 1369, loss = 0.21461023\n",
      "Iteration 2056, loss = 0.18732298\n",
      "Iteration 408, loss = 0.34763672\n",
      "Iteration 449, loss = 0.37446952\n",
      "Iteration 900, loss = 0.30776373\n",
      "Iteration 1370, loss = 0.21441790\n",
      "Iteration 104, loss = 0.42433713\n",
      "Iteration 409, loss = 0.34751532\n",
      "Iteration 2057, loss = 0.18708021\n",
      "Iteration 1371, loss = 0.21425756\n",
      "Iteration 247, loss = 0.45203880\n",
      "Iteration 528, loss = 0.34777117\n",
      "Iteration 410, loss = 0.34740878\n",
      "Iteration 1372, loss = 0.21413280\n",
      "Iteration 450, loss = 0.37433795\n",
      "Iteration 2058, loss = 0.18700615\n",
      "Iteration 901, loss = 0.30765132\n",
      "Iteration 1373, loss = 0.21393265\n",
      "Iteration 411, loss = 0.34729653\n",
      "Iteration 1374, loss = 0.21370751\n",
      "Iteration 105, loss = 0.42345957\n",
      "Iteration 902, loss = 0.30761584\n",
      "Iteration 412, loss = 0.34720722\n",
      "Iteration 1375, loss = 0.21373641\n",
      "Iteration 2059, loss = 0.18698916\n",
      "Iteration 1376, loss = 0.21346289\n",
      "Iteration 529, loss = 0.34755837\n",
      "Iteration 2060, loss = 0.18675769\n",
      "Iteration 1377, loss = 0.21324148\n",
      "Iteration 413, loss = 0.34709408\n",
      "Iteration 903, loss = 0.30741281\n",
      "Iteration 1378, loss = 0.21307089\n",
      "Iteration 2061, loss = 0.18668984\n",
      "Iteration 1379, loss = 0.21300129\n",
      "Iteration 2062, loss = 0.18656546\n",
      "Iteration 106, loss = 0.42255368\n",
      "Iteration 1380, loss = 0.21276822\n",
      "Iteration 248, loss = 0.45163700\n",
      "Iteration 414, loss = 0.34698730\n",
      "Iteration 530, loss = 0.34740109\n",
      "Iteration 904, loss = 0.30733717\n",
      "Iteration 1381, loss = 0.21255432\n",
      "Iteration 2063, loss = 0.18644159\n",
      "Iteration 107, loss = 0.42173308\n",
      "Iteration 2064, loss = 0.18635773\n",
      "Iteration 451, loss = 0.37418668\n",
      "Iteration 108, loss = 0.42085783\n",
      "Iteration 2065, loss = 0.18635809\n",
      "Iteration 415, loss = 0.34689076\n",
      "Iteration 1382, loss = 0.21248286\n",
      "Iteration 2066, loss = 0.18618448\n",
      "Iteration 416, loss = 0.34678243\n",
      "Iteration 531, loss = 0.34724870\n",
      "Iteration 417, loss = 0.34668580\n",
      "Iteration 2067, loss = 0.18607182\n",
      "Iteration 905, loss = 0.30720351\n",
      "Iteration 109, loss = 0.42005208\n",
      "Iteration 418, loss = 0.34657429\n",
      "Iteration 1383, loss = 0.21226131\n",
      "Iteration 2068, loss = 0.18594808Iteration 249, loss = 0.45128279\n",
      "\n",
      "Iteration 419, loss = 0.34649244\n",
      "Iteration 110, loss = 0.41925384\n",
      "Iteration 420, loss = 0.34637609\n",
      "Iteration 2069, loss = 0.18585818\n",
      "Iteration 1384, loss = 0.21204718\n",
      "Iteration 532, loss = 0.34715484\n",
      "Iteration 421, loss = 0.34628233\n",
      "Iteration 452, loss = 0.37402822\n",
      "Iteration 2070, loss = 0.18573939\n",
      "Iteration 111, loss = 0.41847494\n",
      "Iteration 1385, loss = 0.21194371\n",
      "Iteration 422, loss = 0.34617117\n",
      "Iteration 2071, loss = 0.18561072\n",
      "Iteration 423, loss = 0.34607394\n",
      "Iteration 1386, loss = 0.21169594\n",
      "Iteration 906, loss = 0.30710270\n",
      "Iteration 2072, loss = 0.18556362\n",
      "Iteration 250, loss = 0.45094932\n",
      "Iteration 424, loss = 0.34598648\n",
      "Iteration 112, loss = 0.41765887\n",
      "Iteration 907, loss = 0.30710759\n",
      "Iteration 1387, loss = 0.21161923\n",
      "Iteration 425, loss = 0.34588176\n",
      "Iteration 2073, loss = 0.18560673\n",
      "Iteration 1388, loss = 0.21139903\n",
      "Iteration 1389, loss = 0.21125074\n",
      "Iteration 908, loss = 0.30691166\n",
      "Iteration 426, loss = 0.34578078\n",
      "Iteration 2074, loss = 0.18534854\n",
      "Iteration 1390, loss = 0.21105618\n",
      "Iteration 533, loss = 0.34688825\n",
      "Iteration 453, loss = 0.37390789\n",
      "Iteration 427, loss = 0.34567731\n",
      "Iteration 1391, loss = 0.21081386\n",
      "Iteration 113, loss = 0.41690528\n",
      "Iteration 1392, loss = 0.21059956\n",
      "Iteration 2075, loss = 0.18536096\n",
      "Iteration 909, loss = 0.30681019\n",
      "Iteration 534, loss = 0.34682559Iteration 1393, loss = 0.21046526\n",
      "\n",
      "Iteration 428, loss = 0.34558143\n",
      "Iteration 251, loss = 0.45060212\n",
      "Iteration 2076, loss = 0.18512761\n",
      "Iteration 2077, loss = 0.18504948\n",
      "Iteration 114, loss = 0.41611352\n",
      "Iteration 1394, loss = 0.21047182\n",
      "Iteration 2078, loss = 0.18501062\n",
      "Iteration 910, loss = 0.30670721\n",
      "Iteration 2079, loss = 0.18481995\n",
      "Iteration 429, loss = 0.34549363\n",
      "Iteration 115, loss = 0.41540832\n",
      "Iteration 2080, loss = 0.18489194\n",
      "Iteration 454, loss = 0.37374248\n",
      "Iteration 1395, loss = 0.21017414\n",
      "Iteration 535, loss = 0.34649761\n",
      "Iteration 252, loss = 0.45023084\n",
      "Iteration 430, loss = 0.34539751\n",
      "Iteration 116, loss = 0.41466477\n",
      "Iteration 2081, loss = 0.18462611\n",
      "Iteration 2082, loss = 0.18452927\n",
      "Iteration 1396, loss = 0.20999403\n",
      "Iteration 911, loss = 0.30661287\n",
      "Iteration 117, loss = 0.41395252\n",
      "Iteration 536, loss = 0.34634455\n",
      "Iteration 2083, loss = 0.18445754\n",
      "Iteration 431, loss = 0.34530536\n",
      "Iteration 455, loss = 0.37360678\n",
      "Iteration 1397, loss = 0.20976942\n",
      "Iteration 912, loss = 0.30646382\n",
      "Iteration 2084, loss = 0.18433030\n",
      "Iteration 537, loss = 0.34612375\n",
      "Iteration 2085, loss = 0.18423765\n",
      "Iteration 118, loss = 0.41329145\n",
      "Iteration 2086, loss = 0.18414610\n",
      "Iteration 432, loss = 0.34520556\n",
      "Iteration 1398, loss = 0.20960258\n",
      "Iteration 2087, loss = 0.18398874\n",
      "Iteration 538, loss = 0.34593923\n",
      "Iteration 433, loss = 0.34510187\n",
      "Iteration 913, loss = 0.30638203\n",
      "Iteration 2088, loss = 0.18409844\n",
      "Iteration 434, loss = 0.34501619\n",
      "Iteration 1399, loss = 0.20945756\n",
      "Iteration 539, loss = 0.34579058\n",
      "Iteration 456, loss = 0.37344606\n",
      "Iteration 1400, loss = 0.20934962Iteration 253, loss = 0.44996304\n",
      "\n",
      "Iteration 2089, loss = 0.18379798\n",
      "Iteration 119, loss = 0.41257641\n",
      "Iteration 435, loss = 0.34493518\n",
      "Iteration 2090, loss = 0.18379308\n",
      "Iteration 1401, loss = 0.20907606\n",
      "Iteration 2091, loss = 0.18376742\n",
      "Iteration 914, loss = 0.30625692\n",
      "Iteration 436, loss = 0.34480318\n",
      "Iteration 120, loss = 0.41190819\n",
      "Iteration 1402, loss = 0.20884004\n",
      "Iteration 540, loss = 0.34558095\n",
      "Iteration 437, loss = 0.34472949\n",
      "Iteration 457, loss = 0.37337577\n",
      "Iteration 121, loss = 0.41120475\n",
      "Iteration 438, loss = 0.34462584\n",
      "Iteration 2092, loss = 0.18347500\n",
      "Iteration 915, loss = 0.30614484\n",
      "Iteration 1403, loss = 0.20877091\n",
      "Iteration 2093, loss = 0.18339986\n",
      "Iteration 254, loss = 0.44953583\n",
      "Iteration 122, loss = 0.41063892\n",
      "Iteration 1404, loss = 0.20867808\n",
      "Iteration 2094, loss = 0.18333715\n",
      "Iteration 541, loss = 0.34542749\n",
      "Iteration 2095, loss = 0.18322309\n",
      "Iteration 458, loss = 0.37315708\n",
      "Iteration 1405, loss = 0.20843653\n",
      "Iteration 439, loss = 0.34452242\n",
      "Iteration 2096, loss = 0.18311231\n",
      "Iteration 916, loss = 0.30605593\n",
      "Iteration 2097, loss = 0.18302120\n",
      "Iteration 123, loss = 0.40994852\n",
      "Iteration 440, loss = 0.34443627\n",
      "Iteration 2098, loss = 0.18289813\n",
      "Iteration 917, loss = 0.30601092\n",
      "Iteration 542, loss = 0.34528332\n",
      "Iteration 124, loss = 0.40930122\n",
      "Iteration 441, loss = 0.34434921\n",
      "Iteration 255, loss = 0.44924757\n",
      "Iteration 1406, loss = 0.20821963\n",
      "Iteration 2099, loss = 0.18278777\n",
      "Iteration 918, loss = 0.30581479\n",
      "Iteration 125, loss = 0.40870376\n",
      "Iteration 442, loss = 0.34424059\n",
      "Iteration 2100, loss = 0.18275478\n",
      "Iteration 443, loss = 0.34414368\n",
      "Iteration 1407, loss = 0.20796886\n",
      "Iteration 126, loss = 0.40808163\n",
      "Iteration 543, loss = 0.34505643\n",
      "Iteration 2101, loss = 0.18257349\n",
      "Iteration 444, loss = 0.34406224\n",
      "Iteration 919, loss = 0.30570116\n",
      "Iteration 459, loss = 0.37301913\n",
      "Iteration 1408, loss = 0.20781595\n",
      "Iteration 2102, loss = 0.18256893\n",
      "Iteration 127, loss = 0.40749327\n",
      "Iteration 1409, loss = 0.20768281\n",
      "Iteration 2103, loss = 0.18238341\n",
      "Iteration 128, loss = 0.40687953\n",
      "Iteration 2104, loss = 0.18234667\n",
      "Iteration 1410, loss = 0.20742129\n",
      "Iteration 920, loss = 0.30559432\n",
      "Iteration 544, loss = 0.34486232\n",
      "Iteration 1411, loss = 0.20738220\n",
      "Iteration 445, loss = 0.34397066\n",
      "Iteration 2105, loss = 0.18223548\n",
      "Iteration 129, loss = 0.40628715\n",
      "Iteration 1412, loss = 0.20708982\n",
      "Iteration 446, loss = 0.34386378\n",
      "Iteration 1413, loss = 0.20698640\n",
      "Iteration 921, loss = 0.30550338\n",
      "Iteration 2106, loss = 0.18209190\n",
      "Iteration 1414, loss = 0.20681255\n",
      "Iteration 447, loss = 0.34378040\n",
      "Iteration 256, loss = 0.44886571\n",
      "Iteration 545, loss = 0.34470944\n",
      "Iteration 2107, loss = 0.18203729\n",
      "Iteration 922, loss = 0.30540315\n",
      "Iteration 448, loss = 0.34366650\n",
      "Iteration 1415, loss = 0.20662358\n",
      "Iteration 2108, loss = 0.18190601\n",
      "Iteration 460, loss = 0.37286883\n",
      "Iteration 130, loss = 0.40568974\n",
      "Iteration 923, loss = 0.30528925\n",
      "Iteration 1416, loss = 0.20638841\n",
      "Iteration 449, loss = 0.34358912\n",
      "Iteration 546, loss = 0.34450138\n",
      "Iteration 1417, loss = 0.20624011\n",
      "Iteration 450, loss = 0.34349770\n",
      "Iteration 2109, loss = 0.18183830\n",
      "Iteration 924, loss = 0.30517538\n",
      "Iteration 2110, loss = 0.18173543\n",
      "Iteration 1418, loss = 0.20606234\n",
      "Iteration 2111, loss = 0.18160203\n",
      "Iteration 925, loss = 0.30507051\n",
      "Iteration 451, loss = 0.34341144\n",
      "Iteration 131, loss = 0.40515995\n",
      "Iteration 1419, loss = 0.20586245\n",
      "Iteration 547, loss = 0.34430793Iteration 1420, loss = 0.20567918\n",
      "Iteration 2112, loss = 0.18153993\n",
      "Iteration 926, loss = 0.30502691\n",
      "Iteration 461, loss = 0.37272270\n",
      "Iteration 132, loss = 0.40460140\n",
      "Iteration 452, loss = 0.34331707\n",
      "Iteration 2113, loss = 0.18149352\n",
      "Iteration 1421, loss = 0.20553038\n",
      "Iteration 2114, loss = 0.18128469\n",
      "Iteration 257, loss = 0.44851326\n",
      "\n",
      "Iteration 927, loss = 0.30486148\n",
      "Iteration 453, loss = 0.34322104\n",
      "Iteration 1422, loss = 0.20533740\n",
      "Iteration 2115, loss = 0.18119283\n",
      "Iteration 133, loss = 0.40400312\n",
      "Iteration 462, loss = 0.37258531\n",
      "Iteration 1423, loss = 0.20522271\n",
      "Iteration 2116, loss = 0.18109491\n",
      "Iteration 454, loss = 0.34312914\n",
      "Iteration 2117, loss = 0.18100663\n",
      "Iteration 1424, loss = 0.20496616\n",
      "Iteration 928, loss = 0.30475797\n",
      "Iteration 455, loss = 0.34303265\n",
      "Iteration 2118, loss = 0.18088396\n",
      "Iteration 1425, loss = 0.20512966\n",
      "Iteration 456, loss = 0.34294503\n",
      "Iteration 548, loss = 0.34415822\n",
      "Iteration 2119, loss = 0.18086209\n",
      "Iteration 134, loss = 0.40346275\n",
      "Iteration 2120, loss = 0.18074625\n",
      "Iteration 1426, loss = 0.20469853\n",
      "Iteration 135, loss = 0.40295485\n",
      "Iteration 463, loss = 0.37245130\n",
      "Iteration 2121, loss = 0.18063409\n",
      "Iteration 457, loss = 0.34284926\n",
      "Iteration 1427, loss = 0.20453366\n",
      "Iteration 929, loss = 0.30462355\n",
      "Iteration 136, loss = 0.40241269\n",
      "Iteration 1428, loss = 0.20439945\n",
      "Iteration 2122, loss = 0.18051517\n",
      "Iteration 549, loss = 0.34395683\n",
      "Iteration 1429, loss = 0.20412714\n",
      "Iteration 2123, loss = 0.18036113\n",
      "Iteration 458, loss = 0.34275848\n",
      "Iteration 137, loss = 0.40189171\n",
      "Iteration 930, loss = 0.30452518\n",
      "Iteration 2124, loss = 0.18031140\n",
      "Iteration 459, loss = 0.34265659\n",
      "Iteration 464, loss = 0.37232925\n",
      "Iteration 2125, loss = 0.18026430\n",
      "Iteration 1430, loss = 0.20394017\n",
      "Iteration 460, loss = 0.34258255\n",
      "Iteration 258, loss = 0.44822055\n",
      "Iteration 2126, loss = 0.18006675\n",
      "Iteration 138, loss = 0.40136388\n",
      "Iteration 931, loss = 0.30446707\n",
      "Iteration 461, loss = 0.34248631\n",
      "Iteration 2127, loss = 0.18006371\n",
      "Iteration 550, loss = 0.34376955\n",
      "Iteration 1431, loss = 0.20372462\n",
      "Iteration 462, loss = 0.34242497\n",
      "Iteration 932, loss = 0.30429719\n",
      "Iteration 463, loss = 0.34230088\n",
      "Iteration 2128, loss = 0.17990131\n",
      "Iteration 139, loss = 0.40087980\n",
      "Iteration 1432, loss = 0.20360760\n",
      "Iteration 551, loss = 0.34360733\n",
      "Iteration 465, loss = 0.37213971\n",
      "Iteration 1433, loss = 0.20339865\n",
      "Iteration 464, loss = 0.34221041\n",
      "Iteration 2129, loss = 0.17979307\n",
      "Iteration 140, loss = 0.40035177\n",
      "Iteration 933, loss = 0.30422950\n",
      "Iteration 552, loss = 0.34340782\n",
      "Iteration 2130, loss = 0.17967931\n",
      "Iteration 1434, loss = 0.20324102\n",
      "Iteration 465, loss = 0.34212000\n",
      "Iteration 141, loss = 0.39985757\n",
      "Iteration 2131, loss = 0.17959977\n",
      "Iteration 259, loss = 0.44791276\n",
      "Iteration 1435, loss = 0.20323256\n",
      "Iteration 466, loss = 0.34202637\n",
      "Iteration 142, loss = 0.39941174\n",
      "Iteration 2132, loss = 0.17948989\n",
      "Iteration 553, loss = 0.34327977\n",
      "Iteration 2133, loss = 0.17939064\n",
      "Iteration 1436, loss = 0.20290454\n",
      "Iteration 467, loss = 0.34194049\n",
      "Iteration 143, loss = 0.39890880\n",
      "Iteration 2134, loss = 0.17930723\n",
      "Iteration 934, loss = 0.30408735\n",
      "Iteration 466, loss = 0.37204548\n",
      "Iteration 1437, loss = 0.20271546\n",
      "Iteration 1438, loss = 0.20254039\n",
      "Iteration 935, loss = 0.30404481\n",
      "Iteration 2135, loss = 0.17924346\n",
      "Iteration 468, loss = 0.34184611\n",
      "Iteration 144, loss = 0.39846247\n",
      "Iteration 554, loss = 0.34306390\n",
      "Iteration 1439, loss = 0.20233013\n",
      "Iteration 2136, loss = 0.17910585\n",
      "Iteration 936, loss = 0.30388399\n",
      "Iteration 145, loss = 0.39793830\n",
      "Iteration 260, loss = 0.44755241\n",
      "Iteration 2137, loss = 0.17904621\n",
      "Iteration 937, loss = 0.30380366\n",
      "Iteration 469, loss = 0.34175600\n",
      "Iteration 1440, loss = 0.20214287\n",
      "Iteration 2138, loss = 0.17891311\n",
      "Iteration 470, loss = 0.34168885\n",
      "Iteration 2139, loss = 0.17883709\n",
      "Iteration 146, loss = 0.39750311\n",
      "Iteration 938, loss = 0.30364903\n",
      "Iteration 471, loss = 0.34157903\n",
      "Iteration 555, loss = 0.34286041\n",
      "Iteration 467, loss = 0.37184757\n",
      "Iteration 147, loss = 0.39704338\n",
      "Iteration 2140, loss = 0.17868696\n",
      "Iteration 1441, loss = 0.20208686\n",
      "Iteration 2141, loss = 0.17863364\n",
      "Iteration 472, loss = 0.34149993\n",
      "Iteration 1442, loss = 0.20182188\n",
      "Iteration 939, loss = 0.30359097\n",
      "Iteration 2142, loss = 0.17849905\n",
      "Iteration 148, loss = 0.39658694\n",
      "Iteration 473, loss = 0.34140182\n",
      "Iteration 2143, loss = 0.17840174\n",
      "Iteration 556, loss = 0.34267374\n",
      "Iteration 1443, loss = 0.20162281\n",
      "Iteration 940, loss = 0.30343679\n",
      "Iteration 474, loss = 0.34132321\n",
      "Iteration 468, loss = 0.37172431\n",
      "Iteration 475, loss = 0.34123221\n",
      "Iteration 2144, loss = 0.17831414\n",
      "Iteration 557, loss = 0.34254200\n",
      "Iteration 1444, loss = 0.20141455\n",
      "Iteration 941, loss = 0.30333668\n",
      "Iteration 476, loss = 0.34114264\n",
      "Iteration 2145, loss = 0.17821316\n",
      "Iteration 261, loss = 0.44721553\n",
      "Iteration 469, loss = 0.37156022\n",
      "Iteration 942, loss = 0.30323723Iteration 1445, loss = 0.20127628\n",
      "\n",
      "Iteration 2146, loss = 0.17812047\n",
      "Iteration 149, loss = 0.39616656\n",
      "Iteration 2147, loss = 0.17802454\n",
      "Iteration 477, loss = 0.34104910\n",
      "Iteration 2148, loss = 0.17790710\n",
      "Iteration 478, loss = 0.34097068\n",
      "Iteration 470, loss = 0.37145248\n",
      "Iteration 1446, loss = 0.20117080\n",
      "Iteration 943, loss = 0.30315829\n",
      "Iteration 2149, loss = 0.17792371\n",
      "Iteration 150, loss = 0.39573369\n",
      "Iteration 1447, loss = 0.20093829\n",
      "Iteration 558, loss = 0.34231547\n",
      "Iteration 2150, loss = 0.17781621\n",
      "Iteration 479, loss = 0.34088735\n",
      "Iteration 1448, loss = 0.20081945\n",
      "Iteration 471, loss = 0.37129691\n",
      "Iteration 944, loss = 0.30303432\n",
      "Iteration 480, loss = 0.34079106\n",
      "Iteration 151, loss = 0.39526760\n",
      "Iteration 262, loss = 0.44696891\n",
      "Iteration 1449, loss = 0.20063371\n",
      "Iteration 481, loss = 0.34071319\n",
      "Iteration 2151, loss = 0.17763133\n",
      "Iteration 559, loss = 0.34212734\n",
      "Iteration 152, loss = 0.39487147\n",
      "Iteration 482, loss = 0.34061276\n",
      "Iteration 472, loss = 0.37115326\n",
      "Iteration 483, loss = 0.34051162\n",
      "Iteration 2152, loss = 0.17749268\n",
      "Iteration 153, loss = 0.39444188\n",
      "Iteration 484, loss = 0.34042989\n",
      "Iteration 154, loss = 0.39404421\n",
      "Iteration 560, loss = 0.34194573\n",
      "Iteration 945, loss = 0.30294428\n",
      "Iteration 473, loss = 0.37101747\n",
      "Iteration 1450, loss = 0.20041837\n",
      "Iteration 155, loss = 0.39360927\n",
      "Iteration 263, loss = 0.44659142\n",
      "Iteration 2153, loss = 0.17749207\n",
      "Iteration 485, loss = 0.34035711\n",
      "Iteration 946, loss = 0.30287142\n",
      "Iteration 1451, loss = 0.20046755\n",
      "Iteration 561, loss = 0.34182392\n",
      "Iteration 2154, loss = 0.17729942\n",
      "Iteration 947, loss = 0.30266956\n",
      "Iteration 2155, loss = 0.17728697\n",
      "Iteration 486, loss = 0.34026255\n",
      "Iteration 1452, loss = 0.20019503\n",
      "Iteration 562, loss = 0.34168505\n",
      "Iteration 948, loss = 0.30260189\n",
      "Iteration 2156, loss = 0.17719532\n",
      "Iteration 1453, loss = 0.20001275\n",
      "Iteration 156, loss = 0.39319105\n",
      "Iteration 474, loss = 0.37086696\n",
      "Iteration 1454, loss = 0.19975115\n",
      "Iteration 487, loss = 0.34017198\n",
      "Iteration 2157, loss = 0.17713169\n",
      "Iteration 1455, loss = 0.19951858\n",
      "Iteration 157, loss = 0.39280777\n",
      "Iteration 1456, loss = 0.19939969\n",
      "Iteration 2158, loss = 0.17696453\n",
      "Iteration 264, loss = 0.44625274\n",
      "Iteration 488, loss = 0.34009592\n",
      "Iteration 2159, loss = 0.17683891\n",
      "Iteration 489, loss = 0.33998430\n",
      "Iteration 2160, loss = 0.17673298\n",
      "Iteration 949, loss = 0.30252313\n",
      "Iteration 490, loss = 0.33990768\n",
      "Iteration 1457, loss = 0.19923249\n",
      "Iteration 158, loss = 0.39242151\n",
      "Iteration 491, loss = 0.33981844\n",
      "Iteration 1458, loss = 0.19905210\n",
      "Iteration 475, loss = 0.37074991\n",
      "Iteration 950, loss = 0.30238369\n",
      "Iteration 563, loss = 0.34154057\n",
      "Iteration 2161, loss = 0.17661080\n",
      "Iteration 492, loss = 0.33972853\n",
      "Iteration 1459, loss = 0.19891482\n",
      "Iteration 1460, loss = 0.19871386\n",
      "Iteration 493, loss = 0.33965922\n",
      "Iteration 2162, loss = 0.17648049\n",
      "Iteration 951, loss = 0.30228233\n",
      "Iteration 494, loss = 0.33956344\n",
      "Iteration 564, loss = 0.34132266\n",
      "Iteration 159, loss = 0.39201182\n",
      "Iteration 2163, loss = 0.17648267\n",
      "Iteration 495, loss = 0.33948264\n",
      "Iteration 2164, loss = 0.17628872\n",
      "Iteration 496, loss = 0.33938655\n",
      "Iteration 952, loss = 0.30213636\n",
      "Iteration 2165, loss = 0.17618280\n",
      "Iteration 497, loss = 0.33931603\n",
      "Iteration 2166, loss = 0.17611515\n",
      "Iteration 2167, loss = 0.17602458\n",
      "Iteration 160, loss = 0.39165769\n",
      "Iteration 953, loss = 0.30203534\n",
      "Iteration 476, loss = 0.37060279\n",
      "Iteration 1461, loss = 0.19851938\n",
      "Iteration 161, loss = 0.39123997\n",
      "Iteration 2168, loss = 0.17588606\n",
      "Iteration 565, loss = 0.34101772\n",
      "Iteration 1462, loss = 0.19835647\n",
      "Iteration 162, loss = 0.39086211\n",
      "Iteration 954, loss = 0.30195165\n",
      "Iteration 477, loss = 0.37044110\n",
      "Iteration 2169, loss = 0.17580688\n",
      "Iteration 163, loss = 0.39046685\n",
      "Iteration 955, loss = 0.30181350\n",
      "Iteration 1463, loss = 0.19823066\n",
      "Iteration 2170, loss = 0.17574039\n",
      "Iteration 265, loss = 0.44598454\n",
      "Iteration 498, loss = 0.33921541\n",
      "Iteration 956, loss = 0.30167288\n",
      "Iteration 566, loss = 0.34092012\n",
      "Iteration 164, loss = 0.39010833\n",
      "Iteration 2171, loss = 0.17560484\n",
      "Iteration 499, loss = 0.33912454\n",
      "Iteration 957, loss = 0.30160522\n",
      "Iteration 500, loss = 0.33905083\n",
      "Iteration 2172, loss = 0.17553273\n",
      "Iteration 567, loss = 0.34069229\n",
      "Iteration 1464, loss = 0.19813334\n",
      "Iteration 165, loss = 0.38975132\n",
      "Iteration 958, loss = 0.30147422\n",
      "Iteration 2173, loss = 0.17536062\n",
      "Iteration 501, loss = 0.33895431\n",
      "Iteration 166, loss = 0.38936398\n",
      "Iteration 1465, loss = 0.19782604\n",
      "Iteration 2174, loss = 0.17527210\n",
      "Iteration 167, loss = 0.38902124\n",
      "Iteration 2175, loss = 0.17515210\n",
      "Iteration 959, loss = 0.30143061\n",
      "Iteration 266, loss = 0.44558148\n",
      "Iteration 478, loss = 0.37029424\n",
      "Iteration 502, loss = 0.33889018\n",
      "Iteration 168, loss = 0.38866147\n",
      "Iteration 960, loss = 0.30124277\n",
      "Iteration 2176, loss = 0.17503918\n",
      "Iteration 568, loss = 0.34047554\n",
      "Iteration 169, loss = 0.38832563\n",
      "Iteration 1466, loss = 0.19764651\n",
      "Iteration 2177, loss = 0.17494795\n",
      "Iteration 961, loss = 0.30115664\n",
      "Iteration 1467, loss = 0.19748376\n",
      "Iteration 2178, loss = 0.17493148\n",
      "Iteration 2179, loss = 0.17478533\n",
      "Iteration 1468, loss = 0.19732632\n",
      "Iteration 2180, loss = 0.17469562\n",
      "Iteration 503, loss = 0.33882029\n",
      "Iteration 569, loss = 0.34028833\n",
      "Iteration 504, loss = 0.33869095\n",
      "Iteration 1469, loss = 0.19715551\n",
      "Iteration 479, loss = 0.37014385\n",
      "Iteration 962, loss = 0.30102819\n",
      "Iteration 2181, loss = 0.17453925\n",
      "Iteration 170, loss = 0.38795373\n",
      "Iteration 505, loss = 0.33863558\n",
      "Iteration 267, loss = 0.44528847\n",
      "Iteration 506, loss = 0.33854808\n",
      "Iteration 963, loss = 0.30096154\n",
      "Iteration 1470, loss = 0.19702944\n",
      "Iteration 1471, loss = 0.19686328\n",
      "Iteration 964, loss = 0.30082307\n",
      "Iteration 2182, loss = 0.17441678\n",
      "Iteration 507, loss = 0.33843334\n",
      "Iteration 171, loss = 0.38760948\n",
      "Iteration 480, loss = 0.37002804\n",
      "Iteration 508, loss = 0.33841099\n",
      "Iteration 172, loss = 0.38726292\n",
      "Iteration 1472, loss = 0.19657630\n",
      "Iteration 509, loss = 0.33827072\n",
      "Iteration 2183, loss = 0.17430850\n",
      "Iteration 173, loss = 0.38693428\n",
      "Iteration 510, loss = 0.33818738\n",
      "Iteration 965, loss = 0.30070294\n",
      "Iteration 570, loss = 0.34016381\n",
      "Iteration 1473, loss = 0.19648312\n",
      "Iteration 174, loss = 0.38656192\n",
      "Iteration 2184, loss = 0.17421401\n",
      "Iteration 1474, loss = 0.19632271\n",
      "Iteration 1475, loss = 0.19601683\n",
      "Iteration 2185, loss = 0.17409888\n",
      "Iteration 268, loss = 0.44494673\n",
      "Iteration 966, loss = 0.30060553\n",
      "Iteration 511, loss = 0.33809667\n",
      "Iteration 2186, loss = 0.17396922\n",
      "Iteration 512, loss = 0.33800473\n",
      "Iteration 2187, loss = 0.17406377\n",
      "Iteration 481, loss = 0.36985003\n",
      "Iteration 2188, loss = 0.17378249\n",
      "Iteration 513, loss = 0.33793792\n",
      "Iteration 1476, loss = 0.19585538\n",
      "Iteration 2189, loss = 0.17363766\n",
      "Iteration 967, loss = 0.30049329\n",
      "Iteration 175, loss = 0.38623727\n",
      "Iteration 571, loss = 0.33994787\n",
      "Iteration 2190, loss = 0.17361100\n",
      "Iteration 514, loss = 0.33783929\n",
      "Iteration 2191, loss = 0.17341107\n",
      "Iteration 1477, loss = 0.19603614\n",
      "Iteration 968, loss = 0.30041191\n",
      "Iteration 2192, loss = 0.17329330\n",
      "Iteration 572, loss = 0.33997604\n",
      "Iteration 2193, loss = 0.17322869\n",
      "Iteration 515, loss = 0.33776069\n",
      "Iteration 2194, loss = 0.17312462\n",
      "Iteration 516, loss = 0.33766892\n",
      "Iteration 2195, loss = 0.17298916\n",
      "Iteration 1478, loss = 0.19554888\n",
      "Iteration 517, loss = 0.33758683\n",
      "Iteration 2196, loss = 0.17294186\n",
      "Iteration 176, loss = 0.38593664\n",
      "Iteration 2197, loss = 0.17281468\n",
      "Iteration 482, loss = 0.36968980\n",
      "Iteration 969, loss = 0.30033245\n",
      "Iteration 2198, loss = 0.17266737\n",
      "Iteration 518, loss = 0.33750719\n",
      "Iteration 1479, loss = 0.19557286\n",
      "Iteration 573, loss = 0.33958288\n",
      "Iteration 269, loss = 0.44463949\n",
      "Iteration 177, loss = 0.38558899\n",
      "Iteration 2199, loss = 0.17255451\n",
      "Iteration 970, loss = 0.30019554\n",
      "Iteration 519, loss = 0.33741126\n",
      "Iteration 2200, loss = 0.17249433\n",
      "Iteration 1480, loss = 0.19527771\n",
      "Iteration 520, loss = 0.33734941\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 178, loss = 0.38524756\n",
      "Iteration 1481, loss = 0.19499206\n",
      "Iteration 2201, loss = 0.17238169\n",
      "Iteration 971, loss = 0.30003438\n",
      "Iteration 1482, loss = 0.19485740\n",
      "Iteration 179, loss = 0.38495715\n",
      "Iteration 574, loss = 0.33942307\n",
      "Iteration 2202, loss = 0.17222881\n",
      "Iteration 1483, loss = 0.19462798\n",
      "Iteration 180, loss = 0.38461915\n",
      "Iteration 1, loss = 0.75972223\n",
      "Iteration 972, loss = 0.29998808\n",
      "Iteration 2, loss = 0.75734127\n",
      "Iteration 1484, loss = 0.19460255\n",
      "Iteration 3, loss = 0.75393672\n",
      "Iteration 2203, loss = 0.17213099\n",
      "Iteration 483, loss = 0.36961778\n",
      "Iteration 4, loss = 0.74984477\n",
      "Iteration 973, loss = 0.29986548\n",
      "Iteration 2204, loss = 0.17200114\n",
      "Iteration 181, loss = 0.38428316\n",
      "Iteration 575, loss = 0.33921147\n",
      "Iteration 1485, loss = 0.19438210\n",
      "Iteration 5, loss = 0.74536825\n",
      "Iteration 2205, loss = 0.17190975\n",
      "Iteration 182, loss = 0.38400561\n",
      "Iteration 2206, loss = 0.17182330\n",
      "Iteration 1486, loss = 0.19428998\n",
      "Iteration 6, loss = 0.74058442\n",
      "Iteration 1487, loss = 0.19404487\n",
      "Iteration 974, loss = 0.29972365\n",
      "Iteration 2207, loss = 0.17172679\n",
      "Iteration 7, loss = 0.73581838\n",
      "Iteration 1488, loss = 0.19383004\n",
      "Iteration 183, loss = 0.38369048\n",
      "Iteration 2208, loss = 0.17162835\n",
      "Iteration 2209, loss = 0.17144562\n",
      "Iteration 1489, loss = 0.19358404\n",
      "Iteration 184, loss = 0.38337086\n",
      "Iteration 2210, loss = 0.17145253\n",
      "Iteration 576, loss = 0.33904738\n",
      "Iteration 8, loss = 0.73117290\n",
      "Iteration 1490, loss = 0.19344512\n",
      "Iteration 484, loss = 0.36944990\n",
      "Iteration 975, loss = 0.29960621\n",
      "Iteration 2211, loss = 0.17128915\n",
      "Iteration 9, loss = 0.72666244\n",
      "Iteration 2212, loss = 0.17118665\n",
      "Iteration 185, loss = 0.38307267\n",
      "Iteration 976, loss = 0.29949592\n",
      "Iteration 2213, loss = 0.17106298\n",
      "Iteration 10, loss = 0.72212797\n",
      "Iteration 270, loss = 0.44433659\n",
      "Iteration 1491, loss = 0.19333775\n",
      "Iteration 11, loss = 0.71800537\n",
      "Iteration 12, loss = 0.71401957\n",
      "Iteration 1492, loss = 0.19311132\n",
      "Iteration 2214, loss = 0.17092569\n",
      "Iteration 186, loss = 0.38277179\n",
      "Iteration 13, loss = 0.70995283\n",
      "Iteration 977, loss = 0.29947828\n",
      "Iteration 14, loss = 0.70606159\n",
      "Iteration 2215, loss = 0.17083344\n",
      "Iteration 485, loss = 0.36931762\n",
      "Iteration 1493, loss = 0.19297047\n",
      "Iteration 2216, loss = 0.17071822\n",
      "Iteration 15, loss = 0.70250124\n",
      "Iteration 2217, loss = 0.17064363\n",
      "Iteration 1494, loss = 0.19279721\n",
      "Iteration 577, loss = 0.33897529\n",
      "Iteration 486, loss = 0.36914243\n",
      "Iteration 187, loss = 0.38247164\n",
      "Iteration 16, loss = 0.69904996\n",
      "Iteration 1495, loss = 0.19253344\n",
      "Iteration 17, loss = 0.69563114\n",
      "Iteration 271, loss = 0.44401909\n",
      "Iteration 188, loss = 0.38221239\n",
      "Iteration 487, loss = 0.36903935\n",
      "Iteration 18, loss = 0.69235485\n",
      "Iteration 1496, loss = 0.19252997\n",
      "Iteration 1497, loss = 0.19228751\n",
      "Iteration 578, loss = 0.33864054\n",
      "Iteration 1498, loss = 0.19198594\n",
      "Iteration 2218, loss = 0.17053593\n",
      "Iteration 19, loss = 0.68913886\n",
      "Iteration 978, loss = 0.29926141\n",
      "Iteration 20, loss = 0.68599421\n",
      "Iteration 1499, loss = 0.19187837\n",
      "Iteration 189, loss = 0.38188458\n",
      "Iteration 21, loss = 0.68293035\n",
      "Iteration 2219, loss = 0.17043286\n",
      "Iteration 579, loss = 0.33848348\n",
      "Iteration 979, loss = 0.29917266\n",
      "Iteration 272, loss = 0.44372786\n",
      "Iteration 1500, loss = 0.19165298\n",
      "Iteration 22, loss = 0.68002451\n",
      "Iteration 2220, loss = 0.17032199\n",
      "Iteration 980, loss = 0.29907788\n",
      "Iteration 23, loss = 0.67696138\n",
      "Iteration 580, loss = 0.33836230\n",
      "Iteration 488, loss = 0.36888697\n",
      "Iteration 2221, loss = 0.17023434\n",
      "Iteration 1501, loss = 0.19161827\n",
      "Iteration 190, loss = 0.38157272\n",
      "Iteration 981, loss = 0.29894247\n",
      "Iteration 24, loss = 0.67401986\n",
      "Iteration 1502, loss = 0.19134096\n",
      "Iteration 2222, loss = 0.17010269\n",
      "Iteration 982, loss = 0.29883329\n",
      "Iteration 25, loss = 0.67113822\n",
      "Iteration 2223, loss = 0.16996537\n",
      "Iteration 26, loss = 0.66828609\n",
      "Iteration 581, loss = 0.33817295\n",
      "Iteration 2224, loss = 0.17000837\n",
      "Iteration 191, loss = 0.38129810\n",
      "Iteration 1503, loss = 0.19130249\n",
      "Iteration 27, loss = 0.66534584\n",
      "Iteration 2225, loss = 0.16979879\n",
      "Iteration 28, loss = 0.66250527\n",
      "Iteration 2226, loss = 0.16966624\n",
      "Iteration 29, loss = 0.65967886\n",
      "Iteration 192, loss = 0.38099370\n",
      "Iteration 1504, loss = 0.19095870\n",
      "Iteration 30, loss = 0.65689315\n",
      "Iteration 2227, loss = 0.16957683\n",
      "Iteration 983, loss = 0.29875860\n",
      "Iteration 489, loss = 0.36871627\n",
      "Iteration 31, loss = 0.65400477\n",
      "Iteration 273, loss = 0.44344290\n",
      "Iteration 32, loss = 0.65132910\n",
      "Iteration 582, loss = 0.33794772\n",
      "Iteration 2228, loss = 0.16941353\n",
      "Iteration 1505, loss = 0.19096432\n",
      "Iteration 193, loss = 0.38073208\n",
      "Iteration 2229, loss = 0.16934881\n",
      "Iteration 1506, loss = 0.19065297\n",
      "Iteration 1507, loss = 0.19047239\n",
      "Iteration 33, loss = 0.64844874Iteration 194, loss = 0.38046062\n",
      "Iteration 2230, loss = 0.16925493\n",
      "\n",
      "Iteration 984, loss = 0.29865031\n",
      "Iteration 1508, loss = 0.19032864\n",
      "Iteration 490, loss = 0.36859449\n",
      "Iteration 195, loss = 0.38016721\n",
      "Iteration 2231, loss = 0.16911715\n",
      "Iteration 985, loss = 0.29848990\n",
      "Iteration 1509, loss = 0.19020139\n",
      "Iteration 2232, loss = 0.16910277\n",
      "Iteration 1510, loss = 0.19005535\n",
      "Iteration 986, loss = 0.29840205\n",
      "Iteration 34, loss = 0.64560848\n",
      "Iteration 583, loss = 0.33771583\n",
      "Iteration 196, loss = 0.37989203\n",
      "Iteration 274, loss = 0.44317649\n",
      "Iteration 1511, loss = 0.18984032\n",
      "Iteration 2233, loss = 0.16890432\n",
      "Iteration 35, loss = 0.64280403\n",
      "Iteration 987, loss = 0.29827135\n",
      "Iteration 197, loss = 0.37959854\n",
      "Iteration 36, loss = 0.63999623\n",
      "Iteration 2234, loss = 0.16879967\n",
      "Iteration 1512, loss = 0.18960530\n",
      "Iteration 988, loss = 0.29815981\n",
      "Iteration 37, loss = 0.63708346\n",
      "Iteration 198, loss = 0.37933063\n",
      "Iteration 2235, loss = 0.16871454\n",
      "Iteration 584, loss = 0.33756395\n",
      "Iteration 491, loss = 0.36842329\n",
      "Iteration 38, loss = 0.63427552\n",
      "Iteration 1513, loss = 0.18935289\n",
      "Iteration 2236, loss = 0.16861396\n",
      "Iteration 2237, loss = 0.16852585Iteration 1514, loss = 0.18925811\n",
      "\n",
      "Iteration 199, loss = 0.37908333\n",
      "Iteration 39, loss = 0.63144546\n",
      "Iteration 989, loss = 0.29808940\n",
      "Iteration 2238, loss = 0.16834728\n",
      "Iteration 40, loss = 0.62852389\n",
      "Iteration 2239, loss = 0.16828167\n",
      "Iteration 1515, loss = 0.18898701\n",
      "Iteration 41, loss = 0.62566077\n",
      "Iteration 200, loss = 0.37879925\n",
      "Iteration 42, loss = 0.62282482\n",
      "Iteration 2240, loss = 0.16815911\n",
      "Iteration 43, loss = 0.61996750Iteration 1516, loss = 0.18889723\n",
      "\n",
      "Iteration 585, loss = 0.33746258\n",
      "Iteration 275, loss = 0.44281172\n",
      "Iteration 492, loss = 0.36828161\n",
      "Iteration 990, loss = 0.29796352\n",
      "Iteration 2241, loss = 0.16806715\n",
      "Iteration 44, loss = 0.61703122\n",
      "Iteration 1517, loss = 0.18871740\n",
      "Iteration 2242, loss = 0.16795633\n",
      "Iteration 45, loss = 0.61417485\n",
      "Iteration 201, loss = 0.37852087\n",
      "Iteration 991, loss = 0.29783367\n",
      "Iteration 2243, loss = 0.16786226\n",
      "Iteration 1518, loss = 0.18851970\n",
      "Iteration 2244, loss = 0.16775213\n",
      "Iteration 202, loss = 0.37827837\n",
      "Iteration 2245, loss = 0.16766356\n",
      "Iteration 46, loss = 0.61126973\n",
      "Iteration 586, loss = 0.33718695\n",
      "Iteration 203, loss = 0.37801987\n",
      "Iteration 1519, loss = 0.18832182\n",
      "Iteration 2246, loss = 0.16755351Iteration 276, loss = 0.44250702\n",
      "\n",
      "Iteration 992, loss = 0.29770858\n",
      "Iteration 47, loss = 0.60836198\n",
      "Iteration 1520, loss = 0.18812298\n",
      "Iteration 493, loss = 0.36816266\n",
      "Iteration 204, loss = 0.37776125\n",
      "Iteration 587, loss = 0.33703248\n",
      "Iteration 2247, loss = 0.16749975\n",
      "Iteration 205, loss = 0.37751371\n",
      "Iteration 48, loss = 0.60549522\n",
      "Iteration 1521, loss = 0.18802183\n",
      "Iteration 49, loss = 0.60259540\n",
      "Iteration 1522, loss = 0.18776476\n",
      "Iteration 993, loss = 0.29765811\n",
      "Iteration 2248, loss = 0.16730943\n",
      "Iteration 494, loss = 0.36801863\n",
      "Iteration 1523, loss = 0.18757313\n",
      "Iteration 588, loss = 0.33679137\n",
      "Iteration 50, loss = 0.59972863\n",
      "Iteration 2249, loss = 0.16721570\n",
      "Iteration 206, loss = 0.37725092\n",
      "Iteration 2250, loss = 0.16713500\n",
      "Iteration 994, loss = 0.29751516\n",
      "Iteration 2251, loss = 0.16700319\n",
      "Iteration 51, loss = 0.59673018\n",
      "Iteration 277, loss = 0.44220519\n",
      "Iteration 2252, loss = 0.16689371\n",
      "Iteration 1524, loss = 0.18745144\n",
      "Iteration 52, loss = 0.59388268\n",
      "Iteration 589, loss = 0.33668806\n",
      "Iteration 2253, loss = 0.16680130\n",
      "Iteration 995, loss = 0.29736899\n",
      "Iteration 1525, loss = 0.18731310\n",
      "Iteration 207, loss = 0.37699243\n",
      "Iteration 53, loss = 0.59095486\n",
      "Iteration 54, loss = 0.58802251\n",
      "Iteration 2254, loss = 0.16670932\n",
      "Iteration 996, loss = 0.29728230\n",
      "Iteration 1526, loss = 0.18709252\n",
      "Iteration 2255, loss = 0.16654964\n",
      "Iteration 1527, loss = 0.18690393\n",
      "Iteration 55, loss = 0.58509193\n",
      "Iteration 2256, loss = 0.16650093\n",
      "Iteration 997, loss = 0.29721548\n",
      "Iteration 208, loss = 0.37674460\n",
      "Iteration 495, loss = 0.36785822\n",
      "Iteration 1528, loss = 0.18673397\n",
      "Iteration 2257, loss = 0.16637469\n",
      "Iteration 2258, loss = 0.16632388\n",
      "Iteration 56, loss = 0.58218634\n",
      "Iteration 590, loss = 0.33647048\n",
      "Iteration 1529, loss = 0.18647645\n",
      "Iteration 2259, loss = 0.16615651\n",
      "Iteration 278, loss = 0.44194342\n",
      "Iteration 57, loss = 0.57924860\n",
      "Iteration 998, loss = 0.29704536\n",
      "Iteration 2260, loss = 0.16607575\n",
      "Iteration 209, loss = 0.37648661\n",
      "Iteration 1530, loss = 0.18636691\n",
      "Iteration 58, loss = 0.57636665\n",
      "Iteration 2261, loss = 0.16594259\n",
      "Iteration 591, loss = 0.33627867\n",
      "Iteration 1531, loss = 0.18613598\n",
      "Iteration 59, loss = 0.57338646\n",
      "Iteration 2262, loss = 0.16586874\n",
      "Iteration 496, loss = 0.36779752\n",
      "Iteration 1532, loss = 0.18599325\n",
      "Iteration 2263, loss = 0.16578113\n",
      "Iteration 592, loss = 0.33607019\n",
      "Iteration 210, loss = 0.37625044\n",
      "Iteration 60, loss = 0.57050343\n",
      "Iteration 999, loss = 0.29698018\n",
      "Iteration 61, loss = 0.56755712\n",
      "Iteration 2264, loss = 0.16569026\n",
      "Iteration 1533, loss = 0.18583620\n",
      "Iteration 593, loss = 0.33587484\n",
      "Iteration 62, loss = 0.56472019\n",
      "Iteration 2265, loss = 0.16556254\n",
      "Iteration 1534, loss = 0.18562329\n",
      "Iteration 63, loss = 0.56185962\n",
      "Iteration 2266, loss = 0.16546642\n",
      "Iteration 211, loss = 0.37599973\n",
      "Iteration 64, loss = 0.55899661\n",
      "Iteration 1535, loss = 0.18550726\n",
      "Iteration 1000, loss = 0.29683992\n",
      "Iteration 1536, loss = 0.18527277\n",
      "Iteration 212, loss = 0.37578347\n",
      "Iteration 497, loss = 0.36761259\n",
      "Iteration 2267, loss = 0.16534647\n",
      "Iteration 65, loss = 0.55614456\n",
      "Iteration 279, loss = 0.44160036\n",
      "Iteration 2268, loss = 0.16522834\n",
      "Iteration 66, loss = 0.55331196\n",
      "Iteration 2269, loss = 0.16510256\n",
      "Iteration 594, loss = 0.33569592\n",
      "Iteration 1537, loss = 0.18513891\n",
      "Iteration 213, loss = 0.37555374\n",
      "Iteration 67, loss = 0.55053553\n",
      "Iteration 2270, loss = 0.16504229\n",
      "Iteration 214, loss = 0.37529529\n",
      "Iteration 1538, loss = 0.18489904\n",
      "Iteration 68, loss = 0.54763993\n",
      "Iteration 498, loss = 0.36742233\n",
      "Iteration 1001, loss = 0.29671549\n",
      "Iteration 1539, loss = 0.18477352\n",
      "Iteration 215, loss = 0.37505276\n",
      "Iteration 2271, loss = 0.16491167\n",
      "Iteration 1540, loss = 0.18454375\n",
      "Iteration 595, loss = 0.33555185\n",
      "Iteration 69, loss = 0.54488448\n",
      "Iteration 280, loss = 0.44128594\n",
      "Iteration 1541, loss = 0.18452921\n",
      "Iteration 1002, loss = 0.29662586\n",
      "Iteration 70, loss = 0.54218626\n",
      "Iteration 2272, loss = 0.16486756\n",
      "Iteration 499, loss = 0.36732085\n",
      "Iteration 1542, loss = 0.18431561\n",
      "Iteration 1543, loss = 0.18409307\n",
      "Iteration 71, loss = 0.53945778\n",
      "Iteration 2273, loss = 0.16475315Iteration 596, loss = 0.33543086\n",
      "\n",
      "Iteration 216, loss = 0.37481835\n",
      "Iteration 1544, loss = 0.18384272\n",
      "Iteration 1003, loss = 0.29651322\n",
      "Iteration 1545, loss = 0.18370266\n",
      "Iteration 72, loss = 0.53664009\n",
      "Iteration 2274, loss = 0.16467192\n",
      "Iteration 281, loss = 0.44101197\n",
      "Iteration 2275, loss = 0.16454485\n",
      "Iteration 1004, loss = 0.29644397\n",
      "Iteration 1546, loss = 0.18364769\n",
      "Iteration 500, loss = 0.36716199\n",
      "Iteration 217, loss = 0.37460059\n",
      "Iteration 597, loss = 0.33516036\n",
      "Iteration 2276, loss = 0.16437697\n",
      "Iteration 73, loss = 0.53397613\n",
      "Iteration 1547, loss = 0.18364147\n",
      "Iteration 1005, loss = 0.29629434\n",
      "Iteration 74, loss = 0.53135152\n",
      "Iteration 218, loss = 0.37435665\n",
      "Iteration 2277, loss = 0.16439168Iteration 1548, loss = 0.18332288\n",
      "\n",
      "Iteration 75, loss = 0.52867163\n",
      "Iteration 1006, loss = 0.29618371\n",
      "Iteration 219, loss = 0.37412147\n",
      "Iteration 2278, loss = 0.16417551\n",
      "Iteration 1549, loss = 0.18295998\n",
      "Iteration 598, loss = 0.33496315\n",
      "Iteration 76, loss = 0.52620805\n",
      "Iteration 77, loss = 0.52356252\n",
      "Iteration 1007, loss = 0.29607342\n",
      "Iteration 282, loss = 0.44069660\n",
      "Iteration 1550, loss = 0.18283404Iteration 2279, loss = 0.16407170\n",
      "\n",
      "Iteration 220, loss = 0.37390367\n",
      "Iteration 501, loss = 0.36701756\n",
      "Iteration 78, loss = 0.52108799\n",
      "Iteration 2280, loss = 0.16416529\n",
      "Iteration 1551, loss = 0.18280104\n",
      "Iteration 79, loss = 0.51860301\n",
      "Iteration 221, loss = 0.37368129\n",
      "Iteration 2281, loss = 0.16388486\n",
      "Iteration 599, loss = 0.33498299\n",
      "Iteration 2282, loss = 0.16379100\n",
      "Iteration 1008, loss = 0.29593691\n",
      "Iteration 1552, loss = 0.18242845\n",
      "Iteration 80, loss = 0.51625019\n",
      "Iteration 222, loss = 0.37345012\n",
      "Iteration 2283, loss = 0.16371842\n",
      "Iteration 1553, loss = 0.18234658\n",
      "Iteration 600, loss = 0.33462192\n",
      "Iteration 2284, loss = 0.16352328\n",
      "Iteration 283, loss = 0.44041503\n",
      "Iteration 81, loss = 0.51370409\n",
      "Iteration 82, loss = 0.51136489\n",
      "Iteration 1554, loss = 0.18212235\n",
      "Iteration 83, loss = 0.50893083\n",
      "Iteration 1009, loss = 0.29583903\n",
      "Iteration 1555, loss = 0.18193286\n",
      "Iteration 2285, loss = 0.16344923\n",
      "Iteration 502, loss = 0.36687538\n",
      "Iteration 223, loss = 0.37323168\n",
      "Iteration 601, loss = 0.33444586\n",
      "Iteration 1556, loss = 0.18176690\n",
      "Iteration 2286, loss = 0.16339411\n",
      "Iteration 84, loss = 0.50664898\n",
      "Iteration 85, loss = 0.50439603\n",
      "Iteration 1010, loss = 0.29573521\n",
      "Iteration 2287, loss = 0.16321262\n",
      "Iteration 86, loss = 0.50217965\n",
      "Iteration 224, loss = 0.37302040\n",
      "Iteration 87, loss = 0.49991510\n",
      "Iteration 1557, loss = 0.18161359\n",
      "Iteration 88, loss = 0.49781248\n",
      "Iteration 2288, loss = 0.16315472\n",
      "Iteration 225, loss = 0.37279064\n",
      "Iteration 1011, loss = 0.29565200\n",
      "Iteration 2289, loss = 0.16302414\n",
      "Iteration 602, loss = 0.33423499\n",
      "Iteration 89, loss = 0.49571438\n",
      "Iteration 284, loss = 0.44022468\n",
      "Iteration 1558, loss = 0.18144602\n",
      "Iteration 90, loss = 0.49356813\n",
      "Iteration 2290, loss = 0.16296959\n",
      "Iteration 503, loss = 0.36672401\n",
      "Iteration 1012, loss = 0.29550002\n",
      "Iteration 226, loss = 0.37257734\n",
      "Iteration 1559, loss = 0.18119262\n",
      "Iteration 91, loss = 0.49161806\n",
      "Iteration 2291, loss = 0.16282121\n",
      "Iteration 2292, loss = 0.16277575\n",
      "Iteration 92, loss = 0.48952838\n",
      "Iteration 2293, loss = 0.16263414\n",
      "Iteration 504, loss = 0.36658736\n",
      "Iteration 603, loss = 0.33410534\n",
      "Iteration 2294, loss = 0.16248252\n",
      "Iteration 1560, loss = 0.18115241\n",
      "Iteration 227, loss = 0.37235476\n",
      "Iteration 93, loss = 0.48752469\n",
      "Iteration 2295, loss = 0.16242802\n",
      "Iteration 1013, loss = 0.29542436\n",
      "Iteration 94, loss = 0.48557816\n",
      "Iteration 1561, loss = 0.18083409\n",
      "Iteration 228, loss = 0.37215687\n",
      "Iteration 604, loss = 0.33387633\n",
      "Iteration 95, loss = 0.48373592\n",
      "Iteration 2296, loss = 0.16227870\n",
      "Iteration 285, loss = 0.43986173\n",
      "Iteration 229, loss = 0.37194609\n",
      "Iteration 96, loss = 0.48190509Iteration 1562, loss = 0.18068197\n",
      "\n",
      "Iteration 230, loss = 0.37170114\n",
      "Iteration 2297, loss = 0.16226175\n",
      "Iteration 97, loss = 0.48006315\n",
      "Iteration 1563, loss = 0.18054258\n",
      "Iteration 98, loss = 0.47829479\n",
      "Iteration 1014, loss = 0.29526132\n",
      "Iteration 231, loss = 0.37148737\n",
      "Iteration 2298, loss = 0.16214741\n",
      "Iteration 99, loss = 0.47652431\n",
      "Iteration 605, loss = 0.33365915\n",
      "Iteration 100, loss = 0.47478423\n",
      "Iteration 505, loss = 0.36644188\n",
      "Iteration 1564, loss = 0.18057099\n",
      "Iteration 2299, loss = 0.16199721\n",
      "Iteration 232, loss = 0.37129610\n",
      "Iteration 101, loss = 0.47319828\n",
      "Iteration 1015, loss = 0.29526334\n",
      "Iteration 102, loss = 0.47148019\n",
      "Iteration 606, loss = 0.33352024\n",
      "Iteration 1565, loss = 0.18020631\n",
      "Iteration 2300, loss = 0.16191040\n",
      "Iteration 1016, loss = 0.29503943\n",
      "Iteration 2301, loss = 0.16185393\n",
      "Iteration 1566, loss = 0.18016311\n",
      "Iteration 2302, loss = 0.16190730\n",
      "Iteration 233, loss = 0.37107237\n",
      "Iteration 1017, loss = 0.29492650\n",
      "Iteration 103, loss = 0.46990705\n",
      "Iteration 607, loss = 0.33332685\n",
      "Iteration 506, loss = 0.36631150\n",
      "Iteration 104, loss = 0.46840811\n",
      "Iteration 2303, loss = 0.16155612\n",
      "Iteration 105, loss = 0.46678271\n",
      "Iteration 1018, loss = 0.29488618Iteration 1567, loss = 0.17982252\n",
      "\n",
      "Iteration 2304, loss = 0.16144371\n",
      "Iteration 106, loss = 0.46531491\n",
      "Iteration 286, loss = 0.43953127\n",
      "Iteration 1019, loss = 0.29472911\n",
      "Iteration 507, loss = 0.36617345\n",
      "Iteration 2305, loss = 0.16142255\n",
      "Iteration 1568, loss = 0.17959624\n",
      "Iteration 107, loss = 0.46374935Iteration 1020, loss = 0.29461047\n",
      "\n",
      "Iteration 234, loss = 0.37088783\n",
      "Iteration 2306, loss = 0.16132889\n",
      "Iteration 1569, loss = 0.17945069\n",
      "Iteration 1021, loss = 0.29448534\n",
      "Iteration 235, loss = 0.37065446\n",
      "Iteration 108, loss = 0.46239493\n",
      "Iteration 287, loss = 0.43923735\n",
      "Iteration 2307, loss = 0.16112446\n",
      "Iteration 1022, loss = 0.29440126\n",
      "Iteration 236, loss = 0.37049534\n",
      "Iteration 1570, loss = 0.17928254\n",
      "Iteration 109, loss = 0.46095471\n",
      "Iteration 608, loss = 0.33313861\n",
      "Iteration 1023, loss = 0.29433423\n",
      "Iteration 2308, loss = 0.16111026\n",
      "Iteration 237, loss = 0.37024791\n",
      "Iteration 110, loss = 0.45957014\n",
      "Iteration 1571, loss = 0.17910936\n",
      "Iteration 2309, loss = 0.16095298\n",
      "Iteration 288, loss = 0.43896691\n",
      "Iteration 2310, loss = 0.16090173\n",
      "Iteration 1024, loss = 0.29415337\n",
      "Iteration 111, loss = 0.45822907\n",
      "Iteration 238, loss = 0.37005318\n",
      "Iteration 2311, loss = 0.16078195\n",
      "Iteration 1572, loss = 0.17886110\n",
      "Iteration 609, loss = 0.33297595\n",
      "Iteration 2312, loss = 0.16067904\n",
      "Iteration 508, loss = 0.36601060\n",
      "Iteration 1025, loss = 0.29402863\n",
      "Iteration 112, loss = 0.45683053\n",
      "Iteration 2313, loss = 0.16057913\n",
      "Iteration 113, loss = 0.45550837\n",
      "Iteration 2314, loss = 0.16042747\n",
      "Iteration 1573, loss = 0.17878439\n",
      "Iteration 239, loss = 0.36985495\n",
      "Iteration 114, loss = 0.45439248\n",
      "Iteration 1026, loss = 0.29394379\n",
      "Iteration 1574, loss = 0.17864532\n",
      "Iteration 115, loss = 0.45302436\n",
      "Iteration 2315, loss = 0.16033510\n",
      "Iteration 509, loss = 0.36590941\n",
      "Iteration 116, loss = 0.45188577\n",
      "Iteration 117, loss = 0.45064045\n",
      "Iteration 2316, loss = 0.16033784\n",
      "Iteration 1027, loss = 0.29379611\n",
      "Iteration 240, loss = 0.36963646\n",
      "Iteration 118, loss = 0.44947377\n",
      "Iteration 1575, loss = 0.17837590\n",
      "Iteration 610, loss = 0.33276110\n",
      "Iteration 510, loss = 0.36574600\n",
      "Iteration 119, loss = 0.44834319\n",
      "Iteration 1028, loss = 0.29367277\n",
      "Iteration 2317, loss = 0.16020607\n",
      "Iteration 120, loss = 0.44725383\n",
      "Iteration 1576, loss = 0.17813637\n",
      "Iteration 2318, loss = 0.16004320\n",
      "Iteration 121, loss = 0.44612959\n",
      "Iteration 241, loss = 0.36945157\n",
      "Iteration 122, loss = 0.44507462\n",
      "Iteration 511, loss = 0.36562282\n",
      "Iteration 2319, loss = 0.15999676\n",
      "Iteration 1577, loss = 0.17795376\n",
      "Iteration 1029, loss = 0.29361821\n",
      "Iteration 123, loss = 0.44406238\n",
      "Iteration 611, loss = 0.33263315\n",
      "Iteration 242, loss = 0.36926866\n",
      "Iteration 124, loss = 0.44304382\n",
      "Iteration 2320, loss = 0.15983814\n",
      "Iteration 1030, loss = 0.29348727\n",
      "Iteration 1578, loss = 0.17782837\n",
      "Iteration 289, loss = 0.43870666\n",
      "Iteration 243, loss = 0.36904152\n",
      "Iteration 612, loss = 0.33238432\n",
      "Iteration 125, loss = 0.44208180\n",
      "Iteration 2321, loss = 0.15968289\n",
      "Iteration 1579, loss = 0.17760713\n",
      "Iteration 244, loss = 0.36886995\n",
      "Iteration 512, loss = 0.36549863\n",
      "Iteration 1031, loss = 0.29333737\n",
      "Iteration 1580, loss = 0.17748972\n",
      "Iteration 2322, loss = 0.15959858\n",
      "Iteration 613, loss = 0.33216634\n",
      "Iteration 245, loss = 0.36867774\n",
      "Iteration 126, loss = 0.44100255\n",
      "Iteration 1581, loss = 0.17729776\n",
      "Iteration 2323, loss = 0.15950182Iteration 127, loss = 0.44002103\n",
      "\n",
      "Iteration 1032, loss = 0.29326681\n",
      "Iteration 513, loss = 0.36534893\n",
      "Iteration 1582, loss = 0.17707350\n",
      "Iteration 128, loss = 0.43912108\n",
      "Iteration 246, loss = 0.36846487\n",
      "Iteration 614, loss = 0.33199773\n",
      "Iteration 1033, loss = 0.29313448\n",
      "Iteration 129, loss = 0.43816854\n",
      "Iteration 2324, loss = 0.15934720\n",
      "Iteration 514, loss = 0.36522536\n",
      "Iteration 2325, loss = 0.15931388\n",
      "Iteration 615, loss = 0.33181293\n",
      "Iteration 130, loss = 0.43727990\n",
      "Iteration 1034, loss = 0.29302658\n",
      "Iteration 247, loss = 0.36828413Iteration 2326, loss = 0.15913243\n",
      "\n",
      "Iteration 1583, loss = 0.17701559\n",
      "Iteration 131, loss = 0.43641303\n",
      "Iteration 132, loss = 0.43551624\n",
      "Iteration 1035, loss = 0.29287872\n",
      "Iteration 248, loss = 0.36808075\n",
      "Iteration 133, loss = 0.43465718\n",
      "Iteration 290, loss = 0.43835733\n",
      "Iteration 134, loss = 0.43382822\n",
      "Iteration 1584, loss = 0.17687190\n",
      "Iteration 249, loss = 0.36789107\n",
      "Iteration 135, loss = 0.43300905\n",
      "Iteration 2327, loss = 0.15908424\n",
      "Iteration 616, loss = 0.33173420\n",
      "Iteration 136, loss = 0.43220171\n",
      "Iteration 1585, loss = 0.17666888\n",
      "Iteration 250, loss = 0.36770344\n",
      "Iteration 1036, loss = 0.29279878\n",
      "Iteration 2328, loss = 0.15893952\n",
      "Iteration 137, loss = 0.43140975\n",
      "Iteration 1586, loss = 0.17641698\n",
      "Iteration 138, loss = 0.43060345\n",
      "Iteration 251, loss = 0.36752137\n",
      "Iteration 2329, loss = 0.15888829\n",
      "Iteration 139, loss = 0.42991275\n",
      "Iteration 617, loss = 0.33146100\n",
      "Iteration 515, loss = 0.36507441\n",
      "Iteration 140, loss = 0.42909560\n",
      "Iteration 1587, loss = 0.17623954\n",
      "Iteration 252, loss = 0.36733681\n",
      "Iteration 141, loss = 0.42839250\n",
      "Iteration 2330, loss = 0.15884190\n",
      "Iteration 142, loss = 0.42760037\n",
      "Iteration 2331, loss = 0.15862814\n",
      "Iteration 1037, loss = 0.29264544\n",
      "Iteration 2332, loss = 0.15858219\n",
      "Iteration 1588, loss = 0.17598795\n",
      "Iteration 143, loss = 0.42691874\n",
      "Iteration 2333, loss = 0.15849739\n",
      "Iteration 291, loss = 0.43808752\n",
      "Iteration 1589, loss = 0.17582615\n",
      "Iteration 253, loss = 0.36715303\n",
      "Iteration 2334, loss = 0.15835685\n",
      "Iteration 144, loss = 0.42619495\n",
      "Iteration 2335, loss = 0.15823051\n",
      "Iteration 1590, loss = 0.17562441\n",
      "Iteration 145, loss = 0.42551075\n",
      "Iteration 2336, loss = 0.15805883\n",
      "Iteration 1591, loss = 0.17550964\n",
      "Iteration 146, loss = 0.42482186\n",
      "Iteration 1038, loss = 0.29253182\n",
      "Iteration 254, loss = 0.36697439\n",
      "Iteration 1592, loss = 0.17531162\n",
      "Iteration 516, loss = 0.36491441\n",
      "Iteration 1039, loss = 0.29244397\n",
      "Iteration 255, loss = 0.36678614\n",
      "Iteration 1593, loss = 0.17524635\n",
      "Iteration 2337, loss = 0.15800148\n",
      "Iteration 256, loss = 0.36660669\n",
      "Iteration 1040, loss = 0.29235750\n",
      "Iteration 292, loss = 0.43780147\n",
      "Iteration 517, loss = 0.36477407\n",
      "Iteration 2338, loss = 0.15787396\n",
      "Iteration 1594, loss = 0.17502412\n",
      "Iteration 257, loss = 0.36641566\n",
      "Iteration 1041, loss = 0.29216946\n",
      "Iteration 258, loss = 0.36628015\n",
      "Iteration 147, loss = 0.42414488\n",
      "Iteration 1595, loss = 0.17474029\n",
      "Iteration 618, loss = 0.33127196\n",
      "Iteration 148, loss = 0.42342986\n",
      "Iteration 149, loss = 0.42279615\n",
      "Iteration 2339, loss = 0.15779863\n",
      "Iteration 150, loss = 0.42210640\n",
      "Iteration 1042, loss = 0.29208136\n",
      "Iteration 619, loss = 0.33114600\n",
      "Iteration 151, loss = 0.42152364\n",
      "Iteration 1596, loss = 0.17492359\n",
      "Iteration 152, loss = 0.42086748\n",
      "Iteration 2340, loss = 0.15768546\n",
      "Iteration 518, loss = 0.36462356\n",
      "Iteration 153, loss = 0.42021205\n",
      "Iteration 1597, loss = 0.17441641\n",
      "Iteration 154, loss = 0.41962048\n",
      "Iteration 293, loss = 0.43754027\n",
      "Iteration 1043, loss = 0.29197873\n",
      "Iteration 1598, loss = 0.17420907\n",
      "Iteration 2341, loss = 0.15766961\n",
      "Iteration 620, loss = 0.33093010\n",
      "Iteration 155, loss = 0.41903923\n",
      "Iteration 156, loss = 0.41841235\n",
      "Iteration 2342, loss = 0.15760814\n",
      "Iteration 1599, loss = 0.17404804\n",
      "Iteration 621, loss = 0.33070262\n",
      "Iteration 1044, loss = 0.29185861\n",
      "Iteration 157, loss = 0.41780512\n",
      "Iteration 2343, loss = 0.15732422\n",
      "Iteration 158, loss = 0.41722163\n",
      "Iteration 2344, loss = 0.15720365\n",
      "Iteration 259, loss = 0.36607770\n",
      "Iteration 622, loss = 0.33055386\n",
      "Iteration 1600, loss = 0.17394797\n",
      "Iteration 2345, loss = 0.15718086\n",
      "Iteration 519, loss = 0.36454219\n",
      "Iteration 1045, loss = 0.29177506\n",
      "Iteration 159, loss = 0.41666768\n",
      "Iteration 260, loss = 0.36589070\n",
      "Iteration 623, loss = 0.33033788\n",
      "Iteration 294, loss = 0.43725292\n",
      "Iteration 1046, loss = 0.29170441\n",
      "Iteration 261, loss = 0.36570972\n",
      "Iteration 1047, loss = 0.29155395\n",
      "Iteration 520, loss = 0.36439238\n",
      "Iteration 1601, loss = 0.17369554\n",
      "Iteration 262, loss = 0.36556992\n",
      "Iteration 624, loss = 0.33018162\n",
      "Iteration 1048, loss = 0.29147040\n",
      "Iteration 160, loss = 0.41608416\n",
      "Iteration 263, loss = 0.36536874\n",
      "Iteration 1602, loss = 0.17345650\n",
      "Iteration 2346, loss = 0.15706754\n",
      "Iteration 1049, loss = 0.29129416\n",
      "Iteration 161, loss = 0.41551377\n",
      "Iteration 1603, loss = 0.17329638\n",
      "Iteration 2347, loss = 0.15692942\n",
      "Iteration 1604, loss = 0.17311095\n",
      "Iteration 162, loss = 0.41492504\n",
      "Iteration 1605, loss = 0.17296274\n",
      "Iteration 2348, loss = 0.15688915\n",
      "Iteration 1050, loss = 0.29116406\n",
      "Iteration 163, loss = 0.41438480\n",
      "Iteration 1606, loss = 0.17288884\n",
      "Iteration 264, loss = 0.36519497\n",
      "Iteration 625, loss = 0.32998734\n",
      "Iteration 521, loss = 0.36425419\n",
      "Iteration 2349, loss = 0.15676958\n",
      "Iteration 164, loss = 0.41386032\n",
      "Iteration 2350, loss = 0.15659406\n",
      "Iteration 165, loss = 0.41330000\n",
      "Iteration 626, loss = 0.32978518\n",
      "Iteration 2351, loss = 0.15647006\n",
      "Iteration 166, loss = 0.41278692\n",
      "Iteration 1607, loss = 0.17260665\n",
      "Iteration 167, loss = 0.41227229\n",
      "Iteration 2352, loss = 0.15650984\n",
      "Iteration 1051, loss = 0.29105261\n",
      "Iteration 2353, loss = 0.15629333\n",
      "Iteration 168, loss = 0.41171571\n",
      "Iteration 1608, loss = 0.17248472\n",
      "Iteration 2354, loss = 0.15613703\n",
      "Iteration 265, loss = 0.36502805\n",
      "Iteration 1609, loss = 0.17223870\n",
      "Iteration 2355, loss = 0.15604501\n",
      "Iteration 169, loss = 0.41120302\n",
      "Iteration 1610, loss = 0.17224203\n",
      "Iteration 170, loss = 0.41071965\n",
      "Iteration 1611, loss = 0.17193584\n",
      "Iteration 2356, loss = 0.15594128\n",
      "Iteration 2357, loss = 0.15605367\n",
      "Iteration 266, loss = 0.36485591\n",
      "Iteration 1052, loss = 0.29092430\n",
      "Iteration 522, loss = 0.36409026\n",
      "Iteration 171, loss = 0.41014864\n",
      "Iteration 295, loss = 0.43700176\n",
      "Iteration 172, loss = 0.40965576\n",
      "Iteration 173, loss = 0.40917381\n",
      "Iteration 1612, loss = 0.17173291\n",
      "Iteration 1053, loss = 0.29083330\n",
      "Iteration 627, loss = 0.32958723\n",
      "Iteration 174, loss = 0.40867026\n",
      "Iteration 1613, loss = 0.17170131\n",
      "Iteration 1614, loss = 0.17142326\n",
      "Iteration 175, loss = 0.40815119\n",
      "Iteration 1615, loss = 0.17127789\n",
      "Iteration 176, loss = 0.40768626\n",
      "Iteration 1616, loss = 0.17107708\n",
      "Iteration 1054, loss = 0.29071052\n",
      "Iteration 1617, loss = 0.17089346\n",
      "Iteration 177, loss = 0.40716555\n",
      "Iteration 523, loss = 0.36397804\n",
      "Iteration 628, loss = 0.32940291\n",
      "Iteration 296, loss = 0.43668696\n",
      "Iteration 1055, loss = 0.29058468\n",
      "Iteration 178, loss = 0.40674307\n",
      "Iteration 1618, loss = 0.17094334\n",
      "Iteration 179, loss = 0.40625119\n",
      "Iteration 1619, loss = 0.17049034\n",
      "Iteration 629, loss = 0.32920649\n",
      "Iteration 180, loss = 0.40573537\n",
      "Iteration 181, loss = 0.40532348\n",
      "Iteration 1056, loss = 0.29050046\n",
      "Iteration 1620, loss = 0.17032812\n",
      "Iteration 297, loss = 0.43643412\n",
      "Iteration 182, loss = 0.40483651\n",
      "Iteration 524, loss = 0.36381366\n",
      "Iteration 1057, loss = 0.29035296\n",
      "Iteration 630, loss = 0.32904643\n",
      "Iteration 1621, loss = 0.17017247Iteration 183, loss = 0.40440612\n",
      "\n",
      "Iteration 184, loss = 0.40391534\n",
      "Iteration 185, loss = 0.40346205\n",
      "Iteration 1622, loss = 0.16993828\n",
      "Iteration 186, loss = 0.40303674\n",
      "Iteration 1058, loss = 0.29023716\n",
      "Iteration 187, loss = 0.40256560\n",
      "Iteration 525, loss = 0.36368917\n",
      "Iteration 188, loss = 0.40217570\n",
      "Iteration 1623, loss = 0.17009218\n",
      "Iteration 631, loss = 0.32886015\n",
      "Iteration 298, loss = 0.43611085\n",
      "Iteration 1059, loss = 0.29013266\n",
      "Iteration 189, loss = 0.40171840\n",
      "Iteration 1624, loss = 0.16960786\n",
      "Iteration 190, loss = 0.40126357\n",
      "Iteration 1625, loss = 0.16955986Iteration 191, loss = 0.40084072\n",
      "\n",
      "Iteration 526, loss = 0.36353891\n",
      "Iteration 1060, loss = 0.29001510\n",
      "Iteration 192, loss = 0.40038504\n",
      "Iteration 632, loss = 0.32866081\n",
      "Iteration 1626, loss = 0.16923824\n",
      "Iteration 193, loss = 0.39998700\n",
      "Iteration 194, loss = 0.39952354\n",
      "Iteration 633, loss = 0.32872292\n",
      "Iteration 1627, loss = 0.16912976\n",
      "Iteration 1061, loss = 0.28990104\n",
      "Iteration 195, loss = 0.39914079\n",
      "Iteration 1628, loss = 0.16896911\n",
      "Iteration 196, loss = 0.39868597\n",
      "Iteration 299, loss = 0.43590383\n",
      "Iteration 1629, loss = 0.16879829\n",
      "Iteration 197, loss = 0.39830142\n",
      "Iteration 1062, loss = 0.28979620\n",
      "Iteration 1630, loss = 0.16872645\n",
      "Iteration 2358, loss = 0.15583891\n",
      "Iteration 527, loss = 0.36340156\n",
      "Iteration 1631, loss = 0.16839453\n",
      "Iteration 1063, loss = 0.28965238\n",
      "Iteration 1632, loss = 0.16831208\n",
      "Iteration 1633, loss = 0.16797665\n",
      "Iteration 1064, loss = 0.28952920\n",
      "Iteration 198, loss = 0.39788425\n",
      "Iteration 2359, loss = 0.15561815\n",
      "Iteration 199, loss = 0.39747272\n",
      "Iteration 2360, loss = 0.15574737\n",
      "Iteration 200, loss = 0.39706108\n",
      "Iteration 634, loss = 0.32828604\n",
      "Iteration 1065, loss = 0.28942077\n",
      "Iteration 1634, loss = 0.16790750\n",
      "Iteration 201, loss = 0.39665880\n",
      "Iteration 202, loss = 0.39628180\n",
      "Iteration 1066, loss = 0.28933240\n",
      "Iteration 203, loss = 0.39584999\n",
      "Iteration 1635, loss = 0.16775542\n",
      "Iteration 635, loss = 0.32814038\n",
      "Iteration 1636, loss = 0.16748685\n",
      "Iteration 528, loss = 0.36326873\n",
      "Iteration 204, loss = 0.39546252\n",
      "Iteration 1637, loss = 0.16741609\n",
      "Iteration 1067, loss = 0.28920526\n",
      "Iteration 1638, loss = 0.16737401\n",
      "Iteration 300, loss = 0.43555077\n",
      "Iteration 205, loss = 0.39509672\n",
      "Iteration 636, loss = 0.32789954\n",
      "Iteration 206, loss = 0.39468593\n",
      "Iteration 207, loss = 0.39431680\n",
      "Iteration 208, loss = 0.39392041\n",
      "Iteration 1068, loss = 0.28906042\n",
      "Iteration 1639, loss = 0.16710626\n",
      "Iteration 637, loss = 0.32777124\n",
      "Iteration 1640, loss = 0.16685484\n",
      "Iteration 209, loss = 0.39352824\n",
      "Iteration 1641, loss = 0.16671317\n",
      "Iteration 1069, loss = 0.28892763\n",
      "Iteration 529, loss = 0.36313111\n",
      "Iteration 210, loss = 0.39324421\n",
      "Iteration 211, loss = 0.39279387\n",
      "Iteration 301, loss = 0.43525622\n",
      "Iteration 1642, loss = 0.16641132\n",
      "Iteration 1070, loss = 0.28881635\n",
      "Iteration 1643, loss = 0.16619521\n",
      "Iteration 212, loss = 0.39241990\n",
      "Iteration 638, loss = 0.32754000\n",
      "Iteration 1644, loss = 0.16602610\n",
      "Iteration 1071, loss = 0.28876600\n",
      "Iteration 213, loss = 0.39206198\n",
      "Iteration 530, loss = 0.36306085\n",
      "Iteration 1645, loss = 0.16589982\n",
      "Iteration 639, loss = 0.32737060\n",
      "Iteration 214, loss = 0.39169496\n",
      "Iteration 1072, loss = 0.28858866\n",
      "Iteration 302, loss = 0.43499759\n",
      "Iteration 1646, loss = 0.16573871\n",
      "Iteration 215, loss = 0.39131454\n",
      "Iteration 1647, loss = 0.16547119\n",
      "Iteration 216, loss = 0.39095707\n",
      "Iteration 1073, loss = 0.28846375\n",
      "Iteration 640, loss = 0.32718211\n",
      "Iteration 531, loss = 0.36286212\n",
      "Iteration 1648, loss = 0.16536632\n",
      "Iteration 217, loss = 0.39059003\n",
      "Iteration 1074, loss = 0.28838625\n",
      "Iteration 267, loss = 0.36467851\n",
      "Iteration 641, loss = 0.32701536\n",
      "Iteration 2361, loss = 0.15546867\n",
      "Iteration 1649, loss = 0.16516912\n",
      "Iteration 218, loss = 0.39024691\n",
      "Iteration 1075, loss = 0.28824616\n",
      "Iteration 219, loss = 0.38988455\n",
      "Iteration 1650, loss = 0.16497418\n",
      "Iteration 220, loss = 0.38953314\n",
      "Iteration 642, loss = 0.32686051\n",
      "Iteration 303, loss = 0.43470617\n",
      "Iteration 221, loss = 0.38924005\n",
      "Iteration 1076, loss = 0.28811655\n",
      "Iteration 1651, loss = 0.16483382\n",
      "Iteration 532, loss = 0.36275506\n",
      "Iteration 222, loss = 0.38884071\n",
      "Iteration 223, loss = 0.38849130\n",
      "Iteration 224, loss = 0.38816580\n",
      "Iteration 1652, loss = 0.16470270\n",
      "Iteration 643, loss = 0.32658086\n",
      "Iteration 225, loss = 0.38781177\n",
      "Iteration 1653, loss = 0.16449006\n",
      "Iteration 1077, loss = 0.28805522\n",
      "Iteration 2362, loss = 0.15538390\n",
      "Iteration 226, loss = 0.38745989\n",
      "Iteration 644, loss = 0.32640931\n",
      "Iteration 1654, loss = 0.16430652\n",
      "Iteration 2363, loss = 0.15520046\n",
      "Iteration 304, loss = 0.43447332\n",
      "Iteration 533, loss = 0.36260317\n",
      "Iteration 227, loss = 0.38715390\n",
      "Iteration 2364, loss = 0.15520584\n",
      "Iteration 228, loss = 0.38678797\n",
      "Iteration 1655, loss = 0.16408070Iteration 1078, loss = 0.28790071\n",
      "Iteration 2365, loss = 0.15505378\n",
      "Iteration 645, loss = 0.32628882\n",
      "Iteration 268, loss = 0.36453528\n",
      "\n",
      "Iteration 229, loss = 0.38646937\n",
      "Iteration 230, loss = 0.38617666\n",
      "Iteration 1656, loss = 0.16392504\n",
      "Iteration 231, loss = 0.38584125\n",
      "Iteration 1657, loss = 0.16374217\n",
      "Iteration 305, loss = 0.43418480\n",
      "Iteration 2366, loss = 0.15489726\n",
      "Iteration 534, loss = 0.36252589\n",
      "Iteration 232, loss = 0.38552348\n",
      "Iteration 269, loss = 0.36435568\n",
      "Iteration 2367, loss = 0.15486739\n",
      "Iteration 1658, loss = 0.16359112\n",
      "Iteration 2368, loss = 0.15476135\n",
      "Iteration 646, loss = 0.32610219\n",
      "Iteration 270, loss = 0.36419001\n",
      "Iteration 233, loss = 0.38520464\n",
      "Iteration 1079, loss = 0.28782450\n",
      "Iteration 2369, loss = 0.15462839\n",
      "Iteration 234, loss = 0.38489032\n",
      "Iteration 235, loss = 0.38453496\n",
      "Iteration 1080, loss = 0.28766389\n",
      "Iteration 236, loss = 0.38424837\n",
      "Iteration 1659, loss = 0.16339319\n",
      "Iteration 2370, loss = 0.15452153\n",
      "Iteration 271, loss = 0.36401169\n",
      "Iteration 237, loss = 0.38393646\n",
      "Iteration 647, loss = 0.32585265\n",
      "Iteration 1660, loss = 0.16335077\n",
      "Iteration 238, loss = 0.38361826\n",
      "Iteration 2371, loss = 0.15436289\n",
      "Iteration 1661, loss = 0.16301563\n",
      "Iteration 2372, loss = 0.15424781\n",
      "Iteration 1081, loss = 0.28753171\n",
      "Iteration 239, loss = 0.38334312\n",
      "Iteration 272, loss = 0.36386330\n",
      "Iteration 2373, loss = 0.15421050\n",
      "Iteration 535, loss = 0.36232363\n",
      "Iteration 1662, loss = 0.16299884\n",
      "Iteration 2374, loss = 0.15404584\n",
      "Iteration 240, loss = 0.38301458\n",
      "Iteration 273, loss = 0.36368283\n",
      "Iteration 648, loss = 0.32570155\n",
      "Iteration 241, loss = 0.38270955\n",
      "Iteration 1663, loss = 0.16269458\n",
      "Iteration 306, loss = 0.43399489\n",
      "Iteration 2375, loss = 0.15395596\n",
      "Iteration 2376, loss = 0.15388832\n",
      "Iteration 1082, loss = 0.28746810\n",
      "Iteration 274, loss = 0.36352048\n",
      "Iteration 1664, loss = 0.16248565\n",
      "Iteration 242, loss = 0.38243008\n",
      "Iteration 649, loss = 0.32549810\n",
      "Iteration 2377, loss = 0.15383317\n",
      "Iteration 243, loss = 0.38212698\n",
      "Iteration 1083, loss = 0.28736215\n",
      "Iteration 1665, loss = 0.16238383\n",
      "Iteration 244, loss = 0.38183504\n",
      "Iteration 275, loss = 0.36337902\n",
      "Iteration 536, loss = 0.36222861\n",
      "Iteration 2378, loss = 0.15372565\n",
      "Iteration 1084, loss = 0.28718794\n",
      "Iteration 650, loss = 0.32537841\n",
      "Iteration 1666, loss = 0.16223780\n",
      "Iteration 276, loss = 0.36319441\n",
      "Iteration 2379, loss = 0.15353125\n",
      "Iteration 2380, loss = 0.15350605\n",
      "Iteration 245, loss = 0.38157043\n",
      "Iteration 277, loss = 0.36303284\n",
      "Iteration 1667, loss = 0.16218479\n",
      "Iteration 307, loss = 0.43363617\n",
      "Iteration 246, loss = 0.38125214\n",
      "Iteration 1085, loss = 0.28713010\n",
      "Iteration 247, loss = 0.38096588\n",
      "Iteration 1668, loss = 0.16186024\n",
      "Iteration 2381, loss = 0.15331596\n",
      "Iteration 278, loss = 0.36288045\n",
      "Iteration 248, loss = 0.38069900\n",
      "Iteration 1669, loss = 0.16162526\n",
      "Iteration 2382, loss = 0.15323407Iteration 249, loss = 0.38044384\n",
      "\n",
      "Iteration 537, loss = 0.36205239\n",
      "Iteration 279, loss = 0.36275117\n",
      "Iteration 1670, loss = 0.16149272\n",
      "Iteration 2383, loss = 0.15321247\n",
      "Iteration 250, loss = 0.38013197\n",
      "Iteration 1671, loss = 0.16135195\n",
      "Iteration 251, loss = 0.37984793\n",
      "Iteration 2384, loss = 0.15307103\n",
      "Iteration 1086, loss = 0.28695992\n",
      "Iteration 1672, loss = 0.16113853\n",
      "Iteration 651, loss = 0.32512773\n",
      "Iteration 1673, loss = 0.16107599\n",
      "Iteration 2385, loss = 0.15300038\n",
      "Iteration 252, loss = 0.37956532\n",
      "Iteration 1087, loss = 0.28683391\n",
      "Iteration 538, loss = 0.36202136\n",
      "Iteration 253, loss = 0.37928162\n",
      "Iteration 280, loss = 0.36257555\n",
      "Iteration 2386, loss = 0.15285965\n",
      "Iteration 254, loss = 0.37902646\n",
      "Iteration 1088, loss = 0.28672091\n",
      "Iteration 255, loss = 0.37872741\n",
      "Iteration 2387, loss = 0.15268501\n",
      "Iteration 1674, loss = 0.16093494\n",
      "Iteration 256, loss = 0.37846440\n",
      "Iteration 2388, loss = 0.15279565\n",
      "Iteration 652, loss = 0.32496623\n",
      "Iteration 257, loss = 0.37819238\n",
      "Iteration 1089, loss = 0.28662247\n",
      "Iteration 2389, loss = 0.15257514\n",
      "Iteration 308, loss = 0.43334548\n",
      "Iteration 258, loss = 0.37796069\n",
      "Iteration 2390, loss = 0.15241192\n",
      "Iteration 259, loss = 0.37767295\n",
      "Iteration 281, loss = 0.36243271\n",
      "Iteration 260, loss = 0.37740937\n",
      "Iteration 2391, loss = 0.15229676\n",
      "Iteration 1090, loss = 0.28646680\n",
      "Iteration 261, loss = 0.37712439\n",
      "Iteration 1675, loss = 0.16069171\n",
      "Iteration 262, loss = 0.37688366\n",
      "Iteration 1091, loss = 0.28642722\n",
      "Iteration 263, loss = 0.37663189\n",
      "Iteration 2392, loss = 0.15236023\n",
      "Iteration 264, loss = 0.37636517\n",
      "Iteration 282, loss = 0.36223766\n",
      "Iteration 1676, loss = 0.16049383\n",
      "Iteration 309, loss = 0.43322072\n",
      "Iteration 265, loss = 0.37610478\n",
      "Iteration 2393, loss = 0.15211800\n",
      "Iteration 539, loss = 0.36178491\n",
      "Iteration 653, loss = 0.32481209\n",
      "Iteration 266, loss = 0.37583934\n",
      "Iteration 267, loss = 0.37558125\n",
      "Iteration 2394, loss = 0.15204375\n",
      "Iteration 268, loss = 0.37537951\n",
      "Iteration 1677, loss = 0.16025411\n",
      "Iteration 1092, loss = 0.28623910\n",
      "Iteration 269, loss = 0.37506913\n",
      "Iteration 1678, loss = 0.16014600\n",
      "Iteration 283, loss = 0.36209226\n",
      "Iteration 270, loss = 0.37483073\n",
      "Iteration 2395, loss = 0.15193306\n",
      "Iteration 1093, loss = 0.28614991\n",
      "Iteration 654, loss = 0.32457037\n",
      "Iteration 1679, loss = 0.16003268\n",
      "Iteration 2396, loss = 0.15177921\n",
      "Iteration 284, loss = 0.36193342\n",
      "Iteration 1094, loss = 0.28603055\n",
      "Iteration 2397, loss = 0.15168661\n",
      "Iteration 1680, loss = 0.15981768\n",
      "Iteration 540, loss = 0.36165735\n",
      "Iteration 271, loss = 0.37458449\n",
      "Iteration 285, loss = 0.36181128\n",
      "Iteration 1095, loss = 0.28593996\n",
      "Iteration 286, loss = 0.36163547\n",
      "Iteration 272, loss = 0.37432962\n",
      "Iteration 2398, loss = 0.15169254\n",
      "Iteration 1096, loss = 0.28577465\n",
      "Iteration 1681, loss = 0.15982028\n",
      "Iteration 310, loss = 0.43283173\n",
      "Iteration 655, loss = 0.32437639\n",
      "Iteration 273, loss = 0.37409588\n",
      "Iteration 2399, loss = 0.15149196\n",
      "Iteration 1097, loss = 0.28575925\n",
      "Iteration 287, loss = 0.36149625\n",
      "Iteration 1682, loss = 0.15955300\n",
      "Iteration 274, loss = 0.37386496\n",
      "Iteration 2400, loss = 0.15144299\n",
      "Iteration 275, loss = 0.37361644\n",
      "Iteration 1683, loss = 0.15921749\n",
      "Iteration 541, loss = 0.36152251\n",
      "Iteration 656, loss = 0.32422112\n",
      "Iteration 1684, loss = 0.15940247\n",
      "Iteration 276, loss = 0.37335296\n",
      "Iteration 542, loss = 0.36139002\n",
      "Iteration 288, loss = 0.36132519\n",
      "Iteration 277, loss = 0.37310931\n",
      "Iteration 2401, loss = 0.15133829Iteration 1685, loss = 0.15893398\n",
      "\n",
      "Iteration 2402, loss = 0.15119955\n",
      "Iteration 1098, loss = 0.28561180\n",
      "Iteration 2403, loss = 0.15114818\n",
      "Iteration 311, loss = 0.43252540\n",
      "Iteration 278, loss = 0.37289065\n",
      "Iteration 279, loss = 0.37264555\n",
      "Iteration 1686, loss = 0.15924685\n",
      "Iteration 1687, loss = 0.15853720\n",
      "Iteration 289, loss = 0.36118179\n",
      "Iteration 280, loss = 0.37239927\n",
      "Iteration 543, loss = 0.36123790\n",
      "Iteration 2404, loss = 0.15097509Iteration 1099, loss = 0.28547240\n",
      "\n",
      "Iteration 290, loss = 0.36103555\n",
      "Iteration 1688, loss = 0.15849122\n",
      "Iteration 2405, loss = 0.15093727Iteration 281, loss = 0.37217078\n",
      "Iteration 657, loss = 0.32400631\n",
      "\n",
      "Iteration 1100, loss = 0.28536011\n",
      "Iteration 282, loss = 0.37193662\n",
      "Iteration 1689, loss = 0.15816558\n",
      "Iteration 291, loss = 0.36089237\n",
      "Iteration 2406, loss = 0.15078069\n",
      "Iteration 2407, loss = 0.15069095\n",
      "Iteration 283, loss = 0.37172479\n",
      "Iteration 658, loss = 0.32381233\n",
      "Iteration 544, loss = 0.36112680\n",
      "Iteration 2408, loss = 0.15059384\n",
      "Iteration 284, loss = 0.37148283\n",
      "Iteration 1690, loss = 0.15807104\n",
      "Iteration 292, loss = 0.36073695\n",
      "Iteration 545, loss = 0.36098837\n",
      "Iteration 1101, loss = 0.28525050\n",
      "Iteration 285, loss = 0.37125946\n",
      "Iteration 312, loss = 0.43227490\n",
      "Iteration 293, loss = 0.36060238\n",
      "Iteration 2409, loss = 0.15045679Iteration 286, loss = 0.37104514\n",
      "Iteration 1691, loss = 0.15811001\n",
      "\n",
      "Iteration 294, loss = 0.36048398\n",
      "Iteration 659, loss = 0.32366766\n",
      "Iteration 2410, loss = 0.15041641\n",
      "Iteration 2411, loss = 0.15046111\n",
      "Iteration 295, loss = 0.36030389\n",
      "Iteration 1102, loss = 0.28509131\n",
      "Iteration 1692, loss = 0.15769925\n",
      "Iteration 287, loss = 0.37078782\n",
      "Iteration 2412, loss = 0.15015510\n",
      "Iteration 288, loss = 0.37057446\n",
      "Iteration 660, loss = 0.32345449\n",
      "Iteration 296, loss = 0.36016373\n",
      "Iteration 289, loss = 0.37038998\n",
      "Iteration 1693, loss = 0.15775048\n",
      "Iteration 546, loss = 0.36084269\n",
      "Iteration 297, loss = 0.36001857\n",
      "Iteration 661, loss = 0.32340526\n",
      "Iteration 290, loss = 0.37015318\n",
      "Iteration 1694, loss = 0.15769179\n",
      "Iteration 2413, loss = 0.15003507\n",
      "Iteration 313, loss = 0.43200616\n",
      "Iteration 1695, loss = 0.15730870\n",
      "Iteration 291, loss = 0.36995347\n",
      "Iteration 298, loss = 0.35986881\n",
      "Iteration 2414, loss = 0.14999434\n",
      "Iteration 1696, loss = 0.15700179\n",
      "Iteration 1697, loss = 0.15682633\n",
      "Iteration 299, loss = 0.35971665\n",
      "Iteration 547, loss = 0.36070123\n",
      "Iteration 2415, loss = 0.14992149\n",
      "Iteration 292, loss = 0.36972147\n",
      "Iteration 1103, loss = 0.28504855\n",
      "Iteration 1698, loss = 0.15661186\n",
      "Iteration 662, loss = 0.32316087\n",
      "Iteration 1699, loss = 0.15654163\n",
      "Iteration 2416, loss = 0.14976232\n",
      "Iteration 2417, loss = 0.14972218\n",
      "Iteration 1700, loss = 0.15634109\n",
      "Iteration 293, loss = 0.36948590\n",
      "Iteration 2418, loss = 0.14967359\n",
      "Iteration 1701, loss = 0.15626759\n",
      "Iteration 300, loss = 0.35956839\n",
      "Iteration 663, loss = 0.32288301\n",
      "Iteration 2419, loss = 0.14949265\n",
      "Iteration 294, loss = 0.36926264\n",
      "Iteration 1702, loss = 0.15593924\n",
      "Iteration 314, loss = 0.43171323\n",
      "Iteration 2420, loss = 0.14950762\n",
      "Iteration 1104, loss = 0.28487442\n",
      "Iteration 548, loss = 0.36058944\n",
      "Iteration 2421, loss = 0.14923660\n",
      "Iteration 301, loss = 0.35944241\n",
      "Iteration 664, loss = 0.32281884\n",
      "Iteration 1105, loss = 0.28473819\n",
      "Iteration 2422, loss = 0.14919417\n",
      "Iteration 549, loss = 0.36046822\n",
      "Iteration 295, loss = 0.36905297\n",
      "Iteration 315, loss = 0.43151253\n",
      "Iteration 2423, loss = 0.14905257\n",
      "Iteration 2424, loss = 0.14911075\n",
      "Iteration 1703, loss = 0.15574397\n",
      "Iteration 2425, loss = 0.14885353\n",
      "Iteration 296, loss = 0.36887432\n",
      "Iteration 1106, loss = 0.28465996\n",
      "Iteration 550, loss = 0.36034398\n",
      "Iteration 2426, loss = 0.14875949\n",
      "Iteration 1704, loss = 0.15557992\n",
      "Iteration 297, loss = 0.36862402\n",
      "Iteration 1107, loss = 0.28452206\n",
      "Iteration 316, loss = 0.43121166\n",
      "Iteration 302, loss = 0.35927858\n",
      "Iteration 2427, loss = 0.14868524\n",
      "Iteration 551, loss = 0.36024126\n",
      "Iteration 303, loss = 0.35915096\n",
      "Iteration 298, loss = 0.36841996\n",
      "Iteration 2428, loss = 0.14862803\n",
      "Iteration 1705, loss = 0.15538746\n",
      "Iteration 665, loss = 0.32276599\n",
      "Iteration 304, loss = 0.35901942\n",
      "Iteration 299, loss = 0.36820405\n",
      "Iteration 2429, loss = 0.14855615\n",
      "Iteration 1108, loss = 0.28441577\n",
      "Iteration 1706, loss = 0.15523349\n",
      "Iteration 305, loss = 0.35886874\n",
      "Iteration 1707, loss = 0.15514958\n",
      "Iteration 666, loss = 0.32235237\n",
      "Iteration 300, loss = 0.36800177\n",
      "Iteration 2430, loss = 0.14848862\n",
      "Iteration 1109, loss = 0.28431656\n",
      "Iteration 1708, loss = 0.15497674\n",
      "Iteration 2431, loss = 0.14825419\n",
      "Iteration 1110, loss = 0.28417466\n",
      "Iteration 667, loss = 0.32214978\n",
      "Iteration 301, loss = 0.36778624\n",
      "Iteration 1709, loss = 0.15465305\n",
      "Iteration 306, loss = 0.35872926\n",
      "Iteration 2432, loss = 0.14825026\n",
      "Iteration 302, loss = 0.36758144\n",
      "Iteration 317, loss = 0.43096909\n",
      "Iteration 1710, loss = 0.15447144\n",
      "Iteration 2433, loss = 0.14811906\n",
      "Iteration 303, loss = 0.36736509\n",
      "Iteration 2434, loss = 0.14796399\n",
      "Iteration 1711, loss = 0.15429928\n",
      "Iteration 304, loss = 0.36718449\n",
      "Iteration 307, loss = 0.35859005\n",
      "Iteration 305, loss = 0.36696183\n",
      "Iteration 1111, loss = 0.28404505\n",
      "Iteration 306, loss = 0.36673904\n",
      "Iteration 308, loss = 0.35845304\n",
      "Iteration 307, loss = 0.36657835\n",
      "Iteration 1712, loss = 0.15415417\n",
      "Iteration 552, loss = 0.36004777\n",
      "Iteration 2435, loss = 0.14790978\n",
      "Iteration 308, loss = 0.36634694\n",
      "Iteration 668, loss = 0.32202296\n",
      "Iteration 2436, loss = 0.14779578\n",
      "Iteration 309, loss = 0.36612993\n",
      "Iteration 2437, loss = 0.14766580\n",
      "Iteration 310, loss = 0.36594133\n",
      "Iteration 2438, loss = 0.14758217\n",
      "Iteration 309, loss = 0.35830961\n",
      "Iteration 1713, loss = 0.15396898\n",
      "Iteration 1112, loss = 0.28395378\n",
      "Iteration 311, loss = 0.36574715\n",
      "Iteration 318, loss = 0.43066946\n",
      "Iteration 2439, loss = 0.14754004\n",
      "Iteration 312, loss = 0.36555725\n",
      "Iteration 2440, loss = 0.14739384\n",
      "Iteration 313, loss = 0.36535021\n",
      "Iteration 1113, loss = 0.28388872\n",
      "Iteration 310, loss = 0.35816572\n",
      "Iteration 2441, loss = 0.14734763\n",
      "Iteration 669, loss = 0.32195239\n",
      "Iteration 314, loss = 0.36514753\n",
      "Iteration 1714, loss = 0.15392057\n",
      "Iteration 2442, loss = 0.14733567\n",
      "Iteration 315, loss = 0.36494703\n",
      "Iteration 1715, loss = 0.15366741\n",
      "Iteration 2443, loss = 0.14711502\n",
      "Iteration 553, loss = 0.35990116\n",
      "Iteration 2444, loss = 0.14707790\n",
      "Iteration 1114, loss = 0.28373245\n",
      "Iteration 1716, loss = 0.15374979\n",
      "Iteration 311, loss = 0.35805113\n",
      "Iteration 316, loss = 0.36477423\n",
      "Iteration 1115, loss = 0.28364155\n",
      "Iteration 670, loss = 0.32160070\n",
      "Iteration 2445, loss = 0.14692714\n",
      "Iteration 317, loss = 0.36454906\n",
      "Iteration 318, loss = 0.36437870\n",
      "Iteration 1717, loss = 0.15326112\n",
      "Iteration 1116, loss = 0.28353046\n",
      "Iteration 319, loss = 0.36417477\n",
      "Iteration 2446, loss = 0.14688838\n",
      "Iteration 312, loss = 0.35791735\n",
      "Iteration 554, loss = 0.35976102\n",
      "Iteration 2447, loss = 0.14673581\n",
      "Iteration 319, loss = 0.43043842\n",
      "Iteration 320, loss = 0.36399446\n",
      "Iteration 1117, loss = 0.28338207\n",
      "Iteration 2448, loss = 0.14661415\n",
      "Iteration 1718, loss = 0.15309467\n",
      "Iteration 671, loss = 0.32148478\n",
      "Iteration 321, loss = 0.36378314\n",
      "Iteration 555, loss = 0.35963552\n",
      "Iteration 2449, loss = 0.14653455\n",
      "Iteration 322, loss = 0.36363241\n",
      "Iteration 313, loss = 0.35776696\n",
      "Iteration 1118, loss = 0.28328503\n",
      "Iteration 323, loss = 0.36341022\n",
      "Iteration 324, loss = 0.36324191\n",
      "Iteration 672, loss = 0.32129138\n",
      "Iteration 1719, loss = 0.15285039\n",
      "Iteration 2450, loss = 0.14642106\n",
      "Iteration 325, loss = 0.36306114\n",
      "Iteration 2451, loss = 0.14648317\n",
      "Iteration 1720, loss = 0.15270230\n",
      "Iteration 326, loss = 0.36285494\n",
      "Iteration 2452, loss = 0.14624705\n",
      "Iteration 1721, loss = 0.15252209\n",
      "Iteration 1119, loss = 0.28315465\n",
      "Iteration 2453, loss = 0.14618033\n",
      "Iteration 320, loss = 0.43012536\n",
      "Iteration 1722, loss = 0.15237917\n",
      "Iteration 2454, loss = 0.14613988\n",
      "Iteration 314, loss = 0.35763549\n",
      "Iteration 673, loss = 0.32103544\n",
      "Iteration 556, loss = 0.35953760\n",
      "Iteration 1723, loss = 0.15247781\n",
      "Iteration 2455, loss = 0.14602894\n",
      "Iteration 1120, loss = 0.28306989\n",
      "Iteration 1724, loss = 0.15197538\n",
      "Iteration 2456, loss = 0.14589171\n",
      "Iteration 674, loss = 0.32092651\n",
      "Iteration 327, loss = 0.36265094\n",
      "Iteration 2457, loss = 0.14575796\n",
      "Iteration 315, loss = 0.35754088\n",
      "Iteration 2458, loss = 0.14563974\n",
      "Iteration 557, loss = 0.35937064\n",
      "Iteration 1725, loss = 0.15187291\n",
      "Iteration 2459, loss = 0.14558586\n",
      "Iteration 328, loss = 0.36253365\n",
      "Iteration 2460, loss = 0.14553692\n",
      "Iteration 675, loss = 0.32067682\n",
      "Iteration 1121, loss = 0.28293573\n",
      "Iteration 2461, loss = 0.14541406\n",
      "Iteration 329, loss = 0.36230306\n",
      "Iteration 1726, loss = 0.15174000\n",
      "Iteration 2462, loss = 0.14538712\n",
      "Iteration 558, loss = 0.35925356\n",
      "Iteration 316, loss = 0.35737341\n",
      "Iteration 2463, loss = 0.14518994\n",
      "Iteration 676, loss = 0.32047911\n",
      "Iteration 330, loss = 0.36211454\n",
      "Iteration 2464, loss = 0.14504818\n",
      "Iteration 1727, loss = 0.15158546\n",
      "Iteration 331, loss = 0.36197631\n",
      "Iteration 2465, loss = 0.14498755\n",
      "Iteration 1122, loss = 0.28288109\n",
      "Iteration 1728, loss = 0.15155818\n",
      "Iteration 317, loss = 0.35723349\n",
      "Iteration 2466, loss = 0.14486713\n",
      "Iteration 332, loss = 0.36174419\n",
      "Iteration 2467, loss = 0.14482036\n",
      "Iteration 1729, loss = 0.15134508\n",
      "Iteration 677, loss = 0.32036458\n",
      "Iteration 2468, loss = 0.14467550\n",
      "Iteration 318, loss = 0.35711000\n",
      "Iteration 2469, loss = 0.14463072\n",
      "Iteration 333, loss = 0.36163695\n",
      "Iteration 2470, loss = 0.14450867\n",
      "Iteration 319, loss = 0.35696699\n",
      "Iteration 1730, loss = 0.15092549\n",
      "Iteration 678, loss = 0.32009012\n",
      "Iteration 321, loss = 0.42986780\n",
      "Iteration 1123, loss = 0.28275887\n",
      "Iteration 334, loss = 0.36139325\n",
      "Iteration 2471, loss = 0.14437968\n",
      "Iteration 1731, loss = 0.15082433\n",
      "Iteration 559, loss = 0.35912098\n",
      "Iteration 2472, loss = 0.14434977\n",
      "Iteration 2473, loss = 0.14425256\n",
      "Iteration 335, loss = 0.36118989\n",
      "Iteration 2474, loss = 0.14420623\n",
      "Iteration 1124, loss = 0.28257969\n",
      "Iteration 1732, loss = 0.15060304\n",
      "Iteration 2475, loss = 0.14405946\n",
      "Iteration 320, loss = 0.35686954\n",
      "Iteration 336, loss = 0.36105733\n",
      "Iteration 2476, loss = 0.14389939\n",
      "Iteration 1125, loss = 0.28249251\n",
      "Iteration 322, loss = 0.42961311\n",
      "Iteration 2477, loss = 0.14384438\n",
      "Iteration 1733, loss = 0.15063968\n",
      "Iteration 2478, loss = 0.14375888\n",
      "Iteration 337, loss = 0.36085741\n",
      "Iteration 679, loss = 0.31998842\n",
      "Iteration 1734, loss = 0.15025082\n",
      "Iteration 338, loss = 0.36068783\n",
      "Iteration 560, loss = 0.35902746\n",
      "Iteration 1735, loss = 0.15010833\n",
      "Iteration 339, loss = 0.36050654\n",
      "Iteration 321, loss = 0.35671077\n",
      "Iteration 1126, loss = 0.28247262\n",
      "Iteration 1736, loss = 0.14996079\n",
      "Iteration 340, loss = 0.36032608\n",
      "Iteration 2479, loss = 0.14367862\n",
      "Iteration 1737, loss = 0.15022902\n",
      "Iteration 1127, loss = 0.28224392\n",
      "Iteration 1738, loss = 0.14960190\n",
      "Iteration 680, loss = 0.31974287\n",
      "Iteration 561, loss = 0.35883814\n",
      "Iteration 341, loss = 0.36014896\n",
      "Iteration 2480, loss = 0.14362701\n",
      "Iteration 1128, loss = 0.28214589\n",
      "Iteration 1739, loss = 0.14937718\n",
      "Iteration 342, loss = 0.35994755\n",
      "Iteration 2481, loss = 0.14346331\n",
      "Iteration 343, loss = 0.35978440\n",
      "Iteration 681, loss = 0.31960036\n",
      "Iteration 323, loss = 0.42939617\n",
      "Iteration 1129, loss = 0.28202197\n",
      "Iteration 344, loss = 0.35962754\n",
      "Iteration 2482, loss = 0.14342581\n",
      "Iteration 322, loss = 0.35658131\n",
      "Iteration 345, loss = 0.35942264\n",
      "Iteration 1740, loss = 0.14943963\n",
      "Iteration 2483, loss = 0.14327168\n",
      "Iteration 346, loss = 0.35925918\n",
      "Iteration 682, loss = 0.31934527\n",
      "Iteration 1130, loss = 0.28194068\n",
      "Iteration 2484, loss = 0.14316040\n",
      "Iteration 323, loss = 0.35644458\n",
      "Iteration 562, loss = 0.35869747\n",
      "Iteration 347, loss = 0.35911236\n",
      "Iteration 348, loss = 0.35890411\n",
      "Iteration 324, loss = 0.35634209\n",
      "Iteration 2485, loss = 0.14307732\n",
      "Iteration 349, loss = 0.35874472\n",
      "Iteration 1741, loss = 0.14912338\n",
      "Iteration 350, loss = 0.35855956\n",
      "Iteration 324, loss = 0.42915230\n",
      "Iteration 325, loss = 0.35619545\n",
      "Iteration 1131, loss = 0.28179473\n",
      "Iteration 351, loss = 0.35839475\n",
      "Iteration 1742, loss = 0.14890109\n",
      "Iteration 352, loss = 0.35820347\n",
      "Iteration 2486, loss = 0.14299980Iteration 326, loss = 0.35605301\n",
      "\n",
      "Iteration 1743, loss = 0.14874801\n",
      "Iteration 353, loss = 0.35806212\n",
      "Iteration 1132, loss = 0.28174827\n",
      "Iteration 683, loss = 0.31914393\n",
      "Iteration 354, loss = 0.35785773\n",
      "Iteration 1744, loss = 0.14881945\n",
      "Iteration 563, loss = 0.35857942\n",
      "Iteration 2487, loss = 0.14291170\n",
      "Iteration 355, loss = 0.35770560\n",
      "Iteration 1745, loss = 0.14849547\n",
      "Iteration 356, loss = 0.35753050\n",
      "Iteration 1133, loss = 0.28159518\n",
      "Iteration 1746, loss = 0.14827087\n",
      "Iteration 327, loss = 0.35593368\n",
      "Iteration 2488, loss = 0.14297577\n",
      "Iteration 357, loss = 0.35734999\n",
      "Iteration 358, loss = 0.35718853\n",
      "Iteration 1747, loss = 0.14814243\n",
      "Iteration 1134, loss = 0.28161501\n",
      "Iteration 359, loss = 0.35703672\n",
      "Iteration 325, loss = 0.42886828\n",
      "Iteration 360, loss = 0.35687307\n",
      "Iteration 684, loss = 0.31901817\n",
      "Iteration 1135, loss = 0.28135480\n",
      "Iteration 361, loss = 0.35671583\n",
      "Iteration 1748, loss = 0.14786137\n",
      "Iteration 328, loss = 0.35580843\n",
      "Iteration 2489, loss = 0.14273586\n",
      "Iteration 362, loss = 0.35652358\n",
      "Iteration 2490, loss = 0.14263780\n",
      "Iteration 1136, loss = 0.28128142\n",
      "Iteration 2491, loss = 0.14250918\n",
      "Iteration 1749, loss = 0.14765940\n",
      "Iteration 363, loss = 0.35636132\n",
      "Iteration 685, loss = 0.31895608\n",
      "Iteration 326, loss = 0.42864337\n",
      "Iteration 2492, loss = 0.14245133\n",
      "Iteration 1137, loss = 0.28114541\n",
      "Iteration 364, loss = 0.35619023\n",
      "Iteration 1750, loss = 0.14756628\n",
      "Iteration 329, loss = 0.35567974\n",
      "Iteration 2493, loss = 0.14240506\n",
      "Iteration 1138, loss = 0.28102322\n",
      "Iteration 1751, loss = 0.14742642\n",
      "Iteration 365, loss = 0.35610722\n",
      "Iteration 686, loss = 0.31864318\n",
      "Iteration 1752, loss = 0.14787324\n",
      "Iteration 366, loss = 0.35587604\n",
      "Iteration 2494, loss = 0.14226060\n",
      "Iteration 1139, loss = 0.28089281\n",
      "Iteration 564, loss = 0.35843497\n",
      "Iteration 1753, loss = 0.14704184\n",
      "Iteration 1754, loss = 0.14690791\n",
      "Iteration 330, loss = 0.35554565\n",
      "Iteration 367, loss = 0.35574792\n",
      "Iteration 2495, loss = 0.14216477\n",
      "Iteration 1140, loss = 0.28084842\n",
      "Iteration 1755, loss = 0.14677903\n",
      "Iteration 687, loss = 0.31842212\n",
      "Iteration 368, loss = 0.35555861\n",
      "Iteration 1141, loss = 0.28069657\n",
      "Iteration 1756, loss = 0.14648284\n",
      "Iteration 2496, loss = 0.14205202\n",
      "Iteration 331, loss = 0.35543324\n",
      "Iteration 2497, loss = 0.14195024\n",
      "Iteration 369, loss = 0.35539573\n",
      "Iteration 1757, loss = 0.14640189\n",
      "Iteration 2498, loss = 0.14194857\n",
      "Iteration 370, loss = 0.35525570\n",
      "Iteration 2499, loss = 0.14177460\n",
      "Iteration 327, loss = 0.42833305\n",
      "Iteration 332, loss = 0.35531636\n",
      "Iteration 565, loss = 0.35830546\n",
      "Iteration 371, loss = 0.35506628\n",
      "Iteration 1142, loss = 0.28059023\n",
      "Iteration 1758, loss = 0.14619825\n",
      "Iteration 333, loss = 0.35518263\n",
      "Iteration 1759, loss = 0.14600321\n",
      "Iteration 372, loss = 0.35488699\n",
      "Iteration 2500, loss = 0.14172764\n",
      "Iteration 373, loss = 0.35473766\n",
      "Iteration 1143, loss = 0.28048752\n",
      "Iteration 334, loss = 0.35504943\n",
      "Iteration 688, loss = 0.31823510\n",
      "Iteration 1760, loss = 0.14587367\n",
      "Iteration 374, loss = 0.35460118\n",
      "Iteration 328, loss = 0.42806708\n",
      "Iteration 2501, loss = 0.14161558\n",
      "Iteration 1144, loss = 0.28034964\n",
      "Iteration 566, loss = 0.35815879\n",
      "Iteration 375, loss = 0.35442048\n",
      "Iteration 2502, loss = 0.14161179\n",
      "Iteration 1761, loss = 0.14563081\n",
      "Iteration 2503, loss = 0.14137978\n",
      "Iteration 1762, loss = 0.14562701\n",
      "Iteration 2504, loss = 0.14135154\n",
      "Iteration 376, loss = 0.35426644\n",
      "Iteration 335, loss = 0.35493760Iteration 1145, loss = 0.28027393\n",
      "\n",
      "Iteration 1763, loss = 0.14533655\n",
      "Iteration 377, loss = 0.35413514\n",
      "Iteration 689, loss = 0.31805573\n",
      "Iteration 1764, loss = 0.14516773\n",
      "Iteration 378, loss = 0.35393589\n",
      "Iteration 1146, loss = 0.28018016\n",
      "Iteration 2505, loss = 0.14122910\n",
      "Iteration 379, loss = 0.35380727\n",
      "Iteration 1765, loss = 0.14516649\n",
      "Iteration 336, loss = 0.35482343\n",
      "Iteration 1766, loss = 0.14496512\n",
      "Iteration 380, loss = 0.35365853\n",
      "Iteration 1767, loss = 0.14461867\n",
      "Iteration 2506, loss = 0.14116730\n",
      "Iteration 329, loss = 0.42785149\n",
      "Iteration 567, loss = 0.35806216\n",
      "Iteration 1147, loss = 0.28001290\n",
      "Iteration 1768, loss = 0.14447460\n",
      "Iteration 2507, loss = 0.14112993\n",
      "Iteration 381, loss = 0.35347310\n",
      "Iteration 690, loss = 0.31797559\n",
      "Iteration 1769, loss = 0.14450954\n",
      "Iteration 337, loss = 0.35468471\n",
      "Iteration 2508, loss = 0.14100392\n",
      "Iteration 382, loss = 0.35333102\n",
      "Iteration 2509, loss = 0.14091353\n",
      "Iteration 383, loss = 0.35318962\n",
      "Iteration 2510, loss = 0.14080669\n",
      "Iteration 384, loss = 0.35301375\n",
      "Iteration 338, loss = 0.35455579\n",
      "Iteration 1148, loss = 0.27990992\n",
      "Iteration 1770, loss = 0.14410009\n",
      "Iteration 691, loss = 0.31768760\n",
      "Iteration 339, loss = 0.35444607\n",
      "Iteration 385, loss = 0.35284172\n",
      "Iteration 330, loss = 0.42762837\n",
      "Iteration 340, loss = 0.35433191\n",
      "Iteration 692, loss = 0.31749856\n",
      "Iteration 1149, loss = 0.27977734\n",
      "Iteration 1150, loss = 0.27965501\n",
      "Iteration 341, loss = 0.35419988\n",
      "Iteration 386, loss = 0.35268964\n",
      "Iteration 693, loss = 0.31743527\n",
      "Iteration 387, loss = 0.35255009\n",
      "Iteration 331, loss = 0.42732935\n",
      "Iteration 388, loss = 0.35238818\n",
      "Iteration 2511, loss = 0.14065510Iteration 342, loss = 0.35408765\n",
      "\n",
      "Iteration 694, loss = 0.31712943\n",
      "Iteration 1771, loss = 0.14399201\n",
      "Iteration 343, loss = 0.35395780\n",
      "Iteration 2512, loss = 0.14081821\n",
      "Iteration 389, loss = 0.35224763\n",
      "Iteration 695, loss = 0.31699267\n",
      "Iteration 2513, loss = 0.14043969\n",
      "Iteration 1772, loss = 0.14378926\n",
      "Iteration 390, loss = 0.35210039\n",
      "Iteration 2514, loss = 0.14053343\n",
      "Iteration 391, loss = 0.35194335\n",
      "Iteration 1773, loss = 0.14382267\n",
      "Iteration 392, loss = 0.35180286\n",
      "Iteration 2515, loss = 0.14032230\n",
      "Iteration 568, loss = 0.35790347\n",
      "Iteration 1151, loss = 0.27956243\n",
      "Iteration 2516, loss = 0.14018270\n",
      "Iteration 344, loss = 0.35386258\n",
      "Iteration 393, loss = 0.35164163\n",
      "Iteration 394, loss = 0.35148527\n",
      "Iteration 1774, loss = 0.14345896\n",
      "Iteration 395, loss = 0.35132685\n",
      "Iteration 1152, loss = 0.27945098\n",
      "Iteration 345, loss = 0.35372649\n",
      "Iteration 2517, loss = 0.14020436\n",
      "Iteration 1775, loss = 0.14341612\n",
      "Iteration 569, loss = 0.35776450\n",
      "Iteration 1776, loss = 0.14337055\n",
      "Iteration 2518, loss = 0.14001455\n",
      "Iteration 696, loss = 0.31676210\n",
      "Iteration 1777, loss = 0.14306751\n",
      "Iteration 396, loss = 0.35117737\n",
      "Iteration 1778, loss = 0.14297939\n",
      "Iteration 2519, loss = 0.13991507\n",
      "Iteration 397, loss = 0.35103573\n",
      "Iteration 346, loss = 0.35362503\n",
      "Iteration 1153, loss = 0.27932730\n",
      "Iteration 398, loss = 0.35090007\n",
      "Iteration 1779, loss = 0.14276716\n",
      "Iteration 2520, loss = 0.13988046\n",
      "Iteration 399, loss = 0.35073114\n",
      "Iteration 2521, loss = 0.13973012\n",
      "Iteration 1154, loss = 0.27922371\n",
      "Iteration 347, loss = 0.35348714\n",
      "Iteration 697, loss = 0.31661118\n",
      "Iteration 1780, loss = 0.14253991\n",
      "Iteration 2522, loss = 0.13971239\n",
      "Iteration 570, loss = 0.35765997\n",
      "Iteration 1155, loss = 0.27910131\n",
      "Iteration 348, loss = 0.35337498\n",
      "Iteration 400, loss = 0.35058946\n",
      "Iteration 2523, loss = 0.13955340\n",
      "Iteration 1781, loss = 0.14227754\n",
      "Iteration 401, loss = 0.35046970\n",
      "Iteration 349, loss = 0.35326790\n",
      "Iteration 332, loss = 0.42706599\n",
      "Iteration 1782, loss = 0.14216919\n",
      "Iteration 2524, loss = 0.13947848\n",
      "Iteration 2525, loss = 0.13935201\n",
      "Iteration 402, loss = 0.35029403\n",
      "Iteration 1783, loss = 0.14215205\n",
      "Iteration 2526, loss = 0.13947684\n",
      "Iteration 1156, loss = 0.27898524\n",
      "Iteration 1784, loss = 0.14180523\n",
      "Iteration 571, loss = 0.35749526\n",
      "Iteration 1785, loss = 0.14174925\n",
      "Iteration 2527, loss = 0.13924735\n",
      "Iteration 403, loss = 0.35015412\n",
      "Iteration 1157, loss = 0.27887175\n",
      "Iteration 698, loss = 0.31641106\n",
      "Iteration 350, loss = 0.35315394\n",
      "Iteration 2528, loss = 0.13912898\n",
      "Iteration 404, loss = 0.35001550\n",
      "Iteration 2529, loss = 0.13906898\n",
      "Iteration 572, loss = 0.35737400\n",
      "Iteration 1786, loss = 0.14154114\n",
      "Iteration 1158, loss = 0.27878032\n",
      "Iteration 405, loss = 0.34988300\n",
      "Iteration 2530, loss = 0.13890402\n",
      "Iteration 351, loss = 0.35303193\n",
      "Iteration 1787, loss = 0.14144967\n",
      "Iteration 406, loss = 0.34973680\n",
      "Iteration 699, loss = 0.31620337\n",
      "Iteration 2531, loss = 0.13885850\n",
      "Iteration 1159, loss = 0.27867880\n",
      "Iteration 2532, loss = 0.13875504\n",
      "Iteration 407, loss = 0.34958725\n",
      "Iteration 352, loss = 0.35291440\n",
      "Iteration 700, loss = 0.31597338\n",
      "Iteration 2533, loss = 0.13870778\n",
      "Iteration 333, loss = 0.42680338\n",
      "Iteration 408, loss = 0.34943979\n",
      "Iteration 1788, loss = 0.14135115\n",
      "Iteration 573, loss = 0.35724147\n",
      "Iteration 353, loss = 0.35280845\n",
      "Iteration 1160, loss = 0.27857629\n",
      "Iteration 1789, loss = 0.14102333\n",
      "Iteration 409, loss = 0.34929350\n",
      "Iteration 701, loss = 0.31588069\n",
      "Iteration 1790, loss = 0.14088074\n",
      "Iteration 2534, loss = 0.13854414\n",
      "Iteration 354, loss = 0.35267111\n",
      "Iteration 410, loss = 0.34914424\n",
      "Iteration 2535, loss = 0.13847957\n",
      "Iteration 1161, loss = 0.27848834\n",
      "Iteration 2536, loss = 0.13841429\n",
      "Iteration 1791, loss = 0.14080340\n",
      "Iteration 355, loss = 0.35256743\n",
      "Iteration 411, loss = 0.34899986\n",
      "Iteration 1792, loss = 0.14059932\n",
      "Iteration 412, loss = 0.34886256\n",
      "Iteration 2537, loss = 0.13838456\n",
      "Iteration 1793, loss = 0.14041167\n",
      "Iteration 413, loss = 0.34873807\n",
      "Iteration 2538, loss = 0.13827078\n",
      "Iteration 356, loss = 0.35244692\n",
      "Iteration 1162, loss = 0.27834366\n",
      "Iteration 414, loss = 0.34860837\n",
      "Iteration 574, loss = 0.35711428\n",
      "Iteration 357, loss = 0.35233122\n",
      "Iteration 415, loss = 0.34848215\n",
      "Iteration 334, loss = 0.42656038\n",
      "Iteration 2539, loss = 0.13815588\n",
      "Iteration 358, loss = 0.35222789\n",
      "Iteration 416, loss = 0.34834395\n",
      "Iteration 2540, loss = 0.13802469\n",
      "Iteration 1163, loss = 0.27828676\n",
      "Iteration 417, loss = 0.34816436\n",
      "Iteration 702, loss = 0.31563245\n",
      "Iteration 2541, loss = 0.13800641\n",
      "Iteration 1794, loss = 0.14025330\n",
      "Iteration 418, loss = 0.34802441\n",
      "Iteration 1795, loss = 0.14005481\n",
      "Iteration 359, loss = 0.35210789\n",
      "Iteration 2542, loss = 0.13784650\n",
      "Iteration 419, loss = 0.34787843\n",
      "Iteration 575, loss = 0.35698655\n",
      "Iteration 360, loss = 0.35198348\n",
      "Iteration 1796, loss = 0.13993949\n",
      "Iteration 420, loss = 0.34775048\n",
      "Iteration 361, loss = 0.35188266\n",
      "Iteration 2543, loss = 0.13783661\n",
      "Iteration 1164, loss = 0.27808014\n",
      "Iteration 2544, loss = 0.13765102\n",
      "Iteration 703, loss = 0.31559195\n",
      "Iteration 2545, loss = 0.13762582\n",
      "Iteration 362, loss = 0.35177166\n",
      "Iteration 2546, loss = 0.13755424\n",
      "Iteration 363, loss = 0.35166520\n",
      "Iteration 1797, loss = 0.13975896\n",
      "Iteration 1165, loss = 0.27800942\n",
      "Iteration 2547, loss = 0.13736463\n",
      "Iteration 1798, loss = 0.13969609\n",
      "Iteration 1799, loss = 0.13976813\n",
      "Iteration 576, loss = 0.35684709\n",
      "Iteration 364, loss = 0.35154464\n",
      "Iteration 421, loss = 0.34760215\n",
      "Iteration 335, loss = 0.42629543\n",
      "Iteration 704, loss = 0.31529758\n",
      "Iteration 422, loss = 0.34747050\n",
      "Iteration 365, loss = 0.35143549\n",
      "Iteration 423, loss = 0.34732031\n",
      "Iteration 424, loss = 0.34718792\n",
      "Iteration 366, loss = 0.35131741\n",
      "Iteration 2548, loss = 0.13736209\n",
      "Iteration 425, loss = 0.34706919Iteration 705, loss = 0.31515677\n",
      "\n",
      "Iteration 1800, loss = 0.13937413\n",
      "Iteration 1166, loss = 0.27791068\n",
      "Iteration 367, loss = 0.35121449\n",
      "Iteration 2549, loss = 0.13725582\n",
      "Iteration 1801, loss = 0.13918516\n",
      "Iteration 426, loss = 0.34691026\n",
      "Iteration 1167, loss = 0.27788522\n",
      "Iteration 2550, loss = 0.13722067\n",
      "Iteration 427, loss = 0.34682490\n",
      "Iteration 1802, loss = 0.13899664\n",
      "Iteration 428, loss = 0.34664479\n",
      "Iteration 336, loss = 0.42607869\n",
      "Iteration 2551, loss = 0.13705665\n",
      "Iteration 429, loss = 0.34652206\n",
      "Iteration 2552, loss = 0.13700689\n",
      "Iteration 1803, loss = 0.13893491\n",
      "Iteration 2553, loss = 0.13689168\n",
      "Iteration 1168, loss = 0.27766168\n",
      "Iteration 577, loss = 0.35671209\n",
      "Iteration 706, loss = 0.31493974\n",
      "Iteration 2554, loss = 0.13680527\n",
      "Iteration 430, loss = 0.34637834\n",
      "Iteration 1804, loss = 0.13899387\n",
      "Iteration 2555, loss = 0.13669925\n",
      "Iteration 431, loss = 0.34623923\n",
      "Iteration 2556, loss = 0.13669744\n",
      "Iteration 432, loss = 0.34614880\n",
      "Iteration 368, loss = 0.35110353\n",
      "Iteration 2557, loss = 0.13658338\n",
      "Iteration 1805, loss = 0.13887817\n",
      "Iteration 433, loss = 0.34598256\n",
      "Iteration 1169, loss = 0.27757059\n",
      "Iteration 434, loss = 0.34586194\n",
      "Iteration 369, loss = 0.35099684\n",
      "Iteration 707, loss = 0.31485021\n",
      "Iteration 435, loss = 0.34571767\n",
      "Iteration 2558, loss = 0.13653337\n",
      "Iteration 578, loss = 0.35658043\n",
      "Iteration 2559, loss = 0.13632664\n",
      "Iteration 1170, loss = 0.27746202\n",
      "Iteration 436, loss = 0.34560857\n",
      "Iteration 708, loss = 0.31452585\n",
      "Iteration 2560, loss = 0.13631168\n",
      "Iteration 370, loss = 0.35089861\n",
      "Iteration 1806, loss = 0.13840024\n",
      "Iteration 337, loss = 0.42583521\n",
      "Iteration 2561, loss = 0.13617692Iteration 437, loss = 0.34545200\n",
      "\n",
      "Iteration 438, loss = 0.34531085\n",
      "Iteration 2562, loss = 0.13613333\n",
      "Iteration 1807, loss = 0.13827110\n",
      "Iteration 439, loss = 0.34518246\n",
      "Iteration 440, loss = 0.34504866\n",
      "Iteration 371, loss = 0.35077934\n",
      "Iteration 441, loss = 0.34493220\n",
      "Iteration 709, loss = 0.31442072\n",
      "Iteration 1808, loss = 0.13801896\n",
      "Iteration 1171, loss = 0.27729414\n",
      "Iteration 579, loss = 0.35642143\n",
      "Iteration 2563, loss = 0.13600679\n",
      "Iteration 1172, loss = 0.27717785\n",
      "Iteration 2564, loss = 0.13591806\n",
      "Iteration 1809, loss = 0.13802726\n",
      "Iteration 2565, loss = 0.13579317\n",
      "Iteration 372, loss = 0.35067060\n",
      "Iteration 442, loss = 0.34479512\n",
      "Iteration 2566, loss = 0.13581876\n",
      "Iteration 1810, loss = 0.13768211\n",
      "Iteration 580, loss = 0.35628329\n",
      "Iteration 2567, loss = 0.13576508\n",
      "Iteration 1811, loss = 0.13757718\n",
      "Iteration 710, loss = 0.31418827\n",
      "Iteration 2568, loss = 0.13554595\n",
      "Iteration 443, loss = 0.34466975\n",
      "Iteration 1812, loss = 0.13746518\n",
      "Iteration 2569, loss = 0.13544628\n",
      "Iteration 373, loss = 0.35056417\n",
      "Iteration 1813, loss = 0.13736831\n",
      "Iteration 2570, loss = 0.13540085\n",
      "Iteration 444, loss = 0.34452487\n",
      "Iteration 445, loss = 0.34440712\n",
      "Iteration 1814, loss = 0.13717561\n",
      "Iteration 446, loss = 0.34428702\n",
      "Iteration 2571, loss = 0.13542195\n",
      "Iteration 1173, loss = 0.27712499\n",
      "Iteration 1815, loss = 0.13698311\n",
      "Iteration 338, loss = 0.42557905\n",
      "Iteration 447, loss = 0.34414158\n",
      "Iteration 1816, loss = 0.13702941\n",
      "Iteration 2572, loss = 0.13522739\n",
      "Iteration 581, loss = 0.35619646\n",
      "Iteration 2573, loss = 0.13516173\n",
      "Iteration 2574, loss = 0.13502104\n",
      "Iteration 711, loss = 0.31402087\n",
      "Iteration 1817, loss = 0.13674849\n",
      "Iteration 2575, loss = 0.13510529\n",
      "Iteration 374, loss = 0.35045876\n",
      "Iteration 2576, loss = 0.13485606\n",
      "Iteration 2577, loss = 0.13476292\n",
      "Iteration 712, loss = 0.31379086\n",
      "Iteration 2578, loss = 0.13479243\n",
      "Iteration 1818, loss = 0.13667764\n",
      "Iteration 1174, loss = 0.27699709\n",
      "Iteration 339, loss = 0.42531390\n",
      "Iteration 375, loss = 0.35034100\n",
      "Iteration 2579, loss = 0.13460915\n",
      "Iteration 1175, loss = 0.27684753\n",
      "Iteration 1819, loss = 0.13644523\n",
      "Iteration 2580, loss = 0.13458572\n",
      "Iteration 1820, loss = 0.13618751\n",
      "Iteration 2581, loss = 0.13452406\n",
      "Iteration 2582, loss = 0.13443717\n",
      "Iteration 376, loss = 0.35025262\n",
      "Iteration 1821, loss = 0.13604761\n",
      "Iteration 2583, loss = 0.13459359\n",
      "Iteration 1822, loss = 0.13592318\n",
      "Iteration 1176, loss = 0.27677299\n",
      "Iteration 2584, loss = 0.13442345\n",
      "Iteration 713, loss = 0.31359387\n",
      "Iteration 1823, loss = 0.13574470\n",
      "Iteration 2585, loss = 0.13414974\n",
      "Iteration 582, loss = 0.35605849\n",
      "Iteration 377, loss = 0.35013758\n",
      "Iteration 2586, loss = 0.13399931\n",
      "Iteration 2587, loss = 0.13396464\n",
      "Iteration 340, loss = 0.42506989\n",
      "Iteration 1824, loss = 0.13568305\n",
      "Iteration 2588, loss = 0.13381395\n",
      "Iteration 378, loss = 0.35002760\n",
      "Iteration 2589, loss = 0.13373142\n",
      "Iteration 1825, loss = 0.13568839\n",
      "Iteration 2590, loss = 0.13362376\n",
      "Iteration 1826, loss = 0.13527040\n",
      "Iteration 2591, loss = 0.13353920\n",
      "Iteration 1177, loss = 0.27670960\n",
      "Iteration 714, loss = 0.31351542\n",
      "Iteration 379, loss = 0.34993758\n",
      "Iteration 1827, loss = 0.13522246\n",
      "Iteration 1178, loss = 0.27656473\n",
      "Iteration 2592, loss = 0.13342516\n",
      "Iteration 1828, loss = 0.13513527\n",
      "Iteration 715, loss = 0.31325320\n",
      "Iteration 583, loss = 0.35590099\n",
      "Iteration 341, loss = 0.42479205\n",
      "Iteration 1829, loss = 0.13513538\n",
      "Iteration 2593, loss = 0.13342619\n",
      "Iteration 380, loss = 0.34982149\n",
      "Iteration 1830, loss = 0.13468632\n",
      "Iteration 2594, loss = 0.13332615\n",
      "Iteration 1831, loss = 0.13453019\n",
      "Iteration 381, loss = 0.34971240\n",
      "Iteration 1179, loss = 0.27637655\n",
      "Iteration 716, loss = 0.31315304\n",
      "Iteration 2595, loss = 0.13324628\n",
      "Iteration 1832, loss = 0.13448920\n",
      "Iteration 2596, loss = 0.13310845\n",
      "Iteration 1180, loss = 0.27626689\n",
      "Iteration 2597, loss = 0.13302111\n",
      "Iteration 1833, loss = 0.13459792\n",
      "Iteration 382, loss = 0.34962454\n",
      "Iteration 2598, loss = 0.13301050\n",
      "Iteration 1181, loss = 0.27621497\n",
      "Iteration 2599, loss = 0.13293548\n",
      "Iteration 1834, loss = 0.13410734\n",
      "Iteration 717, loss = 0.31281617\n",
      "Iteration 2600, loss = 0.13275952\n",
      "Iteration 1835, loss = 0.13408748\n",
      "Iteration 584, loss = 0.35575043\n",
      "Iteration 1182, loss = 0.27621095\n",
      "Iteration 342, loss = 0.42454440\n",
      "Iteration 2601, loss = 0.13267209\n",
      "Iteration 2602, loss = 0.13256870\n",
      "Iteration 448, loss = 0.34403061\n",
      "Iteration 2603, loss = 0.13262533\n",
      "Iteration 383, loss = 0.34949566\n",
      "Iteration 2604, loss = 0.13240383\n",
      "Iteration 718, loss = 0.31262249\n",
      "Iteration 1183, loss = 0.27591221\n",
      "Iteration 2605, loss = 0.13254362\n",
      "Iteration 449, loss = 0.34390391\n",
      "Iteration 2606, loss = 0.13225341\n",
      "Iteration 1836, loss = 0.13377562\n",
      "Iteration 1837, loss = 0.13377584\n",
      "Iteration 2607, loss = 0.13215914\n",
      "Iteration 1184, loss = 0.27581090\n",
      "Iteration 384, loss = 0.34939995\n",
      "Iteration 2608, loss = 0.13207931\n",
      "Iteration 1838, loss = 0.13357597\n",
      "Iteration 2609, loss = 0.13201938\n",
      "Iteration 719, loss = 0.31251539\n",
      "Iteration 1185, loss = 0.27568724\n",
      "Iteration 1839, loss = 0.13341710\n",
      "Iteration 385, loss = 0.34929328\n",
      "Iteration 2610, loss = 0.13193207\n",
      "Iteration 2611, loss = 0.13189621\n",
      "Iteration 1840, loss = 0.13336800\n",
      "Iteration 386, loss = 0.34923671\n",
      "Iteration 343, loss = 0.42429955\n",
      "Iteration 1841, loss = 0.13318677\n",
      "Iteration 585, loss = 0.35565788\n",
      "Iteration 2612, loss = 0.13171763\n",
      "Iteration 720, loss = 0.31230459\n",
      "Iteration 1186, loss = 0.27570189\n",
      "Iteration 2613, loss = 0.13165100\n",
      "Iteration 387, loss = 0.34908290\n",
      "Iteration 1842, loss = 0.13288685\n",
      "Iteration 1187, loss = 0.27550703\n",
      "Iteration 450, loss = 0.34376302\n",
      "Iteration 721, loss = 0.31214830\n",
      "Iteration 2614, loss = 0.13161739\n",
      "Iteration 1188, loss = 0.27541995\n",
      "Iteration 2615, loss = 0.13151970\n",
      "Iteration 388, loss = 0.34899729\n",
      "Iteration 586, loss = 0.35553105\n",
      "Iteration 2616, loss = 0.13133876\n",
      "Iteration 1843, loss = 0.13272523\n",
      "Iteration 2617, loss = 0.13121556\n",
      "Iteration 451, loss = 0.34364365\n",
      "Iteration 1844, loss = 0.13265187\n",
      "Iteration 722, loss = 0.31190784\n",
      "Iteration 452, loss = 0.34352477\n",
      "Iteration 2618, loss = 0.13118922\n",
      "Iteration 389, loss = 0.34889350\n",
      "Iteration 1845, loss = 0.13336855\n",
      "Iteration 453, loss = 0.34338215\n",
      "Iteration 587, loss = 0.35536396\n",
      "Iteration 454, loss = 0.34332917\n",
      "Iteration 1846, loss = 0.13230504\n",
      "Iteration 2619, loss = 0.13115582\n",
      "Iteration 344, loss = 0.42405900\n",
      "Iteration 2620, loss = 0.13101114\n",
      "Iteration 723, loss = 0.31169709\n",
      "Iteration 390, loss = 0.34879053\n",
      "Iteration 2621, loss = 0.13088550\n",
      "Iteration 1847, loss = 0.13225505\n",
      "Iteration 1189, loss = 0.27521813\n",
      "Iteration 391, loss = 0.34870020\n",
      "Iteration 588, loss = 0.35523575\n",
      "Iteration 1848, loss = 0.13212649\n",
      "Iteration 1190, loss = 0.27512963\n",
      "Iteration 2622, loss = 0.13081566\n",
      "Iteration 1849, loss = 0.13186825\n",
      "Iteration 1191, loss = 0.27505792\n",
      "Iteration 2623, loss = 0.13075565\n",
      "Iteration 392, loss = 0.34860175\n",
      "Iteration 724, loss = 0.31153762\n",
      "Iteration 1850, loss = 0.13176108\n",
      "Iteration 589, loss = 0.35511484\n",
      "Iteration 1851, loss = 0.13154924\n",
      "Iteration 345, loss = 0.42382734\n",
      "Iteration 2624, loss = 0.13067283\n",
      "Iteration 1852, loss = 0.13165138\n",
      "Iteration 725, loss = 0.31140698\n",
      "Iteration 455, loss = 0.34312435\n",
      "Iteration 1192, loss = 0.27489002\n",
      "Iteration 456, loss = 0.34299965\n",
      "Iteration 393, loss = 0.34848309\n",
      "Iteration 1853, loss = 0.13135139\n",
      "Iteration 726, loss = 0.31120517\n",
      "Iteration 590, loss = 0.35498554\n",
      "Iteration 2625, loss = 0.13057742\n",
      "Iteration 1854, loss = 0.13117312\n",
      "Iteration 346, loss = 0.42355967\n",
      "Iteration 2626, loss = 0.13051806\n",
      "Iteration 394, loss = 0.34839615\n",
      "Iteration 1193, loss = 0.27479339\n",
      "Iteration 2627, loss = 0.13037947\n",
      "Iteration 1855, loss = 0.13099149\n",
      "Iteration 727, loss = 0.31100172\n",
      "Iteration 2628, loss = 0.13030001\n",
      "Iteration 395, loss = 0.34828095\n",
      "Iteration 2629, loss = 0.13031146\n",
      "Iteration 591, loss = 0.35484519\n",
      "Iteration 2630, loss = 0.13013817\n",
      "Iteration 1194, loss = 0.27465022\n",
      "Iteration 396, loss = 0.34819672\n",
      "Iteration 1856, loss = 0.13083387\n",
      "Iteration 2631, loss = 0.13010975\n",
      "Iteration 2632, loss = 0.13003806\n",
      "Iteration 1857, loss = 0.13074783\n",
      "Iteration 1195, loss = 0.27458508\n",
      "Iteration 397, loss = 0.34808266\n",
      "Iteration 728, loss = 0.31078195\n",
      "Iteration 347, loss = 0.42334870\n",
      "Iteration 2633, loss = 0.12988622\n",
      "Iteration 398, loss = 0.34799769\n",
      "Iteration 1858, loss = 0.13054132\n",
      "Iteration 1859, loss = 0.13045324\n",
      "Iteration 399, loss = 0.34789413\n",
      "Iteration 2634, loss = 0.12977064\n",
      "Iteration 457, loss = 0.34287613\n",
      "Iteration 729, loss = 0.31064741\n",
      "Iteration 592, loss = 0.35481950\n",
      "Iteration 1196, loss = 0.27450862\n",
      "Iteration 1860, loss = 0.13025892\n",
      "Iteration 400, loss = 0.34778169\n",
      "Iteration 1861, loss = 0.13014860\n",
      "Iteration 2635, loss = 0.12982667\n",
      "Iteration 458, loss = 0.34275434\n",
      "Iteration 1862, loss = 0.13003571\n",
      "Iteration 2636, loss = 0.12965394\n",
      "Iteration 401, loss = 0.34771207\n",
      "Iteration 730, loss = 0.31038520\n",
      "Iteration 2637, loss = 0.12953898\n",
      "Iteration 1197, loss = 0.27442585\n",
      "Iteration 459, loss = 0.34263949\n",
      "Iteration 2638, loss = 0.12948296\n",
      "Iteration 402, loss = 0.34759424\n",
      "Iteration 1863, loss = 0.12981178\n",
      "Iteration 2639, loss = 0.12937387\n",
      "Iteration 348, loss = 0.42306794\n",
      "Iteration 1864, loss = 0.12966704\n",
      "Iteration 2640, loss = 0.12929309\n",
      "Iteration 2641, loss = 0.12927929\n",
      "Iteration 460, loss = 0.34253243\n",
      "Iteration 403, loss = 0.34750765\n",
      "Iteration 2642, loss = 0.12909190\n",
      "Iteration 731, loss = 0.31019253\n",
      "Iteration 593, loss = 0.35458248\n",
      "Iteration 1865, loss = 0.12953329\n",
      "Iteration 461, loss = 0.34239802\n",
      "Iteration 1198, loss = 0.27419207\n",
      "Iteration 2643, loss = 0.12904079\n",
      "Iteration 2644, loss = 0.12891401\n",
      "Iteration 1866, loss = 0.12946771\n",
      "Iteration 462, loss = 0.34226637\n",
      "Iteration 732, loss = 0.30999586\n",
      "Iteration 594, loss = 0.35444461\n",
      "Iteration 2645, loss = 0.12891518Iteration 404, loss = 0.34739779\n",
      "\n",
      "Iteration 463, loss = 0.34217923\n",
      "Iteration 1867, loss = 0.12930403\n",
      "Iteration 464, loss = 0.34200992\n",
      "Iteration 1199, loss = 0.27415086\n",
      "Iteration 465, loss = 0.34191215\n",
      "Iteration 1868, loss = 0.12912877\n",
      "Iteration 2646, loss = 0.12886155\n",
      "Iteration 466, loss = 0.34175256\n",
      "Iteration 405, loss = 0.34736152\n",
      "Iteration 2647, loss = 0.12867494\n",
      "Iteration 1200, loss = 0.27400522\n",
      "Iteration 2648, loss = 0.12865442\n",
      "Iteration 595, loss = 0.35430478\n",
      "Iteration 1869, loss = 0.12911409\n",
      "Iteration 349, loss = 0.42282148\n",
      "Iteration 733, loss = 0.30988513\n",
      "Iteration 406, loss = 0.34720959\n",
      "Iteration 467, loss = 0.34162041\n",
      "Iteration 1870, loss = 0.12900590\n",
      "Iteration 468, loss = 0.34152694\n",
      "Iteration 2649, loss = 0.12851644\n",
      "Iteration 407, loss = 0.34712328\n",
      "Iteration 1871, loss = 0.12872383\n",
      "Iteration 2650, loss = 0.12849103\n",
      "Iteration 469, loss = 0.34139681\n",
      "Iteration 1201, loss = 0.27386867\n",
      "Iteration 2651, loss = 0.12835015\n",
      "Iteration 408, loss = 0.34702187\n",
      "Iteration 470, loss = 0.34125430\n",
      "Iteration 1872, loss = 0.12857280\n",
      "Iteration 409, loss = 0.34693111\n",
      "Iteration 734, loss = 0.30971341\n",
      "Iteration 2652, loss = 0.12837094\n",
      "Iteration 471, loss = 0.34113206\n",
      "Iteration 596, loss = 0.35418772\n",
      "Iteration 1202, loss = 0.27372297\n",
      "Iteration 472, loss = 0.34100906\n",
      "Iteration 2653, loss = 0.12816579\n",
      "Iteration 2654, loss = 0.12807272\n",
      "Iteration 1873, loss = 0.12835961\n",
      "Iteration 735, loss = 0.30958183\n",
      "Iteration 473, loss = 0.34089607\n",
      "Iteration 410, loss = 0.34682384\n",
      "Iteration 1203, loss = 0.27368078\n",
      "Iteration 2655, loss = 0.12802356\n",
      "Iteration 474, loss = 0.34077889\n",
      "Iteration 350, loss = 0.42256263\n",
      "Iteration 1874, loss = 0.12829667Iteration 411, loss = 0.34672015\n",
      "\n",
      "Iteration 475, loss = 0.34064474\n",
      "Iteration 736, loss = 0.30926781\n",
      "Iteration 2656, loss = 0.12798870\n",
      "Iteration 1204, loss = 0.27355895\n",
      "Iteration 476, loss = 0.34053715\n",
      "Iteration 2657, loss = 0.12786662\n",
      "Iteration 1875, loss = 0.12811169\n",
      "Iteration 2658, loss = 0.12775709\n",
      "Iteration 737, loss = 0.30906789\n",
      "Iteration 1205, loss = 0.27352572\n",
      "Iteration 597, loss = 0.35403934\n",
      "Iteration 477, loss = 0.34042754\n",
      "Iteration 2659, loss = 0.12769299\n",
      "Iteration 1876, loss = 0.12800146\n",
      "Iteration 412, loss = 0.34661962\n",
      "Iteration 2660, loss = 0.12763318\n",
      "Iteration 1206, loss = 0.27329332\n",
      "Iteration 478, loss = 0.34029437\n",
      "Iteration 2661, loss = 0.12756688\n",
      "Iteration 738, loss = 0.30891339\n",
      "Iteration 351, loss = 0.42232987\n",
      "Iteration 2662, loss = 0.12745476\n",
      "Iteration 413, loss = 0.34652780\n",
      "Iteration 479, loss = 0.34017338\n",
      "Iteration 1877, loss = 0.12780327\n",
      "Iteration 2663, loss = 0.12736863\n",
      "Iteration 1207, loss = 0.27320699\n",
      "Iteration 598, loss = 0.35392705\n",
      "Iteration 2664, loss = 0.12729233\n",
      "Iteration 480, loss = 0.34009298\n",
      "Iteration 1878, loss = 0.12772450\n",
      "Iteration 739, loss = 0.30874608\n",
      "Iteration 1879, loss = 0.12771440\n",
      "Iteration 481, loss = 0.33993930\n",
      "Iteration 414, loss = 0.34644959\n",
      "Iteration 2665, loss = 0.12725259\n",
      "Iteration 599, loss = 0.35377219\n",
      "Iteration 2666, loss = 0.12709978\n",
      "Iteration 1880, loss = 0.12738755\n",
      "Iteration 482, loss = 0.33981098\n",
      "Iteration 415, loss = 0.34631950\n",
      "Iteration 1208, loss = 0.27304285\n",
      "Iteration 1881, loss = 0.12723129\n",
      "Iteration 352, loss = 0.42212980\n",
      "Iteration 483, loss = 0.33970692\n",
      "Iteration 2667, loss = 0.12721801\n",
      "Iteration 484, loss = 0.33957104\n",
      "Iteration 740, loss = 0.30856671\n",
      "Iteration 416, loss = 0.34623246\n",
      "Iteration 2668, loss = 0.12698734\n",
      "Iteration 485, loss = 0.33947197\n",
      "Iteration 1882, loss = 0.12713055\n",
      "Iteration 2669, loss = 0.12696327\n",
      "Iteration 2670, loss = 0.12684267\n",
      "Iteration 1209, loss = 0.27293469\n",
      "Iteration 486, loss = 0.33935647\n",
      "Iteration 2671, loss = 0.12685752\n",
      "Iteration 1883, loss = 0.12699120\n",
      "Iteration 417, loss = 0.34614014\n",
      "Iteration 2672, loss = 0.12670208\n",
      "Iteration 487, loss = 0.33923056\n",
      "Iteration 741, loss = 0.30827116\n",
      "Iteration 353, loss = 0.42192646\n",
      "Iteration 600, loss = 0.35364299\n",
      "Iteration 2673, loss = 0.12650292\n",
      "Iteration 488, loss = 0.33909624\n",
      "Iteration 1210, loss = 0.27284088\n",
      "Iteration 418, loss = 0.34606961\n",
      "Iteration 2674, loss = 0.12644100\n",
      "Iteration 489, loss = 0.33898303\n",
      "Iteration 1884, loss = 0.12689338\n",
      "Iteration 419, loss = 0.34595404\n",
      "Iteration 1885, loss = 0.12678137\n",
      "Iteration 2675, loss = 0.12635568\n",
      "Iteration 2676, loss = 0.12631698\n",
      "Iteration 1211, loss = 0.27271750\n",
      "Iteration 1886, loss = 0.12660088\n",
      "Iteration 490, loss = 0.33885866\n",
      "Iteration 2677, loss = 0.12618594\n",
      "Iteration 354, loss = 0.42160963\n",
      "Iteration 2678, loss = 0.12612559\n",
      "Iteration 1887, loss = 0.12634688\n",
      "Iteration 420, loss = 0.34585472\n",
      "Iteration 742, loss = 0.30822126\n",
      "Iteration 491, loss = 0.33877310\n",
      "Iteration 1888, loss = 0.12635504\n",
      "Iteration 2679, loss = 0.12603218Iteration 601, loss = 0.35351010\n",
      "\n",
      "Iteration 421, loss = 0.34574712\n",
      "Iteration 1212, loss = 0.27262391\n",
      "Iteration 1889, loss = 0.12625296\n",
      "Iteration 492, loss = 0.33872465\n",
      "Iteration 1890, loss = 0.12592069\n",
      "Iteration 2680, loss = 0.12598632\n",
      "Iteration 493, loss = 0.33851707\n",
      "Iteration 743, loss = 0.30795449\n",
      "Iteration 1891, loss = 0.12590182\n",
      "Iteration 2681, loss = 0.12592496\n",
      "Iteration 422, loss = 0.34567781\n",
      "Iteration 1892, loss = 0.12568700\n",
      "Iteration 1213, loss = 0.27245236\n",
      "Iteration 494, loss = 0.33839011\n",
      "Iteration 2682, loss = 0.12585411\n",
      "Iteration 1893, loss = 0.12547380\n",
      "Iteration 2683, loss = 0.12570380\n",
      "Iteration 1894, loss = 0.12538646\n",
      "Iteration 495, loss = 0.33829025\n",
      "Iteration 423, loss = 0.34556565\n",
      "Iteration 2684, loss = 0.12570623\n",
      "Iteration 744, loss = 0.30780833\n",
      "Iteration 1214, loss = 0.27239842\n",
      "Iteration 2685, loss = 0.12555140\n",
      "Iteration 1895, loss = 0.12531386\n",
      "Iteration 2686, loss = 0.12551259\n",
      "Iteration 496, loss = 0.33815542\n",
      "Iteration 2687, loss = 0.12537420\n",
      "Iteration 355, loss = 0.42136719\n",
      "Iteration 1215, loss = 0.27223590\n",
      "Iteration 497, loss = 0.33804245\n",
      "Iteration 2688, loss = 0.12542495\n",
      "Iteration 424, loss = 0.34546956\n",
      "Iteration 2689, loss = 0.12532523\n",
      "Iteration 602, loss = 0.35337819\n",
      "Iteration 745, loss = 0.30755479\n",
      "Iteration 1216, loss = 0.27212379\n",
      "Iteration 2690, loss = 0.12515248\n",
      "Iteration 1896, loss = 0.12511726\n",
      "Iteration 498, loss = 0.33792617\n",
      "Iteration 499, loss = 0.33782330\n",
      "Iteration 425, loss = 0.34538340\n",
      "Iteration 1217, loss = 0.27200771\n",
      "Iteration 2691, loss = 0.12512908\n",
      "Iteration 500, loss = 0.33771521\n",
      "Iteration 501, loss = 0.33757785\n",
      "Iteration 426, loss = 0.34528030\n",
      "Iteration 1897, loss = 0.12496478\n",
      "Iteration 1218, loss = 0.27189524\n",
      "Iteration 746, loss = 0.30737693\n",
      "Iteration 2692, loss = 0.12502533\n",
      "Iteration 603, loss = 0.35322873\n",
      "Iteration 1898, loss = 0.12483968\n",
      "Iteration 502, loss = 0.33747745\n",
      "Iteration 2693, loss = 0.12489703\n",
      "Iteration 1219, loss = 0.27176993\n",
      "Iteration 427, loss = 0.34518459\n",
      "Iteration 503, loss = 0.33735402\n",
      "Iteration 1899, loss = 0.12475698\n",
      "Iteration 428, loss = 0.34508858\n",
      "Iteration 1220, loss = 0.27172563\n",
      "Iteration 747, loss = 0.30719008\n",
      "Iteration 2694, loss = 0.12478306\n",
      "Iteration 504, loss = 0.33722856\n",
      "Iteration 1900, loss = 0.12452351\n",
      "Iteration 604, loss = 0.35317176\n",
      "Iteration 429, loss = 0.34499589\n",
      "Iteration 1901, loss = 0.12439733\n",
      "Iteration 505, loss = 0.33711661\n",
      "Iteration 1902, loss = 0.12421181\n",
      "Iteration 430, loss = 0.34491093\n",
      "Iteration 1221, loss = 0.27158228\n",
      "Iteration 2695, loss = 0.12472986\n",
      "Iteration 506, loss = 0.33700632\n",
      "Iteration 1903, loss = 0.12411693\n",
      "Iteration 431, loss = 0.34481932\n",
      "Iteration 748, loss = 0.30704286\n",
      "Iteration 605, loss = 0.35302534\n",
      "Iteration 507, loss = 0.33695175\n",
      "Iteration 1904, loss = 0.12400271\n",
      "Iteration 2696, loss = 0.12478117\n",
      "Iteration 508, loss = 0.33677924\n",
      "Iteration 432, loss = 0.34472639\n",
      "Iteration 509, loss = 0.33669953\n",
      "Iteration 2697, loss = 0.12458068\n",
      "Iteration 1905, loss = 0.12384914\n",
      "Iteration 510, loss = 0.33655086\n",
      "Iteration 1222, loss = 0.27141532\n",
      "Iteration 433, loss = 0.34463650\n",
      "Iteration 749, loss = 0.30681221\n",
      "Iteration 511, loss = 0.33642915\n",
      "Iteration 512, loss = 0.33633235\n",
      "Iteration 356, loss = 0.42114687\n",
      "Iteration 1906, loss = 0.12377870\n",
      "Iteration 513, loss = 0.33621826\n",
      "Iteration 1223, loss = 0.27133905\n",
      "Iteration 1907, loss = 0.12370275\n",
      "Iteration 514, loss = 0.33613575\n",
      "Iteration 750, loss = 0.30660381\n",
      "Iteration 434, loss = 0.34453153\n",
      "Iteration 515, loss = 0.33600233\n",
      "Iteration 1908, loss = 0.12353389\n",
      "Iteration 1224, loss = 0.27125655\n",
      "Iteration 606, loss = 0.35285075\n",
      "Iteration 516, loss = 0.33586314\n",
      "Iteration 517, loss = 0.33576041\n",
      "Iteration 1909, loss = 0.12333817\n",
      "Iteration 2698, loss = 0.12459977\n",
      "Iteration 1225, loss = 0.27106311\n",
      "Iteration 435, loss = 0.34445304\n",
      "Iteration 518, loss = 0.33569045\n",
      "Iteration 751, loss = 0.30646675\n",
      "Iteration 2699, loss = 0.12440946\n",
      "Iteration 519, loss = 0.33552721\n",
      "Iteration 436, loss = 0.34435447\n",
      "Iteration 520, loss = 0.33542449\n",
      "Iteration 1910, loss = 0.12314470\n",
      "Iteration 1226, loss = 0.27096045\n",
      "Iteration 521, loss = 0.33529185\n",
      "Iteration 522, loss = 0.33520044\n",
      "Iteration 1911, loss = 0.12324172\n",
      "Iteration 752, loss = 0.30626340\n",
      "Iteration 437, loss = 0.34427066\n",
      "Iteration 2700, loss = 0.12445672\n",
      "Iteration 1912, loss = 0.12295809\n",
      "Iteration 523, loss = 0.33508851\n",
      "Iteration 438, loss = 0.34416330\n",
      "Iteration 524, loss = 0.33495501\n",
      "Iteration 753, loss = 0.30615236\n",
      "Iteration 1913, loss = 0.12279481\n",
      "Iteration 1227, loss = 0.27087479\n",
      "Iteration 607, loss = 0.35273116\n",
      "Iteration 439, loss = 0.34407002\n",
      "Iteration 1914, loss = 0.12264247\n",
      "Iteration 525, loss = 0.33487207\n",
      "Iteration 1915, loss = 0.12245529\n",
      "Iteration 440, loss = 0.34398744\n",
      "Iteration 526, loss = 0.33474309\n",
      "Iteration 357, loss = 0.42098136\n",
      "Iteration 754, loss = 0.30584761\n",
      "Iteration 527, loss = 0.33461985\n",
      "Iteration 1916, loss = 0.12233840\n",
      "Iteration 528, loss = 0.33450016\n",
      "Iteration 441, loss = 0.34388553\n",
      "Iteration 529, loss = 0.33442190\n",
      "Iteration 1228, loss = 0.27074929\n",
      "Iteration 1917, loss = 0.12217081\n",
      "Iteration 530, loss = 0.33431610\n",
      "Iteration 755, loss = 0.30566655\n",
      "Iteration 442, loss = 0.34381539\n",
      "Iteration 531, loss = 0.33419234\n",
      "Iteration 608, loss = 0.35258791\n",
      "Iteration 1918, loss = 0.12205574\n",
      "Iteration 532, loss = 0.33406870\n",
      "Iteration 2701, loss = 0.12424563\n",
      "Iteration 533, loss = 0.33402039\n",
      "Iteration 443, loss = 0.34371022\n",
      "Iteration 1229, loss = 0.27065858\n",
      "Iteration 534, loss = 0.33388265\n",
      "Iteration 2702, loss = 0.12415719\n",
      "Iteration 535, loss = 0.33377475\n",
      "Iteration 1919, loss = 0.12192521\n",
      "Iteration 756, loss = 0.30549928\n",
      "Iteration 536, loss = 0.33361013\n",
      "Iteration 1230, loss = 0.27050281\n",
      "Iteration 1920, loss = 0.12183871\n",
      "Iteration 444, loss = 0.34363634\n",
      "Iteration 537, loss = 0.33353793\n",
      "Iteration 2703, loss = 0.12409270\n",
      "Iteration 358, loss = 0.42069711\n",
      "Iteration 2704, loss = 0.12407256\n",
      "Iteration 609, loss = 0.35244720\n",
      "Iteration 445, loss = 0.34354280\n",
      "Iteration 1921, loss = 0.12179789\n",
      "Iteration 538, loss = 0.33340610\n",
      "Iteration 757, loss = 0.30530737\n",
      "Iteration 1922, loss = 0.12145446\n",
      "Iteration 446, loss = 0.34345641\n",
      "Iteration 1231, loss = 0.27042139\n",
      "Iteration 1923, loss = 0.12135824\n",
      "Iteration 539, loss = 0.33329502\n",
      "Iteration 610, loss = 0.35230932\n",
      "Iteration 447, loss = 0.34335156\n",
      "Iteration 1924, loss = 0.12124031\n",
      "Iteration 540, loss = 0.33318607\n",
      "Iteration 758, loss = 0.30506679\n",
      "Iteration 448, loss = 0.34326061\n",
      "Iteration 359, loss = 0.42045105\n",
      "Iteration 541, loss = 0.33310335\n",
      "Iteration 1925, loss = 0.12119968\n",
      "Iteration 1232, loss = 0.27027876\n",
      "Iteration 542, loss = 0.33297983\n",
      "Iteration 611, loss = 0.35220745\n",
      "Iteration 759, loss = 0.30487561\n",
      "Iteration 1926, loss = 0.12097574\n",
      "Iteration 543, loss = 0.33286566\n",
      "Iteration 449, loss = 0.34317034\n",
      "Iteration 1927, loss = 0.12080972\n",
      "Iteration 1233, loss = 0.27018473Iteration 544, loss = 0.33276144\n",
      "\n",
      "Iteration 612, loss = 0.35202844\n",
      "Iteration 760, loss = 0.30471633\n",
      "Iteration 545, loss = 0.33265557\n",
      "Iteration 360, loss = 0.42021725\n",
      "Iteration 1928, loss = 0.12093365\n",
      "Iteration 1234, loss = 0.27005456\n",
      "Iteration 450, loss = 0.34308867\n",
      "Iteration 546, loss = 0.33253299\n",
      "Iteration 1929, loss = 0.12067680\n",
      "Iteration 547, loss = 0.33241783\n",
      "Iteration 1930, loss = 0.12039777\n",
      "Iteration 548, loss = 0.33232445\n",
      "Iteration 613, loss = 0.35194497\n",
      "Iteration 1931, loss = 0.12044993\n",
      "Iteration 1235, loss = 0.26991625\n",
      "Iteration 451, loss = 0.34300686\n",
      "Iteration 549, loss = 0.33221229\n",
      "Iteration 761, loss = 0.30462284\n",
      "Iteration 1932, loss = 0.12011786\n",
      "Iteration 361, loss = 0.41990832\n",
      "Iteration 550, loss = 0.33211131\n",
      "Iteration 452, loss = 0.34290827\n",
      "Iteration 1236, loss = 0.26986072\n",
      "Iteration 453, loss = 0.34282379\n",
      "Iteration 1933, loss = 0.12007414\n",
      "Iteration 551, loss = 0.33199131\n",
      "Iteration 762, loss = 0.30437216\n",
      "Iteration 614, loss = 0.35176732\n",
      "Iteration 552, loss = 0.33188469\n",
      "Iteration 1934, loss = 0.11991307\n",
      "Iteration 454, loss = 0.34277414\n",
      "Iteration 553, loss = 0.33181920\n",
      "Iteration 1237, loss = 0.26969624\n",
      "Iteration 1935, loss = 0.11976453\n",
      "Iteration 455, loss = 0.34265889\n",
      "Iteration 554, loss = 0.33167323\n",
      "Iteration 1936, loss = 0.11968063\n",
      "Iteration 1937, loss = 0.11975920\n",
      "Iteration 555, loss = 0.33158378Iteration 2705, loss = 0.12404412\n",
      "Iteration 763, loss = 0.30407864\n",
      "Iteration 615, loss = 0.35172446\n",
      "Iteration 456, loss = 0.34256260\n",
      "Iteration 362, loss = 0.41969185\n",
      "Iteration 2706, loss = 0.12383436\n",
      "\n",
      "Iteration 1938, loss = 0.11944031\n",
      "Iteration 1238, loss = 0.26962624\n",
      "Iteration 556, loss = 0.33146519\n",
      "Iteration 1939, loss = 0.11927769\n",
      "Iteration 557, loss = 0.33137486\n",
      "Iteration 764, loss = 0.30393264\n",
      "Iteration 1940, loss = 0.11917938\n",
      "Iteration 457, loss = 0.34246124\n",
      "Iteration 558, loss = 0.33127874\n",
      "Iteration 1941, loss = 0.11915818\n",
      "Iteration 559, loss = 0.33115583\n",
      "Iteration 1239, loss = 0.26945545\n",
      "Iteration 2707, loss = 0.12378526\n",
      "Iteration 458, loss = 0.34237179\n",
      "Iteration 363, loss = 0.41947592\n",
      "Iteration 2708, loss = 0.12370278\n",
      "Iteration 1942, loss = 0.11924857\n",
      "Iteration 616, loss = 0.35152750\n",
      "Iteration 560, loss = 0.33108199\n",
      "Iteration 459, loss = 0.34228337\n",
      "Iteration 561, loss = 0.33094772\n",
      "Iteration 765, loss = 0.30377040\n",
      "Iteration 1943, loss = 0.11878629\n",
      "Iteration 1240, loss = 0.26936353\n",
      "Iteration 2709, loss = 0.12360284\n",
      "Iteration 562, loss = 0.33084592\n",
      "Iteration 1241, loss = 0.26921468\n",
      "Iteration 766, loss = 0.30364866\n",
      "Iteration 460, loss = 0.34219687\n",
      "Iteration 563, loss = 0.33074031\n",
      "Iteration 2710, loss = 0.12352347\n",
      "Iteration 617, loss = 0.35137462\n",
      "Iteration 1944, loss = 0.11863090\n",
      "Iteration 2711, loss = 0.12345139\n",
      "Iteration 564, loss = 0.33063495\n",
      "Iteration 1242, loss = 0.26915289\n",
      "Iteration 364, loss = 0.41919078\n",
      "Iteration 1945, loss = 0.11860576\n",
      "Iteration 461, loss = 0.34210790\n",
      "Iteration 565, loss = 0.33054569\n",
      "Iteration 618, loss = 0.35126439\n",
      "Iteration 462, loss = 0.34202970\n",
      "Iteration 767, loss = 0.30336972\n",
      "Iteration 1243, loss = 0.26902537\n",
      "Iteration 566, loss = 0.33043916\n",
      "Iteration 1946, loss = 0.11851336\n",
      "Iteration 2712, loss = 0.12337983\n",
      "Iteration 1244, loss = 0.26887654\n",
      "Iteration 567, loss = 0.33032503\n",
      "Iteration 619, loss = 0.35111390\n",
      "Iteration 463, loss = 0.34192856\n",
      "Iteration 568, loss = 0.33023959\n",
      "Iteration 2713, loss = 0.12332581\n",
      "Iteration 1947, loss = 0.11824522\n",
      "Iteration 768, loss = 0.30321856\n",
      "Iteration 2714, loss = 0.12327922\n",
      "Iteration 1948, loss = 0.11807345\n",
      "Iteration 569, loss = 0.33011774\n",
      "Iteration 2715, loss = 0.12322849\n",
      "Iteration 1245, loss = 0.26878433\n",
      "Iteration 365, loss = 0.41899130\n",
      "Iteration 2716, loss = 0.12320514\n",
      "Iteration 570, loss = 0.33003924\n",
      "Iteration 769, loss = 0.30302932\n",
      "Iteration 571, loss = 0.32992479\n",
      "Iteration 1949, loss = 0.11814272\n",
      "Iteration 2717, loss = 0.12308927\n",
      "Iteration 464, loss = 0.34184634\n",
      "Iteration 572, loss = 0.32982498\n",
      "Iteration 1246, loss = 0.26866613\n",
      "Iteration 573, loss = 0.32969785\n",
      "Iteration 2718, loss = 0.12303902\n",
      "Iteration 1950, loss = 0.11786505\n",
      "Iteration 770, loss = 0.30274170\n",
      "Iteration 574, loss = 0.32961619\n",
      "Iteration 1951, loss = 0.11779015\n",
      "Iteration 575, loss = 0.32951100\n",
      "Iteration 465, loss = 0.34177411\n",
      "Iteration 1952, loss = 0.11762931\n",
      "Iteration 2719, loss = 0.12281760\n",
      "Iteration 1953, loss = 0.11745594\n",
      "Iteration 2720, loss = 0.12278050\n",
      "Iteration 576, loss = 0.32940453\n",
      "Iteration 620, loss = 0.35097758\n",
      "Iteration 466, loss = 0.34167169\n",
      "Iteration 366, loss = 0.41879850\n",
      "Iteration 1247, loss = 0.26856847\n",
      "Iteration 771, loss = 0.30261468\n",
      "Iteration 467, loss = 0.34158195\n",
      "Iteration 577, loss = 0.32930433\n",
      "Iteration 1954, loss = 0.11761944\n",
      "Iteration 2721, loss = 0.12270685\n",
      "Iteration 578, loss = 0.32920528\n",
      "Iteration 1955, loss = 0.11726922\n",
      "Iteration 1248, loss = 0.26843711\n",
      "Iteration 2722, loss = 0.12258912\n",
      "Iteration 579, loss = 0.32910782\n",
      "Iteration 1956, loss = 0.11705340\n",
      "Iteration 580, loss = 0.32901353\n",
      "Iteration 468, loss = 0.34149919\n",
      "Iteration 2723, loss = 0.12250454\n",
      "Iteration 1957, loss = 0.11689956\n",
      "Iteration 772, loss = 0.30238996\n",
      "Iteration 581, loss = 0.32889181\n",
      "Iteration 469, loss = 0.34142586\n",
      "Iteration 1249, loss = 0.26829202\n",
      "Iteration 2724, loss = 0.12253933\n",
      "Iteration 621, loss = 0.35084341\n",
      "Iteration 1958, loss = 0.11694032\n",
      "Iteration 2725, loss = 0.12241101\n",
      "Iteration 470, loss = 0.34133169\n",
      "Iteration 1250, loss = 0.26820591\n",
      "Iteration 582, loss = 0.32879507\n",
      "Iteration 773, loss = 0.30229266\n",
      "Iteration 2726, loss = 0.12243209\n",
      "Iteration 1959, loss = 0.11686825\n",
      "Iteration 367, loss = 0.41848843\n",
      "Iteration 1251, loss = 0.26811528\n",
      "Iteration 583, loss = 0.32869488\n",
      "Iteration 2727, loss = 0.12219827\n",
      "Iteration 471, loss = 0.34124631\n",
      "Iteration 2728, loss = 0.12213968\n",
      "Iteration 1252, loss = 0.26792334\n",
      "Iteration 622, loss = 0.35075829\n",
      "Iteration 2729, loss = 0.12203438\n",
      "Iteration 584, loss = 0.32856684\n",
      "Iteration 1960, loss = 0.11678932\n",
      "Iteration 2730, loss = 0.12200600\n",
      "Iteration 2731, loss = 0.12193208\n",
      "Iteration 472, loss = 0.34115655\n",
      "Iteration 1961, loss = 0.11644284\n",
      "Iteration 1253, loss = 0.26785430\n",
      "Iteration 2732, loss = 0.12193136\n",
      "Iteration 774, loss = 0.30197806\n",
      "Iteration 623, loss = 0.35058336Iteration 585, loss = 0.32847569\n",
      "\n",
      "Iteration 1254, loss = 0.26780488\n",
      "Iteration 586, loss = 0.32839815\n",
      "Iteration 1962, loss = 0.11639117\n",
      "Iteration 473, loss = 0.34107821\n",
      "Iteration 1963, loss = 0.11628921\n",
      "Iteration 474, loss = 0.34098222\n",
      "Iteration 587, loss = 0.32828455\n",
      "Iteration 1255, loss = 0.26762760\n",
      "Iteration 588, loss = 0.32817543\n",
      "Iteration 475, loss = 0.34090024\n",
      "Iteration 589, loss = 0.32809588\n",
      "Iteration 624, loss = 0.35048136\n",
      "Iteration 1964, loss = 0.11605847\n",
      "Iteration 368, loss = 0.41827669\n",
      "Iteration 2733, loss = 0.12174117\n",
      "Iteration 590, loss = 0.32797975\n",
      "Iteration 775, loss = 0.30175343\n",
      "Iteration 591, loss = 0.32789445\n",
      "Iteration 2734, loss = 0.12166409\n",
      "Iteration 592, loss = 0.32777217\n",
      "Iteration 476, loss = 0.34081026\n",
      "Iteration 1965, loss = 0.11592590\n",
      "Iteration 593, loss = 0.32766289\n",
      "Iteration 1256, loss = 0.26748811\n",
      "Iteration 594, loss = 0.32759986\n",
      "Iteration 2735, loss = 0.12161286\n",
      "Iteration 595, loss = 0.32748255\n",
      "Iteration 625, loss = 0.35031499\n",
      "Iteration 1966, loss = 0.11586259\n",
      "Iteration 596, loss = 0.32740522\n",
      "Iteration 477, loss = 0.34072523\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1257, loss = 0.26734985\n",
      "Iteration 597, loss = 0.32727909\n",
      "Iteration 776, loss = 0.30158411\n",
      "Iteration 598, loss = 0.32718444\n",
      "Iteration 1258, loss = 0.26727073\n",
      "Iteration 599, loss = 0.32708338\n",
      "Iteration 600, loss = 0.32699481\n",
      "Iteration 777, loss = 0.30144317\n",
      "Iteration 1259, loss = 0.26715392\n",
      "Iteration 626, loss = 0.35019247\n",
      "Iteration 1, loss = 0.71934982\n",
      "Iteration 601, loss = 0.32687337\n",
      "Iteration 1260, loss = 0.26704736\n",
      "Iteration 602, loss = 0.32678561\n",
      "Iteration 778, loss = 0.30116181\n",
      "Iteration 1967, loss = 0.11571926\n",
      "Iteration 2, loss = 0.71796058\n",
      "Iteration 627, loss = 0.35004357\n",
      "Iteration 1261, loss = 0.26689959\n",
      "Iteration 603, loss = 0.32669760\n",
      "Iteration 604, loss = 0.32659404\n",
      "Iteration 779, loss = 0.30105893\n",
      "Iteration 1968, loss = 0.11553103\n",
      "Iteration 3, loss = 0.71578089\n",
      "Iteration 1262, loss = 0.26678327\n",
      "Iteration 605, loss = 0.32649244\n",
      "Iteration 606, loss = 0.32640241\n",
      "Iteration 1263, loss = 0.26666714\n",
      "Iteration 780, loss = 0.30081731\n",
      "Iteration 2736, loss = 0.12148167\n",
      "Iteration 628, loss = 0.34989827\n",
      "Iteration 1969, loss = 0.11541049\n",
      "Iteration 607, loss = 0.32629090\n",
      "Iteration 1970, loss = 0.11534525\n",
      "Iteration 2737, loss = 0.12140576\n",
      "Iteration 608, loss = 0.32623298\n",
      "Iteration 781, loss = 0.30062319\n",
      "Iteration 4, loss = 0.71319626\n",
      "Iteration 2738, loss = 0.12133682\n",
      "Iteration 1971, loss = 0.11518471\n",
      "Iteration 2739, loss = 0.12130281\n",
      "Iteration 782, loss = 0.30040735\n",
      "Iteration 2740, loss = 0.12125806\n",
      "Iteration 609, loss = 0.32612331\n",
      "Iteration 369, loss = 0.41802594\n",
      "Iteration 610, loss = 0.32600965\n",
      "Iteration 2741, loss = 0.12118875\n",
      "Iteration 629, loss = 0.34980724\n",
      "Iteration 611, loss = 0.32595950\n",
      "Iteration 1264, loss = 0.26664354\n",
      "Iteration 783, loss = 0.30022451\n",
      "Iteration 612, loss = 0.32580931\n",
      "Iteration 613, loss = 0.32575547\n",
      "Iteration 2742, loss = 0.12107911\n",
      "Iteration 5, loss = 0.71015202\n",
      "Iteration 614, loss = 0.32562812\n",
      "Iteration 784, loss = 0.30026243\n",
      "Iteration 630, loss = 0.34966225\n",
      "Iteration 2743, loss = 0.12099328\n",
      "Iteration 370, loss = 0.41780929\n",
      "Iteration 6, loss = 0.70681390\n",
      "Iteration 1265, loss = 0.26642999\n",
      "Iteration 2744, loss = 0.12093621\n",
      "Iteration 615, loss = 0.32553311\n",
      "Iteration 7, loss = 0.70340648\n",
      "Iteration 2745, loss = 0.12091318\n",
      "Iteration 616, loss = 0.32545164\n",
      "Iteration 2746, loss = 0.12079499\n",
      "Iteration 2747, loss = 0.12063726\n",
      "Iteration 617, loss = 0.32538181\n",
      "Iteration 8, loss = 0.69981681\n",
      "Iteration 1266, loss = 0.26634783\n",
      "Iteration 618, loss = 0.32526163\n",
      "Iteration 785, loss = 0.29979734\n",
      "Iteration 2748, loss = 0.12059035\n",
      "Iteration 619, loss = 0.32515182\n",
      "Iteration 631, loss = 0.34954227\n",
      "Iteration 1267, loss = 0.26620062\n",
      "Iteration 620, loss = 0.32505745\n",
      "Iteration 2749, loss = 0.12053630\n",
      "Iteration 621, loss = 0.32497181\n",
      "Iteration 2750, loss = 0.12046884\n",
      "Iteration 622, loss = 0.32489734\n",
      "Iteration 2751, loss = 0.12058699\n",
      "Iteration 371, loss = 0.41753684\n",
      "Iteration 9, loss = 0.69614201\n",
      "Iteration 2752, loss = 0.12036046\n",
      "Iteration 623, loss = 0.32478488\n",
      "Iteration 1268, loss = 0.26612400\n",
      "Iteration 786, loss = 0.29964266\n",
      "Iteration 2753, loss = 0.12026116\n",
      "Iteration 624, loss = 0.32469394\n",
      "Iteration 2754, loss = 0.12014316\n",
      "Iteration 2755, loss = 0.12014106\n",
      "Iteration 632, loss = 0.34940881\n",
      "Iteration 1269, loss = 0.26607051\n",
      "Iteration 10, loss = 0.69243448\n",
      "Iteration 625, loss = 0.32464447\n",
      "Iteration 626, loss = 0.32450031\n",
      "Iteration 2756, loss = 0.11997299\n",
      "Iteration 787, loss = 0.29947069\n",
      "Iteration 627, loss = 0.32442285\n",
      "Iteration 11, loss = 0.68886990\n",
      "Iteration 2757, loss = 0.11997971\n",
      "Iteration 628, loss = 0.32433300\n",
      "Iteration 2758, loss = 0.11985958\n",
      "Iteration 1270, loss = 0.26584442\n",
      "Iteration 2759, loss = 0.11975020\n",
      "Iteration 12, loss = 0.68508082\n",
      "Iteration 633, loss = 0.34927594\n",
      "Iteration 2760, loss = 0.11971404\n",
      "Iteration 629, loss = 0.32425909\n",
      "Iteration 2761, loss = 0.11963765\n",
      "Iteration 372, loss = 0.41729915\n",
      "Iteration 2762, loss = 0.11958592\n",
      "Iteration 630, loss = 0.32413973\n",
      "Iteration 788, loss = 0.29921508\n",
      "Iteration 13, loss = 0.68143184\n",
      "Iteration 631, loss = 0.32406513\n",
      "Iteration 1271, loss = 0.26577494\n",
      "Iteration 632, loss = 0.32395478\n",
      "Iteration 2763, loss = 0.11943697\n",
      "Iteration 789, loss = 0.29912300\n",
      "Iteration 633, loss = 0.32386256\n",
      "Iteration 2764, loss = 0.11943788\n",
      "Iteration 14, loss = 0.67766544\n",
      "Iteration 373, loss = 0.41711970\n",
      "Iteration 634, loss = 0.34914017\n",
      "Iteration 634, loss = 0.32377249\n",
      "Iteration 790, loss = 0.29883459\n",
      "Iteration 635, loss = 0.32367758\n",
      "Iteration 1972, loss = 0.11531490\n",
      "Iteration 2765, loss = 0.11931882\n",
      "Iteration 2766, loss = 0.11924062\n",
      "Iteration 1272, loss = 0.26565884\n",
      "Iteration 15, loss = 0.67401627\n",
      "Iteration 2767, loss = 0.11917584\n",
      "Iteration 791, loss = 0.29860847\n",
      "Iteration 16, loss = 0.67029325\n",
      "Iteration 636, loss = 0.32358900\n",
      "Iteration 635, loss = 0.34903185\n",
      "Iteration 2768, loss = 0.11922959\n",
      "Iteration 637, loss = 0.32348885\n",
      "Iteration 638, loss = 0.32339518\n",
      "Iteration 792, loss = 0.29846007\n",
      "Iteration 2769, loss = 0.11906503\n",
      "Iteration 1273, loss = 0.26549535\n",
      "Iteration 17, loss = 0.66677853\n",
      "Iteration 374, loss = 0.41679306\n",
      "Iteration 639, loss = 0.32332232\n",
      "Iteration 18, loss = 0.66304994\n",
      "Iteration 640, loss = 0.32322682\n",
      "Iteration 1274, loss = 0.26544057\n",
      "Iteration 641, loss = 0.32313188\n",
      "Iteration 2770, loss = 0.11895177\n",
      "Iteration 642, loss = 0.32305819\n",
      "Iteration 19, loss = 0.65954589\n",
      "Iteration 1275, loss = 0.26532850\n",
      "Iteration 643, loss = 0.32300725\n",
      "Iteration 793, loss = 0.29820448\n",
      "Iteration 2771, loss = 0.11891997\n",
      "Iteration 636, loss = 0.34886491\n",
      "Iteration 20, loss = 0.65600103\n",
      "Iteration 644, loss = 0.32284721\n",
      "Iteration 2772, loss = 0.11881907\n",
      "Iteration 1276, loss = 0.26522946\n",
      "Iteration 2773, loss = 0.11872735\n",
      "Iteration 375, loss = 0.41657205\n",
      "Iteration 2774, loss = 0.11863408\n",
      "Iteration 645, loss = 0.32274390\n",
      "Iteration 2775, loss = 0.11865763\n",
      "Iteration 794, loss = 0.29807672\n",
      "Iteration 646, loss = 0.32266786\n",
      "Iteration 2776, loss = 0.11849094\n",
      "Iteration 21, loss = 0.65241290\n",
      "Iteration 647, loss = 0.32256488\n",
      "Iteration 2777, loss = 0.11862988\n",
      "Iteration 1277, loss = 0.26504789\n",
      "Iteration 648, loss = 0.32248031\n",
      "Iteration 2778, loss = 0.11833542\n",
      "Iteration 637, loss = 0.34876092\n",
      "Iteration 649, loss = 0.32242233\n",
      "Iteration 650, loss = 0.32231363\n",
      "Iteration 2779, loss = 0.11836139\n",
      "Iteration 22, loss = 0.64898813\n",
      "Iteration 651, loss = 0.32219967\n",
      "Iteration 652, loss = 0.32214654\n",
      "Iteration 2780, loss = 0.11818538\n",
      "Iteration 795, loss = 0.29789987\n",
      "Iteration 653, loss = 0.32201500\n",
      "Iteration 1278, loss = 0.26495265\n",
      "Iteration 654, loss = 0.32193185\n",
      "Iteration 2781, loss = 0.11811555\n",
      "Iteration 23, loss = 0.64536981\n",
      "Iteration 655, loss = 0.32184255\n",
      "Iteration 656, loss = 0.32173079\n",
      "Iteration 796, loss = 0.29769296\n",
      "Iteration 2782, loss = 0.11806565\n",
      "Iteration 1279, loss = 0.26485187\n",
      "Iteration 657, loss = 0.32164825\n",
      "Iteration 638, loss = 0.34865735\n",
      "Iteration 376, loss = 0.41641190\n",
      "Iteration 658, loss = 0.32156856\n",
      "Iteration 2783, loss = 0.11796684\n",
      "Iteration 1973, loss = 0.11507308\n",
      "Iteration 659, loss = 0.32149224\n",
      "Iteration 1280, loss = 0.26473528\n",
      "Iteration 660, loss = 0.32138629\n",
      "Iteration 2784, loss = 0.11794660\n",
      "Iteration 24, loss = 0.64184017\n",
      "Iteration 661, loss = 0.32127266\n",
      "Iteration 2785, loss = 0.11783565\n",
      "Iteration 662, loss = 0.32120920\n",
      "Iteration 2786, loss = 0.11777516\n",
      "Iteration 663, loss = 0.32109964\n",
      "Iteration 2787, loss = 0.11777744\n",
      "Iteration 639, loss = 0.34848787\n",
      "Iteration 664, loss = 0.32101330\n",
      "Iteration 2788, loss = 0.11770128\n",
      "Iteration 665, loss = 0.32092955\n",
      "Iteration 797, loss = 0.29746326\n",
      "Iteration 1281, loss = 0.26462596\n",
      "Iteration 666, loss = 0.32081813\n",
      "Iteration 2789, loss = 0.11757290\n",
      "Iteration 25, loss = 0.63845521\n",
      "Iteration 667, loss = 0.32074286\n",
      "Iteration 2790, loss = 0.11752466\n",
      "Iteration 798, loss = 0.29724055\n",
      "Iteration 2791, loss = 0.11741653\n",
      "Iteration 668, loss = 0.32064822\n",
      "Iteration 1282, loss = 0.26449739\n",
      "Iteration 2792, loss = 0.11741011\n",
      "Iteration 640, loss = 0.34838031\n",
      "Iteration 669, loss = 0.32057794\n",
      "Iteration 1974, loss = 0.11477635\n",
      "Iteration 26, loss = 0.63501019\n",
      "Iteration 377, loss = 0.41612470\n",
      "Iteration 2793, loss = 0.11727465\n",
      "Iteration 799, loss = 0.29705560\n",
      "Iteration 670, loss = 0.32051041\n",
      "Iteration 2794, loss = 0.11716549\n",
      "Iteration 1283, loss = 0.26437380\n",
      "Iteration 2795, loss = 0.11713061\n",
      "Iteration 671, loss = 0.32037142\n",
      "Iteration 2796, loss = 0.11703094\n",
      "Iteration 1975, loss = 0.11468151\n",
      "Iteration 672, loss = 0.32029290\n",
      "Iteration 2797, loss = 0.11695536\n",
      "Iteration 673, loss = 0.32018322\n",
      "Iteration 27, loss = 0.63170592\n",
      "Iteration 1976, loss = 0.11455995\n",
      "Iteration 800, loss = 0.29686787\n",
      "Iteration 2798, loss = 0.11690702\n",
      "Iteration 674, loss = 0.32010739\n",
      "Iteration 2799, loss = 0.11687300\n",
      "Iteration 1977, loss = 0.11448620\n",
      "Iteration 28, loss = 0.62835971\n",
      "Iteration 641, loss = 0.34821765\n",
      "Iteration 1284, loss = 0.26429814\n",
      "Iteration 675, loss = 0.32002034\n",
      "Iteration 2800, loss = 0.11685745\n",
      "Iteration 676, loss = 0.31995383\n",
      "Iteration 642, loss = 0.34811112\n",
      "Iteration 1978, loss = 0.11438173\n",
      "Iteration 2801, loss = 0.11669847\n",
      "Iteration 1285, loss = 0.26417859\n",
      "Iteration 677, loss = 0.31981989\n",
      "Iteration 801, loss = 0.29667884\n",
      "Iteration 1286, loss = 0.26413356\n",
      "Iteration 1979, loss = 0.11421718\n",
      "Iteration 2802, loss = 0.11660592\n",
      "Iteration 678, loss = 0.31976186\n",
      "Iteration 643, loss = 0.34794486\n",
      "Iteration 378, loss = 0.41590683\n",
      "Iteration 1287, loss = 0.26407707\n",
      "Iteration 2803, loss = 0.11654600\n",
      "Iteration 679, loss = 0.31965686\n",
      "Iteration 1980, loss = 0.11410465\n",
      "Iteration 2804, loss = 0.11646342\n",
      "Iteration 802, loss = 0.29650217\n",
      "Iteration 2805, loss = 0.11641209\n",
      "Iteration 1981, loss = 0.11398837\n",
      "Iteration 2806, loss = 0.11632448\n",
      "Iteration 680, loss = 0.31959006\n",
      "Iteration 2807, loss = 0.11630580\n",
      "Iteration 29, loss = 0.62496055\n",
      "Iteration 803, loss = 0.29637184\n",
      "Iteration 1982, loss = 0.11391891Iteration 681, loss = 0.31946205\n",
      "\n",
      "Iteration 2808, loss = 0.11624805\n",
      "Iteration 804, loss = 0.29619639\n",
      "Iteration 1288, loss = 0.26386313\n",
      "Iteration 682, loss = 0.31938842\n",
      "Iteration 2809, loss = 0.11610216\n",
      "Iteration 379, loss = 0.41572738\n",
      "Iteration 1983, loss = 0.11372302\n",
      "Iteration 30, loss = 0.62161387\n",
      "Iteration 683, loss = 0.31929995\n",
      "Iteration 2810, loss = 0.11602717\n",
      "Iteration 684, loss = 0.31921484\n",
      "Iteration 1984, loss = 0.11360568\n",
      "Iteration 644, loss = 0.34782359\n",
      "Iteration 31, loss = 0.61829119\n",
      "Iteration 1289, loss = 0.26371588\n",
      "Iteration 2811, loss = 0.11597276\n",
      "Iteration 1985, loss = 0.11358966\n",
      "Iteration 380, loss = 0.41547283Iteration 685, loss = 0.31912208\n",
      "\n",
      "Iteration 2812, loss = 0.11593872\n",
      "Iteration 686, loss = 0.31903139\n",
      "Iteration 1986, loss = 0.11341885\n",
      "Iteration 32, loss = 0.61500486\n",
      "Iteration 687, loss = 0.31893415\n",
      "Iteration 2813, loss = 0.11581782\n",
      "Iteration 688, loss = 0.31884475\n",
      "Iteration 2814, loss = 0.11573333\n",
      "Iteration 689, loss = 0.31877136\n",
      "Iteration 805, loss = 0.29588355\n",
      "Iteration 690, loss = 0.31865256\n",
      "Iteration 1987, loss = 0.11332565\n",
      "Iteration 691, loss = 0.31857487\n",
      "Iteration 645, loss = 0.34768318\n",
      "Iteration 1290, loss = 0.26358413\n",
      "Iteration 2815, loss = 0.11569215\n",
      "Iteration 33, loss = 0.61169951\n",
      "Iteration 692, loss = 0.31849045\n",
      "Iteration 693, loss = 0.31840858\n",
      "Iteration 2816, loss = 0.11563538\n",
      "Iteration 1988, loss = 0.11311503\n",
      "Iteration 694, loss = 0.31830194\n",
      "Iteration 1291, loss = 0.26351847\n",
      "Iteration 2817, loss = 0.11558256\n",
      "Iteration 695, loss = 0.31822371\n",
      "Iteration 1989, loss = 0.11302756\n",
      "Iteration 696, loss = 0.31812921\n",
      "Iteration 1292, loss = 0.26332699\n",
      "Iteration 34, loss = 0.60849583\n",
      "Iteration 697, loss = 0.31804141\n",
      "Iteration 1990, loss = 0.11289209\n",
      "Iteration 646, loss = 0.34757715\n",
      "Iteration 806, loss = 0.29566911\n",
      "Iteration 2818, loss = 0.11552690\n",
      "Iteration 1293, loss = 0.26335296\n",
      "Iteration 1991, loss = 0.11273055\n",
      "Iteration 381, loss = 0.41522821\n",
      "Iteration 1992, loss = 0.11262065\n",
      "Iteration 2819, loss = 0.11536929\n",
      "Iteration 698, loss = 0.31796132\n",
      "Iteration 1993, loss = 0.11271135\n",
      "Iteration 1994, loss = 0.11287649\n",
      "Iteration 807, loss = 0.29550261\n",
      "Iteration 2820, loss = 0.11539148\n",
      "Iteration 699, loss = 0.31787068\n",
      "Iteration 35, loss = 0.60526830\n",
      "Iteration 1294, loss = 0.26309939\n",
      "Iteration 1995, loss = 0.11236967\n",
      "Iteration 647, loss = 0.34752532\n",
      "Iteration 2821, loss = 0.11525439\n",
      "Iteration 36, loss = 0.60210445\n",
      "Iteration 700, loss = 0.31779970\n",
      "Iteration 1996, loss = 0.11210745\n",
      "Iteration 2822, loss = 0.11516021\n",
      "Iteration 1295, loss = 0.26304301\n",
      "Iteration 37, loss = 0.59880601\n",
      "Iteration 808, loss = 0.29536588\n",
      "Iteration 701, loss = 0.31770106\n",
      "Iteration 1997, loss = 0.11219442\n",
      "Iteration 2823, loss = 0.11510930\n",
      "Iteration 1296, loss = 0.26291139\n",
      "Iteration 38, loss = 0.59569706\n",
      "Iteration 702, loss = 0.31760066\n",
      "Iteration 703, loss = 0.31754642\n",
      "Iteration 39, loss = 0.59254877\n",
      "Iteration 1998, loss = 0.11194163\n",
      "Iteration 704, loss = 0.31742665\n",
      "Iteration 382, loss = 0.41494233\n",
      "Iteration 1999, loss = 0.11190969\n",
      "Iteration 705, loss = 0.31734000\n",
      "Iteration 1297, loss = 0.26278450\n",
      "Iteration 706, loss = 0.31724167\n",
      "Iteration 2824, loss = 0.11509052\n",
      "Iteration 707, loss = 0.31716148\n",
      "Iteration 1298, loss = 0.26265446\n",
      "Iteration 2000, loss = 0.11169348\n",
      "Iteration 708, loss = 0.31707513\n",
      "Iteration 648, loss = 0.34733932\n",
      "Iteration 709, loss = 0.31697948\n",
      "Iteration 2825, loss = 0.11494128\n",
      "Iteration 383, loss = 0.41471627\n",
      "Iteration 40, loss = 0.58955015\n",
      "Iteration 710, loss = 0.31688862\n",
      "Iteration 2826, loss = 0.11489619Iteration 809, loss = 0.29516988\n",
      "Iteration 1299, loss = 0.26255481\n",
      "Iteration 2001, loss = 0.11190175\n",
      "\n",
      "Iteration 2827, loss = 0.11485205\n",
      "Iteration 711, loss = 0.31679183\n",
      "Iteration 712, loss = 0.31672290\n",
      "Iteration 713, loss = 0.31662158\n",
      "Iteration 2002, loss = 0.11153037\n",
      "Iteration 2828, loss = 0.11477076\n",
      "Iteration 810, loss = 0.29490946\n",
      "Iteration 649, loss = 0.34721599\n",
      "Iteration 2829, loss = 0.11474372\n",
      "Iteration 1300, loss = 0.26246194\n",
      "Iteration 41, loss = 0.58637143\n",
      "Iteration 714, loss = 0.31653854\n",
      "Iteration 2830, loss = 0.11467317\n",
      "Iteration 2831, loss = 0.11462195\n",
      "Iteration 715, loss = 0.31645045\n",
      "Iteration 650, loss = 0.34706069\n",
      "Iteration 2832, loss = 0.11442974\n",
      "Iteration 1301, loss = 0.26234140\n",
      "Iteration 716, loss = 0.31637312\n",
      "Iteration 2003, loss = 0.11131961\n",
      "Iteration 811, loss = 0.29476947\n",
      "Iteration 2833, loss = 0.11443234\n",
      "Iteration 717, loss = 0.31627037\n",
      "Iteration 1302, loss = 0.26217916\n",
      "Iteration 2834, loss = 0.11437767\n",
      "Iteration 2004, loss = 0.11125267\n",
      "Iteration 718, loss = 0.31619927\n",
      "Iteration 2835, loss = 0.11428367\n",
      "Iteration 2005, loss = 0.11121368\n",
      "Iteration 812, loss = 0.29449483\n",
      "Iteration 2006, loss = 0.11098348\n",
      "Iteration 2836, loss = 0.11419818\n",
      "Iteration 719, loss = 0.31609432\n",
      "Iteration 42, loss = 0.58345676\n",
      "Iteration 384, loss = 0.41451425\n",
      "Iteration 720, loss = 0.31603443\n",
      "Iteration 813, loss = 0.29437982\n",
      "Iteration 2007, loss = 0.11092863\n",
      "Iteration 2837, loss = 0.11414684\n",
      "Iteration 721, loss = 0.31593889\n",
      "Iteration 43, loss = 0.58027329\n",
      "Iteration 1303, loss = 0.26207152\n",
      "Iteration 722, loss = 0.31588558\n",
      "Iteration 651, loss = 0.34692067\n",
      "Iteration 2838, loss = 0.11405023\n",
      "Iteration 2008, loss = 0.11074307\n",
      "Iteration 723, loss = 0.31575653\n",
      "Iteration 2839, loss = 0.11403127Iteration 724, loss = 0.31569231\n",
      "\n",
      "Iteration 2009, loss = 0.11067960\n",
      "Iteration 814, loss = 0.29427138\n",
      "Iteration 44, loss = 0.57728912\n",
      "Iteration 725, loss = 0.31560183\n",
      "Iteration 2840, loss = 0.11390190\n",
      "Iteration 1304, loss = 0.26193989\n",
      "Iteration 726, loss = 0.31550898\n",
      "Iteration 2010, loss = 0.11047506\n",
      "Iteration 2841, loss = 0.11385573\n",
      "Iteration 45, loss = 0.57435537\n",
      "Iteration 652, loss = 0.34684539\n",
      "Iteration 385, loss = 0.41426893\n",
      "Iteration 1305, loss = 0.26187476\n",
      "Iteration 2011, loss = 0.11066385\n",
      "Iteration 727, loss = 0.31541799\n",
      "Iteration 2842, loss = 0.11372477\n",
      "Iteration 728, loss = 0.31532671\n",
      "Iteration 815, loss = 0.29394353\n",
      "Iteration 1306, loss = 0.26172125\n",
      "Iteration 2012, loss = 0.11046361\n",
      "Iteration 2843, loss = 0.11367228\n",
      "Iteration 46, loss = 0.57138427\n",
      "Iteration 729, loss = 0.31523765\n",
      "Iteration 2844, loss = 0.11365172\n",
      "Iteration 730, loss = 0.31516568\n",
      "Iteration 2845, loss = 0.11358625\n",
      "Iteration 731, loss = 0.31508391\n",
      "Iteration 47, loss = 0.56851097\n",
      "Iteration 2846, loss = 0.11346595\n",
      "Iteration 2013, loss = 0.11019157\n",
      "Iteration 732, loss = 0.31499222\n",
      "Iteration 653, loss = 0.34667348\n",
      "Iteration 1307, loss = 0.26161300\n",
      "Iteration 48, loss = 0.56562351\n",
      "Iteration 733, loss = 0.31493801\n",
      "Iteration 2847, loss = 0.11344028\n",
      "Iteration 2014, loss = 0.11016387\n",
      "Iteration 2848, loss = 0.11335523\n",
      "Iteration 816, loss = 0.29377137\n",
      "Iteration 49, loss = 0.56269134\n",
      "Iteration 2849, loss = 0.11328615\n",
      "Iteration 734, loss = 0.31481368\n",
      "Iteration 654, loss = 0.34653680\n",
      "Iteration 2850, loss = 0.11331146\n",
      "Iteration 2015, loss = 0.11014221\n",
      "Iteration 50, loss = 0.55985844\n",
      "Iteration 2851, loss = 0.11310821\n",
      "Iteration 1308, loss = 0.26148960\n",
      "Iteration 735, loss = 0.31472504\n",
      "Iteration 2852, loss = 0.11311581\n",
      "Iteration 386, loss = 0.41404978\n",
      "Iteration 2853, loss = 0.11296536\n",
      "Iteration 2016, loss = 0.10976499\n",
      "Iteration 51, loss = 0.55705944\n",
      "Iteration 655, loss = 0.34639549\n",
      "Iteration 2854, loss = 0.11296635\n",
      "Iteration 2855, loss = 0.11288902\n",
      "Iteration 817, loss = 0.29354816\n",
      "Iteration 1309, loss = 0.26139251\n",
      "Iteration 2017, loss = 0.10998105\n",
      "Iteration 52, loss = 0.55425151\n",
      "Iteration 2856, loss = 0.11279323\n",
      "Iteration 736, loss = 0.31467610\n",
      "Iteration 2857, loss = 0.11283635\n",
      "Iteration 2018, loss = 0.10975165\n",
      "Iteration 818, loss = 0.29338384\n",
      "Iteration 2858, loss = 0.11266486\n",
      "Iteration 2859, loss = 0.11257656\n",
      "Iteration 53, loss = 0.55147910\n",
      "Iteration 737, loss = 0.31456268\n",
      "Iteration 656, loss = 0.34628927\n",
      "Iteration 2019, loss = 0.10954470\n",
      "Iteration 2860, loss = 0.11250635\n",
      "Iteration 2861, loss = 0.11246397\n",
      "Iteration 819, loss = 0.29321411\n",
      "Iteration 54, loss = 0.54872860\n",
      "Iteration 738, loss = 0.31446802\n",
      "Iteration 2862, loss = 0.11235227\n",
      "Iteration 1310, loss = 0.26126495\n",
      "Iteration 2863, loss = 0.11226271\n",
      "Iteration 2020, loss = 0.10935856\n",
      "Iteration 55, loss = 0.54603314\n",
      "Iteration 2864, loss = 0.11224531\n",
      "Iteration 387, loss = 0.41377607\n",
      "Iteration 2865, loss = 0.11211353\n",
      "Iteration 739, loss = 0.31438353\n",
      "Iteration 657, loss = 0.34612583\n",
      "Iteration 56, loss = 0.54337856\n",
      "Iteration 2021, loss = 0.10928654\n",
      "Iteration 2866, loss = 0.11207302\n",
      "Iteration 740, loss = 0.31431847\n",
      "Iteration 57, loss = 0.54067951\n",
      "Iteration 2022, loss = 0.10904020\n",
      "Iteration 2867, loss = 0.11207325\n",
      "Iteration 1311, loss = 0.26115231\n",
      "Iteration 741, loss = 0.31422850\n",
      "Iteration 820, loss = 0.29296286\n",
      "Iteration 2868, loss = 0.11192096\n",
      "Iteration 658, loss = 0.34600084\n",
      "Iteration 2023, loss = 0.10910210\n",
      "Iteration 2869, loss = 0.11186418\n",
      "Iteration 388, loss = 0.41354939\n",
      "Iteration 2024, loss = 0.10918679\n",
      "Iteration 58, loss = 0.53811537\n",
      "Iteration 742, loss = 0.31413251\n",
      "Iteration 2025, loss = 0.10870710\n",
      "Iteration 821, loss = 0.29277677\n",
      "Iteration 1312, loss = 0.26106453\n",
      "Iteration 743, loss = 0.31406096\n",
      "Iteration 2026, loss = 0.10867989\n",
      "Iteration 2870, loss = 0.11185235\n",
      "Iteration 59, loss = 0.53555596\n",
      "Iteration 2871, loss = 0.11181761\n",
      "Iteration 2027, loss = 0.10849837\n",
      "Iteration 2872, loss = 0.11166134\n",
      "Iteration 659, loss = 0.34593232\n",
      "Iteration 744, loss = 0.31397633\n",
      "Iteration 60, loss = 0.53291996\n",
      "Iteration 2873, loss = 0.11154679\n",
      "Iteration 1313, loss = 0.26094660\n",
      "Iteration 2874, loss = 0.11154262\n",
      "Iteration 2028, loss = 0.10841067\n",
      "Iteration 2875, loss = 0.11141889\n",
      "Iteration 1314, loss = 0.26078539\n",
      "Iteration 745, loss = 0.31388556\n",
      "Iteration 61, loss = 0.53040697\n",
      "Iteration 2876, loss = 0.11134172\n",
      "Iteration 2029, loss = 0.10885375\n",
      "Iteration 389, loss = 0.41338771\n",
      "Iteration 746, loss = 0.31380345Iteration 1315, loss = 0.26073465\n",
      "\n",
      "Iteration 660, loss = 0.34576059\n",
      "Iteration 747, loss = 0.31372585\n",
      "Iteration 2877, loss = 0.11128287\n",
      "Iteration 822, loss = 0.29255165\n",
      "Iteration 748, loss = 0.31363124\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1316, loss = 0.26053893\n",
      "Iteration 2030, loss = 0.10822996\n",
      "Iteration 62, loss = 0.52794029\n",
      "Iteration 2878, loss = 0.11120837\n",
      "Iteration 1317, loss = 0.26043347\n",
      "Iteration 661, loss = 0.34565527\n",
      "Iteration 63, loss = 0.52550612\n",
      "Iteration 2031, loss = 0.10814313\n",
      "Iteration 2879, loss = 0.11112101\n",
      "Iteration 390, loss = 0.41313677\n",
      "Iteration 1318, loss = 0.26032661\n",
      "Iteration 823, loss = 0.29241247\n",
      "Iteration 2032, loss = 0.10811267\n",
      "Iteration 2033, loss = 0.10823732\n",
      "Iteration 1319, loss = 0.26020993\n",
      "Iteration 2034, loss = 0.10782832\n",
      "Iteration 2880, loss = 0.11105313\n",
      "Iteration 1, loss = 0.78975559\n",
      "Iteration 2881, loss = 0.11100501\n",
      "Iteration 64, loss = 0.52309635\n",
      "Iteration 2882, loss = 0.11094218\n",
      "Iteration 824, loss = 0.29216290\n",
      "Iteration 2035, loss = 0.10768527\n",
      "Iteration 391, loss = 0.41292209\n",
      "Iteration 2883, loss = 0.11086962\n",
      "Iteration 2036, loss = 0.10751528\n",
      "Iteration 1320, loss = 0.26009619\n",
      "Iteration 2, loss = 0.78797719\n",
      "Iteration 2884, loss = 0.11080428\n",
      "Iteration 2037, loss = 0.10746190\n",
      "Iteration 825, loss = 0.29198380\n",
      "Iteration 662, loss = 0.34550584\n",
      "Iteration 65, loss = 0.52066893\n",
      "Iteration 2885, loss = 0.11077834\n",
      "Iteration 1321, loss = 0.25996103\n",
      "Iteration 2886, loss = 0.11068724\n",
      "Iteration 2038, loss = 0.10743293\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 66, loss = 0.51833045\n",
      "Iteration 826, loss = 0.29180811\n",
      "Iteration 1322, loss = 0.25987482\n",
      "Iteration 3, loss = 0.78533426\n",
      "Iteration 2039, loss = 0.10720624\n",
      "Iteration 67, loss = 0.51600201\n",
      "Iteration 827, loss = 0.29162652\n",
      "Iteration 663, loss = 0.34539685\n",
      "Iteration 68, loss = 0.51365298\n",
      "Iteration 1323, loss = 0.25976553\n",
      "Iteration 2040, loss = 0.10713021\n",
      "Iteration 2041, loss = 0.10700843\n",
      "Iteration 1324, loss = 0.25969246\n",
      "Iteration 392, loss = 0.41264136\n",
      "Iteration 828, loss = 0.29136560\n",
      "Iteration 4, loss = 0.78213526\n",
      "Iteration 664, loss = 0.34522024\n",
      "Iteration 2042, loss = 0.10680569\n",
      "Iteration 69, loss = 0.51143174\n",
      "Iteration 1, loss = 0.77200343\n",
      "Iteration 1325, loss = 0.25949489\n",
      "Iteration 2, loss = 0.77068836\n",
      "Iteration 70, loss = 0.50925134\n",
      "Iteration 5, loss = 0.77836073\n",
      "Iteration 1326, loss = 0.25936630\n",
      "Iteration 2043, loss = 0.10678958\n",
      "Iteration 3, loss = 0.76868251\n",
      "Iteration 4, loss = 0.76625076\n",
      "Iteration 5, loss = 0.76365104\n",
      "Iteration 1327, loss = 0.25925883\n",
      "Iteration 71, loss = 0.50700655\n",
      "Iteration 2044, loss = 0.10678379\n",
      "Iteration 6, loss = 0.76094031\n",
      "Iteration 393, loss = 0.41256900\n",
      "Iteration 2045, loss = 0.10649961\n",
      "Iteration 7, loss = 0.75814293\n",
      "Iteration 665, loss = 0.34507655\n",
      "Iteration 829, loss = 0.29118874\n",
      "Iteration 2046, loss = 0.10645813\n",
      "Iteration 6, loss = 0.77456167\n",
      "Iteration 72, loss = 0.50490525\n",
      "Iteration 2047, loss = 0.10628381\n",
      "Iteration 8, loss = 0.75541971\n",
      "Iteration 1328, loss = 0.25918395\n",
      "Iteration 2048, loss = 0.10615294\n",
      "Iteration 73, loss = 0.50276117\n",
      "Iteration 2049, loss = 0.10609458\n",
      "Iteration 666, loss = 0.34494204\n",
      "Iteration 9, loss = 0.75262707\n",
      "Iteration 2050, loss = 0.10599982\n",
      "Iteration 10, loss = 0.75000451\n",
      "Iteration 394, loss = 0.41216665\n",
      "Iteration 1329, loss = 0.25913126\n",
      "Iteration 74, loss = 0.50067648\n",
      "Iteration 2051, loss = 0.10589224\n",
      "Iteration 11, loss = 0.74758830\n",
      "Iteration 830, loss = 0.29100433\n",
      "Iteration 2052, loss = 0.10586543\n",
      "Iteration 7, loss = 0.77062350\n",
      "Iteration 2053, loss = 0.10555004\n",
      "Iteration 75, loss = 0.49861750\n",
      "Iteration 12, loss = 0.74515725\n",
      "Iteration 1330, loss = 0.25891258\n",
      "Iteration 13, loss = 0.74291245\n",
      "Iteration 667, loss = 0.34480899\n",
      "Iteration 831, loss = 0.29079487\n",
      "Iteration 14, loss = 0.74058812\n",
      "Iteration 1331, loss = 0.25881351\n",
      "Iteration 2054, loss = 0.10545624\n",
      "Iteration 76, loss = 0.49658071\n",
      "Iteration 15, loss = 0.73853271\n",
      "Iteration 2055, loss = 0.10542389\n",
      "Iteration 16, loss = 0.73648537\n",
      "Iteration 77, loss = 0.49462160\n",
      "Iteration 2056, loss = 0.10531011\n",
      "Iteration 17, loss = 0.73465878\n",
      "Iteration 832, loss = 0.29055067\n",
      "Iteration 2057, loss = 0.10521674\n",
      "Iteration 18, loss = 0.73281389\n",
      "Iteration 78, loss = 0.49262735\n",
      "Iteration 2058, loss = 0.10512946\n",
      "Iteration 1332, loss = 0.25874434\n",
      "Iteration 395, loss = 0.41197308\n",
      "Iteration 19, loss = 0.73105073\n",
      "Iteration 668, loss = 0.34467775\n",
      "Iteration 2059, loss = 0.10500021\n",
      "Iteration 20, loss = 0.72932006\n",
      "Iteration 8, loss = 0.76686312\n",
      "Iteration 79, loss = 0.49069067\n",
      "Iteration 21, loss = 0.72769328\n",
      "Iteration 22, loss = 0.72606758\n",
      "Iteration 1333, loss = 0.25860103\n",
      "Iteration 833, loss = 0.29038211\n",
      "Iteration 2060, loss = 0.10476758\n",
      "Iteration 80, loss = 0.48880377\n",
      "Iteration 23, loss = 0.72452065\n",
      "Iteration 9, loss = 0.76296026\n",
      "Iteration 2061, loss = 0.10477687\n",
      "Iteration 1334, loss = 0.25850453\n",
      "Iteration 24, loss = 0.72308965\n",
      "Iteration 81, loss = 0.48692190\n",
      "Iteration 669, loss = 0.34458871\n",
      "Iteration 1335, loss = 0.25833161\n",
      "Iteration 2062, loss = 0.10486577\n",
      "Iteration 834, loss = 0.29015434\n",
      "Iteration 10, loss = 0.75886139\n",
      "Iteration 82, loss = 0.48507577\n",
      "Iteration 1336, loss = 0.25821351\n",
      "Iteration 25, loss = 0.72153667\n",
      "Iteration 2063, loss = 0.10455806\n",
      "Iteration 83, loss = 0.48331866\n",
      "Iteration 670, loss = 0.34449354\n",
      "Iteration 1337, loss = 0.25813140\n",
      "Iteration 26, loss = 0.72010859\n",
      "Iteration 396, loss = 0.41174297\n",
      "Iteration 27, loss = 0.71864434\n",
      "Iteration 1338, loss = 0.25805061\n",
      "Iteration 2064, loss = 0.10449569\n",
      "Iteration 11, loss = 0.75512274\n",
      "Iteration 835, loss = 0.29004220\n",
      "Iteration 84, loss = 0.48143080\n",
      "Iteration 28, loss = 0.71726995\n",
      "Iteration 2065, loss = 0.10429651\n",
      "Iteration 85, loss = 0.47974442\n",
      "Iteration 1339, loss = 0.25787106\n",
      "Iteration 29, loss = 0.71590507\n",
      "Iteration 2066, loss = 0.10422563\n",
      "Iteration 86, loss = 0.47814551\n",
      "Iteration 30, loss = 0.71459204\n",
      "Iteration 2067, loss = 0.10416263\n",
      "Iteration 671, loss = 0.34430403\n",
      "Iteration 836, loss = 0.28980755\n",
      "Iteration 31, loss = 0.71319847\n",
      "Iteration 397, loss = 0.41147840\n",
      "Iteration 2068, loss = 0.10402917\n",
      "Iteration 32, loss = 0.71195023\n",
      "Iteration 12, loss = 0.75125504\n",
      "Iteration 87, loss = 0.47642706\n",
      "Iteration 1340, loss = 0.25776001\n",
      "Iteration 33, loss = 0.71067366\n",
      "Iteration 672, loss = 0.34414657\n",
      "Iteration 34, loss = 0.70943752\n",
      "Iteration 837, loss = 0.28967666\n",
      "Iteration 2069, loss = 0.10407165\n",
      "Iteration 35, loss = 0.70819366\n",
      "Iteration 36, loss = 0.70689288\n",
      "Iteration 88, loss = 0.47482176\n",
      "Iteration 37, loss = 0.70565521\n",
      "Iteration 1341, loss = 0.25775254\n",
      "Iteration 673, loss = 0.34404706\n",
      "Iteration 838, loss = 0.28936632\n",
      "Iteration 38, loss = 0.70437710\n",
      "Iteration 2070, loss = 0.10375307\n",
      "Iteration 13, loss = 0.74786410\n",
      "Iteration 398, loss = 0.41135405\n",
      "Iteration 39, loss = 0.70313644\n",
      "Iteration 1342, loss = 0.25752236\n",
      "Iteration 40, loss = 0.70188808\n",
      "Iteration 41, loss = 0.70072119\n",
      "Iteration 89, loss = 0.47321853\n",
      "Iteration 2071, loss = 0.10370976\n",
      "Iteration 1343, loss = 0.25741090\n",
      "Iteration 674, loss = 0.34392023\n",
      "Iteration 90, loss = 0.47163187\n",
      "Iteration 2072, loss = 0.10352986\n",
      "Iteration 839, loss = 0.28919394\n",
      "Iteration 2073, loss = 0.10343637\n",
      "Iteration 14, loss = 0.74401089\n",
      "Iteration 42, loss = 0.69945458\n",
      "Iteration 91, loss = 0.47017211\n",
      "Iteration 1344, loss = 0.25753306Iteration 2074, loss = 0.10336639\n",
      "\n",
      "Iteration 840, loss = 0.28896418\n",
      "Iteration 1345, loss = 0.25728394\n",
      "Iteration 43, loss = 0.69820711\n",
      "Iteration 92, loss = 0.46859902\n",
      "Iteration 44, loss = 0.69695072\n",
      "Iteration 2075, loss = 0.10327814\n",
      "Iteration 15, loss = 0.74056279\n",
      "Iteration 399, loss = 0.41103987\n",
      "Iteration 93, loss = 0.46714545\n",
      "Iteration 45, loss = 0.69565340\n",
      "Iteration 675, loss = 0.34385059\n",
      "Iteration 1346, loss = 0.25710673\n",
      "Iteration 841, loss = 0.28887054\n",
      "Iteration 94, loss = 0.46566646\n",
      "Iteration 1347, loss = 0.25695033\n",
      "Iteration 46, loss = 0.69441932\n",
      "Iteration 95, loss = 0.46424785\n",
      "Iteration 47, loss = 0.69317090\n",
      "Iteration 842, loss = 0.28854005\n",
      "Iteration 2076, loss = 0.10331855\n",
      "Iteration 16, loss = 0.73720130\n",
      "Iteration 1348, loss = 0.25695932\n",
      "Iteration 48, loss = 0.69186250\n",
      "Iteration 676, loss = 0.34363609\n",
      "Iteration 2077, loss = 0.10322904\n",
      "Iteration 400, loss = 0.41093941\n",
      "Iteration 1349, loss = 0.25673223\n",
      "Iteration 49, loss = 0.69059060\n",
      "Iteration 96, loss = 0.46283412\n",
      "Iteration 843, loss = 0.28835427\n",
      "Iteration 1350, loss = 0.25660498\n",
      "Iteration 677, loss = 0.34357602\n",
      "Iteration 2078, loss = 0.10286644\n",
      "Iteration 2079, loss = 0.10281787\n",
      "Iteration 97, loss = 0.46145116\n",
      "Iteration 844, loss = 0.28813905\n",
      "Iteration 1351, loss = 0.25647550\n",
      "Iteration 2080, loss = 0.10276262\n",
      "Iteration 678, loss = 0.34343640\n",
      "Iteration 50, loss = 0.68927855\n",
      "Iteration 98, loss = 0.46007058\n",
      "Iteration 2081, loss = 0.10259626\n",
      "Iteration 2082, loss = 0.10243951\n",
      "Iteration 99, loss = 0.45882583\n",
      "Iteration 1352, loss = 0.25642461\n",
      "Iteration 401, loss = 0.41071401\n",
      "Iteration 51, loss = 0.68798639\n",
      "Iteration 2083, loss = 0.10262871\n",
      "Iteration 17, loss = 0.73392966\n",
      "Iteration 100, loss = 0.45744869\n",
      "Iteration 679, loss = 0.34333022\n",
      "Iteration 2084, loss = 0.10228009\n",
      "Iteration 845, loss = 0.28799569\n",
      "Iteration 52, loss = 0.68662609\n",
      "Iteration 2085, loss = 0.10218401\n",
      "Iteration 101, loss = 0.45629448\n",
      "Iteration 53, loss = 0.68534357\n",
      "Iteration 2086, loss = 0.10213586\n",
      "Iteration 2087, loss = 0.10199118\n",
      "Iteration 54, loss = 0.68395544\n",
      "Iteration 1353, loss = 0.25623703\n",
      "Iteration 2088, loss = 0.10193552\n",
      "Iteration 55, loss = 0.68265145\n",
      "Iteration 102, loss = 0.45492777\n",
      "Iteration 18, loss = 0.73050785\n",
      "Iteration 56, loss = 0.68125581\n",
      "Iteration 402, loss = 0.41035988\n",
      "Iteration 2089, loss = 0.10175443\n",
      "Iteration 57, loss = 0.67980581\n",
      "Iteration 846, loss = 0.28781683\n",
      "Iteration 58, loss = 0.67847545\n",
      "Iteration 1354, loss = 0.25611464\n",
      "Iteration 59, loss = 0.67703381\n",
      "Iteration 2090, loss = 0.10163569\n",
      "Iteration 847, loss = 0.28762026\n",
      "Iteration 103, loss = 0.45373375\n",
      "Iteration 60, loss = 0.67557962\n",
      "Iteration 680, loss = 0.34313242\n",
      "Iteration 19, loss = 0.72731525\n",
      "Iteration 61, loss = 0.67409277\n",
      "Iteration 104, loss = 0.45254253\n",
      "Iteration 2091, loss = 0.10186511\n",
      "Iteration 848, loss = 0.28737683\n",
      "Iteration 62, loss = 0.67260898\n",
      "Iteration 1355, loss = 0.25601868\n",
      "Iteration 2092, loss = 0.10155450\n",
      "Iteration 105, loss = 0.45137306\n",
      "Iteration 1356, loss = 0.25589968\n",
      "Iteration 403, loss = 0.41011737\n",
      "Iteration 2093, loss = 0.10133454\n",
      "Iteration 681, loss = 0.34309835\n",
      "Iteration 849, loss = 0.28715685\n",
      "Iteration 63, loss = 0.67110176\n",
      "Iteration 1357, loss = 0.25576249\n",
      "Iteration 2094, loss = 0.10134350\n",
      "Iteration 64, loss = 0.66959152\n",
      "Iteration 2095, loss = 0.10122645\n",
      "Iteration 1358, loss = 0.25563331\n",
      "Iteration 2096, loss = 0.10102587\n",
      "Iteration 20, loss = 0.72406526\n",
      "Iteration 65, loss = 0.66798809\n",
      "Iteration 2097, loss = 0.10096301\n",
      "Iteration 106, loss = 0.45021405\n",
      "Iteration 66, loss = 0.66641254\n",
      "Iteration 2098, loss = 0.10094587\n",
      "Iteration 67, loss = 0.66476383\n",
      "Iteration 850, loss = 0.28713003\n",
      "Iteration 682, loss = 0.34287965\n",
      "Iteration 68, loss = 0.66314696\n",
      "Iteration 2099, loss = 0.10079552\n",
      "Iteration 69, loss = 0.66152668\n",
      "Iteration 107, loss = 0.44901433\n",
      "Iteration 851, loss = 0.28676032\n",
      "Iteration 21, loss = 0.72089983\n",
      "Iteration 70, loss = 0.65982234\n",
      "Iteration 1359, loss = 0.25555337\n",
      "Iteration 404, loss = 0.40990867\n",
      "Iteration 683, loss = 0.34278183\n",
      "Iteration 71, loss = 0.65808054\n",
      "Iteration 72, loss = 0.65641639\n",
      "Iteration 852, loss = 0.28655974\n",
      "Iteration 2100, loss = 0.10116902\n",
      "Iteration 108, loss = 0.44790235\n",
      "Iteration 73, loss = 0.65460627\n",
      "Iteration 1360, loss = 0.25540172\n",
      "Iteration 2101, loss = 0.10055282\n",
      "Iteration 684, loss = 0.34267104\n",
      "Iteration 2102, loss = 0.10048306\n",
      "Iteration 1361, loss = 0.25535626\n",
      "Iteration 74, loss = 0.65283092\n",
      "Iteration 2103, loss = 0.10047989\n",
      "Iteration 405, loss = 0.40968864\n",
      "Iteration 1362, loss = 0.25516767\n",
      "Iteration 2104, loss = 0.10017361\n",
      "Iteration 685, loss = 0.34259727\n",
      "Iteration 2105, loss = 0.10025334\n",
      "Iteration 75, loss = 0.65101910\n",
      "Iteration 109, loss = 0.44676785\n",
      "Iteration 853, loss = 0.28633607\n",
      "Iteration 2106, loss = 0.09996515\n",
      "Iteration 22, loss = 0.71775546\n",
      "Iteration 2107, loss = 0.09999598\n",
      "Iteration 76, loss = 0.64912655\n",
      "Iteration 1363, loss = 0.25509977\n",
      "Iteration 2108, loss = 0.09972165\n",
      "Iteration 686, loss = 0.34237878\n",
      "Iteration 110, loss = 0.44572160\n",
      "Iteration 2109, loss = 0.09986764\n",
      "Iteration 77, loss = 0.64722479\n",
      "Iteration 2110, loss = 0.09967347\n",
      "Iteration 1364, loss = 0.25498865\n",
      "Iteration 854, loss = 0.28611902\n",
      "Iteration 111, loss = 0.44467871\n",
      "Iteration 78, loss = 0.64535420\n",
      "Iteration 2111, loss = 0.09948660\n",
      "Iteration 1365, loss = 0.25485741\n",
      "Iteration 406, loss = 0.40943738\n",
      "Iteration 23, loss = 0.71468247\n",
      "Iteration 79, loss = 0.64347490\n",
      "Iteration 2112, loss = 0.09947703\n",
      "Iteration 112, loss = 0.44362865\n",
      "Iteration 1366, loss = 0.25470559\n",
      "Iteration 80, loss = 0.64151133\n",
      "Iteration 81, loss = 0.63948751\n",
      "Iteration 2113, loss = 0.09939815Iteration 855, loss = 0.28591412\n",
      "\n",
      "Iteration 1367, loss = 0.25461030\n",
      "Iteration 113, loss = 0.44258117\n",
      "Iteration 24, loss = 0.71152745\n",
      "Iteration 82, loss = 0.63747886\n",
      "Iteration 687, loss = 0.34224246\n",
      "Iteration 83, loss = 0.63544711\n",
      "Iteration 1368, loss = 0.25448611\n",
      "Iteration 2114, loss = 0.09913602\n",
      "Iteration 84, loss = 0.63335733\n",
      "Iteration 2115, loss = 0.09921419\n",
      "Iteration 114, loss = 0.44154028\n",
      "Iteration 1369, loss = 0.25438023\n",
      "Iteration 2116, loss = 0.09912928\n",
      "Iteration 856, loss = 0.28574448\n",
      "Iteration 85, loss = 0.63122819\n",
      "Iteration 407, loss = 0.40923356\n",
      "Iteration 115, loss = 0.44059945\n",
      "Iteration 86, loss = 0.62917769\n",
      "Iteration 857, loss = 0.28579001\n",
      "Iteration 688, loss = 0.34218697\n",
      "Iteration 87, loss = 0.62695442\n",
      "Iteration 1370, loss = 0.25429285\n",
      "Iteration 88, loss = 0.62479854\n",
      "Iteration 2117, loss = 0.09885697\n",
      "Iteration 89, loss = 0.62263198\n",
      "Iteration 116, loss = 0.43962146\n",
      "Iteration 25, loss = 0.70848562\n",
      "Iteration 689, loss = 0.34196751\n",
      "Iteration 90, loss = 0.62039355\n",
      "Iteration 1371, loss = 0.25412088\n",
      "Iteration 2118, loss = 0.09870428\n",
      "Iteration 858, loss = 0.28549628\n",
      "Iteration 2119, loss = 0.09866435\n",
      "Iteration 26, loss = 0.70538897\n",
      "Iteration 117, loss = 0.43863232\n",
      "Iteration 91, loss = 0.61807163\n",
      "Iteration 1372, loss = 0.25409321\n",
      "Iteration 2120, loss = 0.09855231\n",
      "Iteration 859, loss = 0.28519560\n",
      "Iteration 690, loss = 0.34184932\n",
      "Iteration 2121, loss = 0.09850090\n",
      "Iteration 92, loss = 0.61579920\n",
      "Iteration 2122, loss = 0.09835985\n",
      "Iteration 408, loss = 0.40902057\n",
      "Iteration 118, loss = 0.43770091\n",
      "Iteration 1373, loss = 0.25402179Iteration 93, loss = 0.61358066\n",
      "Iteration 2123, loss = 0.09831287\n",
      "Iteration 860, loss = 0.28492284\n",
      "Iteration 27, loss = 0.70236364\n",
      "Iteration 94, loss = 0.61124998\n",
      "Iteration 691, loss = 0.34170721\n",
      "Iteration 119, loss = 0.43676450\n",
      "\n",
      "Iteration 95, loss = 0.60890069\n",
      "Iteration 2124, loss = 0.09815728\n",
      "Iteration 96, loss = 0.60651285\n",
      "Iteration 1374, loss = 0.25377084\n",
      "Iteration 97, loss = 0.60426490\n",
      "Iteration 692, loss = 0.34160243\n",
      "Iteration 2125, loss = 0.09808281\n",
      "Iteration 28, loss = 0.69930192\n",
      "Iteration 98, loss = 0.60171209\n",
      "Iteration 1375, loss = 0.25367106\n",
      "Iteration 99, loss = 0.59937568\n",
      "Iteration 120, loss = 0.43581872\n",
      "Iteration 861, loss = 0.28470534\n",
      "Iteration 2126, loss = 0.09797800\n",
      "Iteration 100, loss = 0.59698905\n",
      "Iteration 1376, loss = 0.25353017\n",
      "Iteration 693, loss = 0.34145760\n",
      "Iteration 409, loss = 0.40885654\n",
      "Iteration 2127, loss = 0.09787939\n",
      "Iteration 101, loss = 0.59460773\n",
      "Iteration 121, loss = 0.43489354\n",
      "Iteration 1377, loss = 0.25347164\n",
      "Iteration 2128, loss = 0.09771310\n",
      "Iteration 29, loss = 0.69624575\n",
      "Iteration 102, loss = 0.59216575\n",
      "Iteration 2129, loss = 0.09766994\n",
      "Iteration 103, loss = 0.58967148\n",
      "Iteration 1378, loss = 0.25331711\n",
      "Iteration 2130, loss = 0.09753761\n",
      "Iteration 862, loss = 0.28454721\n",
      "Iteration 104, loss = 0.58720141\n",
      "Iteration 30, loss = 0.69323989\n",
      "Iteration 122, loss = 0.43399300\n",
      "Iteration 694, loss = 0.34131653\n",
      "Iteration 2131, loss = 0.09743244\n",
      "Iteration 2132, loss = 0.09743195\n",
      "Iteration 863, loss = 0.28434859\n",
      "Iteration 1379, loss = 0.25319816\n",
      "Iteration 2133, loss = 0.09725403\n",
      "Iteration 105, loss = 0.58477187\n",
      "Iteration 31, loss = 0.69024096\n",
      "Iteration 695, loss = 0.34122935\n",
      "Iteration 2134, loss = 0.09720896\n",
      "Iteration 864, loss = 0.28411791\n",
      "Iteration 2135, loss = 0.09712305\n",
      "Iteration 123, loss = 0.43314771\n",
      "Iteration 106, loss = 0.58234770\n",
      "Iteration 2136, loss = 0.09699003\n",
      "Iteration 410, loss = 0.40859361\n",
      "Iteration 1380, loss = 0.25319126\n",
      "Iteration 107, loss = 0.57977618\n",
      "Iteration 2137, loss = 0.09690063\n",
      "Iteration 696, loss = 0.34106771\n",
      "Iteration 124, loss = 0.43227308\n",
      "Iteration 108, loss = 0.57730259\n",
      "Iteration 2138, loss = 0.09695823\n",
      "Iteration 865, loss = 0.28394698\n",
      "Iteration 2139, loss = 0.09672014\n",
      "Iteration 109, loss = 0.57482648\n",
      "Iteration 125, loss = 0.43138869\n",
      "Iteration 2140, loss = 0.09663060\n",
      "Iteration 32, loss = 0.68710903\n",
      "Iteration 1381, loss = 0.25292738\n",
      "Iteration 2141, loss = 0.09670027\n",
      "Iteration 697, loss = 0.34098075\n",
      "Iteration 866, loss = 0.28376332\n",
      "Iteration 110, loss = 0.57245057\n",
      "Iteration 111, loss = 0.56984397\n",
      "Iteration 126, loss = 0.43058276\n",
      "Iteration 2142, loss = 0.09639323\n",
      "Iteration 112, loss = 0.56743988\n",
      "Iteration 698, loss = 0.34082118\n",
      "Iteration 113, loss = 0.56491618\n",
      "Iteration 33, loss = 0.68409967\n",
      "Iteration 1382, loss = 0.25279261\n",
      "Iteration 2143, loss = 0.09644738\n",
      "Iteration 127, loss = 0.42972125\n",
      "Iteration 867, loss = 0.28352899\n",
      "Iteration 2144, loss = 0.09630806\n",
      "Iteration 114, loss = 0.56242147\n",
      "Iteration 2145, loss = 0.09620107\n",
      "Iteration 411, loss = 0.40834157\n",
      "Iteration 128, loss = 0.42897392\n",
      "Iteration 1383, loss = 0.25275271\n",
      "Iteration 2146, loss = 0.09616613\n",
      "Iteration 699, loss = 0.34069980\n",
      "Iteration 2147, loss = 0.09599357\n",
      "Iteration 34, loss = 0.68105617\n",
      "Iteration 115, loss = 0.55999794\n",
      "Iteration 868, loss = 0.28333018\n",
      "Iteration 2148, loss = 0.09591538\n",
      "Iteration 2149, loss = 0.09579027\n",
      "Iteration 129, loss = 0.42812126\n",
      "Iteration 116, loss = 0.55760676\n",
      "Iteration 1384, loss = 0.25259635\n",
      "Iteration 2150, loss = 0.09562565\n",
      "Iteration 2151, loss = 0.09556477\n",
      "Iteration 412, loss = 0.40810904\n",
      "Iteration 117, loss = 0.55506876\n",
      "Iteration 869, loss = 0.28325161\n",
      "Iteration 2152, loss = 0.09564987\n",
      "Iteration 130, loss = 0.42733745\n",
      "Iteration 700, loss = 0.34057502\n",
      "Iteration 35, loss = 0.67805659\n",
      "Iteration 131, loss = 0.42653320\n",
      "Iteration 2153, loss = 0.09552620\n",
      "Iteration 118, loss = 0.55281531\n",
      "Iteration 870, loss = 0.28293478Iteration 2154, loss = 0.09533367\n",
      "Iteration 1385, loss = 0.25254053\n",
      "\n",
      "Iteration 2155, loss = 0.09528326\n",
      "Iteration 132, loss = 0.42578171\n",
      "Iteration 413, loss = 0.40784755\n",
      "Iteration 119, loss = 0.55026825\n",
      "Iteration 1386, loss = 0.25257345\n",
      "Iteration 871, loss = 0.28266821\n",
      "Iteration 133, loss = 0.42503946\n",
      "Iteration 120, loss = 0.54785897\n",
      "Iteration 36, loss = 0.67505290\n",
      "Iteration 121, loss = 0.54549051\n",
      "Iteration 1387, loss = 0.25224426\n",
      "Iteration 872, loss = 0.28250271\n",
      "Iteration 134, loss = 0.42433853\n",
      "Iteration 2156, loss = 0.09514059\n",
      "Iteration 122, loss = 0.54312071\n",
      "Iteration 873, loss = 0.28235933\n",
      "Iteration 414, loss = 0.40767827\n",
      "Iteration 701, loss = 0.34042233\n",
      "Iteration 37, loss = 0.67193856\n",
      "Iteration 2157, loss = 0.09506566\n",
      "Iteration 123, loss = 0.54087846\n",
      "Iteration 1388, loss = 0.25210821\n",
      "Iteration 874, loss = 0.28223738\n",
      "Iteration 135, loss = 0.42354562\n",
      "Iteration 124, loss = 0.53849192\n",
      "Iteration 2158, loss = 0.09499073\n",
      "Iteration 125, loss = 0.53628646\n",
      "Iteration 1389, loss = 0.25203810\n",
      "Iteration 415, loss = 0.40741576\n",
      "Iteration 136, loss = 0.42289887Iteration 2159, loss = 0.09486858\n",
      "\n",
      "Iteration 126, loss = 0.53394513\n",
      "Iteration 702, loss = 0.34032833\n",
      "Iteration 2160, loss = 0.09471905\n",
      "Iteration 127, loss = 0.53160864\n",
      "Iteration 128, loss = 0.52939835\n",
      "Iteration 137, loss = 0.42214966\n",
      "Iteration 38, loss = 0.66888619\n",
      "Iteration 1390, loss = 0.25190648\n",
      "Iteration 875, loss = 0.28188512\n",
      "Iteration 129, loss = 0.52711420\n",
      "Iteration 2161, loss = 0.09466699\n",
      "Iteration 130, loss = 0.52495778\n",
      "Iteration 703, loss = 0.34018464\n",
      "Iteration 416, loss = 0.40719754\n",
      "Iteration 1391, loss = 0.25183042\n",
      "Iteration 131, loss = 0.52285801\n",
      "Iteration 2162, loss = 0.09463798\n",
      "Iteration 876, loss = 0.28168429\n",
      "Iteration 1392, loss = 0.25164016\n",
      "Iteration 132, loss = 0.52065496\n",
      "Iteration 138, loss = 0.42142093\n",
      "Iteration 704, loss = 0.34002887\n",
      "Iteration 1393, loss = 0.25160617\n",
      "Iteration 2163, loss = 0.09454095\n",
      "Iteration 877, loss = 0.28154809\n",
      "Iteration 1394, loss = 0.25139903\n",
      "Iteration 39, loss = 0.66580890\n",
      "Iteration 878, loss = 0.28128785\n",
      "Iteration 1395, loss = 0.25130193\n",
      "Iteration 2164, loss = 0.09442707\n",
      "Iteration 133, loss = 0.51847554\n",
      "Iteration 1396, loss = 0.25122165\n",
      "Iteration 2165, loss = 0.09435842\n",
      "Iteration 705, loss = 0.33992623\n",
      "Iteration 879, loss = 0.28112027\n",
      "Iteration 139, loss = 0.42070906Iteration 2166, loss = 0.09422668\n",
      "\n",
      "Iteration 40, loss = 0.66279046\n",
      "Iteration 1397, loss = 0.25112447\n",
      "Iteration 134, loss = 0.51636401\n",
      "Iteration 417, loss = 0.40703454\n",
      "Iteration 2167, loss = 0.09415295\n",
      "Iteration 135, loss = 0.51438994\n",
      "Iteration 140, loss = 0.42003720\n",
      "Iteration 880, loss = 0.28097111\n",
      "Iteration 41, loss = 0.65972505\n",
      "Iteration 2168, loss = 0.09405004\n",
      "Iteration 136, loss = 0.51229408\n",
      "Iteration 1398, loss = 0.25100077\n",
      "Iteration 141, loss = 0.41941215\n",
      "Iteration 706, loss = 0.33977581\n",
      "Iteration 418, loss = 0.40676221\n",
      "Iteration 2169, loss = 0.09436149\n",
      "Iteration 137, loss = 0.51023494\n",
      "Iteration 881, loss = 0.28067100\n",
      "Iteration 2170, loss = 0.09393412\n",
      "Iteration 142, loss = 0.41874824\n",
      "Iteration 707, loss = 0.33967160\n",
      "Iteration 138, loss = 0.50830688\n",
      "Iteration 2171, loss = 0.09384331\n",
      "Iteration 1399, loss = 0.25083445\n",
      "Iteration 139, loss = 0.50628763\n",
      "Iteration 42, loss = 0.65668432\n",
      "Iteration 1400, loss = 0.25071267\n",
      "Iteration 708, loss = 0.33955547\n",
      "Iteration 2172, loss = 0.09383916\n",
      "Iteration 419, loss = 0.40653473\n",
      "Iteration 140, loss = 0.50435695\n",
      "Iteration 143, loss = 0.41801918\n",
      "Iteration 43, loss = 0.65355514\n",
      "Iteration 1401, loss = 0.25066205\n",
      "Iteration 141, loss = 0.50235229Iteration 144, loss = 0.41739048\n",
      "Iteration 882, loss = 0.28050812\n",
      "Iteration 44, loss = 0.65048399\n",
      "\n",
      "Iteration 1402, loss = 0.25057186Iteration 145, loss = 0.41676853\n",
      "\n",
      "Iteration 709, loss = 0.33940798\n",
      "Iteration 142, loss = 0.50055894\n",
      "Iteration 143, loss = 0.49873557\n",
      "Iteration 2173, loss = 0.09361515\n",
      "Iteration 144, loss = 0.49699212\n",
      "Iteration 1403, loss = 0.25040714\n",
      "Iteration 2174, loss = 0.09350994\n",
      "Iteration 145, loss = 0.49514795\n",
      "Iteration 883, loss = 0.28029494\n",
      "Iteration 146, loss = 0.41609159\n",
      "Iteration 710, loss = 0.33932998\n",
      "Iteration 147, loss = 0.41553243\n",
      "Iteration 1404, loss = 0.25025545\n",
      "Iteration 420, loss = 0.40636193\n",
      "Iteration 2175, loss = 0.09340348\n",
      "Iteration 146, loss = 0.49334252\n",
      "Iteration 45, loss = 0.64743286\n",
      "Iteration 2176, loss = 0.09362687\n",
      "Iteration 148, loss = 0.41496579\n",
      "Iteration 1405, loss = 0.25020473\n",
      "Iteration 147, loss = 0.49163313\n",
      "Iteration 46, loss = 0.64433576\n",
      "Iteration 2177, loss = 0.09322388\n",
      "Iteration 1406, loss = 0.25000180\n",
      "Iteration 149, loss = 0.41428724\n",
      "Iteration 148, loss = 0.48987144\n",
      "Iteration 884, loss = 0.28007060\n",
      "Iteration 2178, loss = 0.09320456\n",
      "Iteration 149, loss = 0.48830062\n",
      "Iteration 150, loss = 0.41364846\n",
      "Iteration 2179, loss = 0.09326332\n",
      "Iteration 47, loss = 0.64126510\n",
      "Iteration 711, loss = 0.33914679\n",
      "Iteration 150, loss = 0.48666354\n",
      "Iteration 2180, loss = 0.09309109\n",
      "Iteration 421, loss = 0.40611698\n",
      "Iteration 2181, loss = 0.09290839\n",
      "Iteration 151, loss = 0.41308991\n",
      "Iteration 1407, loss = 0.24986008\n",
      "Iteration 151, loss = 0.48504914\n",
      "Iteration 2182, loss = 0.09281807\n",
      "Iteration 885, loss = 0.27986738\n",
      "Iteration 1408, loss = 0.24973000\n",
      "Iteration 2183, loss = 0.09275284Iteration 152, loss = 0.41251776\n",
      "Iteration 152, loss = 0.48348172\n",
      "Iteration 886, loss = 0.27965229\n",
      "\n",
      "Iteration 712, loss = 0.33903108\n",
      "Iteration 153, loss = 0.48193299\n",
      "Iteration 2184, loss = 0.09274101\n",
      "Iteration 1409, loss = 0.24970303\n",
      "Iteration 887, loss = 0.27954254\n",
      "Iteration 2185, loss = 0.09258863\n",
      "Iteration 154, loss = 0.48043294\n",
      "Iteration 153, loss = 0.41186975\n",
      "Iteration 155, loss = 0.47898165\n",
      "Iteration 1410, loss = 0.24952548\n",
      "Iteration 156, loss = 0.47750910\n",
      "Iteration 2186, loss = 0.09246730\n",
      "Iteration 422, loss = 0.40584735\n",
      "Iteration 154, loss = 0.41136407\n",
      "Iteration 157, loss = 0.47620009\n",
      "Iteration 888, loss = 0.27924090\n",
      "Iteration 48, loss = 0.63815796\n",
      "Iteration 713, loss = 0.33889120\n",
      "Iteration 1411, loss = 0.24939160\n",
      "Iteration 158, loss = 0.47471569\n",
      "Iteration 2187, loss = 0.09236586\n",
      "Iteration 889, loss = 0.27906545\n",
      "Iteration 2188, loss = 0.09229478\n",
      "Iteration 155, loss = 0.41079295\n",
      "Iteration 159, loss = 0.47342918\n",
      "Iteration 1412, loss = 0.24927106\n",
      "Iteration 160, loss = 0.47203545\n",
      "Iteration 2189, loss = 0.09230760\n",
      "Iteration 161, loss = 0.47075247\n",
      "Iteration 156, loss = 0.41023924\n",
      "Iteration 890, loss = 0.27929253\n",
      "Iteration 49, loss = 0.63510916\n",
      "Iteration 162, loss = 0.46939918\n",
      "Iteration 1413, loss = 0.24921142\n",
      "Iteration 2190, loss = 0.09215806\n",
      "Iteration 163, loss = 0.46822529\n",
      "Iteration 891, loss = 0.27863915\n",
      "Iteration 2191, loss = 0.09207872\n",
      "Iteration 1414, loss = 0.24906747\n",
      "Iteration 2192, loss = 0.09197000\n",
      "Iteration 892, loss = 0.27855433\n",
      "Iteration 164, loss = 0.46701840\n",
      "Iteration 50, loss = 0.63195588\n",
      "Iteration 714, loss = 0.33876405\n",
      "Iteration 165, loss = 0.46577442\n",
      "Iteration 423, loss = 0.40563231\n",
      "Iteration 166, loss = 0.46453255\n",
      "Iteration 157, loss = 0.40974553\n",
      "Iteration 167, loss = 0.46342447\n",
      "Iteration 2193, loss = 0.09186072\n",
      "Iteration 168, loss = 0.46229678\n",
      "Iteration 158, loss = 0.40914606\n",
      "Iteration 169, loss = 0.46108225\n",
      "Iteration 2194, loss = 0.09185344\n",
      "Iteration 159, loss = 0.40867345\n",
      "Iteration 170, loss = 0.46006667\n",
      "Iteration 1415, loss = 0.24893024\n",
      "Iteration 160, loss = 0.40809030\n",
      "Iteration 424, loss = 0.40544950\n",
      "Iteration 2195, loss = 0.09173191\n",
      "Iteration 893, loss = 0.27837874\n",
      "Iteration 161, loss = 0.40761550Iteration 51, loss = 0.62894724\n",
      "\n",
      "Iteration 2196, loss = 0.09158030\n",
      "Iteration 1416, loss = 0.24884290\n",
      "Iteration 2197, loss = 0.09155524\n",
      "Iteration 1417, loss = 0.24867830\n",
      "Iteration 715, loss = 0.33863746\n",
      "Iteration 52, loss = 0.62577874\n",
      "Iteration 162, loss = 0.40707656\n",
      "Iteration 2198, loss = 0.09145224\n",
      "Iteration 171, loss = 0.45893578\n",
      "Iteration 894, loss = 0.27829496\n",
      "Iteration 172, loss = 0.45790068\n",
      "Iteration 2199, loss = 0.09141941\n",
      "Iteration 173, loss = 0.45682838\n",
      "Iteration 895, loss = 0.27786260\n",
      "Iteration 174, loss = 0.45577629\n",
      "Iteration 425, loss = 0.40516032\n",
      "Iteration 2200, loss = 0.09127198\n",
      "Iteration 896, loss = 0.27765602\n",
      "Iteration 175, loss = 0.45487365\n",
      "Iteration 163, loss = 0.40657264\n",
      "Iteration 176, loss = 0.45384151\n",
      "Iteration 2201, loss = 0.09137056\n",
      "Iteration 1418, loss = 0.24858904\n",
      "Iteration 53, loss = 0.62275373\n",
      "Iteration 177, loss = 0.45279381\n",
      "Iteration 716, loss = 0.33849539\n",
      "Iteration 2202, loss = 0.09112610\n",
      "Iteration 178, loss = 0.45189601\n",
      "Iteration 2203, loss = 0.09102842\n",
      "Iteration 179, loss = 0.45095641\n",
      "Iteration 1419, loss = 0.24848914\n",
      "Iteration 180, loss = 0.45006060\n",
      "Iteration 2204, loss = 0.09089232\n",
      "Iteration 181, loss = 0.44912654\n",
      "Iteration 2205, loss = 0.09091393\n",
      "Iteration 164, loss = 0.40608199\n",
      "Iteration 426, loss = 0.40498781\n",
      "Iteration 2206, loss = 0.09087004\n",
      "Iteration 1420, loss = 0.24832769\n",
      "Iteration 717, loss = 0.33837536\n",
      "Iteration 2207, loss = 0.09082112\n",
      "Iteration 2208, loss = 0.09063501\n",
      "Iteration 182, loss = 0.44824804\n",
      "Iteration 897, loss = 0.27750636\n",
      "Iteration 165, loss = 0.40564243\n",
      "Iteration 54, loss = 0.61957118\n",
      "Iteration 1421, loss = 0.24822957\n",
      "Iteration 183, loss = 0.44742843\n",
      "Iteration 166, loss = 0.40514834\n",
      "Iteration 898, loss = 0.27722763\n",
      "Iteration 2209, loss = 0.09048619Iteration 167, loss = 0.40466537\n",
      "Iteration 718, loss = 0.33827080\n",
      "\n",
      "Iteration 184, loss = 0.44647803\n",
      "Iteration 1422, loss = 0.24811066\n",
      "Iteration 427, loss = 0.40476480\n",
      "Iteration 899, loss = 0.27703574\n",
      "Iteration 2210, loss = 0.09046859\n",
      "Iteration 185, loss = 0.44566276\n",
      "Iteration 2211, loss = 0.09047416\n",
      "Iteration 2212, loss = 0.09028687\n",
      "Iteration 900, loss = 0.27680535\n",
      "Iteration 2213, loss = 0.09015049\n",
      "Iteration 186, loss = 0.44484561\n",
      "Iteration 55, loss = 0.61658905\n",
      "Iteration 2214, loss = 0.09008883Iteration 901, loss = 0.27660659\n",
      "Iteration 187, loss = 0.44404784\n",
      "Iteration 1423, loss = 0.24795090\n",
      "Iteration 428, loss = 0.40448197\n",
      "Iteration 188, loss = 0.44319426\n",
      "Iteration 168, loss = 0.40419522\n",
      "Iteration 902, loss = 0.27642997\n",
      "Iteration 189, loss = 0.44238805\n",
      "Iteration 56, loss = 0.61348803\n",
      "Iteration 719, loss = 0.33813649\n",
      "Iteration 190, loss = 0.44164711\n",
      "\n",
      "Iteration 191, loss = 0.44087801\n",
      "Iteration 903, loss = 0.27619433\n",
      "Iteration 1424, loss = 0.24785321\n",
      "Iteration 57, loss = 0.61048204\n",
      "Iteration 192, loss = 0.44010543\n",
      "Iteration 2215, loss = 0.09002975\n",
      "Iteration 169, loss = 0.40371990\n",
      "Iteration 193, loss = 0.43937263\n",
      "Iteration 2216, loss = 0.09002207\n",
      "Iteration 1425, loss = 0.24776915\n",
      "Iteration 904, loss = 0.27595396\n",
      "Iteration 170, loss = 0.40327937\n",
      "Iteration 194, loss = 0.43861430\n",
      "Iteration 2217, loss = 0.08987033\n",
      "Iteration 1426, loss = 0.24758242\n",
      "Iteration 195, loss = 0.43789534\n",
      "Iteration 720, loss = 0.33800072\n",
      "Iteration 171, loss = 0.40281449\n",
      "Iteration 2218, loss = 0.08980090\n",
      "Iteration 196, loss = 0.43713273\n",
      "Iteration 58, loss = 0.60730813\n",
      "Iteration 1427, loss = 0.24761360Iteration 172, loss = 0.40240888\n",
      "\n",
      "Iteration 721, loss = 0.33789716\n",
      "Iteration 2219, loss = 0.08983244\n",
      "Iteration 197, loss = 0.43642453\n",
      "Iteration 173, loss = 0.40195222\n",
      "Iteration 905, loss = 0.27586830\n",
      "Iteration 2220, loss = 0.08963506\n",
      "Iteration 198, loss = 0.43569915\n",
      "Iteration 2221, loss = 0.08956853\n",
      "Iteration 1428, loss = 0.24742497\n",
      "Iteration 59, loss = 0.60425517\n",
      "Iteration 2222, loss = 0.08951219\n",
      "Iteration 429, loss = 0.40433953\n",
      "Iteration 906, loss = 0.27559048\n",
      "Iteration 199, loss = 0.43500474\n",
      "Iteration 2223, loss = 0.08951291\n",
      "Iteration 174, loss = 0.40155157\n",
      "Iteration 2224, loss = 0.08932283\n",
      "Iteration 200, loss = 0.43430445\n",
      "Iteration 1429, loss = 0.24723974\n",
      "Iteration 175, loss = 0.40107853\n",
      "Iteration 1430, loss = 0.24714488\n",
      "Iteration 2225, loss = 0.08919581\n",
      "Iteration 2226, loss = 0.08919026\n",
      "Iteration 201, loss = 0.43361678\n",
      "Iteration 176, loss = 0.40064537\n",
      "Iteration 1431, loss = 0.24699919\n",
      "Iteration 60, loss = 0.60120100\n",
      "Iteration 722, loss = 0.33776463\n",
      "Iteration 2227, loss = 0.08907692\n",
      "Iteration 907, loss = 0.27539820\n",
      "Iteration 202, loss = 0.43292621\n",
      "Iteration 177, loss = 0.40025759\n",
      "Iteration 2228, loss = 0.08895383\n",
      "Iteration 908, loss = 0.27521733\n",
      "Iteration 430, loss = 0.40403913\n",
      "Iteration 723, loss = 0.33762312\n",
      "Iteration 203, loss = 0.43226029\n",
      "Iteration 2229, loss = 0.08905092\n",
      "Iteration 178, loss = 0.39983453\n",
      "Iteration 204, loss = 0.43166009\n",
      "Iteration 205, loss = 0.43092803\n",
      "Iteration 2230, loss = 0.08893681\n",
      "Iteration 61, loss = 0.59807174\n",
      "Iteration 179, loss = 0.39941456\n",
      "Iteration 206, loss = 0.43025692\n",
      "Iteration 2231, loss = 0.08885851\n",
      "Iteration 2232, loss = 0.08902608\n",
      "Iteration 1432, loss = 0.24686409\n",
      "Iteration 207, loss = 0.42964157\n",
      "Iteration 2233, loss = 0.08863845\n",
      "Iteration 208, loss = 0.42902684\n",
      "Iteration 180, loss = 0.39901226\n",
      "Iteration 2234, loss = 0.08846777\n",
      "Iteration 909, loss = 0.27514032\n",
      "Iteration 209, loss = 0.42837356\n",
      "Iteration 2235, loss = 0.08848158\n",
      "Iteration 210, loss = 0.42778193\n",
      "Iteration 1433, loss = 0.24679771\n",
      "Iteration 181, loss = 0.39860127\n",
      "Iteration 62, loss = 0.59513772\n",
      "Iteration 2236, loss = 0.08838455\n",
      "Iteration 724, loss = 0.33765037\n",
      "Iteration 431, loss = 0.40382642\n",
      "Iteration 211, loss = 0.42712171\n",
      "Iteration 1434, loss = 0.24662355\n",
      "Iteration 2237, loss = 0.08822580\n",
      "Iteration 182, loss = 0.39821071\n",
      "Iteration 212, loss = 0.42655995\n",
      "Iteration 1435, loss = 0.24658869\n",
      "Iteration 910, loss = 0.27474379\n",
      "Iteration 63, loss = 0.59208726\n",
      "Iteration 213, loss = 0.42591974\n",
      "Iteration 214, loss = 0.42532050\n",
      "Iteration 1436, loss = 0.24638589\n",
      "Iteration 215, loss = 0.42470807\n",
      "Iteration 216, loss = 0.42414015\n",
      "Iteration 2238, loss = 0.08832885\n",
      "Iteration 725, loss = 0.33738166\n",
      "Iteration 911, loss = 0.27455101\n",
      "Iteration 183, loss = 0.39786644\n",
      "Iteration 64, loss = 0.58912881\n",
      "Iteration 2239, loss = 0.08817372\n",
      "Iteration 1437, loss = 0.24628789\n",
      "Iteration 217, loss = 0.42355490\n",
      "Iteration 432, loss = 0.40365760\n",
      "Iteration 218, loss = 0.42294025\n",
      "Iteration 2240, loss = 0.08808707\n",
      "Iteration 219, loss = 0.42238993\n",
      "Iteration 2241, loss = 0.08793046\n",
      "Iteration 184, loss = 0.39746030\n",
      "Iteration 1438, loss = 0.24622979\n",
      "Iteration 726, loss = 0.33723941\n",
      "Iteration 220, loss = 0.42181907\n",
      "Iteration 912, loss = 0.27437342\n",
      "Iteration 2242, loss = 0.08818078\n",
      "Iteration 221, loss = 0.42128065\n",
      "Iteration 433, loss = 0.40337734\n",
      "Iteration 2243, loss = 0.08784252\n",
      "Iteration 1439, loss = 0.24604053\n",
      "Iteration 2244, loss = 0.08774956\n",
      "Iteration 185, loss = 0.39706455\n",
      "Iteration 913, loss = 0.27413939\n",
      "Iteration 222, loss = 0.42066733\n",
      "Iteration 2245, loss = 0.08767931\n",
      "Iteration 65, loss = 0.58610774\n",
      "Iteration 186, loss = 0.39668119\n",
      "Iteration 2246, loss = 0.08762204\n",
      "Iteration 914, loss = 0.27398551\n",
      "Iteration 223, loss = 0.42015438\n",
      "Iteration 187, loss = 0.39632010\n",
      "Iteration 727, loss = 0.33712323\n",
      "Iteration 1440, loss = 0.24590107\n",
      "Iteration 2247, loss = 0.08747336\n",
      "Iteration 224, loss = 0.41956029\n",
      "Iteration 66, loss = 0.58311311\n",
      "Iteration 225, loss = 0.41905663\n",
      "Iteration 1441, loss = 0.24585614\n",
      "Iteration 2248, loss = 0.08750564\n",
      "Iteration 226, loss = 0.41849106\n",
      "Iteration 188, loss = 0.39598071\n",
      "Iteration 2249, loss = 0.08735489\n",
      "Iteration 227, loss = 0.41796873\n",
      "Iteration 915, loss = 0.27376413\n",
      "Iteration 728, loss = 0.33697009\n",
      "Iteration 2250, loss = 0.08761632\n",
      "Iteration 434, loss = 0.40318791\n",
      "Iteration 189, loss = 0.39556768\n",
      "Iteration 228, loss = 0.41742903\n",
      "Iteration 1442, loss = 0.24567574\n",
      "Iteration 2251, loss = 0.08732342\n",
      "Iteration 2252, loss = 0.08712524\n",
      "Iteration 190, loss = 0.39521086\n",
      "Iteration 2253, loss = 0.08708953\n",
      "Iteration 229, loss = 0.41686861\n",
      "Iteration 1443, loss = 0.24561618\n",
      "Iteration 916, loss = 0.27354135\n",
      "Iteration 67, loss = 0.58027825\n",
      "Iteration 191, loss = 0.39487405\n",
      "Iteration 230, loss = 0.41636433\n",
      "Iteration 2254, loss = 0.08702199\n",
      "Iteration 231, loss = 0.41581466\n",
      "Iteration 2255, loss = 0.08690498\n",
      "Iteration 729, loss = 0.33686001\n",
      "Iteration 232, loss = 0.41532835\n",
      "Iteration 917, loss = 0.27334671\n",
      "Iteration 192, loss = 0.39451696\n",
      "Iteration 2256, loss = 0.08687304\n",
      "Iteration 435, loss = 0.40299927\n",
      "Iteration 193, loss = 0.39418958\n",
      "Iteration 68, loss = 0.57726221\n",
      "Iteration 233, loss = 0.41482047\n",
      "Iteration 1444, loss = 0.24542604\n",
      "Iteration 918, loss = 0.27315452\n",
      "Iteration 194, loss = 0.39380906\n",
      "Iteration 234, loss = 0.41431470\n",
      "Iteration 1445, loss = 0.24534330\n",
      "Iteration 2257, loss = 0.08676667\n",
      "Iteration 730, loss = 0.33672610\n",
      "Iteration 235, loss = 0.41384052\n",
      "Iteration 2258, loss = 0.08672419\n",
      "Iteration 919, loss = 0.27294889\n",
      "Iteration 1446, loss = 0.24519282\n",
      "Iteration 2259, loss = 0.08675797\n",
      "Iteration 236, loss = 0.41338021\n",
      "Iteration 195, loss = 0.39344835\n",
      "Iteration 69, loss = 0.57447208\n",
      "Iteration 2260, loss = 0.08652628\n",
      "Iteration 2261, loss = 0.08666608\n",
      "Iteration 2262, loss = 0.08642423\n",
      "Iteration 237, loss = 0.41288057\n",
      "Iteration 196, loss = 0.39318598\n",
      "Iteration 1447, loss = 0.24505342\n",
      "Iteration 2263, loss = 0.08631730\n",
      "Iteration 436, loss = 0.40271053\n",
      "Iteration 70, loss = 0.57159539\n",
      "Iteration 2264, loss = 0.08625191\n",
      "Iteration 920, loss = 0.27279121\n",
      "Iteration 1448, loss = 0.24497919\n",
      "Iteration 238, loss = 0.41240317\n",
      "Iteration 2265, loss = 0.08618606\n",
      "Iteration 239, loss = 0.41188381\n",
      "Iteration 197, loss = 0.39282159\n",
      "Iteration 731, loss = 0.33657589\n",
      "Iteration 2266, loss = 0.08611571\n",
      "Iteration 198, loss = 0.39244598\n",
      "Iteration 240, loss = 0.41142105\n",
      "Iteration 2267, loss = 0.08606217\n",
      "Iteration 1449, loss = 0.24486950\n",
      "Iteration 71, loss = 0.56875393\n",
      "Iteration 2268, loss = 0.08608604\n",
      "Iteration 921, loss = 0.27256511\n",
      "Iteration 241, loss = 0.41092335\n",
      "Iteration 199, loss = 0.39213679\n",
      "Iteration 1450, loss = 0.24470735\n",
      "Iteration 2269, loss = 0.08593484\n",
      "Iteration 922, loss = 0.27233210\n",
      "Iteration 242, loss = 0.41046212\n",
      "Iteration 1451, loss = 0.24461481\n",
      "Iteration 200, loss = 0.39186654\n",
      "Iteration 732, loss = 0.33646879\n",
      "Iteration 72, loss = 0.56595523\n",
      "Iteration 1452, loss = 0.24447990\n",
      "Iteration 923, loss = 0.27215637\n",
      "Iteration 243, loss = 0.40996700\n",
      "Iteration 437, loss = 0.40252996\n",
      "Iteration 2270, loss = 0.08586644\n",
      "Iteration 244, loss = 0.40954034\n",
      "Iteration 201, loss = 0.39151929\n",
      "Iteration 2271, loss = 0.08587066\n",
      "Iteration 245, loss = 0.40905243\n",
      "Iteration 733, loss = 0.33635245\n",
      "Iteration 2272, loss = 0.08576753\n",
      "Iteration 246, loss = 0.40856968\n",
      "Iteration 924, loss = 0.27198540\n",
      "Iteration 2273, loss = 0.08556612\n",
      "Iteration 1453, loss = 0.24443803\n",
      "Iteration 247, loss = 0.40814851\n",
      "Iteration 2274, loss = 0.08556571\n",
      "Iteration 248, loss = 0.40767204\n",
      "Iteration 73, loss = 0.56315694\n",
      "Iteration 1454, loss = 0.24422145\n",
      "Iteration 2275, loss = 0.08563198\n",
      "Iteration 249, loss = 0.40726072\n",
      "Iteration 250, loss = 0.40680054\n",
      "Iteration 2276, loss = 0.08545059\n",
      "Iteration 925, loss = 0.27183257\n",
      "Iteration 202, loss = 0.39119263\n",
      "Iteration 74, loss = 0.56041315\n",
      "Iteration 438, loss = 0.40227489\n",
      "Iteration 251, loss = 0.40637381\n",
      "Iteration 2277, loss = 0.08528969\n",
      "Iteration 203, loss = 0.39100194\n",
      "Iteration 1455, loss = 0.24416394\n",
      "Iteration 926, loss = 0.27146794\n",
      "Iteration 2278, loss = 0.08523266\n",
      "Iteration 252, loss = 0.40587997\n",
      "Iteration 204, loss = 0.39059737\n",
      "Iteration 2279, loss = 0.08519255\n",
      "Iteration 1456, loss = 0.24399248\n",
      "Iteration 253, loss = 0.40547789\n",
      "Iteration 734, loss = 0.33626602\n",
      "Iteration 2280, loss = 0.08510115\n",
      "Iteration 205, loss = 0.39030562\n",
      "Iteration 2281, loss = 0.08505359\n",
      "Iteration 927, loss = 0.27130723\n",
      "Iteration 206, loss = 0.38999272\n",
      "Iteration 2282, loss = 0.08493763\n",
      "Iteration 439, loss = 0.40211923\n",
      "Iteration 254, loss = 0.40501885\n",
      "Iteration 1457, loss = 0.24386763\n",
      "Iteration 75, loss = 0.55769394\n",
      "Iteration 207, loss = 0.38977370\n",
      "Iteration 1458, loss = 0.24377611\n",
      "Iteration 255, loss = 0.40457099\n",
      "Iteration 735, loss = 0.33621750\n",
      "Iteration 208, loss = 0.38941190\n",
      "Iteration 2283, loss = 0.08487783\n",
      "Iteration 1459, loss = 0.24362216\n",
      "Iteration 256, loss = 0.40410291\n",
      "Iteration 928, loss = 0.27115868\n",
      "Iteration 1460, loss = 0.24359384\n",
      "Iteration 209, loss = 0.38916151\n",
      "Iteration 76, loss = 0.55499119\n",
      "Iteration 2284, loss = 0.08509642\n",
      "Iteration 257, loss = 0.40368508\n",
      "Iteration 440, loss = 0.40185053\n",
      "Iteration 2285, loss = 0.08475129\n",
      "Iteration 2286, loss = 0.08483581\n",
      "Iteration 2287, loss = 0.08466033\n",
      "Iteration 258, loss = 0.40323577\n",
      "Iteration 77, loss = 0.55236922\n",
      "Iteration 1461, loss = 0.24342617\n",
      "Iteration 210, loss = 0.38886476\n",
      "Iteration 2288, loss = 0.08467018\n",
      "Iteration 736, loss = 0.33600969\n",
      "Iteration 259, loss = 0.40277976\n",
      "Iteration 2289, loss = 0.08450094\n",
      "Iteration 211, loss = 0.38858652\n",
      "Iteration 1462, loss = 0.24328643\n",
      "Iteration 2290, loss = 0.08437720\n",
      "Iteration 929, loss = 0.27094167\n",
      "Iteration 2291, loss = 0.08439268\n",
      "Iteration 260, loss = 0.40234073\n",
      "Iteration 2292, loss = 0.08434048\n",
      "Iteration 78, loss = 0.54976755\n",
      "Iteration 441, loss = 0.40165624\n",
      "Iteration 2293, loss = 0.08423105\n",
      "Iteration 930, loss = 0.27073223\n",
      "Iteration 737, loss = 0.33583239\n",
      "Iteration 212, loss = 0.38828865\n",
      "Iteration 261, loss = 0.40193027\n",
      "Iteration 2294, loss = 0.08436024\n",
      "Iteration 1463, loss = 0.24316277\n",
      "Iteration 2295, loss = 0.08403512\n",
      "Iteration 262, loss = 0.40147846\n",
      "Iteration 1464, loss = 0.24305763\n",
      "Iteration 2296, loss = 0.08401984\n",
      "Iteration 2297, loss = 0.08398656\n",
      "Iteration 1465, loss = 0.24295241\n",
      "Iteration 2298, loss = 0.08387080\n",
      "Iteration 263, loss = 0.40104907\n",
      "Iteration 213, loss = 0.38804048\n",
      "Iteration 442, loss = 0.40138899\n",
      "Iteration 2299, loss = 0.08376092\n",
      "Iteration 931, loss = 0.27050935\n",
      "Iteration 264, loss = 0.40066478\n",
      "Iteration 1466, loss = 0.24278843\n",
      "Iteration 2300, loss = 0.08376302\n",
      "Iteration 265, loss = 0.40024902\n",
      "Iteration 2301, loss = 0.08377374\n",
      "Iteration 266, loss = 0.39979662\n",
      "Iteration 738, loss = 0.33571511\n",
      "Iteration 2302, loss = 0.08356839\n",
      "Iteration 932, loss = 0.27032004\n",
      "Iteration 267, loss = 0.39937207\n",
      "Iteration 2303, loss = 0.08358656\n",
      "Iteration 79, loss = 0.54711284\n",
      "Iteration 268, loss = 0.39898102\n",
      "Iteration 2304, loss = 0.08353624\n",
      "Iteration 1467, loss = 0.24269639\n",
      "Iteration 933, loss = 0.27015458\n",
      "Iteration 2305, loss = 0.08345195\n",
      "Iteration 214, loss = 0.38776364\n",
      "Iteration 269, loss = 0.39855977\n",
      "Iteration 270, loss = 0.39814963\n",
      "Iteration 215, loss = 0.38748803\n",
      "Iteration 443, loss = 0.40117328\n",
      "Iteration 2306, loss = 0.08329996\n",
      "Iteration 1468, loss = 0.24255975\n",
      "Iteration 271, loss = 0.39776798\n",
      "Iteration 80, loss = 0.54454664\n",
      "Iteration 216, loss = 0.38728516\n",
      "Iteration 934, loss = 0.27003530\n",
      "Iteration 272, loss = 0.39736076\n",
      "Iteration 739, loss = 0.33559099\n",
      "Iteration 217, loss = 0.38698410\n",
      "Iteration 1469, loss = 0.24246328\n",
      "Iteration 444, loss = 0.40097067\n",
      "Iteration 2307, loss = 0.08324957\n",
      "Iteration 273, loss = 0.39697993\n",
      "Iteration 935, loss = 0.26959379\n",
      "Iteration 2308, loss = 0.08320327\n",
      "Iteration 1470, loss = 0.24234122\n",
      "Iteration 81, loss = 0.54206870\n",
      "Iteration 218, loss = 0.38670943\n",
      "Iteration 274, loss = 0.39656738\n",
      "Iteration 1471, loss = 0.24225204\n",
      "Iteration 740, loss = 0.33545234\n",
      "Iteration 936, loss = 0.26945599\n",
      "Iteration 2309, loss = 0.08325010\n",
      "Iteration 2310, loss = 0.08315473\n",
      "Iteration 275, loss = 0.39619089\n",
      "Iteration 1472, loss = 0.24211313\n",
      "Iteration 2311, loss = 0.08302893Iteration 219, loss = 0.38649929\n",
      "\n",
      "Iteration 937, loss = 0.26925119\n",
      "Iteration 2312, loss = 0.08294198\n",
      "Iteration 276, loss = 0.39575045\n",
      "Iteration 1473, loss = 0.24201980\n",
      "Iteration 220, loss = 0.38622050\n",
      "Iteration 938, loss = 0.26898018\n",
      "Iteration 1474, loss = 0.24193907\n",
      "Iteration 221, loss = 0.38595430\n",
      "Iteration 2313, loss = 0.08283885\n",
      "Iteration 445, loss = 0.40072623\n",
      "Iteration 741, loss = 0.33532525\n",
      "Iteration 82, loss = 0.53965466\n",
      "Iteration 2314, loss = 0.08283657\n",
      "Iteration 277, loss = 0.39537751\n",
      "Iteration 1475, loss = 0.24174265\n",
      "Iteration 2315, loss = 0.08294571\n",
      "Iteration 278, loss = 0.39500224\n",
      "Iteration 1476, loss = 0.24170326\n",
      "Iteration 83, loss = 0.53715142\n",
      "Iteration 279, loss = 0.39460326\n",
      "Iteration 222, loss = 0.38571744\n",
      "Iteration 939, loss = 0.26882720\n",
      "Iteration 2316, loss = 0.08272389\n",
      "Iteration 280, loss = 0.39419675\n",
      "Iteration 1477, loss = 0.24154726\n",
      "Iteration 2317, loss = 0.08263822\n",
      "Iteration 446, loss = 0.40061745\n",
      "Iteration 281, loss = 0.39384693\n",
      "Iteration 223, loss = 0.38546410\n",
      "Iteration 2318, loss = 0.08253648\n",
      "Iteration 282, loss = 0.39343810\n",
      "Iteration 940, loss = 0.26859245\n",
      "Iteration 2319, loss = 0.08254722\n",
      "Iteration 224, loss = 0.38525981\n",
      "Iteration 1478, loss = 0.24141063\n",
      "Iteration 742, loss = 0.33521082\n",
      "Iteration 283, loss = 0.39306369\n",
      "Iteration 2320, loss = 0.08241101\n",
      "Iteration 84, loss = 0.53479292\n",
      "Iteration 225, loss = 0.38498553\n",
      "Iteration 2321, loss = 0.08232564\n",
      "Iteration 284, loss = 0.39270627\n",
      "Iteration 2322, loss = 0.08226700\n",
      "Iteration 941, loss = 0.26853710\n",
      "Iteration 226, loss = 0.38472347\n",
      "Iteration 1479, loss = 0.24137573\n",
      "Iteration 2323, loss = 0.08231962\n",
      "Iteration 227, loss = 0.38449012\n",
      "Iteration 285, loss = 0.39232291\n",
      "Iteration 1480, loss = 0.24123215\n",
      "Iteration 2324, loss = 0.08215281\n",
      "Iteration 447, loss = 0.40029746\n",
      "Iteration 743, loss = 0.33508308\n",
      "Iteration 286, loss = 0.39195853\n",
      "Iteration 942, loss = 0.26834227\n",
      "Iteration 85, loss = 0.53244576\n",
      "Iteration 2325, loss = 0.08205933\n",
      "Iteration 287, loss = 0.39157131\n",
      "Iteration 228, loss = 0.38429391\n",
      "Iteration 1481, loss = 0.24105598\n",
      "Iteration 288, loss = 0.39122036\n",
      "Iteration 2326, loss = 0.08205922\n",
      "Iteration 289, loss = 0.39084222\n",
      "Iteration 943, loss = 0.26840162\n",
      "Iteration 1482, loss = 0.24096502\n",
      "Iteration 2327, loss = 0.08197779\n",
      "Iteration 744, loss = 0.33495104\n",
      "Iteration 2328, loss = 0.08190468\n",
      "Iteration 290, loss = 0.39050769\n",
      "Iteration 229, loss = 0.38400459\n",
      "Iteration 86, loss = 0.53007662\n",
      "Iteration 2329, loss = 0.08186680\n",
      "Iteration 1483, loss = 0.24080346\n",
      "Iteration 448, loss = 0.40014135\n",
      "Iteration 2330, loss = 0.08174317\n",
      "Iteration 745, loss = 0.33482342\n",
      "Iteration 291, loss = 0.39012664\n",
      "Iteration 2331, loss = 0.08172992\n",
      "Iteration 944, loss = 0.26776448\n",
      "Iteration 292, loss = 0.38973313\n",
      "Iteration 87, loss = 0.52780719\n",
      "Iteration 293, loss = 0.38937130\n",
      "Iteration 2332, loss = 0.08169876\n",
      "Iteration 294, loss = 0.38901860\n",
      "Iteration 1484, loss = 0.24072628\n",
      "Iteration 449, loss = 0.39984251\n",
      "Iteration 2333, loss = 0.08156081\n",
      "Iteration 230, loss = 0.38378363\n",
      "Iteration 2334, loss = 0.08168774\n",
      "Iteration 295, loss = 0.38866395\n",
      "Iteration 2335, loss = 0.08140443\n",
      "Iteration 945, loss = 0.26756474\n",
      "Iteration 746, loss = 0.33470395\n",
      "Iteration 296, loss = 0.38828985\n",
      "Iteration 88, loss = 0.52553230\n",
      "Iteration 1485, loss = 0.24063287\n",
      "Iteration 231, loss = 0.38356662\n",
      "Iteration 297, loss = 0.38793239\n",
      "Iteration 946, loss = 0.26741456\n",
      "Iteration 232, loss = 0.38333825\n",
      "Iteration 1486, loss = 0.24057864\n",
      "Iteration 747, loss = 0.33455723\n",
      "Iteration 298, loss = 0.38762303\n",
      "Iteration 2336, loss = 0.08147505\n",
      "Iteration 450, loss = 0.39966645\n",
      "Iteration 233, loss = 0.38309479\n",
      "Iteration 947, loss = 0.26715306\n",
      "Iteration 234, loss = 0.38289812\n",
      "Iteration 299, loss = 0.38723622\n",
      "Iteration 1487, loss = 0.24034412\n",
      "Iteration 235, loss = 0.38265211\n",
      "Iteration 89, loss = 0.52336027\n",
      "Iteration 2337, loss = 0.08138249\n",
      "Iteration 300, loss = 0.38686837\n",
      "Iteration 948, loss = 0.26707747\n",
      "Iteration 1488, loss = 0.24024646\n",
      "Iteration 236, loss = 0.38241916\n",
      "Iteration 301, loss = 0.38655110\n",
      "Iteration 2338, loss = 0.08126186\n",
      "Iteration 237, loss = 0.38219419\n",
      "Iteration 748, loss = 0.33444620\n",
      "Iteration 2339, loss = 0.08121264\n",
      "Iteration 302, loss = 0.38619054\n",
      "Iteration 2340, loss = 0.08112956\n",
      "Iteration 90, loss = 0.52116059\n",
      "Iteration 451, loss = 0.39949678\n",
      "Iteration 238, loss = 0.38197339\n",
      "Iteration 949, loss = 0.26678034\n",
      "Iteration 1489, loss = 0.24015252\n",
      "Iteration 303, loss = 0.38584939\n",
      "Iteration 2341, loss = 0.08116908\n",
      "Iteration 749, loss = 0.33429680\n",
      "Iteration 950, loss = 0.26652033\n",
      "Iteration 239, loss = 0.38174937\n",
      "Iteration 304, loss = 0.38551643\n",
      "Iteration 91, loss = 0.51907151\n",
      "Iteration 2342, loss = 0.08110439\n",
      "Iteration 1490, loss = 0.24000289\n",
      "Iteration 305, loss = 0.38515607\n",
      "Iteration 951, loss = 0.26638780\n",
      "Iteration 2343, loss = 0.08099339\n",
      "Iteration 306, loss = 0.38481292\n",
      "Iteration 240, loss = 0.38156991\n",
      "Iteration 2344, loss = 0.08099846\n",
      "Iteration 452, loss = 0.39920606\n",
      "Iteration 750, loss = 0.33421981\n",
      "Iteration 1491, loss = 0.23995917\n",
      "Iteration 2345, loss = 0.08096090\n",
      "Iteration 307, loss = 0.38449631\n",
      "Iteration 2346, loss = 0.08075557\n",
      "Iteration 1492, loss = 0.23977956\n",
      "Iteration 241, loss = 0.38132862\n",
      "Iteration 92, loss = 0.51704735\n",
      "Iteration 2347, loss = 0.08068597\n",
      "Iteration 308, loss = 0.38412706\n",
      "Iteration 952, loss = 0.26610785\n",
      "Iteration 309, loss = 0.38379042\n",
      "Iteration 1493, loss = 0.23965577\n",
      "Iteration 751, loss = 0.33405228\n",
      "Iteration 2348, loss = 0.08069667\n",
      "Iteration 953, loss = 0.26597068\n",
      "Iteration 242, loss = 0.38109946\n",
      "Iteration 2349, loss = 0.08067926\n",
      "Iteration 310, loss = 0.38347431\n",
      "Iteration 93, loss = 0.51492977\n",
      "Iteration 243, loss = 0.38088285\n",
      "Iteration 311, loss = 0.38314448\n",
      "Iteration 312, loss = 0.38280945\n",
      "Iteration 954, loss = 0.26572287\n",
      "Iteration 453, loss = 0.39898590\n",
      "Iteration 244, loss = 0.38070248\n",
      "Iteration 1494, loss = 0.23959015\n",
      "Iteration 2350, loss = 0.08058150\n",
      "Iteration 313, loss = 0.38248715\n",
      "Iteration 752, loss = 0.33392291\n",
      "Iteration 245, loss = 0.38047394\n",
      "Iteration 955, loss = 0.26570545\n",
      "Iteration 94, loss = 0.51291811\n",
      "Iteration 2351, loss = 0.08046961\n",
      "Iteration 314, loss = 0.38217921\n",
      "Iteration 246, loss = 0.38027403\n",
      "Iteration 2352, loss = 0.08043559\n",
      "Iteration 1495, loss = 0.23943935\n",
      "Iteration 753, loss = 0.33380346\n",
      "Iteration 2353, loss = 0.08038510\n",
      "Iteration 247, loss = 0.38013273\n",
      "Iteration 95, loss = 0.51096948\n",
      "Iteration 315, loss = 0.38187463\n",
      "Iteration 454, loss = 0.39879829\n",
      "Iteration 248, loss = 0.37986076\n",
      "Iteration 956, loss = 0.26528874\n",
      "Iteration 316, loss = 0.38155159\n",
      "Iteration 2354, loss = 0.08031495\n",
      "Iteration 317, loss = 0.38122531\n",
      "Iteration 1496, loss = 0.23933709\n",
      "Iteration 2355, loss = 0.08040441\n",
      "Iteration 249, loss = 0.37964717\n",
      "Iteration 2356, loss = 0.08018085\n",
      "Iteration 96, loss = 0.50902530\n",
      "Iteration 318, loss = 0.38090690\n",
      "Iteration 2357, loss = 0.08012282\n",
      "Iteration 319, loss = 0.38057282\n",
      "Iteration 250, loss = 0.37947597\n",
      "Iteration 2358, loss = 0.08011519\n",
      "Iteration 320, loss = 0.38029362\n",
      "Iteration 2359, loss = 0.08001473\n",
      "Iteration 251, loss = 0.37929072\n",
      "Iteration 321, loss = 0.38000512\n",
      "Iteration 1497, loss = 0.23924416\n",
      "Iteration 322, loss = 0.37966844\n",
      "Iteration 252, loss = 0.37907616\n",
      "Iteration 323, loss = 0.37933983\n",
      "Iteration 1498, loss = 0.23909045\n",
      "Iteration 455, loss = 0.39856252\n",
      "Iteration 754, loss = 0.33367431\n",
      "Iteration 957, loss = 0.26508281\n",
      "Iteration 1499, loss = 0.23898651\n",
      "Iteration 97, loss = 0.50714329\n",
      "Iteration 1500, loss = 0.23889538\n",
      "Iteration 2360, loss = 0.07998555\n",
      "Iteration 324, loss = 0.37906332\n",
      "Iteration 755, loss = 0.33356354\n",
      "Iteration 98, loss = 0.50522934\n",
      "Iteration 253, loss = 0.37889770\n",
      "Iteration 2361, loss = 0.07992384\n",
      "Iteration 2362, loss = 0.07984812\n",
      "Iteration 325, loss = 0.37874947\n",
      "Iteration 958, loss = 0.26499325\n",
      "Iteration 2363, loss = 0.07981512\n",
      "Iteration 326, loss = 0.37855173\n",
      "Iteration 254, loss = 0.37866822\n",
      "Iteration 959, loss = 0.26470393\n",
      "Iteration 327, loss = 0.37815482\n",
      "Iteration 756, loss = 0.33348002\n",
      "Iteration 328, loss = 0.37787133\n",
      "Iteration 456, loss = 0.39834892\n",
      "Iteration 329, loss = 0.37756391\n",
      "Iteration 99, loss = 0.50355534\n",
      "Iteration 330, loss = 0.37733104\n",
      "Iteration 1501, loss = 0.23870798\n",
      "Iteration 757, loss = 0.33335688\n",
      "Iteration 2364, loss = 0.07973681\n",
      "Iteration 331, loss = 0.37699587\n",
      "Iteration 255, loss = 0.37848185\n",
      "Iteration 960, loss = 0.26455973\n",
      "Iteration 256, loss = 0.37829465\n",
      "Iteration 457, loss = 0.39816730\n",
      "Iteration 758, loss = 0.33317736\n",
      "Iteration 257, loss = 0.37810573\n",
      "Iteration 2365, loss = 0.07970038\n",
      "Iteration 332, loss = 0.37671622\n",
      "Iteration 961, loss = 0.26428401\n",
      "Iteration 333, loss = 0.37649092\n",
      "Iteration 1502, loss = 0.23864030\n",
      "Iteration 334, loss = 0.37612590\n",
      "Iteration 2366, loss = 0.07963440\n",
      "Iteration 962, loss = 0.26409321\n",
      "Iteration 258, loss = 0.37796099\n",
      "Iteration 335, loss = 0.37588071\n",
      "Iteration 759, loss = 0.33306708\n",
      "Iteration 1503, loss = 0.23849549\n",
      "Iteration 2367, loss = 0.07985846\n",
      "Iteration 259, loss = 0.37776032\n",
      "Iteration 2368, loss = 0.07949653\n",
      "Iteration 963, loss = 0.26387015\n",
      "Iteration 2369, loss = 0.07943944\n",
      "Iteration 336, loss = 0.37558245\n",
      "Iteration 2370, loss = 0.07946439\n",
      "Iteration 964, loss = 0.26372075\n",
      "Iteration 337, loss = 0.37530679\n",
      "Iteration 760, loss = 0.33291420\n",
      "Iteration 1504, loss = 0.23842500\n",
      "Iteration 338, loss = 0.37504539\n",
      "Iteration 458, loss = 0.39787502\n",
      "Iteration 339, loss = 0.37476178\n",
      "Iteration 965, loss = 0.26347129\n",
      "Iteration 260, loss = 0.37753755\n",
      "Iteration 340, loss = 0.37446788\n",
      "Iteration 2371, loss = 0.07934107\n",
      "Iteration 100, loss = 0.50165332\n",
      "Iteration 2372, loss = 0.07924875\n",
      "Iteration 1505, loss = 0.23832003\n",
      "Iteration 2373, loss = 0.07929875\n",
      "Iteration 341, loss = 0.37420250\n",
      "Iteration 261, loss = 0.37736610\n",
      "Iteration 342, loss = 0.37396463\n",
      "Iteration 101, loss = 0.49995433\n",
      "Iteration 2374, loss = 0.07927115\n",
      "Iteration 262, loss = 0.37717454\n",
      "Iteration 761, loss = 0.33280423\n",
      "Iteration 966, loss = 0.26327303\n",
      "Iteration 263, loss = 0.37698264\n",
      "Iteration 102, loss = 0.49821610\n",
      "Iteration 1506, loss = 0.23812095\n",
      "Iteration 762, loss = 0.33269363\n",
      "Iteration 264, loss = 0.37679786\n",
      "Iteration 459, loss = 0.39777870\n",
      "Iteration 343, loss = 0.37367460\n",
      "Iteration 2375, loss = 0.07912255\n",
      "Iteration 344, loss = 0.37345527\n",
      "Iteration 2376, loss = 0.07919370\n",
      "Iteration 1507, loss = 0.23803345\n",
      "Iteration 345, loss = 0.37313929\n",
      "Iteration 2377, loss = 0.07910247\n",
      "Iteration 763, loss = 0.33252571\n",
      "Iteration 346, loss = 0.37289849\n",
      "Iteration 967, loss = 0.26309252\n",
      "Iteration 2378, loss = 0.07899444\n",
      "Iteration 347, loss = 0.37262592\n",
      "Iteration 265, loss = 0.37665408\n",
      "Iteration 2379, loss = 0.07886175\n",
      "Iteration 348, loss = 0.37240787\n",
      "Iteration 968, loss = 0.26289456\n",
      "Iteration 2380, loss = 0.07882537\n",
      "Iteration 349, loss = 0.37211042\n",
      "Iteration 103, loss = 0.49646817\n",
      "Iteration 266, loss = 0.37645483\n",
      "Iteration 2381, loss = 0.07880320\n",
      "Iteration 350, loss = 0.37198323\n",
      "Iteration 2382, loss = 0.07870504\n",
      "Iteration 267, loss = 0.37627801\n",
      "Iteration 1508, loss = 0.23800006\n",
      "Iteration 969, loss = 0.26276466\n",
      "Iteration 351, loss = 0.37161535\n",
      "Iteration 268, loss = 0.37609950\n",
      "Iteration 2383, loss = 0.07872573\n",
      "Iteration 1509, loss = 0.23782752\n",
      "Iteration 764, loss = 0.33255028\n",
      "Iteration 352, loss = 0.37134690\n",
      "Iteration 269, loss = 0.37591666\n",
      "Iteration 2384, loss = 0.07883828\n",
      "Iteration 353, loss = 0.37110905\n",
      "Iteration 104, loss = 0.49485054\n",
      "Iteration 460, loss = 0.39745951\n",
      "Iteration 2385, loss = 0.07856148\n",
      "Iteration 970, loss = 0.26250375\n",
      "Iteration 765, loss = 0.33229321\n",
      "Iteration 1510, loss = 0.23767184\n",
      "Iteration 2386, loss = 0.07849983\n",
      "Iteration 270, loss = 0.37574878\n",
      "Iteration 354, loss = 0.37086136\n",
      "Iteration 105, loss = 0.49327622\n",
      "Iteration 271, loss = 0.37559839\n",
      "Iteration 2387, loss = 0.07849726\n",
      "Iteration 766, loss = 0.33216448\n",
      "Iteration 461, loss = 0.39721685\n",
      "Iteration 355, loss = 0.37059642\n",
      "Iteration 2388, loss = 0.07843285\n",
      "Iteration 272, loss = 0.37544536\n",
      "Iteration 1511, loss = 0.23752611\n",
      "Iteration 356, loss = 0.37035422\n",
      "Iteration 971, loss = 0.26218985\n",
      "Iteration 2389, loss = 0.07842737\n",
      "Iteration 2390, loss = 0.07831135\n",
      "Iteration 357, loss = 0.37015340\n",
      "Iteration 2391, loss = 0.07831487\n",
      "Iteration 1512, loss = 0.23742303\n",
      "Iteration 462, loss = 0.39699560\n",
      "Iteration 358, loss = 0.36983955\n",
      "Iteration 972, loss = 0.26225907\n",
      "Iteration 767, loss = 0.33201399\n",
      "Iteration 1513, loss = 0.23732304\n",
      "Iteration 2392, loss = 0.07816214\n",
      "Iteration 106, loss = 0.49165447\n",
      "Iteration 273, loss = 0.37521274\n",
      "Iteration 359, loss = 0.36958244\n",
      "Iteration 1514, loss = 0.23721753\n",
      "Iteration 2393, loss = 0.07816823\n",
      "Iteration 360, loss = 0.36934396\n",
      "Iteration 768, loss = 0.33192340\n",
      "Iteration 1515, loss = 0.23707354\n",
      "Iteration 2394, loss = 0.07812551\n",
      "Iteration 361, loss = 0.36910411\n",
      "Iteration 2395, loss = 0.07805154\n",
      "Iteration 463, loss = 0.39677096\n",
      "Iteration 274, loss = 0.37507327\n",
      "Iteration 362, loss = 0.36888722\n",
      "Iteration 769, loss = 0.33176499\n",
      "Iteration 973, loss = 0.26179609\n",
      "Iteration 107, loss = 0.49005847\n",
      "Iteration 363, loss = 0.36867849\n",
      "Iteration 1516, loss = 0.23693789\n",
      "Iteration 2396, loss = 0.07798139\n",
      "Iteration 364, loss = 0.36839744\n",
      "Iteration 275, loss = 0.37490280\n",
      "Iteration 365, loss = 0.36814447\n",
      "Iteration 770, loss = 0.33167856\n",
      "Iteration 2397, loss = 0.07802513\n",
      "Iteration 366, loss = 0.36792598\n",
      "Iteration 1517, loss = 0.23682713\n",
      "Iteration 464, loss = 0.39654998\n",
      "Iteration 276, loss = 0.37475538\n",
      "Iteration 771, loss = 0.33159576\n",
      "Iteration 1518, loss = 0.23673596\n",
      "Iteration 2398, loss = 0.07789732\n",
      "Iteration 367, loss = 0.36770542\n",
      "Iteration 974, loss = 0.26157272\n",
      "Iteration 108, loss = 0.48855918\n",
      "Iteration 368, loss = 0.36743712\n",
      "Iteration 2399, loss = 0.07790849\n",
      "Iteration 772, loss = 0.33146310\n",
      "Iteration 1519, loss = 0.23665484\n",
      "Iteration 465, loss = 0.39636055\n",
      "Iteration 277, loss = 0.37455462\n",
      "Iteration 369, loss = 0.36722558\n",
      "Iteration 2400, loss = 0.07777983\n",
      "Iteration 370, loss = 0.36698972\n",
      "Iteration 773, loss = 0.33128453\n",
      "Iteration 975, loss = 0.26138412\n",
      "Iteration 1520, loss = 0.23649709\n",
      "Iteration 371, loss = 0.36677834\n",
      "Iteration 2401, loss = 0.07768970\n",
      "Iteration 278, loss = 0.37438789\n",
      "Iteration 466, loss = 0.39617278\n",
      "Iteration 109, loss = 0.48712400\n",
      "Iteration 372, loss = 0.36654400\n",
      "Iteration 774, loss = 0.33117044\n",
      "Iteration 279, loss = 0.37418694\n",
      "Iteration 373, loss = 0.36633020\n",
      "Iteration 2402, loss = 0.07769130\n",
      "Iteration 374, loss = 0.36609460\n",
      "Iteration 1521, loss = 0.23640568\n",
      "Iteration 775, loss = 0.33102556\n",
      "Iteration 976, loss = 0.26120333\n",
      "Iteration 280, loss = 0.37401671\n",
      "Iteration 467, loss = 0.39592733\n",
      "Iteration 375, loss = 0.36586972\n",
      "Iteration 1522, loss = 0.23626798\n",
      "Iteration 110, loss = 0.48568781\n",
      "Iteration 281, loss = 0.37386581\n",
      "Iteration 776, loss = 0.33092680\n",
      "Iteration 2403, loss = 0.07759969\n",
      "Iteration 376, loss = 0.36568241\n",
      "Iteration 282, loss = 0.37371912\n",
      "Iteration 377, loss = 0.36545888\n",
      "Iteration 977, loss = 0.26098135\n",
      "Iteration 777, loss = 0.33080162\n",
      "Iteration 378, loss = 0.36527475\n",
      "Iteration 2404, loss = 0.07760771\n",
      "Iteration 468, loss = 0.39570711\n",
      "Iteration 2405, loss = 0.07764688\n",
      "Iteration 283, loss = 0.37352929\n",
      "Iteration 978, loss = 0.26088439\n",
      "Iteration 1523, loss = 0.23613662\n",
      "Iteration 2406, loss = 0.07744619\n",
      "Iteration 379, loss = 0.36499938\n",
      "Iteration 778, loss = 0.33062577\n",
      "Iteration 1524, loss = 0.23608027\n",
      "Iteration 111, loss = 0.48421604\n",
      "Iteration 2407, loss = 0.07738910\n",
      "Iteration 2408, loss = 0.07731741\n",
      "Iteration 1525, loss = 0.23592371\n",
      "Iteration 284, loss = 0.37335617\n",
      "Iteration 2409, loss = 0.07732352\n",
      "Iteration 1526, loss = 0.23582914\n",
      "Iteration 2410, loss = 0.07721865\n",
      "Iteration 112, loss = 0.48289267\n",
      "Iteration 979, loss = 0.26059322\n",
      "Iteration 285, loss = 0.37319326\n",
      "Iteration 469, loss = 0.39548018\n",
      "Iteration 380, loss = 0.36476019\n",
      "Iteration 779, loss = 0.33054402\n",
      "Iteration 1527, loss = 0.23571464\n",
      "Iteration 2411, loss = 0.07724295\n",
      "Iteration 381, loss = 0.36458804\n",
      "Iteration 113, loss = 0.48151150\n",
      "Iteration 2412, loss = 0.07725102\n",
      "Iteration 382, loss = 0.36435400\n",
      "Iteration 780, loss = 0.33039612\n",
      "Iteration 470, loss = 0.39523258\n",
      "Iteration 980, loss = 0.26041995\n",
      "Iteration 1528, loss = 0.23558751\n",
      "Iteration 383, loss = 0.36412136\n",
      "Iteration 2413, loss = 0.07705655\n",
      "Iteration 384, loss = 0.36393902\n",
      "Iteration 2414, loss = 0.07702754\n",
      "Iteration 781, loss = 0.33025604\n",
      "Iteration 114, loss = 0.48022803\n",
      "Iteration 1529, loss = 0.23548630\n",
      "Iteration 981, loss = 0.26016695\n",
      "Iteration 286, loss = 0.37306628\n",
      "Iteration 471, loss = 0.39503019\n",
      "Iteration 2415, loss = 0.07695946\n",
      "Iteration 1530, loss = 0.23531134\n",
      "Iteration 982, loss = 0.26000116\n",
      "Iteration 287, loss = 0.37286239\n",
      "Iteration 2416, loss = 0.07706444\n",
      "Iteration 782, loss = 0.33019966\n",
      "Iteration 2417, loss = 0.07689236\n",
      "Iteration 1531, loss = 0.23522542\n",
      "Iteration 115, loss = 0.47888802\n",
      "Iteration 288, loss = 0.37273692\n",
      "Iteration 983, loss = 0.25986201\n",
      "Iteration 1532, loss = 0.23509229\n",
      "Iteration 2418, loss = 0.07682554\n",
      "Iteration 385, loss = 0.36375950\n",
      "Iteration 289, loss = 0.37256247\n",
      "Iteration 2419, loss = 0.07681751\n",
      "Iteration 472, loss = 0.39481920\n",
      "Iteration 1533, loss = 0.23500653\n",
      "Iteration 116, loss = 0.47759194\n",
      "Iteration 2420, loss = 0.07686814\n",
      "Iteration 783, loss = 0.33001943\n",
      "Iteration 290, loss = 0.37243492\n",
      "Iteration 2421, loss = 0.07670203\n",
      "Iteration 1534, loss = 0.23485866\n",
      "Iteration 291, loss = 0.37222706\n",
      "Iteration 984, loss = 0.25973418\n",
      "Iteration 2422, loss = 0.07659881\n",
      "Iteration 292, loss = 0.37209700\n",
      "Iteration 1535, loss = 0.23502164\n",
      "Iteration 2423, loss = 0.07656311\n",
      "Iteration 473, loss = 0.39458878\n",
      "Iteration 985, loss = 0.25936503\n",
      "Iteration 293, loss = 0.37194518\n",
      "Iteration 784, loss = 0.32984268\n",
      "Iteration 2424, loss = 0.07650512\n",
      "Iteration 117, loss = 0.47638157\n",
      "Iteration 294, loss = 0.37180773\n",
      "Iteration 2425, loss = 0.07645552\n",
      "Iteration 1536, loss = 0.23467636\n",
      "Iteration 2426, loss = 0.07645760\n",
      "Iteration 986, loss = 0.25942224\n",
      "Iteration 295, loss = 0.37163254\n",
      "Iteration 474, loss = 0.39436069\n",
      "Iteration 2427, loss = 0.07641469\n",
      "Iteration 1537, loss = 0.23448024\n",
      "Iteration 296, loss = 0.37145889\n",
      "Iteration 987, loss = 0.25907406\n",
      "Iteration 785, loss = 0.32973982\n",
      "Iteration 2428, loss = 0.07651237\n",
      "Iteration 118, loss = 0.47515422\n",
      "Iteration 1538, loss = 0.23438135\n",
      "Iteration 297, loss = 0.37131147\n",
      "Iteration 475, loss = 0.39419308\n",
      "Iteration 2429, loss = 0.07636706\n",
      "Iteration 988, loss = 0.25875063\n",
      "Iteration 2430, loss = 0.07632671\n",
      "Iteration 786, loss = 0.32959157\n",
      "Iteration 1539, loss = 0.23423721\n",
      "Iteration 2431, loss = 0.07617766\n",
      "Iteration 119, loss = 0.47396089\n",
      "Iteration 298, loss = 0.37117009\n",
      "Iteration 989, loss = 0.25851745\n",
      "Iteration 2432, loss = 0.07611964\n",
      "Iteration 1540, loss = 0.23427473\n",
      "Iteration 2433, loss = 0.07610524\n",
      "Iteration 299, loss = 0.37099487\n",
      "Iteration 787, loss = 0.32954208\n",
      "Iteration 2434, loss = 0.07604129\n",
      "Iteration 300, loss = 0.37082934\n",
      "Iteration 476, loss = 0.39394219\n",
      "Iteration 2435, loss = 0.07594746\n",
      "Iteration 990, loss = 0.25840451\n",
      "Iteration 2436, loss = 0.07595698\n",
      "Iteration 1541, loss = 0.23401207\n",
      "Iteration 120, loss = 0.47285923\n",
      "Iteration 788, loss = 0.32935605\n",
      "Iteration 301, loss = 0.37069987\n",
      "Iteration 2437, loss = 0.07589076\n",
      "Iteration 991, loss = 0.25825908\n",
      "Iteration 302, loss = 0.37053867\n",
      "Iteration 2438, loss = 0.07585491\n",
      "Iteration 1542, loss = 0.23392727\n",
      "Iteration 789, loss = 0.32923806\n",
      "Iteration 2439, loss = 0.07579769\n",
      "Iteration 1543, loss = 0.23381538\n",
      "Iteration 121, loss = 0.47169193\n",
      "Iteration 992, loss = 0.25800910\n",
      "Iteration 2440, loss = 0.07571118\n",
      "Iteration 303, loss = 0.37047733\n",
      "Iteration 2441, loss = 0.07563430\n",
      "Iteration 477, loss = 0.39378069\n",
      "Iteration 2442, loss = 0.07563517\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 304, loss = 0.37025898\n",
      "Iteration 386, loss = 0.36352791\n",
      "Iteration 305, loss = 0.37011463\n",
      "Iteration 1544, loss = 0.23383590\n",
      "Iteration 993, loss = 0.25771810\n",
      "Iteration 790, loss = 0.32910274\n",
      "Iteration 306, loss = 0.36994977\n",
      "Iteration 122, loss = 0.47058509\n",
      "Iteration 387, loss = 0.36327174\n",
      "Iteration 478, loss = 0.39361177\n",
      "Iteration 1545, loss = 0.23360687\n",
      "Iteration 307, loss = 0.36982969Iteration 791, loss = 0.32897064\n",
      "\n",
      "Iteration 994, loss = 0.25758833\n",
      "Iteration 1, loss = 0.78204619\n",
      "Iteration 1546, loss = 0.23344721\n",
      "Iteration 388, loss = 0.36308500\n",
      "Iteration 995, loss = 0.25732044\n",
      "Iteration 479, loss = 0.39327810\n",
      "Iteration 389, loss = 0.36287270\n",
      "Iteration 123, loss = 0.46948176\n",
      "Iteration 308, loss = 0.36966936\n",
      "Iteration 2, loss = 0.78073113\n",
      "Iteration 792, loss = 0.32881933\n",
      "Iteration 309, loss = 0.36954794\n",
      "Iteration 1547, loss = 0.23333642\n",
      "Iteration 996, loss = 0.25708752\n",
      "Iteration 3, loss = 0.77870967\n",
      "Iteration 124, loss = 0.46840413\n",
      "Iteration 1548, loss = 0.23320846\n",
      "Iteration 310, loss = 0.36938328\n",
      "Iteration 793, loss = 0.32870929\n",
      "Iteration 1549, loss = 0.23319406\n",
      "Iteration 4, loss = 0.77626717\n",
      "Iteration 997, loss = 0.25711745\n",
      "Iteration 480, loss = 0.39307525\n",
      "Iteration 311, loss = 0.36923256\n",
      "Iteration 1550, loss = 0.23308434\n",
      "Iteration 125, loss = 0.46738249\n",
      "Iteration 5, loss = 0.77354155\n",
      "Iteration 312, loss = 0.36910421\n",
      "Iteration 998, loss = 0.25664496Iteration 1551, loss = 0.23284026\n",
      "\n",
      "Iteration 313, loss = 0.36893222\n",
      "Iteration 794, loss = 0.32856470\n",
      "Iteration 126, loss = 0.46630216\n",
      "Iteration 999, loss = 0.25659830\n",
      "Iteration 6, loss = 0.77062830\n",
      "Iteration 390, loss = 0.36269690\n",
      "Iteration 795, loss = 0.32843668\n",
      "Iteration 1552, loss = 0.23272581\n",
      "Iteration 127, loss = 0.46529266\n",
      "Iteration 391, loss = 0.36247218\n",
      "Iteration 314, loss = 0.36879794\n",
      "Iteration 481, loss = 0.39291993\n",
      "Iteration 7, loss = 0.76766823\n",
      "Iteration 315, loss = 0.36865847\n",
      "Iteration 392, loss = 0.36229391\n",
      "Iteration 796, loss = 0.32832689\n",
      "Iteration 393, loss = 0.36206953\n",
      "Iteration 1553, loss = 0.23263230\n",
      "Iteration 8, loss = 0.76469985\n",
      "Iteration 1000, loss = 0.25628383\n",
      "Iteration 316, loss = 0.36850939\n",
      "Iteration 797, loss = 0.32818214\n",
      "Iteration 128, loss = 0.46432457\n",
      "Iteration 394, loss = 0.36186501\n",
      "Iteration 9, loss = 0.76166324\n",
      "Iteration 317, loss = 0.36838145\n",
      "Iteration 1554, loss = 0.23263561\n",
      "Iteration 482, loss = 0.39280842\n",
      "Iteration 798, loss = 0.32805513\n",
      "Iteration 318, loss = 0.36823963\n",
      "Iteration 10, loss = 0.75871533\n",
      "Iteration 1555, loss = 0.23244331\n",
      "Iteration 319, loss = 0.36811358\n",
      "Iteration 1556, loss = 0.23228640\n",
      "Iteration 799, loss = 0.32793320\n",
      "Iteration 320, loss = 0.36795629\n",
      "Iteration 395, loss = 0.36163258\n",
      "Iteration 1557, loss = 0.23224365\n",
      "Iteration 396, loss = 0.36143688\n",
      "Iteration 483, loss = 0.39243557\n",
      "Iteration 321, loss = 0.36788318\n",
      "Iteration 397, loss = 0.36126465\n",
      "Iteration 1558, loss = 0.23206305\n",
      "Iteration 1001, loss = 0.25605922\n",
      "Iteration 11, loss = 0.75576365\n",
      "Iteration 129, loss = 0.46336731\n",
      "Iteration 1559, loss = 0.23189290\n",
      "Iteration 322, loss = 0.36768370\n",
      "Iteration 1002, loss = 0.25599912\n",
      "Iteration 398, loss = 0.36103133\n",
      "Iteration 800, loss = 0.32780111\n",
      "Iteration 484, loss = 0.39219544\n",
      "Iteration 12, loss = 0.75285635\n",
      "Iteration 323, loss = 0.36755562\n",
      "Iteration 1003, loss = 0.25564883\n",
      "Iteration 130, loss = 0.46240091\n",
      "Iteration 1560, loss = 0.23190165\n",
      "Iteration 801, loss = 0.32768473\n",
      "Iteration 13, loss = 0.75012689\n",
      "Iteration 1561, loss = 0.23172201\n",
      "Iteration 324, loss = 0.36742896\n",
      "Iteration 1004, loss = 0.25550216\n",
      "Iteration 1562, loss = 0.23158463\n",
      "Iteration 131, loss = 0.46141933\n",
      "Iteration 14, loss = 0.74743748\n",
      "Iteration 1563, loss = 0.23152169\n",
      "Iteration 485, loss = 0.39195921\n",
      "Iteration 325, loss = 0.36727621\n",
      "Iteration 1564, loss = 0.23143505\n",
      "Iteration 15, loss = 0.74480474\n",
      "Iteration 132, loss = 0.46051539\n",
      "Iteration 1005, loss = 0.25545836\n",
      "Iteration 1565, loss = 0.23121893\n",
      "Iteration 802, loss = 0.32760048\n",
      "Iteration 399, loss = 0.36089832\n",
      "Iteration 400, loss = 0.36067557\n",
      "Iteration 486, loss = 0.39179375\n",
      "Iteration 401, loss = 0.36043800\n",
      "Iteration 402, loss = 0.36023938\n",
      "Iteration 1566, loss = 0.23112127\n",
      "Iteration 16, loss = 0.74206975\n",
      "Iteration 403, loss = 0.36004358\n",
      "Iteration 1006, loss = 0.25505435\n",
      "Iteration 133, loss = 0.45961276\n",
      "Iteration 404, loss = 0.35981861\n",
      "Iteration 803, loss = 0.32742631\n",
      "Iteration 1567, loss = 0.23099546\n",
      "Iteration 405, loss = 0.35963876\n",
      "Iteration 406, loss = 0.35945956\n",
      "Iteration 326, loss = 0.36718161\n",
      "Iteration 407, loss = 0.35922812\n",
      "Iteration 1568, loss = 0.23088009\n",
      "Iteration 17, loss = 0.73950607\n",
      "Iteration 408, loss = 0.35908729\n",
      "Iteration 327, loss = 0.36704332Iteration 1569, loss = 0.23087753\n",
      "\n",
      "Iteration 487, loss = 0.39163953\n",
      "Iteration 1570, loss = 0.23063326\n",
      "Iteration 18, loss = 0.73706433\n",
      "Iteration 1007, loss = 0.25483868\n",
      "Iteration 409, loss = 0.35883337\n",
      "Iteration 1571, loss = 0.23069878\n",
      "Iteration 804, loss = 0.32737349\n",
      "Iteration 410, loss = 0.35868217\n",
      "Iteration 1008, loss = 0.25466086\n",
      "Iteration 411, loss = 0.35845366\n",
      "Iteration 412, loss = 0.35827803\n",
      "Iteration 19, loss = 0.73445573\n",
      "Iteration 134, loss = 0.45869943\n",
      "Iteration 413, loss = 0.35809312\n",
      "Iteration 414, loss = 0.35792061\n",
      "Iteration 1009, loss = 0.25452721\n",
      "Iteration 805, loss = 0.32720829\n",
      "Iteration 415, loss = 0.35776564\n",
      "Iteration 488, loss = 0.39137310\n",
      "Iteration 328, loss = 0.36687380\n",
      "Iteration 416, loss = 0.35750631\n",
      "Iteration 417, loss = 0.35731257\n",
      "Iteration 329, loss = 0.36676030\n",
      "Iteration 418, loss = 0.35714004\n",
      "Iteration 20, loss = 0.73206489\n",
      "Iteration 135, loss = 0.45782194\n",
      "Iteration 419, loss = 0.35694155\n",
      "Iteration 420, loss = 0.35679401\n",
      "Iteration 1010, loss = 0.25436972\n",
      "Iteration 1572, loss = 0.23070935\n",
      "Iteration 421, loss = 0.35657010\n",
      "Iteration 806, loss = 0.32703424\n",
      "Iteration 21, loss = 0.72948007\n",
      "Iteration 1011, loss = 0.25399823\n",
      "Iteration 22, loss = 0.72709925\n",
      "Iteration 330, loss = 0.36662685\n",
      "Iteration 1573, loss = 0.23037097\n",
      "Iteration 1012, loss = 0.25391027\n",
      "Iteration 422, loss = 0.35637930\n",
      "Iteration 423, loss = 0.35621122\n",
      "Iteration 489, loss = 0.39113437\n",
      "Iteration 424, loss = 0.35602564\n",
      "Iteration 807, loss = 0.32690995\n",
      "Iteration 1574, loss = 0.23016533\n",
      "Iteration 136, loss = 0.45698663\n",
      "Iteration 331, loss = 0.36650623\n",
      "Iteration 425, loss = 0.35583466\n",
      "Iteration 426, loss = 0.35570650\n",
      "Iteration 332, loss = 0.36637662\n",
      "Iteration 23, loss = 0.72461757\n",
      "Iteration 427, loss = 0.35551388\n",
      "Iteration 1575, loss = 0.23009080\n",
      "Iteration 1013, loss = 0.25359767\n",
      "Iteration 490, loss = 0.39087497\n",
      "Iteration 428, loss = 0.35530333\n",
      "Iteration 1014, loss = 0.25356599\n",
      "Iteration 137, loss = 0.45616578\n",
      "Iteration 333, loss = 0.36622718\n",
      "Iteration 429, loss = 0.35514101\n",
      "Iteration 1576, loss = 0.22996746\n",
      "Iteration 430, loss = 0.35494855\n",
      "Iteration 334, loss = 0.36611450\n",
      "Iteration 431, loss = 0.35476704\n",
      "Iteration 1577, loss = 0.22979112\n",
      "Iteration 432, loss = 0.35466906\n",
      "Iteration 335, loss = 0.36598816\n",
      "Iteration 433, loss = 0.35440449\n",
      "Iteration 1578, loss = 0.22973051\n",
      "Iteration 24, loss = 0.72231337\n",
      "Iteration 491, loss = 0.39073734\n",
      "Iteration 434, loss = 0.35422094\n",
      "Iteration 808, loss = 0.32678867\n",
      "Iteration 1015, loss = 0.25327037\n",
      "Iteration 336, loss = 0.36584371\n",
      "Iteration 1579, loss = 0.22961535\n",
      "Iteration 435, loss = 0.35403577\n",
      "Iteration 436, loss = 0.35384609\n",
      "Iteration 1016, loss = 0.25301925\n",
      "Iteration 437, loss = 0.35369640\n",
      "Iteration 138, loss = 0.45529415\n",
      "Iteration 337, loss = 0.36571492\n",
      "Iteration 492, loss = 0.39055001\n",
      "Iteration 25, loss = 0.71985774\n",
      "Iteration 438, loss = 0.35352468\n",
      "Iteration 809, loss = 0.32664281\n",
      "Iteration 1017, loss = 0.25279433\n",
      "Iteration 439, loss = 0.35330534\n",
      "Iteration 338, loss = 0.36574491\n",
      "Iteration 1018, loss = 0.25262433\n",
      "Iteration 440, loss = 0.35315594\n",
      "Iteration 1580, loss = 0.22955126\n",
      "Iteration 810, loss = 0.32659649\n",
      "Iteration 339, loss = 0.36543459\n",
      "Iteration 26, loss = 0.71752417\n",
      "Iteration 139, loss = 0.45445636\n",
      "Iteration 493, loss = 0.39028939\n",
      "Iteration 1581, loss = 0.22943320\n",
      "Iteration 140, loss = 0.45371175\n",
      "Iteration 1019, loss = 0.25234521\n",
      "Iteration 441, loss = 0.35299326\n",
      "Iteration 811, loss = 0.32638489\n",
      "Iteration 340, loss = 0.36532058\n",
      "Iteration 27, loss = 0.71510918\n",
      "Iteration 1020, loss = 0.25219966\n",
      "Iteration 442, loss = 0.35277854\n",
      "Iteration 141, loss = 0.45284500\n",
      "Iteration 1582, loss = 0.22935711\n",
      "Iteration 812, loss = 0.32624331\n",
      "Iteration 341, loss = 0.36518067\n",
      "Iteration 443, loss = 0.35263677\n",
      "Iteration 1583, loss = 0.22943482\n",
      "Iteration 342, loss = 0.36506393\n",
      "Iteration 1584, loss = 0.22903447\n",
      "Iteration 444, loss = 0.35252380\n",
      "Iteration 343, loss = 0.36492795\n",
      "Iteration 1021, loss = 0.25193075\n",
      "Iteration 1585, loss = 0.22893884\n",
      "Iteration 28, loss = 0.71274508\n",
      "Iteration 445, loss = 0.35225307\n",
      "Iteration 142, loss = 0.45209393\n",
      "Iteration 494, loss = 0.39004175\n",
      "Iteration 446, loss = 0.35208557\n",
      "Iteration 29, loss = 0.71045565\n",
      "Iteration 447, loss = 0.35191860\n",
      "Iteration 1022, loss = 0.25169860\n",
      "Iteration 448, loss = 0.35173764\n",
      "Iteration 1586, loss = 0.22879563\n",
      "Iteration 30, loss = 0.70806073\n",
      "Iteration 813, loss = 0.32615736\n",
      "Iteration 344, loss = 0.36479612\n",
      "Iteration 449, loss = 0.35156764\n",
      "Iteration 143, loss = 0.45129822\n",
      "Iteration 1023, loss = 0.25164489\n",
      "Iteration 1587, loss = 0.22876166\n",
      "Iteration 814, loss = 0.32604962\n",
      "Iteration 450, loss = 0.35147236\n",
      "Iteration 144, loss = 0.45059917\n",
      "Iteration 345, loss = 0.36470483\n",
      "Iteration 451, loss = 0.35121291\n",
      "Iteration 346, loss = 0.36454587\n",
      "Iteration 1024, loss = 0.25132659\n",
      "Iteration 815, loss = 0.32592377\n",
      "Iteration 1588, loss = 0.22857396\n",
      "Iteration 347, loss = 0.36440776\n",
      "Iteration 1589, loss = 0.22843805\n",
      "Iteration 452, loss = 0.35110015\n",
      "Iteration 31, loss = 0.70562010\n",
      "Iteration 348, loss = 0.36431443\n",
      "Iteration 495, loss = 0.38985366\n",
      "Iteration 1025, loss = 0.25111567\n",
      "Iteration 453, loss = 0.35088627\n",
      "Iteration 816, loss = 0.32577621\n",
      "Iteration 454, loss = 0.35074184\n",
      "Iteration 455, loss = 0.35054390\n",
      "Iteration 145, loss = 0.44979801\n",
      "Iteration 1590, loss = 0.22834290\n",
      "Iteration 456, loss = 0.35037756\n",
      "Iteration 457, loss = 0.35020831\n",
      "Iteration 1026, loss = 0.25093531\n",
      "Iteration 458, loss = 0.35000532\n",
      "Iteration 1591, loss = 0.22840022\n",
      "Iteration 459, loss = 0.34984618\n",
      "Iteration 32, loss = 0.70330714\n",
      "Iteration 349, loss = 0.36414937\n",
      "Iteration 496, loss = 0.38959868\n",
      "Iteration 146, loss = 0.44909687\n",
      "Iteration 460, loss = 0.34969871\n",
      "Iteration 817, loss = 0.32564446\n",
      "Iteration 350, loss = 0.36406355\n",
      "Iteration 33, loss = 0.70088778\n",
      "Iteration 1027, loss = 0.25074239\n",
      "Iteration 461, loss = 0.34955765\n",
      "Iteration 351, loss = 0.36395301\n",
      "Iteration 462, loss = 0.34935079\n",
      "Iteration 1592, loss = 0.22810300\n",
      "Iteration 34, loss = 0.69858593\n",
      "Iteration 818, loss = 0.32549525\n",
      "Iteration 463, loss = 0.34917847\n",
      "Iteration 1028, loss = 0.25048637\n",
      "Iteration 352, loss = 0.36381036\n",
      "Iteration 1593, loss = 0.22797375\n",
      "Iteration 147, loss = 0.44834859\n",
      "Iteration 497, loss = 0.38938038\n",
      "Iteration 464, loss = 0.34900970\n",
      "Iteration 819, loss = 0.32536869\n",
      "Iteration 1029, loss = 0.25026164\n",
      "Iteration 353, loss = 0.36365152\n",
      "Iteration 1594, loss = 0.22794229\n",
      "Iteration 35, loss = 0.69609883\n",
      "Iteration 465, loss = 0.34884791\n",
      "Iteration 466, loss = 0.34867173\n",
      "Iteration 820, loss = 0.32524479\n",
      "Iteration 354, loss = 0.36354024\n",
      "Iteration 1595, loss = 0.22776996\n",
      "Iteration 467, loss = 0.34850033\n",
      "Iteration 36, loss = 0.69364892\n",
      "Iteration 468, loss = 0.34833404\n",
      "Iteration 1030, loss = 0.25009626\n",
      "Iteration 469, loss = 0.34823315\n",
      "Iteration 498, loss = 0.38918261\n",
      "Iteration 1596, loss = 0.22770763\n",
      "Iteration 148, loss = 0.44762017\n",
      "Iteration 355, loss = 0.36340402\n",
      "Iteration 470, loss = 0.34802570\n",
      "Iteration 1597, loss = 0.22756834\n",
      "Iteration 356, loss = 0.36333610\n",
      "Iteration 37, loss = 0.69124884\n",
      "Iteration 1031, loss = 0.24989872\n",
      "Iteration 149, loss = 0.44692191\n",
      "Iteration 1598, loss = 0.22743507\n",
      "Iteration 821, loss = 0.32510396\n",
      "Iteration 357, loss = 0.36314365\n",
      "Iteration 1599, loss = 0.22736098\n",
      "Iteration 38, loss = 0.68889578\n",
      "Iteration 358, loss = 0.36303280\n",
      "Iteration 499, loss = 0.38895911\n",
      "Iteration 471, loss = 0.34783537\n",
      "Iteration 1032, loss = 0.24968029\n",
      "Iteration 39, loss = 0.68640958\n",
      "Iteration 150, loss = 0.44625252\n",
      "Iteration 472, loss = 0.34781618\n",
      "Iteration 822, loss = 0.32499016\n",
      "Iteration 473, loss = 0.34752119\n",
      "Iteration 1600, loss = 0.22715556\n",
      "Iteration 474, loss = 0.34735844\n",
      "Iteration 40, loss = 0.68383523\n",
      "Iteration 151, loss = 0.44559909\n",
      "Iteration 475, loss = 0.34723053\n",
      "Iteration 1601, loss = 0.22707651\n",
      "Iteration 1033, loss = 0.24959602\n",
      "Iteration 476, loss = 0.34702833\n",
      "Iteration 823, loss = 0.32494144\n",
      "Iteration 359, loss = 0.36289131\n",
      "Iteration 41, loss = 0.68148205\n",
      "Iteration 477, loss = 0.34684150\n",
      "Iteration 478, loss = 0.34668694\n",
      "Iteration 1602, loss = 0.22696851\n",
      "Iteration 152, loss = 0.44487800\n",
      "Iteration 479, loss = 0.34657903\n",
      "Iteration 1603, loss = 0.22696912\n",
      "Iteration 480, loss = 0.34639215\n",
      "Iteration 360, loss = 0.36278820\n",
      "Iteration 481, loss = 0.34622314\n",
      "Iteration 1034, loss = 0.24921427\n",
      "Iteration 1604, loss = 0.22668031\n",
      "Iteration 42, loss = 0.67897067\n",
      "Iteration 482, loss = 0.34606292\n",
      "Iteration 361, loss = 0.36265227\n",
      "Iteration 483, loss = 0.34587911\n",
      "Iteration 500, loss = 0.38878059\n",
      "Iteration 824, loss = 0.32471710\n",
      "Iteration 1035, loss = 0.24914197\n",
      "Iteration 153, loss = 0.44424559\n",
      "Iteration 484, loss = 0.34574570\n",
      "Iteration 362, loss = 0.36251086\n",
      "Iteration 485, loss = 0.34556582\n",
      "Iteration 43, loss = 0.67646225\n",
      "Iteration 486, loss = 0.34540507\n",
      "Iteration 1605, loss = 0.22656405\n",
      "Iteration 487, loss = 0.34525417\n",
      "Iteration 825, loss = 0.32460457\n",
      "Iteration 488, loss = 0.34510490\n",
      "Iteration 489, loss = 0.34494042\n",
      "Iteration 363, loss = 0.36241249\n",
      "Iteration 501, loss = 0.38857327\n",
      "Iteration 490, loss = 0.34475949\n",
      "Iteration 154, loss = 0.44357025\n",
      "Iteration 1036, loss = 0.24891483\n",
      "Iteration 826, loss = 0.32451605\n",
      "Iteration 44, loss = 0.67396527\n",
      "Iteration 1606, loss = 0.22654501\n",
      "Iteration 491, loss = 0.34459852\n",
      "Iteration 1037, loss = 0.24867504\n",
      "Iteration 155, loss = 0.44293505\n",
      "Iteration 45, loss = 0.67143068\n",
      "Iteration 1607, loss = 0.22635831\n",
      "Iteration 364, loss = 0.36232519\n",
      "Iteration 492, loss = 0.34443622\n",
      "Iteration 1038, loss = 0.24848138\n",
      "Iteration 502, loss = 0.38828963\n",
      "Iteration 493, loss = 0.34427893\n",
      "Iteration 1608, loss = 0.22628725\n",
      "Iteration 365, loss = 0.36214097\n",
      "Iteration 366, loss = 0.36202320\n",
      "Iteration 494, loss = 0.34414923\n",
      "Iteration 1039, loss = 0.24826266\n",
      "Iteration 1609, loss = 0.22619098\n",
      "Iteration 156, loss = 0.44231620\n",
      "Iteration 495, loss = 0.34402037\n",
      "Iteration 46, loss = 0.66886228\n",
      "Iteration 367, loss = 0.36194688\n",
      "Iteration 496, loss = 0.34387963\n",
      "Iteration 827, loss = 0.32436951\n",
      "Iteration 368, loss = 0.36177200\n",
      "Iteration 497, loss = 0.34368539\n",
      "Iteration 1610, loss = 0.22606786\n",
      "Iteration 503, loss = 0.38808297\n",
      "Iteration 47, loss = 0.66634704\n",
      "Iteration 498, loss = 0.34351405\n",
      "Iteration 1040, loss = 0.24805017\n",
      "Iteration 499, loss = 0.34335957\n",
      "Iteration 369, loss = 0.36165968\n",
      "Iteration 500, loss = 0.34319535\n",
      "Iteration 828, loss = 0.32421288\n",
      "Iteration 1611, loss = 0.22587565\n",
      "Iteration 501, loss = 0.34305225\n",
      "Iteration 48, loss = 0.66379423\n",
      "Iteration 157, loss = 0.44168153\n",
      "Iteration 370, loss = 0.36152751\n",
      "Iteration 502, loss = 0.34289562\n",
      "Iteration 1041, loss = 0.24780333\n",
      "Iteration 504, loss = 0.38784748\n",
      "Iteration 371, loss = 0.36140640\n",
      "Iteration 503, loss = 0.34287435\n",
      "Iteration 49, loss = 0.66116830\n",
      "Iteration 1612, loss = 0.22577908\n",
      "Iteration 504, loss = 0.34263304\n",
      "Iteration 50, loss = 0.65859502\n",
      "Iteration 158, loss = 0.44107124\n",
      "Iteration 1613, loss = 0.22579749\n",
      "Iteration 829, loss = 0.32412524\n",
      "Iteration 1614, loss = 0.22561052\n",
      "Iteration 372, loss = 0.36126446\n",
      "Iteration 505, loss = 0.34241512\n",
      "Iteration 506, loss = 0.34232443\n",
      "Iteration 1042, loss = 0.24769899\n",
      "Iteration 1615, loss = 0.22550436\n",
      "Iteration 505, loss = 0.38763090\n",
      "Iteration 373, loss = 0.36115451\n",
      "Iteration 51, loss = 0.65599140\n",
      "Iteration 830, loss = 0.32396774\n",
      "Iteration 1616, loss = 0.22541507\n",
      "Iteration 1043, loss = 0.24767188\n",
      "Iteration 507, loss = 0.34215275\n",
      "Iteration 374, loss = 0.36103445\n",
      "Iteration 159, loss = 0.44052837\n",
      "Iteration 508, loss = 0.34197653\n",
      "Iteration 509, loss = 0.34185151\n",
      "Iteration 831, loss = 0.32386621\n",
      "Iteration 1617, loss = 0.22519419\n",
      "Iteration 1044, loss = 0.24721815\n",
      "Iteration 52, loss = 0.65343253\n",
      "Iteration 510, loss = 0.34170056\n",
      "Iteration 506, loss = 0.38743710\n",
      "Iteration 160, loss = 0.43987574\n",
      "Iteration 375, loss = 0.36090876\n",
      "Iteration 1045, loss = 0.24701884\n",
      "Iteration 511, loss = 0.34151788\n",
      "Iteration 512, loss = 0.34136236\n",
      "Iteration 53, loss = 0.65073804\n",
      "Iteration 161, loss = 0.43929110\n",
      "Iteration 1618, loss = 0.22516433\n",
      "Iteration 1046, loss = 0.24675402\n",
      "Iteration 507, loss = 0.38731993\n",
      "Iteration 513, loss = 0.34120432\n",
      "Iteration 376, loss = 0.36078307\n",
      "Iteration 832, loss = 0.32374291\n",
      "Iteration 54, loss = 0.64813868\n",
      "Iteration 1619, loss = 0.22503169\n",
      "Iteration 514, loss = 0.34103974\n",
      "Iteration 515, loss = 0.34090257\n",
      "Iteration 833, loss = 0.32358095\n",
      "Iteration 1620, loss = 0.22488691\n",
      "Iteration 516, loss = 0.34076371\n",
      "Iteration 162, loss = 0.43870613\n",
      "Iteration 1621, loss = 0.22476122\n",
      "Iteration 55, loss = 0.64549934\n",
      "Iteration 517, loss = 0.34057495\n",
      "Iteration 508, loss = 0.38698118\n",
      "Iteration 518, loss = 0.34044124\n",
      "Iteration 1622, loss = 0.22476425\n",
      "Iteration 519, loss = 0.34028787\n",
      "Iteration 56, loss = 0.64287590\n",
      "Iteration 377, loss = 0.36086576\n",
      "Iteration 520, loss = 0.34011885\n",
      "Iteration 834, loss = 0.32344467\n",
      "Iteration 1047, loss = 0.24682579\n",
      "Iteration 521, loss = 0.34001900\n",
      "Iteration 57, loss = 0.64010930\n",
      "Iteration 522, loss = 0.33980324\n",
      "Iteration 1623, loss = 0.22453512\n",
      "Iteration 523, loss = 0.33965118\n",
      "Iteration 524, loss = 0.33953097\n",
      "Iteration 1624, loss = 0.22442642\n",
      "Iteration 525, loss = 0.33935457\n",
      "Iteration 378, loss = 0.36054175\n",
      "Iteration 1048, loss = 0.24636971\n",
      "Iteration 163, loss = 0.43815451\n",
      "Iteration 509, loss = 0.38680678\n",
      "Iteration 526, loss = 0.33924257\n",
      "Iteration 527, loss = 0.33904370\n",
      "Iteration 379, loss = 0.36042986\n",
      "Iteration 1625, loss = 0.22433163\n",
      "Iteration 835, loss = 0.32335392\n",
      "Iteration 58, loss = 0.63745676\n",
      "Iteration 380, loss = 0.36029169\n",
      "Iteration 528, loss = 0.33888808\n",
      "Iteration 1049, loss = 0.24626267\n",
      "Iteration 164, loss = 0.43756683\n",
      "Iteration 59, loss = 0.63481276\n",
      "Iteration 381, loss = 0.36017961\n",
      "Iteration 529, loss = 0.33876584\n",
      "Iteration 1050, loss = 0.24592578\n",
      "Iteration 510, loss = 0.38653049\n",
      "Iteration 1626, loss = 0.22416630\n",
      "Iteration 60, loss = 0.63213291\n",
      "Iteration 836, loss = 0.32326958\n",
      "Iteration 530, loss = 0.33860759\n",
      "Iteration 1051, loss = 0.24577225\n",
      "Iteration 382, loss = 0.36005552\n",
      "Iteration 1627, loss = 0.22408255\n",
      "Iteration 531, loss = 0.33851488\n",
      "Iteration 165, loss = 0.43700213\n",
      "Iteration 532, loss = 0.33828924\n",
      "Iteration 1628, loss = 0.22398244\n",
      "Iteration 1052, loss = 0.24566099\n",
      "Iteration 383, loss = 0.35992877\n",
      "Iteration 511, loss = 0.38633799\n",
      "Iteration 1629, loss = 0.22381526\n",
      "Iteration 384, loss = 0.35994161\n",
      "Iteration 837, loss = 0.32307585\n",
      "Iteration 533, loss = 0.33813664\n",
      "Iteration 61, loss = 0.62938088\n",
      "Iteration 534, loss = 0.33803014\n",
      "Iteration 385, loss = 0.35968702\n",
      "Iteration 535, loss = 0.33782245\n",
      "Iteration 386, loss = 0.35957660\n",
      "Iteration 1630, loss = 0.22370756\n",
      "Iteration 512, loss = 0.38614063\n",
      "Iteration 166, loss = 0.43645389\n",
      "Iteration 1053, loss = 0.24530277\n",
      "Iteration 536, loss = 0.33772766\n",
      "Iteration 62, loss = 0.62667672\n",
      "Iteration 537, loss = 0.33753511\n",
      "Iteration 838, loss = 0.32303351\n",
      "Iteration 538, loss = 0.33736243\n",
      "Iteration 387, loss = 0.35944387\n",
      "Iteration 539, loss = 0.33722340\n",
      "Iteration 1631, loss = 0.22363157\n",
      "Iteration 167, loss = 0.43593307\n",
      "Iteration 63, loss = 0.62390668\n",
      "Iteration 513, loss = 0.38588997\n",
      "Iteration 540, loss = 0.33708936\n",
      "Iteration 1632, loss = 0.22346908Iteration 1054, loss = 0.24520855\n",
      "\n",
      "Iteration 388, loss = 0.35932423\n",
      "Iteration 168, loss = 0.43543380\n",
      "Iteration 1633, loss = 0.22334551\n",
      "Iteration 839, loss = 0.32281109\n",
      "Iteration 514, loss = 0.38566948\n",
      "Iteration 389, loss = 0.35921193\n",
      "Iteration 541, loss = 0.33696508\n",
      "Iteration 64, loss = 0.62126808\n",
      "Iteration 1055, loss = 0.24489777\n",
      "Iteration 1634, loss = 0.22329936\n",
      "Iteration 840, loss = 0.32266493\n",
      "Iteration 65, loss = 0.61861376\n",
      "Iteration 542, loss = 0.33680408\n",
      "Iteration 390, loss = 0.35917559\n",
      "Iteration 515, loss = 0.38545754\n",
      "Iteration 543, loss = 0.33662267\n",
      "Iteration 169, loss = 0.43483072\n",
      "Iteration 66, loss = 0.61578940\n",
      "Iteration 841, loss = 0.32261932\n",
      "Iteration 544, loss = 0.33648633\n",
      "Iteration 1635, loss = 0.22317881\n",
      "Iteration 391, loss = 0.35898132\n",
      "Iteration 1056, loss = 0.24483299\n",
      "Iteration 516, loss = 0.38523314\n",
      "Iteration 545, loss = 0.33633245\n",
      "Iteration 170, loss = 0.43430290\n",
      "Iteration 392, loss = 0.35884823\n",
      "Iteration 1636, loss = 0.22300717\n",
      "Iteration 842, loss = 0.32242339\n",
      "Iteration 546, loss = 0.33618497\n",
      "Iteration 67, loss = 0.61299061\n",
      "Iteration 393, loss = 0.35870973\n",
      "Iteration 547, loss = 0.33606643\n",
      "Iteration 517, loss = 0.38500964\n",
      "Iteration 1057, loss = 0.24454143\n",
      "Iteration 1637, loss = 0.22290606\n",
      "Iteration 548, loss = 0.33589808\n",
      "Iteration 394, loss = 0.35861387\n",
      "Iteration 549, loss = 0.33578855\n",
      "Iteration 843, loss = 0.32235420\n",
      "Iteration 171, loss = 0.43381216\n",
      "Iteration 1058, loss = 0.24427022\n",
      "Iteration 518, loss = 0.38485339\n",
      "Iteration 1638, loss = 0.22278666\n",
      "Iteration 68, loss = 0.61035169\n",
      "Iteration 395, loss = 0.35849909\n",
      "Iteration 1639, loss = 0.22267730\n",
      "Iteration 550, loss = 0.33559981\n",
      "Iteration 1640, loss = 0.22253372\n",
      "Iteration 172, loss = 0.43332613\n",
      "Iteration 396, loss = 0.35836192\n",
      "Iteration 551, loss = 0.33544113\n",
      "Iteration 1059, loss = 0.24406543\n",
      "Iteration 69, loss = 0.60758735\n",
      "Iteration 1641, loss = 0.22258235\n",
      "Iteration 552, loss = 0.33529842\n",
      "Iteration 397, loss = 0.35826920\n",
      "Iteration 173, loss = 0.43284058\n",
      "Iteration 553, loss = 0.33515028\n",
      "Iteration 519, loss = 0.38465374\n",
      "Iteration 844, loss = 0.32215547\n",
      "Iteration 554, loss = 0.33498770\n",
      "Iteration 1642, loss = 0.22234589\n",
      "Iteration 70, loss = 0.60494931\n",
      "Iteration 1060, loss = 0.24390738\n",
      "Iteration 1643, loss = 0.22232111\n",
      "Iteration 398, loss = 0.35813516\n",
      "Iteration 555, loss = 0.33490240\n",
      "Iteration 174, loss = 0.43232049\n",
      "Iteration 556, loss = 0.33470495\n",
      "Iteration 520, loss = 0.38436134\n",
      "Iteration 1061, loss = 0.24369002\n",
      "Iteration 557, loss = 0.33454918\n",
      "Iteration 399, loss = 0.35801616\n",
      "Iteration 1644, loss = 0.22206965\n",
      "Iteration 845, loss = 0.32204970\n",
      "Iteration 558, loss = 0.33439330\n",
      "Iteration 559, loss = 0.33425197\n",
      "Iteration 71, loss = 0.60209071\n",
      "Iteration 400, loss = 0.35791789\n",
      "Iteration 1645, loss = 0.22210797\n",
      "Iteration 846, loss = 0.32194897\n",
      "Iteration 401, loss = 0.35782377\n",
      "Iteration 1062, loss = 0.24350370\n",
      "Iteration 560, loss = 0.33408936\n",
      "Iteration 561, loss = 0.33396464\n",
      "Iteration 521, loss = 0.38413310\n",
      "Iteration 562, loss = 0.33382039\n",
      "Iteration 1646, loss = 0.22186258\n",
      "Iteration 72, loss = 0.59953544\n",
      "Iteration 1063, loss = 0.24327813\n",
      "Iteration 847, loss = 0.32177629\n",
      "Iteration 402, loss = 0.35767313\n",
      "Iteration 563, loss = 0.33372282\n",
      "Iteration 175, loss = 0.43178143\n",
      "Iteration 73, loss = 0.59680355\n",
      "Iteration 564, loss = 0.33355018\n",
      "Iteration 1647, loss = 0.22178924\n",
      "Iteration 848, loss = 0.32169438\n",
      "Iteration 565, loss = 0.33336989\n",
      "Iteration 522, loss = 0.38392178\n",
      "Iteration 74, loss = 0.59403488\n",
      "Iteration 566, loss = 0.33323384\n",
      "Iteration 1064, loss = 0.24307935\n",
      "Iteration 176, loss = 0.43130976\n",
      "Iteration 403, loss = 0.35752313\n",
      "Iteration 1648, loss = 0.22165414\n",
      "Iteration 75, loss = 0.59140350\n",
      "Iteration 404, loss = 0.35741787\n",
      "Iteration 849, loss = 0.32155790\n",
      "Iteration 177, loss = 0.43082009\n",
      "Iteration 405, loss = 0.35728483\n",
      "Iteration 567, loss = 0.33307159\n",
      "Iteration 1065, loss = 0.24283536\n",
      "Iteration 1649, loss = 0.22151122\n",
      "Iteration 568, loss = 0.33292776\n",
      "Iteration 406, loss = 0.35720733\n",
      "Iteration 1650, loss = 0.22147677\n",
      "Iteration 850, loss = 0.32142864\n",
      "Iteration 407, loss = 0.35707914\n",
      "Iteration 1651, loss = 0.22125065\n",
      "Iteration 1066, loss = 0.24293217\n",
      "Iteration 569, loss = 0.33279895\n",
      "Iteration 76, loss = 0.58886167\n",
      "Iteration 408, loss = 0.35694934\n",
      "Iteration 570, loss = 0.33263036\n",
      "Iteration 523, loss = 0.38369896\n",
      "Iteration 178, loss = 0.43042569\n",
      "Iteration 571, loss = 0.33248416\n",
      "Iteration 1652, loss = 0.22118030\n",
      "Iteration 409, loss = 0.35682498\n",
      "Iteration 851, loss = 0.32135995\n",
      "Iteration 1653, loss = 0.22103545\n",
      "Iteration 1067, loss = 0.24240665\n",
      "Iteration 77, loss = 0.58608802\n",
      "Iteration 1654, loss = 0.22096315\n",
      "Iteration 572, loss = 0.33240726\n",
      "Iteration 852, loss = 0.32113347\n",
      "Iteration 179, loss = 0.42989863\n",
      "Iteration 410, loss = 0.35670611\n",
      "Iteration 1655, loss = 0.22083735\n",
      "Iteration 1068, loss = 0.24239515\n",
      "Iteration 573, loss = 0.33217935\n",
      "Iteration 574, loss = 0.33208593\n",
      "Iteration 411, loss = 0.35660662\n",
      "Iteration 575, loss = 0.33190585\n",
      "Iteration 853, loss = 0.32102867\n",
      "Iteration 180, loss = 0.42941589\n",
      "Iteration 412, loss = 0.35648935\n",
      "Iteration 576, loss = 0.33178298\n",
      "Iteration 524, loss = 0.38349069\n",
      "Iteration 577, loss = 0.33161939\n",
      "Iteration 1656, loss = 0.22068455\n",
      "Iteration 1069, loss = 0.24207536\n",
      "Iteration 578, loss = 0.33148914\n",
      "Iteration 78, loss = 0.58342410\n",
      "Iteration 413, loss = 0.35635944\n",
      "Iteration 1657, loss = 0.22057490\n",
      "Iteration 854, loss = 0.32094888\n",
      "Iteration 414, loss = 0.35625969\n",
      "Iteration 181, loss = 0.42898827\n",
      "Iteration 79, loss = 0.58075486\n",
      "Iteration 1658, loss = 0.22046898\n",
      "Iteration 579, loss = 0.33131366\n",
      "Iteration 580, loss = 0.33118125\n",
      "Iteration 1659, loss = 0.22039016\n",
      "Iteration 525, loss = 0.38326985\n",
      "Iteration 581, loss = 0.33102815\n",
      "Iteration 415, loss = 0.35611233\n",
      "Iteration 582, loss = 0.33089845\n",
      "Iteration 80, loss = 0.57823210\n",
      "Iteration 182, loss = 0.42849263\n",
      "Iteration 1070, loss = 0.24187811\n",
      "Iteration 855, loss = 0.32084481\n",
      "Iteration 583, loss = 0.33075585\n",
      "Iteration 584, loss = 0.33061345\n",
      "Iteration 1071, loss = 0.24157383\n",
      "Iteration 585, loss = 0.33044678\n",
      "Iteration 586, loss = 0.33033470\n",
      "Iteration 1660, loss = 0.22022794\n",
      "Iteration 1072, loss = 0.24175592\n",
      "Iteration 416, loss = 0.35601051\n",
      "Iteration 587, loss = 0.33016029\n",
      "Iteration 81, loss = 0.57573564\n",
      "Iteration 526, loss = 0.38304372\n",
      "Iteration 1661, loss = 0.22015058\n",
      "Iteration 856, loss = 0.32070689\n",
      "Iteration 183, loss = 0.42804515\n",
      "Iteration 417, loss = 0.35587569\n",
      "Iteration 1662, loss = 0.21996528\n",
      "Iteration 588, loss = 0.33009333\n",
      "Iteration 1073, loss = 0.24124796\n",
      "Iteration 82, loss = 0.57310566\n",
      "Iteration 857, loss = 0.32055217\n",
      "Iteration 418, loss = 0.35579096\n",
      "Iteration 589, loss = 0.32987605\n",
      "Iteration 527, loss = 0.38297389\n",
      "Iteration 1663, loss = 0.21989127\n",
      "Iteration 590, loss = 0.32972562\n",
      "Iteration 419, loss = 0.35566292\n",
      "Iteration 184, loss = 0.42761766\n",
      "Iteration 1074, loss = 0.24102718\n",
      "Iteration 591, loss = 0.32959848\n",
      "Iteration 1664, loss = 0.21993528\n",
      "Iteration 420, loss = 0.35552134\n",
      "Iteration 592, loss = 0.32944175Iteration 83, loss = 0.57061309\n",
      "Iteration 1665, loss = 0.21962305\n",
      "\n",
      "Iteration 421, loss = 0.35544628\n",
      "Iteration 528, loss = 0.38262895\n",
      "Iteration 185, loss = 0.42716243\n",
      "Iteration 858, loss = 0.32046105\n",
      "Iteration 1666, loss = 0.21961353\n",
      "Iteration 593, loss = 0.32929059\n",
      "Iteration 84, loss = 0.56808226\n",
      "Iteration 1075, loss = 0.24083654\n",
      "Iteration 594, loss = 0.32918673\n",
      "Iteration 422, loss = 0.35532066\n",
      "Iteration 1667, loss = 0.21942842\n",
      "Iteration 595, loss = 0.32902278\n",
      "Iteration 596, loss = 0.32886480\n",
      "Iteration 85, loss = 0.56558174\n",
      "Iteration 186, loss = 0.42673498\n",
      "Iteration 859, loss = 0.32024959\n",
      "Iteration 423, loss = 0.35521306\n",
      "Iteration 1668, loss = 0.21929240\n",
      "Iteration 1076, loss = 0.24061936\n",
      "Iteration 597, loss = 0.32875674\n",
      "Iteration 424, loss = 0.35506756\n",
      "Iteration 1669, loss = 0.21915541\n",
      "Iteration 187, loss = 0.42627618\n",
      "Iteration 598, loss = 0.32857558\n",
      "Iteration 425, loss = 0.35495687\n",
      "Iteration 86, loss = 0.56306542\n",
      "Iteration 599, loss = 0.32847871\n",
      "Iteration 529, loss = 0.38271764\n",
      "Iteration 426, loss = 0.35487294\n",
      "Iteration 1077, loss = 0.24034849\n",
      "Iteration 87, loss = 0.56051634\n",
      "Iteration 600, loss = 0.32832815\n",
      "Iteration 188, loss = 0.42584173\n",
      "Iteration 1670, loss = 0.21914735\n",
      "Iteration 88, loss = 0.55805050\n",
      "Iteration 601, loss = 0.32819465\n",
      "Iteration 860, loss = 0.32013312\n",
      "Iteration 189, loss = 0.42543587\n",
      "Iteration 1671, loss = 0.21892794\n",
      "Iteration 602, loss = 0.32801843\n",
      "Iteration 1078, loss = 0.24019353\n",
      "Iteration 530, loss = 0.38227012\n",
      "Iteration 427, loss = 0.35472612\n",
      "Iteration 603, loss = 0.32790269\n",
      "Iteration 89, loss = 0.55571459\n",
      "Iteration 604, loss = 0.32772666\n",
      "Iteration 428, loss = 0.35465856\n",
      "Iteration 1672, loss = 0.21885058\n",
      "Iteration 531, loss = 0.38198226\n",
      "Iteration 190, loss = 0.42501332\n",
      "Iteration 605, loss = 0.32759713\n",
      "Iteration 90, loss = 0.55332786\n",
      "Iteration 606, loss = 0.32744346\n",
      "Iteration 429, loss = 0.35448103\n",
      "Iteration 607, loss = 0.32733027\n",
      "Iteration 861, loss = 0.32005828\n",
      "Iteration 1079, loss = 0.23991521\n",
      "Iteration 1673, loss = 0.21867628\n",
      "Iteration 608, loss = 0.32720628\n",
      "Iteration 430, loss = 0.35437267\n",
      "Iteration 609, loss = 0.32703842\n",
      "Iteration 532, loss = 0.38175921\n",
      "Iteration 431, loss = 0.35425747\n",
      "Iteration 1674, loss = 0.21865194\n",
      "Iteration 91, loss = 0.55099768\n",
      "Iteration 610, loss = 0.32691966\n",
      "Iteration 432, loss = 0.35415438\n",
      "Iteration 611, loss = 0.32672241\n",
      "Iteration 1675, loss = 0.21842580\n",
      "Iteration 1080, loss = 0.23988765\n",
      "Iteration 862, loss = 0.31988805\n",
      "Iteration 433, loss = 0.35411045\n",
      "Iteration 92, loss = 0.54863606\n",
      "Iteration 434, loss = 0.35389638\n",
      "Iteration 612, loss = 0.32665298\n",
      "Iteration 1676, loss = 0.21835495\n",
      "Iteration 191, loss = 0.42463348\n",
      "Iteration 613, loss = 0.32645281\n",
      "Iteration 1081, loss = 0.23975705\n",
      "Iteration 93, loss = 0.54638199\n",
      "Iteration 863, loss = 0.31975706\n",
      "Iteration 533, loss = 0.38156641\n",
      "Iteration 614, loss = 0.32638286\n",
      "Iteration 1677, loss = 0.21821371\n",
      "Iteration 435, loss = 0.35380138\n",
      "Iteration 1082, loss = 0.23959349\n",
      "Iteration 94, loss = 0.54407119\n",
      "Iteration 615, loss = 0.32620977\n",
      "Iteration 616, loss = 0.32603847\n",
      "Iteration 95, loss = 0.54180261\n",
      "Iteration 617, loss = 0.32589219\n",
      "Iteration 534, loss = 0.38132682\n",
      "Iteration 436, loss = 0.35367003\n",
      "Iteration 618, loss = 0.32574568\n",
      "Iteration 864, loss = 0.31965515\n",
      "Iteration 192, loss = 0.42419078\n",
      "Iteration 1678, loss = 0.21814083\n",
      "Iteration 96, loss = 0.53960515\n",
      "Iteration 619, loss = 0.32561216\n",
      "Iteration 1083, loss = 0.23929528\n",
      "Iteration 437, loss = 0.35355803\n",
      "Iteration 193, loss = 0.42377766\n",
      "Iteration 620, loss = 0.32545064\n",
      "Iteration 1084, loss = 0.23891644\n",
      "Iteration 438, loss = 0.35344985\n",
      "Iteration 621, loss = 0.32531558\n",
      "Iteration 1679, loss = 0.21798250\n",
      "Iteration 97, loss = 0.53736653\n",
      "Iteration 439, loss = 0.35335033\n",
      "Iteration 622, loss = 0.32515984\n",
      "Iteration 535, loss = 0.38109732\n",
      "Iteration 623, loss = 0.32506629\n",
      "Iteration 865, loss = 0.31949233\n",
      "Iteration 440, loss = 0.35323393\n",
      "Iteration 194, loss = 0.42342195\n",
      "Iteration 1680, loss = 0.21785220\n",
      "Iteration 624, loss = 0.32487249\n",
      "Iteration 1085, loss = 0.23870193\n",
      "Iteration 441, loss = 0.35311596\n",
      "Iteration 98, loss = 0.53530972\n",
      "Iteration 625, loss = 0.32474292\n",
      "Iteration 1681, loss = 0.21777606\n",
      "Iteration 1682, loss = 0.21765044\n",
      "Iteration 195, loss = 0.42297245\n",
      "Iteration 866, loss = 0.31950050\n",
      "Iteration 626, loss = 0.32458516\n",
      "Iteration 536, loss = 0.38089043\n",
      "Iteration 1086, loss = 0.23866447\n",
      "Iteration 442, loss = 0.35301547\n",
      "Iteration 627, loss = 0.32446122\n",
      "Iteration 1683, loss = 0.21752263\n",
      "Iteration 1087, loss = 0.23835329\n",
      "Iteration 1684, loss = 0.21735953\n",
      "Iteration 443, loss = 0.35294540\n",
      "Iteration 628, loss = 0.32437367\n",
      "Iteration 537, loss = 0.38072039\n",
      "Iteration 1685, loss = 0.21728239\n",
      "Iteration 99, loss = 0.53314206\n",
      "Iteration 867, loss = 0.31924353\n",
      "Iteration 629, loss = 0.32418270\n",
      "Iteration 1686, loss = 0.21714566\n",
      "Iteration 1088, loss = 0.23808388\n",
      "Iteration 196, loss = 0.42259068\n",
      "Iteration 444, loss = 0.35277174\n",
      "Iteration 1687, loss = 0.21702653\n",
      "Iteration 630, loss = 0.32400621\n",
      "Iteration 631, loss = 0.32387830\n",
      "Iteration 538, loss = 0.38053351\n",
      "Iteration 197, loss = 0.42216167\n",
      "Iteration 1688, loss = 0.21692330\n",
      "Iteration 632, loss = 0.32373780\n",
      "Iteration 445, loss = 0.35266658\n",
      "Iteration 100, loss = 0.53106675\n",
      "Iteration 633, loss = 0.32360805\n",
      "Iteration 1089, loss = 0.23790352\n",
      "Iteration 634, loss = 0.32346013\n",
      "Iteration 1689, loss = 0.21677891\n",
      "Iteration 868, loss = 0.31912228\n",
      "Iteration 101, loss = 0.52902296\n",
      "Iteration 446, loss = 0.35257796\n",
      "Iteration 539, loss = 0.38025848\n",
      "Iteration 635, loss = 0.32333969\n",
      "Iteration 1090, loss = 0.23775120\n",
      "Iteration 1690, loss = 0.21667510\n",
      "Iteration 102, loss = 0.52696509\n",
      "Iteration 198, loss = 0.42180554\n",
      "Iteration 447, loss = 0.35242661\n",
      "Iteration 636, loss = 0.32319137\n",
      "Iteration 869, loss = 0.31908069\n",
      "Iteration 637, loss = 0.32303561\n",
      "Iteration 448, loss = 0.35231023\n",
      "Iteration 638, loss = 0.32293920\n",
      "Iteration 540, loss = 0.37999421\n",
      "Iteration 639, loss = 0.32276734\n",
      "Iteration 1091, loss = 0.23760093\n",
      "Iteration 449, loss = 0.35220011\n",
      "Iteration 1092, loss = 0.23748759Iteration 199, loss = 0.42141300\n",
      "Iteration 103, loss = 0.52499283\n",
      "Iteration 450, loss = 0.35206819\n",
      "Iteration 870, loss = 0.31891167\n",
      "\n",
      "Iteration 541, loss = 0.37980345\n",
      "Iteration 640, loss = 0.32262701\n",
      "Iteration 200, loss = 0.42104062\n",
      "Iteration 451, loss = 0.35196920\n",
      "Iteration 871, loss = 0.31875398\n",
      "Iteration 1093, loss = 0.23733533\n",
      "Iteration 542, loss = 0.37960584\n",
      "Iteration 1691, loss = 0.21663915\n",
      "Iteration 104, loss = 0.52305127\n",
      "Iteration 641, loss = 0.32250248\n",
      "Iteration 201, loss = 0.42063613\n",
      "Iteration 642, loss = 0.32237289\n",
      "Iteration 643, loss = 0.32221202\n",
      "Iteration 1692, loss = 0.21644907\n",
      "Iteration 452, loss = 0.35187915\n",
      "Iteration 105, loss = 0.52111977\n",
      "Iteration 1693, loss = 0.21636539\n",
      "Iteration 644, loss = 0.32207630\n",
      "Iteration 872, loss = 0.31871521\n",
      "Iteration 453, loss = 0.35172632\n",
      "Iteration 106, loss = 0.51922430\n",
      "Iteration 202, loss = 0.42027123\n",
      "Iteration 645, loss = 0.32197745\n",
      "Iteration 1094, loss = 0.23687955\n",
      "Iteration 543, loss = 0.37936726\n",
      "Iteration 873, loss = 0.31851413\n",
      "Iteration 646, loss = 0.32186688\n",
      "Iteration 1694, loss = 0.21628734\n",
      "Iteration 647, loss = 0.32165823\n",
      "Iteration 1095, loss = 0.23672077\n",
      "Iteration 874, loss = 0.31838155\n",
      "Iteration 454, loss = 0.35164980\n",
      "Iteration 203, loss = 0.41989779\n",
      "Iteration 648, loss = 0.32157667\n",
      "Iteration 1695, loss = 0.21616868\n",
      "Iteration 455, loss = 0.35151495\n",
      "Iteration 1096, loss = 0.23654041\n",
      "Iteration 649, loss = 0.32139600\n",
      "Iteration 107, loss = 0.51734056\n",
      "Iteration 204, loss = 0.41951894\n",
      "Iteration 1696, loss = 0.21594679\n",
      "Iteration 544, loss = 0.37926955\n",
      "Iteration 650, loss = 0.32126094\n",
      "Iteration 456, loss = 0.35141433\n",
      "Iteration 1697, loss = 0.21587714\n",
      "Iteration 1097, loss = 0.23630733\n",
      "Iteration 651, loss = 0.32112250\n",
      "Iteration 457, loss = 0.35130502\n",
      "Iteration 1698, loss = 0.21572383\n",
      "Iteration 108, loss = 0.51541321\n",
      "Iteration 458, loss = 0.35116008\n",
      "Iteration 652, loss = 0.32096769\n",
      "Iteration 1699, loss = 0.21562876\n",
      "Iteration 1098, loss = 0.23622909\n",
      "Iteration 459, loss = 0.35107770\n",
      "Iteration 205, loss = 0.41915554\n",
      "Iteration 653, loss = 0.32084183\n",
      "Iteration 1700, loss = 0.21552515\n",
      "Iteration 654, loss = 0.32069332\n",
      "Iteration 1099, loss = 0.23583001\n",
      "Iteration 545, loss = 0.37893476\n",
      "Iteration 655, loss = 0.32055603\n",
      "Iteration 460, loss = 0.35096344\n",
      "Iteration 206, loss = 0.41879185\n",
      "Iteration 109, loss = 0.51371495\n",
      "Iteration 656, loss = 0.32043628\n",
      "Iteration 875, loss = 0.31824292\n",
      "Iteration 1701, loss = 0.21545283\n",
      "Iteration 1100, loss = 0.23561148\n",
      "Iteration 657, loss = 0.32035476\n",
      "Iteration 461, loss = 0.35085973\n",
      "Iteration 658, loss = 0.32016259\n",
      "Iteration 207, loss = 0.41844850\n",
      "Iteration 1702, loss = 0.21534199\n",
      "Iteration 659, loss = 0.32001541\n",
      "Iteration 110, loss = 0.51185884\n",
      "Iteration 660, loss = 0.31987224\n",
      "Iteration 546, loss = 0.37884081\n",
      "Iteration 462, loss = 0.35075986\n",
      "Iteration 661, loss = 0.31978629\n",
      "Iteration 111, loss = 0.51010402\n",
      "Iteration 1703, loss = 0.21525381\n",
      "Iteration 1101, loss = 0.23543192\n",
      "Iteration 876, loss = 0.31810019\n",
      "Iteration 662, loss = 0.31966868\n",
      "Iteration 463, loss = 0.35060752\n",
      "Iteration 663, loss = 0.31947165\n",
      "Iteration 664, loss = 0.31935936\n",
      "Iteration 1704, loss = 0.21514232\n",
      "Iteration 208, loss = 0.41808059\n",
      "Iteration 1102, loss = 0.23528651\n",
      "Iteration 464, loss = 0.35048054\n",
      "Iteration 665, loss = 0.31921877\n",
      "Iteration 465, loss = 0.35038072\n",
      "Iteration 1103, loss = 0.23511766\n",
      "Iteration 209, loss = 0.41771638\n",
      "Iteration 547, loss = 0.37846823\n",
      "Iteration 466, loss = 0.35032423\n",
      "Iteration 1705, loss = 0.21508188\n",
      "Iteration 666, loss = 0.31907521\n",
      "Iteration 112, loss = 0.50833079\n",
      "Iteration 667, loss = 0.31894635\n",
      "Iteration 877, loss = 0.31801449\n",
      "Iteration 668, loss = 0.31880177\n",
      "Iteration 467, loss = 0.35016502\n",
      "Iteration 1104, loss = 0.23495349\n",
      "Iteration 1706, loss = 0.21490261\n",
      "Iteration 113, loss = 0.50665779\n",
      "Iteration 669, loss = 0.31872459\n",
      "Iteration 210, loss = 0.41735957\n",
      "Iteration 670, loss = 0.31851255\n",
      "Iteration 878, loss = 0.31785918\n",
      "Iteration 1707, loss = 0.21478435\n",
      "Iteration 671, loss = 0.31841224\n",
      "Iteration 672, loss = 0.31842805\n",
      "Iteration 548, loss = 0.37834406\n",
      "Iteration 468, loss = 0.35007242\n",
      "Iteration 673, loss = 0.31812675\n",
      "Iteration 1708, loss = 0.21459517\n",
      "Iteration 1105, loss = 0.23485759\n",
      "Iteration 114, loss = 0.50498423\n",
      "Iteration 1709, loss = 0.21458727\n",
      "Iteration 469, loss = 0.34992879\n",
      "Iteration 674, loss = 0.31809792\n",
      "Iteration 879, loss = 0.31773557\n",
      "Iteration 1710, loss = 0.21431577\n",
      "Iteration 470, loss = 0.34982210\n",
      "Iteration 211, loss = 0.41701322\n",
      "Iteration 471, loss = 0.34969774\n",
      "Iteration 1106, loss = 0.23442009\n",
      "Iteration 675, loss = 0.31786620\n",
      "Iteration 549, loss = 0.37806686\n",
      "Iteration 676, loss = 0.31774637\n",
      "Iteration 472, loss = 0.34959550\n",
      "Iteration 1711, loss = 0.21432338\n",
      "Iteration 880, loss = 0.31762147\n",
      "Iteration 677, loss = 0.31760664\n",
      "Iteration 1107, loss = 0.23421734\n",
      "Iteration 678, loss = 0.31751246\n",
      "Iteration 115, loss = 0.50340703\n",
      "Iteration 212, loss = 0.41669675\n",
      "Iteration 679, loss = 0.31735681\n",
      "Iteration 680, loss = 0.31721947\n",
      "Iteration 681, loss = 0.31723481\n",
      "Iteration 881, loss = 0.31747235\n",
      "Iteration 1108, loss = 0.23401739\n",
      "Iteration 550, loss = 0.37782523\n",
      "Iteration 473, loss = 0.34947115\n",
      "Iteration 1712, loss = 0.21414359\n",
      "Iteration 116, loss = 0.50173855\n",
      "Iteration 682, loss = 0.31694808\n",
      "Iteration 1713, loss = 0.21403016\n",
      "Iteration 474, loss = 0.34936902\n",
      "Iteration 683, loss = 0.31680345\n",
      "Iteration 1714, loss = 0.21395086\n",
      "Iteration 475, loss = 0.34928807\n",
      "Iteration 882, loss = 0.31741440\n",
      "Iteration 117, loss = 0.50011768\n",
      "Iteration 213, loss = 0.41630638\n",
      "Iteration 684, loss = 0.31670064\n",
      "Iteration 1109, loss = 0.23385599\n",
      "Iteration 476, loss = 0.34915123\n",
      "Iteration 118, loss = 0.49857359\n",
      "Iteration 685, loss = 0.31654512\n",
      "Iteration 1715, loss = 0.21385421\n",
      "Iteration 551, loss = 0.37769701\n",
      "Iteration 883, loss = 0.31721031\n",
      "Iteration 686, loss = 0.31642059\n",
      "Iteration 477, loss = 0.34902955\n",
      "Iteration 687, loss = 0.31631657\n",
      "Iteration 478, loss = 0.34893356\n",
      "Iteration 688, loss = 0.31615323\n",
      "Iteration 119, loss = 0.49697318\n",
      "Iteration 1110, loss = 0.23367634\n",
      "Iteration 689, loss = 0.31611224\n",
      "Iteration 214, loss = 0.41597898\n",
      "Iteration 1716, loss = 0.21365243\n",
      "Iteration 690, loss = 0.31588349\n",
      "Iteration 1111, loss = 0.23342819\n",
      "Iteration 479, loss = 0.34884574\n",
      "Iteration 691, loss = 0.31580630\n",
      "Iteration 692, loss = 0.31562998\n",
      "Iteration 120, loss = 0.49549383\n",
      "Iteration 480, loss = 0.34875910\n",
      "Iteration 1717, loss = 0.21358079\n",
      "Iteration 693, loss = 0.31555684\n",
      "Iteration 694, loss = 0.31535738\n",
      "Iteration 884, loss = 0.31713607\n",
      "Iteration 1112, loss = 0.23321006\n",
      "Iteration 552, loss = 0.37743352\n",
      "Iteration 1718, loss = 0.21349978\n",
      "Iteration 695, loss = 0.31524963\n",
      "Iteration 121, loss = 0.49400213\n",
      "Iteration 696, loss = 0.31515906\n",
      "Iteration 215, loss = 0.41568047\n",
      "Iteration 481, loss = 0.34859817\n",
      "Iteration 697, loss = 0.31502808\n",
      "Iteration 122, loss = 0.49252141\n",
      "Iteration 698, loss = 0.31484183\n",
      "Iteration 482, loss = 0.34852427\n",
      "Iteration 699, loss = 0.31472360\n",
      "Iteration 1113, loss = 0.23298445\n",
      "Iteration 700, loss = 0.31461864\n",
      "Iteration 885, loss = 0.31695395\n",
      "Iteration 483, loss = 0.34839449\n",
      "Iteration 701, loss = 0.31449789\n",
      "Iteration 702, loss = 0.31433285\n",
      "Iteration 216, loss = 0.41532257\n",
      "Iteration 1114, loss = 0.23274184\n",
      "Iteration 1719, loss = 0.21331305\n",
      "Iteration 553, loss = 0.37718710\n",
      "Iteration 123, loss = 0.49101815\n",
      "Iteration 484, loss = 0.34836621\n",
      "Iteration 886, loss = 0.31685966\n",
      "Iteration 703, loss = 0.31420656\n",
      "Iteration 1115, loss = 0.23262698\n",
      "Iteration 485, loss = 0.34815884\n",
      "Iteration 1720, loss = 0.21319100\n",
      "Iteration 704, loss = 0.31411271\n",
      "Iteration 124, loss = 0.48968968\n",
      "Iteration 217, loss = 0.41499066\n",
      "Iteration 705, loss = 0.31395354\n",
      "Iteration 1721, loss = 0.21310633\n",
      "Iteration 1116, loss = 0.23233906\n",
      "Iteration 887, loss = 0.31676957\n",
      "Iteration 486, loss = 0.34804768\n",
      "Iteration 1722, loss = 0.21293961\n",
      "Iteration 706, loss = 0.31382001\n",
      "Iteration 554, loss = 0.37702805\n",
      "Iteration 125, loss = 0.48818772\n",
      "Iteration 487, loss = 0.34794867\n",
      "Iteration 1117, loss = 0.23210821\n",
      "Iteration 707, loss = 0.31367948\n",
      "Iteration 488, loss = 0.34782730\n",
      "Iteration 218, loss = 0.41465577\n",
      "Iteration 126, loss = 0.48682182\n",
      "Iteration 1723, loss = 0.21285525\n",
      "Iteration 1118, loss = 0.23193060\n",
      "Iteration 708, loss = 0.31354834\n",
      "Iteration 489, loss = 0.34773313\n",
      "Iteration 709, loss = 0.31347172\n",
      "Iteration 888, loss = 0.31656980\n",
      "Iteration 219, loss = 0.41431581\n",
      "Iteration 710, loss = 0.31327951\n",
      "Iteration 1119, loss = 0.23187527\n",
      "Iteration 1724, loss = 0.21271020Iteration 490, loss = 0.34764393\n",
      "Iteration 889, loss = 0.31645690\n",
      "Iteration 555, loss = 0.37702826\n",
      "Iteration 1120, loss = 0.23152205\n",
      "Iteration 127, loss = 0.48567260\n",
      "\n",
      "Iteration 711, loss = 0.31316844\n",
      "Iteration 491, loss = 0.34752199\n",
      "Iteration 1725, loss = 0.21272317\n",
      "Iteration 712, loss = 0.31301804\n",
      "Iteration 220, loss = 0.41400849\n",
      "Iteration 890, loss = 0.31633203\n",
      "Iteration 492, loss = 0.34741927\n",
      "Iteration 713, loss = 0.31292396\n",
      "Iteration 1121, loss = 0.23147101\n",
      "Iteration 714, loss = 0.31276874\n",
      "Iteration 493, loss = 0.34727350\n",
      "Iteration 715, loss = 0.31263143\n",
      "Iteration 1726, loss = 0.21261710\n",
      "Iteration 891, loss = 0.31622193\n",
      "Iteration 716, loss = 0.31256518\n",
      "Iteration 494, loss = 0.34717318\n",
      "Iteration 221, loss = 0.41367538\n",
      "Iteration 556, loss = 0.37651748\n",
      "Iteration 717, loss = 0.31238571\n",
      "Iteration 128, loss = 0.48413716\n",
      "Iteration 1122, loss = 0.23156408\n",
      "Iteration 718, loss = 0.31223412\n",
      "Iteration 892, loss = 0.31607913\n",
      "Iteration 719, loss = 0.31212017\n",
      "Iteration 720, loss = 0.31197829\n",
      "Iteration 1727, loss = 0.21259139\n",
      "Iteration 495, loss = 0.34706299\n",
      "Iteration 721, loss = 0.31187979\n",
      "Iteration 222, loss = 0.41338904\n",
      "Iteration 1728, loss = 0.21226487\n",
      "Iteration 129, loss = 0.48289659\n",
      "Iteration 1123, loss = 0.23087605\n",
      "Iteration 557, loss = 0.37630973\n",
      "Iteration 496, loss = 0.34695594\n",
      "Iteration 893, loss = 0.31602191\n",
      "Iteration 722, loss = 0.31172448\n",
      "Iteration 223, loss = 0.41307313\n",
      "Iteration 1124, loss = 0.23076318\n",
      "Iteration 1729, loss = 0.21217189\n",
      "Iteration 130, loss = 0.48152608\n",
      "Iteration 723, loss = 0.31161985\n",
      "Iteration 497, loss = 0.34684034\n",
      "Iteration 1730, loss = 0.21200706\n",
      "Iteration 724, loss = 0.31152202\n",
      "Iteration 894, loss = 0.31584460\n",
      "Iteration 224, loss = 0.41274972\n",
      "Iteration 131, loss = 0.48032738\n",
      "Iteration 1125, loss = 0.23055307\n",
      "Iteration 498, loss = 0.34680206\n",
      "Iteration 725, loss = 0.31142045\n",
      "Iteration 1731, loss = 0.21194187\n",
      "Iteration 558, loss = 0.37607284\n",
      "Iteration 726, loss = 0.31128807\n",
      "Iteration 225, loss = 0.41243080\n",
      "Iteration 727, loss = 0.31111355\n",
      "Iteration 1732, loss = 0.21177871\n",
      "Iteration 728, loss = 0.31102467\n",
      "Iteration 499, loss = 0.34665207\n",
      "Iteration 132, loss = 0.47903246\n",
      "Iteration 1126, loss = 0.23025073\n",
      "Iteration 895, loss = 0.31571556\n",
      "Iteration 729, loss = 0.31081978\n",
      "Iteration 559, loss = 0.37599531\n",
      "Iteration 730, loss = 0.31068960\n",
      "Iteration 500, loss = 0.34651984\n",
      "Iteration 731, loss = 0.31059938\n",
      "Iteration 226, loss = 0.41216076\n",
      "Iteration 1733, loss = 0.21169904\n",
      "Iteration 732, loss = 0.31049048\n",
      "Iteration 733, loss = 0.31032191\n",
      "Iteration 896, loss = 0.31565232\n",
      "Iteration 501, loss = 0.34640272\n",
      "Iteration 1734, loss = 0.21157157\n",
      "Iteration 133, loss = 0.47775358\n",
      "Iteration 734, loss = 0.31019411\n",
      "Iteration 1127, loss = 0.23008970\n",
      "Iteration 735, loss = 0.31016812\n",
      "Iteration 502, loss = 0.34633133\n",
      "Iteration 560, loss = 0.37573002\n",
      "Iteration 227, loss = 0.41184178\n",
      "Iteration 736, loss = 0.30994387\n",
      "Iteration 1735, loss = 0.21145782\n",
      "Iteration 897, loss = 0.31547730\n",
      "Iteration 737, loss = 0.30986719\n",
      "Iteration 1736, loss = 0.21131196\n",
      "Iteration 134, loss = 0.47658389\n",
      "Iteration 1737, loss = 0.21119829\n",
      "Iteration 503, loss = 0.34621515\n",
      "Iteration 1128, loss = 0.22985618\n",
      "Iteration 738, loss = 0.30968592\n",
      "Iteration 561, loss = 0.37534055\n",
      "Iteration 228, loss = 0.41152455\n",
      "Iteration 504, loss = 0.34609509\n",
      "Iteration 135, loss = 0.47539986\n",
      "Iteration 1738, loss = 0.21109778\n",
      "Iteration 739, loss = 0.30955153\n",
      "Iteration 898, loss = 0.31532138\n",
      "Iteration 740, loss = 0.30948532\n",
      "Iteration 1129, loss = 0.22980978\n",
      "Iteration 505, loss = 0.34599251\n",
      "Iteration 741, loss = 0.30933975\n",
      "Iteration 1739, loss = 0.21098997\n",
      "Iteration 1130, loss = 0.22939515\n",
      "Iteration 899, loss = 0.31526074\n",
      "Iteration 136, loss = 0.47418164\n",
      "Iteration 229, loss = 0.41122029\n",
      "Iteration 742, loss = 0.30915740\n",
      "Iteration 506, loss = 0.34587455\n",
      "Iteration 1740, loss = 0.21099972\n",
      "Iteration 900, loss = 0.31506559\n",
      "Iteration 1131, loss = 0.22921473\n",
      "Iteration 743, loss = 0.30903503\n",
      "Iteration 562, loss = 0.37519100\n",
      "Iteration 507, loss = 0.34576965\n",
      "Iteration 137, loss = 0.47306696\n",
      "Iteration 744, loss = 0.30893657\n",
      "Iteration 901, loss = 0.31494566\n",
      "Iteration 230, loss = 0.41096529\n",
      "Iteration 1741, loss = 0.21070920\n",
      "Iteration 138, loss = 0.47188005\n",
      "Iteration 508, loss = 0.34566970\n",
      "Iteration 1742, loss = 0.21062489\n",
      "Iteration 745, loss = 0.30878731\n",
      "Iteration 231, loss = 0.41063376\n",
      "Iteration 902, loss = 0.31484227\n",
      "Iteration 563, loss = 0.37493624\n",
      "Iteration 1743, loss = 0.21064076\n",
      "Iteration 1132, loss = 0.22912697\n",
      "Iteration 746, loss = 0.30864473\n",
      "Iteration 139, loss = 0.47080405\n",
      "Iteration 509, loss = 0.34554337\n",
      "Iteration 747, loss = 0.30852411\n",
      "Iteration 1133, loss = 0.22890563\n",
      "Iteration 1744, loss = 0.21035436\n",
      "Iteration 903, loss = 0.31476539\n",
      "Iteration 748, loss = 0.30852966\n",
      "Iteration 749, loss = 0.30828635\n",
      "Iteration 232, loss = 0.41035166\n",
      "Iteration 1745, loss = 0.21034456\n",
      "Iteration 1134, loss = 0.22875136\n",
      "Iteration 510, loss = 0.34544403\n",
      "Iteration 750, loss = 0.30814862\n",
      "Iteration 140, loss = 0.46963616\n",
      "Iteration 564, loss = 0.37469258\n",
      "Iteration 751, loss = 0.30799002\n",
      "Iteration 233, loss = 0.41005069\n",
      "Iteration 1746, loss = 0.21017937\n",
      "Iteration 752, loss = 0.30789668\n",
      "Iteration 904, loss = 0.31460178\n",
      "Iteration 511, loss = 0.34533699\n",
      "Iteration 753, loss = 0.30778487\n",
      "Iteration 1135, loss = 0.22852813\n",
      "Iteration 1747, loss = 0.21004591\n",
      "Iteration 512, loss = 0.34521501\n",
      "Iteration 754, loss = 0.30761327\n",
      "Iteration 1748, loss = 0.20989518\n",
      "Iteration 141, loss = 0.46863472\n",
      "Iteration 1749, loss = 0.20988598\n",
      "Iteration 905, loss = 0.31454695\n",
      "Iteration 234, loss = 0.40978463\n",
      "Iteration 142, loss = 0.46751035\n",
      "Iteration 513, loss = 0.34511379\n",
      "Iteration 1136, loss = 0.22832567\n",
      "Iteration 1750, loss = 0.20966016\n",
      "Iteration 755, loss = 0.30749317\n",
      "Iteration 906, loss = 0.31431612\n",
      "Iteration 565, loss = 0.37452093\n",
      "Iteration 143, loss = 0.46648054\n",
      "Iteration 1137, loss = 0.22805633\n",
      "Iteration 756, loss = 0.30737887\n",
      "Iteration 514, loss = 0.34503973\n",
      "Iteration 757, loss = 0.30723163\n",
      "Iteration 515, loss = 0.34490058\n",
      "Iteration 758, loss = 0.30709531\n",
      "Iteration 1138, loss = 0.22781661\n",
      "Iteration 1751, loss = 0.20954153\n",
      "Iteration 907, loss = 0.31426153\n",
      "Iteration 516, loss = 0.34496198\n",
      "Iteration 235, loss = 0.40949859\n",
      "Iteration 759, loss = 0.30700008\n",
      "Iteration 144, loss = 0.46539410\n",
      "Iteration 517, loss = 0.34470152\n",
      "Iteration 1752, loss = 0.20944954\n",
      "Iteration 566, loss = 0.37424946\n",
      "Iteration 145, loss = 0.46446382\n",
      "Iteration 760, loss = 0.30685676\n",
      "Iteration 518, loss = 0.34457837\n",
      "Iteration 761, loss = 0.30678163\n",
      "Iteration 908, loss = 0.31414098\n",
      "Iteration 1139, loss = 0.22759244\n",
      "Iteration 762, loss = 0.30660199\n",
      "Iteration 1753, loss = 0.20937868\n",
      "Iteration 236, loss = 0.40918404\n",
      "Iteration 146, loss = 0.46333998\n",
      "Iteration 763, loss = 0.30657916\n",
      "Iteration 1754, loss = 0.20925174\n",
      "Iteration 519, loss = 0.34451511\n",
      "Iteration 1755, loss = 0.20911395\n",
      "Iteration 147, loss = 0.46238468\n",
      "Iteration 567, loss = 0.37404586\n",
      "Iteration 764, loss = 0.30634993\n",
      "Iteration 1140, loss = 0.22742371\n",
      "Iteration 237, loss = 0.40893000\n",
      "Iteration 765, loss = 0.30623332\n",
      "Iteration 148, loss = 0.46139228\n",
      "Iteration 520, loss = 0.34438043\n",
      "Iteration 766, loss = 0.30612622\n",
      "Iteration 909, loss = 0.31396242\n",
      "Iteration 1141, loss = 0.22718946\n",
      "Iteration 1756, loss = 0.20900401\n",
      "Iteration 521, loss = 0.34427536\n",
      "Iteration 149, loss = 0.46040305\n",
      "Iteration 767, loss = 0.30599674\n",
      "Iteration 238, loss = 0.40863706\n",
      "Iteration 1757, loss = 0.20885400\n",
      "Iteration 522, loss = 0.34415637\n",
      "Iteration 568, loss = 0.37383425\n",
      "Iteration 1142, loss = 0.22703260\n",
      "Iteration 150, loss = 0.45955490\n",
      "Iteration 768, loss = 0.30584494\n",
      "Iteration 910, loss = 0.31381714\n",
      "Iteration 523, loss = 0.34404013\n",
      "Iteration 239, loss = 0.40835018\n",
      "Iteration 769, loss = 0.30572149\n",
      "Iteration 524, loss = 0.34396012\n",
      "Iteration 770, loss = 0.30561160\n",
      "Iteration 151, loss = 0.45851369\n",
      "Iteration 1758, loss = 0.20878082\n",
      "Iteration 771, loss = 0.30550564\n",
      "Iteration 1143, loss = 0.22671928\n",
      "Iteration 525, loss = 0.34389519\n",
      "Iteration 911, loss = 0.31369863\n",
      "Iteration 772, loss = 0.30537326\n",
      "Iteration 1759, loss = 0.20876226\n",
      "Iteration 773, loss = 0.30522462\n",
      "Iteration 774, loss = 0.30509419\n",
      "Iteration 1760, loss = 0.20851407\n",
      "Iteration 526, loss = 0.34377629\n",
      "Iteration 775, loss = 0.30495167\n",
      "Iteration 152, loss = 0.45758786\n",
      "Iteration 776, loss = 0.30482750\n",
      "Iteration 1761, loss = 0.20841351\n",
      "Iteration 527, loss = 0.34365505\n",
      "Iteration 1144, loss = 0.22671938\n",
      "Iteration 912, loss = 0.31359490\n",
      "Iteration 569, loss = 0.37367135\n",
      "Iteration 153, loss = 0.45666954\n",
      "Iteration 240, loss = 0.40806068\n",
      "Iteration 528, loss = 0.34353675\n",
      "Iteration 777, loss = 0.30472796\n",
      "Iteration 1762, loss = 0.20828917\n",
      "Iteration 529, loss = 0.34340785\n",
      "Iteration 154, loss = 0.45578730\n",
      "Iteration 1145, loss = 0.22634741\n",
      "Iteration 241, loss = 0.40781025\n",
      "Iteration 1763, loss = 0.20821749\n",
      "Iteration 570, loss = 0.37336674\n",
      "Iteration 778, loss = 0.30461178\n",
      "Iteration 1764, loss = 0.20806181\n",
      "Iteration 779, loss = 0.30445092\n",
      "Iteration 530, loss = 0.34329765\n",
      "Iteration 780, loss = 0.30436878\n",
      "Iteration 242, loss = 0.40752399\n",
      "Iteration 1146, loss = 0.22629046\n",
      "Iteration 913, loss = 0.31343358\n",
      "Iteration 781, loss = 0.30427880\n",
      "Iteration 155, loss = 0.45486517\n",
      "Iteration 1765, loss = 0.20790936\n",
      "Iteration 531, loss = 0.34318616Iteration 782, loss = 0.30407255\n",
      "\n",
      "Iteration 1766, loss = 0.20782731\n",
      "Iteration 571, loss = 0.37319731\n",
      "Iteration 1147, loss = 0.22583358\n",
      "Iteration 783, loss = 0.30396713\n",
      "Iteration 243, loss = 0.40724487\n",
      "Iteration 156, loss = 0.45403582\n",
      "Iteration 914, loss = 0.31333425\n",
      "Iteration 784, loss = 0.30381072\n",
      "Iteration 1767, loss = 0.20769182\n",
      "Iteration 1148, loss = 0.22575723\n",
      "Iteration 532, loss = 0.34307069\n",
      "Iteration 785, loss = 0.30370944\n",
      "Iteration 786, loss = 0.30361906\n",
      "Iteration 533, loss = 0.34299522\n",
      "Iteration 915, loss = 0.31322370\n",
      "Iteration 1149, loss = 0.22541293\n",
      "Iteration 534, loss = 0.34287546\n",
      "Iteration 1150, loss = 0.22528058\n",
      "Iteration 244, loss = 0.40700521\n",
      "Iteration 157, loss = 0.45311697\n",
      "Iteration 1768, loss = 0.20754739\n",
      "Iteration 787, loss = 0.30344368\n",
      "Iteration 535, loss = 0.34276915\n",
      "Iteration 1151, loss = 0.22517363\n",
      "Iteration 788, loss = 0.30335607\n",
      "Iteration 1769, loss = 0.20755483\n",
      "Iteration 158, loss = 0.45228096\n",
      "Iteration 916, loss = 0.31307117\n",
      "Iteration 572, loss = 0.37291959\n",
      "Iteration 1770, loss = 0.20730775\n",
      "Iteration 245, loss = 0.40671129\n",
      "Iteration 1152, loss = 0.22479786\n",
      "Iteration 159, loss = 0.45147661\n",
      "Iteration 1771, loss = 0.20719419\n",
      "Iteration 536, loss = 0.34270541\n",
      "Iteration 573, loss = 0.37270556\n",
      "Iteration 246, loss = 0.40645702\n",
      "Iteration 1772, loss = 0.20715932\n",
      "Iteration 160, loss = 0.45066296\n",
      "Iteration 789, loss = 0.30321034\n",
      "Iteration 917, loss = 0.31305332\n",
      "Iteration 537, loss = 0.34259028\n",
      "Iteration 790, loss = 0.30325733\n",
      "Iteration 1153, loss = 0.22465024\n",
      "Iteration 247, loss = 0.40619334\n",
      "Iteration 1773, loss = 0.20696198\n",
      "Iteration 791, loss = 0.30299060\n",
      "Iteration 538, loss = 0.34244668\n",
      "Iteration 574, loss = 0.37242857\n",
      "Iteration 792, loss = 0.30281275\n",
      "Iteration 248, loss = 0.40594271\n",
      "Iteration 1154, loss = 0.22459186\n",
      "Iteration 793, loss = 0.30269609\n",
      "Iteration 539, loss = 0.34234121\n",
      "Iteration 1774, loss = 0.20686120\n",
      "Iteration 161, loss = 0.44974486\n",
      "Iteration 918, loss = 0.31283244\n",
      "Iteration 249, loss = 0.40565493\n",
      "Iteration 1775, loss = 0.20674218\n",
      "Iteration 794, loss = 0.30257140\n",
      "Iteration 1155, loss = 0.22420099\n",
      "Iteration 795, loss = 0.30245994\n",
      "Iteration 540, loss = 0.34226421\n",
      "Iteration 250, loss = 0.40538756\n",
      "Iteration 796, loss = 0.30230805\n",
      "Iteration 797, loss = 0.30217512\n",
      "Iteration 1156, loss = 0.22392717\n",
      "Iteration 1776, loss = 0.20688143\n",
      "Iteration 798, loss = 0.30210848\n",
      "Iteration 162, loss = 0.44900656\n",
      "Iteration 251, loss = 0.40517574\n",
      "Iteration 799, loss = 0.30197885\n",
      "Iteration 919, loss = 0.31266830\n",
      "Iteration 1777, loss = 0.20655538\n",
      "Iteration 163, loss = 0.44822174\n",
      "Iteration 541, loss = 0.34213234\n",
      "Iteration 252, loss = 0.40491079\n",
      "Iteration 1157, loss = 0.22374285\n",
      "Iteration 1158, loss = 0.22359185\n",
      "Iteration 542, loss = 0.34201147\n",
      "Iteration 1778, loss = 0.20639033\n",
      "Iteration 543, loss = 0.34192473\n",
      "Iteration 1779, loss = 0.20653106\n",
      "Iteration 253, loss = 0.40462432\n",
      "Iteration 800, loss = 0.30187872\n",
      "Iteration 575, loss = 0.37233322\n",
      "Iteration 801, loss = 0.30167132\n",
      "Iteration 1780, loss = 0.20622879\n",
      "Iteration 164, loss = 0.44743921\n",
      "Iteration 802, loss = 0.30154487\n",
      "Iteration 803, loss = 0.30148943\n",
      "Iteration 165, loss = 0.44661181\n",
      "Iteration 804, loss = 0.30137393\n",
      "Iteration 805, loss = 0.30115759\n",
      "Iteration 544, loss = 0.34184496\n",
      "Iteration 1159, loss = 0.22341450\n",
      "Iteration 254, loss = 0.40437901\n",
      "Iteration 576, loss = 0.37218926\n",
      "Iteration 806, loss = 0.30102751\n",
      "Iteration 1160, loss = 0.22312996\n",
      "Iteration 166, loss = 0.44588030\n",
      "Iteration 255, loss = 0.40418279\n",
      "Iteration 807, loss = 0.30091230\n",
      "Iteration 808, loss = 0.30076457\n",
      "Iteration 167, loss = 0.44515030\n",
      "Iteration 920, loss = 0.31253474\n",
      "Iteration 545, loss = 0.34170562\n",
      "Iteration 1161, loss = 0.22301557\n",
      "Iteration 809, loss = 0.30065151\n",
      "Iteration 1781, loss = 0.20612331\n",
      "Iteration 810, loss = 0.30050972\n",
      "Iteration 256, loss = 0.40390027\n",
      "Iteration 546, loss = 0.34163133\n",
      "Iteration 1782, loss = 0.20600499\n",
      "Iteration 168, loss = 0.44445573\n",
      "Iteration 811, loss = 0.30036137\n",
      "Iteration 257, loss = 0.40365194\n",
      "Iteration 577, loss = 0.37177437\n",
      "Iteration 547, loss = 0.34152575\n",
      "Iteration 812, loss = 0.30027526\n",
      "Iteration 813, loss = 0.30013125\n",
      "Iteration 258, loss = 0.40338720\n",
      "Iteration 1783, loss = 0.20584211\n",
      "Iteration 169, loss = 0.44370027\n",
      "Iteration 921, loss = 0.31243430\n",
      "Iteration 814, loss = 0.29998548\n",
      "Iteration 548, loss = 0.34144122\n",
      "Iteration 1162, loss = 0.22269326\n",
      "Iteration 1784, loss = 0.20569797\n",
      "Iteration 815, loss = 0.29988541\n",
      "Iteration 1163, loss = 0.22277130\n",
      "Iteration 549, loss = 0.34130472\n",
      "Iteration 259, loss = 0.40316362\n",
      "Iteration 816, loss = 0.29976092\n",
      "Iteration 550, loss = 0.34120819\n",
      "Iteration 922, loss = 0.31233050\n",
      "Iteration 1785, loss = 0.20557065\n",
      "Iteration 817, loss = 0.29961430\n",
      "Iteration 170, loss = 0.44294709\n",
      "Iteration 818, loss = 0.29948093\n",
      "Iteration 578, loss = 0.37166937\n",
      "Iteration 1164, loss = 0.22243268\n",
      "Iteration 260, loss = 0.40290892\n",
      "Iteration 819, loss = 0.29933234\n",
      "Iteration 923, loss = 0.31217308\n",
      "Iteration 551, loss = 0.34108107\n",
      "Iteration 171, loss = 0.44223810\n",
      "Iteration 820, loss = 0.29920842\n",
      "Iteration 1786, loss = 0.20549022\n",
      "Iteration 261, loss = 0.40267551\n",
      "Iteration 924, loss = 0.31206030\n",
      "Iteration 1787, loss = 0.20539571\n",
      "Iteration 552, loss = 0.34096936\n",
      "Iteration 172, loss = 0.44155743\n",
      "Iteration 553, loss = 0.34086714\n",
      "Iteration 821, loss = 0.29909299\n",
      "Iteration 173, loss = 0.44085907\n",
      "Iteration 822, loss = 0.29902602\n",
      "Iteration 579, loss = 0.37139210\n",
      "Iteration 554, loss = 0.34078813\n",
      "Iteration 925, loss = 0.31190044\n",
      "Iteration 1165, loss = 0.22230770\n",
      "Iteration 823, loss = 0.29884571\n",
      "Iteration 262, loss = 0.40244315\n",
      "Iteration 1788, loss = 0.20525552\n",
      "Iteration 555, loss = 0.34065553\n",
      "Iteration 580, loss = 0.37115454\n",
      "Iteration 556, loss = 0.34055655Iteration 1166, loss = 0.22202248\n",
      "\n",
      "Iteration 1789, loss = 0.20511927\n",
      "Iteration 174, loss = 0.44021542\n",
      "Iteration 824, loss = 0.29869948\n",
      "Iteration 557, loss = 0.34048160\n",
      "Iteration 825, loss = 0.29856474\n",
      "Iteration 558, loss = 0.34037012\n",
      "Iteration 263, loss = 0.40221277\n",
      "Iteration 1167, loss = 0.22173374\n",
      "Iteration 926, loss = 0.31182957\n",
      "Iteration 175, loss = 0.43954237\n",
      "Iteration 1790, loss = 0.20526387\n",
      "Iteration 826, loss = 0.29842565\n",
      "Iteration 176, loss = 0.43889439\n",
      "Iteration 264, loss = 0.40195508\n",
      "Iteration 559, loss = 0.34022192\n",
      "Iteration 827, loss = 0.29833537\n",
      "Iteration 1791, loss = 0.20487416\n",
      "Iteration 828, loss = 0.29824746\n",
      "Iteration 1168, loss = 0.22150434\n",
      "Iteration 1792, loss = 0.20482446\n",
      "Iteration 265, loss = 0.40173421\n",
      "Iteration 560, loss = 0.34018463\n",
      "Iteration 829, loss = 0.29805925\n",
      "Iteration 927, loss = 0.31165730\n",
      "Iteration 581, loss = 0.37096703\n",
      "Iteration 561, loss = 0.34003771\n",
      "Iteration 177, loss = 0.43823698\n",
      "Iteration 830, loss = 0.29794637\n",
      "Iteration 928, loss = 0.31151801\n",
      "Iteration 831, loss = 0.29782159\n",
      "Iteration 1169, loss = 0.22127953\n",
      "Iteration 266, loss = 0.40154639\n",
      "Iteration 562, loss = 0.33993089\n",
      "Iteration 1793, loss = 0.20466284\n",
      "Iteration 832, loss = 0.29768331\n",
      "Iteration 1794, loss = 0.20466171\n",
      "Iteration 1170, loss = 0.22113550\n",
      "Iteration 833, loss = 0.29755203\n",
      "Iteration 1795, loss = 0.20440631\n",
      "Iteration 267, loss = 0.40127847\n",
      "Iteration 178, loss = 0.43756139\n",
      "Iteration 834, loss = 0.29746292\n",
      "Iteration 563, loss = 0.33982922\n",
      "Iteration 1796, loss = 0.20437182\n",
      "Iteration 582, loss = 0.37066230\n",
      "Iteration 268, loss = 0.40104254\n",
      "Iteration 929, loss = 0.31142309\n",
      "Iteration 1797, loss = 0.20428275\n",
      "Iteration 1171, loss = 0.22091153\n",
      "Iteration 179, loss = 0.43696985\n",
      "Iteration 835, loss = 0.29729209\n",
      "Iteration 180, loss = 0.43629360\n",
      "Iteration 583, loss = 0.37045623\n",
      "Iteration 836, loss = 0.29720253\n",
      "Iteration 269, loss = 0.40081464\n",
      "Iteration 564, loss = 0.33972313\n",
      "Iteration 1798, loss = 0.20408789\n",
      "Iteration 181, loss = 0.43568397\n",
      "Iteration 930, loss = 0.31129024\n",
      "Iteration 837, loss = 0.29701747\n",
      "Iteration 838, loss = 0.29693548\n",
      "Iteration 565, loss = 0.33962606\n",
      "Iteration 839, loss = 0.29681565\n",
      "Iteration 270, loss = 0.40062881\n",
      "Iteration 1799, loss = 0.20392800\n",
      "Iteration 840, loss = 0.29662476\n",
      "Iteration 1172, loss = 0.22074944\n",
      "Iteration 566, loss = 0.33958112\n",
      "Iteration 931, loss = 0.31115649\n",
      "Iteration 1800, loss = 0.20381741\n",
      "Iteration 841, loss = 0.29659628\n",
      "Iteration 1801, loss = 0.20375352\n",
      "Iteration 584, loss = 0.37029185\n",
      "Iteration 567, loss = 0.33941433\n",
      "Iteration 1802, loss = 0.20360618\n",
      "Iteration 1173, loss = 0.22039161\n",
      "Iteration 842, loss = 0.29635786\n",
      "Iteration 1803, loss = 0.20352213\n",
      "Iteration 271, loss = 0.40037624\n",
      "Iteration 182, loss = 0.43514412\n",
      "Iteration 843, loss = 0.29632140\n",
      "Iteration 932, loss = 0.31111385\n",
      "Iteration 568, loss = 0.33931455\n",
      "Iteration 585, loss = 0.36999743\n",
      "Iteration 1804, loss = 0.20334989\n",
      "Iteration 272, loss = 0.40018454\n",
      "Iteration 1174, loss = 0.22063264\n",
      "Iteration 844, loss = 0.29619665\n",
      "Iteration 1175, loss = 0.22004139\n",
      "Iteration 933, loss = 0.31090278\n",
      "Iteration 183, loss = 0.43450776\n",
      "Iteration 845, loss = 0.29598528\n",
      "Iteration 1805, loss = 0.20330510\n",
      "Iteration 569, loss = 0.33923060\n",
      "Iteration 846, loss = 0.29588793\n",
      "Iteration 570, loss = 0.33918009\n",
      "Iteration 934, loss = 0.31075705\n",
      "Iteration 1806, loss = 0.20310539\n",
      "Iteration 184, loss = 0.43391766\n",
      "Iteration 571, loss = 0.33897595\n",
      "Iteration 847, loss = 0.29575548\n",
      "Iteration 572, loss = 0.33889722\n",
      "Iteration 1176, loss = 0.21984819\n",
      "Iteration 935, loss = 0.31062142\n",
      "Iteration 848, loss = 0.29566496\n",
      "Iteration 849, loss = 0.29546472\n",
      "Iteration 1807, loss = 0.20301544\n",
      "Iteration 586, loss = 0.36981162\n",
      "Iteration 850, loss = 0.29536831\n",
      "Iteration 185, loss = 0.43332026\n",
      "Iteration 273, loss = 0.39993986\n",
      "Iteration 1808, loss = 0.20295114\n",
      "Iteration 573, loss = 0.33876715\n",
      "Iteration 851, loss = 0.29525689\n",
      "Iteration 1177, loss = 0.21977805\n",
      "Iteration 852, loss = 0.29513319\n",
      "Iteration 587, loss = 0.36956745\n",
      "Iteration 936, loss = 0.31054877\n",
      "Iteration 274, loss = 0.39975295\n",
      "Iteration 1809, loss = 0.20284053\n",
      "Iteration 853, loss = 0.29497631\n",
      "Iteration 1810, loss = 0.20282182\n",
      "Iteration 186, loss = 0.43273684\n",
      "Iteration 574, loss = 0.33865758\n",
      "Iteration 854, loss = 0.29485860\n",
      "Iteration 1178, loss = 0.21965350\n",
      "Iteration 855, loss = 0.29479800\n",
      "Iteration 275, loss = 0.39954535\n",
      "Iteration 575, loss = 0.33869367\n",
      "Iteration 856, loss = 0.29456147\n",
      "Iteration 588, loss = 0.36935304\n",
      "Iteration 857, loss = 0.29444028\n",
      "Iteration 1811, loss = 0.20255719\n",
      "Iteration 576, loss = 0.33846574\n",
      "Iteration 858, loss = 0.29434060\n",
      "Iteration 1179, loss = 0.21925882\n",
      "Iteration 187, loss = 0.43218475\n",
      "Iteration 937, loss = 0.31035665\n",
      "Iteration 859, loss = 0.29421627\n",
      "Iteration 860, loss = 0.29406739\n",
      "Iteration 276, loss = 0.39930554\n",
      "Iteration 861, loss = 0.29395002\n",
      "Iteration 589, loss = 0.36911686\n",
      "Iteration 577, loss = 0.33837760\n",
      "Iteration 1812, loss = 0.20245860\n",
      "Iteration 862, loss = 0.29379843\n",
      "Iteration 1180, loss = 0.21920465Iteration 277, loss = 0.39909351\n",
      "\n",
      "Iteration 188, loss = 0.43158290\n",
      "Iteration 863, loss = 0.29369133\n",
      "Iteration 189, loss = 0.43104953\n",
      "Iteration 278, loss = 0.39886912\n",
      "Iteration 864, loss = 0.29356618\n",
      "Iteration 578, loss = 0.33826818\n",
      "Iteration 590, loss = 0.36894136\n",
      "Iteration 1813, loss = 0.20235318\n",
      "Iteration 938, loss = 0.31023201\n",
      "Iteration 190, loss = 0.43045965\n",
      "Iteration 865, loss = 0.29346069\n",
      "Iteration 1181, loss = 0.21894752\n",
      "Iteration 866, loss = 0.29330448\n",
      "Iteration 191, loss = 0.42997940\n",
      "Iteration 279, loss = 0.39867835\n",
      "Iteration 1814, loss = 0.20223989\n",
      "Iteration 867, loss = 0.29315609\n",
      "Iteration 579, loss = 0.33822983\n",
      "Iteration 939, loss = 0.31011730\n",
      "Iteration 1182, loss = 0.21897172\n",
      "Iteration 1815, loss = 0.20209135\n",
      "Iteration 868, loss = 0.29302175\n",
      "Iteration 1183, loss = 0.21839401\n",
      "Iteration 1816, loss = 0.20196389\n",
      "Iteration 940, loss = 0.31001652\n",
      "Iteration 192, loss = 0.42940070\n",
      "Iteration 591, loss = 0.36866646\n",
      "Iteration 869, loss = 0.29288991\n",
      "Iteration 870, loss = 0.29281887\n",
      "Iteration 580, loss = 0.33809311\n",
      "Iteration 193, loss = 0.42889595\n",
      "Iteration 871, loss = 0.29262178\n",
      "Iteration 1817, loss = 0.20181186\n",
      "Iteration 872, loss = 0.29248937\n",
      "Iteration 280, loss = 0.39844269\n",
      "Iteration 873, loss = 0.29240564\n",
      "Iteration 1184, loss = 0.21818677\n",
      "Iteration 194, loss = 0.42831854\n",
      "Iteration 592, loss = 0.36851374\n",
      "Iteration 874, loss = 0.29234274\n",
      "Iteration 941, loss = 0.30986156\n",
      "Iteration 581, loss = 0.33793593\n",
      "Iteration 1818, loss = 0.20175070\n",
      "Iteration 875, loss = 0.29211469\n",
      "Iteration 195, loss = 0.42781486\n",
      "Iteration 281, loss = 0.39826423\n",
      "Iteration 1185, loss = 0.21815718\n",
      "Iteration 876, loss = 0.29196285\n",
      "Iteration 582, loss = 0.33782991\n",
      "Iteration 1819, loss = 0.20172746\n",
      "Iteration 282, loss = 0.39805678\n",
      "Iteration 877, loss = 0.29188035\n",
      "Iteration 196, loss = 0.42736234\n",
      "Iteration 1820, loss = 0.20156080\n",
      "Iteration 878, loss = 0.29173932\n",
      "Iteration 942, loss = 0.30974962\n",
      "Iteration 1186, loss = 0.21790345\n",
      "Iteration 583, loss = 0.33772090\n",
      "Iteration 879, loss = 0.29158744\n",
      "Iteration 1821, loss = 0.20143558\n",
      "Iteration 880, loss = 0.29151926\n",
      "Iteration 197, loss = 0.42680822\n",
      "Iteration 881, loss = 0.29134515\n",
      "Iteration 593, loss = 0.36842713\n",
      "Iteration 584, loss = 0.33764719\n",
      "Iteration 943, loss = 0.30977203\n",
      "Iteration 882, loss = 0.29118138\n",
      "Iteration 585, loss = 0.33753116\n",
      "Iteration 1822, loss = 0.20123519\n",
      "Iteration 883, loss = 0.29113168\n",
      "Iteration 1187, loss = 0.21769932\n",
      "Iteration 884, loss = 0.29091881\n",
      "Iteration 283, loss = 0.39785931\n",
      "Iteration 885, loss = 0.29075844\n",
      "Iteration 198, loss = 0.42631331\n",
      "Iteration 1188, loss = 0.21741182\n",
      "Iteration 586, loss = 0.33743218\n",
      "Iteration 594, loss = 0.36805631\n",
      "Iteration 886, loss = 0.29066436\n",
      "Iteration 587, loss = 0.33732524\n",
      "Iteration 1823, loss = 0.20120899\n",
      "Iteration 199, loss = 0.42581040\n",
      "Iteration 887, loss = 0.29049667\n",
      "Iteration 944, loss = 0.30947265\n",
      "Iteration 888, loss = 0.29035717\n",
      "Iteration 588, loss = 0.33722357\n",
      "Iteration 1824, loss = 0.20111650\n",
      "Iteration 889, loss = 0.29025745\n",
      "Iteration 1189, loss = 0.21742821\n",
      "Iteration 1825, loss = 0.20096328\n",
      "Iteration 890, loss = 0.29011236\n",
      "Iteration 589, loss = 0.33720359\n",
      "Iteration 200, loss = 0.42531891\n",
      "Iteration 891, loss = 0.28994549\n",
      "Iteration 1826, loss = 0.20081233\n",
      "Iteration 284, loss = 0.39762023\n",
      "Iteration 892, loss = 0.28983282\n",
      "Iteration 590, loss = 0.33701536\n",
      "Iteration 893, loss = 0.28969307\n",
      "Iteration 201, loss = 0.42489002\n",
      "Iteration 894, loss = 0.28953230\n",
      "Iteration 1190, loss = 0.21709844\n",
      "Iteration 595, loss = 0.36782184\n",
      "Iteration 945, loss = 0.30938092\n",
      "Iteration 285, loss = 0.39742906\n",
      "Iteration 895, loss = 0.28956028\n",
      "Iteration 591, loss = 0.33695233\n",
      "Iteration 1191, loss = 0.21682422\n",
      "Iteration 202, loss = 0.42440497\n",
      "Iteration 1827, loss = 0.20074017\n",
      "Iteration 896, loss = 0.28927639\n",
      "Iteration 592, loss = 0.33679384\n",
      "Iteration 1828, loss = 0.20056808\n",
      "Iteration 596, loss = 0.36756713\n",
      "Iteration 1192, loss = 0.21662347\n",
      "Iteration 897, loss = 0.28911712\n",
      "Iteration 946, loss = 0.30929047\n",
      "Iteration 593, loss = 0.33671104\n",
      "Iteration 898, loss = 0.28900729\n",
      "Iteration 286, loss = 0.39724295\n",
      "Iteration 594, loss = 0.33657521\n",
      "Iteration 899, loss = 0.28889824\n",
      "Iteration 1193, loss = 0.21647683\n",
      "Iteration 203, loss = 0.42389710\n",
      "Iteration 900, loss = 0.28872340\n",
      "Iteration 597, loss = 0.36739154\n",
      "Iteration 1829, loss = 0.20042776\n",
      "Iteration 204, loss = 0.42347825\n",
      "Iteration 1194, loss = 0.21623309\n",
      "Iteration 901, loss = 0.28870817\n",
      "Iteration 902, loss = 0.28846977\n",
      "Iteration 947, loss = 0.30915414\n",
      "Iteration 595, loss = 0.33648009\n",
      "Iteration 1830, loss = 0.20038046\n",
      "Iteration 903, loss = 0.28831124\n",
      "Iteration 287, loss = 0.39704649\n",
      "Iteration 904, loss = 0.28820149\n",
      "Iteration 1831, loss = 0.20020402\n",
      "Iteration 596, loss = 0.33644548\n",
      "Iteration 1195, loss = 0.21599378\n",
      "Iteration 948, loss = 0.30899413\n",
      "Iteration 597, loss = 0.33630194\n",
      "Iteration 205, loss = 0.42310750\n",
      "Iteration 1832, loss = 0.20013298\n",
      "Iteration 598, loss = 0.33617340\n",
      "Iteration 598, loss = 0.36712873\n",
      "Iteration 905, loss = 0.28809520\n",
      "Iteration 1833, loss = 0.20003545\n",
      "Iteration 1196, loss = 0.21576371\n",
      "Iteration 906, loss = 0.28787469\n",
      "Iteration 599, loss = 0.33607301\n",
      "Iteration 1834, loss = 0.19988278\n",
      "Iteration 288, loss = 0.39682380\n",
      "Iteration 600, loss = 0.33595917\n",
      "Iteration 1197, loss = 0.21558296\n",
      "Iteration 907, loss = 0.28775093\n",
      "Iteration 949, loss = 0.30886481\n",
      "Iteration 1835, loss = 0.19978216\n",
      "Iteration 601, loss = 0.33585513\n",
      "Iteration 908, loss = 0.28760798\n",
      "Iteration 206, loss = 0.42257961\n",
      "Iteration 1836, loss = 0.19965138\n",
      "Iteration 1198, loss = 0.21541084\n",
      "Iteration 909, loss = 0.28748442\n",
      "Iteration 950, loss = 0.30872139\n",
      "Iteration 289, loss = 0.39664825\n",
      "Iteration 602, loss = 0.33575256\n",
      "Iteration 599, loss = 0.36690346\n",
      "Iteration 910, loss = 0.28734510\n",
      "Iteration 603, loss = 0.33571854\n",
      "Iteration 911, loss = 0.28723243\n",
      "Iteration 912, loss = 0.28709693\n",
      "Iteration 207, loss = 0.42215859\n",
      "Iteration 604, loss = 0.33556342\n",
      "Iteration 1837, loss = 0.19953149\n",
      "Iteration 290, loss = 0.39646501\n",
      "Iteration 1199, loss = 0.21523296\n",
      "Iteration 913, loss = 0.28699501\n",
      "Iteration 208, loss = 0.42170133\n",
      "Iteration 951, loss = 0.30858896\n",
      "Iteration 914, loss = 0.28677131\n",
      "Iteration 915, loss = 0.28672390\n",
      "Iteration 291, loss = 0.39624514\n",
      "Iteration 916, loss = 0.28650949\n",
      "Iteration 1838, loss = 0.19940726\n",
      "Iteration 952, loss = 0.30847389\n",
      "Iteration 605, loss = 0.33544747\n",
      "Iteration 1200, loss = 0.21506767\n",
      "Iteration 1839, loss = 0.19937710\n",
      "Iteration 917, loss = 0.28636611\n",
      "Iteration 292, loss = 0.39606349\n",
      "Iteration 209, loss = 0.42125544\n",
      "Iteration 918, loss = 0.28622069\n",
      "Iteration 600, loss = 0.36668232\n",
      "Iteration 1840, loss = 0.19916893\n",
      "Iteration 919, loss = 0.28610346\n",
      "Iteration 953, loss = 0.30844913\n",
      "Iteration 606, loss = 0.33534203\n",
      "Iteration 920, loss = 0.28596065\n",
      "Iteration 921, loss = 0.28585419\n",
      "Iteration 607, loss = 0.33525073\n",
      "Iteration 1201, loss = 0.21481472\n",
      "Iteration 293, loss = 0.39586526\n",
      "Iteration 1841, loss = 0.19906888\n",
      "Iteration 608, loss = 0.33513086\n",
      "Iteration 922, loss = 0.28570405\n",
      "Iteration 210, loss = 0.42086950\n",
      "Iteration 601, loss = 0.36642820\n",
      "Iteration 954, loss = 0.30822826\n",
      "Iteration 1842, loss = 0.19893055\n",
      "Iteration 923, loss = 0.28556662\n",
      "Iteration 1202, loss = 0.21457857\n",
      "Iteration 609, loss = 0.33505664\n",
      "Iteration 924, loss = 0.28549130\n",
      "Iteration 211, loss = 0.42043879\n",
      "Iteration 925, loss = 0.28529806\n",
      "Iteration 1843, loss = 0.19880201\n",
      "Iteration 610, loss = 0.33496349\n",
      "Iteration 294, loss = 0.39568588\n",
      "Iteration 212, loss = 0.42001823\n",
      "Iteration 926, loss = 0.28512531\n",
      "Iteration 1844, loss = 0.19879475\n",
      "Iteration 955, loss = 0.30807392\n",
      "Iteration 1203, loss = 0.21432692\n",
      "Iteration 213, loss = 0.41959172\n",
      "Iteration 927, loss = 0.28502698\n",
      "Iteration 1845, loss = 0.19864296\n",
      "Iteration 928, loss = 0.28485786\n",
      "Iteration 611, loss = 0.33484075\n",
      "Iteration 929, loss = 0.28472337\n",
      "Iteration 1204, loss = 0.21414460\n",
      "Iteration 930, loss = 0.28457668\n",
      "Iteration 295, loss = 0.39548584\n",
      "Iteration 602, loss = 0.36619315\n",
      "Iteration 1846, loss = 0.19849848\n",
      "Iteration 612, loss = 0.33471940\n",
      "Iteration 931, loss = 0.28449709\n",
      "Iteration 1205, loss = 0.21392714\n",
      "Iteration 932, loss = 0.28430048\n",
      "Iteration 613, loss = 0.33461228\n",
      "Iteration 1847, loss = 0.19844390\n",
      "Iteration 214, loss = 0.41923641\n",
      "Iteration 956, loss = 0.30794308\n",
      "Iteration 296, loss = 0.39527844\n",
      "Iteration 933, loss = 0.28418334\n",
      "Iteration 614, loss = 0.33453823\n",
      "Iteration 603, loss = 0.36599268\n",
      "Iteration 934, loss = 0.28405869\n",
      "Iteration 1206, loss = 0.21385152\n",
      "Iteration 615, loss = 0.33440468\n",
      "Iteration 1848, loss = 0.19829988\n",
      "Iteration 215, loss = 0.41880232\n",
      "Iteration 957, loss = 0.30782519\n",
      "Iteration 616, loss = 0.33430577\n",
      "Iteration 935, loss = 0.28400535\n",
      "Iteration 617, loss = 0.33425009\n",
      "Iteration 936, loss = 0.28375394\n",
      "Iteration 216, loss = 0.41842215\n",
      "Iteration 297, loss = 0.39509650\n",
      "Iteration 618, loss = 0.33413555\n",
      "Iteration 1849, loss = 0.19814984\n",
      "Iteration 937, loss = 0.28367955\n",
      "Iteration 1207, loss = 0.21366785\n",
      "Iteration 604, loss = 0.36587313\n",
      "Iteration 958, loss = 0.30782281\n",
      "Iteration 619, loss = 0.33399774\n",
      "Iteration 938, loss = 0.28347437\n",
      "Iteration 217, loss = 0.41801086\n",
      "Iteration 939, loss = 0.28342917\n",
      "Iteration 1850, loss = 0.19810220\n",
      "Iteration 940, loss = 0.28323102\n",
      "Iteration 620, loss = 0.33390001\n",
      "Iteration 1208, loss = 0.21340548\n",
      "Iteration 941, loss = 0.28309108\n",
      "Iteration 605, loss = 0.36553737\n",
      "Iteration 959, loss = 0.30755594\n",
      "Iteration 1851, loss = 0.19791490\n",
      "Iteration 298, loss = 0.39490027\n",
      "Iteration 218, loss = 0.41765324\n",
      "Iteration 942, loss = 0.28294924\n",
      "Iteration 621, loss = 0.33379454\n",
      "Iteration 1852, loss = 0.19785419\n",
      "Iteration 1209, loss = 0.21310484\n",
      "Iteration 299, loss = 0.39472785\n",
      "Iteration 943, loss = 0.28280636\n",
      "Iteration 606, loss = 0.36531210\n",
      "Iteration 1853, loss = 0.19776149\n",
      "Iteration 622, loss = 0.33369845\n",
      "Iteration 219, loss = 0.41729910\n",
      "Iteration 1854, loss = 0.19753762\n",
      "Iteration 1210, loss = 0.21292051\n",
      "Iteration 960, loss = 0.30745450\n",
      "Iteration 944, loss = 0.28268284\n",
      "Iteration 220, loss = 0.41687877\n",
      "Iteration 1211, loss = 0.21268069\n",
      "Iteration 623, loss = 0.33358847\n",
      "Iteration 607, loss = 0.36520488\n",
      "Iteration 1855, loss = 0.19746393\n",
      "Iteration 945, loss = 0.28253770\n",
      "Iteration 300, loss = 0.39452948\n",
      "Iteration 946, loss = 0.28247057\n",
      "Iteration 1856, loss = 0.19729555\n",
      "Iteration 947, loss = 0.28232446\n",
      "Iteration 624, loss = 0.33349395\n",
      "Iteration 948, loss = 0.28211868\n",
      "Iteration 221, loss = 0.41650185\n",
      "Iteration 1212, loss = 0.21247902\n",
      "Iteration 961, loss = 0.30730496\n",
      "Iteration 949, loss = 0.28199962\n",
      "Iteration 301, loss = 0.39432515\n",
      "Iteration 1857, loss = 0.19724075\n",
      "Iteration 625, loss = 0.33339371\n",
      "Iteration 950, loss = 0.28181648\n",
      "Iteration 1213, loss = 0.21227202\n",
      "Iteration 1858, loss = 0.19707303\n",
      "Iteration 626, loss = 0.33329846\n",
      "Iteration 222, loss = 0.41613474\n",
      "Iteration 608, loss = 0.36498940\n",
      "Iteration 1859, loss = 0.19700582\n",
      "Iteration 951, loss = 0.28181837\n",
      "Iteration 302, loss = 0.39413113\n",
      "Iteration 223, loss = 0.41577557\n",
      "Iteration 962, loss = 0.30718865\n",
      "Iteration 1860, loss = 0.19691026\n",
      "Iteration 1214, loss = 0.21209885\n",
      "Iteration 952, loss = 0.28160856\n",
      "Iteration 627, loss = 0.33321304\n",
      "Iteration 224, loss = 0.41543927\n",
      "Iteration 953, loss = 0.28171543\n",
      "Iteration 303, loss = 0.39394609\n",
      "Iteration 1861, loss = 0.19674699\n",
      "Iteration 1215, loss = 0.21193915Iteration 954, loss = 0.28136486\n",
      "\n",
      "Iteration 963, loss = 0.30707188\n",
      "Iteration 225, loss = 0.41506104\n",
      "Iteration 628, loss = 0.33308619\n",
      "Iteration 955, loss = 0.28122580\n",
      "Iteration 226, loss = 0.41473256\n",
      "Iteration 956, loss = 0.28111493\n",
      "Iteration 1216, loss = 0.21183134\n",
      "Iteration 629, loss = 0.33302360\n",
      "Iteration 964, loss = 0.30691121\n",
      "Iteration 609, loss = 0.36460653\n",
      "Iteration 304, loss = 0.39384907\n",
      "Iteration 630, loss = 0.33287293\n",
      "Iteration 1862, loss = 0.19667980\n",
      "Iteration 631, loss = 0.33281467\n",
      "Iteration 227, loss = 0.41444586Iteration 957, loss = 0.28098236\n",
      "\n",
      "Iteration 965, loss = 0.30684054\n",
      "Iteration 958, loss = 0.28076202\n",
      "Iteration 1217, loss = 0.21162571\n",
      "Iteration 632, loss = 0.33269991\n",
      "Iteration 959, loss = 0.28061397\n",
      "Iteration 1863, loss = 0.19653804\n",
      "Iteration 228, loss = 0.41404354\n",
      "Iteration 633, loss = 0.33264385\n",
      "Iteration 960, loss = 0.28060105\n",
      "Iteration 1864, loss = 0.19638213\n",
      "Iteration 1218, loss = 0.21130573\n",
      "Iteration 634, loss = 0.33246630\n",
      "Iteration 229, loss = 0.41372795\n",
      "Iteration 610, loss = 0.36442795\n",
      "Iteration 305, loss = 0.39358654\n",
      "Iteration 1865, loss = 0.19633436\n",
      "Iteration 961, loss = 0.28037318\n",
      "Iteration 962, loss = 0.28036331\n",
      "Iteration 1219, loss = 0.21114262\n",
      "Iteration 635, loss = 0.33241404\n",
      "Iteration 1866, loss = 0.19622073\n",
      "Iteration 636, loss = 0.33227138\n",
      "Iteration 963, loss = 0.28013636\n",
      "Iteration 611, loss = 0.36422499\n",
      "Iteration 306, loss = 0.39341467\n",
      "Iteration 230, loss = 0.41337772\n",
      "Iteration 1867, loss = 0.19605541\n",
      "Iteration 966, loss = 0.30677916\n",
      "Iteration 637, loss = 0.33217339\n",
      "Iteration 1868, loss = 0.19595366\n",
      "Iteration 964, loss = 0.27995409\n",
      "Iteration 1220, loss = 0.21094875\n",
      "Iteration 231, loss = 0.41302697\n",
      "Iteration 965, loss = 0.27987984\n",
      "Iteration 307, loss = 0.39323804\n",
      "Iteration 966, loss = 0.27978487\n",
      "Iteration 1221, loss = 0.21074972\n",
      "Iteration 232, loss = 0.41272226\n",
      "Iteration 967, loss = 0.30664474\n",
      "Iteration 1869, loss = 0.19591240\n",
      "Iteration 612, loss = 0.36394917\n",
      "Iteration 638, loss = 0.33209327\n",
      "Iteration 967, loss = 0.27965769\n",
      "Iteration 1222, loss = 0.21065441\n",
      "Iteration 968, loss = 0.27943201\n",
      "Iteration 308, loss = 0.39306414\n",
      "Iteration 968, loss = 0.30641598\n",
      "Iteration 233, loss = 0.41241850\n",
      "Iteration 1870, loss = 0.19574296\n",
      "Iteration 969, loss = 0.27932024\n",
      "Iteration 639, loss = 0.33197707\n",
      "Iteration 1871, loss = 0.19558006\n",
      "Iteration 970, loss = 0.27917732\n",
      "Iteration 640, loss = 0.33188009\n",
      "Iteration 1223, loss = 0.21059227\n",
      "Iteration 1872, loss = 0.19563725\n",
      "Iteration 969, loss = 0.30638766\n",
      "Iteration 971, loss = 0.27904969\n",
      "Iteration 1873, loss = 0.19534901\n",
      "Iteration 972, loss = 0.27891454\n",
      "Iteration 641, loss = 0.33175661\n",
      "Iteration 613, loss = 0.36373533\n",
      "Iteration 973, loss = 0.27878679\n",
      "Iteration 234, loss = 0.41209300\n",
      "Iteration 1874, loss = 0.19524244\n",
      "Iteration 1224, loss = 0.21022826\n",
      "Iteration 642, loss = 0.33166360\n",
      "Iteration 974, loss = 0.27861818\n",
      "Iteration 975, loss = 0.27847830\n",
      "Iteration 309, loss = 0.39286212\n",
      "Iteration 643, loss = 0.33156923\n",
      "Iteration 1875, loss = 0.19515089\n",
      "Iteration 976, loss = 0.27836151\n",
      "Iteration 970, loss = 0.30617127\n",
      "Iteration 1225, loss = 0.20989782\n",
      "Iteration 235, loss = 0.41180118\n",
      "Iteration 310, loss = 0.39269864\n",
      "Iteration 977, loss = 0.27825319\n",
      "Iteration 644, loss = 0.33151056\n",
      "Iteration 1876, loss = 0.19512565\n",
      "Iteration 1226, loss = 0.20988363\n",
      "Iteration 978, loss = 0.27808259\n",
      "Iteration 971, loss = 0.30601730\n",
      "Iteration 645, loss = 0.33138578\n",
      "Iteration 979, loss = 0.27813739\n",
      "Iteration 1877, loss = 0.19510265\n",
      "Iteration 614, loss = 0.36351105\n",
      "Iteration 646, loss = 0.33125835\n",
      "Iteration 972, loss = 0.30591498\n",
      "Iteration 311, loss = 0.39251325\n",
      "Iteration 980, loss = 0.27785875\n",
      "Iteration 236, loss = 0.41144838\n",
      "Iteration 647, loss = 0.33115988\n",
      "Iteration 1878, loss = 0.19482508\n",
      "Iteration 648, loss = 0.33105667\n",
      "Iteration 1227, loss = 0.20953774\n",
      "Iteration 615, loss = 0.36347127\n",
      "Iteration 237, loss = 0.41116762\n",
      "Iteration 649, loss = 0.33092972\n",
      "Iteration 973, loss = 0.30580546\n",
      "Iteration 1228, loss = 0.20929523\n",
      "Iteration 650, loss = 0.33086144\n",
      "Iteration 1879, loss = 0.19471082\n",
      "Iteration 238, loss = 0.41084149\n",
      "Iteration 981, loss = 0.27767671\n",
      "Iteration 312, loss = 0.39238275\n",
      "Iteration 1229, loss = 0.20925772Iteration 982, loss = 0.27757018\n",
      "Iteration 1880, loss = 0.19456443\n",
      "Iteration 239, loss = 0.41053029\n",
      "Iteration 983, loss = 0.27741552\n",
      "Iteration 651, loss = 0.33073515\n",
      "Iteration 974, loss = 0.30564770\n",
      "Iteration 616, loss = 0.36308345\n",
      "Iteration 984, loss = 0.27725515\n",
      "Iteration 1881, loss = 0.19443399\n",
      "\n",
      "Iteration 652, loss = 0.33064359\n",
      "Iteration 985, loss = 0.27713896\n",
      "Iteration 975, loss = 0.30551573\n",
      "Iteration 313, loss = 0.39223333\n",
      "Iteration 1230, loss = 0.20951013\n",
      "Iteration 240, loss = 0.41029877\n",
      "Iteration 986, loss = 0.27699651\n",
      "Iteration 653, loss = 0.33054146\n",
      "Iteration 987, loss = 0.27684376\n",
      "Iteration 988, loss = 0.27672473\n",
      "Iteration 241, loss = 0.41000852\n",
      "Iteration 1882, loss = 0.19437983\n",
      "Iteration 617, loss = 0.36292114\n",
      "Iteration 989, loss = 0.27658997\n",
      "Iteration 990, loss = 0.27644605\n",
      "Iteration 1883, loss = 0.19422035\n",
      "Iteration 654, loss = 0.33043381\n",
      "Iteration 242, loss = 0.40964733\n",
      "Iteration 991, loss = 0.27628906\n",
      "Iteration 1884, loss = 0.19407916\n",
      "Iteration 314, loss = 0.39204142\n",
      "Iteration 1231, loss = 0.20894447\n",
      "Iteration 655, loss = 0.33031716\n",
      "Iteration 976, loss = 0.30540516\n",
      "Iteration 992, loss = 0.27618838\n",
      "Iteration 1885, loss = 0.19397573\n",
      "Iteration 656, loss = 0.33026242\n",
      "Iteration 1232, loss = 0.20847701\n",
      "Iteration 993, loss = 0.27602436\n",
      "Iteration 315, loss = 0.39182243Iteration 618, loss = 0.36279544\n",
      "Iteration 1886, loss = 0.19387282\n",
      "Iteration 657, loss = 0.33019112\n",
      "Iteration 994, loss = 0.27592171\n",
      "Iteration 243, loss = 0.40936776\n",
      "\n",
      "Iteration 1233, loss = 0.20840662\n",
      "Iteration 995, loss = 0.27575428\n",
      "Iteration 977, loss = 0.30527791\n",
      "Iteration 244, loss = 0.40909026\n",
      "Iteration 996, loss = 0.27560371\n",
      "Iteration 658, loss = 0.33003637\n",
      "Iteration 316, loss = 0.39165343\n",
      "Iteration 619, loss = 0.36259555\n",
      "Iteration 245, loss = 0.40880255\n",
      "Iteration 1887, loss = 0.19374430\n",
      "Iteration 1888, loss = 0.19398675\n",
      "Iteration 317, loss = 0.39147286\n",
      "Iteration 997, loss = 0.27554195\n",
      "Iteration 1234, loss = 0.20810757\n",
      "Iteration 998, loss = 0.27534099\n",
      "Iteration 246, loss = 0.40854824\n",
      "Iteration 620, loss = 0.36212819\n",
      "Iteration 1235, loss = 0.20793461\n",
      "Iteration 659, loss = 0.32992977\n",
      "Iteration 1889, loss = 0.19359769\n",
      "Iteration 1236, loss = 0.20766170\n",
      "Iteration 660, loss = 0.32985557\n",
      "Iteration 1890, loss = 0.19342684\n",
      "Iteration 999, loss = 0.27518796\n",
      "Iteration 1237, loss = 0.20754921\n",
      "Iteration 1000, loss = 0.27506037\n",
      "Iteration 978, loss = 0.30517736\n",
      "Iteration 661, loss = 0.32987404\n",
      "Iteration 1891, loss = 0.19324990\n",
      "Iteration 318, loss = 0.39130330\n",
      "Iteration 247, loss = 0.40821779\n",
      "Iteration 1892, loss = 0.19315503\n",
      "Iteration 662, loss = 0.32963427\n",
      "Iteration 1001, loss = 0.27496832\n",
      "Iteration 1893, loss = 0.19305998\n",
      "Iteration 319, loss = 0.39115302\n",
      "Iteration 663, loss = 0.32952769\n",
      "Iteration 1002, loss = 0.27480118\n",
      "Iteration 1003, loss = 0.27469478\n",
      "Iteration 1004, loss = 0.27452040\n",
      "Iteration 248, loss = 0.40792927\n",
      "Iteration 979, loss = 0.30500801\n",
      "Iteration 664, loss = 0.32943971\n",
      "Iteration 1005, loss = 0.27437378\n",
      "Iteration 1238, loss = 0.20732601\n",
      "Iteration 1894, loss = 0.19297179\n",
      "Iteration 665, loss = 0.32931472\n",
      "Iteration 1006, loss = 0.27424813\n",
      "Iteration 249, loss = 0.40770860\n",
      "Iteration 1895, loss = 0.19297360\n",
      "Iteration 666, loss = 0.32920642\n",
      "Iteration 621, loss = 0.36188975\n",
      "Iteration 1239, loss = 0.20711261\n",
      "Iteration 1007, loss = 0.27410945\n",
      "Iteration 980, loss = 0.30488786\n",
      "Iteration 667, loss = 0.32910777\n",
      "Iteration 1008, loss = 0.27395601\n",
      "Iteration 1896, loss = 0.19281428\n",
      "Iteration 1009, loss = 0.27383830\n",
      "Iteration 320, loss = 0.39096899\n",
      "Iteration 1897, loss = 0.19258119\n",
      "Iteration 250, loss = 0.40741870\n",
      "Iteration 1240, loss = 0.20723989\n",
      "Iteration 981, loss = 0.30488036\n",
      "Iteration 668, loss = 0.32904253\n",
      "Iteration 622, loss = 0.36167885\n",
      "Iteration 1010, loss = 0.27367117\n",
      "Iteration 1898, loss = 0.19250769\n",
      "Iteration 669, loss = 0.32891078\n",
      "Iteration 1241, loss = 0.20674711\n",
      "Iteration 1011, loss = 0.27369057\n",
      "Iteration 321, loss = 0.39080626\n",
      "Iteration 1899, loss = 0.19242728\n",
      "Iteration 1012, loss = 0.27342665\n",
      "Iteration 1242, loss = 0.20654758\n",
      "Iteration 251, loss = 0.40713558\n",
      "Iteration 1013, loss = 0.27324049\n",
      "Iteration 982, loss = 0.30462518\n",
      "Iteration 1243, loss = 0.20633989\n",
      "Iteration 1900, loss = 0.19224925\n",
      "Iteration 670, loss = 0.32879539\n",
      "Iteration 1014, loss = 0.27309614\n",
      "Iteration 252, loss = 0.40691166\n",
      "Iteration 322, loss = 0.39064374\n",
      "Iteration 671, loss = 0.32873735\n",
      "Iteration 983, loss = 0.30451142\n",
      "Iteration 1901, loss = 0.19214159\n",
      "Iteration 1015, loss = 0.27296674\n",
      "Iteration 1016, loss = 0.27287049\n",
      "Iteration 672, loss = 0.32864471\n",
      "Iteration 323, loss = 0.39044763\n",
      "Iteration 1017, loss = 0.27272331\n",
      "Iteration 623, loss = 0.36154971\n",
      "Iteration 253, loss = 0.40658598\n",
      "Iteration 1902, loss = 0.19200440\n",
      "Iteration 984, loss = 0.30440655\n",
      "Iteration 673, loss = 0.32852322\n",
      "Iteration 1244, loss = 0.20615496\n",
      "Iteration 1018, loss = 0.27260390\n",
      "Iteration 254, loss = 0.40640262\n",
      "Iteration 1019, loss = 0.27239653\n",
      "Iteration 1903, loss = 0.19188929\n",
      "Iteration 1020, loss = 0.27235001\n",
      "Iteration 624, loss = 0.36122498\n",
      "Iteration 985, loss = 0.30424930\n",
      "Iteration 1245, loss = 0.20589869\n",
      "Iteration 255, loss = 0.40609763\n",
      "Iteration 674, loss = 0.32843235\n",
      "Iteration 324, loss = 0.39029392\n",
      "Iteration 1904, loss = 0.19180561\n",
      "Iteration 1021, loss = 0.27211671\n",
      "Iteration 1022, loss = 0.27199752\n",
      "Iteration 625, loss = 0.36122988\n",
      "Iteration 256, loss = 0.40583372\n",
      "Iteration 1246, loss = 0.20575999\n",
      "Iteration 675, loss = 0.32828455\n",
      "Iteration 1905, loss = 0.19209590\n",
      "Iteration 986, loss = 0.30414135\n",
      "Iteration 1023, loss = 0.27183202\n",
      "Iteration 676, loss = 0.32820470\n",
      "Iteration 325, loss = 0.39011355\n",
      "Iteration 1906, loss = 0.19159536\n",
      "Iteration 1024, loss = 0.27174884\n",
      "Iteration 677, loss = 0.32813634\n",
      "Iteration 1907, loss = 0.19146476\n",
      "Iteration 678, loss = 0.32798942\n",
      "Iteration 1025, loss = 0.27162164\n",
      "Iteration 326, loss = 0.38996188\n",
      "Iteration 1908, loss = 0.19140746\n",
      "Iteration 626, loss = 0.36077766\n",
      "Iteration 1247, loss = 0.20549157\n",
      "Iteration 257, loss = 0.40555302\n",
      "Iteration 1909, loss = 0.19121570\n",
      "Iteration 1026, loss = 0.27141983\n",
      "Iteration 987, loss = 0.30400371\n",
      "Iteration 679, loss = 0.32791104\n",
      "Iteration 1027, loss = 0.27131580\n",
      "Iteration 1028, loss = 0.27116682\n",
      "Iteration 1248, loss = 0.20530576\n",
      "Iteration 1029, loss = 0.27103388\n",
      "Iteration 680, loss = 0.32777895\n",
      "Iteration 1249, loss = 0.20511446\n",
      "Iteration 1030, loss = 0.27086086\n",
      "Iteration 258, loss = 0.40529462\n",
      "Iteration 627, loss = 0.36054687\n",
      "Iteration 681, loss = 0.32771554\n",
      "Iteration 1031, loss = 0.27071165\n",
      "Iteration 327, loss = 0.38977549\n",
      "Iteration 682, loss = 0.32762799\n",
      "Iteration 1032, loss = 0.27056057\n",
      "Iteration 1910, loss = 0.19115621\n",
      "Iteration 988, loss = 0.30395530\n",
      "Iteration 1033, loss = 0.27053586\n",
      "Iteration 328, loss = 0.38961645\n",
      "Iteration 1034, loss = 0.27031690\n",
      "Iteration 683, loss = 0.32748458\n",
      "Iteration 628, loss = 0.36036140\n",
      "Iteration 259, loss = 0.40504064\n",
      "Iteration 1911, loss = 0.19106309\n",
      "Iteration 1035, loss = 0.27016345\n",
      "Iteration 684, loss = 0.32739141\n",
      "Iteration 1250, loss = 0.20533785\n",
      "Iteration 1036, loss = 0.27001828\n",
      "Iteration 685, loss = 0.32733009\n",
      "Iteration 989, loss = 0.30375794\n",
      "Iteration 260, loss = 0.40493039\n",
      "Iteration 1037, loss = 0.26991065\n",
      "Iteration 1912, loss = 0.19095621\n",
      "Iteration 629, loss = 0.36012953\n",
      "Iteration 261, loss = 0.40460463\n",
      "Iteration 686, loss = 0.32718572\n",
      "Iteration 1251, loss = 0.20472472\n",
      "Iteration 1913, loss = 0.19079755\n",
      "Iteration 1038, loss = 0.26973963\n",
      "Iteration 329, loss = 0.38943970\n",
      "Iteration 1039, loss = 0.26959849\n",
      "Iteration 1914, loss = 0.19080670\n",
      "Iteration 1040, loss = 0.26944884\n",
      "Iteration 990, loss = 0.30361403\n",
      "Iteration 687, loss = 0.32705679\n",
      "Iteration 1041, loss = 0.26932480\n",
      "Iteration 1252, loss = 0.20479602\n",
      "Iteration 1915, loss = 0.19063065\n",
      "Iteration 688, loss = 0.32695438\n",
      "Iteration 262, loss = 0.40431587\n",
      "Iteration 330, loss = 0.38929611\n",
      "Iteration 1916, loss = 0.19060355\n",
      "Iteration 1042, loss = 0.26915439\n",
      "Iteration 1253, loss = 0.20442211\n",
      "Iteration 630, loss = 0.35987353\n",
      "Iteration 1043, loss = 0.26900290\n",
      "Iteration 689, loss = 0.32685405\n",
      "Iteration 1917, loss = 0.19032108\n",
      "Iteration 1044, loss = 0.26898077\n",
      "Iteration 331, loss = 0.38922501\n",
      "Iteration 690, loss = 0.32682713\n",
      "Iteration 263, loss = 0.40413884\n",
      "Iteration 1254, loss = 0.20420680\n",
      "Iteration 1045, loss = 0.26876155\n",
      "Iteration 991, loss = 0.30350585\n",
      "Iteration 691, loss = 0.32664817\n",
      "Iteration 1918, loss = 0.19023077\n",
      "Iteration 631, loss = 0.35963644\n",
      "Iteration 1255, loss = 0.20398016\n",
      "Iteration 1046, loss = 0.26869989\n",
      "Iteration 1919, loss = 0.19017390\n",
      "Iteration 332, loss = 0.38895622\n",
      "Iteration 692, loss = 0.32657895\n",
      "Iteration 1047, loss = 0.26846252\n",
      "Iteration 1048, loss = 0.26834782\n",
      "Iteration 992, loss = 0.30340981\n",
      "Iteration 693, loss = 0.32645879\n",
      "Iteration 1049, loss = 0.26829779\n",
      "Iteration 264, loss = 0.40385284\n",
      "Iteration 1256, loss = 0.20376747\n",
      "Iteration 1920, loss = 0.19004528\n",
      "Iteration 1050, loss = 0.26806824\n",
      "Iteration 694, loss = 0.32633681\n",
      "Iteration 1921, loss = 0.18985662\n",
      "Iteration 695, loss = 0.32625961\n",
      "Iteration 1257, loss = 0.20386731\n",
      "Iteration 1051, loss = 0.26790521\n",
      "Iteration 265, loss = 0.40365890\n",
      "Iteration 1922, loss = 0.18973000\n",
      "Iteration 993, loss = 0.30329530\n",
      "Iteration 696, loss = 0.32614416\n",
      "Iteration 333, loss = 0.38883153\n",
      "Iteration 1923, loss = 0.18965623\n",
      "Iteration 632, loss = 0.35945225\n",
      "Iteration 1052, loss = 0.26775877\n",
      "Iteration 1258, loss = 0.20344814\n",
      "Iteration 994, loss = 0.30312627\n",
      "Iteration 1924, loss = 0.18979708\n",
      "Iteration 697, loss = 0.32607693\n",
      "Iteration 266, loss = 0.40332866\n",
      "Iteration 1053, loss = 0.26773295\n",
      "Iteration 1259, loss = 0.20322106\n",
      "Iteration 1054, loss = 0.26752314\n",
      "Iteration 334, loss = 0.38863983\n",
      "Iteration 1055, loss = 0.26741057\n",
      "Iteration 1925, loss = 0.18944900\n",
      "Iteration 1056, loss = 0.26724728\n",
      "Iteration 1260, loss = 0.20322351\n",
      "Iteration 1057, loss = 0.26716253\n",
      "Iteration 267, loss = 0.40312129\n",
      "Iteration 1926, loss = 0.18931512\n",
      "Iteration 698, loss = 0.32598679\n",
      "Iteration 1058, loss = 0.26691156\n",
      "Iteration 633, loss = 0.35917512\n",
      "Iteration 1261, loss = 0.20278434\n",
      "Iteration 1927, loss = 0.18938372\n",
      "Iteration 699, loss = 0.32584260\n",
      "Iteration 335, loss = 0.38849912\n",
      "Iteration 1059, loss = 0.26679257\n",
      "Iteration 268, loss = 0.40285098\n",
      "Iteration 995, loss = 0.30297939\n",
      "Iteration 1060, loss = 0.26661722\n",
      "Iteration 1262, loss = 0.20252284\n",
      "Iteration 1928, loss = 0.18914890\n",
      "Iteration 634, loss = 0.35894804\n",
      "Iteration 700, loss = 0.32574805\n",
      "Iteration 1929, loss = 0.18901162\n",
      "Iteration 336, loss = 0.38833591\n",
      "Iteration 1930, loss = 0.18887907\n",
      "Iteration 1263, loss = 0.20241328\n",
      "Iteration 701, loss = 0.32562545\n",
      "Iteration 269, loss = 0.40268054\n",
      "Iteration 1931, loss = 0.18880210\n",
      "Iteration 1061, loss = 0.26644719Iteration 996, loss = 0.30286801\n",
      "Iteration 702, loss = 0.32553012\n",
      "Iteration 1264, loss = 0.20226401\n",
      "\n",
      "Iteration 703, loss = 0.32542656\n",
      "Iteration 270, loss = 0.40242724\n",
      "Iteration 1062, loss = 0.26632958\n",
      "Iteration 337, loss = 0.38814199\n",
      "Iteration 1265, loss = 0.20211526\n",
      "Iteration 1932, loss = 0.18862396\n",
      "Iteration 1063, loss = 0.26618977\n",
      "Iteration 997, loss = 0.30279043\n",
      "Iteration 1064, loss = 0.26626208\n",
      "Iteration 704, loss = 0.32531428\n",
      "Iteration 1933, loss = 0.18850208\n",
      "Iteration 1266, loss = 0.20176339\n",
      "Iteration 1065, loss = 0.26586991\n",
      "Iteration 635, loss = 0.35875536\n",
      "Iteration 271, loss = 0.40215266\n",
      "Iteration 705, loss = 0.32519952\n",
      "Iteration 1267, loss = 0.20184761\n",
      "Iteration 1934, loss = 0.18849432\n",
      "Iteration 706, loss = 0.32510811\n",
      "Iteration 1066, loss = 0.26587617\n",
      "Iteration 1067, loss = 0.26558980\n",
      "Iteration 338, loss = 0.38799420\n",
      "Iteration 272, loss = 0.40192923\n",
      "Iteration 707, loss = 0.32502058\n",
      "Iteration 998, loss = 0.30262992\n",
      "Iteration 1068, loss = 0.26545785\n",
      "Iteration 1268, loss = 0.20156243\n",
      "Iteration 1935, loss = 0.18835654\n",
      "Iteration 273, loss = 0.40175158\n",
      "Iteration 708, loss = 0.32496610\n",
      "Iteration 1069, loss = 0.26533373\n",
      "Iteration 1269, loss = 0.20121779\n",
      "Iteration 1936, loss = 0.18826425\n",
      "Iteration 339, loss = 0.38783979\n",
      "Iteration 274, loss = 0.40146170\n",
      "Iteration 1070, loss = 0.26518663\n",
      "Iteration 1270, loss = 0.20100208\n",
      "Iteration 636, loss = 0.35859967\n",
      "Iteration 1071, loss = 0.26507037\n",
      "Iteration 709, loss = 0.32478899\n",
      "Iteration 1072, loss = 0.26484591\n",
      "Iteration 999, loss = 0.30251275\n",
      "Iteration 1937, loss = 0.18808313\n",
      "Iteration 275, loss = 0.40125470\n",
      "Iteration 1073, loss = 0.26475803\n",
      "Iteration 1074, loss = 0.26456531\n",
      "Iteration 340, loss = 0.38768063\n",
      "Iteration 1938, loss = 0.18812891\n",
      "Iteration 1075, loss = 0.26442303\n",
      "Iteration 710, loss = 0.32469173\n",
      "Iteration 276, loss = 0.40100859\n",
      "Iteration 1939, loss = 0.18790635\n",
      "Iteration 1076, loss = 0.26430643\n",
      "Iteration 1271, loss = 0.20084830\n",
      "Iteration 1000, loss = 0.30235433\n",
      "Iteration 1940, loss = 0.18778896\n",
      "Iteration 711, loss = 0.32460455\n",
      "Iteration 341, loss = 0.38751787\n",
      "Iteration 637, loss = 0.35826849\n",
      "Iteration 1941, loss = 0.18764324\n",
      "Iteration 277, loss = 0.40078631\n",
      "Iteration 1077, loss = 0.26412359\n",
      "Iteration 1001, loss = 0.30227627\n",
      "Iteration 1078, loss = 0.26397168\n",
      "Iteration 1942, loss = 0.18756876\n",
      "Iteration 1079, loss = 0.26385171\n",
      "Iteration 1272, loss = 0.20088577\n",
      "Iteration 712, loss = 0.32449694\n",
      "Iteration 1080, loss = 0.26369070\n",
      "Iteration 1943, loss = 0.18739328\n",
      "Iteration 1002, loss = 0.30213293\n",
      "Iteration 342, loss = 0.38736754\n",
      "Iteration 638, loss = 0.35805720\n",
      "Iteration 1081, loss = 0.26356005\n",
      "Iteration 1944, loss = 0.18728845\n",
      "Iteration 1082, loss = 0.26351143\n",
      "Iteration 1945, loss = 0.18727323\n",
      "Iteration 1273, loss = 0.20050641Iteration 1083, loss = 0.26326713\n",
      "\n",
      "Iteration 278, loss = 0.40054736\n",
      "Iteration 1084, loss = 0.26306314\n",
      "Iteration 1946, loss = 0.18708047\n",
      "Iteration 639, loss = 0.35782440\n",
      "Iteration 1085, loss = 0.26292287\n",
      "Iteration 713, loss = 0.32437110\n",
      "Iteration 1947, loss = 0.18706991\n",
      "Iteration 1274, loss = 0.20036049\n",
      "Iteration 279, loss = 0.40034110\n",
      "Iteration 1086, loss = 0.26283565\n",
      "Iteration 343, loss = 0.38724127\n",
      "Iteration 1087, loss = 0.26266826\n",
      "Iteration 1948, loss = 0.18691647\n",
      "Iteration 1088, loss = 0.26244619\n",
      "Iteration 1003, loss = 0.30196769\n",
      "Iteration 1089, loss = 0.26235444\n",
      "Iteration 280, loss = 0.40010329\n",
      "Iteration 714, loss = 0.32426256\n",
      "Iteration 1275, loss = 0.20018167\n",
      "Iteration 1949, loss = 0.18676955\n",
      "Iteration 1090, loss = 0.26217233\n",
      "Iteration 715, loss = 0.32420494\n",
      "Iteration 1091, loss = 0.26200670\n",
      "Iteration 1950, loss = 0.18667856\n",
      "Iteration 1092, loss = 0.26187405\n",
      "Iteration 716, loss = 0.32409689\n",
      "Iteration 344, loss = 0.38704530\n",
      "Iteration 1093, loss = 0.26180805\n",
      "Iteration 1004, loss = 0.30187865\n",
      "Iteration 717, loss = 0.32396568\n",
      "Iteration 1951, loss = 0.18662705\n",
      "Iteration 1094, loss = 0.26158312\n",
      "Iteration 640, loss = 0.35760420\n",
      "Iteration 1095, loss = 0.26147581\n",
      "Iteration 1096, loss = 0.26128322\n",
      "Iteration 1276, loss = 0.19993694\n",
      "Iteration 1097, loss = 0.26112808\n",
      "Iteration 281, loss = 0.39993754\n",
      "Iteration 345, loss = 0.38689245\n",
      "Iteration 1005, loss = 0.30177921\n",
      "Iteration 718, loss = 0.32385057\n",
      "Iteration 1952, loss = 0.18646045\n",
      "Iteration 1277, loss = 0.19958256\n",
      "Iteration 1098, loss = 0.26100908\n",
      "Iteration 1099, loss = 0.26092536\n",
      "Iteration 1953, loss = 0.18630241\n",
      "Iteration 641, loss = 0.35744331\n",
      "Iteration 1278, loss = 0.19946839\n",
      "Iteration 719, loss = 0.32375280\n",
      "Iteration 1100, loss = 0.26067304\n",
      "Iteration 282, loss = 0.39969526\n",
      "Iteration 346, loss = 0.38688924\n",
      "Iteration 1954, loss = 0.18619615\n",
      "Iteration 1279, loss = 0.19928215\n",
      "Iteration 1101, loss = 0.26085865\n",
      "Iteration 720, loss = 0.32367033\n",
      "Iteration 1006, loss = 0.30163075\n",
      "Iteration 283, loss = 0.39946526\n",
      "Iteration 1102, loss = 0.26034624\n",
      "Iteration 721, loss = 0.32356572\n",
      "Iteration 347, loss = 0.38658137\n",
      "Iteration 1955, loss = 0.18609197\n",
      "Iteration 642, loss = 0.35712504\n",
      "Iteration 1103, loss = 0.26030548\n",
      "Iteration 1007, loss = 0.30146295\n",
      "Iteration 1956, loss = 0.18598336\n",
      "Iteration 1104, loss = 0.26009420\n",
      "Iteration 1280, loss = 0.19913367\n",
      "Iteration 284, loss = 0.39920785\n",
      "Iteration 722, loss = 0.32346064\n",
      "Iteration 1281, loss = 0.19878750\n",
      "Iteration 1957, loss = 0.18593094\n",
      "Iteration 1008, loss = 0.30133551\n",
      "Iteration 1958, loss = 0.18576769\n",
      "Iteration 1105, loss = 0.26006115\n",
      "Iteration 723, loss = 0.32335479\n",
      "Iteration 285, loss = 0.39898583\n",
      "Iteration 348, loss = 0.38657088\n",
      "Iteration 1009, loss = 0.30118728\n",
      "Iteration 724, loss = 0.32327398\n",
      "Iteration 1282, loss = 0.19859784\n",
      "Iteration 1959, loss = 0.18564474\n",
      "Iteration 1106, loss = 0.25976870\n",
      "Iteration 1107, loss = 0.25967646\n",
      "Iteration 286, loss = 0.39879502Iteration 643, loss = 0.35698244\n",
      "\n",
      "Iteration 725, loss = 0.32313614\n",
      "Iteration 1108, loss = 0.25945723\n",
      "Iteration 1960, loss = 0.18576928\n",
      "Iteration 1283, loss = 0.19878990\n",
      "Iteration 1109, loss = 0.25930701\n",
      "Iteration 1110, loss = 0.25917865\n",
      "Iteration 349, loss = 0.38627950\n",
      "Iteration 287, loss = 0.39860686\n",
      "Iteration 1961, loss = 0.18542916\n",
      "Iteration 726, loss = 0.32318982\n",
      "Iteration 1284, loss = 0.19841823\n",
      "Iteration 1010, loss = 0.30111496\n",
      "Iteration 1111, loss = 0.25903397\n",
      "Iteration 727, loss = 0.32295853\n",
      "Iteration 1962, loss = 0.18554160\n",
      "Iteration 1112, loss = 0.25886365\n",
      "Iteration 288, loss = 0.39833161\n",
      "Iteration 1285, loss = 0.19813057\n",
      "Iteration 728, loss = 0.32282267\n",
      "Iteration 1113, loss = 0.25878781\n",
      "Iteration 289, loss = 0.39810163\n",
      "Iteration 1963, loss = 0.18524251\n",
      "Iteration 729, loss = 0.32271830\n",
      "Iteration 350, loss = 0.38613650\n",
      "Iteration 644, loss = 0.35666318\n",
      "Iteration 1011, loss = 0.30100914\n",
      "Iteration 1114, loss = 0.25860310\n",
      "Iteration 1964, loss = 0.18511933\n",
      "Iteration 730, loss = 0.32262817\n",
      "Iteration 1286, loss = 0.19787800\n",
      "Iteration 1115, loss = 0.25839707\n",
      "Iteration 1116, loss = 0.25826480\n",
      "Iteration 351, loss = 0.38598439\n",
      "Iteration 1287, loss = 0.19770837\n",
      "Iteration 1965, loss = 0.18500691\n",
      "Iteration 1117, loss = 0.25811704\n",
      "Iteration 290, loss = 0.39791000\n",
      "Iteration 731, loss = 0.32251617\n",
      "Iteration 1118, loss = 0.25793806\n",
      "Iteration 1288, loss = 0.19755835\n",
      "Iteration 1966, loss = 0.18484293\n",
      "Iteration 1119, loss = 0.25784260\n",
      "Iteration 1012, loss = 0.30082625\n",
      "Iteration 732, loss = 0.32239355\n",
      "Iteration 291, loss = 0.39769578\n",
      "Iteration 352, loss = 0.38582259\n",
      "Iteration 733, loss = 0.32230065\n",
      "Iteration 1120, loss = 0.25759295\n",
      "Iteration 1967, loss = 0.18473901\n",
      "Iteration 1013, loss = 0.30072614\n",
      "Iteration 734, loss = 0.32217913\n",
      "Iteration 1289, loss = 0.19731482\n",
      "Iteration 292, loss = 0.39749105\n",
      "Iteration 1121, loss = 0.25748247\n",
      "Iteration 645, loss = 0.35650935\n",
      "Iteration 1122, loss = 0.25736857\n",
      "Iteration 353, loss = 0.38570505\n",
      "Iteration 1968, loss = 0.18462708\n",
      "Iteration 735, loss = 0.32209841\n",
      "Iteration 293, loss = 0.39729718\n",
      "Iteration 1123, loss = 0.25717434\n",
      "Iteration 1969, loss = 0.18454315\n",
      "Iteration 1124, loss = 0.25706271\n",
      "Iteration 1290, loss = 0.19710236\n",
      "Iteration 1970, loss = 0.18444070\n",
      "Iteration 1125, loss = 0.25693587\n",
      "Iteration 736, loss = 0.32201734\n",
      "Iteration 1126, loss = 0.25680611\n",
      "Iteration 294, loss = 0.39709954\n",
      "Iteration 1014, loss = 0.30054417\n",
      "Iteration 1127, loss = 0.25657211\n",
      "Iteration 1291, loss = 0.19689113\n",
      "Iteration 1971, loss = 0.18427110\n",
      "Iteration 1128, loss = 0.25641216\n",
      "Iteration 646, loss = 0.35625704\n",
      "Iteration 1129, loss = 0.25627354\n",
      "Iteration 1130, loss = 0.25607938\n",
      "Iteration 295, loss = 0.39693524\n",
      "Iteration 1292, loss = 0.19674850\n",
      "Iteration 737, loss = 0.32191629\n",
      "Iteration 1131, loss = 0.25601091\n",
      "Iteration 1972, loss = 0.18442170\n",
      "Iteration 1132, loss = 0.25579025\n",
      "Iteration 354, loss = 0.38551649\n",
      "Iteration 1973, loss = 0.18433954\n",
      "Iteration 1133, loss = 0.25560717\n",
      "Iteration 1015, loss = 0.30042931\n",
      "Iteration 1134, loss = 0.25548619\n",
      "Iteration 1293, loss = 0.19650898\n",
      "Iteration 1974, loss = 0.18393845\n",
      "Iteration 1135, loss = 0.25531681\n",
      "Iteration 296, loss = 0.39663530\n",
      "Iteration 1136, loss = 0.25516102\n",
      "Iteration 1294, loss = 0.19630221\n",
      "Iteration 1975, loss = 0.18389436\n",
      "Iteration 647, loss = 0.35602988\n",
      "Iteration 1137, loss = 0.25500541\n",
      "Iteration 1138, loss = 0.25484176\n",
      "Iteration 738, loss = 0.32181105\n",
      "Iteration 1976, loss = 0.18370080\n",
      "Iteration 355, loss = 0.38537152\n",
      "Iteration 297, loss = 0.39647368\n",
      "Iteration 1295, loss = 0.19623220\n",
      "Iteration 739, loss = 0.32167109\n",
      "Iteration 1977, loss = 0.18377983\n",
      "Iteration 1139, loss = 0.25465841\n",
      "Iteration 1016, loss = 0.30033115\n",
      "Iteration 1140, loss = 0.25454601\n",
      "Iteration 740, loss = 0.32156710\n",
      "Iteration 298, loss = 0.39629477\n",
      "Iteration 1141, loss = 0.25435139\n",
      "Iteration 741, loss = 0.32149969\n",
      "Iteration 1978, loss = 0.18363709\n",
      "Iteration 1296, loss = 0.19612806\n",
      "Iteration 299, loss = 0.39608345\n",
      "Iteration 1142, loss = 0.25423219\n",
      "Iteration 648, loss = 0.35578107\n",
      "Iteration 1143, loss = 0.25407221\n",
      "Iteration 742, loss = 0.32134028\n",
      "Iteration 356, loss = 0.38523836Iteration 300, loss = 0.39583683\n",
      "\n",
      "Iteration 1017, loss = 0.30021288\n",
      "Iteration 1144, loss = 0.25388457\n",
      "Iteration 1297, loss = 0.19567044\n",
      "Iteration 743, loss = 0.32124338\n",
      "Iteration 1145, loss = 0.25373599\n",
      "Iteration 1979, loss = 0.18347814\n",
      "Iteration 301, loss = 0.39567938\n",
      "Iteration 744, loss = 0.32118359\n",
      "Iteration 1146, loss = 0.25362565\n",
      "Iteration 1298, loss = 0.19556522\n",
      "Iteration 1018, loss = 0.30005244\n",
      "Iteration 1980, loss = 0.18336248\n",
      "Iteration 745, loss = 0.32103968\n",
      "Iteration 1147, loss = 0.25349700\n",
      "Iteration 302, loss = 0.39546072\n",
      "Iteration 357, loss = 0.38505901\n",
      "Iteration 1299, loss = 0.19535581\n",
      "Iteration 1148, loss = 0.25339596\n",
      "Iteration 1019, loss = 0.29990965\n",
      "Iteration 1149, loss = 0.25323378\n",
      "Iteration 1150, loss = 0.25298134\n",
      "Iteration 358, loss = 0.38493231\n",
      "Iteration 746, loss = 0.32091629\n",
      "Iteration 1151, loss = 0.25278437\n",
      "Iteration 1300, loss = 0.19523780\n",
      "Iteration 1152, loss = 0.25266562\n",
      "Iteration 649, loss = 0.35555508\n",
      "Iteration 303, loss = 0.39525580\n",
      "Iteration 747, loss = 0.32082776\n",
      "Iteration 1153, loss = 0.25246300\n",
      "Iteration 1154, loss = 0.25231383\n",
      "Iteration 359, loss = 0.38474693\n",
      "Iteration 748, loss = 0.32071070\n",
      "Iteration 1301, loss = 0.19513209\n",
      "Iteration 1981, loss = 0.18320683\n",
      "Iteration 1155, loss = 0.25217453\n",
      "Iteration 1020, loss = 0.29978123\n",
      "Iteration 1156, loss = 0.25200720\n",
      "Iteration 749, loss = 0.32063124\n",
      "Iteration 304, loss = 0.39510371\n",
      "Iteration 1302, loss = 0.19471578\n",
      "Iteration 1157, loss = 0.25180657\n",
      "Iteration 360, loss = 0.38460868\n",
      "Iteration 750, loss = 0.32053975\n",
      "Iteration 1982, loss = 0.18310872\n",
      "Iteration 1158, loss = 0.25170369\n",
      "Iteration 1159, loss = 0.25150153\n",
      "Iteration 1983, loss = 0.18295669\n",
      "Iteration 1303, loss = 0.19461455\n",
      "Iteration 1160, loss = 0.25141832\n",
      "Iteration 1021, loss = 0.29968017\n",
      "Iteration 361, loss = 0.38446813\n",
      "Iteration 1161, loss = 0.25117487\n",
      "Iteration 751, loss = 0.32043502\n",
      "Iteration 305, loss = 0.39483811\n",
      "Iteration 1984, loss = 0.18293784\n",
      "Iteration 1162, loss = 0.25111980\n",
      "Iteration 650, loss = 0.35541749\n",
      "Iteration 752, loss = 0.32030044\n",
      "Iteration 1985, loss = 0.18284416\n",
      "Iteration 1163, loss = 0.25088632\n",
      "Iteration 1164, loss = 0.25074379\n",
      "Iteration 1304, loss = 0.19450833\n",
      "Iteration 753, loss = 0.32020627\n",
      "Iteration 1165, loss = 0.25061733\n",
      "Iteration 1022, loss = 0.29952595\n",
      "Iteration 306, loss = 0.39469787\n",
      "Iteration 1166, loss = 0.25041739\n",
      "Iteration 754, loss = 0.32007934\n",
      "Iteration 362, loss = 0.38431207\n",
      "Iteration 1167, loss = 0.25026748\n",
      "Iteration 1986, loss = 0.18268563\n",
      "Iteration 1168, loss = 0.25010836\n",
      "Iteration 1169, loss = 0.24997346\n",
      "Iteration 363, loss = 0.38416031\n",
      "Iteration 307, loss = 0.39439478\n",
      "Iteration 755, loss = 0.31996805\n",
      "Iteration 1170, loss = 0.24976799\n",
      "Iteration 1171, loss = 0.24965881\n",
      "Iteration 651, loss = 0.35522567\n",
      "Iteration 1023, loss = 0.29942967\n",
      "Iteration 1172, loss = 0.24952336\n",
      "Iteration 1305, loss = 0.19431304\n",
      "Iteration 756, loss = 0.31987979\n",
      "Iteration 1173, loss = 0.24933466\n",
      "Iteration 308, loss = 0.39420983\n",
      "Iteration 1987, loss = 0.18254690\n",
      "Iteration 1174, loss = 0.24916897\n",
      "Iteration 757, loss = 0.31980694\n",
      "Iteration 1175, loss = 0.24895315\n",
      "Iteration 364, loss = 0.38404727\n",
      "Iteration 1024, loss = 0.29928655\n",
      "Iteration 758, loss = 0.31973743\n",
      "Iteration 1176, loss = 0.24879313\n",
      "Iteration 309, loss = 0.39406755\n",
      "Iteration 652, loss = 0.35489374\n",
      "Iteration 1177, loss = 0.24867214\n",
      "Iteration 1988, loss = 0.18240200\n",
      "Iteration 1306, loss = 0.19395352\n",
      "Iteration 759, loss = 0.31961667\n",
      "Iteration 1178, loss = 0.24859831\n",
      "Iteration 1179, loss = 0.24834415\n",
      "Iteration 760, loss = 0.31947533\n",
      "Iteration 1180, loss = 0.24819325\n",
      "Iteration 1307, loss = 0.19386005\n",
      "Iteration 761, loss = 0.31935079\n",
      "Iteration 1181, loss = 0.24804975\n",
      "Iteration 1989, loss = 0.18234077\n",
      "Iteration 653, loss = 0.35465321\n",
      "Iteration 365, loss = 0.38385284\n",
      "Iteration 1182, loss = 0.24784176\n",
      "Iteration 310, loss = 0.39388401\n",
      "Iteration 1990, loss = 0.18223490\n",
      "Iteration 1183, loss = 0.24770464\n",
      "Iteration 762, loss = 0.31924579\n",
      "Iteration 1184, loss = 0.24756819\n",
      "Iteration 1308, loss = 0.19374967\n",
      "Iteration 1991, loss = 0.18221017\n",
      "Iteration 311, loss = 0.39371326\n",
      "Iteration 1185, loss = 0.24735355\n",
      "Iteration 1025, loss = 0.29920398\n",
      "Iteration 1309, loss = 0.19351479\n",
      "Iteration 1186, loss = 0.24721527\n",
      "Iteration 763, loss = 0.31911231\n",
      "Iteration 1992, loss = 0.18203614\n",
      "Iteration 312, loss = 0.39347510\n",
      "Iteration 1187, loss = 0.24707354\n",
      "Iteration 1188, loss = 0.24700112\n",
      "Iteration 764, loss = 0.31902720\n",
      "Iteration 366, loss = 0.38373152\n",
      "Iteration 1993, loss = 0.18192182\n",
      "Iteration 1189, loss = 0.24675261\n",
      "Iteration 1310, loss = 0.19326426\n",
      "Iteration 765, loss = 0.31891685\n",
      "Iteration 654, loss = 0.35450683\n",
      "Iteration 1994, loss = 0.18176615Iteration 766, loss = 0.31881207\n",
      "Iteration 1026, loss = 0.29906396\n",
      "Iteration 313, loss = 0.39324335\n",
      "\n",
      "Iteration 1190, loss = 0.24666260\n",
      "Iteration 1311, loss = 0.19352832\n",
      "Iteration 767, loss = 0.31872527\n",
      "Iteration 314, loss = 0.39308314\n",
      "Iteration 655, loss = 0.35424983\n",
      "Iteration 1191, loss = 0.24648356\n",
      "Iteration 367, loss = 0.38357808\n",
      "Iteration 1995, loss = 0.18164555\n",
      "Iteration 1192, loss = 0.24622515\n",
      "Iteration 768, loss = 0.31861293\n",
      "Iteration 1312, loss = 0.19287449\n",
      "Iteration 1193, loss = 0.24605347\n",
      "Iteration 1027, loss = 0.29888687\n",
      "Iteration 1996, loss = 0.18153052\n",
      "Iteration 769, loss = 0.31854189\n",
      "Iteration 1194, loss = 0.24592994\n",
      "Iteration 1313, loss = 0.19266672\n",
      "Iteration 315, loss = 0.39290127\n",
      "Iteration 770, loss = 0.31839189\n",
      "Iteration 1314, loss = 0.19252686\n",
      "Iteration 1028, loss = 0.29880288\n",
      "Iteration 1195, loss = 0.24579953\n",
      "Iteration 771, loss = 0.31828624\n",
      "Iteration 316, loss = 0.39268594\n",
      "Iteration 368, loss = 0.38342855\n",
      "Iteration 656, loss = 0.35397221\n",
      "Iteration 1196, loss = 0.24556265\n",
      "Iteration 1997, loss = 0.18151880\n",
      "Iteration 772, loss = 0.31818879\n",
      "Iteration 1197, loss = 0.24548769\n",
      "Iteration 1315, loss = 0.19229281\n",
      "Iteration 773, loss = 0.31807557\n",
      "Iteration 1998, loss = 0.18136986Iteration 774, loss = 0.31797788\n",
      "\n",
      "Iteration 1316, loss = 0.19223283\n",
      "Iteration 657, loss = 0.35383360\n",
      "Iteration 317, loss = 0.39248604\n",
      "Iteration 1198, loss = 0.24523952\n",
      "Iteration 369, loss = 0.38327832\n",
      "Iteration 775, loss = 0.31789298\n",
      "Iteration 1029, loss = 0.29864398\n",
      "Iteration 1999, loss = 0.18123840\n",
      "Iteration 776, loss = 0.31775621\n",
      "Iteration 1199, loss = 0.24509679\n",
      "Iteration 370, loss = 0.38313796\n",
      "Iteration 777, loss = 0.31764171\n",
      "Iteration 658, loss = 0.35357089\n",
      "Iteration 2000, loss = 0.18114645\n",
      "Iteration 318, loss = 0.39229954\n",
      "Iteration 1200, loss = 0.24494629\n",
      "Iteration 1030, loss = 0.29853370\n",
      "Iteration 1317, loss = 0.19221675\n",
      "Iteration 1201, loss = 0.24484568\n",
      "Iteration 778, loss = 0.31757703\n",
      "Iteration 1202, loss = 0.24480373\n",
      "Iteration 1318, loss = 0.19168255\n",
      "Iteration 1203, loss = 0.24443583\n",
      "Iteration 371, loss = 0.38298276\n",
      "Iteration 779, loss = 0.31749069\n",
      "Iteration 2001, loss = 0.18098183\n",
      "Iteration 1204, loss = 0.24431130\n",
      "Iteration 1205, loss = 0.24429284\n",
      "Iteration 1319, loss = 0.19149225\n",
      "Iteration 780, loss = 0.31735064\n",
      "Iteration 319, loss = 0.39214555\n",
      "Iteration 1206, loss = 0.24412014\n",
      "Iteration 2002, loss = 0.18094888\n",
      "Iteration 1031, loss = 0.29840816\n",
      "Iteration 1207, loss = 0.24383343\n",
      "Iteration 372, loss = 0.38286730\n",
      "Iteration 320, loss = 0.39202578\n",
      "Iteration 781, loss = 0.31723120\n",
      "Iteration 659, loss = 0.35332230\n",
      "Iteration 1208, loss = 0.24361285\n",
      "Iteration 2003, loss = 0.18079481\n",
      "Iteration 1320, loss = 0.19132647\n",
      "Iteration 1032, loss = 0.29826619\n",
      "Iteration 1209, loss = 0.24349011\n",
      "Iteration 782, loss = 0.31712446\n",
      "Iteration 2004, loss = 0.18076499\n",
      "Iteration 321, loss = 0.39172233\n",
      "Iteration 373, loss = 0.38269827\n",
      "Iteration 1210, loss = 0.24330445\n",
      "Iteration 2005, loss = 0.18064975\n",
      "Iteration 783, loss = 0.31700646\n",
      "Iteration 322, loss = 0.39154794\n",
      "Iteration 1211, loss = 0.24311009\n",
      "Iteration 784, loss = 0.31692690\n",
      "Iteration 2006, loss = 0.18047601\n",
      "Iteration 1321, loss = 0.19114412\n",
      "Iteration 323, loss = 0.39136168\n",
      "Iteration 1212, loss = 0.24299938\n",
      "Iteration 1033, loss = 0.29810883\n",
      "Iteration 2007, loss = 0.18039353\n",
      "Iteration 1213, loss = 0.24296501\n",
      "Iteration 660, loss = 0.35306752\n",
      "Iteration 785, loss = 0.31684272\n",
      "Iteration 1214, loss = 0.24261343\n",
      "Iteration 2008, loss = 0.18028509\n",
      "Iteration 324, loss = 0.39117261\n",
      "Iteration 1215, loss = 0.24244323\n",
      "Iteration 1322, loss = 0.19101200\n",
      "Iteration 1216, loss = 0.24230091\n",
      "Iteration 374, loss = 0.38257956\n",
      "Iteration 2009, loss = 0.18014717\n",
      "Iteration 1217, loss = 0.24222233\n",
      "Iteration 786, loss = 0.31668922\n",
      "Iteration 1034, loss = 0.29802415\n",
      "Iteration 2010, loss = 0.18038064\n",
      "Iteration 1323, loss = 0.19084798\n",
      "Iteration 661, loss = 0.35308753\n",
      "Iteration 325, loss = 0.39099698\n",
      "Iteration 2011, loss = 0.18002647\n",
      "Iteration 1218, loss = 0.24205684\n",
      "Iteration 1324, loss = 0.19056994\n",
      "Iteration 1035, loss = 0.29787715\n",
      "Iteration 326, loss = 0.39083780\n",
      "Iteration 1219, loss = 0.24187601\n",
      "Iteration 2012, loss = 0.17992523\n",
      "Iteration 1220, loss = 0.24164345\n",
      "Iteration 662, loss = 0.35264736\n",
      "Iteration 787, loss = 0.31657626\n",
      "Iteration 1221, loss = 0.24150232\n",
      "Iteration 327, loss = 0.39066257\n",
      "Iteration 1325, loss = 0.19035767\n",
      "Iteration 1222, loss = 0.24135982\n",
      "Iteration 1223, loss = 0.24118245\n",
      "Iteration 2013, loss = 0.17973474\n",
      "Iteration 328, loss = 0.39042502\n",
      "Iteration 1326, loss = 0.19018220\n",
      "Iteration 1224, loss = 0.24109746\n",
      "Iteration 788, loss = 0.31648749\n",
      "Iteration 663, loss = 0.35242174\n",
      "Iteration 375, loss = 0.38241711\n",
      "Iteration 329, loss = 0.39027408\n",
      "Iteration 2014, loss = 0.17963609\n",
      "Iteration 1225, loss = 0.24086224\n",
      "Iteration 1327, loss = 0.18998225\n",
      "Iteration 1036, loss = 0.29774844\n",
      "Iteration 789, loss = 0.31635930\n",
      "Iteration 1226, loss = 0.24077429\n",
      "Iteration 330, loss = 0.39004913\n",
      "Iteration 2015, loss = 0.17957296\n",
      "Iteration 1227, loss = 0.24057753\n",
      "Iteration 664, loss = 0.35220817\n",
      "Iteration 2016, loss = 0.17937810\n",
      "Iteration 790, loss = 0.31629237\n",
      "Iteration 1037, loss = 0.29759418\n",
      "Iteration 1228, loss = 0.24038267\n",
      "Iteration 1328, loss = 0.18985149\n",
      "Iteration 2017, loss = 0.17933424\n",
      "Iteration 791, loss = 0.31614581\n",
      "Iteration 331, loss = 0.38989266\n",
      "Iteration 1229, loss = 0.24026360\n",
      "Iteration 1329, loss = 0.18962983\n",
      "Iteration 1230, loss = 0.24002942\n",
      "Iteration 2018, loss = 0.17920947\n",
      "Iteration 1231, loss = 0.23995534\n",
      "Iteration 332, loss = 0.38970993\n",
      "Iteration 792, loss = 0.31604976\n",
      "Iteration 1038, loss = 0.29748815\n",
      "Iteration 1232, loss = 0.23974763\n",
      "Iteration 1330, loss = 0.18940735\n",
      "Iteration 2019, loss = 0.17907174\n",
      "Iteration 665, loss = 0.35197125\n",
      "Iteration 333, loss = 0.38953362\n",
      "Iteration 1233, loss = 0.23956334\n",
      "Iteration 2020, loss = 0.17895081\n",
      "Iteration 1331, loss = 0.18925542\n",
      "Iteration 793, loss = 0.31594084\n",
      "Iteration 2021, loss = 0.17886512\n",
      "Iteration 334, loss = 0.38935143\n",
      "Iteration 1234, loss = 0.23944040\n",
      "Iteration 1039, loss = 0.29738314\n",
      "Iteration 794, loss = 0.31583293\n",
      "Iteration 2022, loss = 0.17875839\n",
      "Iteration 1235, loss = 0.23933679\n",
      "Iteration 1332, loss = 0.18902217\n",
      "Iteration 1236, loss = 0.23909229\n",
      "Iteration 795, loss = 0.31575170\n",
      "Iteration 335, loss = 0.38912040\n",
      "Iteration 1237, loss = 0.23910053\n",
      "Iteration 1040, loss = 0.29721617\n",
      "Iteration 1333, loss = 0.18897132\n",
      "Iteration 1238, loss = 0.23879018\n",
      "Iteration 1239, loss = 0.23869576\n",
      "Iteration 2023, loss = 0.17880887\n",
      "Iteration 666, loss = 0.35178115\n",
      "Iteration 1240, loss = 0.23848400\n",
      "Iteration 336, loss = 0.38908565\n",
      "Iteration 1334, loss = 0.18859373\n",
      "Iteration 796, loss = 0.31562947\n",
      "Iteration 1241, loss = 0.23832147\n",
      "Iteration 2024, loss = 0.17869160\n",
      "Iteration 1242, loss = 0.23816374\n",
      "Iteration 1335, loss = 0.18874828\n",
      "Iteration 2025, loss = 0.17862359\n",
      "Iteration 1243, loss = 0.23793635\n",
      "Iteration 337, loss = 0.38876299\n",
      "Iteration 797, loss = 0.31549869\n",
      "Iteration 2026, loss = 0.17852591\n",
      "Iteration 1041, loss = 0.29709803\n",
      "Iteration 1336, loss = 0.18828703\n",
      "Iteration 1244, loss = 0.23777208\n",
      "Iteration 2027, loss = 0.17827093\n",
      "Iteration 1245, loss = 0.23769902\n",
      "Iteration 667, loss = 0.35154496\n",
      "Iteration 798, loss = 0.31541541\n",
      "Iteration 376, loss = 0.38228806\n",
      "Iteration 338, loss = 0.38861776\n",
      "Iteration 1246, loss = 0.23744729\n",
      "Iteration 1337, loss = 0.18825974\n",
      "Iteration 1247, loss = 0.23731015\n",
      "Iteration 799, loss = 0.31533004\n",
      "Iteration 1248, loss = 0.23720888\n",
      "Iteration 339, loss = 0.38846251\n",
      "Iteration 2028, loss = 0.17813425\n",
      "Iteration 800, loss = 0.31519519\n",
      "Iteration 1338, loss = 0.18794177\n",
      "Iteration 1249, loss = 0.23698537\n",
      "Iteration 1042, loss = 0.29695674\n",
      "Iteration 1250, loss = 0.23684776\n",
      "Iteration 340, loss = 0.38824871\n",
      "Iteration 1251, loss = 0.23680103\n",
      "Iteration 2029, loss = 0.17805393\n",
      "Iteration 801, loss = 0.31506934\n",
      "Iteration 668, loss = 0.35134911\n",
      "Iteration 1252, loss = 0.23650683\n",
      "Iteration 1043, loss = 0.29682624\n",
      "Iteration 1253, loss = 0.23646255\n",
      "Iteration 802, loss = 0.31496513\n",
      "Iteration 341, loss = 0.38816465\n",
      "Iteration 1254, loss = 0.23621223\n",
      "Iteration 1339, loss = 0.18800788\n",
      "Iteration 2030, loss = 0.17798486\n",
      "Iteration 803, loss = 0.31489379\n",
      "Iteration 1255, loss = 0.23600276\n",
      "Iteration 1044, loss = 0.29670040\n",
      "Iteration 669, loss = 0.35106665\n",
      "Iteration 342, loss = 0.38788031\n",
      "Iteration 804, loss = 0.31478619\n",
      "Iteration 1340, loss = 0.18773482\n",
      "Iteration 1256, loss = 0.23589693\n",
      "Iteration 2031, loss = 0.17780021\n",
      "Iteration 1257, loss = 0.23574334\n",
      "Iteration 805, loss = 0.31466105\n",
      "Iteration 1258, loss = 0.23553934\n",
      "Iteration 2032, loss = 0.17775545\n",
      "Iteration 1045, loss = 0.29658428\n",
      "Iteration 806, loss = 0.31455309\n",
      "Iteration 1259, loss = 0.23539848\n",
      "Iteration 1341, loss = 0.18754575\n",
      "Iteration 1260, loss = 0.23526565\n",
      "Iteration 807, loss = 0.31444719\n",
      "Iteration 343, loss = 0.38775028\n",
      "Iteration 1261, loss = 0.23511541\n",
      "Iteration 2033, loss = 0.17758150\n",
      "Iteration 1046, loss = 0.29646570\n",
      "Iteration 670, loss = 0.35099779\n",
      "Iteration 1262, loss = 0.23497012\n",
      "Iteration 808, loss = 0.31432286\n",
      "Iteration 1263, loss = 0.23474399\n",
      "Iteration 2034, loss = 0.17759450\n",
      "Iteration 344, loss = 0.38748593\n",
      "Iteration 809, loss = 0.31424075\n",
      "Iteration 1264, loss = 0.23464497\n",
      "Iteration 1342, loss = 0.18745386\n",
      "Iteration 2035, loss = 0.17735952\n",
      "Iteration 1265, loss = 0.23444165\n",
      "Iteration 1047, loss = 0.29630529\n",
      "Iteration 345, loss = 0.38732115\n",
      "Iteration 810, loss = 0.31414849\n",
      "Iteration 1266, loss = 0.23426560\n",
      "Iteration 1343, loss = 0.18696874\n",
      "Iteration 2036, loss = 0.17729830\n",
      "Iteration 1048, loss = 0.29618592\n",
      "Iteration 811, loss = 0.31400640\n",
      "Iteration 346, loss = 0.38721168\n",
      "Iteration 671, loss = 0.35068883\n",
      "Iteration 1267, loss = 0.23409133\n",
      "Iteration 812, loss = 0.31390772\n",
      "Iteration 2037, loss = 0.17723167\n",
      "Iteration 347, loss = 0.38696216\n",
      "Iteration 1268, loss = 0.23392324\n",
      "Iteration 813, loss = 0.31380444\n",
      "Iteration 672, loss = 0.35041490\n",
      "Iteration 2038, loss = 0.17711883\n",
      "Iteration 1049, loss = 0.29606794\n",
      "Iteration 348, loss = 0.38678852\n",
      "Iteration 1269, loss = 0.23378477\n",
      "Iteration 1344, loss = 0.18688156\n",
      "Iteration 2039, loss = 0.17700429\n",
      "Iteration 1270, loss = 0.23367844\n",
      "Iteration 377, loss = 0.38213386\n",
      "Iteration 1271, loss = 0.23342086\n",
      "Iteration 1345, loss = 0.18653486\n",
      "Iteration 2040, loss = 0.17683404\n",
      "Iteration 349, loss = 0.38665072\n",
      "Iteration 1272, loss = 0.23325018\n",
      "Iteration 378, loss = 0.38198718\n",
      "Iteration 1050, loss = 0.29600113\n",
      "Iteration 673, loss = 0.35022492\n",
      "Iteration 814, loss = 0.31367596\n",
      "Iteration 1273, loss = 0.23325101\n",
      "Iteration 2041, loss = 0.17678802\n",
      "Iteration 1274, loss = 0.23298348\n",
      "Iteration 1275, loss = 0.23279748\n",
      "Iteration 379, loss = 0.38188647\n",
      "Iteration 1346, loss = 0.18648989\n",
      "Iteration 1276, loss = 0.23263920\n",
      "Iteration 2042, loss = 0.17664077\n",
      "Iteration 1051, loss = 0.29582485\n",
      "Iteration 1277, loss = 0.23259678\n",
      "Iteration 350, loss = 0.38646580\n",
      "Iteration 1278, loss = 0.23230707\n",
      "Iteration 815, loss = 0.31359033\n",
      "Iteration 674, loss = 0.34994518\n",
      "Iteration 2043, loss = 0.17655912\n",
      "Iteration 1279, loss = 0.23224933\n",
      "Iteration 816, loss = 0.31347282\n",
      "Iteration 380, loss = 0.38171735\n",
      "Iteration 817, loss = 0.31338444\n",
      "Iteration 1052, loss = 0.29570450\n",
      "Iteration 1280, loss = 0.23198841\n",
      "Iteration 2044, loss = 0.17667125Iteration 1281, loss = 0.23177866\n",
      "Iteration 1347, loss = 0.18625219\n",
      "Iteration 351, loss = 0.38630566\n",
      "Iteration 818, loss = 0.31326033\n",
      "\n",
      "Iteration 1282, loss = 0.23175109\n",
      "Iteration 819, loss = 0.31316632\n",
      "Iteration 1348, loss = 0.18619090\n",
      "Iteration 820, loss = 0.31306944\n",
      "Iteration 381, loss = 0.38165916\n",
      "Iteration 675, loss = 0.34975476\n",
      "Iteration 1283, loss = 0.23147195\n",
      "Iteration 1284, loss = 0.23130745\n",
      "Iteration 1285, loss = 0.23114941\n",
      "Iteration 1053, loss = 0.29556239\n",
      "Iteration 1349, loss = 0.18587097\n",
      "Iteration 1286, loss = 0.23098681\n",
      "Iteration 2045, loss = 0.17641234\n",
      "Iteration 1287, loss = 0.23080064\n",
      "Iteration 352, loss = 0.38609821\n",
      "Iteration 1350, loss = 0.18563808\n",
      "Iteration 2046, loss = 0.17622560\n",
      "Iteration 821, loss = 0.31293562\n",
      "Iteration 1288, loss = 0.23066340\n",
      "Iteration 1054, loss = 0.29544181\n",
      "Iteration 382, loss = 0.38142159\n",
      "Iteration 822, loss = 0.31289931\n",
      "Iteration 1351, loss = 0.18556017\n",
      "Iteration 1289, loss = 0.23063748\n",
      "Iteration 353, loss = 0.38592399\n",
      "Iteration 676, loss = 0.34953615\n",
      "Iteration 383, loss = 0.38130701\n",
      "Iteration 2047, loss = 0.17614952\n",
      "Iteration 1290, loss = 0.23039521\n",
      "Iteration 823, loss = 0.31273501\n",
      "Iteration 1291, loss = 0.23014123\n",
      "Iteration 354, loss = 0.38572784\n",
      "Iteration 824, loss = 0.31261419\n",
      "Iteration 1055, loss = 0.29531130\n",
      "Iteration 1352, loss = 0.18528923\n",
      "Iteration 1292, loss = 0.23001670\n",
      "Iteration 355, loss = 0.38562285\n",
      "Iteration 384, loss = 0.38115626\n",
      "Iteration 825, loss = 0.31256300\n",
      "Iteration 1293, loss = 0.22982774Iteration 2048, loss = 0.17606466\n",
      "Iteration 356, loss = 0.38540841\n",
      "\n",
      "Iteration 826, loss = 0.31240268\n",
      "Iteration 1294, loss = 0.22973298\n",
      "Iteration 1056, loss = 0.29529365\n",
      "Iteration 357, loss = 0.38523721\n",
      "Iteration 2049, loss = 0.17592263\n",
      "Iteration 677, loss = 0.34924937\n",
      "Iteration 1295, loss = 0.22954064\n",
      "Iteration 1353, loss = 0.18514686\n",
      "Iteration 2050, loss = 0.17576406\n",
      "Iteration 827, loss = 0.31230923\n",
      "Iteration 1057, loss = 0.29502204\n",
      "Iteration 828, loss = 0.31223678\n",
      "Iteration 1296, loss = 0.22934645\n",
      "Iteration 2051, loss = 0.17577689\n",
      "Iteration 1297, loss = 0.22918388\n",
      "Iteration 358, loss = 0.38507580\n",
      "Iteration 678, loss = 0.34908052\n",
      "Iteration 1298, loss = 0.22913994\n",
      "Iteration 2052, loss = 0.17556916\n",
      "Iteration 829, loss = 0.31208477\n",
      "Iteration 385, loss = 0.38100716\n",
      "Iteration 2053, loss = 0.17561293\n",
      "Iteration 1299, loss = 0.22884303\n",
      "Iteration 1058, loss = 0.29486471\n",
      "Iteration 1300, loss = 0.22869703\n",
      "Iteration 359, loss = 0.38487191\n",
      "Iteration 1301, loss = 0.22852556\n",
      "Iteration 1059, loss = 0.29478334\n",
      "Iteration 1354, loss = 0.18517101\n",
      "Iteration 1302, loss = 0.22848908\n",
      "Iteration 830, loss = 0.31197682\n",
      "Iteration 1303, loss = 0.22829152\n",
      "Iteration 360, loss = 0.38471487\n",
      "Iteration 1304, loss = 0.22802660\n",
      "Iteration 386, loss = 0.38087701\n",
      "Iteration 2054, loss = 0.17540341\n",
      "Iteration 831, loss = 0.31193065\n",
      "Iteration 1305, loss = 0.22793333\n",
      "Iteration 1060, loss = 0.29462791\n",
      "Iteration 1306, loss = 0.22770203\n",
      "Iteration 361, loss = 0.38455213\n",
      "Iteration 1307, loss = 0.22768261\n",
      "Iteration 679, loss = 0.34881889\n",
      "Iteration 2055, loss = 0.17528926\n",
      "Iteration 1308, loss = 0.22765264\n",
      "Iteration 387, loss = 0.38073781\n",
      "Iteration 832, loss = 0.31178426\n",
      "Iteration 1309, loss = 0.22725413\n",
      "Iteration 2056, loss = 0.17519278\n",
      "Iteration 1310, loss = 0.22734666\n",
      "Iteration 362, loss = 0.38433737\n",
      "Iteration 833, loss = 0.31164871\n",
      "Iteration 1311, loss = 0.22694428\n",
      "Iteration 388, loss = 0.38057586\n",
      "Iteration 1312, loss = 0.22690126\n",
      "Iteration 1061, loss = 0.29450838\n",
      "Iteration 363, loss = 0.38419204\n",
      "Iteration 1313, loss = 0.22678056\n",
      "Iteration 834, loss = 0.31156052\n",
      "Iteration 680, loss = 0.34858361\n",
      "Iteration 2057, loss = 0.17509113\n",
      "Iteration 1314, loss = 0.22646988\n",
      "Iteration 835, loss = 0.31144818\n",
      "Iteration 1062, loss = 0.29438974\n",
      "Iteration 1315, loss = 0.22632961\n",
      "Iteration 364, loss = 0.38410653\n",
      "Iteration 389, loss = 0.38046655\n",
      "Iteration 836, loss = 0.31135471\n",
      "Iteration 2058, loss = 0.17494855\n",
      "Iteration 1316, loss = 0.22615527\n",
      "Iteration 681, loss = 0.34837636\n",
      "Iteration 365, loss = 0.38384804\n",
      "Iteration 837, loss = 0.31122445\n",
      "Iteration 1317, loss = 0.22598491\n",
      "Iteration 2059, loss = 0.17488039\n",
      "Iteration 1318, loss = 0.22579490\n",
      "Iteration 1319, loss = 0.22565057\n",
      "Iteration 366, loss = 0.38365538\n",
      "Iteration 2060, loss = 0.17483117\n",
      "Iteration 1063, loss = 0.29437576\n",
      "Iteration 2061, loss = 0.17467861\n",
      "Iteration 682, loss = 0.34817024\n",
      "Iteration 1320, loss = 0.22549550\n",
      "Iteration 838, loss = 0.31111398\n",
      "Iteration 390, loss = 0.38033294\n",
      "Iteration 1321, loss = 0.22555608\n",
      "Iteration 839, loss = 0.31104214\n",
      "Iteration 2062, loss = 0.17457127\n",
      "Iteration 1322, loss = 0.22515661\n",
      "Iteration 367, loss = 0.38349972\n",
      "Iteration 2063, loss = 0.17451217\n",
      "Iteration 1323, loss = 0.22501247\n",
      "Iteration 1064, loss = 0.29421810\n",
      "Iteration 1324, loss = 0.22500863\n",
      "Iteration 2064, loss = 0.17442717\n",
      "Iteration 1325, loss = 0.22465959\n",
      "Iteration 391, loss = 0.38018673\n",
      "Iteration 840, loss = 0.31090625\n",
      "Iteration 1326, loss = 0.22450972\n",
      "Iteration 2065, loss = 0.17427768\n",
      "Iteration 841, loss = 0.31082093\n",
      "Iteration 683, loss = 0.34802414\n",
      "Iteration 368, loss = 0.38332894\n",
      "Iteration 842, loss = 0.31071018\n",
      "Iteration 392, loss = 0.38010472\n",
      "Iteration 843, loss = 0.31056805\n",
      "Iteration 369, loss = 0.38319484\n",
      "Iteration 1065, loss = 0.29399955\n",
      "Iteration 2066, loss = 0.17418702\n",
      "Iteration 1327, loss = 0.22437380\n",
      "Iteration 844, loss = 0.31047435\n",
      "Iteration 393, loss = 0.37993991\n",
      "Iteration 1328, loss = 0.22416596\n",
      "Iteration 2067, loss = 0.17404216\n",
      "Iteration 370, loss = 0.38306765\n",
      "Iteration 845, loss = 0.31041436\n",
      "Iteration 684, loss = 0.34774775\n",
      "Iteration 846, loss = 0.31026645\n",
      "Iteration 371, loss = 0.38280185\n",
      "Iteration 1329, loss = 0.22398507\n",
      "Iteration 2068, loss = 0.17397185\n",
      "Iteration 394, loss = 0.37978709\n",
      "Iteration 847, loss = 0.31017698\n",
      "Iteration 1066, loss = 0.29389689\n",
      "Iteration 372, loss = 0.38267237\n",
      "Iteration 2069, loss = 0.17383795\n",
      "Iteration 395, loss = 0.37963522\n",
      "Iteration 848, loss = 0.31002760\n",
      "Iteration 685, loss = 0.34745923\n",
      "Iteration 1330, loss = 0.22381850\n",
      "Iteration 849, loss = 0.30992749\n",
      "Iteration 2070, loss = 0.17376872\n",
      "Iteration 1067, loss = 0.29376041\n",
      "Iteration 1331, loss = 0.22365261\n",
      "Iteration 373, loss = 0.38253381\n",
      "Iteration 2071, loss = 0.17363898\n",
      "Iteration 1332, loss = 0.22355382\n",
      "Iteration 850, loss = 0.30983596\n",
      "Iteration 374, loss = 0.38231564\n",
      "Iteration 1333, loss = 0.22334657\n",
      "Iteration 375, loss = 0.38216694\n",
      "Iteration 396, loss = 0.37948793\n",
      "Iteration 851, loss = 0.30970665\n",
      "Iteration 1068, loss = 0.29359735\n",
      "Iteration 2072, loss = 0.17352641\n",
      "Iteration 1334, loss = 0.22320961\n",
      "Iteration 686, loss = 0.34722718\n",
      "Iteration 1335, loss = 0.22311929\n",
      "Iteration 376, loss = 0.38200661\n",
      "Iteration 1355, loss = 0.18477211\n",
      "Iteration 1336, loss = 0.22286808\n",
      "Iteration 377, loss = 0.38190275\n",
      "Iteration 852, loss = 0.30961628\n",
      "Iteration 2073, loss = 0.17342443\n",
      "Iteration 397, loss = 0.37935245\n",
      "Iteration 687, loss = 0.34709808\n",
      "Iteration 1356, loss = 0.18453508\n",
      "Iteration 853, loss = 0.30947582\n",
      "Iteration 378, loss = 0.38175590\n",
      "Iteration 1337, loss = 0.22275827\n",
      "Iteration 2074, loss = 0.17330700\n",
      "Iteration 854, loss = 0.30937449\n",
      "Iteration 398, loss = 0.37923241\n",
      "Iteration 1338, loss = 0.22260606\n",
      "Iteration 1339, loss = 0.22235209\n",
      "Iteration 1069, loss = 0.29354732\n",
      "Iteration 1357, loss = 0.18433987\n",
      "Iteration 855, loss = 0.30927278\n",
      "Iteration 1340, loss = 0.22216257\n",
      "Iteration 379, loss = 0.38150910\n",
      "Iteration 1070, loss = 0.29336981\n",
      "Iteration 856, loss = 0.30919541\n",
      "Iteration 1341, loss = 0.22208723\n",
      "Iteration 399, loss = 0.37907305\n",
      "Iteration 2075, loss = 0.17321724\n",
      "Iteration 380, loss = 0.38131148\n",
      "Iteration 857, loss = 0.30904787\n",
      "Iteration 1342, loss = 0.22186908\n",
      "Iteration 688, loss = 0.34676837\n",
      "Iteration 1071, loss = 0.29323593\n",
      "Iteration 1343, loss = 0.22166923\n",
      "Iteration 2076, loss = 0.17310905\n",
      "Iteration 400, loss = 0.37899575\n",
      "Iteration 858, loss = 0.30894477\n",
      "Iteration 1344, loss = 0.22153062\n",
      "Iteration 2077, loss = 0.17299292\n",
      "Iteration 381, loss = 0.38119788\n",
      "Iteration 859, loss = 0.30882659\n",
      "Iteration 2078, loss = 0.17295357\n",
      "Iteration 1358, loss = 0.18432142\n",
      "Iteration 1072, loss = 0.29307730\n",
      "Iteration 860, loss = 0.30873515\n",
      "Iteration 1345, loss = 0.22158763\n",
      "Iteration 861, loss = 0.30862118\n",
      "Iteration 1346, loss = 0.22121084\n",
      "Iteration 2079, loss = 0.17285416\n",
      "Iteration 1359, loss = 0.18406710\n",
      "Iteration 1347, loss = 0.22104206\n",
      "Iteration 689, loss = 0.34665967Iteration 1073, loss = 0.29297930\n",
      "\n",
      "Iteration 382, loss = 0.38097169\n",
      "Iteration 401, loss = 0.37882036\n",
      "Iteration 1360, loss = 0.18390154\n",
      "Iteration 2080, loss = 0.17273255\n",
      "Iteration 862, loss = 0.30851163\n",
      "Iteration 1348, loss = 0.22086146\n",
      "Iteration 383, loss = 0.38084373\n",
      "Iteration 1349, loss = 0.22066791\n",
      "Iteration 1361, loss = 0.18379629\n",
      "Iteration 863, loss = 0.30840403Iteration 1350, loss = 0.22052152\n",
      "Iteration 1074, loss = 0.29283180\n",
      "\n",
      "Iteration 2081, loss = 0.17260865\n",
      "Iteration 1351, loss = 0.22040054\n",
      "Iteration 2082, loss = 0.17254355\n",
      "Iteration 1352, loss = 0.22020492\n",
      "Iteration 384, loss = 0.38070365\n",
      "Iteration 2083, loss = 0.17237608\n",
      "Iteration 864, loss = 0.30831210\n",
      "Iteration 402, loss = 0.37867045\n",
      "Iteration 1362, loss = 0.18338664\n",
      "Iteration 385, loss = 0.38050892\n",
      "Iteration 2084, loss = 0.17231453\n",
      "Iteration 1353, loss = 0.21999565\n",
      "Iteration 690, loss = 0.34638143\n",
      "Iteration 2085, loss = 0.17221143\n",
      "Iteration 403, loss = 0.37854429\n",
      "Iteration 865, loss = 0.30818190\n",
      "Iteration 1354, loss = 0.21982279Iteration 2086, loss = 0.17210736\n",
      "\n",
      "Iteration 866, loss = 0.30809905\n",
      "Iteration 1075, loss = 0.29273938\n",
      "Iteration 404, loss = 0.37841163\n",
      "Iteration 867, loss = 0.30801104\n",
      "Iteration 1363, loss = 0.18326519\n",
      "Iteration 386, loss = 0.38036751\n",
      "Iteration 2087, loss = 0.17196398\n",
      "Iteration 1355, loss = 0.21965154\n",
      "Iteration 868, loss = 0.30785206\n",
      "Iteration 1356, loss = 0.21953837\n",
      "Iteration 1364, loss = 0.18318530\n",
      "Iteration 869, loss = 0.30774556\n",
      "Iteration 1357, loss = 0.21932243\n",
      "Iteration 405, loss = 0.37827722\n",
      "Iteration 691, loss = 0.34614593\n",
      "Iteration 2088, loss = 0.17199858\n",
      "Iteration 1365, loss = 0.18297151\n",
      "Iteration 870, loss = 0.30773421\n",
      "Iteration 1076, loss = 0.29267266\n",
      "Iteration 1358, loss = 0.21922038\n",
      "Iteration 406, loss = 0.37812453\n",
      "Iteration 387, loss = 0.38024325\n",
      "Iteration 871, loss = 0.30753987\n",
      "Iteration 1077, loss = 0.29251466\n",
      "Iteration 1359, loss = 0.21918372\n",
      "Iteration 1366, loss = 0.18269807\n",
      "Iteration 872, loss = 0.30747862\n",
      "Iteration 2089, loss = 0.17179673\n",
      "Iteration 1360, loss = 0.21896926\n",
      "Iteration 1361, loss = 0.21869978\n",
      "Iteration 873, loss = 0.30729728\n",
      "Iteration 407, loss = 0.37800171\n",
      "Iteration 692, loss = 0.34584402\n",
      "Iteration 1362, loss = 0.21860743\n",
      "Iteration 388, loss = 0.38010055\n",
      "Iteration 1367, loss = 0.18258665\n",
      "Iteration 1363, loss = 0.21833316\n",
      "Iteration 2090, loss = 0.17172315\n",
      "Iteration 1078, loss = 0.29229737\n",
      "Iteration 1364, loss = 0.21823029\n",
      "Iteration 408, loss = 0.37787125Iteration 874, loss = 0.30719861\n",
      "\n",
      "Iteration 389, loss = 0.37991328\n",
      "Iteration 1365, loss = 0.21799700\n",
      "Iteration 875, loss = 0.30709058\n",
      "Iteration 1366, loss = 0.21803922\n",
      "Iteration 1367, loss = 0.21765886\n",
      "Iteration 390, loss = 0.37963675\n",
      "Iteration 1368, loss = 0.18230723\n",
      "Iteration 1368, loss = 0.21750750\n",
      "Iteration 2091, loss = 0.17164828\n",
      "Iteration 1369, loss = 0.21739399\n",
      "Iteration 876, loss = 0.30698319\n",
      "Iteration 391, loss = 0.37953409\n",
      "Iteration 1079, loss = 0.29221324\n",
      "Iteration 693, loss = 0.34565281\n",
      "Iteration 1370, loss = 0.21718816\n",
      "Iteration 877, loss = 0.30685673\n",
      "Iteration 1371, loss = 0.21700026\n",
      "Iteration 392, loss = 0.37931910\n",
      "Iteration 409, loss = 0.37779052\n",
      "Iteration 1372, loss = 0.21688116\n",
      "Iteration 2092, loss = 0.17151464\n",
      "Iteration 878, loss = 0.30684452\n",
      "Iteration 1369, loss = 0.18218113\n",
      "Iteration 1373, loss = 0.21669337\n",
      "Iteration 1374, loss = 0.21647800\n",
      "Iteration 2093, loss = 0.17138662\n",
      "Iteration 1375, loss = 0.21636686\n",
      "Iteration 1080, loss = 0.29210415\n",
      "Iteration 879, loss = 0.30666034\n",
      "Iteration 1370, loss = 0.18250440\n",
      "Iteration 1376, loss = 0.21629299\n",
      "Iteration 2094, loss = 0.17140505\n",
      "Iteration 1377, loss = 0.21601952\n",
      "Iteration 2095, loss = 0.17129583\n",
      "Iteration 1378, loss = 0.21582752\n",
      "Iteration 410, loss = 0.37759694\n",
      "Iteration 1371, loss = 0.18188942\n",
      "Iteration 1081, loss = 0.29194974\n",
      "Iteration 393, loss = 0.37917148\n",
      "Iteration 2096, loss = 0.17107071\n",
      "Iteration 880, loss = 0.30659841\n",
      "Iteration 694, loss = 0.34553795\n",
      "Iteration 1379, loss = 0.21568616\n",
      "Iteration 1372, loss = 0.18175145\n",
      "Iteration 881, loss = 0.30646675\n",
      "Iteration 394, loss = 0.37900930\n",
      "Iteration 1082, loss = 0.29188536\n",
      "Iteration 2097, loss = 0.17104057\n",
      "Iteration 1380, loss = 0.21551218\n",
      "Iteration 411, loss = 0.37749769\n",
      "Iteration 882, loss = 0.30637080\n",
      "Iteration 1373, loss = 0.18151014\n",
      "Iteration 1381, loss = 0.21534140\n",
      "Iteration 883, loss = 0.30622517\n",
      "Iteration 1083, loss = 0.29170076Iteration 1382, loss = 0.21523320\n",
      "\n",
      "Iteration 695, loss = 0.34522783\n",
      "Iteration 1383, loss = 0.21506503\n",
      "Iteration 1374, loss = 0.18130607\n",
      "Iteration 412, loss = 0.37733023\n",
      "Iteration 395, loss = 0.37887879\n",
      "Iteration 1384, loss = 0.21483321\n",
      "Iteration 1385, loss = 0.21470310\n",
      "Iteration 1375, loss = 0.18121741\n",
      "Iteration 884, loss = 0.30616693\n",
      "Iteration 1386, loss = 0.21451986\n",
      "Iteration 396, loss = 0.37874762\n",
      "Iteration 2098, loss = 0.17100048\n",
      "Iteration 1387, loss = 0.21451800\n",
      "Iteration 885, loss = 0.30600605\n",
      "Iteration 2099, loss = 0.17075768\n",
      "Iteration 696, loss = 0.34508345\n",
      "Iteration 1084, loss = 0.29154938\n",
      "Iteration 397, loss = 0.37850106\n",
      "Iteration 1388, loss = 0.21414130\n",
      "Iteration 886, loss = 0.30589402\n",
      "Iteration 2100, loss = 0.17072641\n",
      "Iteration 413, loss = 0.37718368\n",
      "Iteration 887, loss = 0.30580557\n",
      "Iteration 398, loss = 0.37836122\n",
      "Iteration 1376, loss = 0.18087331\n",
      "Iteration 1085, loss = 0.29140728\n",
      "Iteration 1389, loss = 0.21399000\n",
      "Iteration 1390, loss = 0.21381823\n",
      "Iteration 2101, loss = 0.17055419\n",
      "Iteration 1391, loss = 0.21367260\n",
      "Iteration 888, loss = 0.30566599\n",
      "Iteration 399, loss = 0.37815300\n",
      "Iteration 697, loss = 0.34485295\n",
      "Iteration 414, loss = 0.37707687\n",
      "Iteration 1086, loss = 0.29131391\n",
      "Iteration 1377, loss = 0.18081010\n",
      "Iteration 2102, loss = 0.17060458\n",
      "Iteration 889, loss = 0.30554042\n",
      "Iteration 1392, loss = 0.21345936\n",
      "Iteration 400, loss = 0.37798602\n",
      "Iteration 415, loss = 0.37692372\n",
      "Iteration 2103, loss = 0.17041452\n",
      "Iteration 1393, loss = 0.21332003\n",
      "Iteration 1394, loss = 0.21319673\n",
      "Iteration 698, loss = 0.34447937\n",
      "Iteration 2104, loss = 0.17030491\n",
      "Iteration 401, loss = 0.37785066\n",
      "Iteration 890, loss = 0.30550885\n",
      "Iteration 1378, loss = 0.18056385\n",
      "Iteration 1395, loss = 0.21308139\n",
      "Iteration 1396, loss = 0.21288935\n",
      "Iteration 1087, loss = 0.29117565\n",
      "Iteration 1397, loss = 0.21272496\n",
      "Iteration 2105, loss = 0.17021579\n",
      "Iteration 402, loss = 0.37770845\n",
      "Iteration 1398, loss = 0.21247863\n",
      "Iteration 891, loss = 0.30537515\n",
      "Iteration 1379, loss = 0.18044326\n",
      "Iteration 1399, loss = 0.21248472\n",
      "Iteration 2106, loss = 0.17016022\n",
      "Iteration 1400, loss = 0.21217435\n",
      "Iteration 2107, loss = 0.17001350\n",
      "Iteration 1401, loss = 0.21205804\n",
      "Iteration 1380, loss = 0.18040931\n",
      "Iteration 699, loss = 0.34429782\n",
      "Iteration 892, loss = 0.30523870\n",
      "Iteration 2108, loss = 0.16988487\n",
      "Iteration 1088, loss = 0.29103604\n",
      "Iteration 416, loss = 0.37688725\n",
      "Iteration 1402, loss = 0.21181377\n",
      "Iteration 2109, loss = 0.16973849\n",
      "Iteration 893, loss = 0.30515188\n",
      "Iteration 1403, loss = 0.21164587\n",
      "Iteration 403, loss = 0.37750021\n",
      "Iteration 2110, loss = 0.16977860\n",
      "Iteration 1381, loss = 0.18020693\n",
      "Iteration 1089, loss = 0.29089716\n",
      "Iteration 1404, loss = 0.21155394\n",
      "Iteration 894, loss = 0.30504284\n",
      "Iteration 2111, loss = 0.16958271\n",
      "Iteration 1382, loss = 0.18036821\n",
      "Iteration 404, loss = 0.37745463\n",
      "Iteration 1405, loss = 0.21150033\n",
      "Iteration 700, loss = 0.34403448\n",
      "Iteration 1090, loss = 0.29083419\n",
      "Iteration 417, loss = 0.37670758\n",
      "Iteration 895, loss = 0.30489795\n",
      "Iteration 2112, loss = 0.16956293\n",
      "Iteration 1406, loss = 0.21120612\n",
      "Iteration 1383, loss = 0.17967655Iteration 896, loss = 0.30481136\n",
      "Iteration 2113, loss = 0.16934951\n",
      "\n",
      "Iteration 1407, loss = 0.21102578\n",
      "Iteration 1408, loss = 0.21082778\n",
      "Iteration 897, loss = 0.30468037\n",
      "Iteration 1409, loss = 0.21067589\n",
      "Iteration 2114, loss = 0.16927629\n",
      "Iteration 418, loss = 0.37651662\n",
      "Iteration 405, loss = 0.37717280\n",
      "Iteration 898, loss = 0.30457031\n",
      "Iteration 1410, loss = 0.21046380\n",
      "Iteration 2115, loss = 0.16919833\n",
      "Iteration 1411, loss = 0.21046386\n",
      "Iteration 899, loss = 0.30446182\n",
      "Iteration 2116, loss = 0.16904313\n",
      "Iteration 419, loss = 0.37641373\n",
      "Iteration 1412, loss = 0.21022829\n",
      "Iteration 1413, loss = 0.20999653\n",
      "Iteration 406, loss = 0.37702791\n",
      "Iteration 1414, loss = 0.20980096\n",
      "Iteration 1091, loss = 0.29064760Iteration 1415, loss = 0.20982294\n",
      "\n",
      "Iteration 1416, loss = 0.20947246\n",
      "Iteration 1417, loss = 0.20936475\n",
      "Iteration 407, loss = 0.37695038\n",
      "Iteration 701, loss = 0.34384946\n",
      "Iteration 1384, loss = 0.17964650\n",
      "Iteration 2117, loss = 0.16896944\n",
      "Iteration 408, loss = 0.37668142\n",
      "Iteration 900, loss = 0.30437032\n",
      "Iteration 420, loss = 0.37626109\n",
      "Iteration 1418, loss = 0.20912406\n",
      "Iteration 901, loss = 0.30424933\n",
      "Iteration 1419, loss = 0.20918276\n",
      "Iteration 902, loss = 0.30418692\n",
      "Iteration 1420, loss = 0.20878385\n",
      "Iteration 1092, loss = 0.29055808\n",
      "Iteration 2118, loss = 0.16894100\n",
      "Iteration 702, loss = 0.34367916\n",
      "Iteration 1385, loss = 0.17938960\n",
      "Iteration 1421, loss = 0.20860159\n",
      "Iteration 1422, loss = 0.20846471\n",
      "Iteration 1093, loss = 0.29042830\n",
      "Iteration 1386, loss = 0.17918297\n",
      "Iteration 2119, loss = 0.16879070\n",
      "Iteration 903, loss = 0.30401676\n",
      "Iteration 1423, loss = 0.20848548Iteration 409, loss = 0.37653862\n",
      "\n",
      "Iteration 421, loss = 0.37612101\n",
      "Iteration 1387, loss = 0.17914025\n",
      "Iteration 1424, loss = 0.20813638\n",
      "Iteration 2120, loss = 0.16875387\n",
      "Iteration 703, loss = 0.34338080\n",
      "Iteration 1425, loss = 0.20803869\n",
      "Iteration 1426, loss = 0.20775890\n",
      "Iteration 904, loss = 0.30392480\n",
      "Iteration 1094, loss = 0.29028089\n",
      "Iteration 1388, loss = 0.17888359\n",
      "Iteration 2121, loss = 0.16857711\n",
      "Iteration 1427, loss = 0.20767314\n",
      "Iteration 410, loss = 0.37641706\n",
      "Iteration 1428, loss = 0.20740579\n",
      "Iteration 704, loss = 0.34314962\n",
      "Iteration 2122, loss = 0.16851851\n",
      "Iteration 905, loss = 0.30379517\n",
      "Iteration 422, loss = 0.37599806\n",
      "Iteration 1429, loss = 0.20748091\n",
      "Iteration 1430, loss = 0.20706582\n",
      "Iteration 411, loss = 0.37621683\n",
      "Iteration 1431, loss = 0.20690811Iteration 1389, loss = 0.17860453\n",
      "\n",
      "Iteration 1095, loss = 0.29017863\n",
      "Iteration 705, loss = 0.34304823\n",
      "Iteration 1432, loss = 0.20679218\n",
      "Iteration 2123, loss = 0.16842096\n",
      "Iteration 1390, loss = 0.17843546\n",
      "Iteration 412, loss = 0.37608722\n",
      "Iteration 906, loss = 0.30369913\n",
      "Iteration 1433, loss = 0.20658078\n",
      "Iteration 907, loss = 0.30363111\n",
      "Iteration 2124, loss = 0.16837463\n",
      "Iteration 1096, loss = 0.29004501\n",
      "Iteration 413, loss = 0.37594391\n",
      "Iteration 423, loss = 0.37597225\n",
      "Iteration 1434, loss = 0.20641717\n",
      "Iteration 1391, loss = 0.17857425\n",
      "Iteration 1435, loss = 0.20639572\n",
      "Iteration 2125, loss = 0.16821066\n",
      "Iteration 706, loss = 0.34269912\n",
      "Iteration 908, loss = 0.30347058\n",
      "Iteration 1097, loss = 0.28990577\n",
      "Iteration 1436, loss = 0.20611412\n",
      "Iteration 414, loss = 0.37575237\n",
      "Iteration 1437, loss = 0.20596825\n",
      "Iteration 909, loss = 0.30335226\n",
      "Iteration 1438, loss = 0.20580738\n",
      "Iteration 2126, loss = 0.16809511\n",
      "Iteration 415, loss = 0.37557380\n",
      "Iteration 2127, loss = 0.16796487\n",
      "Iteration 910, loss = 0.30328743\n",
      "Iteration 424, loss = 0.37573131\n",
      "Iteration 1392, loss = 0.17831304\n",
      "Iteration 1439, loss = 0.20560796\n",
      "Iteration 911, loss = 0.30313277\n",
      "Iteration 1440, loss = 0.20542158\n",
      "Iteration 1098, loss = 0.28979446\n",
      "Iteration 416, loss = 0.37542488\n",
      "Iteration 912, loss = 0.30304165\n",
      "Iteration 707, loss = 0.34249860\n",
      "Iteration 1441, loss = 0.20524053\n",
      "Iteration 2128, loss = 0.16792880\n",
      "Iteration 1393, loss = 0.17804867\n",
      "Iteration 913, loss = 0.30298237\n",
      "Iteration 1442, loss = 0.20506011\n",
      "Iteration 417, loss = 0.37525154\n",
      "Iteration 1443, loss = 0.20485205\n",
      "Iteration 1394, loss = 0.17776391\n",
      "Iteration 914, loss = 0.30281710\n",
      "Iteration 425, loss = 0.37559134\n",
      "Iteration 1099, loss = 0.28968815\n",
      "Iteration 2129, loss = 0.16779942\n",
      "Iteration 1444, loss = 0.20468746\n",
      "Iteration 1395, loss = 0.17773203\n",
      "Iteration 418, loss = 0.37507502\n",
      "Iteration 2130, loss = 0.16775792\n",
      "Iteration 708, loss = 0.34219394\n",
      "Iteration 1396, loss = 0.17766034\n",
      "Iteration 915, loss = 0.30267712\n",
      "Iteration 1445, loss = 0.20455965\n",
      "Iteration 2131, loss = 0.16758114\n",
      "Iteration 426, loss = 0.37545941\n",
      "Iteration 419, loss = 0.37492800\n",
      "Iteration 1446, loss = 0.20444186\n",
      "Iteration 1100, loss = 0.28954814\n",
      "Iteration 916, loss = 0.30260394\n",
      "Iteration 2132, loss = 0.16747756\n",
      "Iteration 420, loss = 0.37484876\n",
      "Iteration 1397, loss = 0.17725623\n",
      "Iteration 1447, loss = 0.20425593\n",
      "Iteration 917, loss = 0.30252794\n",
      "Iteration 421, loss = 0.37468299\n",
      "Iteration 2133, loss = 0.16738726\n",
      "Iteration 1101, loss = 0.28939018\n",
      "Iteration 918, loss = 0.30235866\n",
      "Iteration 1448, loss = 0.20409234\n",
      "Iteration 427, loss = 0.37535067Iteration 2134, loss = 0.16732400\n",
      "\n",
      "Iteration 709, loss = 0.34202204\n",
      "Iteration 1398, loss = 0.17709642\n",
      "Iteration 1449, loss = 0.20403707\n",
      "Iteration 919, loss = 0.30228227\n",
      "Iteration 1450, loss = 0.20369206\n",
      "Iteration 422, loss = 0.37457452\n",
      "Iteration 1451, loss = 0.20357286\n",
      "Iteration 1399, loss = 0.17709626\n",
      "Iteration 2135, loss = 0.16716588\n",
      "Iteration 1452, loss = 0.20346545\n",
      "Iteration 920, loss = 0.30214900\n",
      "Iteration 1453, loss = 0.20338179\n",
      "Iteration 428, loss = 0.37519850\n",
      "Iteration 1102, loss = 0.28932978\n",
      "Iteration 2136, loss = 0.16707194\n",
      "Iteration 423, loss = 0.37426411\n",
      "Iteration 1454, loss = 0.20333445\n",
      "Iteration 1400, loss = 0.17677352\n",
      "Iteration 2137, loss = 0.16702605\n",
      "Iteration 424, loss = 0.37417231\n",
      "Iteration 1455, loss = 0.20292141\n",
      "Iteration 710, loss = 0.34179027\n",
      "Iteration 1401, loss = 0.17662778\n",
      "Iteration 1456, loss = 0.20269484\n",
      "Iteration 921, loss = 0.30201755\n",
      "Iteration 425, loss = 0.37398277\n",
      "Iteration 1457, loss = 0.20259440\n",
      "Iteration 2138, loss = 0.16703614\n",
      "Iteration 1103, loss = 0.28913825\n",
      "Iteration 1402, loss = 0.17652400\n",
      "Iteration 1458, loss = 0.20238914\n",
      "Iteration 429, loss = 0.37508660\n",
      "Iteration 1459, loss = 0.20229115\n",
      "Iteration 426, loss = 0.37382411\n",
      "Iteration 711, loss = 0.34182416\n",
      "Iteration 922, loss = 0.30192212\n",
      "Iteration 1104, loss = 0.28904264\n",
      "Iteration 1460, loss = 0.20200052\n",
      "Iteration 2139, loss = 0.16679831\n",
      "Iteration 427, loss = 0.37366426\n",
      "Iteration 1461, loss = 0.20184636\n",
      "Iteration 1462, loss = 0.20176535\n",
      "Iteration 1403, loss = 0.17621052\n",
      "Iteration 430, loss = 0.37495842\n",
      "Iteration 1463, loss = 0.20164359\n",
      "Iteration 2140, loss = 0.16674497\n",
      "Iteration 1464, loss = 0.20136782\n",
      "Iteration 923, loss = 0.30181037\n",
      "Iteration 1105, loss = 0.28891977\n",
      "Iteration 1465, loss = 0.20121535\n",
      "Iteration 428, loss = 0.37348675\n",
      "Iteration 1404, loss = 0.17609167\n",
      "Iteration 2141, loss = 0.16662610\n",
      "Iteration 1466, loss = 0.20108405\n",
      "Iteration 712, loss = 0.34133701\n",
      "Iteration 2142, loss = 0.16659311\n",
      "Iteration 1467, loss = 0.20097235\n",
      "Iteration 431, loss = 0.37484587\n",
      "Iteration 924, loss = 0.30169998\n",
      "Iteration 1468, loss = 0.20072942\n",
      "Iteration 429, loss = 0.37334556Iteration 1469, loss = 0.20055409\n",
      "\n",
      "Iteration 2143, loss = 0.16638059\n",
      "Iteration 1470, loss = 0.20032078\n",
      "Iteration 1471, loss = 0.20015081\n",
      "Iteration 432, loss = 0.37467913\n",
      "Iteration 1405, loss = 0.17616441\n",
      "Iteration 925, loss = 0.30158875\n",
      "Iteration 430, loss = 0.37313755\n",
      "Iteration 1472, loss = 0.20002326\n",
      "Iteration 1106, loss = 0.28876812\n",
      "Iteration 2144, loss = 0.16636078\n",
      "Iteration 926, loss = 0.30149231\n",
      "Iteration 1473, loss = 0.19985180\n",
      "Iteration 927, loss = 0.30137052\n",
      "Iteration 713, loss = 0.34110949\n",
      "Iteration 1406, loss = 0.17570870\n",
      "Iteration 1474, loss = 0.19964220\n",
      "Iteration 2145, loss = 0.16624386\n",
      "Iteration 928, loss = 0.30128904\n",
      "Iteration 431, loss = 0.37305301\n",
      "Iteration 1475, loss = 0.19941190\n",
      "Iteration 433, loss = 0.37454151\n",
      "Iteration 1107, loss = 0.28864772\n",
      "Iteration 929, loss = 0.30114202\n",
      "Iteration 2146, loss = 0.16610320\n",
      "Iteration 1476, loss = 0.19933403\n",
      "Iteration 1407, loss = 0.17563893\n",
      "Iteration 1477, loss = 0.19935510\n",
      "Iteration 1108, loss = 0.28849975\n",
      "Iteration 434, loss = 0.37445998\n",
      "Iteration 930, loss = 0.30104200\n",
      "Iteration 2147, loss = 0.16605100\n",
      "Iteration 1408, loss = 0.17543419\n",
      "Iteration 1478, loss = 0.19890941\n",
      "Iteration 432, loss = 0.37286403\n",
      "Iteration 2148, loss = 0.16590042\n",
      "Iteration 931, loss = 0.30092650\n",
      "Iteration 1479, loss = 0.19889381\n",
      "Iteration 714, loss = 0.34087473\n",
      "Iteration 435, loss = 0.37429678\n",
      "Iteration 932, loss = 0.30080128\n",
      "Iteration 2149, loss = 0.16589951\n",
      "Iteration 1109, loss = 0.28842163\n",
      "Iteration 1480, loss = 0.19877659\n",
      "Iteration 1481, loss = 0.19843998\n",
      "Iteration 433, loss = 0.37268546\n",
      "Iteration 2150, loss = 0.16570246\n",
      "Iteration 1482, loss = 0.19836069\n",
      "Iteration 1409, loss = 0.17523840\n",
      "Iteration 715, loss = 0.34072473\n",
      "Iteration 436, loss = 0.37418939\n",
      "Iteration 2151, loss = 0.16562375\n",
      "Iteration 1483, loss = 0.19829100\n",
      "Iteration 1110, loss = 0.28837083\n",
      "Iteration 2152, loss = 0.16558617\n",
      "Iteration 1484, loss = 0.19799823\n",
      "Iteration 434, loss = 0.37253464\n",
      "Iteration 1485, loss = 0.19773919\n",
      "Iteration 1111, loss = 0.28813476\n",
      "Iteration 1486, loss = 0.19767186\n",
      "Iteration 1410, loss = 0.17517814\n",
      "Iteration 2153, loss = 0.16549645\n",
      "Iteration 716, loss = 0.34051221\n",
      "Iteration 1411, loss = 0.17484283\n",
      "Iteration 2154, loss = 0.16533192\n",
      "Iteration 1487, loss = 0.19748263\n",
      "Iteration 1488, loss = 0.19722902\n",
      "Iteration 2155, loss = 0.16527522\n",
      "Iteration 435, loss = 0.37238261\n",
      "Iteration 1412, loss = 0.17477451\n",
      "Iteration 1112, loss = 0.28802024\n",
      "Iteration 2156, loss = 0.16525842\n",
      "Iteration 436, loss = 0.37222615\n",
      "Iteration 1489, loss = 0.19705380\n",
      "Iteration 437, loss = 0.37408030\n",
      "Iteration 1413, loss = 0.17472898\n",
      "Iteration 1490, loss = 0.19690860\n",
      "Iteration 2157, loss = 0.16500346\n",
      "Iteration 1491, loss = 0.19689763\n",
      "Iteration 1113, loss = 0.28788833\n",
      "Iteration 1492, loss = 0.19669101\n",
      "Iteration 2158, loss = 0.16492907\n",
      "Iteration 437, loss = 0.37215842\n",
      "Iteration 933, loss = 0.30075736\n",
      "Iteration 1414, loss = 0.17438266\n",
      "Iteration 2159, loss = 0.16483376\n",
      "Iteration 1493, loss = 0.19645283\n",
      "Iteration 438, loss = 0.37388433\n",
      "Iteration 438, loss = 0.37190424\n",
      "Iteration 1494, loss = 0.19637889\n",
      "Iteration 1415, loss = 0.17422224\n",
      "Iteration 1495, loss = 0.19610831\n",
      "Iteration 934, loss = 0.30061519\n",
      "Iteration 439, loss = 0.37178084\n",
      "Iteration 1496, loss = 0.19601647\n",
      "Iteration 1114, loss = 0.28787380\n",
      "Iteration 439, loss = 0.37376970\n",
      "Iteration 1497, loss = 0.19582427\n",
      "Iteration 2160, loss = 0.16472998\n",
      "Iteration 717, loss = 0.34019462\n",
      "Iteration 1416, loss = 0.17415504\n",
      "Iteration 2161, loss = 0.16466327\n",
      "Iteration 1498, loss = 0.19555736\n",
      "Iteration 1499, loss = 0.19539867\n",
      "Iteration 2162, loss = 0.16452666\n",
      "Iteration 440, loss = 0.37157330\n",
      "Iteration 1500, loss = 0.19522285\n",
      "Iteration 1115, loss = 0.28764420\n",
      "Iteration 1501, loss = 0.19512032\n",
      "Iteration 2163, loss = 0.16444525\n",
      "Iteration 1417, loss = 0.17392714\n",
      "Iteration 440, loss = 0.37363361\n",
      "Iteration 935, loss = 0.30049820\n",
      "Iteration 1502, loss = 0.19504567\n",
      "Iteration 441, loss = 0.37147747\n",
      "Iteration 1503, loss = 0.19474397\n",
      "Iteration 718, loss = 0.33996233\n",
      "Iteration 1116, loss = 0.28754806\n",
      "Iteration 2164, loss = 0.16437649\n",
      "Iteration 441, loss = 0.37349275\n",
      "Iteration 1504, loss = 0.19457477\n",
      "Iteration 1505, loss = 0.19461359\n",
      "Iteration 442, loss = 0.37125768\n",
      "Iteration 1506, loss = 0.19424136\n",
      "Iteration 2165, loss = 0.16431270\n",
      "Iteration 1418, loss = 0.17374732\n",
      "Iteration 1507, loss = 0.19413374\n",
      "Iteration 442, loss = 0.37342905\n",
      "Iteration 443, loss = 0.37111349\n",
      "Iteration 1117, loss = 0.28737974\n",
      "Iteration 1508, loss = 0.19395275\n",
      "Iteration 2166, loss = 0.16419346\n",
      "Iteration 719, loss = 0.33986495Iteration 1509, loss = 0.19381270\n",
      "\n",
      "Iteration 1419, loss = 0.17370572\n",
      "Iteration 1510, loss = 0.19370850\n",
      "Iteration 2167, loss = 0.16405668\n",
      "Iteration 1511, loss = 0.19338748\n",
      "Iteration 1118, loss = 0.28726303\n",
      "Iteration 936, loss = 0.30037095\n",
      "Iteration 1420, loss = 0.17343614\n",
      "Iteration 1512, loss = 0.19339138\n",
      "Iteration 2168, loss = 0.16396037\n",
      "Iteration 1513, loss = 0.19307963\n",
      "Iteration 444, loss = 0.37094397\n",
      "Iteration 1514, loss = 0.19295946\n",
      "Iteration 1421, loss = 0.17338553\n",
      "Iteration 937, loss = 0.30031350\n",
      "Iteration 443, loss = 0.37327451\n",
      "Iteration 2169, loss = 0.16386223\n",
      "Iteration 1515, loss = 0.19278926\n",
      "Iteration 1516, loss = 0.19262966\n",
      "Iteration 1422, loss = 0.17308275\n",
      "Iteration 1517, loss = 0.19260804\n",
      "Iteration 2170, loss = 0.16383285\n",
      "Iteration 720, loss = 0.33966739\n",
      "Iteration 1119, loss = 0.28713102\n",
      "Iteration 1423, loss = 0.17295536\n",
      "Iteration 2171, loss = 0.16366149\n",
      "Iteration 1518, loss = 0.19222274\n",
      "Iteration 445, loss = 0.37084601\n",
      "Iteration 1519, loss = 0.19216991\n",
      "Iteration 938, loss = 0.30016563\n",
      "Iteration 1520, loss = 0.19186385\n",
      "Iteration 1424, loss = 0.17275556\n",
      "Iteration 444, loss = 0.37311063\n",
      "Iteration 1521, loss = 0.19194664\n",
      "Iteration 2172, loss = 0.16368557\n",
      "Iteration 1522, loss = 0.19161435\n",
      "Iteration 1425, loss = 0.17259702\n",
      "Iteration 2173, loss = 0.16361218\n",
      "Iteration 1523, loss = 0.19138792\n",
      "Iteration 1120, loss = 0.28700657\n",
      "Iteration 446, loss = 0.37065037\n",
      "Iteration 1524, loss = 0.19127414\n",
      "Iteration 445, loss = 0.37299953\n",
      "Iteration 1525, loss = 0.19138519\n",
      "Iteration 721, loss = 0.33931962\n",
      "Iteration 2174, loss = 0.16339002\n",
      "Iteration 1526, loss = 0.19090670\n",
      "Iteration 1426, loss = 0.17242001\n",
      "Iteration 447, loss = 0.37050788\n",
      "Iteration 1527, loss = 0.19075157\n",
      "Iteration 2175, loss = 0.16335939\n",
      "Iteration 1528, loss = 0.19056784\n",
      "Iteration 446, loss = 0.37285297\n",
      "Iteration 2176, loss = 0.16320443\n",
      "Iteration 1529, loss = 0.19037401\n",
      "Iteration 1121, loss = 0.28688377\n",
      "Iteration 1427, loss = 0.17232334\n",
      "Iteration 1530, loss = 0.19025035\n",
      "Iteration 448, loss = 0.37034373\n",
      "Iteration 722, loss = 0.33907074\n",
      "Iteration 447, loss = 0.37275521\n",
      "Iteration 1531, loss = 0.19009976\n",
      "Iteration 1532, loss = 0.18995337\n",
      "Iteration 2177, loss = 0.16318049\n",
      "Iteration 449, loss = 0.37014501\n",
      "Iteration 1428, loss = 0.17217149\n",
      "Iteration 1122, loss = 0.28677253\n",
      "Iteration 723, loss = 0.33884942\n",
      "Iteration 2178, loss = 0.16308343\n",
      "Iteration 448, loss = 0.37258150\n",
      "Iteration 1533, loss = 0.18976898\n",
      "Iteration 2179, loss = 0.16296298\n",
      "Iteration 450, loss = 0.36999889\n",
      "Iteration 1429, loss = 0.17189081\n",
      "Iteration 724, loss = 0.33862045\n",
      "Iteration 1534, loss = 0.18968652\n",
      "Iteration 2180, loss = 0.16288186\n",
      "Iteration 1535, loss = 0.18945249\n",
      "Iteration 1123, loss = 0.28664260\n",
      "Iteration 449, loss = 0.37245226\n",
      "Iteration 1430, loss = 0.17173105\n",
      "Iteration 451, loss = 0.36985870\n",
      "Iteration 2181, loss = 0.16289434\n",
      "Iteration 939, loss = 0.30008387\n",
      "Iteration 725, loss = 0.33841282\n",
      "Iteration 450, loss = 0.37238329\n",
      "Iteration 1536, loss = 0.18931583\n",
      "Iteration 2182, loss = 0.16263978\n",
      "Iteration 940, loss = 0.29992009\n",
      "Iteration 452, loss = 0.36969688\n",
      "Iteration 1537, loss = 0.18907633\n",
      "Iteration 1124, loss = 0.28651382\n",
      "Iteration 2183, loss = 0.16257667\n",
      "Iteration 1538, loss = 0.18893045\n",
      "Iteration 941, loss = 0.29981658\n",
      "Iteration 1431, loss = 0.17159544\n",
      "Iteration 1539, loss = 0.18898353\n",
      "Iteration 2184, loss = 0.16242941\n",
      "Iteration 453, loss = 0.36959062\n",
      "Iteration 1540, loss = 0.18868179\n",
      "Iteration 726, loss = 0.33824856\n",
      "Iteration 942, loss = 0.29968686\n",
      "Iteration 454, loss = 0.36946838\n",
      "Iteration 2185, loss = 0.16234864\n",
      "Iteration 451, loss = 0.37225149\n",
      "Iteration 1432, loss = 0.17148033\n",
      "Iteration 1541, loss = 0.18853298\n",
      "Iteration 1542, loss = 0.18826195\n",
      "Iteration 455, loss = 0.36920927\n",
      "Iteration 943, loss = 0.29957383\n",
      "Iteration 1543, loss = 0.18810397\n",
      "Iteration 452, loss = 0.37206058\n",
      "Iteration 1125, loss = 0.28637335\n",
      "Iteration 2186, loss = 0.16231600\n",
      "Iteration 456, loss = 0.36909703\n",
      "Iteration 727, loss = 0.33792973\n",
      "Iteration 1433, loss = 0.17128423\n",
      "Iteration 1544, loss = 0.18790325\n",
      "Iteration 2187, loss = 0.16216703\n",
      "Iteration 944, loss = 0.29950367\n",
      "Iteration 2188, loss = 0.16208987\n",
      "Iteration 945, loss = 0.29940318\n",
      "Iteration 453, loss = 0.37197075\n",
      "Iteration 1545, loss = 0.18781571\n",
      "Iteration 2189, loss = 0.16193861\n",
      "Iteration 1434, loss = 0.17110143\n",
      "Iteration 946, loss = 0.29928077\n",
      "Iteration 1546, loss = 0.18798326\n",
      "Iteration 457, loss = 0.36894386\n",
      "Iteration 728, loss = 0.33770035\n",
      "Iteration 1126, loss = 0.28629736\n",
      "Iteration 947, loss = 0.29913090\n",
      "Iteration 1547, loss = 0.18767983\n",
      "Iteration 1548, loss = 0.18739061\n",
      "Iteration 2190, loss = 0.16191289\n",
      "Iteration 454, loss = 0.37182930\n",
      "Iteration 1435, loss = 0.17090146\n",
      "Iteration 948, loss = 0.29907023\n",
      "Iteration 1549, loss = 0.18709201\n",
      "Iteration 2191, loss = 0.16175009\n",
      "Iteration 1127, loss = 0.28615387\n",
      "Iteration 949, loss = 0.29890809\n",
      "Iteration 1550, loss = 0.18711933\n",
      "Iteration 458, loss = 0.36885386\n",
      "Iteration 2192, loss = 0.16169618\n",
      "Iteration 1551, loss = 0.18681305\n",
      "Iteration 729, loss = 0.33747724\n",
      "Iteration 459, loss = 0.36860428\n",
      "Iteration 1552, loss = 0.18662805\n",
      "Iteration 1436, loss = 0.17072631\n",
      "Iteration 950, loss = 0.29881272\n",
      "Iteration 1553, loss = 0.18645918\n",
      "Iteration 455, loss = 0.37168876\n",
      "Iteration 2193, loss = 0.16158992\n",
      "Iteration 730, loss = 0.33725903\n",
      "Iteration 1554, loss = 0.18633218\n",
      "Iteration 1128, loss = 0.28609729\n",
      "Iteration 460, loss = 0.36848879\n",
      "Iteration 2194, loss = 0.16159613\n",
      "Iteration 951, loss = 0.29869106\n",
      "Iteration 1555, loss = 0.18623713\n",
      "Iteration 1556, loss = 0.18594625\n",
      "Iteration 1437, loss = 0.17071403\n",
      "Iteration 731, loss = 0.33705017\n",
      "Iteration 2195, loss = 0.16150326\n",
      "Iteration 456, loss = 0.37155220\n",
      "Iteration 461, loss = 0.36828171\n",
      "Iteration 1557, loss = 0.18580956\n",
      "Iteration 952, loss = 0.29859369\n",
      "Iteration 1558, loss = 0.18564325\n",
      "Iteration 2196, loss = 0.16127948\n",
      "Iteration 953, loss = 0.29848175\n",
      "Iteration 462, loss = 0.36812724\n",
      "Iteration 1438, loss = 0.17047211\n",
      "Iteration 1559, loss = 0.18541628\n",
      "Iteration 1129, loss = 0.28588288\n",
      "Iteration 2197, loss = 0.16116887\n",
      "Iteration 954, loss = 0.29843699\n",
      "Iteration 463, loss = 0.36799685\n",
      "Iteration 1560, loss = 0.18530846\n",
      "Iteration 1439, loss = 0.17043499\n",
      "Iteration 1561, loss = 0.18513415\n",
      "Iteration 2198, loss = 0.16119653\n",
      "Iteration 732, loss = 0.33681559\n",
      "Iteration 1562, loss = 0.18508786\n",
      "Iteration 457, loss = 0.37144228\n",
      "Iteration 1563, loss = 0.18478119\n",
      "Iteration 1564, loss = 0.18470406\n",
      "Iteration 1565, loss = 0.18448419\n",
      "Iteration 955, loss = 0.29823638\n",
      "Iteration 464, loss = 0.36782983\n",
      "Iteration 1440, loss = 0.17037999\n",
      "Iteration 1130, loss = 0.28575552\n",
      "Iteration 458, loss = 0.37131552\n",
      "Iteration 1566, loss = 0.18435521\n",
      "Iteration 956, loss = 0.29815065\n",
      "Iteration 733, loss = 0.33656976\n",
      "Iteration 1441, loss = 0.17004356\n",
      "Iteration 1567, loss = 0.18416217\n",
      "Iteration 957, loss = 0.29807933\n",
      "Iteration 459, loss = 0.37118383\n",
      "Iteration 1131, loss = 0.28573748\n",
      "Iteration 1568, loss = 0.18396724\n",
      "Iteration 958, loss = 0.29791065\n",
      "Iteration 1442, loss = 0.16990885\n",
      "Iteration 465, loss = 0.36765188\n",
      "Iteration 1569, loss = 0.18388094\n",
      "Iteration 1132, loss = 0.28555707\n",
      "Iteration 1570, loss = 0.18365121\n",
      "Iteration 959, loss = 0.29782540\n",
      "Iteration 1443, loss = 0.16964055\n",
      "Iteration 734, loss = 0.33636735\n",
      "Iteration 960, loss = 0.29768790\n",
      "Iteration 1571, loss = 0.18360670\n",
      "Iteration 460, loss = 0.37106066\n",
      "Iteration 1444, loss = 0.16952229\n",
      "Iteration 1572, loss = 0.18331900\n",
      "Iteration 466, loss = 0.36760087\n",
      "Iteration 1573, loss = 0.18327719\n",
      "Iteration 1133, loss = 0.28540717\n",
      "Iteration 961, loss = 0.29757458\n",
      "Iteration 1574, loss = 0.18295176\n",
      "Iteration 1445, loss = 0.16945920\n",
      "Iteration 1575, loss = 0.18294892\n",
      "Iteration 962, loss = 0.29744636\n",
      "Iteration 1576, loss = 0.18261343\n",
      "Iteration 735, loss = 0.33618169\n",
      "Iteration 963, loss = 0.29739160\n",
      "Iteration 461, loss = 0.37093173\n",
      "Iteration 1446, loss = 0.16939284\n",
      "Iteration 467, loss = 0.36740254\n",
      "Iteration 1577, loss = 0.18244327\n",
      "Iteration 1134, loss = 0.28528850\n",
      "Iteration 964, loss = 0.29725417\n",
      "Iteration 1447, loss = 0.16899362\n",
      "Iteration 468, loss = 0.36727846\n",
      "Iteration 1578, loss = 0.18232297\n",
      "Iteration 462, loss = 0.37080471\n",
      "Iteration 965, loss = 0.29712787\n",
      "Iteration 1579, loss = 0.18224569\n",
      "Iteration 736, loss = 0.33590280\n",
      "Iteration 469, loss = 0.36705715\n",
      "Iteration 1448, loss = 0.16886371\n",
      "Iteration 966, loss = 0.29700574\n",
      "Iteration 1135, loss = 0.28510683\n",
      "Iteration 1580, loss = 0.18198810\n",
      "Iteration 463, loss = 0.37066326\n",
      "Iteration 470, loss = 0.36695836\n",
      "Iteration 1581, loss = 0.18182284\n",
      "Iteration 967, loss = 0.29691439\n",
      "Iteration 1449, loss = 0.16875125\n",
      "Iteration 1582, loss = 0.18172554\n",
      "Iteration 968, loss = 0.29680215\n",
      "Iteration 1583, loss = 0.18169757\n",
      "Iteration 471, loss = 0.36690310\n",
      "Iteration 464, loss = 0.37057429\n",
      "Iteration 1136, loss = 0.28497550\n",
      "Iteration 1584, loss = 0.18136990\n",
      "Iteration 1450, loss = 0.16868904\n",
      "Iteration 969, loss = 0.29674546\n",
      "Iteration 737, loss = 0.33567650\n",
      "Iteration 472, loss = 0.36667980\n",
      "Iteration 970, loss = 0.29656170\n",
      "Iteration 1585, loss = 0.18113865\n",
      "Iteration 1586, loss = 0.18104199\n",
      "Iteration 473, loss = 0.36645338\n",
      "Iteration 971, loss = 0.29648480\n",
      "Iteration 1137, loss = 0.28486355\n",
      "Iteration 1587, loss = 0.18084961\n",
      "Iteration 1451, loss = 0.16851134\n",
      "Iteration 972, loss = 0.29638059\n",
      "Iteration 465, loss = 0.37040351\n",
      "Iteration 1588, loss = 0.18075154\n",
      "Iteration 1452, loss = 0.16824888\n",
      "Iteration 973, loss = 0.29624197\n",
      "Iteration 474, loss = 0.36636223\n",
      "Iteration 1138, loss = 0.28472763\n",
      "Iteration 1589, loss = 0.18066067\n",
      "Iteration 738, loss = 0.33546796\n",
      "Iteration 1590, loss = 0.18032769\n",
      "Iteration 2199, loss = 0.16101941\n",
      "Iteration 466, loss = 0.37030997\n",
      "Iteration 974, loss = 0.29612104\n",
      "Iteration 2200, loss = 0.16097607\n",
      "Iteration 1591, loss = 0.18027990\n",
      "Iteration 475, loss = 0.36619681\n",
      "Iteration 1453, loss = 0.16810465\n",
      "Iteration 1592, loss = 0.17998483\n",
      "Iteration 467, loss = 0.37015669\n",
      "Iteration 1139, loss = 0.28466388\n",
      "Iteration 975, loss = 0.29600726\n",
      "Iteration 1593, loss = 0.17987027\n",
      "Iteration 739, loss = 0.33526543\n",
      "Iteration 1454, loss = 0.16809996\n",
      "Iteration 976, loss = 0.29587354\n",
      "Iteration 476, loss = 0.36602732\n",
      "Iteration 1594, loss = 0.17972030\n",
      "Iteration 1140, loss = 0.28458420\n",
      "Iteration 468, loss = 0.37005543\n",
      "Iteration 1595, loss = 0.17961260\n",
      "Iteration 477, loss = 0.36587244\n",
      "Iteration 977, loss = 0.29592733\n",
      "Iteration 1596, loss = 0.17947302\n",
      "Iteration 1455, loss = 0.16789525\n",
      "Iteration 740, loss = 0.33498198\n",
      "Iteration 478, loss = 0.36573253\n",
      "Iteration 1141, loss = 0.28432998\n",
      "Iteration 1597, loss = 0.17922672\n",
      "Iteration 978, loss = 0.29566344\n",
      "Iteration 1456, loss = 0.16762652\n",
      "Iteration 469, loss = 0.36991231\n",
      "Iteration 1598, loss = 0.17909852\n",
      "Iteration 1457, loss = 0.16761080\n",
      "Iteration 479, loss = 0.36564060\n",
      "Iteration 1599, loss = 0.17894957\n",
      "Iteration 979, loss = 0.29555082\n",
      "Iteration 1458, loss = 0.16737807\n",
      "Iteration 741, loss = 0.33485266\n",
      "Iteration 1142, loss = 0.28422352\n",
      "Iteration 1600, loss = 0.17870831\n",
      "Iteration 980, loss = 0.29544881\n",
      "Iteration 480, loss = 0.36539398\n",
      "Iteration 981, loss = 0.29530985\n",
      "Iteration 470, loss = 0.36982206\n",
      "Iteration 1459, loss = 0.16719310\n",
      "Iteration 1601, loss = 0.17861371\n",
      "Iteration 481, loss = 0.36530986\n",
      "Iteration 2201, loss = 0.16096895\n",
      "Iteration 982, loss = 0.29520342\n",
      "Iteration 1602, loss = 0.17843117\n",
      "Iteration 983, loss = 0.29519635\n",
      "Iteration 482, loss = 0.36509742\n",
      "Iteration 1143, loss = 0.28410482\n",
      "Iteration 1460, loss = 0.16693655\n",
      "Iteration 2202, loss = 0.16074143\n",
      "Iteration 1603, loss = 0.17837079\n",
      "Iteration 2203, loss = 0.16087488\n",
      "Iteration 471, loss = 0.36969298\n",
      "Iteration 984, loss = 0.29503143\n",
      "Iteration 1144, loss = 0.28398728\n",
      "Iteration 742, loss = 0.33459917\n",
      "Iteration 1604, loss = 0.17805522\n",
      "Iteration 2204, loss = 0.16076850\n",
      "Iteration 1605, loss = 0.17792293\n",
      "Iteration 1606, loss = 0.17781950\n",
      "Iteration 2205, loss = 0.16054407\n",
      "Iteration 483, loss = 0.36496836\n",
      "Iteration 1145, loss = 0.28382388\n",
      "Iteration 2206, loss = 0.16038543\n",
      "Iteration 1607, loss = 0.17763443\n",
      "Iteration 985, loss = 0.29487058\n",
      "Iteration 1461, loss = 0.16695453\n",
      "Iteration 2207, loss = 0.16032408\n",
      "Iteration 472, loss = 0.36956155\n",
      "Iteration 1608, loss = 0.17746376\n",
      "Iteration 1609, loss = 0.17735624\n",
      "Iteration 1146, loss = 0.28372404\n",
      "Iteration 484, loss = 0.36481979\n",
      "Iteration 2208, loss = 0.16022578\n",
      "Iteration 2209, loss = 0.16013327\n",
      "Iteration 1610, loss = 0.17719366\n",
      "Iteration 743, loss = 0.33430713\n",
      "Iteration 1147, loss = 0.28365622\n",
      "Iteration 2210, loss = 0.16003216\n",
      "Iteration 986, loss = 0.29476098\n",
      "Iteration 1611, loss = 0.17702786\n",
      "Iteration 1462, loss = 0.16681432\n",
      "Iteration 473, loss = 0.36942378\n",
      "Iteration 485, loss = 0.36468330\n",
      "Iteration 1612, loss = 0.17692212\n",
      "Iteration 2211, loss = 0.15987375\n",
      "Iteration 1613, loss = 0.17673094\n",
      "Iteration 1148, loss = 0.28347477\n",
      "Iteration 474, loss = 0.36930536\n",
      "Iteration 1463, loss = 0.16663517\n",
      "Iteration 744, loss = 0.33414343\n",
      "Iteration 987, loss = 0.29463179\n",
      "Iteration 1614, loss = 0.17662486\n",
      "Iteration 2212, loss = 0.15988215\n",
      "Iteration 486, loss = 0.36449209\n",
      "Iteration 2213, loss = 0.15976342\n",
      "Iteration 475, loss = 0.36923634\n",
      "Iteration 1615, loss = 0.17640416\n",
      "Iteration 1464, loss = 0.16644732\n",
      "Iteration 988, loss = 0.29452488\n",
      "Iteration 2214, loss = 0.15965344\n",
      "Iteration 745, loss = 0.33390376\n",
      "Iteration 487, loss = 0.36431667\n",
      "Iteration 1149, loss = 0.28338234\n",
      "Iteration 1616, loss = 0.17621373\n",
      "Iteration 2215, loss = 0.15969531\n",
      "Iteration 989, loss = 0.29445114\n",
      "Iteration 1617, loss = 0.17609457\n",
      "Iteration 2216, loss = 0.15945235\n",
      "Iteration 488, loss = 0.36426825\n",
      "Iteration 2217, loss = 0.15931884\n",
      "Iteration 990, loss = 0.29428782\n",
      "Iteration 489, loss = 0.36402418\n",
      "Iteration 476, loss = 0.36904540\n",
      "Iteration 1618, loss = 0.17601096\n",
      "Iteration 2218, loss = 0.15931878\n",
      "Iteration 1150, loss = 0.28320572\n",
      "Iteration 1465, loss = 0.16623299\n",
      "Iteration 746, loss = 0.33370556\n",
      "Iteration 2219, loss = 0.15960202\n",
      "Iteration 1619, loss = 0.17568146\n",
      "Iteration 2220, loss = 0.15908209\n",
      "Iteration 1151, loss = 0.28326233\n",
      "Iteration 991, loss = 0.29419363\n",
      "Iteration 1620, loss = 0.17558978\n",
      "Iteration 1621, loss = 0.17540217\n",
      "Iteration 2221, loss = 0.15908277\n",
      "Iteration 992, loss = 0.29407643\n",
      "Iteration 1622, loss = 0.17526614\n",
      "Iteration 490, loss = 0.36397586\n",
      "Iteration 1466, loss = 0.16613296\n",
      "Iteration 2222, loss = 0.15893702\n",
      "Iteration 477, loss = 0.36891937\n",
      "Iteration 2223, loss = 0.15882099\n",
      "Iteration 1623, loss = 0.17516947\n",
      "Iteration 2224, loss = 0.15875874\n",
      "Iteration 491, loss = 0.36370946\n",
      "Iteration 747, loss = 0.33359468\n",
      "Iteration 1152, loss = 0.28296991Iteration 2225, loss = 0.15883133\n",
      "Iteration 1467, loss = 0.16598583\n",
      "\n",
      "Iteration 1624, loss = 0.17504261\n",
      "Iteration 993, loss = 0.29395425\n",
      "Iteration 478, loss = 0.36878983\n",
      "Iteration 2226, loss = 0.15857161\n",
      "Iteration 1625, loss = 0.17484537\n",
      "Iteration 2227, loss = 0.15843092\n",
      "Iteration 994, loss = 0.29390489\n",
      "Iteration 748, loss = 0.33325570\n",
      "Iteration 2228, loss = 0.15833405\n",
      "Iteration 492, loss = 0.36362894\n",
      "Iteration 995, loss = 0.29371296\n",
      "Iteration 1626, loss = 0.17456851\n",
      "Iteration 2229, loss = 0.15828624\n",
      "Iteration 1468, loss = 0.16601710\n",
      "Iteration 996, loss = 0.29361824\n",
      "Iteration 493, loss = 0.36340127\n",
      "Iteration 1627, loss = 0.17446875\n",
      "Iteration 479, loss = 0.36869571\n",
      "Iteration 749, loss = 0.33299370\n",
      "Iteration 2230, loss = 0.15820701\n",
      "Iteration 1469, loss = 0.16563830\n",
      "Iteration 997, loss = 0.29354754\n",
      "Iteration 2231, loss = 0.15811903\n",
      "Iteration 494, loss = 0.36324315\n",
      "Iteration 1628, loss = 0.17439236\n",
      "Iteration 1153, loss = 0.28288288\n",
      "Iteration 2232, loss = 0.15797573\n",
      "Iteration 1470, loss = 0.16549191\n",
      "Iteration 750, loss = 0.33276357\n",
      "Iteration 495, loss = 0.36318219\n",
      "Iteration 1629, loss = 0.17424186\n",
      "Iteration 2233, loss = 0.15795576\n",
      "Iteration 998, loss = 0.29337377\n",
      "Iteration 2234, loss = 0.15788335\n",
      "Iteration 480, loss = 0.36855486\n",
      "Iteration 496, loss = 0.36294827\n",
      "Iteration 2235, loss = 0.15776381\n",
      "Iteration 1471, loss = 0.16547006\n",
      "Iteration 497, loss = 0.36276462\n",
      "Iteration 751, loss = 0.33256026\n",
      "Iteration 1630, loss = 0.17446682\n",
      "Iteration 999, loss = 0.29329549\n",
      "Iteration 1154, loss = 0.28270477\n",
      "Iteration 2236, loss = 0.15761875\n",
      "Iteration 1631, loss = 0.17380863\n",
      "Iteration 481, loss = 0.36840573\n",
      "Iteration 1000, loss = 0.29318851\n",
      "Iteration 1632, loss = 0.17369577\n",
      "Iteration 1633, loss = 0.17354181\n",
      "Iteration 498, loss = 0.36263279\n",
      "Iteration 1001, loss = 0.29305588\n",
      "Iteration 2237, loss = 0.15756413\n",
      "Iteration 1634, loss = 0.17330493\n",
      "Iteration 1472, loss = 0.16518349\n",
      "Iteration 499, loss = 0.36249169\n",
      "Iteration 2238, loss = 0.15757530\n",
      "Iteration 1002, loss = 0.29299936\n",
      "Iteration 1635, loss = 0.17323589\n",
      "Iteration 752, loss = 0.33234620\n",
      "Iteration 2239, loss = 0.15735162\n",
      "Iteration 1473, loss = 0.16503516\n",
      "Iteration 482, loss = 0.36827990\n",
      "Iteration 1155, loss = 0.28259586\n",
      "Iteration 1636, loss = 0.17304652\n",
      "Iteration 2240, loss = 0.15736487\n",
      "Iteration 1003, loss = 0.29282651\n",
      "Iteration 2241, loss = 0.15713167\n",
      "Iteration 500, loss = 0.36235071\n",
      "Iteration 1637, loss = 0.17283490\n",
      "Iteration 753, loss = 0.33204989\n",
      "Iteration 1004, loss = 0.29272066\n",
      "Iteration 2242, loss = 0.15708232\n",
      "Iteration 1638, loss = 0.17283485\n",
      "Iteration 2243, loss = 0.15694953\n",
      "Iteration 1005, loss = 0.29258891\n",
      "Iteration 1474, loss = 0.16484476\n",
      "Iteration 483, loss = 0.36825332\n",
      "Iteration 1639, loss = 0.17278938\n",
      "Iteration 2244, loss = 0.15692217\n",
      "Iteration 1156, loss = 0.28244610\n",
      "Iteration 1006, loss = 0.29248306\n",
      "Iteration 1640, loss = 0.17236580\n",
      "Iteration 501, loss = 0.36220594\n",
      "Iteration 2245, loss = 0.15686560\n",
      "Iteration 1641, loss = 0.17219968\n",
      "Iteration 1007, loss = 0.29238965\n",
      "Iteration 1475, loss = 0.16474627\n",
      "Iteration 754, loss = 0.33192057\n",
      "Iteration 484, loss = 0.36807144\n",
      "Iteration 1008, loss = 0.29223046\n",
      "Iteration 2246, loss = 0.15671887\n",
      "Iteration 1642, loss = 0.17218730\n",
      "Iteration 502, loss = 0.36200157\n",
      "Iteration 1643, loss = 0.17193126\n",
      "Iteration 1157, loss = 0.28237413\n",
      "Iteration 1476, loss = 0.16460966\n",
      "Iteration 1644, loss = 0.17178108\n",
      "Iteration 2247, loss = 0.15669259\n",
      "Iteration 503, loss = 0.36185560\n",
      "Iteration 1645, loss = 0.17168075\n",
      "Iteration 755, loss = 0.33160589\n",
      "Iteration 2248, loss = 0.15655420\n",
      "Iteration 1477, loss = 0.16441640\n",
      "Iteration 1009, loss = 0.29216033\n",
      "Iteration 1158, loss = 0.28222758\n",
      "Iteration 2249, loss = 0.15644298\n",
      "Iteration 1478, loss = 0.16433086\n",
      "Iteration 1010, loss = 0.29203370\n",
      "Iteration 485, loss = 0.36794370\n",
      "Iteration 1646, loss = 0.17144526\n",
      "Iteration 2250, loss = 0.15632845\n",
      "Iteration 1647, loss = 0.17132822\n",
      "Iteration 1011, loss = 0.29200450\n",
      "Iteration 1479, loss = 0.16423475\n",
      "Iteration 1648, loss = 0.17113341\n",
      "Iteration 1649, loss = 0.17105659\n",
      "Iteration 504, loss = 0.36170470\n",
      "Iteration 1650, loss = 0.17079321\n",
      "Iteration 1159, loss = 0.28209376\n",
      "Iteration 2251, loss = 0.15630844\n",
      "Iteration 1012, loss = 0.29175788\n",
      "Iteration 756, loss = 0.33141169\n",
      "Iteration 505, loss = 0.36163218\n",
      "Iteration 486, loss = 0.36780947\n",
      "Iteration 1651, loss = 0.17063631\n",
      "Iteration 1480, loss = 0.16398034\n",
      "Iteration 2252, loss = 0.15623099\n",
      "Iteration 1652, loss = 0.17066875\n",
      "Iteration 487, loss = 0.36772012\n",
      "Iteration 757, loss = 0.33119435\n",
      "Iteration 2253, loss = 0.15608681\n",
      "Iteration 1013, loss = 0.29169312\n",
      "Iteration 1653, loss = 0.17040192\n",
      "Iteration 1160, loss = 0.28193863\n",
      "Iteration 506, loss = 0.36138386\n",
      "Iteration 2254, loss = 0.15606123\n",
      "Iteration 1014, loss = 0.29153099\n",
      "Iteration 2255, loss = 0.15601548\n",
      "Iteration 1161, loss = 0.28185090\n",
      "Iteration 758, loss = 0.33106640\n",
      "Iteration 2256, loss = 0.15592820\n",
      "Iteration 507, loss = 0.36126783\n",
      "Iteration 1654, loss = 0.17041829\n",
      "Iteration 1481, loss = 0.16381093\n",
      "Iteration 1015, loss = 0.29145358\n",
      "Iteration 2257, loss = 0.15572646\n",
      "Iteration 1655, loss = 0.17006074\n",
      "Iteration 2258, loss = 0.15572257\n",
      "Iteration 1016, loss = 0.29133671\n",
      "Iteration 488, loss = 0.36756879\n",
      "Iteration 508, loss = 0.36110733\n",
      "Iteration 2259, loss = 0.15555598\n",
      "Iteration 1482, loss = 0.16368244\n",
      "Iteration 1656, loss = 0.16996400\n",
      "Iteration 1162, loss = 0.28172497\n",
      "Iteration 2260, loss = 0.15562883\n",
      "Iteration 759, loss = 0.33076466\n",
      "Iteration 1017, loss = 0.29120533\n",
      "Iteration 1657, loss = 0.16979891\n",
      "Iteration 489, loss = 0.36741460\n",
      "Iteration 1658, loss = 0.16976559\n",
      "Iteration 1483, loss = 0.16358310\n",
      "Iteration 1659, loss = 0.16953108\n",
      "Iteration 2261, loss = 0.15547890\n",
      "Iteration 1018, loss = 0.29113001\n",
      "Iteration 1660, loss = 0.16937595\n",
      "Iteration 509, loss = 0.36096939\n",
      "Iteration 2262, loss = 0.15526572\n",
      "Iteration 1163, loss = 0.28159446\n",
      "Iteration 1484, loss = 0.16341905\n",
      "Iteration 1661, loss = 0.16914385\n",
      "Iteration 1019, loss = 0.29098751\n",
      "Iteration 490, loss = 0.36732602\n",
      "Iteration 760, loss = 0.33051349\n",
      "Iteration 2263, loss = 0.15523559\n",
      "Iteration 1485, loss = 0.16317375\n",
      "Iteration 1020, loss = 0.29087125\n",
      "Iteration 1662, loss = 0.16902174\n",
      "Iteration 510, loss = 0.36077166\n",
      "Iteration 1164, loss = 0.28143843\n",
      "Iteration 1021, loss = 0.29084112\n",
      "Iteration 1663, loss = 0.16881944\n",
      "Iteration 2264, loss = 0.15518360\n",
      "Iteration 1022, loss = 0.29066547\n",
      "Iteration 1664, loss = 0.16863631\n",
      "Iteration 491, loss = 0.36717018\n",
      "Iteration 2265, loss = 0.15506029\n",
      "Iteration 511, loss = 0.36065043\n",
      "Iteration 1023, loss = 0.29055193\n",
      "Iteration 761, loss = 0.33029656\n",
      "Iteration 1665, loss = 0.16850910\n",
      "Iteration 1024, loss = 0.29045764\n",
      "Iteration 2266, loss = 0.15496627\n",
      "Iteration 1165, loss = 0.28133052\n",
      "Iteration 492, loss = 0.36707512\n",
      "Iteration 512, loss = 0.36049747\n",
      "Iteration 1666, loss = 0.16838621\n",
      "Iteration 1486, loss = 0.16310420\n",
      "Iteration 762, loss = 0.33017609\n",
      "Iteration 1667, loss = 0.16824017\n",
      "Iteration 1025, loss = 0.29030973\n",
      "Iteration 493, loss = 0.36692148\n",
      "Iteration 2267, loss = 0.15482473\n",
      "Iteration 1668, loss = 0.16836876\n",
      "Iteration 1487, loss = 0.16295135\n",
      "Iteration 1166, loss = 0.28119202\n",
      "Iteration 2268, loss = 0.15478756\n",
      "Iteration 1669, loss = 0.16787914\n",
      "Iteration 1026, loss = 0.29021388\n",
      "Iteration 513, loss = 0.36032657\n",
      "Iteration 1670, loss = 0.16780267\n",
      "Iteration 1488, loss = 0.16298644\n",
      "Iteration 1027, loss = 0.29006441\n",
      "Iteration 1671, loss = 0.16757645\n",
      "Iteration 1672, loss = 0.16744491\n",
      "Iteration 2269, loss = 0.15467804\n",
      "Iteration 494, loss = 0.36682440\n",
      "Iteration 1489, loss = 0.16276727\n",
      "Iteration 1673, loss = 0.16732203\n",
      "Iteration 1674, loss = 0.16719412\n",
      "Iteration 514, loss = 0.36013292\n",
      "Iteration 1490, loss = 0.16251504\n",
      "Iteration 1028, loss = 0.28991502\n",
      "Iteration 2270, loss = 0.15457166\n",
      "Iteration 1167, loss = 0.28106346\n",
      "Iteration 495, loss = 0.36667174\n",
      "Iteration 763, loss = 0.32994562\n",
      "Iteration 1675, loss = 0.16708942\n",
      "Iteration 1491, loss = 0.16236194\n",
      "Iteration 1676, loss = 0.16691711\n",
      "Iteration 2271, loss = 0.15455045\n",
      "Iteration 515, loss = 0.35999768\n",
      "Iteration 1677, loss = 0.16668653\n",
      "Iteration 1029, loss = 0.28981372\n",
      "Iteration 516, loss = 0.35980277\n",
      "Iteration 2272, loss = 0.15447380\n",
      "Iteration 1678, loss = 0.16670816\n",
      "Iteration 1492, loss = 0.16226119\n",
      "Iteration 1168, loss = 0.28092813\n",
      "Iteration 1679, loss = 0.16640552\n",
      "Iteration 2273, loss = 0.15437453\n",
      "Iteration 1680, loss = 0.16625991\n",
      "Iteration 496, loss = 0.36656254\n",
      "Iteration 1681, loss = 0.16657018\n",
      "Iteration 1030, loss = 0.28970862\n",
      "Iteration 1682, loss = 0.16609416\n",
      "Iteration 1493, loss = 0.16208983\n",
      "Iteration 2274, loss = 0.15424450\n",
      "Iteration 1169, loss = 0.28083745\n",
      "Iteration 1683, loss = 0.16582507\n",
      "Iteration 1031, loss = 0.28957859\n",
      "Iteration 764, loss = 0.32974730\n",
      "Iteration 2275, loss = 0.15420300\n",
      "Iteration 517, loss = 0.35972263\n",
      "Iteration 1684, loss = 0.16567794\n",
      "Iteration 2276, loss = 0.15406000\n",
      "Iteration 497, loss = 0.36642746\n",
      "Iteration 518, loss = 0.35956039\n",
      "Iteration 1032, loss = 0.28948978\n",
      "Iteration 1685, loss = 0.16567805\n",
      "Iteration 2277, loss = 0.15404613\n",
      "Iteration 519, loss = 0.35935091\n",
      "Iteration 1494, loss = 0.16194032\n",
      "Iteration 498, loss = 0.36629528\n",
      "Iteration 2278, loss = 0.15389791\n",
      "Iteration 765, loss = 0.32939664\n",
      "Iteration 1170, loss = 0.28076145\n",
      "Iteration 1686, loss = 0.16551291\n",
      "Iteration 520, loss = 0.35922619\n",
      "Iteration 2279, loss = 0.15383743\n",
      "Iteration 1033, loss = 0.28933979\n",
      "Iteration 521, loss = 0.35903067\n",
      "Iteration 1687, loss = 0.16519341\n",
      "Iteration 1495, loss = 0.16179406\n",
      "Iteration 2280, loss = 0.15374632\n",
      "Iteration 1688, loss = 0.16509385\n",
      "Iteration 522, loss = 0.35891608\n",
      "Iteration 1689, loss = 0.16489908\n",
      "Iteration 2281, loss = 0.15367676\n",
      "Iteration 1690, loss = 0.16475665\n",
      "Iteration 499, loss = 0.36619546\n",
      "Iteration 1034, loss = 0.28920843\n",
      "Iteration 1691, loss = 0.16469998\n",
      "Iteration 766, loss = 0.32917028\n",
      "Iteration 1496, loss = 0.16177342\n",
      "Iteration 1171, loss = 0.28059787\n",
      "Iteration 1692, loss = 0.16466024\n",
      "Iteration 2282, loss = 0.15355858\n",
      "Iteration 1693, loss = 0.16442934\n",
      "Iteration 1035, loss = 0.28912615\n",
      "Iteration 1694, loss = 0.16426873\n",
      "Iteration 523, loss = 0.35878974\n",
      "Iteration 1036, loss = 0.28900033\n",
      "Iteration 1695, loss = 0.16400736\n",
      "Iteration 767, loss = 0.32905053\n",
      "Iteration 1172, loss = 0.28048291\n",
      "Iteration 1696, loss = 0.16377915\n",
      "Iteration 2283, loss = 0.15346899\n",
      "Iteration 1497, loss = 0.16163209\n",
      "Iteration 500, loss = 0.36607784\n",
      "Iteration 2284, loss = 0.15344608\n",
      "Iteration 1037, loss = 0.28889837\n",
      "Iteration 1697, loss = 0.16372844\n",
      "Iteration 524, loss = 0.35862779\n",
      "Iteration 1498, loss = 0.16140753\n",
      "Iteration 1698, loss = 0.16351017\n",
      "Iteration 2285, loss = 0.15342526\n",
      "Iteration 501, loss = 0.36592970\n",
      "Iteration 1173, loss = 0.28032956\n",
      "Iteration 1699, loss = 0.16343769\n",
      "Iteration 525, loss = 0.35840290\n",
      "Iteration 1038, loss = 0.28875046\n",
      "Iteration 1700, loss = 0.16342263\n",
      "Iteration 768, loss = 0.32871527\n",
      "Iteration 1701, loss = 0.16311776\n",
      "Iteration 1039, loss = 0.28863571\n",
      "Iteration 2286, loss = 0.15325387\n",
      "Iteration 1499, loss = 0.16156840\n",
      "Iteration 1174, loss = 0.28022628\n",
      "Iteration 1040, loss = 0.28858215\n",
      "Iteration 502, loss = 0.36584016\n",
      "Iteration 526, loss = 0.35821915\n",
      "Iteration 1702, loss = 0.16290895\n",
      "Iteration 1041, loss = 0.28841083\n",
      "Iteration 2287, loss = 0.15313812\n",
      "Iteration 1500, loss = 0.16131527\n",
      "Iteration 1703, loss = 0.16284767\n",
      "Iteration 1704, loss = 0.16262597\n",
      "Iteration 527, loss = 0.35807334\n",
      "Iteration 769, loss = 0.32852342\n",
      "Iteration 2288, loss = 0.15305154\n",
      "Iteration 1705, loss = 0.16247350\n",
      "Iteration 503, loss = 0.36566840\n",
      "Iteration 1042, loss = 0.28829016\n",
      "Iteration 2289, loss = 0.15328587\n",
      "Iteration 1501, loss = 0.16100572\n",
      "Iteration 1706, loss = 0.16226426\n",
      "Iteration 528, loss = 0.35790606\n",
      "Iteration 1175, loss = 0.28006576\n",
      "Iteration 2290, loss = 0.15287723\n",
      "Iteration 1502, loss = 0.16072157\n",
      "Iteration 1707, loss = 0.16223533\n",
      "Iteration 1043, loss = 0.28819197\n",
      "Iteration 529, loss = 0.35773442\n",
      "Iteration 504, loss = 0.36555611\n",
      "Iteration 770, loss = 0.32833563\n",
      "Iteration 1708, loss = 0.16219101\n",
      "Iteration 2291, loss = 0.15277370\n",
      "Iteration 1503, loss = 0.16054861\n",
      "Iteration 1044, loss = 0.28805562\n",
      "Iteration 1709, loss = 0.16190364\n",
      "Iteration 1710, loss = 0.16172519\n",
      "Iteration 1176, loss = 0.27996912\n",
      "Iteration 1504, loss = 0.16045282\n",
      "Iteration 530, loss = 0.35758273\n",
      "Iteration 505, loss = 0.36541697\n",
      "Iteration 1045, loss = 0.28796143\n",
      "Iteration 1711, loss = 0.16157797\n",
      "Iteration 1046, loss = 0.28787704\n",
      "Iteration 2292, loss = 0.15284133\n",
      "Iteration 1505, loss = 0.16028423\n",
      "Iteration 1177, loss = 0.27986196\n",
      "Iteration 1712, loss = 0.16138347\n",
      "Iteration 531, loss = 0.35743534\n",
      "Iteration 1713, loss = 0.16123075Iteration 1047, loss = 0.28772962\n",
      "Iteration 532, loss = 0.35729309\n",
      "Iteration 1178, loss = 0.27969656\n",
      "Iteration 771, loss = 0.32809349\n",
      "\n",
      "Iteration 2293, loss = 0.15260766\n",
      "Iteration 1506, loss = 0.16020874\n",
      "Iteration 1048, loss = 0.28760740\n",
      "Iteration 1714, loss = 0.16107390\n",
      "Iteration 506, loss = 0.36536217\n",
      "Iteration 1715, loss = 0.16093155\n",
      "Iteration 533, loss = 0.35710765\n",
      "Iteration 2294, loss = 0.15269990\n",
      "Iteration 1716, loss = 0.16101541\n",
      "Iteration 1717, loss = 0.16064712\n",
      "Iteration 1049, loss = 0.28748921\n",
      "Iteration 534, loss = 0.35701286\n",
      "Iteration 772, loss = 0.32781247\n",
      "Iteration 1179, loss = 0.27965416\n",
      "Iteration 2295, loss = 0.15248320\n",
      "Iteration 1718, loss = 0.16048017\n",
      "Iteration 1050, loss = 0.28736987\n",
      "Iteration 1719, loss = 0.16042298\n",
      "Iteration 1507, loss = 0.16006255\n",
      "Iteration 507, loss = 0.36520001\n",
      "Iteration 535, loss = 0.35676090\n",
      "Iteration 1051, loss = 0.28730545\n",
      "Iteration 2296, loss = 0.15252992\n",
      "Iteration 1180, loss = 0.27944452\n",
      "Iteration 1720, loss = 0.16015879\n",
      "Iteration 1052, loss = 0.28711678\n",
      "Iteration 536, loss = 0.35673337\n",
      "Iteration 2297, loss = 0.15224022\n",
      "Iteration 1508, loss = 0.16004502\n",
      "Iteration 1721, loss = 0.16007203\n",
      "Iteration 1722, loss = 0.15992415\n",
      "Iteration 537, loss = 0.35644203\n",
      "Iteration 1053, loss = 0.28702274\n",
      "Iteration 773, loss = 0.32758858\n",
      "Iteration 1723, loss = 0.15988416\n",
      "Iteration 1181, loss = 0.27933866\n",
      "Iteration 1724, loss = 0.15955089\n",
      "Iteration 2298, loss = 0.15219096\n",
      "Iteration 1725, loss = 0.15940781\n",
      "Iteration 1509, loss = 0.15972932\n",
      "Iteration 1054, loss = 0.28689025\n",
      "Iteration 1726, loss = 0.15937210\n",
      "Iteration 538, loss = 0.35632043\n",
      "Iteration 1055, loss = 0.28676372\n",
      "Iteration 1182, loss = 0.27920880\n",
      "Iteration 2299, loss = 0.15212146\n",
      "Iteration 1056, loss = 0.28668351\n",
      "Iteration 508, loss = 0.36503709\n",
      "Iteration 774, loss = 0.32744205\n",
      "Iteration 1727, loss = 0.15915497\n",
      "Iteration 539, loss = 0.35614391\n",
      "Iteration 1510, loss = 0.16001253\n",
      "Iteration 2300, loss = 0.15201790\n",
      "Iteration 1183, loss = 0.27908209\n",
      "Iteration 1057, loss = 0.28653183\n",
      "Iteration 1728, loss = 0.15903438\n",
      "Iteration 540, loss = 0.35598873\n",
      "Iteration 1058, loss = 0.28643118\n",
      "Iteration 1511, loss = 0.15961589\n",
      "Iteration 1729, loss = 0.15894436\n",
      "Iteration 1059, loss = 0.28629754\n",
      "Iteration 2301, loss = 0.15192867\n",
      "Iteration 1730, loss = 0.15864028\n",
      "Iteration 1512, loss = 0.15930519\n",
      "Iteration 1731, loss = 0.15857921\n",
      "Iteration 1060, loss = 0.28617160\n",
      "Iteration 2302, loss = 0.15201322\n",
      "Iteration 541, loss = 0.35585863\n",
      "Iteration 509, loss = 0.36494609\n",
      "Iteration 1732, loss = 0.15832487\n",
      "Iteration 775, loss = 0.32720693\n",
      "Iteration 1061, loss = 0.28604677\n",
      "Iteration 2303, loss = 0.15171846\n",
      "Iteration 1513, loss = 0.15927738\n",
      "Iteration 1733, loss = 0.15830145\n",
      "Iteration 1062, loss = 0.28595078\n",
      "Iteration 2304, loss = 0.15163625\n",
      "Iteration 1184, loss = 0.27902466\n",
      "Iteration 542, loss = 0.35569130\n",
      "Iteration 1063, loss = 0.28586915\n",
      "Iteration 2305, loss = 0.15165163\n",
      "Iteration 1514, loss = 0.15927565\n",
      "Iteration 1064, loss = 0.28570764\n",
      "Iteration 543, loss = 0.35552691\n",
      "Iteration 2306, loss = 0.15150713\n",
      "Iteration 510, loss = 0.36480205\n",
      "Iteration 1065, loss = 0.28559314\n",
      "Iteration 776, loss = 0.32707198\n",
      "Iteration 2307, loss = 0.15141457\n",
      "Iteration 1185, loss = 0.27883766\n",
      "Iteration 1734, loss = 0.15813397\n",
      "Iteration 1066, loss = 0.28549556\n",
      "Iteration 544, loss = 0.35540014\n",
      "Iteration 511, loss = 0.36470230\n",
      "Iteration 1067, loss = 0.28539077\n",
      "Iteration 1515, loss = 0.15887696\n",
      "Iteration 1735, loss = 0.15787353\n",
      "Iteration 2308, loss = 0.15132020\n",
      "Iteration 1736, loss = 0.15775639\n",
      "Iteration 1068, loss = 0.28521455\n",
      "Iteration 545, loss = 0.35516268\n",
      "Iteration 1186, loss = 0.27872968\n",
      "Iteration 1737, loss = 0.15806751\n",
      "Iteration 1738, loss = 0.15746076\n",
      "Iteration 512, loss = 0.36462934\n",
      "Iteration 2309, loss = 0.15126305\n",
      "Iteration 546, loss = 0.35504207\n",
      "Iteration 1069, loss = 0.28510564\n",
      "Iteration 1516, loss = 0.15887806\n",
      "Iteration 2310, loss = 0.15117402\n",
      "Iteration 1739, loss = 0.15728792\n",
      "Iteration 777, loss = 0.32678251\n",
      "Iteration 1187, loss = 0.27859863\n",
      "Iteration 1740, loss = 0.15724732\n",
      "Iteration 1517, loss = 0.15872283\n",
      "Iteration 1741, loss = 0.15702628\n",
      "Iteration 2311, loss = 0.15123391\n",
      "Iteration 1742, loss = 0.15693264\n",
      "Iteration 547, loss = 0.35488681\n",
      "Iteration 1070, loss = 0.28498134\n",
      "Iteration 1518, loss = 0.15868440\n",
      "Iteration 1743, loss = 0.15673323\n",
      "Iteration 2312, loss = 0.15104047\n",
      "Iteration 513, loss = 0.36444090\n",
      "Iteration 1744, loss = 0.15659339\n",
      "Iteration 2313, loss = 0.15099031\n",
      "Iteration 1745, loss = 0.15648338\n",
      "Iteration 778, loss = 0.32653802\n",
      "Iteration 1519, loss = 0.15838312\n",
      "Iteration 1188, loss = 0.27848577\n",
      "Iteration 2314, loss = 0.15083062\n",
      "Iteration 1071, loss = 0.28491849\n",
      "Iteration 2315, loss = 0.15074095\n",
      "Iteration 1189, loss = 0.27835795\n",
      "Iteration 514, loss = 0.36431654\n",
      "Iteration 1746, loss = 0.15622293\n",
      "Iteration 1072, loss = 0.28473236\n",
      "Iteration 1747, loss = 0.15611498Iteration 1190, loss = 0.27829920\n",
      "Iteration 548, loss = 0.35471361\n",
      "Iteration 515, loss = 0.36420748\n",
      "\n",
      "Iteration 1191, loss = 0.27814209\n",
      "Iteration 1520, loss = 0.15839378\n",
      "Iteration 2316, loss = 0.15072045\n",
      "Iteration 1748, loss = 0.15592301\n",
      "Iteration 1073, loss = 0.28459533\n",
      "Iteration 1192, loss = 0.27798893\n",
      "Iteration 549, loss = 0.35454295\n",
      "Iteration 1749, loss = 0.15591262\n",
      "Iteration 2317, loss = 0.15057465\n",
      "Iteration 1521, loss = 0.15820318\n",
      "Iteration 2318, loss = 0.15059087\n",
      "Iteration 2319, loss = 0.15040316\n",
      "Iteration 1522, loss = 0.15792575\n",
      "Iteration 1750, loss = 0.15560470\n",
      "Iteration 550, loss = 0.35441325\n",
      "Iteration 1751, loss = 0.15546544\n",
      "Iteration 779, loss = 0.32629886\n",
      "Iteration 516, loss = 0.36406557\n",
      "Iteration 1752, loss = 0.15563183\n",
      "Iteration 1074, loss = 0.28453538\n",
      "Iteration 1523, loss = 0.15824035\n",
      "Iteration 1753, loss = 0.15515088\n",
      "Iteration 1754, loss = 0.15510327\n",
      "Iteration 1075, loss = 0.28439449\n",
      "Iteration 551, loss = 0.35427773\n",
      "Iteration 1524, loss = 0.15764590\n",
      "Iteration 2320, loss = 0.15070639\n",
      "Iteration 1755, loss = 0.15503457\n",
      "Iteration 1756, loss = 0.15471052\n",
      "Iteration 1525, loss = 0.15757053\n",
      "Iteration 1526, loss = 0.15740697\n",
      "Iteration 1076, loss = 0.28428342\n",
      "Iteration 1757, loss = 0.15462653\n",
      "Iteration 552, loss = 0.35412736\n",
      "Iteration 780, loss = 0.32601747\n",
      "Iteration 517, loss = 0.36394765\n",
      "Iteration 1193, loss = 0.27788086\n",
      "Iteration 553, loss = 0.35403435\n",
      "Iteration 1194, loss = 0.27774362\n",
      "Iteration 2321, loss = 0.15027195\n",
      "Iteration 1758, loss = 0.15446155\n",
      "Iteration 518, loss = 0.36380737\n",
      "Iteration 554, loss = 0.35377340\n",
      "Iteration 1077, loss = 0.28416039\n",
      "Iteration 1527, loss = 0.15726536\n",
      "Iteration 2322, loss = 0.15016711\n",
      "Iteration 1759, loss = 0.15427897\n",
      "Iteration 1078, loss = 0.28402119\n",
      "Iteration 1195, loss = 0.27760625\n",
      "Iteration 1079, loss = 0.28395215\n",
      "Iteration 1528, loss = 0.15710436\n",
      "Iteration 1760, loss = 0.15413635\n",
      "Iteration 519, loss = 0.36374163\n",
      "Iteration 2323, loss = 0.15012717\n",
      "Iteration 1080, loss = 0.28378133\n",
      "Iteration 1196, loss = 0.27749850\n",
      "Iteration 555, loss = 0.35358951\n",
      "Iteration 1529, loss = 0.15705952\n",
      "Iteration 1761, loss = 0.15405062\n",
      "Iteration 1081, loss = 0.28365884\n",
      "Iteration 2324, loss = 0.15003539\n",
      "Iteration 1762, loss = 0.15398823\n",
      "Iteration 781, loss = 0.32577490\n",
      "Iteration 556, loss = 0.35340291\n",
      "Iteration 2325, loss = 0.14998280\n",
      "Iteration 1763, loss = 0.15363193\n",
      "Iteration 2326, loss = 0.14984995\n",
      "Iteration 557, loss = 0.35328515\n",
      "Iteration 1764, loss = 0.15348025\n",
      "Iteration 1082, loss = 0.28358710\n",
      "Iteration 1197, loss = 0.27738025\n",
      "Iteration 1765, loss = 0.15337017\n",
      "Iteration 520, loss = 0.36358360\n",
      "Iteration 782, loss = 0.32558217\n",
      "Iteration 1530, loss = 0.15684777\n",
      "Iteration 558, loss = 0.35308328\n",
      "Iteration 2327, loss = 0.14977139\n",
      "Iteration 1766, loss = 0.15319090\n",
      "Iteration 1083, loss = 0.28342336\n",
      "Iteration 1767, loss = 0.15297656\n",
      "Iteration 559, loss = 0.35293677\n",
      "Iteration 1084, loss = 0.28327021\n",
      "Iteration 2328, loss = 0.14968151\n",
      "Iteration 521, loss = 0.36345638\n",
      "Iteration 1768, loss = 0.15293278\n",
      "Iteration 1198, loss = 0.27724443\n",
      "Iteration 1769, loss = 0.15280850\n",
      "Iteration 2329, loss = 0.14961743\n",
      "Iteration 560, loss = 0.35276106\n",
      "Iteration 1770, loss = 0.15262282\n",
      "Iteration 1531, loss = 0.15677761\n",
      "Iteration 2330, loss = 0.14953819\n",
      "Iteration 522, loss = 0.36333478\n",
      "Iteration 783, loss = 0.32534781\n",
      "Iteration 1771, loss = 0.15240120\n",
      "Iteration 1532, loss = 0.15679834\n",
      "Iteration 1199, loss = 0.27710820\n",
      "Iteration 1085, loss = 0.28320779\n",
      "Iteration 1772, loss = 0.15231656\n",
      "Iteration 561, loss = 0.35260818\n",
      "Iteration 1773, loss = 0.15226654\n",
      "Iteration 1533, loss = 0.15646959\n",
      "Iteration 2331, loss = 0.14943186\n",
      "Iteration 1774, loss = 0.15192486\n",
      "Iteration 523, loss = 0.36321211\n",
      "Iteration 562, loss = 0.35251590\n",
      "Iteration 1775, loss = 0.15177801\n",
      "Iteration 1086, loss = 0.28303849\n",
      "Iteration 2332, loss = 0.14931047\n",
      "Iteration 1776, loss = 0.15171632\n",
      "Iteration 784, loss = 0.32511947\n",
      "Iteration 1087, loss = 0.28292069\n",
      "Iteration 1777, loss = 0.15153599\n",
      "Iteration 2333, loss = 0.14927180\n",
      "Iteration 1534, loss = 0.15636560\n",
      "Iteration 563, loss = 0.35235781\n",
      "Iteration 1778, loss = 0.15138358\n",
      "Iteration 1200, loss = 0.27701494\n",
      "Iteration 1779, loss = 0.15113002\n",
      "Iteration 2334, loss = 0.14916636\n",
      "Iteration 1780, loss = 0.15106038Iteration 1088, loss = 0.28281606\n",
      "\n",
      "Iteration 1201, loss = 0.27686360\n",
      "Iteration 2335, loss = 0.14934974\n",
      "Iteration 524, loss = 0.36309958Iteration 785, loss = 0.32492950\n",
      "\n",
      "Iteration 1535, loss = 0.15629376\n",
      "Iteration 1781, loss = 0.15089555\n",
      "Iteration 1089, loss = 0.28266751\n",
      "Iteration 2336, loss = 0.14904959\n",
      "Iteration 564, loss = 0.35220608\n",
      "Iteration 525, loss = 0.36296654\n",
      "Iteration 1536, loss = 0.15621936\n",
      "Iteration 1782, loss = 0.15067714\n",
      "Iteration 1090, loss = 0.28255612\n",
      "Iteration 2337, loss = 0.14899293\n",
      "Iteration 565, loss = 0.35196470\n",
      "Iteration 1091, loss = 0.28245682\n",
      "Iteration 1783, loss = 0.15057668\n",
      "Iteration 2338, loss = 0.14892266\n",
      "Iteration 1784, loss = 0.15038212\n",
      "Iteration 1202, loss = 0.27673434\n",
      "Iteration 566, loss = 0.35181507\n",
      "Iteration 526, loss = 0.36291796\n",
      "Iteration 1092, loss = 0.28230356\n",
      "Iteration 2339, loss = 0.14884585\n",
      "Iteration 1537, loss = 0.15585571\n",
      "Iteration 1093, loss = 0.28219952\n",
      "Iteration 2340, loss = 0.14868467\n",
      "Iteration 1785, loss = 0.15049927\n",
      "Iteration 786, loss = 0.32465230\n",
      "Iteration 1538, loss = 0.15599344\n",
      "Iteration 2341, loss = 0.14860944\n",
      "Iteration 1203, loss = 0.27666550\n",
      "Iteration 567, loss = 0.35166736\n",
      "Iteration 1786, loss = 0.15010110\n",
      "Iteration 527, loss = 0.36273016\n",
      "Iteration 1094, loss = 0.28208604\n",
      "Iteration 1787, loss = 0.14998284\n",
      "Iteration 2342, loss = 0.14861328\n",
      "Iteration 1788, loss = 0.14970678\n",
      "Iteration 1095, loss = 0.28197035\n",
      "Iteration 1789, loss = 0.14960389\n",
      "Iteration 1539, loss = 0.15575129\n",
      "Iteration 1790, loss = 0.14949196\n",
      "Iteration 787, loss = 0.32454886\n",
      "Iteration 2343, loss = 0.14852126\n",
      "Iteration 528, loss = 0.36259036\n",
      "Iteration 568, loss = 0.35148571\n",
      "Iteration 1791, loss = 0.14926254\n",
      "Iteration 1204, loss = 0.27651279\n",
      "Iteration 2344, loss = 0.14842139\n",
      "Iteration 1096, loss = 0.28181754\n",
      "Iteration 569, loss = 0.35129577\n",
      "Iteration 1792, loss = 0.14928311\n",
      "Iteration 2345, loss = 0.14829983\n",
      "Iteration 788, loss = 0.32426527\n",
      "Iteration 529, loss = 0.36247239\n",
      "Iteration 570, loss = 0.35115932\n",
      "Iteration 1205, loss = 0.27646641\n",
      "Iteration 1793, loss = 0.14894528\n",
      "Iteration 1097, loss = 0.28176909\n",
      "Iteration 1540, loss = 0.15561088\n",
      "Iteration 2346, loss = 0.14818300\n",
      "Iteration 1794, loss = 0.14875096\n",
      "Iteration 571, loss = 0.35098229\n",
      "Iteration 1795, loss = 0.14868005\n",
      "Iteration 1098, loss = 0.28161288\n",
      "Iteration 1541, loss = 0.15543567\n",
      "Iteration 1796, loss = 0.14874914\n",
      "Iteration 1206, loss = 0.27635243\n",
      "Iteration 2347, loss = 0.14828857\n",
      "Iteration 1099, loss = 0.28149602\n",
      "Iteration 1797, loss = 0.14843720\n",
      "Iteration 530, loss = 0.36238780\n",
      "Iteration 1798, loss = 0.14816030\n",
      "Iteration 1100, loss = 0.28137237\n",
      "Iteration 1542, loss = 0.15528473\n",
      "Iteration 789, loss = 0.32399824\n",
      "Iteration 1799, loss = 0.14834218\n",
      "Iteration 572, loss = 0.35092608\n",
      "Iteration 1101, loss = 0.28127779\n",
      "Iteration 1800, loss = 0.14792499\n",
      "Iteration 2348, loss = 0.14808421\n",
      "Iteration 531, loss = 0.36227044\n",
      "Iteration 1102, loss = 0.28111670\n",
      "Iteration 1207, loss = 0.27611467\n",
      "Iteration 2349, loss = 0.14795579\n",
      "Iteration 573, loss = 0.35065127\n",
      "Iteration 1801, loss = 0.14768828\n",
      "Iteration 1103, loss = 0.28103460\n",
      "Iteration 1802, loss = 0.14757846\n",
      "Iteration 1543, loss = 0.15504960\n",
      "Iteration 1803, loss = 0.14737909\n",
      "Iteration 790, loss = 0.32374547\n",
      "Iteration 2350, loss = 0.14789302\n",
      "Iteration 1104, loss = 0.28086397\n",
      "Iteration 574, loss = 0.35048497\n",
      "Iteration 1544, loss = 0.15499325\n",
      "Iteration 1804, loss = 0.14728665\n",
      "Iteration 1208, loss = 0.27596832\n",
      "Iteration 532, loss = 0.36212385\n",
      "Iteration 1805, loss = 0.14720734\n",
      "Iteration 1545, loss = 0.15491090\n",
      "Iteration 575, loss = 0.35035726\n",
      "Iteration 1806, loss = 0.14709017\n",
      "Iteration 1807, loss = 0.14709664\n",
      "Iteration 2351, loss = 0.14781247\n",
      "Iteration 1546, loss = 0.15479927\n",
      "Iteration 1808, loss = 0.14660386\n",
      "Iteration 791, loss = 0.32356643\n",
      "Iteration 1809, loss = 0.14652366\n",
      "Iteration 1105, loss = 0.28074497\n",
      "Iteration 1810, loss = 0.14628665\n",
      "Iteration 576, loss = 0.35016221\n",
      "Iteration 1547, loss = 0.15460360\n",
      "Iteration 1811, loss = 0.14617375\n",
      "Iteration 2352, loss = 0.14772680\n",
      "Iteration 1106, loss = 0.28062910\n",
      "Iteration 1812, loss = 0.14595209\n",
      "Iteration 533, loss = 0.36199215\n",
      "Iteration 1209, loss = 0.27597402\n",
      "Iteration 1548, loss = 0.15455784\n",
      "Iteration 1107, loss = 0.28049192\n",
      "Iteration 1813, loss = 0.14579433\n",
      "Iteration 2353, loss = 0.14768268\n",
      "Iteration 1814, loss = 0.14573458\n",
      "Iteration 1210, loss = 0.27578564\n",
      "Iteration 1549, loss = 0.15463253\n",
      "Iteration 577, loss = 0.35008265\n",
      "Iteration 1815, loss = 0.14554851\n",
      "Iteration 534, loss = 0.36193595\n",
      "Iteration 1816, loss = 0.14541270\n",
      "Iteration 1108, loss = 0.28038045\n",
      "Iteration 2354, loss = 0.14753663\n",
      "Iteration 1817, loss = 0.14524928\n",
      "Iteration 792, loss = 0.32343677\n",
      "Iteration 1550, loss = 0.15424918\n",
      "Iteration 2355, loss = 0.14749087Iteration 1818, loss = 0.14512950\n",
      "\n",
      "Iteration 1109, loss = 0.28024027\n",
      "Iteration 578, loss = 0.34984027\n",
      "Iteration 535, loss = 0.36175423\n",
      "Iteration 1819, loss = 0.14488938\n",
      "Iteration 1211, loss = 0.27561663\n",
      "Iteration 2356, loss = 0.14745890\n",
      "Iteration 1820, loss = 0.14498355\n",
      "Iteration 579, loss = 0.34968730\n",
      "Iteration 1551, loss = 0.15407779\n",
      "Iteration 1110, loss = 0.28012036\n",
      "Iteration 536, loss = 0.36168623\n",
      "Iteration 1821, loss = 0.14462817\n",
      "Iteration 580, loss = 0.34954983\n",
      "Iteration 1212, loss = 0.27556943\n",
      "Iteration 537, loss = 0.36150870\n",
      "Iteration 1822, loss = 0.14446055\n",
      "Iteration 2357, loss = 0.14727914\n",
      "Iteration 1552, loss = 0.15402087\n",
      "Iteration 1111, loss = 0.28000263\n",
      "Iteration 793, loss = 0.32304291\n",
      "Iteration 581, loss = 0.34950795\n",
      "Iteration 2358, loss = 0.14725565\n",
      "Iteration 1553, loss = 0.15387555\n",
      "Iteration 2359, loss = 0.14718282\n",
      "Iteration 582, loss = 0.34925643\n",
      "Iteration 1823, loss = 0.14428125\n",
      "Iteration 1554, loss = 0.15365747\n",
      "Iteration 1112, loss = 0.27987873\n",
      "Iteration 2360, loss = 0.14712877\n",
      "Iteration 1213, loss = 0.27537994\n",
      "Iteration 1824, loss = 0.14418788\n",
      "Iteration 2361, loss = 0.14706804\n",
      "Iteration 1555, loss = 0.15351442\n",
      "Iteration 538, loss = 0.36151512\n",
      "Iteration 2362, loss = 0.14689988\n",
      "Iteration 1825, loss = 0.14446291\n",
      "Iteration 794, loss = 0.32290759\n",
      "Iteration 1113, loss = 0.27979321\n",
      "Iteration 1826, loss = 0.14390591\n",
      "Iteration 1556, loss = 0.15342543\n",
      "Iteration 583, loss = 0.34905706\n",
      "Iteration 2363, loss = 0.14684372\n",
      "Iteration 1214, loss = 0.27528383\n",
      "Iteration 1114, loss = 0.27966273\n",
      "Iteration 1827, loss = 0.14380862\n",
      "Iteration 584, loss = 0.34888419\n",
      "Iteration 1115, loss = 0.27955133\n",
      "Iteration 795, loss = 0.32265905\n",
      "Iteration 1828, loss = 0.14351956\n",
      "Iteration 2364, loss = 0.14677206\n",
      "Iteration 585, loss = 0.34881013\n",
      "Iteration 1829, loss = 0.14347790\n",
      "Iteration 2365, loss = 0.14669915\n",
      "Iteration 1215, loss = 0.27511337\n",
      "Iteration 539, loss = 0.36128003\n",
      "Iteration 1830, loss = 0.14324721\n",
      "Iteration 1116, loss = 0.27942926\n",
      "Iteration 586, loss = 0.34856108\n",
      "Iteration 1831, loss = 0.14304976\n",
      "Iteration 2366, loss = 0.14659373\n",
      "Iteration 1832, loss = 0.14287211\n",
      "Iteration 1557, loss = 0.15337733\n",
      "Iteration 587, loss = 0.34843490\n",
      "Iteration 1833, loss = 0.14287892\n",
      "Iteration 1117, loss = 0.27931128\n",
      "Iteration 796, loss = 0.32251205\n",
      "Iteration 2367, loss = 0.14667525\n",
      "Iteration 1834, loss = 0.14295579\n",
      "Iteration 1835, loss = 0.14248909\n",
      "Iteration 2368, loss = 0.14646331\n",
      "Iteration 1558, loss = 0.15313424\n",
      "Iteration 1216, loss = 0.27507526\n",
      "Iteration 588, loss = 0.34821377\n",
      "Iteration 2369, loss = 0.14631971\n",
      "Iteration 1836, loss = 0.14229394\n",
      "Iteration 1559, loss = 0.15309692\n",
      "Iteration 540, loss = 0.36116356\n",
      "Iteration 2370, loss = 0.14635699\n",
      "Iteration 1560, loss = 0.15296598\n",
      "Iteration 1217, loss = 0.27494148\n",
      "Iteration 1837, loss = 0.14215116\n",
      "Iteration 2371, loss = 0.14624382\n",
      "Iteration 1561, loss = 0.15273404\n",
      "Iteration 1838, loss = 0.14206483\n",
      "Iteration 541, loss = 0.36103687\n",
      "Iteration 2372, loss = 0.14612593\n",
      "Iteration 1839, loss = 0.14187310\n",
      "Iteration 1562, loss = 0.15272132\n",
      "Iteration 797, loss = 0.32214610\n",
      "Iteration 1118, loss = 0.27918816\n",
      "Iteration 1840, loss = 0.14164543\n",
      "Iteration 589, loss = 0.34810352\n",
      "Iteration 2373, loss = 0.14601224\n",
      "Iteration 542, loss = 0.36091338\n",
      "Iteration 1841, loss = 0.14158510\n",
      "Iteration 1218, loss = 0.27482796\n",
      "Iteration 1842, loss = 0.14147962\n",
      "Iteration 1119, loss = 0.27903509\n",
      "Iteration 1563, loss = 0.15251429\n",
      "Iteration 1843, loss = 0.14118708\n",
      "Iteration 1219, loss = 0.27466068\n",
      "Iteration 1844, loss = 0.14108603\n",
      "Iteration 2374, loss = 0.14592495\n",
      "Iteration 1845, loss = 0.14111065\n",
      "Iteration 590, loss = 0.34791579\n",
      "Iteration 1564, loss = 0.15242290\n",
      "Iteration 1120, loss = 0.27893306\n",
      "Iteration 798, loss = 0.32201730\n",
      "Iteration 2375, loss = 0.14591224\n",
      "Iteration 1846, loss = 0.14077663\n",
      "Iteration 1220, loss = 0.27451400\n",
      "Iteration 1565, loss = 0.15233772\n",
      "Iteration 543, loss = 0.36082237\n",
      "Iteration 1121, loss = 0.27880606\n",
      "Iteration 591, loss = 0.34775934\n",
      "Iteration 1122, loss = 0.27869798\n",
      "Iteration 1566, loss = 0.15216521\n",
      "Iteration 2376, loss = 0.14578594\n",
      "Iteration 1123, loss = 0.27855985\n",
      "Iteration 592, loss = 0.34767101\n",
      "Iteration 799, loss = 0.32170633\n",
      "Iteration 1567, loss = 0.15196860\n",
      "Iteration 2377, loss = 0.14572763\n",
      "Iteration 1124, loss = 0.27842821\n",
      "Iteration 544, loss = 0.36066463\n",
      "Iteration 1847, loss = 0.14069005\n",
      "Iteration 2378, loss = 0.14562031\n",
      "Iteration 593, loss = 0.34744762\n",
      "Iteration 800, loss = 0.32156323\n",
      "Iteration 1568, loss = 0.15195359\n",
      "Iteration 1125, loss = 0.27835700\n",
      "Iteration 545, loss = 0.36054463\n",
      "Iteration 1848, loss = 0.14046038Iteration 2379, loss = 0.14561557\n",
      "\n",
      "Iteration 1569, loss = 0.15174501\n",
      "Iteration 801, loss = 0.32125704\n",
      "Iteration 1849, loss = 0.14037707\n",
      "Iteration 594, loss = 0.34728394\n",
      "Iteration 2380, loss = 0.14557532\n",
      "Iteration 1570, loss = 0.15175235\n",
      "Iteration 1850, loss = 0.14035784\n",
      "Iteration 1851, loss = 0.14001508\n",
      "Iteration 1852, loss = 0.13996961\n",
      "Iteration 1221, loss = 0.27437232\n",
      "Iteration 1126, loss = 0.27821967\n",
      "Iteration 1853, loss = 0.13979651\n",
      "Iteration 1854, loss = 0.13971334\n",
      "Iteration 1855, loss = 0.13945709\n",
      "Iteration 546, loss = 0.36043063\n",
      "Iteration 2381, loss = 0.14540915\n",
      "Iteration 1856, loss = 0.13930295\n",
      "Iteration 595, loss = 0.34708651\n",
      "Iteration 802, loss = 0.32112122\n",
      "Iteration 1222, loss = 0.27422673\n",
      "Iteration 2382, loss = 0.14531680\n",
      "Iteration 1857, loss = 0.13920206\n",
      "Iteration 1127, loss = 0.27807701\n",
      "Iteration 596, loss = 0.34699188\n",
      "Iteration 1128, loss = 0.27797636\n",
      "Iteration 1858, loss = 0.13893441\n",
      "Iteration 1223, loss = 0.27417312\n",
      "Iteration 1859, loss = 0.13880674\n",
      "Iteration 2383, loss = 0.14524962\n",
      "Iteration 547, loss = 0.36028829\n",
      "Iteration 1860, loss = 0.13875465\n",
      "Iteration 1861, loss = 0.13854662\n",
      "Iteration 1129, loss = 0.27790589\n",
      "Iteration 1571, loss = 0.15170841\n",
      "Iteration 1862, loss = 0.13839915\n",
      "Iteration 803, loss = 0.32082260\n",
      "Iteration 597, loss = 0.34680774\n",
      "Iteration 1863, loss = 0.13834497\n",
      "Iteration 1130, loss = 0.27774831\n",
      "Iteration 1224, loss = 0.27397980\n",
      "Iteration 1131, loss = 0.27759699\n",
      "Iteration 548, loss = 0.36020662\n",
      "Iteration 1572, loss = 0.15137550\n",
      "Iteration 1864, loss = 0.13820785\n",
      "Iteration 1865, loss = 0.13808352\n",
      "Iteration 2384, loss = 0.14514867\n",
      "Iteration 1866, loss = 0.13788488\n",
      "Iteration 1225, loss = 0.27386499\n",
      "Iteration 1132, loss = 0.27750364\n",
      "Iteration 1867, loss = 0.13762670\n",
      "Iteration 2385, loss = 0.14507644\n",
      "Iteration 598, loss = 0.34662241\n",
      "Iteration 1133, loss = 0.27734719\n",
      "Iteration 1573, loss = 0.15141800\n",
      "Iteration 1868, loss = 0.13771416\n",
      "Iteration 549, loss = 0.36005709\n",
      "Iteration 1869, loss = 0.13753207\n",
      "Iteration 2386, loss = 0.14499176\n",
      "Iteration 1574, loss = 0.15131547\n",
      "Iteration 1226, loss = 0.27375039\n",
      "Iteration 1134, loss = 0.27736223\n",
      "Iteration 1870, loss = 0.13752683\n",
      "Iteration 2387, loss = 0.14493519\n",
      "Iteration 1871, loss = 0.13732465\n",
      "Iteration 1135, loss = 0.27711139\n",
      "Iteration 1872, loss = 0.13698756\n",
      "Iteration 804, loss = 0.32061033\n",
      "Iteration 2388, loss = 0.14485809\n",
      "Iteration 1136, loss = 0.27708904\n",
      "Iteration 1575, loss = 0.15111001\n",
      "Iteration 550, loss = 0.35998477\n",
      "Iteration 1873, loss = 0.13679794\n",
      "Iteration 1137, loss = 0.27686472\n",
      "Iteration 2389, loss = 0.14483051\n",
      "Iteration 1874, loss = 0.13667014\n",
      "Iteration 1576, loss = 0.15095069\n",
      "Iteration 1875, loss = 0.13671642\n",
      "Iteration 1876, loss = 0.13644455\n",
      "Iteration 1577, loss = 0.15089790\n",
      "Iteration 1877, loss = 0.13620271\n",
      "Iteration 1227, loss = 0.27369186\n",
      "Iteration 805, loss = 0.32036167\n",
      "Iteration 2390, loss = 0.14470754\n",
      "Iteration 1138, loss = 0.27675069\n",
      "Iteration 1878, loss = 0.13608080\n",
      "Iteration 551, loss = 0.35984779\n",
      "Iteration 2391, loss = 0.14463053\n",
      "Iteration 599, loss = 0.34673457\n",
      "Iteration 2392, loss = 0.14452296\n",
      "Iteration 552, loss = 0.35969916\n",
      "Iteration 1578, loss = 0.15075339\n",
      "Iteration 1228, loss = 0.27349473\n",
      "Iteration 1139, loss = 0.27662513\n",
      "Iteration 2393, loss = 0.14446300\n",
      "Iteration 1140, loss = 0.27649103\n",
      "Iteration 1579, loss = 0.15066623\n",
      "Iteration 1879, loss = 0.13604233\n",
      "Iteration 1229, loss = 0.27337800\n",
      "Iteration 806, loss = 0.32011032\n",
      "Iteration 1580, loss = 0.15045007\n",
      "Iteration 2394, loss = 0.14441600\n",
      "Iteration 1880, loss = 0.13581519\n",
      "Iteration 553, loss = 0.35957676\n",
      "Iteration 2395, loss = 0.14449458\n",
      "Iteration 1881, loss = 0.13573731\n",
      "Iteration 1581, loss = 0.15025675\n",
      "Iteration 1141, loss = 0.27638278\n",
      "Iteration 1882, loss = 0.13555988\n",
      "Iteration 2396, loss = 0.14419944\n",
      "Iteration 1883, loss = 0.13544395\n",
      "Iteration 2397, loss = 0.14412305\n",
      "Iteration 1142, loss = 0.27625164\n",
      "Iteration 1884, loss = 0.13523480\n",
      "Iteration 554, loss = 0.35944395\n",
      "Iteration 1885, loss = 0.13508245\n",
      "Iteration 1143, loss = 0.27612245\n",
      "Iteration 2398, loss = 0.14404085\n",
      "Iteration 807, loss = 0.31986415\n",
      "Iteration 1886, loss = 0.13504923\n",
      "Iteration 2399, loss = 0.14401171\n",
      "Iteration 1230, loss = 0.27325293\n",
      "Iteration 1582, loss = 0.15014417\n",
      "Iteration 1887, loss = 0.13478540\n",
      "Iteration 2400, loss = 0.14395528\n",
      "Iteration 1888, loss = 0.13474108\n",
      "Iteration 1583, loss = 0.15000160\n",
      "Iteration 1889, loss = 0.13450339\n",
      "Iteration 1144, loss = 0.27604214\n",
      "Iteration 1231, loss = 0.27313941\n",
      "Iteration 2401, loss = 0.14388333\n",
      "Iteration 1890, loss = 0.13443592\n",
      "Iteration 1584, loss = 0.14998712\n",
      "Iteration 1145, loss = 0.27590142\n",
      "Iteration 1891, loss = 0.13435065\n",
      "Iteration 555, loss = 0.35934151\n",
      "Iteration 1585, loss = 0.14995140\n",
      "Iteration 1892, loss = 0.13404235\n",
      "Iteration 808, loss = 0.31966322\n",
      "Iteration 2402, loss = 0.14382461\n",
      "Iteration 1232, loss = 0.27302488\n",
      "Iteration 1146, loss = 0.27574703\n",
      "Iteration 1893, loss = 0.13400757\n",
      "Iteration 2403, loss = 0.14367172\n",
      "Iteration 1586, loss = 0.14983301\n",
      "Iteration 1147, loss = 0.27565859\n",
      "Iteration 1894, loss = 0.13382639\n",
      "Iteration 1895, loss = 0.13362566\n",
      "Iteration 1233, loss = 0.27289804\n",
      "Iteration 2404, loss = 0.14363141\n",
      "Iteration 1896, loss = 0.13351568\n",
      "Iteration 556, loss = 0.35929549\n",
      "Iteration 1897, loss = 0.13336543\n",
      "Iteration 1587, loss = 0.14959764\n",
      "Iteration 2405, loss = 0.14366058\n",
      "Iteration 1898, loss = 0.13320596\n",
      "Iteration 1148, loss = 0.27550766\n",
      "Iteration 809, loss = 0.31959731\n",
      "Iteration 1899, loss = 0.13305325\n",
      "Iteration 2406, loss = 0.14342794\n",
      "Iteration 1588, loss = 0.14998134\n",
      "Iteration 557, loss = 0.35910190\n",
      "Iteration 1900, loss = 0.13291476\n",
      "Iteration 1234, loss = 0.27276483\n",
      "Iteration 1901, loss = 0.13289000\n",
      "Iteration 1589, loss = 0.14932331\n",
      "Iteration 2407, loss = 0.14336714\n",
      "Iteration 1902, loss = 0.13263799\n",
      "Iteration 1149, loss = 0.27538890\n",
      "Iteration 1903, loss = 0.13249345\n",
      "Iteration 1150, loss = 0.27525484\n",
      "Iteration 1904, loss = 0.13242169\n",
      "Iteration 2408, loss = 0.14345938\n",
      "Iteration 1905, loss = 0.13229740\n",
      "Iteration 1151, loss = 0.27514765\n",
      "Iteration 1590, loss = 0.14923817\n",
      "Iteration 1906, loss = 0.13209092\n",
      "Iteration 558, loss = 0.35899822\n",
      "Iteration 1235, loss = 0.27267030\n",
      "Iteration 1152, loss = 0.27507279\n",
      "Iteration 2409, loss = 0.14322797\n",
      "Iteration 1907, loss = 0.13210743\n",
      "Iteration 1153, loss = 0.27488697\n",
      "Iteration 2410, loss = 0.14314090\n",
      "Iteration 810, loss = 0.31927960\n",
      "Iteration 1591, loss = 0.14909431\n",
      "Iteration 1908, loss = 0.13185643\n",
      "Iteration 1154, loss = 0.27476858\n",
      "Iteration 1909, loss = 0.13171792\n",
      "Iteration 1910, loss = 0.13157883\n",
      "Iteration 1236, loss = 0.27259200\n",
      "Iteration 1911, loss = 0.13147871\n",
      "Iteration 1155, loss = 0.27464393\n",
      "Iteration 1912, loss = 0.13123078\n",
      "Iteration 559, loss = 0.35886430\n",
      "Iteration 1592, loss = 0.14898746\n",
      "Iteration 2411, loss = 0.14308046\n",
      "Iteration 1913, loss = 0.13126977\n",
      "Iteration 1914, loss = 0.13096561\n",
      "Iteration 1915, loss = 0.13086857\n",
      "Iteration 1156, loss = 0.27458275\n",
      "Iteration 2412, loss = 0.14306405\n",
      "Iteration 560, loss = 0.35873839\n",
      "Iteration 1916, loss = 0.13077144\n",
      "Iteration 1593, loss = 0.14885611\n",
      "Iteration 2413, loss = 0.14292056\n",
      "Iteration 811, loss = 0.31898501\n",
      "Iteration 1237, loss = 0.27241593\n",
      "Iteration 1157, loss = 0.27440053\n",
      "Iteration 2414, loss = 0.14286287\n",
      "Iteration 2415, loss = 0.14274661\n",
      "Iteration 600, loss = 0.34639134\n",
      "Iteration 1158, loss = 0.27430134\n",
      "Iteration 2416, loss = 0.14262079\n",
      "Iteration 1917, loss = 0.13058464\n",
      "Iteration 1594, loss = 0.14884126\n",
      "Iteration 1159, loss = 0.27423602\n",
      "Iteration 1918, loss = 0.13046085\n",
      "Iteration 1238, loss = 0.27228267\n",
      "Iteration 1160, loss = 0.27408876\n",
      "Iteration 561, loss = 0.35860939\n",
      "Iteration 601, loss = 0.34618179\n",
      "Iteration 1161, loss = 0.27398677\n",
      "Iteration 1919, loss = 0.13028531\n",
      "Iteration 1920, loss = 0.13023831\n",
      "Iteration 1921, loss = 0.13020304\n",
      "Iteration 1922, loss = 0.13001053\n",
      "Iteration 602, loss = 0.34597412\n",
      "Iteration 1923, loss = 0.12980246\n",
      "Iteration 562, loss = 0.35849778\n",
      "Iteration 1924, loss = 0.12965338\n",
      "Iteration 1239, loss = 0.27215094\n",
      "Iteration 1925, loss = 0.12961935\n",
      "Iteration 2417, loss = 0.14259879\n",
      "Iteration 1162, loss = 0.27383601\n",
      "Iteration 1926, loss = 0.12954685\n",
      "Iteration 812, loss = 0.31876529\n",
      "Iteration 1595, loss = 0.14871712\n",
      "Iteration 1927, loss = 0.12922082\n",
      "Iteration 603, loss = 0.34589809\n",
      "Iteration 1928, loss = 0.12914515\n",
      "Iteration 2418, loss = 0.14248208\n",
      "Iteration 604, loss = 0.34566775\n",
      "Iteration 1163, loss = 0.27382305\n",
      "Iteration 2419, loss = 0.14252459\n",
      "Iteration 605, loss = 0.34548645\n",
      "Iteration 1596, loss = 0.14841100\n",
      "Iteration 2420, loss = 0.14258295\n",
      "Iteration 563, loss = 0.35838834\n",
      "Iteration 813, loss = 0.31858528\n",
      "Iteration 2421, loss = 0.14226105\n",
      "Iteration 1929, loss = 0.12899604\n",
      "Iteration 606, loss = 0.34533601\n",
      "Iteration 1164, loss = 0.27353787\n",
      "Iteration 1240, loss = 0.27202237\n",
      "Iteration 1930, loss = 0.12882950\n",
      "Iteration 1931, loss = 0.12881836\n",
      "Iteration 1165, loss = 0.27349524\n",
      "Iteration 1597, loss = 0.14843456\n",
      "Iteration 1932, loss = 0.12856843\n",
      "Iteration 1166, loss = 0.27332952\n",
      "Iteration 564, loss = 0.35826622\n",
      "Iteration 2422, loss = 0.14227844\n",
      "Iteration 607, loss = 0.34519217\n",
      "Iteration 1241, loss = 0.27194548\n",
      "Iteration 1167, loss = 0.27322528\n",
      "Iteration 1168, loss = 0.27309744\n",
      "Iteration 565, loss = 0.35813583\n",
      "Iteration 1598, loss = 0.14846566\n",
      "Iteration 1169, loss = 0.27298096\n",
      "Iteration 814, loss = 0.31835881\n",
      "Iteration 2423, loss = 0.14214698\n",
      "Iteration 1933, loss = 0.12854633\n",
      "Iteration 1170, loss = 0.27281784\n",
      "Iteration 566, loss = 0.35801847\n",
      "Iteration 1934, loss = 0.12830295\n",
      "Iteration 608, loss = 0.34502578\n",
      "Iteration 2424, loss = 0.14213681\n",
      "Iteration 1599, loss = 0.14815490\n",
      "Iteration 815, loss = 0.31818582\n",
      "Iteration 1242, loss = 0.27174794\n",
      "Iteration 2425, loss = 0.14196549\n",
      "Iteration 1171, loss = 0.27270034\n",
      "Iteration 609, loss = 0.34487072\n",
      "Iteration 1935, loss = 0.12827041\n",
      "Iteration 1600, loss = 0.14809421\n",
      "Iteration 567, loss = 0.35793587\n",
      "Iteration 2426, loss = 0.14190612\n",
      "Iteration 610, loss = 0.34467023Iteration 1936, loss = 0.12808184\n",
      "Iteration 1601, loss = 0.14793199\n",
      "Iteration 1243, loss = 0.27165303\n",
      "\n",
      "Iteration 2427, loss = 0.14181166\n",
      "Iteration 816, loss = 0.31785275\n",
      "Iteration 1937, loss = 0.12810136\n",
      "Iteration 1938, loss = 0.12785065\n",
      "Iteration 568, loss = 0.35777579\n",
      "Iteration 1244, loss = 0.27155626\n",
      "Iteration 1602, loss = 0.14783861\n",
      "Iteration 1939, loss = 0.12758789\n",
      "Iteration 1172, loss = 0.27263469\n",
      "Iteration 1940, loss = 0.12765563\n",
      "Iteration 1941, loss = 0.12736682\n",
      "Iteration 611, loss = 0.34465138\n",
      "Iteration 2428, loss = 0.14179221\n",
      "Iteration 1173, loss = 0.27250407\n",
      "Iteration 1942, loss = 0.12728276\n",
      "Iteration 1943, loss = 0.12717477\n",
      "Iteration 2429, loss = 0.14169015\n",
      "Iteration 1174, loss = 0.27237430\n",
      "Iteration 1944, loss = 0.12711698\n",
      "Iteration 817, loss = 0.31760982\n",
      "Iteration 1245, loss = 0.27143930\n",
      "Iteration 1603, loss = 0.14769159\n",
      "Iteration 1175, loss = 0.27227683\n",
      "Iteration 1176, loss = 0.27215839\n",
      "Iteration 1945, loss = 0.12683718Iteration 1246, loss = 0.27128906\n",
      "Iteration 612, loss = 0.34443865\n",
      "\n",
      "Iteration 569, loss = 0.35766244\n",
      "Iteration 1177, loss = 0.27200951\n",
      "Iteration 1946, loss = 0.12691538\n",
      "Iteration 1604, loss = 0.14767627\n",
      "Iteration 1178, loss = 0.27188996\n",
      "Iteration 2430, loss = 0.14158404\n",
      "Iteration 1947, loss = 0.12668232\n",
      "Iteration 1179, loss = 0.27173816\n",
      "Iteration 570, loss = 0.35755495\n",
      "Iteration 1948, loss = 0.12643305\n",
      "Iteration 1605, loss = 0.14744138\n",
      "Iteration 613, loss = 0.34418318\n",
      "Iteration 1180, loss = 0.27162843\n",
      "Iteration 1247, loss = 0.27115594\n",
      "Iteration 614, loss = 0.34401490\n",
      "Iteration 2431, loss = 0.14148877\n",
      "Iteration 571, loss = 0.35745483\n",
      "Iteration 1949, loss = 0.12641306\n",
      "Iteration 1606, loss = 0.14729996\n",
      "Iteration 1181, loss = 0.27154437\n",
      "Iteration 818, loss = 0.31740933\n",
      "Iteration 1950, loss = 0.12638452\n",
      "Iteration 1182, loss = 0.27151077\n",
      "Iteration 615, loss = 0.34390364\n",
      "Iteration 2432, loss = 0.14145051\n",
      "Iteration 1248, loss = 0.27101588\n",
      "Iteration 1951, loss = 0.12609457\n",
      "Iteration 2433, loss = 0.14136690\n",
      "Iteration 616, loss = 0.34375719\n",
      "Iteration 572, loss = 0.35734984\n",
      "Iteration 2434, loss = 0.14130246\n",
      "Iteration 1952, loss = 0.12599543\n",
      "Iteration 1607, loss = 0.14719572\n",
      "Iteration 617, loss = 0.34353913\n",
      "Iteration 2435, loss = 0.14126966\n",
      "Iteration 1183, loss = 0.27125535\n",
      "Iteration 1953, loss = 0.12581095\n",
      "Iteration 618, loss = 0.34347701\n",
      "Iteration 1608, loss = 0.14708992\n",
      "Iteration 1184, loss = 0.27114660\n",
      "Iteration 1954, loss = 0.12568399\n",
      "Iteration 2436, loss = 0.14115758\n",
      "Iteration 1249, loss = 0.27092183\n",
      "Iteration 819, loss = 0.31712917\n",
      "Iteration 619, loss = 0.34324353\n",
      "Iteration 1185, loss = 0.27101870\n",
      "Iteration 1955, loss = 0.12558647\n",
      "Iteration 573, loss = 0.35721572\n",
      "Iteration 620, loss = 0.34305946\n",
      "Iteration 1956, loss = 0.12539111\n",
      "Iteration 1609, loss = 0.14716392\n",
      "Iteration 1186, loss = 0.27097883\n",
      "Iteration 1957, loss = 0.12520502\n",
      "Iteration 1958, loss = 0.12519193\n",
      "Iteration 621, loss = 0.34290220\n",
      "Iteration 1959, loss = 0.12522592\n",
      "Iteration 1960, loss = 0.12499367\n",
      "Iteration 2437, loss = 0.14107674\n",
      "Iteration 622, loss = 0.34273974\n",
      "Iteration 1961, loss = 0.12482947\n",
      "Iteration 574, loss = 0.35708525\n",
      "Iteration 1187, loss = 0.27081248\n",
      "Iteration 820, loss = 0.31707841\n",
      "Iteration 1610, loss = 0.14679926\n",
      "Iteration 1250, loss = 0.27095175\n",
      "Iteration 1962, loss = 0.12472050\n",
      "Iteration 1188, loss = 0.27067455\n",
      "Iteration 2438, loss = 0.14114345\n",
      "Iteration 1963, loss = 0.12490065\n",
      "Iteration 1189, loss = 0.27057200\n",
      "Iteration 1964, loss = 0.12440500\n",
      "Iteration 1611, loss = 0.14678362\n",
      "Iteration 2439, loss = 0.14097215\n",
      "Iteration 821, loss = 0.31670291\n",
      "Iteration 575, loss = 0.35698702\n",
      "Iteration 2440, loss = 0.14086210\n",
      "Iteration 1251, loss = 0.27069353\n",
      "Iteration 1612, loss = 0.14677895\n",
      "Iteration 623, loss = 0.34253682\n",
      "Iteration 1190, loss = 0.27042003\n",
      "Iteration 1965, loss = 0.12423596\n",
      "Iteration 1966, loss = 0.12416733\n",
      "Iteration 576, loss = 0.35686096\n",
      "Iteration 2441, loss = 0.14080064\n",
      "Iteration 1191, loss = 0.27040837\n",
      "Iteration 1967, loss = 0.12405070\n",
      "Iteration 1613, loss = 0.14652423\n",
      "Iteration 822, loss = 0.31649597\n",
      "Iteration 1252, loss = 0.27053277\n",
      "Iteration 1192, loss = 0.27019635\n",
      "Iteration 1968, loss = 0.12388917\n",
      "Iteration 1969, loss = 0.12369671\n",
      "Iteration 1193, loss = 0.27007036\n",
      "Iteration 1614, loss = 0.14646516\n",
      "Iteration 2442, loss = 0.14075079\n",
      "Iteration 624, loss = 0.34239378\n",
      "Iteration 1970, loss = 0.12377149\n",
      "Iteration 1194, loss = 0.26991660\n",
      "Iteration 2443, loss = 0.14059822\n",
      "Iteration 1253, loss = 0.27040941\n",
      "Iteration 625, loss = 0.34222182\n",
      "Iteration 1971, loss = 0.12340376\n",
      "Iteration 1195, loss = 0.26989162\n",
      "Iteration 2444, loss = 0.14052214\n",
      "Iteration 1972, loss = 0.12344203\n",
      "Iteration 1615, loss = 0.14632315\n",
      "Iteration 823, loss = 0.31628344\n",
      "Iteration 577, loss = 0.35675250\n",
      "Iteration 1973, loss = 0.12323586\n",
      "Iteration 2445, loss = 0.14045510\n",
      "Iteration 1974, loss = 0.12318457\n",
      "Iteration 1196, loss = 0.26971419\n",
      "Iteration 1616, loss = 0.14626410\n",
      "Iteration 626, loss = 0.34205853\n",
      "Iteration 1975, loss = 0.12291727\n",
      "Iteration 1197, loss = 0.26964884\n",
      "Iteration 1976, loss = 0.12278343\n",
      "Iteration 578, loss = 0.35662705\n",
      "Iteration 1617, loss = 0.14617929\n",
      "Iteration 1977, loss = 0.12286335\n",
      "Iteration 1198, loss = 0.26942629\n",
      "Iteration 2446, loss = 0.14034946\n",
      "Iteration 1978, loss = 0.12298906\n",
      "Iteration 627, loss = 0.34190606\n",
      "Iteration 1618, loss = 0.14595266\n",
      "Iteration 1254, loss = 0.27026472\n",
      "Iteration 2447, loss = 0.14029588\n",
      "Iteration 1979, loss = 0.12243881\n",
      "Iteration 1199, loss = 0.26931142\n",
      "Iteration 824, loss = 0.31605721\n",
      "Iteration 579, loss = 0.35648843\n",
      "Iteration 1619, loss = 0.14587438\n",
      "Iteration 2448, loss = 0.14023816\n",
      "Iteration 1200, loss = 0.26927779\n",
      "Iteration 1980, loss = 0.12234059\n",
      "Iteration 1620, loss = 0.14571198\n",
      "Iteration 1201, loss = 0.26908982\n",
      "Iteration 628, loss = 0.34173559\n",
      "Iteration 1981, loss = 0.12230098\n",
      "Iteration 2449, loss = 0.14019721\n",
      "Iteration 1621, loss = 0.14574033\n",
      "Iteration 580, loss = 0.35637038\n",
      "Iteration 1255, loss = 0.27020152\n",
      "Iteration 629, loss = 0.34154133\n",
      "Iteration 1982, loss = 0.12206993\n",
      "Iteration 825, loss = 0.31577178\n",
      "Iteration 1622, loss = 0.14545879\n",
      "Iteration 1983, loss = 0.12187521\n",
      "Iteration 2450, loss = 0.14024815\n",
      "Iteration 1202, loss = 0.26896763\n",
      "Iteration 630, loss = 0.34138548\n",
      "Iteration 1256, loss = 0.27001386\n",
      "Iteration 2451, loss = 0.14016585\n",
      "Iteration 1984, loss = 0.12193113\n",
      "Iteration 1623, loss = 0.14536759\n",
      "Iteration 1985, loss = 0.12168407\n",
      "Iteration 2452, loss = 0.13993685\n",
      "Iteration 1986, loss = 0.12152100\n",
      "Iteration 631, loss = 0.34128701\n",
      "Iteration 1987, loss = 0.12140511\n",
      "Iteration 1203, loss = 0.26890824\n",
      "Iteration 1257, loss = 0.26992256\n",
      "Iteration 1988, loss = 0.12133906\n",
      "Iteration 581, loss = 0.35625077\n",
      "Iteration 1624, loss = 0.14524816\n",
      "Iteration 1204, loss = 0.26871616\n",
      "Iteration 2453, loss = 0.13983518\n",
      "Iteration 826, loss = 0.31557349\n",
      "Iteration 1989, loss = 0.12110691\n",
      "Iteration 1205, loss = 0.26863244\n",
      "Iteration 2454, loss = 0.13982116\n",
      "Iteration 1258, loss = 0.26983303\n",
      "Iteration 632, loss = 0.34104989\n",
      "Iteration 1990, loss = 0.12103721\n",
      "Iteration 1625, loss = 0.14528976\n",
      "Iteration 1206, loss = 0.26847359\n",
      "Iteration 582, loss = 0.35616684\n",
      "Iteration 2455, loss = 0.13969715\n",
      "Iteration 1991, loss = 0.12087555\n",
      "Iteration 827, loss = 0.31534308\n",
      "Iteration 633, loss = 0.34099669\n",
      "Iteration 1992, loss = 0.12071829\n",
      "Iteration 1259, loss = 0.26967664\n",
      "Iteration 2456, loss = 0.13965425\n",
      "Iteration 1207, loss = 0.26835757\n",
      "Iteration 634, loss = 0.34089362\n",
      "Iteration 1993, loss = 0.12060297\n",
      "Iteration 1626, loss = 0.14503530\n",
      "Iteration 2457, loss = 0.13956142\n",
      "Iteration 1208, loss = 0.26823525\n",
      "Iteration 583, loss = 0.35602094\n",
      "Iteration 635, loss = 0.34058954\n",
      "Iteration 1994, loss = 0.12065512\n",
      "Iteration 2458, loss = 0.13957529\n",
      "Iteration 828, loss = 0.31514322\n",
      "Iteration 2459, loss = 0.13948694\n",
      "Iteration 1995, loss = 0.12032737\n",
      "Iteration 1209, loss = 0.26809819\n",
      "Iteration 1996, loss = 0.12020657\n",
      "Iteration 636, loss = 0.34036046\n",
      "Iteration 2460, loss = 0.13935575\n",
      "Iteration 1997, loss = 0.12023181\n",
      "Iteration 584, loss = 0.35590981\n",
      "Iteration 1260, loss = 0.26963601\n",
      "Iteration 1627, loss = 0.14491308\n",
      "Iteration 1998, loss = 0.12003494\n",
      "Iteration 2461, loss = 0.13923712\n",
      "Iteration 1210, loss = 0.26798117\n",
      "Iteration 637, loss = 0.34020985\n",
      "Iteration 1999, loss = 0.11994316\n",
      "Iteration 829, loss = 0.31492118\n",
      "Iteration 2000, loss = 0.11970352\n",
      "Iteration 585, loss = 0.35581950\n",
      "Iteration 2001, loss = 0.11965697\n",
      "Iteration 1211, loss = 0.26789078\n",
      "Iteration 2002, loss = 0.11971075\n",
      "Iteration 1261, loss = 0.26958447\n",
      "Iteration 2462, loss = 0.13921200\n",
      "Iteration 2003, loss = 0.11933613\n",
      "Iteration 2004, loss = 0.11932019\n",
      "Iteration 638, loss = 0.34003138\n",
      "Iteration 1628, loss = 0.14482481\n",
      "Iteration 2463, loss = 0.13911359\n",
      "Iteration 1212, loss = 0.26774527\n",
      "Iteration 2005, loss = 0.11929005\n",
      "Iteration 830, loss = 0.31475029\n",
      "Iteration 2006, loss = 0.11899346\n",
      "Iteration 1213, loss = 0.26762709\n",
      "Iteration 586, loss = 0.35576058\n",
      "Iteration 639, loss = 0.33989131\n",
      "Iteration 2007, loss = 0.11885699\n",
      "Iteration 1262, loss = 0.26934807\n",
      "Iteration 2464, loss = 0.13899245\n",
      "Iteration 1629, loss = 0.14481871\n",
      "Iteration 2008, loss = 0.11871205\n",
      "Iteration 587, loss = 0.35556387\n",
      "Iteration 831, loss = 0.31441132\n",
      "Iteration 2009, loss = 0.11878436\n",
      "Iteration 1214, loss = 0.26752629\n",
      "Iteration 2010, loss = 0.11851636\n",
      "Iteration 1630, loss = 0.14462125\n",
      "Iteration 1215, loss = 0.26738464\n",
      "Iteration 2465, loss = 0.13896621\n",
      "Iteration 640, loss = 0.33975309\n",
      "Iteration 2011, loss = 0.11852504\n",
      "Iteration 588, loss = 0.35543477\n",
      "Iteration 832, loss = 0.31422075\n",
      "Iteration 1263, loss = 0.26917353\n",
      "Iteration 1631, loss = 0.14455852\n",
      "Iteration 2012, loss = 0.11839139\n",
      "Iteration 641, loss = 0.33961370\n",
      "Iteration 2013, loss = 0.11821651\n",
      "Iteration 1216, loss = 0.26728147\n",
      "Iteration 2466, loss = 0.13906927\n",
      "Iteration 2014, loss = 0.11800399\n",
      "Iteration 642, loss = 0.33942696\n",
      "Iteration 2015, loss = 0.11810593\n",
      "Iteration 2467, loss = 0.13877923\n",
      "Iteration 833, loss = 0.31397277\n",
      "Iteration 1632, loss = 0.14448858\n",
      "Iteration 1264, loss = 0.26907702\n",
      "Iteration 2016, loss = 0.11779620\n",
      "Iteration 589, loss = 0.35535515\n",
      "Iteration 1217, loss = 0.26714686\n",
      "Iteration 2468, loss = 0.13877020\n",
      "Iteration 2017, loss = 0.11758853\n",
      "Iteration 590, loss = 0.35533302\n",
      "Iteration 2018, loss = 0.11750964\n",
      "Iteration 1633, loss = 0.14500942\n",
      "Iteration 643, loss = 0.33917808\n",
      "Iteration 2469, loss = 0.13862268\n",
      "Iteration 1218, loss = 0.26702219\n",
      "Iteration 2019, loss = 0.11745178\n",
      "Iteration 834, loss = 0.31373613\n",
      "Iteration 1265, loss = 0.26903906\n",
      "Iteration 2470, loss = 0.13856502\n",
      "Iteration 1219, loss = 0.26692466\n",
      "Iteration 644, loss = 0.33901825\n",
      "Iteration 1634, loss = 0.14417604Iteration 2020, loss = 0.11742923\n",
      "\n",
      "Iteration 2471, loss = 0.13858518\n",
      "Iteration 591, loss = 0.35507644\n",
      "Iteration 1266, loss = 0.26883403\n",
      "Iteration 2021, loss = 0.11722884\n",
      "Iteration 645, loss = 0.33887302\n",
      "Iteration 1220, loss = 0.26684149\n",
      "Iteration 2472, loss = 0.13844872\n",
      "Iteration 2022, loss = 0.11700102\n",
      "Iteration 646, loss = 0.33868911\n",
      "Iteration 835, loss = 0.31364340\n",
      "Iteration 1221, loss = 0.26665522\n",
      "Iteration 592, loss = 0.35500459\n",
      "Iteration 2473, loss = 0.13835714\n",
      "Iteration 2023, loss = 0.11688925\n",
      "Iteration 1267, loss = 0.26866172\n",
      "Iteration 1635, loss = 0.14420538\n",
      "Iteration 647, loss = 0.33851225\n",
      "Iteration 2474, loss = 0.13831594\n",
      "Iteration 593, loss = 0.35484957\n",
      "Iteration 2024, loss = 0.11675346\n",
      "Iteration 1636, loss = 0.14394226\n",
      "Iteration 1222, loss = 0.26651181\n",
      "Iteration 2025, loss = 0.11669128\n",
      "Iteration 1268, loss = 0.26858270\n",
      "Iteration 594, loss = 0.35472392\n",
      "Iteration 648, loss = 0.33842946\n",
      "Iteration 2475, loss = 0.13824132\n",
      "Iteration 2026, loss = 0.11652204\n",
      "Iteration 1223, loss = 0.26644700\n",
      "Iteration 836, loss = 0.31329991\n",
      "Iteration 2027, loss = 0.11676650\n",
      "Iteration 1637, loss = 0.14385915\n",
      "Iteration 2028, loss = 0.11627666\n",
      "Iteration 595, loss = 0.35460862\n",
      "Iteration 2029, loss = 0.11632028\n",
      "Iteration 1638, loss = 0.14371386\n",
      "Iteration 2476, loss = 0.13810780\n",
      "Iteration 2030, loss = 0.11613340\n",
      "Iteration 1224, loss = 0.26643157\n",
      "Iteration 2031, loss = 0.11618684\n",
      "Iteration 649, loss = 0.33817108\n",
      "Iteration 1269, loss = 0.26861793\n",
      "Iteration 2477, loss = 0.13813207\n",
      "Iteration 2032, loss = 0.11588877\n",
      "Iteration 1639, loss = 0.14370344\n",
      "Iteration 2033, loss = 0.11593331\n",
      "Iteration 2034, loss = 0.11570038\n",
      "Iteration 650, loss = 0.33803286\n",
      "Iteration 1225, loss = 0.26617181\n",
      "Iteration 1640, loss = 0.14357548\n",
      "Iteration 2478, loss = 0.13805953\n",
      "Iteration 2035, loss = 0.11589015\n",
      "Iteration 837, loss = 0.31307885\n",
      "Iteration 1641, loss = 0.14341897\n",
      "Iteration 2479, loss = 0.13787909\n",
      "Iteration 651, loss = 0.33782873\n",
      "Iteration 1270, loss = 0.26836325\n",
      "Iteration 2036, loss = 0.11552508\n",
      "Iteration 1226, loss = 0.26603985\n",
      "Iteration 2037, loss = 0.11530340\n",
      "Iteration 1642, loss = 0.14334311\n",
      "Iteration 652, loss = 0.33764601\n",
      "Iteration 2038, loss = 0.11521387\n",
      "Iteration 2480, loss = 0.13782091\n",
      "Iteration 1643, loss = 0.14317525\n",
      "Iteration 838, loss = 0.31281658\n",
      "Iteration 653, loss = 0.33749411\n",
      "Iteration 1227, loss = 0.26596238\n",
      "Iteration 1271, loss = 0.26817193\n",
      "Iteration 596, loss = 0.35452374\n",
      "Iteration 2481, loss = 0.13775377\n",
      "Iteration 2039, loss = 0.11497576\n",
      "Iteration 2482, loss = 0.13776173\n",
      "Iteration 654, loss = 0.33730715\n",
      "Iteration 2040, loss = 0.11485253\n",
      "Iteration 2483, loss = 0.13765738\n",
      "Iteration 1228, loss = 0.26582500\n",
      "Iteration 2041, loss = 0.11480899\n",
      "Iteration 1644, loss = 0.14305748\n",
      "Iteration 2042, loss = 0.11462999\n",
      "Iteration 1272, loss = 0.26811443\n",
      "Iteration 655, loss = 0.33717480\n",
      "Iteration 597, loss = 0.35436803\n",
      "Iteration 2043, loss = 0.11448176\n",
      "Iteration 1229, loss = 0.26571714\n",
      "Iteration 1645, loss = 0.14292928\n",
      "Iteration 2484, loss = 0.13766161\n",
      "Iteration 656, loss = 0.33699850\n",
      "Iteration 839, loss = 0.31261093\n",
      "Iteration 598, loss = 0.35430045\n",
      "Iteration 2044, loss = 0.11444529\n",
      "Iteration 1230, loss = 0.26557602\n",
      "Iteration 657, loss = 0.33686807\n",
      "Iteration 2045, loss = 0.11429881\n",
      "Iteration 1646, loss = 0.14295231\n",
      "Iteration 2485, loss = 0.13750261\n",
      "Iteration 1273, loss = 0.26794777\n",
      "Iteration 2046, loss = 0.11414054\n",
      "Iteration 2486, loss = 0.13738634\n",
      "Iteration 1231, loss = 0.26542783\n",
      "Iteration 599, loss = 0.35414916\n",
      "Iteration 658, loss = 0.33666085\n",
      "Iteration 840, loss = 0.31241069\n",
      "Iteration 1647, loss = 0.14279771\n",
      "Iteration 1232, loss = 0.26538657\n",
      "Iteration 2047, loss = 0.11410731\n",
      "Iteration 1233, loss = 0.26523388\n",
      "Iteration 2048, loss = 0.11416441\n",
      "Iteration 2487, loss = 0.13734961\n",
      "Iteration 1274, loss = 0.26787320\n",
      "Iteration 1234, loss = 0.26516059\n",
      "Iteration 659, loss = 0.33647695\n",
      "Iteration 1648, loss = 0.14272817\n",
      "Iteration 2049, loss = 0.11402584\n",
      "Iteration 1235, loss = 0.26496599\n",
      "Iteration 2488, loss = 0.13721994\n",
      "Iteration 660, loss = 0.33633076Iteration 2050, loss = 0.11380843\n",
      "\n",
      "Iteration 1236, loss = 0.26483800\n",
      "Iteration 841, loss = 0.31231133\n",
      "Iteration 1275, loss = 0.26769208\n",
      "Iteration 600, loss = 0.35405064\n",
      "Iteration 1649, loss = 0.14255293\n",
      "Iteration 1237, loss = 0.26477920\n",
      "Iteration 1276, loss = 0.26757252\n",
      "Iteration 601, loss = 0.35392483\n",
      "Iteration 1238, loss = 0.26459180\n",
      "Iteration 1650, loss = 0.14251552\n",
      "Iteration 842, loss = 0.31190555\n",
      "Iteration 2489, loss = 0.13717906\n",
      "Iteration 2051, loss = 0.11358566\n",
      "Iteration 1651, loss = 0.14238970\n",
      "Iteration 1239, loss = 0.26450639\n",
      "Iteration 602, loss = 0.35379889\n",
      "Iteration 2052, loss = 0.11363548\n",
      "Iteration 2490, loss = 0.13708215\n",
      "Iteration 661, loss = 0.33614799\n",
      "Iteration 1277, loss = 0.26747254\n",
      "Iteration 1652, loss = 0.14222301\n",
      "Iteration 2053, loss = 0.11333835\n",
      "Iteration 2491, loss = 0.13701281\n",
      "Iteration 1240, loss = 0.26436178\n",
      "Iteration 2054, loss = 0.11327901\n",
      "Iteration 2055, loss = 0.11313845\n",
      "Iteration 603, loss = 0.35365715\n",
      "Iteration 843, loss = 0.31179515\n",
      "Iteration 1241, loss = 0.26421493\n",
      "Iteration 2492, loss = 0.13690129\n",
      "Iteration 1278, loss = 0.26737130\n",
      "Iteration 662, loss = 0.33594898\n",
      "Iteration 1242, loss = 0.26410715\n",
      "Iteration 2056, loss = 0.11298820\n",
      "Iteration 1243, loss = 0.26399090\n",
      "Iteration 2057, loss = 0.11296198\n",
      "Iteration 663, loss = 0.33582724\n",
      "Iteration 844, loss = 0.31143998\n",
      "Iteration 2493, loss = 0.13693816\n",
      "Iteration 1653, loss = 0.14215738\n",
      "Iteration 1244, loss = 0.26386065\n",
      "Iteration 2058, loss = 0.11282611\n",
      "Iteration 2494, loss = 0.13679026\n",
      "Iteration 1279, loss = 0.26719128\n",
      "Iteration 1654, loss = 0.14201116\n",
      "Iteration 2059, loss = 0.11266485\n",
      "Iteration 664, loss = 0.33574402\n",
      "Iteration 604, loss = 0.35358226\n",
      "Iteration 1245, loss = 0.26378024\n",
      "Iteration 1655, loss = 0.14204383\n",
      "Iteration 665, loss = 0.33564070\n",
      "Iteration 2495, loss = 0.13674296\n",
      "Iteration 845, loss = 0.31129092Iteration 2060, loss = 0.11260891\n",
      "\n",
      "Iteration 2061, loss = 0.11247458\n",
      "Iteration 1246, loss = 0.26361885\n",
      "Iteration 666, loss = 0.33525349\n",
      "Iteration 1280, loss = 0.26710172\n",
      "Iteration 2062, loss = 0.11233812\n",
      "Iteration 2496, loss = 0.13663813\n",
      "Iteration 1656, loss = 0.14182877\n",
      "Iteration 2063, loss = 0.11233611\n",
      "Iteration 667, loss = 0.33510335\n",
      "Iteration 605, loss = 0.35350721\n",
      "Iteration 1247, loss = 0.26354635\n",
      "Iteration 2064, loss = 0.11237223\n",
      "Iteration 1657, loss = 0.14169111\n",
      "Iteration 668, loss = 0.33492023\n",
      "Iteration 2065, loss = 0.11208522\n",
      "Iteration 2497, loss = 0.13659010\n",
      "Iteration 2066, loss = 0.11189923\n",
      "Iteration 1658, loss = 0.14156675\n",
      "Iteration 1281, loss = 0.26706590\n",
      "Iteration 846, loss = 0.31111676\n",
      "Iteration 1248, loss = 0.26341998\n",
      "Iteration 669, loss = 0.33489061\n",
      "Iteration 2067, loss = 0.11183228\n",
      "Iteration 2068, loss = 0.11166320\n",
      "Iteration 606, loss = 0.35332108\n",
      "Iteration 2498, loss = 0.13663412\n",
      "Iteration 2069, loss = 0.11170982\n",
      "Iteration 1249, loss = 0.26327550\n",
      "Iteration 670, loss = 0.33463570\n",
      "Iteration 2070, loss = 0.11151512\n",
      "Iteration 2071, loss = 0.11140884\n",
      "Iteration 1659, loss = 0.14155729\n",
      "Iteration 607, loss = 0.35323397\n",
      "Iteration 1250, loss = 0.26314933\n",
      "Iteration 2499, loss = 0.13643697\n",
      "Iteration 2072, loss = 0.11120234\n",
      "Iteration 1282, loss = 0.26680358\n",
      "Iteration 1660, loss = 0.14142116\n",
      "Iteration 2073, loss = 0.11114245\n",
      "Iteration 671, loss = 0.33436943\n",
      "Iteration 2074, loss = 0.11103314\n",
      "Iteration 1661, loss = 0.14147883\n",
      "Iteration 1251, loss = 0.26301838\n",
      "Iteration 608, loss = 0.35310027\n",
      "Iteration 847, loss = 0.31077971\n",
      "Iteration 2500, loss = 0.13640438\n",
      "Iteration 2075, loss = 0.11086665\n",
      "Iteration 1662, loss = 0.14128256\n",
      "Iteration 2076, loss = 0.11089741\n",
      "Iteration 609, loss = 0.35300395\n",
      "Iteration 2501, loss = 0.13630861\n",
      "Iteration 1252, loss = 0.26291936\n",
      "Iteration 2077, loss = 0.11067541\n",
      "Iteration 1663, loss = 0.14106368\n",
      "Iteration 1283, loss = 0.26683087\n",
      "Iteration 672, loss = 0.33423020\n",
      "Iteration 2078, loss = 0.11053629\n",
      "Iteration 1253, loss = 0.26280843\n",
      "Iteration 1284, loss = 0.26674975\n",
      "Iteration 1254, loss = 0.26271723\n",
      "Iteration 848, loss = 0.31059433\n",
      "Iteration 2079, loss = 0.11050170\n",
      "Iteration 1664, loss = 0.14100329\n",
      "Iteration 610, loss = 0.35288221\n",
      "Iteration 2080, loss = 0.11039992\n",
      "Iteration 1665, loss = 0.14085829\n",
      "Iteration 2081, loss = 0.11024810\n",
      "Iteration 673, loss = 0.33409322\n",
      "Iteration 2082, loss = 0.11018968\n",
      "Iteration 1255, loss = 0.26256141\n",
      "Iteration 849, loss = 0.31035396\n",
      "Iteration 2502, loss = 0.13622969\n",
      "Iteration 1285, loss = 0.26648120\n",
      "Iteration 1256, loss = 0.26242460\n",
      "Iteration 2083, loss = 0.11009280\n",
      "Iteration 611, loss = 0.35272325\n",
      "Iteration 1666, loss = 0.14076840\n",
      "Iteration 2084, loss = 0.10991939\n",
      "Iteration 2503, loss = 0.13617397\n",
      "Iteration 1286, loss = 0.26632378\n",
      "Iteration 1257, loss = 0.26227148\n",
      "Iteration 2085, loss = 0.10979453\n",
      "Iteration 2086, loss = 0.10971019\n",
      "Iteration 612, loss = 0.35258697\n",
      "Iteration 1258, loss = 0.26217852\n",
      "Iteration 1667, loss = 0.14070644\n",
      "Iteration 2504, loss = 0.13615403\n",
      "Iteration 2087, loss = 0.10958004\n",
      "Iteration 850, loss = 0.31006455\n",
      "Iteration 1259, loss = 0.26205687\n",
      "Iteration 2088, loss = 0.10947041\n",
      "Iteration 1287, loss = 0.26617441\n",
      "Iteration 613, loss = 0.35250683\n",
      "Iteration 1260, loss = 0.26194232\n",
      "Iteration 2505, loss = 0.13598306\n",
      "Iteration 2089, loss = 0.10969682\n",
      "Iteration 1668, loss = 0.14060564\n",
      "Iteration 2090, loss = 0.10928153\n",
      "Iteration 1261, loss = 0.26181255\n",
      "Iteration 2506, loss = 0.13592499\n",
      "Iteration 2091, loss = 0.10924288\n",
      "Iteration 1669, loss = 0.14043830\n",
      "Iteration 1262, loss = 0.26169588\n",
      "Iteration 2507, loss = 0.13609003\n",
      "Iteration 851, loss = 0.30983273\n",
      "Iteration 2092, loss = 0.10901057\n",
      "Iteration 1288, loss = 0.26607933\n",
      "Iteration 2508, loss = 0.13593734\n",
      "Iteration 1670, loss = 0.14036252\n",
      "Iteration 2093, loss = 0.10894266\n",
      "Iteration 614, loss = 0.35236682\n",
      "Iteration 2094, loss = 0.10890869\n",
      "Iteration 1263, loss = 0.26157897\n",
      "Iteration 1671, loss = 0.14032710\n",
      "Iteration 2509, loss = 0.13572171\n",
      "Iteration 1264, loss = 0.26144123\n",
      "Iteration 2095, loss = 0.10877055\n",
      "Iteration 852, loss = 0.30961789\n",
      "Iteration 1289, loss = 0.26606792\n",
      "Iteration 1265, loss = 0.26133895\n",
      "Iteration 615, loss = 0.35228441\n",
      "Iteration 1672, loss = 0.14055264\n",
      "Iteration 2096, loss = 0.10864930\n",
      "Iteration 1266, loss = 0.26119805\n",
      "Iteration 2510, loss = 0.13569732\n",
      "Iteration 2097, loss = 0.10851000\n",
      "Iteration 1267, loss = 0.26113665\n",
      "Iteration 1673, loss = 0.14001785\n",
      "Iteration 2098, loss = 0.10839430\n",
      "Iteration 2511, loss = 0.13561880\n",
      "Iteration 853, loss = 0.30962321\n",
      "Iteration 1268, loss = 0.26097986\n",
      "Iteration 616, loss = 0.35211836\n",
      "Iteration 1290, loss = 0.26587531\n",
      "Iteration 2099, loss = 0.10831000\n",
      "Iteration 2100, loss = 0.10824600\n",
      "Iteration 1674, loss = 0.13997475\n",
      "Iteration 2512, loss = 0.13546394\n",
      "Iteration 2101, loss = 0.10814319\n",
      "Iteration 1269, loss = 0.26086201\n",
      "Iteration 2102, loss = 0.10810344\n",
      "Iteration 2513, loss = 0.13544170\n",
      "Iteration 2103, loss = 0.10805041\n",
      "Iteration 1291, loss = 0.26572207\n",
      "Iteration 854, loss = 0.30917139\n",
      "Iteration 2104, loss = 0.10774251\n",
      "Iteration 2514, loss = 0.13540744\n",
      "Iteration 1270, loss = 0.26070943\n",
      "Iteration 617, loss = 0.35199994\n",
      "Iteration 2105, loss = 0.10765084\n",
      "Iteration 1675, loss = 0.13994011\n",
      "Iteration 2106, loss = 0.10757024\n",
      "Iteration 2515, loss = 0.13527489\n",
      "Iteration 1271, loss = 0.26067354\n",
      "Iteration 2107, loss = 0.10760193\n",
      "Iteration 1292, loss = 0.26557343\n",
      "Iteration 2108, loss = 0.10739600\n",
      "Iteration 2516, loss = 0.13523194\n",
      "Iteration 618, loss = 0.35190721\n",
      "Iteration 2109, loss = 0.10741411\n",
      "Iteration 2517, loss = 0.13517665\n",
      "Iteration 1272, loss = 0.26050156\n",
      "Iteration 2110, loss = 0.10724641\n",
      "Iteration 855, loss = 0.30893738\n",
      "Iteration 1676, loss = 0.13971528\n",
      "Iteration 2518, loss = 0.13510193\n",
      "Iteration 1273, loss = 0.26034463\n",
      "Iteration 1293, loss = 0.26544195\n",
      "Iteration 2519, loss = 0.13523945\n",
      "Iteration 2111, loss = 0.10710004Iteration 619, loss = 0.35174977\n",
      "\n",
      "Iteration 2520, loss = 0.13490455\n",
      "Iteration 1274, loss = 0.26023977\n",
      "Iteration 1677, loss = 0.13962876\n",
      "Iteration 2112, loss = 0.10723161\n",
      "Iteration 856, loss = 0.30867837\n",
      "Iteration 1294, loss = 0.26532344\n",
      "Iteration 674, loss = 0.33389339\n",
      "Iteration 1678, loss = 0.13955874\n",
      "Iteration 620, loss = 0.35162746\n",
      "Iteration 2113, loss = 0.10696487\n",
      "Iteration 2521, loss = 0.13486006\n",
      "Iteration 1295, loss = 0.26528764\n",
      "Iteration 2522, loss = 0.13476224\n",
      "Iteration 1679, loss = 0.13937255\n",
      "Iteration 2114, loss = 0.10672458\n",
      "Iteration 2523, loss = 0.13468066\n",
      "Iteration 1275, loss = 0.26011705\n",
      "Iteration 1680, loss = 0.13941053\n",
      "Iteration 621, loss = 0.35152300\n",
      "Iteration 2115, loss = 0.10679726\n",
      "Iteration 2524, loss = 0.13464261\n",
      "Iteration 1296, loss = 0.26512537\n",
      "Iteration 857, loss = 0.30850991\n",
      "Iteration 1681, loss = 0.13925551\n",
      "Iteration 2525, loss = 0.13457470\n",
      "Iteration 2116, loss = 0.10656152\n",
      "Iteration 622, loss = 0.35141398\n",
      "Iteration 1276, loss = 0.25998816\n",
      "Iteration 2526, loss = 0.13455713\n",
      "Iteration 1682, loss = 0.13912676\n",
      "Iteration 2117, loss = 0.10654707\n",
      "Iteration 2527, loss = 0.13439684\n",
      "Iteration 623, loss = 0.35130629\n",
      "Iteration 2118, loss = 0.10633028\n",
      "Iteration 1277, loss = 0.25985206\n",
      "Iteration 2119, loss = 0.10626204\n",
      "Iteration 2528, loss = 0.13434890\n",
      "Iteration 1683, loss = 0.13915355\n",
      "Iteration 675, loss = 0.33374550\n",
      "Iteration 1297, loss = 0.26498647\n",
      "Iteration 2120, loss = 0.10621681\n",
      "Iteration 858, loss = 0.30825238\n",
      "Iteration 2121, loss = 0.10620989\n",
      "Iteration 1278, loss = 0.25976309\n",
      "Iteration 1684, loss = 0.13907346\n",
      "Iteration 2529, loss = 0.13428370\n",
      "Iteration 624, loss = 0.35116635\n",
      "Iteration 2122, loss = 0.10596801\n",
      "Iteration 1685, loss = 0.13873662\n",
      "Iteration 676, loss = 0.33354466\n",
      "Iteration 1298, loss = 0.26489907\n",
      "Iteration 1279, loss = 0.25964449\n",
      "Iteration 2123, loss = 0.10584472\n",
      "Iteration 2530, loss = 0.13418833\n",
      "Iteration 1686, loss = 0.13866584\n",
      "Iteration 859, loss = 0.30806705\n",
      "Iteration 2124, loss = 0.10572220\n",
      "Iteration 1299, loss = 0.26470483\n",
      "Iteration 2125, loss = 0.10557898\n",
      "Iteration 625, loss = 0.35103416\n",
      "Iteration 2531, loss = 0.13413321\n",
      "Iteration 1280, loss = 0.25952301\n",
      "Iteration 1687, loss = 0.13856221\n",
      "Iteration 677, loss = 0.33352582\n",
      "Iteration 2126, loss = 0.10544770\n",
      "Iteration 2532, loss = 0.13415935\n",
      "Iteration 2127, loss = 0.10545853\n",
      "Iteration 1281, loss = 0.25938935\n",
      "Iteration 1688, loss = 0.13861955\n",
      "Iteration 2128, loss = 0.10526189\n",
      "Iteration 626, loss = 0.35094999\n",
      "Iteration 860, loss = 0.30781343\n",
      "Iteration 2533, loss = 0.13398501\n",
      "Iteration 1689, loss = 0.13864596\n",
      "Iteration 2129, loss = 0.10520329\n",
      "Iteration 1282, loss = 0.25932740\n",
      "Iteration 678, loss = 0.33320578\n",
      "Iteration 1300, loss = 0.26459694\n",
      "Iteration 2534, loss = 0.13402634\n",
      "Iteration 1283, loss = 0.25917799\n",
      "Iteration 2130, loss = 0.10506020\n",
      "Iteration 1690, loss = 0.13822171\n",
      "Iteration 2535, loss = 0.13384954\n",
      "Iteration 2131, loss = 0.10496338\n",
      "Iteration 627, loss = 0.35079454\n",
      "Iteration 679, loss = 0.33300845\n",
      "Iteration 1284, loss = 0.25904012\n",
      "Iteration 1301, loss = 0.26449261\n",
      "Iteration 861, loss = 0.30758511\n",
      "Iteration 2132, loss = 0.10488957\n",
      "Iteration 1691, loss = 0.13819449\n",
      "Iteration 2536, loss = 0.13382068\n",
      "Iteration 1285, loss = 0.25895683\n",
      "Iteration 2133, loss = 0.10478974\n",
      "Iteration 680, loss = 0.33287287\n",
      "Iteration 1302, loss = 0.26434299\n",
      "Iteration 2134, loss = 0.10468704\n",
      "Iteration 2537, loss = 0.13373535\n",
      "Iteration 1286, loss = 0.25891960\n",
      "Iteration 628, loss = 0.35072274\n",
      "Iteration 1692, loss = 0.13804452\n",
      "Iteration 2538, loss = 0.13374095\n",
      "Iteration 2135, loss = 0.10466148\n",
      "Iteration 681, loss = 0.33267203\n",
      "Iteration 2539, loss = 0.13357949\n",
      "Iteration 1303, loss = 0.26424055\n",
      "Iteration 1693, loss = 0.13803482\n",
      "Iteration 1287, loss = 0.25878870\n",
      "Iteration 862, loss = 0.30734835\n",
      "Iteration 2136, loss = 0.10462709\n",
      "Iteration 2540, loss = 0.13354850\n",
      "Iteration 629, loss = 0.35057030\n",
      "Iteration 682, loss = 0.33248561\n",
      "Iteration 2137, loss = 0.10440231\n",
      "Iteration 683, loss = 0.33226184\n",
      "Iteration 1304, loss = 0.26411788\n",
      "Iteration 2138, loss = 0.10434818\n",
      "Iteration 1694, loss = 0.13791071\n",
      "Iteration 2541, loss = 0.13346475\n",
      "Iteration 2139, loss = 0.10417618\n",
      "Iteration 2140, loss = 0.10419763\n",
      "Iteration 1288, loss = 0.25864908\n",
      "Iteration 863, loss = 0.30719932\n",
      "Iteration 2141, loss = 0.10414664\n",
      "Iteration 684, loss = 0.33214113\n",
      "Iteration 1695, loss = 0.13777198\n",
      "Iteration 630, loss = 0.35045410\n",
      "Iteration 2142, loss = 0.10390466\n",
      "Iteration 2542, loss = 0.13347903\n",
      "Iteration 1289, loss = 0.25841783\n",
      "Iteration 685, loss = 0.33198459\n",
      "Iteration 2143, loss = 0.10386948\n",
      "Iteration 2144, loss = 0.10373069\n",
      "Iteration 1696, loss = 0.13767101\n",
      "Iteration 864, loss = 0.30686689\n",
      "Iteration 1305, loss = 0.26396714\n",
      "Iteration 2145, loss = 0.10364118\n",
      "Iteration 2543, loss = 0.13335199\n",
      "Iteration 1290, loss = 0.25832668\n",
      "Iteration 2146, loss = 0.10358339\n",
      "Iteration 686, loss = 0.33175391\n",
      "Iteration 2544, loss = 0.13332797\n",
      "Iteration 2147, loss = 0.10349225\n",
      "Iteration 631, loss = 0.35032678\n",
      "Iteration 2545, loss = 0.13322682\n",
      "Iteration 1697, loss = 0.13754846\n",
      "Iteration 2148, loss = 0.10333659\n",
      "Iteration 1291, loss = 0.25819625\n",
      "Iteration 687, loss = 0.33154277\n",
      "Iteration 2149, loss = 0.10322545\n",
      "Iteration 2546, loss = 0.13314090\n",
      "Iteration 2150, loss = 0.10310032\n",
      "Iteration 1292, loss = 0.25810329\n",
      "Iteration 1306, loss = 0.26385551\n",
      "Iteration 2547, loss = 0.13315377\n",
      "Iteration 1293, loss = 0.25804911\n",
      "Iteration 632, loss = 0.35021457\n",
      "Iteration 688, loss = 0.33136723\n",
      "Iteration 865, loss = 0.30657707\n",
      "Iteration 2151, loss = 0.10306881\n",
      "Iteration 1294, loss = 0.25781007\n",
      "Iteration 1698, loss = 0.13748597\n",
      "Iteration 2548, loss = 0.13297645\n",
      "Iteration 689, loss = 0.33118774\n",
      "Iteration 2152, loss = 0.10298062\n",
      "Iteration 1295, loss = 0.25771151\n",
      "Iteration 2549, loss = 0.13290754\n",
      "Iteration 690, loss = 0.33115746\n",
      "Iteration 2153, loss = 0.10298056\n",
      "Iteration 1307, loss = 0.26375294\n",
      "Iteration 866, loss = 0.30651342\n",
      "Iteration 1296, loss = 0.25760953\n",
      "Iteration 1699, loss = 0.13765512\n",
      "Iteration 633, loss = 0.35010345\n",
      "Iteration 2154, loss = 0.10268552\n",
      "Iteration 2550, loss = 0.13289550\n",
      "Iteration 1297, loss = 0.25746675\n",
      "Iteration 2155, loss = 0.10266370\n",
      "Iteration 2156, loss = 0.10253603\n",
      "Iteration 1298, loss = 0.25733317\n",
      "Iteration 1700, loss = 0.13719793\n",
      "Iteration 2157, loss = 0.10251633\n",
      "Iteration 2551, loss = 0.13275670\n",
      "Iteration 1308, loss = 0.26359685\n",
      "Iteration 2158, loss = 0.10246734\n",
      "Iteration 1299, loss = 0.25723008\n",
      "Iteration 867, loss = 0.30610828\n",
      "Iteration 2159, loss = 0.10227775\n",
      "Iteration 634, loss = 0.34999302\n",
      "Iteration 1300, loss = 0.25709797\n",
      "Iteration 2552, loss = 0.13279564\n",
      "Iteration 1701, loss = 0.13719362\n",
      "Iteration 2160, loss = 0.10214424\n",
      "Iteration 2553, loss = 0.13263950\n",
      "Iteration 2161, loss = 0.10203928\n",
      "Iteration 868, loss = 0.30586927\n",
      "Iteration 1301, loss = 0.25701852\n",
      "Iteration 2162, loss = 0.10196179\n",
      "Iteration 691, loss = 0.33082209\n",
      "Iteration 1309, loss = 0.26345597\n",
      "Iteration 635, loss = 0.34983633\n",
      "Iteration 2554, loss = 0.13258878\n",
      "Iteration 1302, loss = 0.25687160\n",
      "Iteration 2163, loss = 0.10198922\n",
      "Iteration 2555, loss = 0.13250783\n",
      "Iteration 1702, loss = 0.13716995\n",
      "Iteration 1303, loss = 0.25671393\n",
      "Iteration 2164, loss = 0.10175279\n",
      "Iteration 2556, loss = 0.13244589\n",
      "Iteration 1304, loss = 0.25658266\n",
      "Iteration 2165, loss = 0.10175715\n",
      "Iteration 1310, loss = 0.26336030\n",
      "Iteration 636, loss = 0.34975214\n",
      "Iteration 2557, loss = 0.13240538\n",
      "Iteration 869, loss = 0.30562059\n",
      "Iteration 1703, loss = 0.13695561\n",
      "Iteration 2558, loss = 0.13231246\n",
      "Iteration 2166, loss = 0.10162974\n",
      "Iteration 1305, loss = 0.25653165\n",
      "Iteration 1311, loss = 0.26325663\n",
      "Iteration 2167, loss = 0.10148718\n",
      "Iteration 2559, loss = 0.13223845\n",
      "Iteration 1704, loss = 0.13687897\n",
      "Iteration 637, loss = 0.34962101\n",
      "Iteration 2168, loss = 0.10142789\n",
      "Iteration 2560, loss = 0.13215738\n",
      "Iteration 1306, loss = 0.25635862\n",
      "Iteration 2169, loss = 0.10127324\n",
      "Iteration 870, loss = 0.30543626\n",
      "Iteration 1705, loss = 0.13679237\n",
      "Iteration 1307, loss = 0.25621163\n",
      "Iteration 2561, loss = 0.13208543\n",
      "Iteration 2170, loss = 0.10135327\n",
      "Iteration 2171, loss = 0.10120506\n",
      "Iteration 1312, loss = 0.26314369\n",
      "Iteration 638, loss = 0.34950169\n",
      "Iteration 2562, loss = 0.13204525\n",
      "Iteration 2172, loss = 0.10109420\n",
      "Iteration 1308, loss = 0.25609075\n",
      "Iteration 1706, loss = 0.13685796\n",
      "Iteration 2173, loss = 0.10102176\n",
      "Iteration 2563, loss = 0.13194108\n",
      "Iteration 2174, loss = 0.10087360\n",
      "Iteration 1309, loss = 0.25600402\n",
      "Iteration 871, loss = 0.30513918\n",
      "Iteration 1313, loss = 0.26298488\n",
      "Iteration 2175, loss = 0.10080556\n",
      "Iteration 1707, loss = 0.13653572\n",
      "Iteration 639, loss = 0.34937306\n",
      "Iteration 2564, loss = 0.13199725\n",
      "Iteration 2176, loss = 0.10068589\n",
      "Iteration 1310, loss = 0.25590482\n",
      "Iteration 2177, loss = 0.10061948\n",
      "Iteration 2178, loss = 0.10050039\n",
      "Iteration 1311, loss = 0.25576026\n",
      "Iteration 2565, loss = 0.13191697\n",
      "Iteration 2179, loss = 0.10061620\n",
      "Iteration 2180, loss = 0.10030418\n",
      "Iteration 1708, loss = 0.13646420\n",
      "Iteration 1314, loss = 0.26285015\n",
      "Iteration 1312, loss = 0.25567931\n",
      "Iteration 640, loss = 0.34930362\n",
      "Iteration 2181, loss = 0.10026889\n",
      "Iteration 872, loss = 0.30492625\n",
      "Iteration 2566, loss = 0.13197963\n",
      "Iteration 2182, loss = 0.10011513\n",
      "Iteration 1313, loss = 0.25550547\n",
      "Iteration 1315, loss = 0.26279506\n",
      "Iteration 2183, loss = 0.10030984\n",
      "Iteration 1314, loss = 0.25536423\n",
      "Iteration 1709, loss = 0.13644432\n",
      "Iteration 2184, loss = 0.10019867\n",
      "Iteration 2567, loss = 0.13185575\n",
      "Iteration 1315, loss = 0.25529001\n",
      "Iteration 1710, loss = 0.13637543\n",
      "Iteration 873, loss = 0.30469850\n",
      "Iteration 1316, loss = 0.25513666\n",
      "Iteration 2185, loss = 0.09998718\n",
      "Iteration 641, loss = 0.34914780\n",
      "Iteration 1711, loss = 0.13613008\n",
      "Iteration 1317, loss = 0.25500476\n",
      "Iteration 2568, loss = 0.13164153\n",
      "Iteration 2186, loss = 0.09987942\n",
      "Iteration 1316, loss = 0.26257043\n",
      "Iteration 1318, loss = 0.25494403\n",
      "Iteration 1712, loss = 0.13600989\n",
      "Iteration 874, loss = 0.30462850\n",
      "Iteration 2569, loss = 0.13162719\n",
      "Iteration 2187, loss = 0.09984707\n",
      "Iteration 1319, loss = 0.25477223\n",
      "Iteration 642, loss = 0.34902501\n",
      "Iteration 2188, loss = 0.09966725\n",
      "Iteration 2189, loss = 0.09955293\n",
      "Iteration 2570, loss = 0.13152071\n",
      "Iteration 1713, loss = 0.13590536\n",
      "Iteration 2190, loss = 0.09939504\n",
      "Iteration 1320, loss = 0.25465156\n",
      "Iteration 1317, loss = 0.26248762\n",
      "Iteration 2571, loss = 0.13146719\n",
      "Iteration 1321, loss = 0.25453887\n",
      "Iteration 2191, loss = 0.09930540\n",
      "Iteration 1714, loss = 0.13581896\n",
      "Iteration 643, loss = 0.34892042\n",
      "Iteration 875, loss = 0.30422186\n",
      "Iteration 2572, loss = 0.13142191\n",
      "Iteration 2192, loss = 0.09919734\n",
      "Iteration 1715, loss = 0.13574175\n",
      "Iteration 2193, loss = 0.09915953\n",
      "Iteration 2573, loss = 0.13128428\n",
      "Iteration 1322, loss = 0.25443042\n",
      "Iteration 1318, loss = 0.26234788\n",
      "Iteration 1716, loss = 0.13562601\n",
      "Iteration 644, loss = 0.34878437\n",
      "Iteration 2574, loss = 0.13124770\n",
      "Iteration 2194, loss = 0.09911026\n",
      "Iteration 2575, loss = 0.13117369\n",
      "Iteration 1323, loss = 0.25428511\n",
      "Iteration 2195, loss = 0.09901525\n",
      "Iteration 1319, loss = 0.26226266\n",
      "Iteration 692, loss = 0.33070566\n",
      "Iteration 2576, loss = 0.13111641\n",
      "Iteration 1717, loss = 0.13554391\n",
      "Iteration 2196, loss = 0.09884311\n",
      "Iteration 876, loss = 0.30400963\n",
      "Iteration 1324, loss = 0.25418436\n",
      "Iteration 2577, loss = 0.13114559\n",
      "Iteration 693, loss = 0.33060205Iteration 645, loss = 0.34863765\n",
      "\n",
      "Iteration 2197, loss = 0.09898678\n",
      "Iteration 2578, loss = 0.13113727\n",
      "Iteration 1325, loss = 0.25402516\n",
      "Iteration 1718, loss = 0.13544106\n",
      "Iteration 2198, loss = 0.09873233\n",
      "Iteration 2199, loss = 0.09866560\n",
      "Iteration 1326, loss = 0.25391630\n",
      "Iteration 694, loss = 0.33037309\n",
      "Iteration 1719, loss = 0.13548455\n",
      "Iteration 2200, loss = 0.09854061\n",
      "Iteration 877, loss = 0.30372156\n",
      "Iteration 646, loss = 0.34853495\n",
      "Iteration 1320, loss = 0.26212470\n",
      "Iteration 2579, loss = 0.13090764\n",
      "Iteration 2201, loss = 0.09841486\n",
      "Iteration 1327, loss = 0.25379040\n",
      "Iteration 695, loss = 0.33039223\n",
      "Iteration 2580, loss = 0.13083248\n",
      "Iteration 2202, loss = 0.09833985\n",
      "Iteration 1720, loss = 0.13524216\n",
      "Iteration 1328, loss = 0.25370515\n",
      "Iteration 2203, loss = 0.09828802\n",
      "Iteration 2581, loss = 0.13079224\n",
      "Iteration 2204, loss = 0.09819393\n",
      "Iteration 1329, loss = 0.25359981\n",
      "Iteration 2205, loss = 0.09827864\n",
      "Iteration 647, loss = 0.34858805\n",
      "Iteration 2206, loss = 0.09800827\n",
      "Iteration 1721, loss = 0.13510171\n",
      "Iteration 1321, loss = 0.26199295\n",
      "Iteration 1330, loss = 0.25342780\n",
      "Iteration 2582, loss = 0.13079367\n",
      "Iteration 2207, loss = 0.09806313\n",
      "Iteration 696, loss = 0.33004413\n",
      "Iteration 2208, loss = 0.09781556\n",
      "Iteration 1331, loss = 0.25338484\n",
      "Iteration 1722, loss = 0.13502725\n",
      "Iteration 1322, loss = 0.26187335\n",
      "Iteration 2209, loss = 0.09772251\n",
      "Iteration 878, loss = 0.30351537\n",
      "Iteration 1332, loss = 0.25329043\n",
      "Iteration 2583, loss = 0.13064175\n",
      "Iteration 697, loss = 0.32990287\n",
      "Iteration 1723, loss = 0.13492666\n",
      "Iteration 2210, loss = 0.09778651\n",
      "Iteration 648, loss = 0.34830902\n",
      "Iteration 1323, loss = 0.26176029\n",
      "Iteration 1333, loss = 0.25313851\n",
      "Iteration 698, loss = 0.32963715\n",
      "Iteration 2211, loss = 0.09766316\n",
      "Iteration 649, loss = 0.34818234\n",
      "Iteration 2584, loss = 0.13061623\n",
      "Iteration 2212, loss = 0.09747631\n",
      "Iteration 1724, loss = 0.13490432\n",
      "Iteration 1324, loss = 0.26165565\n",
      "Iteration 2213, loss = 0.09736252\n",
      "Iteration 699, loss = 0.32938757\n",
      "Iteration 2214, loss = 0.09735179\n",
      "Iteration 1334, loss = 0.25298420\n",
      "Iteration 2215, loss = 0.09726984\n",
      "Iteration 1325, loss = 0.26151874\n",
      "Iteration 2585, loss = 0.13070832\n",
      "Iteration 1725, loss = 0.13479127\n",
      "Iteration 879, loss = 0.30328499\n",
      "Iteration 2216, loss = 0.09728090\n",
      "Iteration 700, loss = 0.32926045\n",
      "Iteration 2217, loss = 0.09705340\n",
      "Iteration 2586, loss = 0.13045416\n",
      "Iteration 1335, loss = 0.25278294\n",
      "Iteration 2218, loss = 0.09697857\n",
      "Iteration 650, loss = 0.34806233\n",
      "Iteration 2219, loss = 0.09691830\n",
      "Iteration 701, loss = 0.32927292\n",
      "Iteration 1326, loss = 0.26142688\n",
      "Iteration 2220, loss = 0.09680794\n",
      "Iteration 2587, loss = 0.13039734\n",
      "Iteration 1726, loss = 0.13469517\n",
      "Iteration 1336, loss = 0.25267553\n",
      "Iteration 2221, loss = 0.09669174\n",
      "Iteration 2588, loss = 0.13037315\n",
      "Iteration 651, loss = 0.34794411\n",
      "Iteration 702, loss = 0.32895673\n",
      "Iteration 2222, loss = 0.09674289\n",
      "Iteration 2589, loss = 0.13026361\n",
      "Iteration 1337, loss = 0.25257075\n",
      "Iteration 880, loss = 0.30305609\n",
      "Iteration 2590, loss = 0.13026039\n",
      "Iteration 1727, loss = 0.13456846\n",
      "Iteration 703, loss = 0.32871702\n",
      "Iteration 1327, loss = 0.26126382\n",
      "Iteration 652, loss = 0.34786031\n",
      "Iteration 2223, loss = 0.09675649\n",
      "Iteration 2591, loss = 0.13017597\n",
      "Iteration 1338, loss = 0.25246310\n",
      "Iteration 1728, loss = 0.13453130\n",
      "Iteration 2224, loss = 0.09644783\n",
      "Iteration 1339, loss = 0.25230004\n",
      "Iteration 704, loss = 0.32847989\n",
      "Iteration 653, loss = 0.34770327\n",
      "Iteration 2225, loss = 0.09635977\n",
      "Iteration 2226, loss = 0.09632370\n",
      "Iteration 705, loss = 0.32837265\n",
      "Iteration 1729, loss = 0.13440710\n",
      "Iteration 2592, loss = 0.13005775\n",
      "Iteration 2227, loss = 0.09616729\n",
      "Iteration 654, loss = 0.34757624\n",
      "Iteration 1328, loss = 0.26117164\n",
      "Iteration 1340, loss = 0.25220561\n",
      "Iteration 2228, loss = 0.09624184\n",
      "Iteration 881, loss = 0.30279183\n",
      "Iteration 2229, loss = 0.09603415\n",
      "Iteration 1730, loss = 0.13428072\n",
      "Iteration 1341, loss = 0.25214155\n",
      "Iteration 2230, loss = 0.09604169\n",
      "Iteration 706, loss = 0.32814649\n",
      "Iteration 1342, loss = 0.25193147\n",
      "Iteration 2231, loss = 0.09596502\n",
      "Iteration 882, loss = 0.30256378\n",
      "Iteration 707, loss = 0.32805661\n",
      "Iteration 1343, loss = 0.25181154\n",
      "Iteration 2593, loss = 0.13000789\n",
      "Iteration 2232, loss = 0.09584014\n",
      "Iteration 1329, loss = 0.26118111\n",
      "Iteration 2233, loss = 0.09570814\n",
      "Iteration 655, loss = 0.34747048\n",
      "Iteration 708, loss = 0.32778111\n",
      "Iteration 2234, loss = 0.09568369\n",
      "Iteration 2235, loss = 0.09562803\n",
      "Iteration 1731, loss = 0.13415806\n",
      "Iteration 2236, loss = 0.09553732\n",
      "Iteration 2594, loss = 0.12997698\n",
      "Iteration 2237, loss = 0.09539715\n",
      "Iteration 656, loss = 0.34735727\n",
      "Iteration 1732, loss = 0.13406407\n",
      "Iteration 1344, loss = 0.25188366\n",
      "Iteration 1330, loss = 0.26089967\n",
      "Iteration 883, loss = 0.30245051\n",
      "Iteration 1733, loss = 0.13416859\n",
      "Iteration 709, loss = 0.32760681\n",
      "Iteration 2595, loss = 0.12992733\n",
      "Iteration 1345, loss = 0.25160862\n",
      "Iteration 657, loss = 0.34723174\n",
      "Iteration 2238, loss = 0.09536905\n",
      "Iteration 1734, loss = 0.13383606\n",
      "Iteration 1331, loss = 0.26075426\n",
      "Iteration 2596, loss = 0.12981476\n",
      "Iteration 2239, loss = 0.09525293\n",
      "Iteration 1346, loss = 0.25151074\n",
      "Iteration 1735, loss = 0.13382064\n",
      "Iteration 2240, loss = 0.09515552\n",
      "Iteration 884, loss = 0.30214659\n",
      "Iteration 658, loss = 0.34713132\n",
      "Iteration 2241, loss = 0.09503178\n",
      "Iteration 710, loss = 0.32743346\n",
      "Iteration 1347, loss = 0.25135384\n",
      "Iteration 2597, loss = 0.12980304\n",
      "Iteration 1348, loss = 0.25125548\n",
      "Iteration 2242, loss = 0.09499098\n",
      "Iteration 1332, loss = 0.26063739\n",
      "Iteration 1736, loss = 0.13370776\n",
      "Iteration 1349, loss = 0.25108333\n",
      "Iteration 659, loss = 0.34701498\n",
      "Iteration 2243, loss = 0.09490254\n",
      "Iteration 885, loss = 0.30201351\n",
      "Iteration 2598, loss = 0.12972123\n",
      "Iteration 2244, loss = 0.09497835\n",
      "Iteration 1350, loss = 0.25093602\n",
      "Iteration 2245, loss = 0.09481897\n",
      "Iteration 660, loss = 0.34687602\n",
      "Iteration 711, loss = 0.32730788\n",
      "Iteration 1351, loss = 0.25083025\n",
      "Iteration 2599, loss = 0.12962163\n",
      "Iteration 2246, loss = 0.09460573\n",
      "Iteration 1737, loss = 0.13356972\n",
      "Iteration 2247, loss = 0.09459796\n",
      "Iteration 2600, loss = 0.12959794\n",
      "Iteration 1333, loss = 0.26056382Iteration 1352, loss = 0.25072985\n",
      "\n",
      "Iteration 2601, loss = 0.12955106\n",
      "Iteration 2248, loss = 0.09468806\n",
      "Iteration 1738, loss = 0.13347183\n",
      "Iteration 886, loss = 0.30166812\n",
      "Iteration 712, loss = 0.32705862\n",
      "Iteration 2249, loss = 0.09447145\n",
      "Iteration 661, loss = 0.34676360\n",
      "Iteration 1353, loss = 0.25056862\n",
      "Iteration 1739, loss = 0.13361107\n",
      "Iteration 713, loss = 0.32684833\n",
      "Iteration 1354, loss = 0.25046146\n",
      "Iteration 2250, loss = 0.09439349\n",
      "Iteration 2602, loss = 0.12951555\n",
      "Iteration 2251, loss = 0.09425887\n",
      "Iteration 1334, loss = 0.26038736\n",
      "Iteration 662, loss = 0.34665005\n",
      "Iteration 1355, loss = 0.25034675\n",
      "Iteration 1740, loss = 0.13356552\n",
      "Iteration 714, loss = 0.32669653\n",
      "Iteration 2252, loss = 0.09423459\n",
      "Iteration 1356, loss = 0.25021021\n",
      "Iteration 2603, loss = 0.12948818\n",
      "Iteration 715, loss = 0.32656814\n",
      "Iteration 2253, loss = 0.09409828\n",
      "Iteration 663, loss = 0.34655686\n",
      "Iteration 887, loss = 0.30140514\n",
      "Iteration 2254, loss = 0.09416476\n",
      "Iteration 1741, loss = 0.13314625\n",
      "Iteration 716, loss = 0.32642302\n",
      "Iteration 2604, loss = 0.12931346\n",
      "Iteration 2255, loss = 0.09393979\n",
      "Iteration 1357, loss = 0.25006121\n",
      "Iteration 2256, loss = 0.09388054\n",
      "Iteration 1742, loss = 0.13315377\n",
      "Iteration 717, loss = 0.32608773\n",
      "Iteration 2605, loss = 0.12930592\n",
      "Iteration 1358, loss = 0.24994422\n",
      "Iteration 1335, loss = 0.26029159\n",
      "Iteration 1743, loss = 0.13301911\n",
      "Iteration 2257, loss = 0.09385047\n",
      "Iteration 2606, loss = 0.12934326\n",
      "Iteration 888, loss = 0.30114735\n",
      "Iteration 664, loss = 0.34641527\n",
      "Iteration 1744, loss = 0.13301274\n",
      "Iteration 2258, loss = 0.09376634\n",
      "Iteration 2607, loss = 0.12913927\n",
      "Iteration 718, loss = 0.32589271\n",
      "Iteration 1359, loss = 0.24983719\n",
      "Iteration 2259, loss = 0.09358300\n",
      "Iteration 665, loss = 0.34630387\n",
      "Iteration 2608, loss = 0.12904068\n",
      "Iteration 1745, loss = 0.13284685\n",
      "Iteration 1360, loss = 0.24970847\n",
      "Iteration 2260, loss = 0.09355671\n",
      "Iteration 1336, loss = 0.26015077\n",
      "Iteration 2609, loss = 0.12913277\n",
      "Iteration 719, loss = 0.32575041\n",
      "Iteration 2261, loss = 0.09357485\n",
      "Iteration 889, loss = 0.30095298\n",
      "Iteration 2262, loss = 0.09340145\n",
      "Iteration 2610, loss = 0.12893309\n",
      "Iteration 666, loss = 0.34618046\n",
      "Iteration 1746, loss = 0.13317728\n",
      "Iteration 2263, loss = 0.09328914\n",
      "Iteration 1337, loss = 0.26013907\n",
      "Iteration 1361, loss = 0.24972731\n",
      "Iteration 720, loss = 0.32556391\n",
      "Iteration 2264, loss = 0.09323123\n",
      "Iteration 890, loss = 0.30098640\n",
      "Iteration 2265, loss = 0.09311092\n",
      "Iteration 721, loss = 0.32541680\n",
      "Iteration 1747, loss = 0.13260237\n",
      "Iteration 2266, loss = 0.09304396\n",
      "Iteration 1362, loss = 0.24944658\n",
      "Iteration 2267, loss = 0.09303017\n",
      "Iteration 1338, loss = 0.25997129\n",
      "Iteration 2611, loss = 0.12892802\n",
      "Iteration 2268, loss = 0.09298539\n",
      "Iteration 722, loss = 0.32515880\n",
      "Iteration 1363, loss = 0.24940492\n",
      "Iteration 2269, loss = 0.09282903\n",
      "Iteration 1748, loss = 0.13257815\n",
      "Iteration 891, loss = 0.30057308\n",
      "Iteration 2270, loss = 0.09274172\n",
      "Iteration 723, loss = 0.32496739\n",
      "Iteration 667, loss = 0.34606070\n",
      "Iteration 2271, loss = 0.09268369\n",
      "Iteration 2612, loss = 0.12882627\n",
      "Iteration 1364, loss = 0.24921854\n",
      "Iteration 2272, loss = 0.09258918\n",
      "Iteration 2273, loss = 0.09253631\n",
      "Iteration 1749, loss = 0.13250772\n",
      "Iteration 1365, loss = 0.24910447\n",
      "Iteration 1339, loss = 0.25989525\n",
      "Iteration 892, loss = 0.30031638\n",
      "Iteration 1366, loss = 0.24898895\n",
      "Iteration 2613, loss = 0.12879713\n",
      "Iteration 724, loss = 0.32484917\n",
      "Iteration 2274, loss = 0.09243774\n",
      "Iteration 1367, loss = 0.24886077\n",
      "Iteration 1750, loss = 0.13235481\n",
      "Iteration 725, loss = 0.32467473\n",
      "Iteration 2275, loss = 0.09241853\n",
      "Iteration 893, loss = 0.30003040\n",
      "Iteration 1368, loss = 0.24870476\n",
      "Iteration 2614, loss = 0.12878417\n",
      "Iteration 1751, loss = 0.13220580\n",
      "Iteration 668, loss = 0.34593498\n",
      "Iteration 1369, loss = 0.24859881\n",
      "Iteration 2276, loss = 0.09242489\n",
      "Iteration 2277, loss = 0.09222373\n",
      "Iteration 1370, loss = 0.24855533\n",
      "Iteration 1340, loss = 0.25967590\n",
      "Iteration 726, loss = 0.32447384\n",
      "Iteration 2278, loss = 0.09210495\n",
      "Iteration 2279, loss = 0.09222900\n",
      "Iteration 2615, loss = 0.12872914\n",
      "Iteration 1371, loss = 0.24833638\n",
      "Iteration 2280, loss = 0.09199488\n",
      "Iteration 894, loss = 0.29981821\n",
      "Iteration 1752, loss = 0.13216817\n",
      "Iteration 2616, loss = 0.12859075\n",
      "Iteration 2281, loss = 0.09202070\n",
      "Iteration 1372, loss = 0.24821650\n",
      "Iteration 2617, loss = 0.12864083\n",
      "Iteration 1753, loss = 0.13208925\n",
      "Iteration 727, loss = 0.32426006\n",
      "Iteration 1373, loss = 0.24820230\n",
      "Iteration 2282, loss = 0.09189389\n",
      "Iteration 669, loss = 0.34586467\n",
      "Iteration 2618, loss = 0.12844899\n",
      "Iteration 1341, loss = 0.25964324\n",
      "Iteration 1374, loss = 0.24801532\n",
      "Iteration 1754, loss = 0.13188670\n",
      "Iteration 728, loss = 0.32405119\n",
      "Iteration 1375, loss = 0.24783135\n",
      "Iteration 895, loss = 0.29953716\n",
      "Iteration 1342, loss = 0.25947928\n",
      "Iteration 1376, loss = 0.24771406\n",
      "Iteration 2283, loss = 0.09179401\n",
      "Iteration 729, loss = 0.32392821\n",
      "Iteration 670, loss = 0.34575896\n",
      "Iteration 2284, loss = 0.09177356\n",
      "Iteration 1377, loss = 0.24760236\n",
      "Iteration 1755, loss = 0.13179474\n",
      "Iteration 2619, loss = 0.12837889\n",
      "Iteration 2285, loss = 0.09163781\n",
      "Iteration 1343, loss = 0.25944692\n",
      "Iteration 730, loss = 0.32369088\n",
      "Iteration 2286, loss = 0.09153862\n",
      "Iteration 2287, loss = 0.09148682\n",
      "Iteration 1756, loss = 0.13171659\n",
      "Iteration 2620, loss = 0.12836303\n",
      "Iteration 731, loss = 0.32350157\n",
      "Iteration 1378, loss = 0.24744553\n",
      "Iteration 2288, loss = 0.09140063\n",
      "Iteration 2621, loss = 0.12830552\n",
      "Iteration 2289, loss = 0.09142113\n",
      "Iteration 1757, loss = 0.13161799\n",
      "Iteration 1344, loss = 0.25919110\n",
      "Iteration 2622, loss = 0.12818930\n",
      "Iteration 1379, loss = 0.24732252\n",
      "Iteration 896, loss = 0.29928239\n",
      "Iteration 671, loss = 0.34558387\n",
      "Iteration 732, loss = 0.32330656\n",
      "Iteration 2290, loss = 0.09122088\n",
      "Iteration 1380, loss = 0.24728638\n",
      "Iteration 2291, loss = 0.09117895\n",
      "Iteration 2623, loss = 0.12814916\n",
      "Iteration 2292, loss = 0.09120092\n",
      "Iteration 733, loss = 0.32315199\n",
      "Iteration 1381, loss = 0.24708734\n",
      "Iteration 2293, loss = 0.09105378\n",
      "Iteration 1345, loss = 0.25907558\n",
      "Iteration 2294, loss = 0.09114610\n",
      "Iteration 672, loss = 0.34545426\n",
      "Iteration 1758, loss = 0.13155590\n",
      "Iteration 1382, loss = 0.24693399\n",
      "Iteration 2295, loss = 0.09088534\n",
      "Iteration 1759, loss = 0.13142250\n",
      "Iteration 897, loss = 0.29913991\n",
      "Iteration 2296, loss = 0.09076212\n",
      "Iteration 2297, loss = 0.09074970\n",
      "Iteration 2624, loss = 0.12814316\n",
      "Iteration 734, loss = 0.32304208\n",
      "Iteration 1346, loss = 0.25905568\n",
      "Iteration 735, loss = 0.32275032\n",
      "Iteration 1383, loss = 0.24683453\n",
      "Iteration 2298, loss = 0.09071281\n",
      "Iteration 2625, loss = 0.12807659\n",
      "Iteration 1384, loss = 0.24677359\n",
      "Iteration 1760, loss = 0.13140163\n",
      "Iteration 2299, loss = 0.09057027\n",
      "Iteration 2626, loss = 0.12805074\n",
      "Iteration 2300, loss = 0.09051519\n",
      "Iteration 673, loss = 0.34533804\n",
      "Iteration 736, loss = 0.32253364\n",
      "Iteration 2301, loss = 0.09055582\n",
      "Iteration 2627, loss = 0.12788525\n",
      "Iteration 898, loss = 0.29889283\n",
      "Iteration 2302, loss = 0.09033907\n",
      "Iteration 1347, loss = 0.25888962\n",
      "Iteration 1385, loss = 0.24659606\n",
      "Iteration 2628, loss = 0.12783275\n",
      "Iteration 737, loss = 0.32235282\n",
      "Iteration 2629, loss = 0.12779838\n",
      "Iteration 1761, loss = 0.13138941\n",
      "Iteration 2303, loss = 0.09026874\n",
      "Iteration 674, loss = 0.34522303\n",
      "Iteration 2630, loss = 0.12768326\n",
      "Iteration 738, loss = 0.32221066\n",
      "Iteration 1386, loss = 0.24654796\n",
      "Iteration 1762, loss = 0.13112522\n",
      "Iteration 2631, loss = 0.12766625\n",
      "Iteration 2304, loss = 0.09023875\n",
      "Iteration 2305, loss = 0.09011906\n",
      "Iteration 675, loss = 0.34513626\n",
      "Iteration 1763, loss = 0.13102362\n",
      "Iteration 2306, loss = 0.09005950\n",
      "Iteration 2307, loss = 0.08999149\n",
      "Iteration 1348, loss = 0.25872774\n",
      "Iteration 1764, loss = 0.13095452\n",
      "Iteration 2308, loss = 0.08994700\n",
      "Iteration 2632, loss = 0.12758405\n",
      "Iteration 676, loss = 0.34497804\n",
      "Iteration 1387, loss = 0.24635896\n",
      "Iteration 2309, loss = 0.08983143\n",
      "Iteration 899, loss = 0.29859827\n",
      "Iteration 1349, loss = 0.25858600\n",
      "Iteration 739, loss = 0.32199358\n",
      "Iteration 2310, loss = 0.08992619\n",
      "Iteration 2633, loss = 0.12752665\n",
      "Iteration 2311, loss = 0.08971507\n",
      "Iteration 1388, loss = 0.24618802\n",
      "Iteration 1765, loss = 0.13081134\n",
      "Iteration 2634, loss = 0.12748250\n",
      "Iteration 2312, loss = 0.08974724\n",
      "Iteration 1350, loss = 0.25847697\n",
      "Iteration 677, loss = 0.34497027\n",
      "Iteration 1389, loss = 0.24608875\n",
      "Iteration 2313, loss = 0.08956303\n",
      "Iteration 740, loss = 0.32189189\n",
      "Iteration 2635, loss = 0.12741026\n",
      "Iteration 900, loss = 0.29847039\n",
      "Iteration 1351, loss = 0.25837184\n",
      "Iteration 1766, loss = 0.13081647\n",
      "Iteration 741, loss = 0.32156594\n",
      "Iteration 2314, loss = 0.08957784\n",
      "Iteration 2636, loss = 0.12737184\n",
      "Iteration 2315, loss = 0.08953312\n",
      "Iteration 1352, loss = 0.25819173\n",
      "Iteration 742, loss = 0.32146091\n",
      "Iteration 1390, loss = 0.24595569\n",
      "Iteration 678, loss = 0.34477177\n",
      "Iteration 2316, loss = 0.08936602\n",
      "Iteration 1767, loss = 0.13083314\n",
      "Iteration 1391, loss = 0.24583628\n",
      "Iteration 2317, loss = 0.08945962\n",
      "Iteration 2637, loss = 0.12736701\n",
      "Iteration 2318, loss = 0.08923947\n",
      "Iteration 743, loss = 0.32127375\n",
      "Iteration 2638, loss = 0.12725595\n",
      "Iteration 901, loss = 0.29849995\n",
      "Iteration 1353, loss = 0.25809783\n",
      "Iteration 2319, loss = 0.08927634\n",
      "Iteration 2639, loss = 0.12720790\n",
      "Iteration 744, loss = 0.32109030\n",
      "Iteration 1392, loss = 0.24570159\n",
      "Iteration 1768, loss = 0.13054110\n",
      "Iteration 679, loss = 0.34471037\n",
      "Iteration 2320, loss = 0.08933015\n",
      "Iteration 1354, loss = 0.25795405\n",
      "Iteration 745, loss = 0.32084115\n",
      "Iteration 2640, loss = 0.12717389\n",
      "Iteration 2321, loss = 0.08901612\n",
      "Iteration 1393, loss = 0.24558722\n",
      "Iteration 902, loss = 0.29793591\n",
      "Iteration 2322, loss = 0.08899487\n",
      "Iteration 1355, loss = 0.25788293\n",
      "Iteration 746, loss = 0.32067974\n",
      "Iteration 2323, loss = 0.08902405\n",
      "Iteration 1769, loss = 0.13046355\n",
      "Iteration 1394, loss = 0.24542833\n",
      "Iteration 2641, loss = 0.12702854\n",
      "Iteration 2324, loss = 0.08878413\n",
      "Iteration 747, loss = 0.32048335\n",
      "Iteration 2325, loss = 0.08881110\n",
      "Iteration 680, loss = 0.34452208\n",
      "Iteration 1395, loss = 0.24543493\n",
      "Iteration 2326, loss = 0.08878459\n",
      "Iteration 2642, loss = 0.12701423\n",
      "Iteration 1396, loss = 0.24529436\n",
      "Iteration 1356, loss = 0.25781584\n",
      "Iteration 2327, loss = 0.08858032\n",
      "Iteration 1397, loss = 0.24509620\n",
      "Iteration 1770, loss = 0.13034957\n",
      "Iteration 2328, loss = 0.08851920\n",
      "Iteration 2643, loss = 0.12694693\n",
      "Iteration 2329, loss = 0.08841747\n",
      "Iteration 1357, loss = 0.25764876\n",
      "Iteration 2330, loss = 0.08841809\n",
      "Iteration 903, loss = 0.29766366\n",
      "Iteration 1398, loss = 0.24493842\n",
      "Iteration 681, loss = 0.34441732\n",
      "Iteration 1771, loss = 0.13022885\n",
      "Iteration 748, loss = 0.32028774\n",
      "Iteration 1772, loss = 0.13016064\n",
      "Iteration 2331, loss = 0.08853048\n",
      "Iteration 2644, loss = 0.12689676\n",
      "Iteration 1399, loss = 0.24477557\n",
      "Iteration 749, loss = 0.32009421\n",
      "Iteration 682, loss = 0.34426847\n",
      "Iteration 2332, loss = 0.08823693\n",
      "Iteration 2645, loss = 0.12693533\n",
      "Iteration 1358, loss = 0.25747867\n",
      "Iteration 1400, loss = 0.24470894\n",
      "Iteration 2333, loss = 0.08823530\n",
      "Iteration 2334, loss = 0.08835460\n",
      "Iteration 1773, loss = 0.13008146\n",
      "Iteration 2646, loss = 0.12682013\n",
      "Iteration 750, loss = 0.31997793\n",
      "Iteration 2335, loss = 0.08801422\n",
      "Iteration 1401, loss = 0.24467375\n",
      "Iteration 1359, loss = 0.25743681\n",
      "Iteration 2647, loss = 0.12670750\n",
      "Iteration 2336, loss = 0.08798573\n",
      "Iteration 1774, loss = 0.12992804\n",
      "Iteration 904, loss = 0.29748598\n",
      "Iteration 2337, loss = 0.08799207\n",
      "Iteration 2648, loss = 0.12664051\n",
      "Iteration 751, loss = 0.31990040\n",
      "Iteration 1775, loss = 0.12998981\n",
      "Iteration 683, loss = 0.34416461\n",
      "Iteration 1402, loss = 0.24459904\n",
      "Iteration 2338, loss = 0.08782924\n",
      "Iteration 1360, loss = 0.25726609\n",
      "Iteration 2339, loss = 0.08776959\n",
      "Iteration 2649, loss = 0.12662099\n",
      "Iteration 2340, loss = 0.08768247\n",
      "Iteration 1776, loss = 0.12999695\n",
      "Iteration 1403, loss = 0.24432049\n",
      "Iteration 684, loss = 0.34407798\n",
      "Iteration 905, loss = 0.29718886\n",
      "Iteration 752, loss = 0.31956237\n",
      "Iteration 2341, loss = 0.08767510\n",
      "Iteration 2650, loss = 0.12657464\n",
      "Iteration 1361, loss = 0.25715962\n",
      "Iteration 1404, loss = 0.24417422\n",
      "Iteration 1777, loss = 0.12977223\n",
      "Iteration 2342, loss = 0.08763132\n",
      "Iteration 753, loss = 0.31935122\n",
      "Iteration 1405, loss = 0.24412069\n",
      "Iteration 2651, loss = 0.12645068\n",
      "Iteration 1362, loss = 0.25700956\n",
      "Iteration 2343, loss = 0.08751104\n",
      "Iteration 1778, loss = 0.12959595\n",
      "Iteration 2344, loss = 0.08740033\n",
      "Iteration 685, loss = 0.34396950\n",
      "Iteration 2652, loss = 0.12646373\n",
      "Iteration 1406, loss = 0.24397663\n",
      "Iteration 1363, loss = 0.25690813\n",
      "Iteration 1779, loss = 0.12949171\n",
      "Iteration 2345, loss = 0.08749847\n",
      "Iteration 754, loss = 0.31917056\n",
      "Iteration 906, loss = 0.29694038\n",
      "Iteration 2653, loss = 0.12632099\n",
      "Iteration 2346, loss = 0.08730746\n",
      "Iteration 2347, loss = 0.08736416\n",
      "Iteration 1364, loss = 0.25676996\n",
      "Iteration 2654, loss = 0.12635775\n",
      "Iteration 1407, loss = 0.24377825\n",
      "Iteration 686, loss = 0.34379637\n",
      "Iteration 1780, loss = 0.12942087\n",
      "Iteration 2348, loss = 0.08719140\n",
      "Iteration 1365, loss = 0.25663731\n",
      "Iteration 2655, loss = 0.12624324\n",
      "Iteration 755, loss = 0.31917910\n",
      "Iteration 2349, loss = 0.08714937\n",
      "Iteration 756, loss = 0.31880584\n",
      "Iteration 687, loss = 0.34366663\n",
      "Iteration 2350, loss = 0.08707542\n",
      "Iteration 907, loss = 0.29673867\n",
      "Iteration 1366, loss = 0.25650164\n",
      "Iteration 1408, loss = 0.24368828\n",
      "Iteration 757, loss = 0.31865245\n",
      "Iteration 2656, loss = 0.12617846\n",
      "Iteration 2351, loss = 0.08704147\n",
      "Iteration 688, loss = 0.34357599\n",
      "Iteration 1367, loss = 0.25640493\n",
      "Iteration 2657, loss = 0.12615611\n",
      "Iteration 758, loss = 0.31844883\n",
      "Iteration 1409, loss = 0.24356604\n",
      "Iteration 908, loss = 0.29658222\n",
      "Iteration 689, loss = 0.34344620\n",
      "Iteration 2352, loss = 0.08701350Iteration 1781, loss = 0.12953632\n",
      "\n",
      "Iteration 759, loss = 0.31823243\n",
      "Iteration 1410, loss = 0.24341306\n",
      "Iteration 2658, loss = 0.12602333\n",
      "Iteration 1368, loss = 0.25626911\n",
      "Iteration 2353, loss = 0.08689975\n",
      "Iteration 1411, loss = 0.24329509\n",
      "Iteration 760, loss = 0.31815512\n",
      "Iteration 1782, loss = 0.12926470\n",
      "Iteration 909, loss = 0.29629998\n",
      "Iteration 1369, loss = 0.25616304\n",
      "Iteration 690, loss = 0.34331493\n",
      "Iteration 761, loss = 0.31786206\n",
      "Iteration 2659, loss = 0.12607749\n",
      "Iteration 1412, loss = 0.24322174\n",
      "Iteration 2354, loss = 0.08676140\n",
      "Iteration 1783, loss = 0.12919298\n",
      "Iteration 2660, loss = 0.12594480\n",
      "Iteration 762, loss = 0.31770606\n",
      "Iteration 2661, loss = 0.12591624\n",
      "Iteration 1413, loss = 0.24307037\n",
      "Iteration 1784, loss = 0.12909082\n",
      "Iteration 910, loss = 0.29607444\n",
      "Iteration 2355, loss = 0.08678490\n",
      "Iteration 1370, loss = 0.25601435\n",
      "Iteration 2356, loss = 0.08666005\n",
      "Iteration 2662, loss = 0.12581096\n",
      "Iteration 691, loss = 0.34321027\n",
      "Iteration 763, loss = 0.31744446\n",
      "Iteration 1414, loss = 0.24290833\n",
      "Iteration 2663, loss = 0.12577126\n",
      "Iteration 1785, loss = 0.12894262\n",
      "Iteration 2357, loss = 0.08663531\n",
      "Iteration 2358, loss = 0.08653409\n",
      "Iteration 2359, loss = 0.08648170\n",
      "Iteration 2664, loss = 0.12573854\n",
      "Iteration 1371, loss = 0.25610142\n",
      "Iteration 1415, loss = 0.24279905\n",
      "Iteration 2665, loss = 0.12571940\n",
      "Iteration 1786, loss = 0.12886813\n",
      "Iteration 764, loss = 0.31725405\n",
      "Iteration 2360, loss = 0.08643614\n",
      "Iteration 911, loss = 0.29581541\n",
      "Iteration 2361, loss = 0.08634762\n",
      "Iteration 2362, loss = 0.08630647\n",
      "Iteration 765, loss = 0.31710616\n",
      "Iteration 1416, loss = 0.24265963\n",
      "Iteration 2666, loss = 0.12568922\n",
      "Iteration 692, loss = 0.34308543\n",
      "Iteration 1787, loss = 0.12886245\n",
      "Iteration 1372, loss = 0.25587670\n",
      "Iteration 2363, loss = 0.08626718\n",
      "Iteration 766, loss = 0.31702213\n",
      "Iteration 1788, loss = 0.12874959\n",
      "Iteration 1417, loss = 0.24257689\n",
      "Iteration 693, loss = 0.34295340\n",
      "Iteration 2667, loss = 0.12562275\n",
      "Iteration 2364, loss = 0.08618073\n",
      "Iteration 1373, loss = 0.25567712\n",
      "Iteration 767, loss = 0.31670691\n",
      "Iteration 2668, loss = 0.12549179\n",
      "Iteration 1418, loss = 0.24244297\n",
      "Iteration 2365, loss = 0.08613876\n",
      "Iteration 1374, loss = 0.25557389\n",
      "Iteration 912, loss = 0.29559230\n",
      "Iteration 1789, loss = 0.12858592\n",
      "Iteration 2669, loss = 0.12547056\n",
      "Iteration 1419, loss = 0.24232002\n",
      "Iteration 2366, loss = 0.08610867\n",
      "Iteration 2670, loss = 0.12541667\n",
      "Iteration 1420, loss = 0.24213143\n",
      "Iteration 2367, loss = 0.08617652\n",
      "Iteration 694, loss = 0.34284280\n",
      "Iteration 1375, loss = 0.25541756\n",
      "Iteration 1790, loss = 0.12855873\n",
      "Iteration 768, loss = 0.31669132\n",
      "Iteration 2368, loss = 0.08598077\n",
      "Iteration 2369, loss = 0.08584399\n",
      "Iteration 1421, loss = 0.24202045\n",
      "Iteration 1791, loss = 0.12838227\n",
      "Iteration 2671, loss = 0.12532425\n",
      "Iteration 2370, loss = 0.08588283\n",
      "Iteration 913, loss = 0.29541968\n",
      "Iteration 1376, loss = 0.25534051\n",
      "Iteration 2371, loss = 0.08585745\n",
      "Iteration 2372, loss = 0.08571282\n",
      "Iteration 1422, loss = 0.24190860\n",
      "Iteration 2373, loss = 0.08578286\n",
      "Iteration 769, loss = 0.31638676\n",
      "Iteration 2672, loss = 0.12523844\n",
      "Iteration 695, loss = 0.34271894\n",
      "Iteration 2374, loss = 0.08567740\n",
      "Iteration 2375, loss = 0.08556890\n",
      "Iteration 1377, loss = 0.25526789\n",
      "Iteration 2673, loss = 0.12518092\n",
      "Iteration 1792, loss = 0.12832196\n",
      "Iteration 1423, loss = 0.24174758\n",
      "Iteration 2376, loss = 0.08551172\n",
      "Iteration 914, loss = 0.29513286\n",
      "Iteration 2674, loss = 0.12513849\n",
      "Iteration 1424, loss = 0.24170487\n",
      "Iteration 1378, loss = 0.25507442\n",
      "Iteration 770, loss = 0.31608838\n",
      "Iteration 2377, loss = 0.08536808\n",
      "Iteration 1425, loss = 0.24167652\n",
      "Iteration 2675, loss = 0.12514829\n",
      "Iteration 771, loss = 0.31596068\n",
      "Iteration 1426, loss = 0.24142218\n",
      "Iteration 2378, loss = 0.08534780\n",
      "Iteration 696, loss = 0.34258206\n",
      "Iteration 1379, loss = 0.25497580\n",
      "Iteration 1427, loss = 0.24144326\n",
      "Iteration 2676, loss = 0.12497885\n",
      "Iteration 915, loss = 0.29501059\n",
      "Iteration 772, loss = 0.31592547\n",
      "Iteration 2379, loss = 0.08519922\n",
      "Iteration 1428, loss = 0.24122823\n",
      "Iteration 1380, loss = 0.25485762\n",
      "Iteration 1793, loss = 0.12819157\n",
      "Iteration 1429, loss = 0.24100988\n",
      "Iteration 773, loss = 0.31555405\n",
      "Iteration 697, loss = 0.34248932\n",
      "Iteration 2380, loss = 0.08522769\n",
      "Iteration 2677, loss = 0.12494640\n",
      "Iteration 1381, loss = 0.25470444\n",
      "Iteration 2381, loss = 0.08508649\n",
      "Iteration 1794, loss = 0.12827051\n",
      "Iteration 916, loss = 0.29482242\n",
      "Iteration 2382, loss = 0.08515083\n",
      "Iteration 1430, loss = 0.24096468\n",
      "Iteration 698, loss = 0.34234701\n",
      "Iteration 2678, loss = 0.12486931\n",
      "Iteration 2383, loss = 0.08494311\n",
      "Iteration 1431, loss = 0.24076319\n",
      "Iteration 1795, loss = 0.12808025\n",
      "Iteration 2384, loss = 0.08496625\n",
      "Iteration 1432, loss = 0.24061595\n",
      "Iteration 1382, loss = 0.25460618\n",
      "Iteration 2385, loss = 0.08489319\n",
      "Iteration 1796, loss = 0.12801110\n",
      "Iteration 2386, loss = 0.08478528\n",
      "Iteration 1433, loss = 0.24048025\n",
      "Iteration 2679, loss = 0.12487246\n",
      "Iteration 917, loss = 0.29451621\n",
      "Iteration 2387, loss = 0.08471494\n",
      "Iteration 699, loss = 0.34222340\n",
      "Iteration 2388, loss = 0.08472014\n",
      "Iteration 2680, loss = 0.12479808\n",
      "Iteration 2389, loss = 0.08464358\n",
      "Iteration 1797, loss = 0.12785183\n",
      "Iteration 1383, loss = 0.25447501\n",
      "Iteration 1434, loss = 0.24036447\n",
      "Iteration 2681, loss = 0.12479566\n",
      "Iteration 700, loss = 0.34210913\n",
      "Iteration 2390, loss = 0.08463076\n",
      "Iteration 1435, loss = 0.24031390\n",
      "Iteration 1798, loss = 0.12787743\n",
      "Iteration 2391, loss = 0.08454288\n",
      "Iteration 2682, loss = 0.12470395\n",
      "Iteration 1436, loss = 0.24013248\n",
      "Iteration 1799, loss = 0.12769873\n",
      "Iteration 1437, loss = 0.24011946\n",
      "Iteration 1384, loss = 0.25432196\n",
      "Iteration 2392, loss = 0.08447579\n",
      "Iteration 701, loss = 0.34198960\n",
      "Iteration 1438, loss = 0.23988301\n",
      "Iteration 1800, loss = 0.12757847\n",
      "Iteration 2683, loss = 0.12459620\n",
      "Iteration 2393, loss = 0.08436551\n",
      "Iteration 918, loss = 0.29423095\n",
      "Iteration 2394, loss = 0.08438894\n",
      "Iteration 1385, loss = 0.25427491\n",
      "Iteration 1439, loss = 0.23974550\n",
      "Iteration 702, loss = 0.34187976\n",
      "Iteration 2395, loss = 0.08425369\n",
      "Iteration 1801, loss = 0.12764455\n",
      "Iteration 2396, loss = 0.08415501\n",
      "Iteration 1440, loss = 0.23960609\n",
      "Iteration 2684, loss = 0.12456400\n",
      "Iteration 2397, loss = 0.08419694\n",
      "Iteration 1802, loss = 0.12742870\n",
      "Iteration 2398, loss = 0.08425807\n",
      "Iteration 1441, loss = 0.23947703\n",
      "Iteration 703, loss = 0.34175994\n",
      "Iteration 2399, loss = 0.08398753\n",
      "Iteration 2400, loss = 0.08391948\n",
      "Iteration 2685, loss = 0.12453654\n",
      "Iteration 1803, loss = 0.12753260\n",
      "Iteration 1386, loss = 0.25408891\n",
      "Iteration 1442, loss = 0.23936920\n",
      "Iteration 2686, loss = 0.12443544\n",
      "Iteration 919, loss = 0.29394628\n",
      "Iteration 2401, loss = 0.08384705\n",
      "Iteration 1804, loss = 0.12728172\n",
      "Iteration 704, loss = 0.34162512\n",
      "Iteration 1443, loss = 0.23925541\n",
      "Iteration 2687, loss = 0.12447984\n",
      "Iteration 2402, loss = 0.08402852\n",
      "Iteration 1387, loss = 0.25405182\n",
      "Iteration 2403, loss = 0.08377436\n",
      "Iteration 1444, loss = 0.23915590\n",
      "Iteration 705, loss = 0.34148906\n",
      "Iteration 2404, loss = 0.08378511\n",
      "Iteration 1805, loss = 0.12720797\n",
      "Iteration 2405, loss = 0.08378564\n",
      "Iteration 2688, loss = 0.12441098\n",
      "Iteration 2406, loss = 0.08357273\n",
      "Iteration 1806, loss = 0.12713802\n",
      "Iteration 2407, loss = 0.08359382\n",
      "Iteration 2689, loss = 0.12429830\n",
      "Iteration 920, loss = 0.29373347\n",
      "Iteration 1445, loss = 0.23899461\n",
      "Iteration 2408, loss = 0.08346120\n",
      "Iteration 1388, loss = 0.25391187\n",
      "Iteration 2690, loss = 0.12421738\n",
      "Iteration 1446, loss = 0.23885555\n",
      "Iteration 706, loss = 0.34145439\n",
      "Iteration 1807, loss = 0.12719661\n",
      "Iteration 2409, loss = 0.08338972\n",
      "Iteration 2691, loss = 0.12421943\n",
      "Iteration 1447, loss = 0.23872544\n",
      "Iteration 2692, loss = 0.12412293\n",
      "Iteration 2410, loss = 0.08338826\n",
      "Iteration 1808, loss = 0.12697000Iteration 1448, loss = 0.23866274\n",
      "\n",
      "Iteration 1389, loss = 0.25388762\n",
      "Iteration 2693, loss = 0.12401469\n",
      "Iteration 707, loss = 0.34127583\n",
      "Iteration 2411, loss = 0.08330635\n",
      "Iteration 921, loss = 0.29351502\n",
      "Iteration 2694, loss = 0.12396964\n",
      "Iteration 1809, loss = 0.12688129\n",
      "Iteration 1390, loss = 0.25368032\n",
      "Iteration 2412, loss = 0.08329874\n",
      "Iteration 1449, loss = 0.23847374\n",
      "Iteration 708, loss = 0.34115458\n",
      "Iteration 2695, loss = 0.12401621\n",
      "Iteration 1810, loss = 0.12679438\n",
      "Iteration 2413, loss = 0.08319261\n",
      "Iteration 1450, loss = 0.23836606\n",
      "Iteration 2414, loss = 0.08311151\n",
      "Iteration 2415, loss = 0.08308336\n",
      "Iteration 1811, loss = 0.12671653\n",
      "Iteration 2416, loss = 0.08304043\n",
      "Iteration 774, loss = 0.31532668\n",
      "Iteration 2696, loss = 0.12388965\n",
      "Iteration 2417, loss = 0.08294241\n",
      "Iteration 1451, loss = 0.23823825\n",
      "Iteration 2418, loss = 0.08293053\n",
      "Iteration 709, loss = 0.34103538\n",
      "Iteration 922, loss = 0.29332849\n",
      "Iteration 2419, loss = 0.08286435\n",
      "Iteration 1812, loss = 0.12660618\n",
      "Iteration 2420, loss = 0.08278800\n",
      "Iteration 1391, loss = 0.25355291\n",
      "Iteration 2697, loss = 0.12379520\n",
      "Iteration 775, loss = 0.31516804\n",
      "Iteration 1452, loss = 0.23811634\n",
      "Iteration 2421, loss = 0.08267735\n",
      "Iteration 1453, loss = 0.23796671\n",
      "Iteration 1813, loss = 0.12650852\n",
      "Iteration 1392, loss = 0.25336923\n",
      "Iteration 2422, loss = 0.08262069\n",
      "Iteration 1454, loss = 0.23785006\n",
      "Iteration 923, loss = 0.29321064\n",
      "Iteration 710, loss = 0.34092742\n",
      "Iteration 2698, loss = 0.12379338\n",
      "Iteration 1814, loss = 0.12644115\n",
      "Iteration 2423, loss = 0.08256957\n",
      "Iteration 2424, loss = 0.08253474\n",
      "Iteration 2699, loss = 0.12371974\n",
      "Iteration 1455, loss = 0.23779759\n",
      "Iteration 1393, loss = 0.25326482\n",
      "Iteration 2425, loss = 0.08253391\n",
      "Iteration 2700, loss = 0.12383310\n",
      "Iteration 1456, loss = 0.23770878\n",
      "Iteration 776, loss = 0.31501147\n",
      "Iteration 1394, loss = 0.25316768\n",
      "Iteration 1457, loss = 0.23754257\n",
      "Iteration 2426, loss = 0.08239993\n",
      "Iteration 777, loss = 0.31482703\n",
      "Iteration 1815, loss = 0.12658136\n",
      "Iteration 1458, loss = 0.23743597\n",
      "Iteration 2701, loss = 0.12356094\n",
      "Iteration 711, loss = 0.34076273\n",
      "Iteration 778, loss = 0.31455119\n",
      "Iteration 1816, loss = 0.12625231\n",
      "Iteration 2427, loss = 0.08235344\n",
      "Iteration 1459, loss = 0.23724826\n",
      "Iteration 2428, loss = 0.08250135\n",
      "Iteration 2702, loss = 0.12358906\n",
      "Iteration 1395, loss = 0.25301741\n",
      "Iteration 1817, loss = 0.12629776\n",
      "Iteration 924, loss = 0.29292322\n",
      "Iteration 2429, loss = 0.08223533\n",
      "Iteration 2703, loss = 0.12363442\n",
      "Iteration 1818, loss = 0.12640184\n",
      "Iteration 2430, loss = 0.08214960\n",
      "Iteration 1396, loss = 0.25286492\n",
      "Iteration 779, loss = 0.31438744\n",
      "Iteration 2704, loss = 0.12342430\n",
      "Iteration 712, loss = 0.34068541\n",
      "Iteration 1460, loss = 0.23717909\n",
      "Iteration 2431, loss = 0.08213756\n",
      "Iteration 2705, loss = 0.12337528\n",
      "Iteration 2432, loss = 0.08214959\n",
      "Iteration 1397, loss = 0.25282636\n",
      "Iteration 1461, loss = 0.23699444\n",
      "Iteration 1819, loss = 0.12606377\n",
      "Iteration 925, loss = 0.29278795\n",
      "Iteration 2433, loss = 0.08206465\n",
      "Iteration 713, loss = 0.34053869\n",
      "Iteration 2706, loss = 0.12342554\n",
      "Iteration 1820, loss = 0.12592159\n",
      "Iteration 1462, loss = 0.23684192\n",
      "Iteration 2707, loss = 0.12329574\n",
      "Iteration 1398, loss = 0.25267191\n",
      "Iteration 1463, loss = 0.23672140\n",
      "Iteration 714, loss = 0.34041286\n",
      "Iteration 1821, loss = 0.12589268\n",
      "Iteration 2708, loss = 0.12328350\n",
      "Iteration 926, loss = 0.29239921\n",
      "Iteration 1464, loss = 0.23659997\n",
      "Iteration 2434, loss = 0.08196182Iteration 1822, loss = 0.12587160\n",
      "\n",
      "Iteration 1399, loss = 0.25250679\n",
      "Iteration 2709, loss = 0.12317214\n",
      "Iteration 715, loss = 0.34029147\n",
      "Iteration 1465, loss = 0.23646243\n",
      "Iteration 927, loss = 0.29241720\n",
      "Iteration 780, loss = 0.31415167\n",
      "Iteration 1466, loss = 0.23635101\n",
      "Iteration 1823, loss = 0.12587611\n",
      "Iteration 2435, loss = 0.08194518\n",
      "Iteration 2710, loss = 0.12322927\n",
      "Iteration 1467, loss = 0.23622168\n",
      "Iteration 2711, loss = 0.12310518\n",
      "Iteration 2436, loss = 0.08186193\n",
      "Iteration 1824, loss = 0.12583090\n",
      "Iteration 1400, loss = 0.25238296\n",
      "Iteration 716, loss = 0.34016818\n",
      "Iteration 2437, loss = 0.08180132\n",
      "Iteration 781, loss = 0.31394390\n",
      "Iteration 2438, loss = 0.08173307\n",
      "Iteration 1825, loss = 0.12565981\n",
      "Iteration 2439, loss = 0.08170227\n",
      "Iteration 2712, loss = 0.12307416\n",
      "Iteration 2440, loss = 0.08160506\n",
      "Iteration 1468, loss = 0.23612526\n",
      "Iteration 928, loss = 0.29201381\n",
      "Iteration 2441, loss = 0.08164430\n",
      "Iteration 1401, loss = 0.25227500\n",
      "Iteration 2713, loss = 0.12300391\n",
      "Iteration 782, loss = 0.31376095\n",
      "Iteration 2442, loss = 0.08158524\n",
      "Iteration 717, loss = 0.34006976\n",
      "Iteration 2443, loss = 0.08143571\n",
      "Iteration 1469, loss = 0.23604589\n",
      "Iteration 1826, loss = 0.12550431\n",
      "Iteration 2444, loss = 0.08142961\n",
      "Iteration 1402, loss = 0.25214482\n",
      "Iteration 783, loss = 0.31362086\n",
      "Iteration 1470, loss = 0.23585743\n",
      "Iteration 2445, loss = 0.08135128\n",
      "Iteration 718, loss = 0.33994602\n",
      "Iteration 2714, loss = 0.12286897\n",
      "Iteration 1471, loss = 0.23572792\n",
      "Iteration 2446, loss = 0.08134207\n",
      "Iteration 784, loss = 0.31353351\n",
      "Iteration 2447, loss = 0.08130187\n",
      "Iteration 2715, loss = 0.12281355\n",
      "Iteration 2448, loss = 0.08117089\n",
      "Iteration 1827, loss = 0.12548764\n",
      "Iteration 929, loss = 0.29169523\n",
      "Iteration 2716, loss = 0.12289811\n",
      "Iteration 719, loss = 0.33980691\n",
      "Iteration 1472, loss = 0.23561587\n",
      "Iteration 2449, loss = 0.08116534\n",
      "Iteration 1403, loss = 0.25208150\n",
      "Iteration 1828, loss = 0.12532847\n",
      "Iteration 1473, loss = 0.23549819\n",
      "Iteration 785, loss = 0.31320751\n",
      "Iteration 720, loss = 0.33974116\n",
      "Iteration 2717, loss = 0.12279648\n",
      "Iteration 1474, loss = 0.23541889\n",
      "Iteration 1829, loss = 0.12533516\n",
      "Iteration 2450, loss = 0.08121522\n",
      "Iteration 786, loss = 0.31305249\n",
      "Iteration 1475, loss = 0.23523057\n",
      "Iteration 2451, loss = 0.08108555\n",
      "Iteration 2718, loss = 0.12271591\n",
      "Iteration 1404, loss = 0.25202194\n",
      "Iteration 1476, loss = 0.23510883\n",
      "Iteration 930, loss = 0.29146799\n",
      "Iteration 1830, loss = 0.12529434\n",
      "Iteration 2452, loss = 0.08109242\n",
      "Iteration 2719, loss = 0.12270824\n",
      "Iteration 721, loss = 0.33969213\n",
      "Iteration 2453, loss = 0.08098480\n",
      "Iteration 1831, loss = 0.12510475\n",
      "Iteration 1477, loss = 0.23504808\n",
      "Iteration 787, loss = 0.31299341\n",
      "Iteration 2720, loss = 0.12257846\n",
      "Iteration 2454, loss = 0.08093175\n",
      "Iteration 1405, loss = 0.25188709\n",
      "Iteration 722, loss = 0.33945402\n",
      "Iteration 2721, loss = 0.12249725\n",
      "Iteration 2455, loss = 0.08087795\n",
      "Iteration 1832, loss = 0.12501368\n",
      "Iteration 1478, loss = 0.23485579\n",
      "Iteration 2456, loss = 0.08081316\n",
      "Iteration 788, loss = 0.31260900\n",
      "Iteration 1833, loss = 0.12494592\n",
      "Iteration 1406, loss = 0.25166010\n",
      "Iteration 2457, loss = 0.08077346\n",
      "Iteration 789, loss = 0.31241782\n",
      "Iteration 1479, loss = 0.23482003\n",
      "Iteration 931, loss = 0.29127308\n",
      "Iteration 2458, loss = 0.08069199\n",
      "Iteration 1480, loss = 0.23467363\n",
      "Iteration 723, loss = 0.33936263\n",
      "Iteration 1834, loss = 0.12484753\n",
      "Iteration 2459, loss = 0.08059707\n",
      "Iteration 2460, loss = 0.08062563\n",
      "Iteration 790, loss = 0.31222362\n",
      "Iteration 2722, loss = 0.12246134\n",
      "Iteration 1835, loss = 0.12490826\n",
      "Iteration 2461, loss = 0.08062611\n",
      "Iteration 1481, loss = 0.23452983\n",
      "Iteration 2462, loss = 0.08043587\n",
      "Iteration 724, loss = 0.33924449\n",
      "Iteration 2723, loss = 0.12238358\n",
      "Iteration 1482, loss = 0.23436285\n",
      "Iteration 2463, loss = 0.08039277\n",
      "Iteration 932, loss = 0.29098663\n",
      "Iteration 1836, loss = 0.12473513\n",
      "Iteration 2464, loss = 0.08038159\n",
      "Iteration 1483, loss = 0.23430096\n",
      "Iteration 791, loss = 0.31201816\n",
      "Iteration 1407, loss = 0.25165631\n",
      "Iteration 725, loss = 0.33907772\n",
      "Iteration 1837, loss = 0.12469316\n",
      "Iteration 2465, loss = 0.08037081\n",
      "Iteration 1484, loss = 0.23413692\n",
      "Iteration 2724, loss = 0.12234480\n",
      "Iteration 792, loss = 0.31185478\n",
      "Iteration 726, loss = 0.33897578\n",
      "Iteration 2466, loss = 0.08024456\n",
      "Iteration 1838, loss = 0.12455888\n",
      "Iteration 1408, loss = 0.25146647\n",
      "Iteration 2467, loss = 0.08017401\n",
      "Iteration 933, loss = 0.29091814\n",
      "Iteration 2725, loss = 0.12228816\n",
      "Iteration 1485, loss = 0.23404315\n",
      "Iteration 2468, loss = 0.08020420\n",
      "Iteration 793, loss = 0.31168534\n",
      "Iteration 2469, loss = 0.08010320\n",
      "Iteration 2470, loss = 0.08005258\n",
      "Iteration 727, loss = 0.33885479\n",
      "Iteration 2471, loss = 0.08002199\n",
      "Iteration 794, loss = 0.31144111\n",
      "Iteration 1409, loss = 0.25131447\n",
      "Iteration 2726, loss = 0.12227401\n",
      "Iteration 1839, loss = 0.12460555\n",
      "Iteration 1486, loss = 0.23401613\n",
      "Iteration 2472, loss = 0.07991807\n",
      "Iteration 2727, loss = 0.12217651\n",
      "Iteration 2473, loss = 0.07991349\n",
      "Iteration 728, loss = 0.33872636\n",
      "Iteration 934, loss = 0.29061454\n",
      "Iteration 2474, loss = 0.07980648\n",
      "Iteration 2728, loss = 0.12235578\n",
      "Iteration 1840, loss = 0.12441740\n",
      "Iteration 2475, loss = 0.07980210\n",
      "Iteration 1487, loss = 0.23375151\n",
      "Iteration 1410, loss = 0.25133318\n",
      "Iteration 1841, loss = 0.12435598\n",
      "Iteration 1488, loss = 0.23367012\n",
      "Iteration 795, loss = 0.31130973\n",
      "Iteration 2476, loss = 0.07975938\n",
      "Iteration 2729, loss = 0.12208962\n",
      "Iteration 2477, loss = 0.07970336\n",
      "Iteration 1489, loss = 0.23357031\n",
      "Iteration 729, loss = 0.33860675\n",
      "Iteration 1411, loss = 0.25110331\n",
      "Iteration 2478, loss = 0.07961654\n",
      "Iteration 2479, loss = 0.07961461\n",
      "Iteration 796, loss = 0.31117075\n",
      "Iteration 2730, loss = 0.12203375\n",
      "Iteration 730, loss = 0.33849929\n",
      "Iteration 1490, loss = 0.23354324\n",
      "Iteration 2480, loss = 0.07958787\n",
      "Iteration 2731, loss = 0.12195896\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 797, loss = 0.31083929\n",
      "Iteration 935, loss = 0.29037829\n",
      "Iteration 1491, loss = 0.23330035\n",
      "Iteration 1842, loss = 0.12434719\n",
      "Iteration 2481, loss = 0.07948372\n",
      "Iteration 1412, loss = 0.25095477\n",
      "Iteration 1492, loss = 0.23314712\n",
      "Iteration 798, loss = 0.31065017\n",
      "Iteration 1843, loss = 0.12416503\n",
      "Iteration 2482, loss = 0.07943897\n",
      "Iteration 2483, loss = 0.07942874\n",
      "Iteration 731, loss = 0.33836951\n",
      "Iteration 1493, loss = 0.23303775\n",
      "Iteration 2484, loss = 0.07940377\n",
      "Iteration 2485, loss = 0.07927031\n",
      "Iteration 1844, loss = 0.12420898\n",
      "Iteration 2486, loss = 0.07925025\n",
      "Iteration 936, loss = 0.29013950\n",
      "Iteration 1413, loss = 0.25082922\n",
      "Iteration 799, loss = 0.31044292\n",
      "Iteration 2487, loss = 0.07920487\n",
      "Iteration 1845, loss = 0.12405201\n",
      "Iteration 1494, loss = 0.23291448\n",
      "Iteration 732, loss = 0.33824285\n",
      "Iteration 2488, loss = 0.07925107\n",
      "Iteration 1846, loss = 0.12403997\n",
      "Iteration 2489, loss = 0.07907865\n",
      "Iteration 1414, loss = 0.25081268\n",
      "Iteration 800, loss = 0.31026848\n",
      "Iteration 1495, loss = 0.23277236\n",
      "Iteration 733, loss = 0.33813953\n",
      "Iteration 937, loss = 0.28984992\n",
      "Iteration 2490, loss = 0.07909826\n",
      "Iteration 1496, loss = 0.23264659\n",
      "Iteration 2491, loss = 0.07896516\n",
      "Iteration 1847, loss = 0.12397566\n",
      "Iteration 2492, loss = 0.07893472\n",
      "Iteration 1415, loss = 0.25061782\n",
      "Iteration 801, loss = 0.31007506Iteration 734, loss = 0.33801668\n",
      "\n",
      "Iteration 1497, loss = 0.23255880\n",
      "Iteration 2493, loss = 0.07886575\n",
      "Iteration 1, loss = 0.80689323\n",
      "Iteration 2494, loss = 0.07883769\n",
      "Iteration 802, loss = 0.30992652\n",
      "Iteration 2495, loss = 0.07888355\n",
      "Iteration 735, loss = 0.33793053\n",
      "Iteration 1498, loss = 0.23241421\n",
      "Iteration 1848, loss = 0.12392127\n",
      "Iteration 938, loss = 0.28971787\n",
      "Iteration 2496, loss = 0.07885741\n",
      "Iteration 2497, loss = 0.07875702\n",
      "Iteration 1416, loss = 0.25060070\n",
      "Iteration 803, loss = 0.30967132\n",
      "Iteration 2498, loss = 0.07874583\n",
      "Iteration 1849, loss = 0.12386111\n",
      "Iteration 736, loss = 0.33777323\n",
      "Iteration 1499, loss = 0.23230791\n",
      "Iteration 2499, loss = 0.07866282\n",
      "Iteration 2, loss = 0.80624538\n",
      "Iteration 1417, loss = 0.25036435\n",
      "Iteration 1500, loss = 0.23225746\n",
      "Iteration 939, loss = 0.28947275\n",
      "Iteration 2500, loss = 0.07856358\n",
      "Iteration 1850, loss = 0.12371286\n",
      "Iteration 737, loss = 0.33763669\n",
      "Iteration 2501, loss = 0.07854110\n",
      "Iteration 804, loss = 0.30960584\n",
      "Iteration 1501, loss = 0.23203808Iteration 2502, loss = 0.07845417\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "Iteration 805, loss = 0.30925516\n",
      "Iteration 1418, loss = 0.25022318\n",
      "Iteration 940, loss = 0.28929623\n",
      "Iteration 1, loss = 0.76680202\n",
      "Iteration 2, loss = 0.76226031\n",
      "Iteration 3, loss = 0.80526766\n",
      "Iteration 3, loss = 0.75537155\n",
      "Iteration 4, loss = 0.74701206\n",
      "Iteration 5, loss = 0.73799390\n",
      "Iteration 6, loss = 0.72827470\n",
      "Iteration 7, loss = 0.71816714\n",
      "Iteration 8, loss = 0.70836381\n",
      "Iteration 806, loss = 0.30911770\n",
      "Iteration 9, loss = 0.69867524\n",
      "Iteration 1851, loss = 0.12362699\n",
      "Iteration 10, loss = 0.68917372\n",
      "Iteration 11, loss = 0.68007720\n",
      "Iteration 12, loss = 0.67162933\n",
      "Iteration 13, loss = 0.66298610\n",
      "Iteration 14, loss = 0.65523371\n",
      "Iteration 15, loss = 0.64752217\n",
      "Iteration 1502, loss = 0.23201481\n",
      "Iteration 16, loss = 0.64028640\n",
      "Iteration 17, loss = 0.63345218\n",
      "Iteration 18, loss = 0.62658443\n",
      "Iteration 738, loss = 0.33754314\n",
      "Iteration 19, loss = 0.62015686\n",
      "Iteration 20, loss = 0.61405289\n",
      "Iteration 21, loss = 0.60825906\n",
      "Iteration 22, loss = 0.60258325\n",
      "Iteration 23, loss = 0.59714757\n",
      "Iteration 24, loss = 0.59198488\n",
      "Iteration 1503, loss = 0.23179669\n",
      "Iteration 1852, loss = 0.12349680\n",
      "Iteration 1419, loss = 0.25011980\n",
      "Iteration 25, loss = 0.58697465\n",
      "Iteration 941, loss = 0.28904400\n",
      "Iteration 26, loss = 0.58216380\n",
      "Iteration 807, loss = 0.30891236\n",
      "Iteration 27, loss = 0.57767131\n",
      "Iteration 28, loss = 0.57302488\n",
      "Iteration 29, loss = 0.56875843\n",
      "Iteration 1504, loss = 0.23168347\n",
      "Iteration 30, loss = 0.56456973\n",
      "Iteration 739, loss = 0.33739694\n",
      "Iteration 31, loss = 0.56047562\n",
      "Iteration 808, loss = 0.30874276\n",
      "Iteration 32, loss = 0.55656743\n",
      "Iteration 4, loss = 0.80407605\n",
      "Iteration 1853, loss = 0.12350174\n",
      "Iteration 33, loss = 0.55275572\n",
      "Iteration 34, loss = 0.54908173\n",
      "Iteration 35, loss = 0.54544923\n",
      "Iteration 1420, loss = 0.24997752\n",
      "Iteration 36, loss = 0.54195985\n",
      "Iteration 37, loss = 0.53866888\n",
      "Iteration 1505, loss = 0.23156326\n",
      "Iteration 38, loss = 0.53537329\n",
      "Iteration 809, loss = 0.30850747\n",
      "Iteration 942, loss = 0.28870147\n",
      "Iteration 39, loss = 0.53223268\n",
      "Iteration 40, loss = 0.52908229\n",
      "Iteration 41, loss = 0.52607840\n",
      "Iteration 42, loss = 0.52314209\n",
      "Iteration 740, loss = 0.33728654\n",
      "Iteration 43, loss = 0.52022571\n",
      "Iteration 1506, loss = 0.23143237\n",
      "Iteration 44, loss = 0.51750465\n",
      "Iteration 1854, loss = 0.12335937\n",
      "Iteration 45, loss = 0.51473946\n",
      "Iteration 46, loss = 0.51211156\n",
      "Iteration 810, loss = 0.30829566\n",
      "Iteration 47, loss = 0.50956585\n",
      "Iteration 943, loss = 0.28859160\n",
      "Iteration 48, loss = 0.50696609\n",
      "Iteration 49, loss = 0.50455879\n",
      "Iteration 5, loss = 0.80262096\n",
      "Iteration 1421, loss = 0.24991080\n",
      "Iteration 50, loss = 0.50218175\n",
      "Iteration 1507, loss = 0.23128537\n",
      "Iteration 811, loss = 0.30812044\n",
      "Iteration 1855, loss = 0.12333733\n",
      "Iteration 741, loss = 0.33717531\n",
      "Iteration 1508, loss = 0.23116830\n",
      "Iteration 812, loss = 0.30791638\n",
      "Iteration 944, loss = 0.28828383\n",
      "Iteration 51, loss = 0.49984740\n",
      "Iteration 1509, loss = 0.23105789\n",
      "Iteration 52, loss = 0.49754311\n",
      "Iteration 53, loss = 0.49534253\n",
      "Iteration 1856, loss = 0.12325941\n",
      "Iteration 1422, loss = 0.24975641\n",
      "Iteration 54, loss = 0.49316495\n",
      "Iteration 742, loss = 0.33705792\n",
      "Iteration 55, loss = 0.49099288\n",
      "Iteration 1510, loss = 0.23090076\n",
      "Iteration 56, loss = 0.48894434\n",
      "Iteration 57, loss = 0.48690306\n",
      "Iteration 58, loss = 0.48496340\n",
      "Iteration 59, loss = 0.48298751\n",
      "Iteration 60, loss = 0.48110759\n",
      "Iteration 61, loss = 0.47924265\n",
      "Iteration 62, loss = 0.47740121\n",
      "Iteration 63, loss = 0.47563616\n",
      "Iteration 64, loss = 0.47388027\n",
      "Iteration 1511, loss = 0.23079206\n",
      "Iteration 65, loss = 0.47213937\n",
      "Iteration 1857, loss = 0.12335774\n",
      "Iteration 813, loss = 0.30778333\n",
      "Iteration 66, loss = 0.47049739\n",
      "Iteration 1512, loss = 0.23067967\n",
      "Iteration 67, loss = 0.46881695\n",
      "Iteration 1423, loss = 0.24963620\n",
      "Iteration 743, loss = 0.33695385\n",
      "Iteration 68, loss = 0.46723107\n",
      "Iteration 1858, loss = 0.12314074\n",
      "Iteration 69, loss = 0.46562106\n",
      "Iteration 1859, loss = 0.12296971\n",
      "Iteration 814, loss = 0.30754567\n",
      "Iteration 70, loss = 0.46406572\n",
      "Iteration 1424, loss = 0.24951933\n",
      "Iteration 1513, loss = 0.23057588\n",
      "Iteration 6, loss = 0.80111438\n",
      "Iteration 71, loss = 0.46255076\n",
      "Iteration 1860, loss = 0.12291863\n",
      "Iteration 72, loss = 0.46102386\n",
      "Iteration 1514, loss = 0.23042981\n",
      "Iteration 945, loss = 0.28807738\n",
      "Iteration 73, loss = 0.45960078\n",
      "Iteration 74, loss = 0.45812438\n",
      "Iteration 75, loss = 0.45672152\n",
      "Iteration 815, loss = 0.30733726\n",
      "Iteration 76, loss = 0.45530070\n",
      "Iteration 77, loss = 0.45398809\n",
      "Iteration 78, loss = 0.45258970\n",
      "Iteration 1515, loss = 0.23027855\n",
      "Iteration 744, loss = 0.33681347\n",
      "Iteration 1425, loss = 0.24940643\n",
      "Iteration 79, loss = 0.45123393\n",
      "Iteration 1861, loss = 0.12286986\n",
      "Iteration 7, loss = 0.79950244\n",
      "Iteration 80, loss = 0.44998138\n",
      "Iteration 81, loss = 0.44873993\n",
      "Iteration 816, loss = 0.30714221\n",
      "Iteration 82, loss = 0.44744726\n",
      "Iteration 1516, loss = 0.23019254\n",
      "Iteration 83, loss = 0.44621466\n",
      "Iteration 84, loss = 0.44504510\n",
      "Iteration 1862, loss = 0.12308168\n",
      "Iteration 85, loss = 0.44383934\n",
      "Iteration 1517, loss = 0.23004028\n",
      "Iteration 817, loss = 0.30695644\n",
      "Iteration 1426, loss = 0.24925641\n",
      "Iteration 86, loss = 0.44265882\n",
      "Iteration 946, loss = 0.28781897\n",
      "Iteration 87, loss = 0.44152288\n",
      "Iteration 1518, loss = 0.22993378\n",
      "Iteration 745, loss = 0.33671588\n",
      "Iteration 818, loss = 0.30682192\n",
      "Iteration 88, loss = 0.44039580\n",
      "Iteration 1863, loss = 0.12275403\n",
      "Iteration 89, loss = 0.43927317\n",
      "Iteration 8, loss = 0.79788722\n",
      "Iteration 1519, loss = 0.22980481\n",
      "Iteration 90, loss = 0.43820711\n",
      "Iteration 819, loss = 0.30679094\n",
      "Iteration 947, loss = 0.28764839\n",
      "Iteration 91, loss = 0.43714178\n",
      "Iteration 92, loss = 0.43605131\n",
      "Iteration 93, loss = 0.43500889\n",
      "Iteration 1427, loss = 0.24911317\n",
      "Iteration 94, loss = 0.43402393\n",
      "Iteration 95, loss = 0.43300970\n",
      "Iteration 1520, loss = 0.22963514\n",
      "Iteration 96, loss = 0.43202465\n",
      "Iteration 820, loss = 0.30634126\n",
      "Iteration 97, loss = 0.43105024\n",
      "Iteration 98, loss = 0.43009726\n",
      "Iteration 746, loss = 0.33657579\n",
      "Iteration 1864, loss = 0.12264654\n",
      "Iteration 99, loss = 0.42912855\n",
      "Iteration 948, loss = 0.28750162\n",
      "Iteration 100, loss = 0.42819584\n",
      "Iteration 101, loss = 0.42730481\n",
      "Iteration 102, loss = 0.42641481\n",
      "Iteration 1521, loss = 0.22957676\n",
      "Iteration 103, loss = 0.42554403\n",
      "Iteration 104, loss = 0.42466673\n",
      "Iteration 821, loss = 0.30621742\n",
      "Iteration 105, loss = 0.42377791\n",
      "Iteration 106, loss = 0.42292754\n",
      "Iteration 9, loss = 0.79624932\n",
      "Iteration 107, loss = 0.42211439\n",
      "Iteration 108, loss = 0.42127232\n",
      "Iteration 109, loss = 0.42048245\n",
      "Iteration 110, loss = 0.41966458\n",
      "Iteration 111, loss = 0.41889236\n",
      "Iteration 747, loss = 0.33644277\n",
      "Iteration 112, loss = 0.41809450\n",
      "Iteration 1428, loss = 0.24900076\n",
      "Iteration 113, loss = 0.41736153\n",
      "Iteration 1522, loss = 0.22941615\n",
      "Iteration 114, loss = 0.41659521\n",
      "Iteration 115, loss = 0.41586479\n",
      "Iteration 116, loss = 0.41510859\n",
      "Iteration 1865, loss = 0.12258134\n",
      "Iteration 117, loss = 0.41439371\n",
      "Iteration 118, loss = 0.41368832\n",
      "Iteration 748, loss = 0.33635370\n",
      "Iteration 119, loss = 0.41297763\n",
      "Iteration 1866, loss = 0.12259893\n",
      "Iteration 949, loss = 0.28718424\n",
      "Iteration 1523, loss = 0.22927424\n",
      "Iteration 822, loss = 0.30598445\n",
      "Iteration 120, loss = 0.41228355\n",
      "Iteration 1429, loss = 0.24896263\n",
      "Iteration 121, loss = 0.41159135\n",
      "Iteration 122, loss = 0.41093138\n",
      "Iteration 123, loss = 0.41027325\n",
      "Iteration 749, loss = 0.33619733\n",
      "Iteration 124, loss = 0.40960228\n",
      "Iteration 125, loss = 0.40893859\n",
      "Iteration 126, loss = 0.40831212\n",
      "Iteration 127, loss = 0.40765127\n",
      "Iteration 823, loss = 0.30582626\n",
      "Iteration 128, loss = 0.40705155\n",
      "Iteration 129, loss = 0.40641828\n",
      "Iteration 1524, loss = 0.22920167\n",
      "Iteration 950, loss = 0.28703450\n",
      "Iteration 130, loss = 0.40582755\n",
      "Iteration 131, loss = 0.40521097\n",
      "Iteration 1430, loss = 0.24873292\n",
      "Iteration 132, loss = 0.40462036\n",
      "Iteration 10, loss = 0.79452539\n",
      "Iteration 1867, loss = 0.12247601\n",
      "Iteration 133, loss = 0.40401617\n",
      "Iteration 134, loss = 0.40343056\n",
      "Iteration 135, loss = 0.40286592\n",
      "Iteration 136, loss = 0.40227957\n",
      "Iteration 137, loss = 0.40173842\n",
      "Iteration 1525, loss = 0.22907005\n",
      "Iteration 750, loss = 0.33614839\n",
      "Iteration 138, loss = 0.40118285\n",
      "Iteration 1431, loss = 0.24864083\n",
      "Iteration 824, loss = 0.30557652\n",
      "Iteration 139, loss = 0.40061323\n",
      "Iteration 140, loss = 0.40006913\n",
      "Iteration 1526, loss = 0.22903364\n",
      "Iteration 141, loss = 0.39953784\n",
      "Iteration 951, loss = 0.28679008\n",
      "Iteration 142, loss = 0.39901186\n",
      "Iteration 1868, loss = 0.12236740\n",
      "Iteration 143, loss = 0.39847062\n",
      "Iteration 751, loss = 0.33596229\n",
      "Iteration 144, loss = 0.39794442\n",
      "Iteration 145, loss = 0.39743175\n",
      "Iteration 1527, loss = 0.22880608\n",
      "Iteration 825, loss = 0.30540223\n",
      "Iteration 11, loss = 0.79290132\n",
      "Iteration 146, loss = 0.39692474\n",
      "Iteration 1869, loss = 0.12238813\n",
      "Iteration 752, loss = 0.33586336\n",
      "Iteration 147, loss = 0.39642905\n",
      "Iteration 148, loss = 0.39589600\n",
      "Iteration 1528, loss = 0.22872893\n",
      "Iteration 1432, loss = 0.24852312\n",
      "Iteration 149, loss = 0.39542254\n",
      "Iteration 150, loss = 0.39491492\n",
      "Iteration 151, loss = 0.39442754\n",
      "Iteration 1870, loss = 0.12228323\n",
      "Iteration 152, loss = 0.39398315\n",
      "Iteration 1529, loss = 0.22853894\n",
      "Iteration 153, loss = 0.39350263\n",
      "Iteration 154, loss = 0.39299841\n",
      "Iteration 155, loss = 0.39255799\n",
      "Iteration 12, loss = 0.79113347Iteration 156, loss = 0.39208977\n",
      "\n",
      "Iteration 826, loss = 0.30519433\n",
      "Iteration 157, loss = 0.39165026\n",
      "Iteration 952, loss = 0.28664746\n",
      "Iteration 158, loss = 0.39117572\n",
      "Iteration 1433, loss = 0.24846317\n",
      "Iteration 159, loss = 0.39072885\n",
      "Iteration 160, loss = 0.39028312\n",
      "Iteration 161, loss = 0.38985645\n",
      "Iteration 162, loss = 0.38941729\n",
      "Iteration 163, loss = 0.38898847\n",
      "Iteration 164, loss = 0.38852240\n",
      "Iteration 165, loss = 0.38810820\n",
      "Iteration 166, loss = 0.38772393\n",
      "Iteration 1530, loss = 0.22840630\n",
      "Iteration 167, loss = 0.38728479\n",
      "Iteration 168, loss = 0.38685116\n",
      "Iteration 753, loss = 0.33573799\n",
      "Iteration 169, loss = 0.38644814\n",
      "Iteration 1871, loss = 0.12217344\n",
      "Iteration 170, loss = 0.38604764\n",
      "Iteration 171, loss = 0.38565653\n",
      "Iteration 172, loss = 0.38524543\n",
      "Iteration 173, loss = 0.38486842\n",
      "Iteration 174, loss = 0.38447575\n",
      "Iteration 175, loss = 0.38407680\n",
      "Iteration 176, loss = 0.38368260\n",
      "Iteration 177, loss = 0.38332176\n",
      "Iteration 178, loss = 0.38294569\n",
      "Iteration 179, loss = 0.38256501\n",
      "Iteration 180, loss = 0.38218407\n",
      "Iteration 181, loss = 0.38183278\n",
      "Iteration 1434, loss = 0.24830883\n",
      "Iteration 182, loss = 0.38145337\n",
      "Iteration 827, loss = 0.30511373\n",
      "Iteration 183, loss = 0.38111230\n",
      "Iteration 953, loss = 0.28632210\n",
      "Iteration 1531, loss = 0.22829841\n",
      "Iteration 184, loss = 0.38073011\n",
      "Iteration 185, loss = 0.38037693Iteration 1872, loss = 0.12248805\n",
      "\n",
      "Iteration 754, loss = 0.33562174\n",
      "Iteration 186, loss = 0.38000007\n",
      "Iteration 1532, loss = 0.22819232\n",
      "Iteration 187, loss = 0.37966649\n",
      "Iteration 188, loss = 0.37932250\n",
      "Iteration 828, loss = 0.30478623\n",
      "Iteration 1533, loss = 0.22806834\n",
      "Iteration 189, loss = 0.37897356\n",
      "Iteration 190, loss = 0.37862742\n",
      "Iteration 13, loss = 0.78942475\n",
      "Iteration 191, loss = 0.37828837\n",
      "Iteration 1873, loss = 0.12202360\n",
      "Iteration 192, loss = 0.37795111\n",
      "Iteration 193, loss = 0.37760455\n",
      "Iteration 1534, loss = 0.22797485\n",
      "Iteration 829, loss = 0.30472015\n",
      "Iteration 194, loss = 0.37727156\n",
      "Iteration 954, loss = 0.28612045\n",
      "Iteration 195, loss = 0.37692791\n",
      "Iteration 1874, loss = 0.12198061\n",
      "Iteration 196, loss = 0.37661689\n",
      "Iteration 1535, loss = 0.22803079\n",
      "Iteration 1435, loss = 0.24819450\n",
      "Iteration 830, loss = 0.30462726\n",
      "Iteration 197, loss = 0.37630001\n",
      "Iteration 755, loss = 0.33556892\n",
      "Iteration 198, loss = 0.37597621\n",
      "Iteration 1875, loss = 0.12191650\n",
      "Iteration 199, loss = 0.37562248\n",
      "Iteration 831, loss = 0.30423539\n",
      "Iteration 200, loss = 0.37533208\n",
      "Iteration 1536, loss = 0.22768824\n",
      "Iteration 201, loss = 0.37498997\n",
      "Iteration 202, loss = 0.37465815\n",
      "Iteration 1876, loss = 0.12186630\n",
      "Iteration 203, loss = 0.37434480\n",
      "Iteration 204, loss = 0.37403876\n",
      "Iteration 205, loss = 0.37373326\n",
      "Iteration 1436, loss = 0.24804743\n",
      "Iteration 832, loss = 0.30401190\n",
      "Iteration 756, loss = 0.33539317\n",
      "Iteration 206, loss = 0.37342467\n",
      "Iteration 1537, loss = 0.22756682\n",
      "Iteration 1877, loss = 0.12190762\n",
      "Iteration 955, loss = 0.28599846\n",
      "Iteration 207, loss = 0.37310140\n",
      "Iteration 14, loss = 0.78774501\n",
      "Iteration 1538, loss = 0.22741336\n",
      "Iteration 757, loss = 0.33530343\n",
      "Iteration 1437, loss = 0.24803074\n",
      "Iteration 833, loss = 0.30386495\n",
      "Iteration 1878, loss = 0.12170924\n",
      "Iteration 1539, loss = 0.22726428\n",
      "Iteration 758, loss = 0.33515619\n",
      "Iteration 834, loss = 0.30362876\n",
      "Iteration 1438, loss = 0.24788256\n",
      "Iteration 956, loss = 0.28570322\n",
      "Iteration 1540, loss = 0.22733307\n",
      "Iteration 15, loss = 0.78618292\n",
      "Iteration 1879, loss = 0.12171746\n",
      "Iteration 208, loss = 0.37282204\n",
      "Iteration 209, loss = 0.37249210\n",
      "Iteration 210, loss = 0.37219450\n",
      "Iteration 211, loss = 0.37189541\n",
      "Iteration 212, loss = 0.37159652\n",
      "Iteration 213, loss = 0.37130621\n",
      "Iteration 1541, loss = 0.22703449\n",
      "Iteration 214, loss = 0.37101324\n",
      "Iteration 1439, loss = 0.24768370\n",
      "Iteration 215, loss = 0.37073207\n",
      "Iteration 216, loss = 0.37042909\n",
      "Iteration 217, loss = 0.37015227\n",
      "Iteration 759, loss = 0.33503347\n",
      "Iteration 218, loss = 0.36985171\n",
      "Iteration 1542, loss = 0.22697821\n",
      "Iteration 219, loss = 0.36956821\n",
      "Iteration 835, loss = 0.30344317\n",
      "Iteration 220, loss = 0.36929237\n",
      "Iteration 1880, loss = 0.12163826\n",
      "Iteration 221, loss = 0.36902363\n",
      "Iteration 222, loss = 0.36873678\n",
      "Iteration 223, loss = 0.36845846\n",
      "Iteration 224, loss = 0.36820178\n",
      "Iteration 16, loss = 0.78441903\n",
      "Iteration 1881, loss = 0.12170320\n",
      "Iteration 836, loss = 0.30326558\n",
      "Iteration 1543, loss = 0.22681748\n",
      "Iteration 225, loss = 0.36790731\n",
      "Iteration 957, loss = 0.28543574\n",
      "Iteration 1544, loss = 0.22679748\n",
      "Iteration 226, loss = 0.36765741\n",
      "Iteration 1882, loss = 0.12147077\n",
      "Iteration 837, loss = 0.30308427\n",
      "Iteration 227, loss = 0.36738360\n",
      "Iteration 1545, loss = 0.22657472\n",
      "Iteration 228, loss = 0.36711214\n",
      "Iteration 838, loss = 0.30285923\n",
      "Iteration 1883, loss = 0.12145230\n",
      "Iteration 17, loss = 0.78276312\n",
      "Iteration 1546, loss = 0.22645689\n",
      "Iteration 229, loss = 0.36685752\n",
      "Iteration 958, loss = 0.28524041\n",
      "Iteration 230, loss = 0.36658194\n",
      "Iteration 839, loss = 0.30268676\n",
      "Iteration 231, loss = 0.36631665\n",
      "Iteration 232, loss = 0.36604238\n",
      "Iteration 233, loss = 0.36581459\n",
      "Iteration 234, loss = 0.36553394\n",
      "Iteration 1884, loss = 0.12140317\n",
      "Iteration 235, loss = 0.36528193\n",
      "Iteration 236, loss = 0.36504095\n",
      "Iteration 237, loss = 0.36477454\n",
      "Iteration 238, loss = 0.36451238\n",
      "Iteration 239, loss = 0.36427988\n",
      "Iteration 1547, loss = 0.22632258\n",
      "Iteration 240, loss = 0.36401036\n",
      "Iteration 1885, loss = 0.12126773\n",
      "Iteration 241, loss = 0.36376902\n",
      "Iteration 1548, loss = 0.22625047\n",
      "Iteration 840, loss = 0.30242938\n",
      "Iteration 242, loss = 0.36353032\n",
      "Iteration 1440, loss = 0.24765487\n",
      "Iteration 243, loss = 0.36327069\n",
      "Iteration 1549, loss = 0.22621916\n",
      "Iteration 244, loss = 0.36303448\n",
      "Iteration 245, loss = 0.36279952\n",
      "Iteration 246, loss = 0.36255727\n",
      "Iteration 247, loss = 0.36230293\n",
      "Iteration 18, loss = 0.78110842\n",
      "Iteration 248, loss = 0.36207003\n",
      "Iteration 249, loss = 0.36183869\n",
      "Iteration 250, loss = 0.36160138\n",
      "Iteration 251, loss = 0.36136543\n",
      "Iteration 1550, loss = 0.22602929\n",
      "Iteration 252, loss = 0.36113724\n",
      "Iteration 253, loss = 0.36089800\n",
      "Iteration 1886, loss = 0.12118088\n",
      "Iteration 254, loss = 0.36067872\n",
      "Iteration 255, loss = 0.36045517\n",
      "Iteration 760, loss = 0.33494536\n",
      "Iteration 256, loss = 0.36021275\n",
      "Iteration 1551, loss = 0.22603253\n",
      "Iteration 257, loss = 0.35998603\n",
      "Iteration 258, loss = 0.35975092\n",
      "Iteration 259, loss = 0.35955695\n",
      "Iteration 260, loss = 0.35931715\n",
      "Iteration 261, loss = 0.35909764\n",
      "Iteration 1441, loss = 0.24747078\n",
      "Iteration 262, loss = 0.35889439\n",
      "Iteration 263, loss = 0.35867257\n",
      "Iteration 264, loss = 0.35843482\n",
      "Iteration 841, loss = 0.30222843\n",
      "Iteration 265, loss = 0.35821419\n",
      "Iteration 266, loss = 0.35800673\n",
      "Iteration 267, loss = 0.35782556\n",
      "Iteration 268, loss = 0.35758907\n",
      "Iteration 269, loss = 0.35738172\n",
      "Iteration 270, loss = 0.35716881\n",
      "Iteration 271, loss = 0.35696080\n",
      "Iteration 1552, loss = 0.22567020\n",
      "Iteration 272, loss = 0.35672691\n",
      "Iteration 959, loss = 0.28497763\n",
      "Iteration 1442, loss = 0.24733614\n",
      "Iteration 761, loss = 0.33478542Iteration 273, loss = 0.35653509\n",
      "\n",
      "Iteration 274, loss = 0.35631878\n",
      "Iteration 19, loss = 0.77946678\n",
      "Iteration 275, loss = 0.35612642\n",
      "Iteration 276, loss = 0.35593468\n",
      "Iteration 277, loss = 0.35573025\n",
      "Iteration 842, loss = 0.30200874\n",
      "Iteration 278, loss = 0.35551568\n",
      "Iteration 279, loss = 0.35532259\n",
      "Iteration 280, loss = 0.35511498\n",
      "Iteration 281, loss = 0.35493081\n",
      "Iteration 282, loss = 0.35473263\n",
      "Iteration 283, loss = 0.35452403\n",
      "Iteration 284, loss = 0.35432599\n",
      "Iteration 285, loss = 0.35414715\n",
      "Iteration 286, loss = 0.35394662\n",
      "Iteration 287, loss = 0.35376546\n",
      "Iteration 762, loss = 0.33469937\n",
      "Iteration 1443, loss = 0.24734799\n",
      "Iteration 288, loss = 0.35356044\n",
      "Iteration 289, loss = 0.35337440\n",
      "Iteration 290, loss = 0.35316478\n",
      "Iteration 843, loss = 0.30189635\n",
      "Iteration 291, loss = 0.35299174\n",
      "Iteration 292, loss = 0.35279993\n",
      "Iteration 293, loss = 0.35261187\n",
      "Iteration 294, loss = 0.35243177\n",
      "Iteration 295, loss = 0.35225355\n",
      "Iteration 296, loss = 0.35204732\n",
      "Iteration 297, loss = 0.35188118\n",
      "Iteration 1553, loss = 0.22558506\n",
      "Iteration 298, loss = 0.35169781\n",
      "Iteration 763, loss = 0.33458820\n",
      "Iteration 1444, loss = 0.24704817\n",
      "Iteration 299, loss = 0.35152308\n",
      "Iteration 300, loss = 0.35133038\n",
      "Iteration 301, loss = 0.35114672\n",
      "Iteration 20, loss = 0.77781757\n",
      "Iteration 302, loss = 0.35099865\n",
      "Iteration 960, loss = 0.28473626\n",
      "Iteration 303, loss = 0.35078666\n",
      "Iteration 844, loss = 0.30163537\n",
      "Iteration 1887, loss = 0.12121269\n",
      "Iteration 304, loss = 0.35061726\n",
      "Iteration 1554, loss = 0.22553607\n",
      "Iteration 305, loss = 0.35044278\n",
      "Iteration 306, loss = 0.35025394\n",
      "Iteration 307, loss = 0.35009229\n",
      "Iteration 764, loss = 0.33452057\n",
      "Iteration 308, loss = 0.34991252\n",
      "Iteration 309, loss = 0.34972507\n",
      "Iteration 310, loss = 0.34954870\n",
      "Iteration 845, loss = 0.30149016\n",
      "Iteration 311, loss = 0.34938797\n",
      "Iteration 312, loss = 0.34921871\n",
      "Iteration 313, loss = 0.34904362\n",
      "Iteration 314, loss = 0.34888776\n",
      "Iteration 315, loss = 0.34868838\n",
      "Iteration 316, loss = 0.34852728\n",
      "Iteration 846, loss = 0.30124654\n",
      "Iteration 317, loss = 0.34835219\n",
      "Iteration 318, loss = 0.34818507\n",
      "Iteration 319, loss = 0.34802140\n",
      "Iteration 1888, loss = 0.12118582\n",
      "Iteration 1445, loss = 0.24692851\n",
      "Iteration 320, loss = 0.34785072\n",
      "Iteration 765, loss = 0.33431384\n",
      "Iteration 321, loss = 0.34769148\n",
      "Iteration 322, loss = 0.34752515\n",
      "Iteration 323, loss = 0.34735134\n",
      "Iteration 1555, loss = 0.22536722\n",
      "Iteration 324, loss = 0.34719953\n",
      "Iteration 325, loss = 0.34703260\n",
      "Iteration 326, loss = 0.34686683\n",
      "Iteration 1889, loss = 0.12100350\n",
      "Iteration 327, loss = 0.34670191\n",
      "Iteration 21, loss = 0.77614108\n",
      "Iteration 328, loss = 0.34655049\n",
      "Iteration 329, loss = 0.34637203\n",
      "Iteration 961, loss = 0.28450977\n",
      "Iteration 330, loss = 0.34624369\n",
      "Iteration 331, loss = 0.34606031\n",
      "Iteration 332, loss = 0.34591771\n",
      "Iteration 766, loss = 0.33420821\n",
      "Iteration 1446, loss = 0.24681025\n",
      "Iteration 333, loss = 0.34577004\n",
      "Iteration 1890, loss = 0.12090040\n",
      "Iteration 1556, loss = 0.22528298\n",
      "Iteration 334, loss = 0.34558699\n",
      "Iteration 335, loss = 0.34543661\n",
      "Iteration 336, loss = 0.34530110\n",
      "Iteration 337, loss = 0.34512787\n",
      "Iteration 847, loss = 0.30116812\n",
      "Iteration 338, loss = 0.34496948\n",
      "Iteration 339, loss = 0.34482144\n",
      "Iteration 340, loss = 0.34466516\n",
      "Iteration 341, loss = 0.34451375\n",
      "Iteration 22, loss = 0.77448729\n",
      "Iteration 342, loss = 0.34436023\n",
      "Iteration 1557, loss = 0.22512372\n",
      "Iteration 343, loss = 0.34420600\n",
      "Iteration 344, loss = 0.34405670\n",
      "Iteration 345, loss = 0.34389888\n",
      "Iteration 346, loss = 0.34375530\n",
      "Iteration 347, loss = 0.34360819\n",
      "Iteration 848, loss = 0.30088625\n",
      "Iteration 348, loss = 0.34346983\n",
      "Iteration 1891, loss = 0.12093373\n",
      "Iteration 767, loss = 0.33407361\n",
      "Iteration 349, loss = 0.34331604\n",
      "Iteration 1447, loss = 0.24669743\n",
      "Iteration 350, loss = 0.34316316\n",
      "Iteration 351, loss = 0.34302647\n",
      "Iteration 352, loss = 0.34287343\n",
      "Iteration 353, loss = 0.34271643\n",
      "Iteration 354, loss = 0.34257912\n",
      "Iteration 355, loss = 0.34242428\n",
      "Iteration 849, loss = 0.30063328\n",
      "Iteration 356, loss = 0.34228893\n",
      "Iteration 357, loss = 0.34214861\n",
      "Iteration 358, loss = 0.34200543\n",
      "Iteration 1448, loss = 0.24658590\n",
      "Iteration 962, loss = 0.28428635\n",
      "Iteration 1558, loss = 0.22496217\n",
      "Iteration 359, loss = 0.34186576\n",
      "Iteration 360, loss = 0.34171561\n",
      "Iteration 1892, loss = 0.12084516\n",
      "Iteration 768, loss = 0.33395768\n",
      "Iteration 361, loss = 0.34158570\n",
      "Iteration 362, loss = 0.34143434\n",
      "Iteration 363, loss = 0.34129776\n",
      "Iteration 364, loss = 0.34115984\n",
      "Iteration 365, loss = 0.34103630\n",
      "Iteration 1559, loss = 0.22481297\n",
      "Iteration 366, loss = 0.34088251\n",
      "Iteration 367, loss = 0.34073730\n",
      "Iteration 368, loss = 0.34060142\n",
      "Iteration 369, loss = 0.34047518\n",
      "Iteration 1449, loss = 0.24648117\n",
      "Iteration 370, loss = 0.34033645\n",
      "Iteration 371, loss = 0.34020234\n",
      "Iteration 23, loss = 0.77277830\n",
      "Iteration 372, loss = 0.34006395\n",
      "Iteration 1560, loss = 0.22479405\n",
      "Iteration 373, loss = 0.33992717\n",
      "Iteration 374, loss = 0.33979904\n",
      "Iteration 375, loss = 0.33966028\n",
      "Iteration 376, loss = 0.33956717\n",
      "Iteration 377, loss = 0.33941850\n",
      "Iteration 963, loss = 0.28406408\n",
      "Iteration 378, loss = 0.33927023\n",
      "Iteration 1450, loss = 0.24630294\n",
      "Iteration 379, loss = 0.33913448\n",
      "Iteration 380, loss = 0.33901074\n",
      "Iteration 381, loss = 0.33887960\n",
      "Iteration 769, loss = 0.33385852\n",
      "Iteration 382, loss = 0.33874491\n",
      "Iteration 383, loss = 0.33864295\n",
      "Iteration 1561, loss = 0.22463977\n",
      "Iteration 850, loss = 0.30050039\n",
      "Iteration 384, loss = 0.33849082\n",
      "Iteration 385, loss = 0.33836319\n",
      "Iteration 1562, loss = 0.22445808\n",
      "Iteration 386, loss = 0.33825026\n",
      "Iteration 387, loss = 0.33811736\n",
      "Iteration 1893, loss = 0.12073762\n",
      "Iteration 851, loss = 0.30029802\n",
      "Iteration 388, loss = 0.33798196\n",
      "Iteration 24, loss = 0.77110546\n",
      "Iteration 389, loss = 0.33787084\n",
      "Iteration 390, loss = 0.33774376\n",
      "Iteration 391, loss = 0.33762095\n",
      "Iteration 392, loss = 0.33749866\n",
      "Iteration 393, loss = 0.33736449\n",
      "Iteration 394, loss = 0.33723444\n",
      "Iteration 395, loss = 0.33713522\n",
      "Iteration 1563, loss = 0.22434679\n",
      "Iteration 396, loss = 0.33700892\n",
      "Iteration 397, loss = 0.33687622\n",
      "Iteration 398, loss = 0.33675231\n",
      "Iteration 964, loss = 0.28397061\n",
      "Iteration 399, loss = 0.33662977\n",
      "Iteration 1451, loss = 0.24628899\n",
      "Iteration 400, loss = 0.33651738\n",
      "Iteration 401, loss = 0.33639646\n",
      "Iteration 402, loss = 0.33627367\n",
      "Iteration 403, loss = 0.33614524\n",
      "Iteration 404, loss = 0.33603626\n",
      "Iteration 405, loss = 0.33591877\n",
      "Iteration 770, loss = 0.33374335\n",
      "Iteration 406, loss = 0.33579157\n",
      "Iteration 1894, loss = 0.12069739\n",
      "Iteration 407, loss = 0.33567622\n",
      "Iteration 408, loss = 0.33557280\n",
      "Iteration 409, loss = 0.33544084\n",
      "Iteration 410, loss = 0.33534267\n",
      "Iteration 411, loss = 0.33521555\n",
      "Iteration 412, loss = 0.33509654\n",
      "Iteration 413, loss = 0.33497280\n",
      "Iteration 414, loss = 0.33486284\n",
      "Iteration 415, loss = 0.33475292\n",
      "Iteration 416, loss = 0.33464070\n",
      "Iteration 417, loss = 0.33451750\n",
      "Iteration 418, loss = 0.33443432\n",
      "Iteration 852, loss = 0.30019191\n",
      "Iteration 419, loss = 0.33429380\n",
      "Iteration 420, loss = 0.33419989\n",
      "Iteration 421, loss = 0.33407649\n",
      "Iteration 1452, loss = 0.24606186\n",
      "Iteration 422, loss = 0.33397254\n",
      "Iteration 1564, loss = 0.22426744\n",
      "Iteration 423, loss = 0.33387982\n",
      "Iteration 965, loss = 0.28371893\n",
      "Iteration 1565, loss = 0.22409312\n",
      "Iteration 1895, loss = 0.12065648\n",
      "Iteration 25, loss = 0.76946315\n",
      "Iteration 424, loss = 0.33374181\n",
      "Iteration 771, loss = 0.33367889\n",
      "Iteration 425, loss = 0.33363611\n",
      "Iteration 1566, loss = 0.22400806\n",
      "Iteration 426, loss = 0.33353307\n",
      "Iteration 1567, loss = 0.22386391\n",
      "Iteration 427, loss = 0.33342322\n",
      "Iteration 428, loss = 0.33332045\n",
      "Iteration 1453, loss = 0.24596457\n",
      "Iteration 429, loss = 0.33320829\n",
      "Iteration 966, loss = 0.28335651\n",
      "Iteration 1896, loss = 0.12081362\n",
      "Iteration 430, loss = 0.33310018\n",
      "Iteration 431, loss = 0.33300046\n",
      "Iteration 853, loss = 0.29988653\n",
      "Iteration 432, loss = 0.33289374\n",
      "Iteration 433, loss = 0.33277827\n",
      "Iteration 1897, loss = 0.12042153\n",
      "Iteration 434, loss = 0.33268315\n",
      "Iteration 1568, loss = 0.22381529\n",
      "Iteration 435, loss = 0.33257721\n",
      "Iteration 436, loss = 0.33248583\n",
      "Iteration 437, loss = 0.33236654\n",
      "Iteration 438, loss = 0.33227353\n",
      "Iteration 439, loss = 0.33216145\n",
      "Iteration 440, loss = 0.33205887\n",
      "Iteration 1569, loss = 0.22367745\n",
      "Iteration 441, loss = 0.33195624\n",
      "Iteration 1454, loss = 0.24581329\n",
      "Iteration 442, loss = 0.33186597\n",
      "Iteration 772, loss = 0.33354372\n",
      "Iteration 443, loss = 0.33175975\n",
      "Iteration 444, loss = 0.33165802\n",
      "Iteration 445, loss = 0.33156570\n",
      "Iteration 446, loss = 0.33147072\n",
      "Iteration 447, loss = 0.33136658\n",
      "Iteration 967, loss = 0.28321330\n",
      "Iteration 448, loss = 0.33126250\n",
      "Iteration 449, loss = 0.33117720\n",
      "Iteration 450, loss = 0.33106966\n",
      "Iteration 451, loss = 0.33097297\n",
      "Iteration 452, loss = 0.33088173\n",
      "Iteration 854, loss = 0.29967256\n",
      "Iteration 453, loss = 0.33079360\n",
      "Iteration 454, loss = 0.33069746\n",
      "Iteration 455, loss = 0.33059748\n",
      "Iteration 1455, loss = 0.24573800\n",
      "Iteration 456, loss = 0.33048971\n",
      "Iteration 457, loss = 0.33039716\n",
      "Iteration 1898, loss = 0.12040409\n",
      "Iteration 26, loss = 0.76777874\n",
      "Iteration 1570, loss = 0.22347516\n",
      "Iteration 773, loss = 0.33339489\n",
      "Iteration 458, loss = 0.33030637\n",
      "Iteration 459, loss = 0.33020795\n",
      "Iteration 460, loss = 0.33012221\n",
      "Iteration 461, loss = 0.33002332\n",
      "Iteration 1571, loss = 0.22343453\n",
      "Iteration 462, loss = 0.32993484\n",
      "Iteration 463, loss = 0.32984238\n",
      "Iteration 855, loss = 0.29940877\n",
      "Iteration 464, loss = 0.32974650\n",
      "Iteration 465, loss = 0.32965501\n",
      "Iteration 466, loss = 0.32956127\n",
      "Iteration 467, loss = 0.32946613\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1572, loss = 0.22343337\n",
      "Iteration 1456, loss = 0.24563674\n",
      "Iteration 774, loss = 0.33325801\n",
      "Iteration 1573, loss = 0.22316877\n",
      "Iteration 1, loss = 0.68982326\n",
      "Iteration 2, loss = 0.68701415\n",
      "Iteration 3, loss = 0.68277459\n",
      "Iteration 4, loss = 0.67752965\n",
      "Iteration 5, loss = 0.67184021\n",
      "Iteration 6, loss = 0.66587939\n",
      "Iteration 7, loss = 0.65951428\n",
      "Iteration 1899, loss = 0.12037023\n",
      "Iteration 775, loss = 0.33316607\n",
      "Iteration 8, loss = 0.65294622\n",
      "Iteration 1574, loss = 0.22297584\n",
      "Iteration 9, loss = 0.64672353\n",
      "Iteration 10, loss = 0.64042528\n",
      "Iteration 968, loss = 0.28307599\n",
      "Iteration 27, loss = 0.76599125\n",
      "Iteration 11, loss = 0.63449540\n",
      "Iteration 12, loss = 0.62842035\n",
      "Iteration 1457, loss = 0.24553118\n",
      "Iteration 13, loss = 0.62264898\n",
      "Iteration 1900, loss = 0.12027130\n",
      "Iteration 1575, loss = 0.22286952\n",
      "Iteration 776, loss = 0.33305652\n",
      "Iteration 856, loss = 0.29933871\n",
      "Iteration 1901, loss = 0.12026359\n",
      "Iteration 1458, loss = 0.24539187\n",
      "Iteration 969, loss = 0.28284774\n",
      "Iteration 14, loss = 0.61709512\n",
      "Iteration 28, loss = 0.76426260\n",
      "Iteration 15, loss = 0.61183019\n",
      "Iteration 857, loss = 0.29922004\n",
      "Iteration 16, loss = 0.60656003\n",
      "Iteration 17, loss = 0.60143626\n",
      "Iteration 1576, loss = 0.22285934\n",
      "Iteration 1459, loss = 0.24520157\n",
      "Iteration 18, loss = 0.59664418\n",
      "Iteration 777, loss = 0.33289021\n",
      "Iteration 1577, loss = 0.22262169\n",
      "Iteration 858, loss = 0.29898577\n",
      "Iteration 1460, loss = 0.24513559\n",
      "Iteration 19, loss = 0.59179915Iteration 1578, loss = 0.22256197\n",
      "Iteration 1902, loss = 0.12021567\n",
      "Iteration 778, loss = 0.33276775\n",
      "Iteration 859, loss = 0.29882284\n",
      "Iteration 1579, loss = 0.22245499\n",
      "Iteration 1461, loss = 0.24496557\n",
      "Iteration 1580, loss = 0.22232706\n",
      "Iteration 1903, loss = 0.12026564\n",
      "Iteration 779, loss = 0.33265681\n",
      "Iteration 860, loss = 0.29842424\n",
      "Iteration 1904, loss = 0.12002070\n",
      "Iteration 1581, loss = 0.22215656\n",
      "\n",
      "Iteration 20, loss = 0.58720730\n",
      "Iteration 21, loss = 0.58283846\n",
      "Iteration 22, loss = 0.57843626\n",
      "Iteration 970, loss = 0.28247877\n",
      "Iteration 23, loss = 0.57434677\n",
      "Iteration 29, loss = 0.76250721\n",
      "Iteration 24, loss = 0.57019373\n",
      "Iteration 1905, loss = 0.11996578\n",
      "Iteration 25, loss = 0.56632701\n",
      "Iteration 26, loss = 0.56232715\n",
      "Iteration 27, loss = 0.55867172\n",
      "Iteration 861, loss = 0.29823659\n",
      "Iteration 28, loss = 0.55499383\n",
      "Iteration 29, loss = 0.55150443\n",
      "Iteration 30, loss = 0.54797651\n",
      "Iteration 31, loss = 0.54466650\n",
      "Iteration 780, loss = 0.33259201\n",
      "Iteration 971, loss = 0.28244740\n",
      "Iteration 32, loss = 0.54129668\n",
      "Iteration 33, loss = 0.53806447\n",
      "Iteration 34, loss = 0.53494903\n",
      "Iteration 1582, loss = 0.22208404\n",
      "Iteration 1906, loss = 0.12008245\n",
      "Iteration 35, loss = 0.53187516\n",
      "Iteration 1462, loss = 0.24486382\n",
      "Iteration 36, loss = 0.52887457\n",
      "Iteration 37, loss = 0.52595310\n",
      "Iteration 38, loss = 0.52306694\n",
      "Iteration 39, loss = 0.52030985\n",
      "Iteration 1583, loss = 0.22215215\n",
      "Iteration 30, loss = 0.76069720\n",
      "Iteration 40, loss = 0.51757139\n",
      "Iteration 862, loss = 0.29814817\n",
      "Iteration 41, loss = 0.51490748\n",
      "Iteration 42, loss = 0.51224398\n",
      "Iteration 1907, loss = 0.11989629\n",
      "Iteration 781, loss = 0.33244834\n",
      "Iteration 1584, loss = 0.22177202\n",
      "Iteration 972, loss = 0.28214592\n",
      "Iteration 43, loss = 0.50964119\n",
      "Iteration 44, loss = 0.50721989\n",
      "Iteration 1908, loss = 0.11984831\n",
      "Iteration 45, loss = 0.50473799\n",
      "Iteration 46, loss = 0.50235248\n",
      "Iteration 1585, loss = 0.22180010\n",
      "Iteration 47, loss = 0.49999452\n",
      "Iteration 1463, loss = 0.24473079\n",
      "Iteration 863, loss = 0.29787654\n",
      "Iteration 48, loss = 0.49763616\n",
      "Iteration 31, loss = 0.75890522\n",
      "Iteration 1586, loss = 0.22157097\n",
      "Iteration 782, loss = 0.33240241\n",
      "Iteration 49, loss = 0.49538358\n",
      "Iteration 1909, loss = 0.11972562\n",
      "Iteration 50, loss = 0.49316470\n",
      "Iteration 1587, loss = 0.22151651\n",
      "Iteration 864, loss = 0.29765343Iteration 51, loss = 0.49098606\n",
      "Iteration 52, loss = 0.48884215\n",
      "Iteration 53, loss = 0.48676934\n",
      "Iteration 54, loss = 0.48472378\n",
      "Iteration 55, loss = 0.48273432\n",
      "\n",
      "Iteration 56, loss = 0.48075674\n",
      "Iteration 1464, loss = 0.24469040\n",
      "Iteration 57, loss = 0.47878331\n",
      "Iteration 783, loss = 0.33225767\n",
      "Iteration 973, loss = 0.28187190\n",
      "Iteration 1910, loss = 0.11978921\n",
      "Iteration 1588, loss = 0.22134092\n",
      "Iteration 32, loss = 0.75705276\n",
      "Iteration 58, loss = 0.47690991\n",
      "Iteration 59, loss = 0.47507106\n",
      "Iteration 60, loss = 0.47322151\n",
      "Iteration 61, loss = 0.47142874\n",
      "Iteration 1911, loss = 0.11960997\n",
      "Iteration 62, loss = 0.46965910\n",
      "Iteration 784, loss = 0.33205859\n",
      "Iteration 1589, loss = 0.22118964\n",
      "Iteration 1465, loss = 0.24456088\n",
      "Iteration 63, loss = 0.46795113\n",
      "Iteration 64, loss = 0.46625696\n",
      "Iteration 865, loss = 0.29748250\n",
      "Iteration 65, loss = 0.46454188\n",
      "Iteration 66, loss = 0.46297482\n",
      "Iteration 33, loss = 0.75524197\n",
      "Iteration 67, loss = 0.46134342\n",
      "Iteration 68, loss = 0.45982030\n",
      "Iteration 69, loss = 0.45823491\n",
      "Iteration 70, loss = 0.45669361\n",
      "Iteration 1590, loss = 0.22109792\n",
      "Iteration 71, loss = 0.45516785\n",
      "Iteration 974, loss = 0.28167400\n",
      "Iteration 72, loss = 0.45373381\n",
      "Iteration 1912, loss = 0.11955818\n",
      "Iteration 73, loss = 0.45229283\n",
      "Iteration 785, loss = 0.33195532\n",
      "Iteration 1591, loss = 0.22108755\n",
      "Iteration 1913, loss = 0.11951964\n",
      "Iteration 975, loss = 0.28150483\n",
      "Iteration 1592, loss = 0.22086303\n",
      "Iteration 1466, loss = 0.24438582\n",
      "Iteration 1914, loss = 0.11937812\n",
      "Iteration 866, loss = 0.29730991\n",
      "Iteration 34, loss = 0.75336461\n",
      "Iteration 1593, loss = 0.22066950\n",
      "Iteration 74, loss = 0.45083210\n",
      "Iteration 1594, loss = 0.22060233\n",
      "Iteration 1595, loss = 0.22046867\n",
      "Iteration 976, loss = 0.28116731\n",
      "Iteration 75, loss = 0.44942554\n",
      "Iteration 76, loss = 0.44811410\n",
      "Iteration 1467, loss = 0.24424577\n",
      "Iteration 77, loss = 0.44676939\n",
      "Iteration 786, loss = 0.33181190\n",
      "Iteration 78, loss = 0.44545787\n",
      "Iteration 867, loss = 0.29701838\n",
      "Iteration 79, loss = 0.44409763\n",
      "Iteration 80, loss = 0.44278664\n",
      "Iteration 81, loss = 0.44152764\n",
      "Iteration 1915, loss = 0.11938986\n",
      "Iteration 977, loss = 0.28094522\n",
      "Iteration 82, loss = 0.44028765\n",
      "Iteration 83, loss = 0.43905335\n",
      "Iteration 84, loss = 0.43790606\n",
      "Iteration 1596, loss = 0.22039473\n",
      "Iteration 85, loss = 0.43663970\n",
      "Iteration 868, loss = 0.29691004\n",
      "Iteration 86, loss = 0.43550128\n",
      "Iteration 87, loss = 0.43437500\n",
      "Iteration 35, loss = 0.75147279\n",
      "Iteration 88, loss = 0.43319939\n",
      "Iteration 1468, loss = 0.24410480\n",
      "Iteration 89, loss = 0.43208487\n",
      "Iteration 90, loss = 0.43100078\n",
      "Iteration 1916, loss = 0.11941600\n",
      "Iteration 869, loss = 0.29666387\n",
      "Iteration 787, loss = 0.33172964\n",
      "Iteration 91, loss = 0.42990216\n",
      "Iteration 92, loss = 0.42886906\n",
      "Iteration 93, loss = 0.42778989\n",
      "Iteration 1597, loss = 0.22020107\n",
      "Iteration 870, loss = 0.29642039\n",
      "Iteration 94, loss = 0.42676839\n",
      "Iteration 95, loss = 0.42577435\n",
      "Iteration 1917, loss = 0.11919285\n",
      "Iteration 96, loss = 0.42471807\n",
      "Iteration 1598, loss = 0.22009805\n",
      "Iteration 97, loss = 0.42375279\n",
      "Iteration 1469, loss = 0.24400926\n",
      "Iteration 98, loss = 0.42276855\n",
      "Iteration 99, loss = 0.42182895\n",
      "Iteration 1599, loss = 0.22001447\n",
      "Iteration 100, loss = 0.42087780\n",
      "Iteration 101, loss = 0.41994472\n",
      "Iteration 102, loss = 0.41900529\n",
      "Iteration 36, loss = 0.74949916\n",
      "Iteration 871, loss = 0.29619314\n",
      "Iteration 103, loss = 0.41810356\n",
      "Iteration 1918, loss = 0.11927129\n",
      "Iteration 104, loss = 0.41722561\n",
      "Iteration 788, loss = 0.33159617\n",
      "Iteration 105, loss = 0.41633668\n",
      "Iteration 978, loss = 0.28091253\n",
      "Iteration 106, loss = 0.41547261\n",
      "Iteration 1470, loss = 0.24397552\n",
      "Iteration 107, loss = 0.41459077\n",
      "Iteration 1919, loss = 0.11913917\n",
      "Iteration 872, loss = 0.29600322\n",
      "Iteration 979, loss = 0.28058849\n",
      "Iteration 789, loss = 0.33146306\n",
      "Iteration 37, loss = 0.74762431\n",
      "Iteration 108, loss = 0.41377373\n",
      "Iteration 1471, loss = 0.24389336\n",
      "Iteration 109, loss = 0.41292891\n",
      "Iteration 1920, loss = 0.11905715\n",
      "Iteration 1600, loss = 0.21982141\n",
      "Iteration 110, loss = 0.41208539\n",
      "Iteration 873, loss = 0.29581920\n",
      "Iteration 111, loss = 0.41130146\n",
      "Iteration 1601, loss = 0.21976839\n",
      "Iteration 112, loss = 0.41050573\n",
      "Iteration 113, loss = 0.40970547\n",
      "Iteration 874, loss = 0.29570506\n",
      "Iteration 38, loss = 0.74559776\n",
      "Iteration 114, loss = 0.40892138\n",
      "Iteration 1921, loss = 0.11899123\n",
      "Iteration 1602, loss = 0.21957951\n",
      "Iteration 115, loss = 0.40815397\n",
      "Iteration 116, loss = 0.40737752\n",
      "Iteration 117, loss = 0.40663413\n",
      "Iteration 1472, loss = 0.24366190\n",
      "Iteration 118, loss = 0.40592205\n",
      "Iteration 790, loss = 0.33134991\n",
      "Iteration 119, loss = 0.40517852\n",
      "Iteration 1603, loss = 0.21961491\n",
      "Iteration 120, loss = 0.40447036\n",
      "Iteration 121, loss = 0.40373562\n",
      "Iteration 980, loss = 0.28038790\n",
      "Iteration 122, loss = 0.40303881\n",
      "Iteration 875, loss = 0.29539423\n",
      "Iteration 1604, loss = 0.21933376\n",
      "Iteration 123, loss = 0.40235584\n",
      "Iteration 791, loss = 0.33127204\n",
      "Iteration 39, loss = 0.74358465\n",
      "Iteration 124, loss = 0.40166547\n",
      "Iteration 125, loss = 0.40097264\n",
      "Iteration 126, loss = 0.40032462\n",
      "Iteration 127, loss = 0.39965160\n",
      "Iteration 1605, loss = 0.21937444\n",
      "Iteration 876, loss = 0.29521635\n",
      "Iteration 128, loss = 0.39899858\n",
      "Iteration 1473, loss = 0.24348286\n",
      "Iteration 129, loss = 0.39835544\n",
      "Iteration 130, loss = 0.39772331\n",
      "Iteration 1606, loss = 0.21917276\n",
      "Iteration 131, loss = 0.39710088\n",
      "Iteration 132, loss = 0.39646481\n",
      "Iteration 792, loss = 0.33111655\n",
      "Iteration 981, loss = 0.28009758\n",
      "Iteration 133, loss = 0.39587144\n",
      "Iteration 134, loss = 0.39525673\n",
      "Iteration 1607, loss = 0.21901484\n",
      "Iteration 135, loss = 0.39467447\n",
      "Iteration 136, loss = 0.39409665\n",
      "Iteration 877, loss = 0.29501108\n",
      "Iteration 137, loss = 0.39350689\n",
      "Iteration 138, loss = 0.39289859\n",
      "Iteration 139, loss = 0.39236468\n",
      "Iteration 40, loss = 0.74158047\n",
      "Iteration 1608, loss = 0.21892618\n",
      "Iteration 140, loss = 0.39178785\n",
      "Iteration 141, loss = 0.39120737\n",
      "Iteration 142, loss = 0.39068033\n",
      "Iteration 1474, loss = 0.24342485\n",
      "Iteration 143, loss = 0.39015138\n",
      "Iteration 1609, loss = 0.21880212\n",
      "Iteration 982, loss = 0.27984996\n",
      "Iteration 144, loss = 0.38962630\n",
      "Iteration 793, loss = 0.33097987\n",
      "Iteration 878, loss = 0.29489129\n",
      "Iteration 145, loss = 0.38908873\n",
      "Iteration 1922, loss = 0.11914329\n",
      "Iteration 146, loss = 0.38856461\n",
      "Iteration 879, loss = 0.29460522\n",
      "Iteration 1610, loss = 0.21865884\n",
      "Iteration 147, loss = 0.38802372\n",
      "Iteration 148, loss = 0.38751325\n",
      "Iteration 1475, loss = 0.24327689\n",
      "Iteration 149, loss = 0.38699748\n",
      "Iteration 150, loss = 0.38650265\n",
      "Iteration 151, loss = 0.38602859\n",
      "Iteration 794, loss = 0.33088556\n",
      "Iteration 152, loss = 0.38552154\n",
      "Iteration 153, loss = 0.38503306\n",
      "Iteration 154, loss = 0.38452226\n",
      "Iteration 1611, loss = 0.21850588\n",
      "Iteration 155, loss = 0.38409360\n",
      "Iteration 156, loss = 0.38360200\n",
      "Iteration 157, loss = 0.38311328\n",
      "Iteration 158, loss = 0.38266938\n",
      "Iteration 983, loss = 0.27965531\n",
      "Iteration 880, loss = 0.29437571\n",
      "Iteration 159, loss = 0.38220641\n",
      "Iteration 1612, loss = 0.21839987\n",
      "Iteration 160, loss = 0.38175163\n",
      "Iteration 795, loss = 0.33072012\n",
      "Iteration 161, loss = 0.38131606\n",
      "Iteration 41, loss = 0.73954153\n",
      "Iteration 1476, loss = 0.24324379\n",
      "Iteration 162, loss = 0.38087785\n",
      "Iteration 881, loss = 0.29418224\n",
      "Iteration 163, loss = 0.38042232\n",
      "Iteration 164, loss = 0.37999370\n",
      "Iteration 1613, loss = 0.21833069\n",
      "Iteration 165, loss = 0.37954199\n",
      "Iteration 166, loss = 0.37912481\n",
      "Iteration 167, loss = 0.37869260\n",
      "Iteration 168, loss = 0.37826760\n",
      "Iteration 1614, loss = 0.21820104\n",
      "Iteration 169, loss = 0.37787062\n",
      "Iteration 882, loss = 0.29400232\n",
      "Iteration 170, loss = 0.37745859\n",
      "Iteration 171, loss = 0.37703667\n",
      "Iteration 172, loss = 0.37663994\n",
      "Iteration 173, loss = 0.37625190\n",
      "Iteration 1615, loss = 0.21810415\n",
      "Iteration 174, loss = 0.37585267\n",
      "Iteration 42, loss = 0.73743475\n",
      "Iteration 175, loss = 0.37544093\n",
      "Iteration 796, loss = 0.33064530\n",
      "Iteration 176, loss = 0.37504841\n",
      "Iteration 883, loss = 0.29379544\n",
      "Iteration 1616, loss = 0.21800115\n",
      "Iteration 177, loss = 0.37468091\n",
      "Iteration 984, loss = 0.27938237\n",
      "Iteration 178, loss = 0.37429465\n",
      "Iteration 1477, loss = 0.24303182\n",
      "Iteration 179, loss = 0.37390993\n",
      "Iteration 180, loss = 0.37354328\n",
      "Iteration 1617, loss = 0.21781879\n",
      "Iteration 181, loss = 0.37317559\n",
      "Iteration 182, loss = 0.37279092\n",
      "Iteration 884, loss = 0.29359799\n",
      "Iteration 183, loss = 0.37241920\n",
      "Iteration 184, loss = 0.37206817\n",
      "Iteration 185, loss = 0.37170269\n",
      "Iteration 1618, loss = 0.21768020\n",
      "Iteration 186, loss = 0.37134114\n",
      "Iteration 43, loss = 0.73527738\n",
      "Iteration 187, loss = 0.37099805\n",
      "Iteration 188, loss = 0.37064366\n",
      "Iteration 189, loss = 0.37030567\n",
      "Iteration 885, loss = 0.29337215\n",
      "Iteration 1619, loss = 0.21757700\n",
      "Iteration 797, loss = 0.33050687\n",
      "Iteration 190, loss = 0.36997668\n",
      "Iteration 191, loss = 0.36962137\n",
      "Iteration 1620, loss = 0.21745445\n",
      "Iteration 192, loss = 0.36925294\n",
      "Iteration 886, loss = 0.29317924\n",
      "Iteration 1478, loss = 0.24292404\n",
      "Iteration 193, loss = 0.36893385\n",
      "Iteration 44, loss = 0.73313692\n",
      "Iteration 1621, loss = 0.21732576\n",
      "Iteration 194, loss = 0.36861100\n",
      "Iteration 195, loss = 0.36826529\n",
      "Iteration 887, loss = 0.29321606\n",
      "Iteration 985, loss = 0.27921635\n",
      "Iteration 1622, loss = 0.21732701\n",
      "Iteration 196, loss = 0.36793487\n",
      "Iteration 798, loss = 0.33039148\n",
      "Iteration 197, loss = 0.36761561\n",
      "Iteration 1623, loss = 0.21715132\n",
      "Iteration 198, loss = 0.36729427\n",
      "Iteration 199, loss = 0.36698789\n",
      "Iteration 888, loss = 0.29280765\n",
      "Iteration 1624, loss = 0.21696507\n",
      "Iteration 45, loss = 0.73094458\n",
      "Iteration 200, loss = 0.36665710\n",
      "Iteration 799, loss = 0.33023950\n",
      "Iteration 1479, loss = 0.24279452\n",
      "Iteration 201, loss = 0.36633830\n",
      "Iteration 202, loss = 0.36602193\n",
      "Iteration 203, loss = 0.36572004\n",
      "Iteration 204, loss = 0.36541454\n",
      "Iteration 205, loss = 0.36509786\n",
      "Iteration 1625, loss = 0.21688968\n",
      "Iteration 889, loss = 0.29254986\n",
      "Iteration 206, loss = 0.36482345\n",
      "Iteration 986, loss = 0.27912418\n",
      "Iteration 1480, loss = 0.24265180\n",
      "Iteration 207, loss = 0.36450227\n",
      "Iteration 1626, loss = 0.21682034\n",
      "Iteration 208, loss = 0.36421172\n",
      "Iteration 800, loss = 0.33014139\n",
      "Iteration 209, loss = 0.36391983\n",
      "Iteration 1627, loss = 0.21673728\n",
      "Iteration 210, loss = 0.36361378\n",
      "Iteration 211, loss = 0.36334172\n",
      "Iteration 212, loss = 0.36306166\n",
      "Iteration 890, loss = 0.29260464\n",
      "Iteration 46, loss = 0.72876861\n",
      "Iteration 1481, loss = 0.24259754\n",
      "Iteration 213, loss = 0.36276811\n",
      "Iteration 1923, loss = 0.11887094\n",
      "Iteration 214, loss = 0.36246933\n",
      "Iteration 215, loss = 0.36221399\n",
      "Iteration 801, loss = 0.33000349\n",
      "Iteration 216, loss = 0.36190804\n",
      "Iteration 217, loss = 0.36163233\n",
      "Iteration 218, loss = 0.36137263\n",
      "Iteration 1628, loss = 0.21660530\n",
      "Iteration 219, loss = 0.36108649\n",
      "Iteration 220, loss = 0.36081994\n",
      "Iteration 1482, loss = 0.24244944\n",
      "Iteration 1924, loss = 0.11876499\n",
      "Iteration 221, loss = 0.36054185\n",
      "Iteration 222, loss = 0.36029587\n",
      "Iteration 47, loss = 0.72645467\n",
      "Iteration 223, loss = 0.36002414\n",
      "Iteration 224, loss = 0.35974690\n",
      "Iteration 225, loss = 0.35950230\n",
      "Iteration 226, loss = 0.35924411\n",
      "Iteration 987, loss = 0.27878888\n",
      "Iteration 891, loss = 0.29213235\n",
      "Iteration 227, loss = 0.35897038\n",
      "Iteration 1925, loss = 0.11876439\n",
      "Iteration 228, loss = 0.35873016\n",
      "Iteration 229, loss = 0.35848099\n",
      "Iteration 1483, loss = 0.24235553\n",
      "Iteration 230, loss = 0.35822202\n",
      "Iteration 1629, loss = 0.21636374\n",
      "Iteration 231, loss = 0.35797479\n",
      "Iteration 802, loss = 0.32990088\n",
      "Iteration 232, loss = 0.35772611\n",
      "Iteration 233, loss = 0.35748186\n",
      "Iteration 48, loss = 0.72413088\n",
      "Iteration 1484, loss = 0.24222903\n",
      "Iteration 234, loss = 0.35723910\n",
      "Iteration 1630, loss = 0.21626128\n",
      "Iteration 235, loss = 0.35701168\n",
      "Iteration 803, loss = 0.32979079\n",
      "Iteration 1926, loss = 0.11885986\n",
      "Iteration 236, loss = 0.35675489\n",
      "Iteration 237, loss = 0.35651344\n",
      "Iteration 1631, loss = 0.21615499\n",
      "Iteration 988, loss = 0.27851129\n",
      "Iteration 238, loss = 0.35628534\n",
      "Iteration 239, loss = 0.35604190\n",
      "Iteration 1485, loss = 0.24204592\n",
      "Iteration 1632, loss = 0.21606741\n",
      "Iteration 240, loss = 0.35581287\n",
      "Iteration 241, loss = 0.35557867\n",
      "Iteration 892, loss = 0.29193490\n",
      "Iteration 242, loss = 0.35533794\n",
      "Iteration 243, loss = 0.35513436\n",
      "Iteration 244, loss = 0.35490713\n",
      "Iteration 49, loss = 0.72178321\n",
      "Iteration 245, loss = 0.35467145\n",
      "Iteration 1486, loss = 0.24192099\n",
      "Iteration 804, loss = 0.32972102\n",
      "Iteration 246, loss = 0.35444485\n",
      "Iteration 893, loss = 0.29174255\n",
      "Iteration 1633, loss = 0.21588809\n",
      "Iteration 247, loss = 0.35422687\n",
      "Iteration 1487, loss = 0.24180462\n",
      "Iteration 894, loss = 0.29194794\n",
      "Iteration 805, loss = 0.32954706\n",
      "Iteration 1634, loss = 0.21585756\n",
      "Iteration 1927, loss = 0.11856448\n",
      "Iteration 248, loss = 0.35399151\n",
      "Iteration 249, loss = 0.35379971\n",
      "Iteration 250, loss = 0.35357820\n",
      "Iteration 251, loss = 0.35336143\n",
      "Iteration 252, loss = 0.35314447\n",
      "Iteration 253, loss = 0.35291355\n",
      "Iteration 254, loss = 0.35271639\n",
      "Iteration 1635, loss = 0.21573861\n",
      "Iteration 255, loss = 0.35249152\n",
      "Iteration 989, loss = 0.27833564\n",
      "Iteration 1928, loss = 0.11855959\n",
      "Iteration 256, loss = 0.35229327\n",
      "Iteration 895, loss = 0.29142108\n",
      "Iteration 257, loss = 0.35209101\n",
      "Iteration 258, loss = 0.35188685\n",
      "Iteration 259, loss = 0.35169549\n",
      "Iteration 50, loss = 0.71935537\n",
      "Iteration 1929, loss = 0.11854184\n",
      "Iteration 1636, loss = 0.21553658\n",
      "Iteration 806, loss = 0.32941835\n",
      "Iteration 1488, loss = 0.24178130\n",
      "Iteration 260, loss = 0.35147330\n",
      "Iteration 1637, loss = 0.21546204\n",
      "Iteration 261, loss = 0.35127617\n",
      "Iteration 262, loss = 0.35108121\n",
      "Iteration 263, loss = 0.35088537\n",
      "Iteration 264, loss = 0.35068873\n",
      "Iteration 51, loss = 0.71703838\n",
      "Iteration 265, loss = 0.35049136\n",
      "Iteration 896, loss = 0.29108721\n",
      "Iteration 266, loss = 0.35029844\n",
      "Iteration 1638, loss = 0.21532949\n",
      "Iteration 1489, loss = 0.24161771\n",
      "Iteration 1930, loss = 0.11870562\n",
      "Iteration 267, loss = 0.35009576\n",
      "Iteration 807, loss = 0.32929418\n",
      "Iteration 268, loss = 0.34991453\n",
      "Iteration 269, loss = 0.34972541\n",
      "Iteration 1639, loss = 0.21529428\n",
      "Iteration 270, loss = 0.34954031Iteration 1490, loss = 0.24142063\n",
      "\n",
      "Iteration 271, loss = 0.34933012\n",
      "Iteration 990, loss = 0.27810472\n",
      "Iteration 808, loss = 0.32920014\n",
      "Iteration 1931, loss = 0.11834537\n",
      "Iteration 1640, loss = 0.21509257\n",
      "Iteration 272, loss = 0.34915438\n",
      "Iteration 273, loss = 0.34897534\n",
      "Iteration 1641, loss = 0.21501640\n",
      "Iteration 897, loss = 0.29090452\n",
      "Iteration 274, loss = 0.34878717\n",
      "Iteration 1491, loss = 0.24133171\n",
      "Iteration 1642, loss = 0.21486857\n",
      "Iteration 275, loss = 0.34860816\n",
      "Iteration 898, loss = 0.29070182\n",
      "Iteration 276, loss = 0.34842549\n",
      "Iteration 277, loss = 0.34824699\n",
      "Iteration 1932, loss = 0.11836019\n",
      "Iteration 278, loss = 0.34806076\n",
      "Iteration 279, loss = 0.34789002\n",
      "Iteration 280, loss = 0.34771986\n",
      "Iteration 281, loss = 0.34752037\n",
      "Iteration 52, loss = 0.71453196\n",
      "Iteration 282, loss = 0.34737302\n",
      "Iteration 1643, loss = 0.21478567\n",
      "Iteration 283, loss = 0.34716761\n",
      "Iteration 284, loss = 0.34699482\n",
      "Iteration 899, loss = 0.29050937\n",
      "Iteration 1933, loss = 0.11825217\n",
      "Iteration 991, loss = 0.27796316\n",
      "Iteration 285, loss = 0.34683532\n",
      "Iteration 286, loss = 0.34665778\n",
      "Iteration 287, loss = 0.34650185\n",
      "Iteration 1644, loss = 0.21463057\n",
      "Iteration 288, loss = 0.34631178\n",
      "Iteration 289, loss = 0.34617359\n",
      "Iteration 290, loss = 0.34599235\n",
      "Iteration 900, loss = 0.29029474\n",
      "Iteration 291, loss = 0.34581819\n",
      "Iteration 292, loss = 0.34565052\n",
      "Iteration 1645, loss = 0.21460514\n",
      "Iteration 293, loss = 0.34549491\n",
      "Iteration 294, loss = 0.34532465\n",
      "Iteration 1934, loss = 0.11819940\n",
      "Iteration 295, loss = 0.34516831\n",
      "Iteration 296, loss = 0.34501576\n",
      "Iteration 901, loss = 0.29009128\n",
      "Iteration 1646, loss = 0.21434819\n",
      "Iteration 992, loss = 0.27783258\n",
      "Iteration 297, loss = 0.34485588\n",
      "Iteration 298, loss = 0.34468778\n",
      "Iteration 299, loss = 0.34452845\n",
      "Iteration 300, loss = 0.34436865\n",
      "Iteration 301, loss = 0.34419912\n",
      "Iteration 1647, loss = 0.21433066\n",
      "Iteration 302, loss = 0.34405056\n",
      "Iteration 303, loss = 0.34390958\n",
      "Iteration 304, loss = 0.34373738\n",
      "Iteration 53, loss = 0.71212065\n",
      "Iteration 902, loss = 0.28987557\n",
      "Iteration 305, loss = 0.34359841\n",
      "Iteration 1935, loss = 0.11821284\n",
      "Iteration 1492, loss = 0.24117150\n",
      "Iteration 1648, loss = 0.21414782\n",
      "Iteration 306, loss = 0.34343591\n",
      "Iteration 809, loss = 0.32904747\n",
      "Iteration 307, loss = 0.34330254\n",
      "Iteration 903, loss = 0.28971041\n",
      "Iteration 1936, loss = 0.11808386\n",
      "Iteration 308, loss = 0.34313165\n",
      "Iteration 309, loss = 0.34299294\n",
      "Iteration 1649, loss = 0.21407420\n",
      "Iteration 310, loss = 0.34283459\n",
      "Iteration 904, loss = 0.28947595\n",
      "Iteration 311, loss = 0.34268835\n",
      "Iteration 312, loss = 0.34252695\n",
      "Iteration 313, loss = 0.34239286\n",
      "Iteration 1937, loss = 0.11803769\n",
      "Iteration 810, loss = 0.32901722\n",
      "Iteration 1493, loss = 0.24111979\n",
      "Iteration 314, loss = 0.34224666\n",
      "Iteration 1650, loss = 0.21406332\n",
      "Iteration 315, loss = 0.34209094\n",
      "Iteration 54, loss = 0.70954477\n",
      "Iteration 993, loss = 0.27740177\n",
      "Iteration 316, loss = 0.34195113\n",
      "Iteration 811, loss = 0.32883212\n",
      "Iteration 317, loss = 0.34179759\n",
      "Iteration 1938, loss = 0.11796338\n",
      "Iteration 318, loss = 0.34166656\n",
      "Iteration 1494, loss = 0.24099045\n",
      "Iteration 905, loss = 0.28931466\n",
      "Iteration 319, loss = 0.34152608\n",
      "Iteration 320, loss = 0.34136550\n",
      "Iteration 321, loss = 0.34122993\n",
      "Iteration 1651, loss = 0.21376526\n",
      "Iteration 322, loss = 0.34110336\n",
      "Iteration 55, loss = 0.70703760\n",
      "Iteration 323, loss = 0.34095355\n",
      "Iteration 324, loss = 0.34080382\n",
      "Iteration 812, loss = 0.32867880\n",
      "Iteration 325, loss = 0.34066595\n",
      "Iteration 906, loss = 0.28906647\n",
      "Iteration 1939, loss = 0.11795639\n",
      "Iteration 326, loss = 0.34052875\n",
      "Iteration 327, loss = 0.34039668\n",
      "Iteration 1652, loss = 0.21366046\n",
      "Iteration 328, loss = 0.34026443\n",
      "Iteration 813, loss = 0.32861769\n",
      "Iteration 1495, loss = 0.24085084\n",
      "Iteration 907, loss = 0.28886314\n",
      "Iteration 329, loss = 0.34011916\n",
      "Iteration 330, loss = 0.33998720\n",
      "Iteration 994, loss = 0.27716722\n",
      "Iteration 331, loss = 0.33983901\n",
      "Iteration 1653, loss = 0.21352159\n",
      "Iteration 1940, loss = 0.11787408\n",
      "Iteration 332, loss = 0.33971008\n",
      "Iteration 333, loss = 0.33959823\n",
      "Iteration 334, loss = 0.33943645\n",
      "Iteration 1654, loss = 0.21356059\n",
      "Iteration 908, loss = 0.28866178\n",
      "Iteration 335, loss = 0.33931420\n",
      "Iteration 336, loss = 0.33917293\n",
      "Iteration 337, loss = 0.33904277\n",
      "Iteration 1941, loss = 0.11775989\n",
      "Iteration 814, loss = 0.32847332\n",
      "Iteration 338, loss = 0.33892059\n",
      "Iteration 56, loss = 0.70445821\n",
      "Iteration 909, loss = 0.28850327\n",
      "Iteration 1655, loss = 0.21330691\n",
      "Iteration 339, loss = 0.33880831\n",
      "Iteration 995, loss = 0.27694893\n",
      "Iteration 1942, loss = 0.11774294\n",
      "Iteration 340, loss = 0.33865581\n",
      "Iteration 1496, loss = 0.24077094\n",
      "Iteration 815, loss = 0.32837117\n",
      "Iteration 341, loss = 0.33851725\n",
      "Iteration 1656, loss = 0.21324540\n",
      "Iteration 1943, loss = 0.11767721\n",
      "Iteration 910, loss = 0.28821397\n",
      "Iteration 996, loss = 0.27676553\n",
      "Iteration 342, loss = 0.33838727\n",
      "Iteration 1657, loss = 0.21304789\n",
      "Iteration 57, loss = 0.70181165\n",
      "Iteration 343, loss = 0.33827153\n",
      "Iteration 344, loss = 0.33814097\n",
      "Iteration 345, loss = 0.33800379\n",
      "Iteration 346, loss = 0.33788235\n",
      "Iteration 1658, loss = 0.21294589\n",
      "Iteration 1944, loss = 0.11758664\n",
      "Iteration 347, loss = 0.33776052\n",
      "Iteration 348, loss = 0.33762812\n",
      "Iteration 1497, loss = 0.24061369\n",
      "Iteration 816, loss = 0.32824701\n",
      "Iteration 911, loss = 0.28799836\n",
      "Iteration 349, loss = 0.33751103\n",
      "Iteration 1945, loss = 0.11755865\n",
      "Iteration 1659, loss = 0.21284302\n",
      "Iteration 350, loss = 0.33737700\n",
      "Iteration 1946, loss = 0.11757625\n",
      "Iteration 351, loss = 0.33726717\n",
      "Iteration 1660, loss = 0.21272168\n",
      "Iteration 352, loss = 0.33713318\n",
      "Iteration 353, loss = 0.33699985\n",
      "Iteration 997, loss = 0.27649234\n",
      "Iteration 354, loss = 0.33689167\n",
      "Iteration 355, loss = 0.33676940\n",
      "Iteration 356, loss = 0.33666465\n",
      "Iteration 1947, loss = 0.11740579\n",
      "Iteration 357, loss = 0.33653652\n",
      "Iteration 58, loss = 0.69913988\n",
      "Iteration 1498, loss = 0.24057423\n",
      "Iteration 358, loss = 0.33640898\n",
      "Iteration 1661, loss = 0.21259506\n",
      "Iteration 359, loss = 0.33628503\n",
      "Iteration 912, loss = 0.28790326\n",
      "Iteration 360, loss = 0.33616868\n",
      "Iteration 361, loss = 0.33605839\n",
      "Iteration 362, loss = 0.33593120\n",
      "Iteration 363, loss = 0.33582106\n",
      "Iteration 364, loss = 0.33570702\n",
      "Iteration 817, loss = 0.32811596\n",
      "Iteration 1662, loss = 0.21247953\n",
      "Iteration 365, loss = 0.33559088\n",
      "Iteration 366, loss = 0.33547459\n",
      "Iteration 367, loss = 0.33536776\n",
      "Iteration 368, loss = 0.33525662\n",
      "Iteration 369, loss = 0.33514939\n",
      "Iteration 370, loss = 0.33503729\n",
      "Iteration 371, loss = 0.33491277\n",
      "Iteration 1948, loss = 0.11732838\n",
      "Iteration 372, loss = 0.33480502\n",
      "Iteration 373, loss = 0.33469002\n",
      "Iteration 913, loss = 0.28766576\n",
      "Iteration 374, loss = 0.33458238\n",
      "Iteration 375, loss = 0.33446325\n",
      "Iteration 1663, loss = 0.21239376\n",
      "Iteration 376, loss = 0.33435929\n",
      "Iteration 377, loss = 0.33425597\n",
      "Iteration 378, loss = 0.33414756\n",
      "Iteration 59, loss = 0.69641388\n",
      "Iteration 379, loss = 0.33404100\n",
      "Iteration 380, loss = 0.33392761\n",
      "Iteration 381, loss = 0.33382268\n",
      "Iteration 818, loss = 0.32800412\n",
      "Iteration 1499, loss = 0.24032834\n",
      "Iteration 1664, loss = 0.21226062\n",
      "Iteration 1949, loss = 0.11732947\n",
      "Iteration 914, loss = 0.28753755\n",
      "Iteration 998, loss = 0.27634308\n",
      "Iteration 382, loss = 0.33371389\n",
      "Iteration 1665, loss = 0.21219263\n",
      "Iteration 383, loss = 0.33360531\n",
      "Iteration 915, loss = 0.28720948\n",
      "Iteration 384, loss = 0.33350414\n",
      "Iteration 60, loss = 0.69377695\n",
      "Iteration 1950, loss = 0.11744119\n",
      "Iteration 819, loss = 0.32785769\n",
      "Iteration 385, loss = 0.33340406\n",
      "Iteration 1666, loss = 0.21207428\n",
      "Iteration 1500, loss = 0.24029294\n",
      "Iteration 386, loss = 0.33328939\n",
      "Iteration 916, loss = 0.28698281\n",
      "Iteration 387, loss = 0.33318898\n",
      "Iteration 388, loss = 0.33309846\n",
      "Iteration 389, loss = 0.33299171\n",
      "Iteration 1951, loss = 0.11718682\n",
      "Iteration 390, loss = 0.33288095\n",
      "Iteration 391, loss = 0.33278035\n",
      "Iteration 392, loss = 0.33267896\n",
      "Iteration 1667, loss = 0.21191196\n",
      "Iteration 917, loss = 0.28676943\n",
      "Iteration 393, loss = 0.33257533\n",
      "Iteration 999, loss = 0.27620286\n",
      "Iteration 918, loss = 0.28659907\n",
      "Iteration 1501, loss = 0.24010308\n",
      "Iteration 394, loss = 0.33248097\n",
      "Iteration 1668, loss = 0.21178693\n",
      "Iteration 395, loss = 0.33238284\n",
      "Iteration 396, loss = 0.33230306\n",
      "Iteration 820, loss = 0.32779399\n",
      "Iteration 397, loss = 0.33218278\n",
      "Iteration 1000, loss = 0.27585671\n",
      "Iteration 398, loss = 0.33207900\n",
      "Iteration 919, loss = 0.28633003\n",
      "Iteration 399, loss = 0.33198371\n",
      "Iteration 400, loss = 0.33187604\n",
      "Iteration 1952, loss = 0.11718735\n",
      "Iteration 61, loss = 0.69101300\n",
      "Iteration 401, loss = 0.33180200\n",
      "Iteration 1669, loss = 0.21165689\n",
      "Iteration 1502, loss = 0.23998580\n",
      "Iteration 920, loss = 0.28631306\n",
      "Iteration 1670, loss = 0.21161923\n",
      "Iteration 62, loss = 0.68817392Iteration 821, loss = 0.32759443\n",
      "\n",
      "Iteration 1503, loss = 0.23984111\n",
      "Iteration 1001, loss = 0.27565401\n",
      "Iteration 1671, loss = 0.21146334\n",
      "Iteration 1672, loss = 0.21135395\n",
      "Iteration 402, loss = 0.33168223\n",
      "Iteration 1504, loss = 0.23973860\n",
      "Iteration 403, loss = 0.33159297\n",
      "Iteration 822, loss = 0.32751420\n",
      "Iteration 404, loss = 0.33149295\n",
      "Iteration 1953, loss = 0.11707628\n",
      "Iteration 405, loss = 0.33139932\n",
      "Iteration 1673, loss = 0.21121947\n",
      "Iteration 406, loss = 0.33129890\n",
      "Iteration 921, loss = 0.28599505\n",
      "Iteration 1002, loss = 0.27543497\n",
      "Iteration 1674, loss = 0.21115712\n",
      "Iteration 407, loss = 0.33120865\n",
      "Iteration 408, loss = 0.33111166\n",
      "Iteration 922, loss = 0.28578706\n",
      "Iteration 63, loss = 0.68542291\n",
      "Iteration 409, loss = 0.33101365\n",
      "Iteration 1954, loss = 0.11720347\n",
      "Iteration 1675, loss = 0.21095729\n",
      "Iteration 1505, loss = 0.23961122\n",
      "Iteration 410, loss = 0.33092188\n",
      "Iteration 411, loss = 0.33085043\n",
      "Iteration 923, loss = 0.28553536\n",
      "Iteration 412, loss = 0.33074066\n",
      "Iteration 823, loss = 0.32742898\n",
      "Iteration 413, loss = 0.33063865\n",
      "Iteration 1676, loss = 0.21085349\n",
      "Iteration 1506, loss = 0.23952064\n",
      "Iteration 414, loss = 0.33054712\n",
      "Iteration 1677, loss = 0.21078088\n",
      "Iteration 64, loss = 0.68249301\n",
      "Iteration 415, loss = 0.33045417\n",
      "Iteration 1955, loss = 0.11717479\n",
      "Iteration 1003, loss = 0.27541810\n",
      "Iteration 416, loss = 0.33036705\n",
      "Iteration 1678, loss = 0.21062230\n",
      "Iteration 417, loss = 0.33028619\n",
      "Iteration 924, loss = 0.28529536\n",
      "Iteration 418, loss = 0.33018494\n",
      "Iteration 1679, loss = 0.21049981\n",
      "Iteration 824, loss = 0.32723374\n",
      "Iteration 419, loss = 0.33009031\n",
      "Iteration 420, loss = 0.32999453\n",
      "Iteration 1507, loss = 0.23934411\n",
      "Iteration 1956, loss = 0.11686988\n",
      "Iteration 421, loss = 0.32992496\n",
      "Iteration 422, loss = 0.32981776\n",
      "Iteration 423, loss = 0.32974140\n",
      "Iteration 424, loss = 0.32964448\n",
      "Iteration 425, loss = 0.32955014\n",
      "Iteration 426, loss = 0.32946864\n",
      "Iteration 65, loss = 0.67970958\n",
      "Iteration 1957, loss = 0.11683355\n",
      "Iteration 427, loss = 0.32938966\n",
      "Iteration 1680, loss = 0.21036229\n",
      "Iteration 925, loss = 0.28524743\n",
      "Iteration 428, loss = 0.32929850\n",
      "Iteration 429, loss = 0.32920370\n",
      "Iteration 1508, loss = 0.23926958\n",
      "Iteration 430, loss = 0.32913145\n",
      "Iteration 431, loss = 0.32903397\n",
      "Iteration 1004, loss = 0.27504053\n",
      "Iteration 1681, loss = 0.21033171\n",
      "Iteration 432, loss = 0.32894358\n",
      "Iteration 926, loss = 0.28483331\n",
      "Iteration 433, loss = 0.32886616\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 66, loss = 0.67681110\n",
      "Iteration 1682, loss = 0.21018254\n",
      "Iteration 1958, loss = 0.11678246\n",
      "Iteration 927, loss = 0.28462505\n",
      "Iteration 1683, loss = 0.21006387\n",
      "Iteration 825, loss = 0.32716704\n",
      "Iteration 1509, loss = 0.23910458\n",
      "Iteration 1959, loss = 0.11681898\n",
      "Iteration 928, loss = 0.28441334\n",
      "Iteration 1005, loss = 0.27473081\n",
      "Iteration 1, loss = 0.74339482\n",
      "Iteration 1960, loss = 0.11704505\n",
      "Iteration 1510, loss = 0.23898050\n",
      "Iteration 2, loss = 0.73825150\n",
      "Iteration 1684, loss = 0.20988412\n",
      "Iteration 67, loss = 0.67394832\n",
      "Iteration 826, loss = 0.32706119\n",
      "Iteration 3, loss = 0.73077450\n",
      "Iteration 1511, loss = 0.23896497\n",
      "Iteration 929, loss = 0.28428648\n",
      "Iteration 1685, loss = 0.20988956\n",
      "Iteration 4, loss = 0.72112477\n",
      "Iteration 1961, loss = 0.11660795\n",
      "Iteration 68, loss = 0.67103267\n",
      "Iteration 1006, loss = 0.27459109\n",
      "Iteration 5, loss = 0.71096961\n",
      "Iteration 930, loss = 0.28404955\n",
      "Iteration 6, loss = 0.70067646\n",
      "Iteration 1686, loss = 0.20964479\n",
      "Iteration 1512, loss = 0.23876410\n",
      "Iteration 7, loss = 0.68966423\n",
      "Iteration 8, loss = 0.67909754\n",
      "Iteration 931, loss = 0.28384345\n",
      "Iteration 9, loss = 0.66903130\n",
      "Iteration 827, loss = 0.32692953\n",
      "Iteration 10, loss = 0.65958836\n",
      "Iteration 1962, loss = 0.11657165\n",
      "Iteration 11, loss = 0.65003164\n",
      "Iteration 12, loss = 0.64107211\n",
      "Iteration 1687, loss = 0.20958056\n",
      "Iteration 13, loss = 0.63281573\n",
      "Iteration 14, loss = 0.62483055\n",
      "Iteration 932, loss = 0.28360037\n",
      "Iteration 15, loss = 0.61760910\n",
      "Iteration 16, loss = 0.61036953\n",
      "Iteration 1688, loss = 0.20950687\n",
      "Iteration 17, loss = 0.60365218\n",
      "Iteration 1513, loss = 0.23864009\n",
      "Iteration 18, loss = 0.59724633\n",
      "Iteration 1963, loss = 0.11668859\n",
      "Iteration 1007, loss = 0.27428666\n",
      "Iteration 828, loss = 0.32682904\n",
      "Iteration 19, loss = 0.59106203\n",
      "Iteration 1689, loss = 0.20931252\n",
      "Iteration 20, loss = 0.58545776\n",
      "Iteration 21, loss = 0.57996434\n",
      "Iteration 22, loss = 0.57459012\n",
      "Iteration 1690, loss = 0.20923860\n",
      "Iteration 933, loss = 0.28341119\n",
      "Iteration 23, loss = 0.56960120\n",
      "Iteration 1964, loss = 0.11654645\n",
      "Iteration 24, loss = 0.56466075\n",
      "Iteration 25, loss = 0.55989957\n",
      "Iteration 26, loss = 0.55551529\n",
      "Iteration 27, loss = 0.55115101\n",
      "Iteration 69, loss = 0.66805463\n",
      "Iteration 28, loss = 0.54694699Iteration 934, loss = 0.28327419\n",
      "\n",
      "Iteration 1965, loss = 0.11636383\n",
      "Iteration 29, loss = 0.54292174\n",
      "Iteration 1514, loss = 0.23859313\n",
      "Iteration 30, loss = 0.53908221\n",
      "Iteration 829, loss = 0.32665388\n",
      "Iteration 31, loss = 0.53530336\n",
      "Iteration 32, loss = 0.53167357\n",
      "Iteration 1966, loss = 0.11637345\n",
      "Iteration 33, loss = 0.52806554\n",
      "Iteration 34, loss = 0.52462561\n",
      "Iteration 35, loss = 0.52134844\n",
      "Iteration 1515, loss = 0.23837671\n",
      "Iteration 1008, loss = 0.27413300\n",
      "Iteration 36, loss = 0.51807936\n",
      "Iteration 1691, loss = 0.20908817\n",
      "Iteration 37, loss = 0.51495043\n",
      "Iteration 38, loss = 0.51188727\n",
      "Iteration 39, loss = 0.50891552\n",
      "Iteration 830, loss = 0.32654075\n",
      "Iteration 40, loss = 0.50600128\n",
      "Iteration 935, loss = 0.28292618\n",
      "Iteration 41, loss = 0.50318880\n",
      "Iteration 1692, loss = 0.20901590\n",
      "Iteration 1967, loss = 0.11630307\n",
      "Iteration 42, loss = 0.50044589\n",
      "Iteration 1516, loss = 0.23830672\n",
      "Iteration 43, loss = 0.49781771\n",
      "Iteration 1009, loss = 0.27398409\n",
      "Iteration 1968, loss = 0.11621379\n",
      "Iteration 44, loss = 0.49519882\n",
      "Iteration 1693, loss = 0.20883814\n",
      "Iteration 45, loss = 0.49261408\n",
      "Iteration 831, loss = 0.32643342\n",
      "Iteration 1517, loss = 0.23813250\n",
      "Iteration 70, loss = 0.66506508\n",
      "Iteration 46, loss = 0.49018866\n",
      "Iteration 47, loss = 0.48772533\n",
      "Iteration 936, loss = 0.28277545\n",
      "Iteration 1969, loss = 0.11613942\n",
      "Iteration 48, loss = 0.48537757\n",
      "Iteration 49, loss = 0.48306602\n",
      "Iteration 50, loss = 0.48079255\n",
      "Iteration 51, loss = 0.47864024\n",
      "Iteration 1694, loss = 0.20877546\n",
      "Iteration 1518, loss = 0.23801908\n",
      "Iteration 937, loss = 0.28254637\n",
      "Iteration 1695, loss = 0.20862710\n",
      "Iteration 52, loss = 0.47647868\n",
      "Iteration 53, loss = 0.47432491\n",
      "Iteration 54, loss = 0.47236499\n",
      "Iteration 1519, loss = 0.23789770\n",
      "Iteration 832, loss = 0.32627658\n",
      "Iteration 1696, loss = 0.20862243\n",
      "Iteration 55, loss = 0.47030987\n",
      "Iteration 56, loss = 0.46831769\n",
      "Iteration 1970, loss = 0.11623225\n",
      "Iteration 938, loss = 0.28235045\n",
      "Iteration 71, loss = 0.66211982\n",
      "Iteration 57, loss = 0.46635402\n",
      "Iteration 58, loss = 0.46453138\n",
      "Iteration 1971, loss = 0.11612710\n",
      "Iteration 59, loss = 0.46268642\n",
      "Iteration 939, loss = 0.28222656\n",
      "Iteration 1697, loss = 0.20840978\n",
      "Iteration 1010, loss = 0.27360398\n",
      "Iteration 60, loss = 0.46087202\n",
      "Iteration 1972, loss = 0.11603025\n",
      "Iteration 1520, loss = 0.23774104\n",
      "Iteration 833, loss = 0.32617248\n",
      "Iteration 940, loss = 0.28191935\n",
      "Iteration 61, loss = 0.45911718\n",
      "Iteration 62, loss = 0.45743285\n",
      "Iteration 1973, loss = 0.11596306\n",
      "Iteration 63, loss = 0.45568486\n",
      "Iteration 64, loss = 0.45408057\n",
      "Iteration 1521, loss = 0.23759897\n",
      "Iteration 834, loss = 0.32601201\n",
      "Iteration 65, loss = 0.45242082\n",
      "Iteration 1974, loss = 0.11605201\n",
      "Iteration 1011, loss = 0.27354983\n",
      "Iteration 66, loss = 0.45082091\n",
      "Iteration 72, loss = 0.65909814\n",
      "Iteration 1698, loss = 0.20826632\n",
      "Iteration 67, loss = 0.44932331\n",
      "Iteration 1975, loss = 0.11578856\n",
      "Iteration 835, loss = 0.32592541\n",
      "Iteration 68, loss = 0.44773356\n",
      "Iteration 941, loss = 0.28167931\n",
      "Iteration 1522, loss = 0.23750792\n",
      "Iteration 69, loss = 0.44624435\n",
      "Iteration 70, loss = 0.44479519\n",
      "Iteration 1976, loss = 0.11587680\n",
      "Iteration 71, loss = 0.44337581\n",
      "Iteration 72, loss = 0.44196350\n",
      "Iteration 73, loss = 0.44056767\n",
      "Iteration 74, loss = 0.43916998\n",
      "Iteration 942, loss = 0.28167490\n",
      "Iteration 836, loss = 0.32581236\n",
      "Iteration 75, loss = 0.43786277\n",
      "Iteration 1523, loss = 0.23742066\n",
      "Iteration 76, loss = 0.43655408\n",
      "Iteration 1699, loss = 0.20812874\n",
      "Iteration 77, loss = 0.43529710\n",
      "Iteration 78, loss = 0.43403564\n",
      "Iteration 943, loss = 0.28138170\n",
      "Iteration 1977, loss = 0.11577632\n",
      "Iteration 79, loss = 0.43275796\n",
      "Iteration 80, loss = 0.43158524\n",
      "Iteration 1700, loss = 0.20804482\n",
      "Iteration 837, loss = 0.32567796\n",
      "Iteration 81, loss = 0.43041074\n",
      "Iteration 82, loss = 0.42920545\n",
      "Iteration 83, loss = 0.42806437\n",
      "Iteration 73, loss = 0.65619652\n",
      "Iteration 84, loss = 0.42694470\n",
      "Iteration 1524, loss = 0.23729010\n",
      "Iteration 85, loss = 0.42580728\n",
      "Iteration 1701, loss = 0.20795145\n",
      "Iteration 86, loss = 0.42470759\n",
      "Iteration 87, loss = 0.42364692\n",
      "Iteration 944, loss = 0.28107814\n",
      "Iteration 88, loss = 0.42260929\n",
      "Iteration 1702, loss = 0.20784403\n",
      "Iteration 89, loss = 0.42159908\n",
      "Iteration 90, loss = 0.42056794\n",
      "Iteration 91, loss = 0.41957048\n",
      "Iteration 92, loss = 0.41855406\n",
      "Iteration 838, loss = 0.32556838\n",
      "Iteration 93, loss = 0.41760864\n",
      "Iteration 1703, loss = 0.20774108Iteration 94, loss = 0.41665248\n",
      "\n",
      "Iteration 95, loss = 0.41573560\n",
      "Iteration 96, loss = 0.41475367\n",
      "Iteration 1978, loss = 0.11583531\n",
      "Iteration 97, loss = 0.41388938\n",
      "Iteration 74, loss = 0.65312965\n",
      "Iteration 1012, loss = 0.27323365\n",
      "Iteration 1704, loss = 0.20762113\n",
      "Iteration 1525, loss = 0.23714298\n",
      "Iteration 1705, loss = 0.20750521\n",
      "Iteration 1979, loss = 0.11575138\n",
      "Iteration 1706, loss = 0.20732721\n",
      "Iteration 839, loss = 0.32540270\n",
      "Iteration 1707, loss = 0.20717536\n",
      "Iteration 1980, loss = 0.11558956\n",
      "Iteration 840, loss = 0.32530049\n",
      "Iteration 1708, loss = 0.20709228\n",
      "Iteration 1013, loss = 0.27291484\n",
      "Iteration 1981, loss = 0.11550331\n",
      "Iteration 1709, loss = 0.20702249\n",
      "Iteration 1982, loss = 0.11548830\n",
      "Iteration 841, loss = 0.32519491\n",
      "Iteration 98, loss = 0.41297457\n",
      "Iteration 99, loss = 0.41211022\n",
      "Iteration 1983, loss = 0.11539438\n",
      "Iteration 100, loss = 0.41122118\n",
      "Iteration 945, loss = 0.28083592\n",
      "Iteration 75, loss = 0.65009561\n",
      "Iteration 101, loss = 0.41039098\n",
      "Iteration 1984, loss = 0.11532784\n",
      "Iteration 1526, loss = 0.23705589\n",
      "Iteration 102, loss = 0.40953903\n",
      "Iteration 946, loss = 0.28068545\n",
      "Iteration 1985, loss = 0.11531654\n",
      "Iteration 103, loss = 0.40870024\n",
      "Iteration 1527, loss = 0.23691924\n",
      "Iteration 947, loss = 0.28037353\n",
      "Iteration 104, loss = 0.40789408\n",
      "Iteration 1710, loss = 0.20684793\n",
      "Iteration 1986, loss = 0.11523960\n",
      "Iteration 105, loss = 0.40710580\n",
      "Iteration 106, loss = 0.40631598\n",
      "Iteration 107, loss = 0.40551986\n",
      "Iteration 108, loss = 0.40477975\n",
      "Iteration 1711, loss = 0.20677294\n",
      "Iteration 109, loss = 0.40398048\n",
      "Iteration 1014, loss = 0.27283223\n",
      "Iteration 110, loss = 0.40325770\n",
      "Iteration 111, loss = 0.40255990\n",
      "Iteration 948, loss = 0.28036223\n",
      "Iteration 112, loss = 0.40183654\n",
      "Iteration 113, loss = 0.40109533\n",
      "Iteration 114, loss = 0.40038634\n",
      "Iteration 1712, loss = 0.20659156\n",
      "Iteration 115, loss = 0.39968968\n",
      "Iteration 1528, loss = 0.23681739\n",
      "Iteration 76, loss = 0.64711112\n",
      "Iteration 116, loss = 0.39900187\n",
      "Iteration 117, loss = 0.39833442\n",
      "Iteration 949, loss = 0.27997028\n",
      "Iteration 118, loss = 0.39767225\n",
      "Iteration 119, loss = 0.39700725\n",
      "Iteration 842, loss = 0.32503781\n",
      "Iteration 120, loss = 0.39635208\n",
      "Iteration 1987, loss = 0.11525604\n",
      "Iteration 1713, loss = 0.20654699\n",
      "Iteration 121, loss = 0.39572035\n",
      "Iteration 950, loss = 0.27972906\n",
      "Iteration 122, loss = 0.39508840\n",
      "Iteration 1529, loss = 0.23670030\n",
      "Iteration 123, loss = 0.39447249\n",
      "Iteration 951, loss = 0.27952472\n",
      "Iteration 124, loss = 0.39384694\n",
      "Iteration 77, loss = 0.64399163\n",
      "Iteration 125, loss = 0.39323960\n",
      "Iteration 1530, loss = 0.23656919\n",
      "Iteration 1714, loss = 0.20647814\n",
      "Iteration 1015, loss = 0.27252352\n",
      "Iteration 126, loss = 0.39264008\n",
      "Iteration 1988, loss = 0.11516060\n",
      "Iteration 843, loss = 0.32494517\n",
      "Iteration 1715, loss = 0.20624117\n",
      "Iteration 952, loss = 0.27933162\n",
      "Iteration 127, loss = 0.39204128\n",
      "Iteration 128, loss = 0.39145391\n",
      "Iteration 129, loss = 0.39088250\n",
      "Iteration 130, loss = 0.39032263\n",
      "Iteration 131, loss = 0.38974875\n",
      "Iteration 1989, loss = 0.11506585\n",
      "Iteration 132, loss = 0.38920033\n",
      "Iteration 1531, loss = 0.23647789\n",
      "Iteration 133, loss = 0.38863608\n",
      "Iteration 134, loss = 0.38810698\n",
      "Iteration 1716, loss = 0.20617397\n",
      "Iteration 135, loss = 0.38757016\n",
      "Iteration 136, loss = 0.38702603\n",
      "Iteration 953, loss = 0.27916702\n",
      "Iteration 78, loss = 0.64094910\n",
      "Iteration 844, loss = 0.32482711\n",
      "Iteration 1717, loss = 0.20611102\n",
      "Iteration 1532, loss = 0.23638066\n",
      "Iteration 137, loss = 0.38652506\n",
      "Iteration 1990, loss = 0.11505394\n",
      "Iteration 138, loss = 0.38601001\n",
      "Iteration 1718, loss = 0.20593945\n",
      "Iteration 1016, loss = 0.27229364\n",
      "Iteration 139, loss = 0.38548934\n",
      "Iteration 845, loss = 0.32466225\n",
      "Iteration 1719, loss = 0.20578114\n",
      "Iteration 140, loss = 0.38499710\n",
      "Iteration 954, loss = 0.27891508\n",
      "Iteration 1991, loss = 0.11497095\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 141, loss = 0.38446549\n",
      "Iteration 79, loss = 0.63798901\n",
      "Iteration 1533, loss = 0.23617113\n",
      "Iteration 142, loss = 0.38398597\n",
      "Iteration 143, loss = 0.38353265\n",
      "Iteration 846, loss = 0.32459633\n",
      "Iteration 144, loss = 0.38302792\n",
      "Iteration 145, loss = 0.38253988\n",
      "Iteration 146, loss = 0.38209851\n",
      "Iteration 1720, loss = 0.20569647\n",
      "Iteration 147, loss = 0.38162732\n",
      "Iteration 955, loss = 0.27866894\n",
      "Iteration 1534, loss = 0.23616164\n",
      "Iteration 148, loss = 0.38115542\n",
      "Iteration 847, loss = 0.32441250\n",
      "Iteration 1721, loss = 0.20556177\n",
      "Iteration 956, loss = 0.27859071\n",
      "Iteration 1, loss = 0.77627331\n",
      "Iteration 80, loss = 0.63492119\n",
      "Iteration 2, loss = 0.77219606\n",
      "Iteration 3, loss = 0.76619528\n",
      "Iteration 149, loss = 0.38069835\n",
      "Iteration 848, loss = 0.32432405\n",
      "Iteration 957, loss = 0.27832270\n",
      "Iteration 4, loss = 0.75907385\n",
      "Iteration 5, loss = 0.75085846\n",
      "Iteration 150, loss = 0.38022859\n",
      "Iteration 1017, loss = 0.27214284\n",
      "Iteration 6, loss = 0.74265784\n",
      "Iteration 1722, loss = 0.20545636\n",
      "Iteration 7, loss = 0.73413467\n",
      "Iteration 8, loss = 0.72599957\n",
      "Iteration 151, loss = 0.37978544\n",
      "Iteration 958, loss = 0.27814145\n",
      "Iteration 9, loss = 0.71773771\n",
      "Iteration 1723, loss = 0.20546260\n",
      "Iteration 152, loss = 0.37938065\n",
      "Iteration 10, loss = 0.70962476\n",
      "Iteration 153, loss = 0.37892455\n",
      "Iteration 11, loss = 0.70229264\n",
      "Iteration 1535, loss = 0.23593720\n",
      "Iteration 154, loss = 0.37851205\n",
      "Iteration 12, loss = 0.69497805\n",
      "Iteration 13, loss = 0.68795279\n",
      "Iteration 155, loss = 0.37808358\n",
      "Iteration 156, loss = 0.37764311\n",
      "Iteration 849, loss = 0.32417406\n",
      "Iteration 14, loss = 0.68124048\n",
      "Iteration 959, loss = 0.27779728\n",
      "Iteration 1724, loss = 0.20529962\n",
      "Iteration 1018, loss = 0.27186337\n",
      "Iteration 81, loss = 0.63181410\n",
      "Iteration 157, loss = 0.37723159\n",
      "Iteration 15, loss = 0.67465657\n",
      "Iteration 158, loss = 0.37681806\n",
      "Iteration 1536, loss = 0.23582021\n",
      "Iteration 159, loss = 0.37641600\n",
      "Iteration 960, loss = 0.27760618\n",
      "Iteration 16, loss = 0.66850186\n",
      "Iteration 160, loss = 0.37599331\n",
      "Iteration 1725, loss = 0.20519202\n",
      "Iteration 161, loss = 0.37561292\n",
      "Iteration 162, loss = 0.37523299\n",
      "Iteration 850, loss = 0.32405414\n",
      "Iteration 17, loss = 0.66239409\n",
      "Iteration 82, loss = 0.62884533\n",
      "Iteration 163, loss = 0.37483280\n",
      "Iteration 18, loss = 0.65680895\n",
      "Iteration 1537, loss = 0.23571063\n",
      "Iteration 961, loss = 0.27741322\n",
      "Iteration 164, loss = 0.37443368\n",
      "Iteration 19, loss = 0.65099580\n",
      "Iteration 20, loss = 0.64573738\n",
      "Iteration 21, loss = 0.64038007\n",
      "Iteration 165, loss = 0.37405432\n",
      "Iteration 22, loss = 0.63531562\n",
      "Iteration 1726, loss = 0.20512612\n",
      "Iteration 23, loss = 0.63025865\n",
      "Iteration 166, loss = 0.37366671\n",
      "Iteration 24, loss = 0.62560819\n",
      "Iteration 962, loss = 0.27723440\n",
      "Iteration 1538, loss = 0.23558279\n",
      "Iteration 167, loss = 0.37330870\n",
      "Iteration 1019, loss = 0.27167092\n",
      "Iteration 168, loss = 0.37292452\n",
      "Iteration 851, loss = 0.32394291\n",
      "Iteration 169, loss = 0.37255149\n",
      "Iteration 25, loss = 0.62086435\n",
      "Iteration 170, loss = 0.37219838\n",
      "Iteration 171, loss = 0.37182477\n",
      "Iteration 26, loss = 0.61625838\n",
      "Iteration 27, loss = 0.61188032\n",
      "Iteration 28, loss = 0.60731042\n",
      "Iteration 1727, loss = 0.20489046\n",
      "Iteration 172, loss = 0.37146825\n",
      "Iteration 29, loss = 0.60309860\n",
      "Iteration 30, loss = 0.59888812\n",
      "Iteration 31, loss = 0.59488186\n",
      "Iteration 173, loss = 0.37109609\n",
      "Iteration 852, loss = 0.32382101\n",
      "Iteration 32, loss = 0.59082325\n",
      "Iteration 174, loss = 0.37074811\n",
      "Iteration 963, loss = 0.27699907\n",
      "Iteration 1020, loss = 0.27150181\n",
      "Iteration 175, loss = 0.37041626\n",
      "Iteration 33, loss = 0.58679730\n",
      "Iteration 176, loss = 0.37005057\n",
      "Iteration 177, loss = 0.36970988\n",
      "Iteration 1539, loss = 0.23546670\n",
      "Iteration 1728, loss = 0.20470806\n",
      "Iteration 178, loss = 0.36937975\n",
      "Iteration 179, loss = 0.36904572\n",
      "Iteration 180, loss = 0.36871567\n",
      "Iteration 853, loss = 0.32368935\n",
      "Iteration 181, loss = 0.36836127\n",
      "Iteration 964, loss = 0.27687259\n",
      "Iteration 34, loss = 0.58292025\n",
      "Iteration 35, loss = 0.57905008\n",
      "Iteration 182, loss = 0.36805176\n",
      "Iteration 36, loss = 0.57518868\n",
      "Iteration 1021, loss = 0.27117205\n",
      "Iteration 183, loss = 0.36772337\n",
      "Iteration 37, loss = 0.57147689\n",
      "Iteration 83, loss = 0.62582424\n",
      "Iteration 1729, loss = 0.20462603\n",
      "Iteration 184, loss = 0.36739983\n",
      "Iteration 185, loss = 0.36707406\n",
      "Iteration 38, loss = 0.56794055\n",
      "Iteration 854, loss = 0.32358300\n",
      "Iteration 186, loss = 0.36676693\n",
      "Iteration 39, loss = 0.56432768\n",
      "Iteration 187, loss = 0.36644405\n",
      "Iteration 40, loss = 0.56073767\n",
      "Iteration 188, loss = 0.36612243\n",
      "Iteration 41, loss = 0.55734540\n",
      "Iteration 189, loss = 0.36581329\n",
      "Iteration 42, loss = 0.55385007\n",
      "Iteration 190, loss = 0.36550774\n",
      "Iteration 43, loss = 0.55050491\n",
      "Iteration 965, loss = 0.27651803\n",
      "Iteration 191, loss = 0.36520690\n",
      "Iteration 44, loss = 0.54717798\n",
      "Iteration 855, loss = 0.32348806\n",
      "Iteration 192, loss = 0.36490333\n",
      "Iteration 45, loss = 0.54403388\n",
      "Iteration 1730, loss = 0.20455318\n",
      "Iteration 193, loss = 0.36459925\n",
      "Iteration 46, loss = 0.54072149\n",
      "Iteration 47, loss = 0.53760281\n",
      "Iteration 194, loss = 0.36431299\n",
      "Iteration 48, loss = 0.53447617\n",
      "Iteration 49, loss = 0.53147060\n",
      "Iteration 195, loss = 0.36401514\n",
      "Iteration 1540, loss = 0.23533845\n",
      "Iteration 196, loss = 0.36373009\n",
      "Iteration 1731, loss = 0.20440473\n",
      "Iteration 966, loss = 0.27629390\n",
      "Iteration 50, loss = 0.52842925\n",
      "Iteration 197, loss = 0.36342014\n",
      "Iteration 198, loss = 0.36314850\n",
      "Iteration 199, loss = 0.36285279\n",
      "Iteration 51, loss = 0.52547214\n",
      "Iteration 1732, loss = 0.20427048\n",
      "Iteration 200, loss = 0.36256880\n",
      "Iteration 201, loss = 0.36230369\n",
      "Iteration 967, loss = 0.27619892\n",
      "Iteration 52, loss = 0.52261245\n",
      "Iteration 202, loss = 0.36201287\n",
      "Iteration 84, loss = 0.62284965\n",
      "Iteration 203, loss = 0.36172347\n",
      "Iteration 1733, loss = 0.20413323\n",
      "Iteration 53, loss = 0.51979891\n",
      "Iteration 204, loss = 0.36147012\n",
      "Iteration 205, loss = 0.36118576\n",
      "Iteration 54, loss = 0.51702087\n",
      "Iteration 55, loss = 0.51417632\n",
      "Iteration 206, loss = 0.36092550\n",
      "Iteration 56, loss = 0.51155661\n",
      "Iteration 1022, loss = 0.27095677\n",
      "Iteration 57, loss = 0.50896059\n",
      "Iteration 856, loss = 0.32333166\n",
      "Iteration 207, loss = 0.36065445\n",
      "Iteration 58, loss = 0.50631447\n",
      "Iteration 59, loss = 0.50379338\n",
      "Iteration 60, loss = 0.50120459\n",
      "Iteration 208, loss = 0.36039341\n",
      "Iteration 968, loss = 0.27588301\n",
      "Iteration 61, loss = 0.49873232\n",
      "Iteration 62, loss = 0.49634465\n",
      "Iteration 209, loss = 0.36010562\n",
      "Iteration 1541, loss = 0.23525011\n",
      "Iteration 63, loss = 0.49400787\n",
      "Iteration 64, loss = 0.49172314\n",
      "Iteration 210, loss = 0.35987505\n",
      "Iteration 1023, loss = 0.27110795\n",
      "Iteration 65, loss = 0.48940164\n",
      "Iteration 211, loss = 0.35960570\n",
      "Iteration 66, loss = 0.48716206\n",
      "Iteration 67, loss = 0.48503299\n",
      "Iteration 212, loss = 0.35934114\n",
      "Iteration 1542, loss = 0.23516255\n",
      "Iteration 969, loss = 0.27566205\n",
      "Iteration 213, loss = 0.35909906\n",
      "Iteration 1734, loss = 0.20416446\n",
      "Iteration 68, loss = 0.48283958\n",
      "Iteration 85, loss = 0.61980708\n",
      "Iteration 857, loss = 0.32321609\n",
      "Iteration 214, loss = 0.35882240\n",
      "Iteration 215, loss = 0.35859435\n",
      "Iteration 1543, loss = 0.23493485\n",
      "Iteration 69, loss = 0.48068877\n",
      "Iteration 1735, loss = 0.20395101\n",
      "Iteration 216, loss = 0.35832875\n",
      "Iteration 70, loss = 0.47867633\n",
      "Iteration 970, loss = 0.27569212\n",
      "Iteration 217, loss = 0.35806651\n",
      "Iteration 1736, loss = 0.20383940\n",
      "Iteration 218, loss = 0.35783981\n",
      "Iteration 71, loss = 0.47662162\n",
      "Iteration 1024, loss = 0.27051148\n",
      "Iteration 219, loss = 0.35760546\n",
      "Iteration 1737, loss = 0.20364413\n",
      "Iteration 220, loss = 0.35733421\n",
      "Iteration 1544, loss = 0.23480373\n",
      "Iteration 221, loss = 0.35708860\n",
      "Iteration 858, loss = 0.32321534\n",
      "Iteration 86, loss = 0.61673861\n",
      "Iteration 72, loss = 0.47466792\n",
      "Iteration 971, loss = 0.27521648\n",
      "Iteration 222, loss = 0.35685264\n",
      "Iteration 223, loss = 0.35662177\n",
      "Iteration 224, loss = 0.35638774\n",
      "Iteration 972, loss = 0.27531100\n",
      "Iteration 1545, loss = 0.23474194\n",
      "Iteration 225, loss = 0.35615999\n",
      "Iteration 73, loss = 0.47266776\n",
      "Iteration 1738, loss = 0.20360817\n",
      "Iteration 226, loss = 0.35592709\n",
      "Iteration 1025, loss = 0.27037073\n",
      "Iteration 973, loss = 0.27480512\n",
      "Iteration 87, loss = 0.61381569\n",
      "Iteration 1546, loss = 0.23459021\n",
      "Iteration 227, loss = 0.35568968\n",
      "Iteration 1739, loss = 0.20344221\n",
      "Iteration 974, loss = 0.27459896\n",
      "Iteration 228, loss = 0.35544517\n",
      "Iteration 859, loss = 0.32296267\n",
      "Iteration 229, loss = 0.35522302\n",
      "Iteration 1547, loss = 0.23443613\n",
      "Iteration 230, loss = 0.35499280\n",
      "Iteration 1740, loss = 0.20339477\n",
      "Iteration 231, loss = 0.35476621\n",
      "Iteration 975, loss = 0.27440034\n",
      "Iteration 74, loss = 0.47078507\n",
      "Iteration 75, loss = 0.46887425\n",
      "Iteration 232, loss = 0.35455041\n",
      "Iteration 76, loss = 0.46702515\n",
      "Iteration 233, loss = 0.35431341\n",
      "Iteration 88, loss = 0.61078882\n",
      "Iteration 77, loss = 0.46526005\n",
      "Iteration 1026, loss = 0.27016902\n",
      "Iteration 234, loss = 0.35408918\n",
      "Iteration 78, loss = 0.46346173\n",
      "Iteration 1741, loss = 0.20321215\n",
      "Iteration 79, loss = 0.46177004\n",
      "Iteration 235, loss = 0.35388351\n",
      "Iteration 80, loss = 0.46002234\n",
      "Iteration 860, loss = 0.32283911\n",
      "Iteration 81, loss = 0.45833905\n",
      "Iteration 236, loss = 0.35366674\n",
      "Iteration 82, loss = 0.45663443\n",
      "Iteration 83, loss = 0.45505250\n",
      "Iteration 976, loss = 0.27421108\n",
      "Iteration 237, loss = 0.35344322\n",
      "Iteration 84, loss = 0.45348903\n",
      "Iteration 85, loss = 0.45197359\n",
      "Iteration 238, loss = 0.35323025\n",
      "Iteration 86, loss = 0.45041785\n",
      "Iteration 1742, loss = 0.20313438\n",
      "Iteration 87, loss = 0.44893340\n",
      "Iteration 88, loss = 0.44748858\n",
      "Iteration 239, loss = 0.35301186\n",
      "Iteration 89, loss = 0.44600564\n",
      "Iteration 90, loss = 0.44456528\n",
      "Iteration 1548, loss = 0.23431514\n",
      "Iteration 240, loss = 0.35279445\n",
      "Iteration 91, loss = 0.44326542\n",
      "Iteration 241, loss = 0.35259448\n",
      "Iteration 92, loss = 0.44190163\n",
      "Iteration 1743, loss = 0.20309868\n",
      "Iteration 861, loss = 0.32274724\n",
      "Iteration 242, loss = 0.35237234\n",
      "Iteration 243, loss = 0.35216574\n",
      "Iteration 93, loss = 0.44058969\n",
      "Iteration 977, loss = 0.27390301\n",
      "Iteration 244, loss = 0.35196505\n",
      "Iteration 245, loss = 0.35175316\n",
      "Iteration 94, loss = 0.43932521\n",
      "Iteration 246, loss = 0.35154785\n",
      "Iteration 1744, loss = 0.20287593\n",
      "Iteration 89, loss = 0.60784744\n",
      "Iteration 247, loss = 0.35134664\n",
      "Iteration 95, loss = 0.43804431\n",
      "Iteration 248, loss = 0.35113253\n",
      "Iteration 1027, loss = 0.26998489\n",
      "Iteration 249, loss = 0.35094411\n",
      "Iteration 96, loss = 0.43687809\n",
      "Iteration 97, loss = 0.43562602\n",
      "Iteration 250, loss = 0.35074379\n",
      "Iteration 98, loss = 0.43443642\n",
      "Iteration 99, loss = 0.43335857\n",
      "Iteration 251, loss = 0.35053900\n",
      "Iteration 100, loss = 0.43215166\n",
      "Iteration 1745, loss = 0.20282660\n",
      "Iteration 252, loss = 0.35032737\n",
      "Iteration 101, loss = 0.43110034\n",
      "Iteration 102, loss = 0.43001197\n",
      "Iteration 253, loss = 0.35013378\n",
      "Iteration 862, loss = 0.32263164\n",
      "Iteration 103, loss = 0.42896010\n",
      "Iteration 104, loss = 0.42793310\n",
      "Iteration 254, loss = 0.34993998\n",
      "Iteration 105, loss = 0.42691000\n",
      "Iteration 106, loss = 0.42587801\n",
      "Iteration 255, loss = 0.34974577\n",
      "Iteration 1746, loss = 0.20268024\n",
      "Iteration 107, loss = 0.42491848\n",
      "Iteration 256, loss = 0.34956034\n",
      "Iteration 257, loss = 0.34936339\n",
      "Iteration 108, loss = 0.42394078\n",
      "Iteration 978, loss = 0.27374395\n",
      "Iteration 109, loss = 0.42299526\n",
      "Iteration 1549, loss = 0.23420916\n",
      "Iteration 110, loss = 0.42205956\n",
      "Iteration 1028, loss = 0.26966266\n",
      "Iteration 111, loss = 0.42116865\n",
      "Iteration 90, loss = 0.60491800\n",
      "Iteration 112, loss = 0.42021591\n",
      "Iteration 258, loss = 0.34916608\n",
      "Iteration 863, loss = 0.32244964\n",
      "Iteration 113, loss = 0.41937602\n",
      "Iteration 259, loss = 0.34897926\n",
      "Iteration 114, loss = 0.41848058\n",
      "Iteration 1747, loss = 0.20250154\n",
      "Iteration 115, loss = 0.41764623\n",
      "Iteration 116, loss = 0.41681126\n",
      "Iteration 117, loss = 0.41598094\n",
      "Iteration 118, loss = 0.41516825\n",
      "Iteration 1550, loss = 0.23408625\n",
      "Iteration 864, loss = 0.32233137\n",
      "Iteration 119, loss = 0.41438975\n",
      "Iteration 120, loss = 0.41359177\n",
      "Iteration 979, loss = 0.27351181\n",
      "Iteration 260, loss = 0.34879319\n",
      "Iteration 91, loss = 0.60213499\n",
      "Iteration 121, loss = 0.41283544\n",
      "Iteration 261, loss = 0.34860676\n",
      "Iteration 1748, loss = 0.20237840\n",
      "Iteration 262, loss = 0.34841311\n",
      "Iteration 1551, loss = 0.23403591\n",
      "Iteration 263, loss = 0.34821772\n",
      "Iteration 980, loss = 0.27330058\n",
      "Iteration 122, loss = 0.41206067\n",
      "Iteration 264, loss = 0.34805261\n",
      "Iteration 123, loss = 0.41131961\n",
      "Iteration 1749, loss = 0.20235558\n",
      "Iteration 981, loss = 0.27304253\n",
      "Iteration 124, loss = 0.41060839\n",
      "Iteration 125, loss = 0.40986768\n",
      "Iteration 265, loss = 0.34785363\n",
      "Iteration 865, loss = 0.32223149\n",
      "Iteration 126, loss = 0.40918261\n",
      "Iteration 1029, loss = 0.26960335\n",
      "Iteration 127, loss = 0.40848965\n",
      "Iteration 266, loss = 0.34767768\n",
      "Iteration 128, loss = 0.40779306\n",
      "Iteration 1750, loss = 0.20215144\n",
      "Iteration 129, loss = 0.40711206\n",
      "Iteration 1552, loss = 0.23383351\n",
      "Iteration 267, loss = 0.34749262\n",
      "Iteration 982, loss = 0.27283242\n",
      "Iteration 268, loss = 0.34730855\n",
      "Iteration 92, loss = 0.59914880\n",
      "Iteration 130, loss = 0.40644706\n",
      "Iteration 866, loss = 0.32216743\n",
      "Iteration 269, loss = 0.34715492\n",
      "Iteration 131, loss = 0.40577647\n",
      "Iteration 270, loss = 0.34695631\n",
      "Iteration 132, loss = 0.40516635\n",
      "Iteration 1751, loss = 0.20204618\n",
      "Iteration 271, loss = 0.34677686\n",
      "Iteration 133, loss = 0.40450772\n",
      "Iteration 272, loss = 0.34659472\n",
      "Iteration 273, loss = 0.34641434\n",
      "Iteration 983, loss = 0.27276138\n",
      "Iteration 274, loss = 0.34624515\n",
      "Iteration 134, loss = 0.40384149\n",
      "Iteration 1553, loss = 0.23371360\n",
      "Iteration 275, loss = 0.34607316\n",
      "Iteration 276, loss = 0.34590283\n",
      "Iteration 135, loss = 0.40324347\n",
      "Iteration 93, loss = 0.59633633\n",
      "Iteration 277, loss = 0.34572514\n",
      "Iteration 867, loss = 0.32199028\n",
      "Iteration 1030, loss = 0.26922228\n",
      "Iteration 1752, loss = 0.20190002\n",
      "Iteration 278, loss = 0.34554300\n",
      "Iteration 136, loss = 0.40264838\n",
      "Iteration 279, loss = 0.34538163\n",
      "Iteration 137, loss = 0.40210095\n",
      "Iteration 984, loss = 0.27244211\n",
      "Iteration 138, loss = 0.40144774\n",
      "Iteration 868, loss = 0.32184086\n",
      "Iteration 280, loss = 0.34520744\n",
      "Iteration 139, loss = 0.40087856\n",
      "Iteration 1554, loss = 0.23366510\n",
      "Iteration 281, loss = 0.34503020\n",
      "Iteration 140, loss = 0.40030323\n",
      "Iteration 94, loss = 0.59353900\n",
      "Iteration 141, loss = 0.39971299\n",
      "Iteration 282, loss = 0.34488976\n",
      "Iteration 869, loss = 0.32174656\n",
      "Iteration 1753, loss = 0.20180689\n",
      "Iteration 283, loss = 0.34471177\n",
      "Iteration 284, loss = 0.34454068\n",
      "Iteration 985, loss = 0.27230695\n",
      "Iteration 142, loss = 0.39915107\n",
      "Iteration 1555, loss = 0.23362462\n",
      "Iteration 1754, loss = 0.20167444\n",
      "Iteration 285, loss = 0.34437278\n",
      "Iteration 143, loss = 0.39859933\n",
      "Iteration 1031, loss = 0.26908874\n",
      "Iteration 286, loss = 0.34418819\n",
      "Iteration 144, loss = 0.39801489\n",
      "Iteration 986, loss = 0.27219570\n",
      "Iteration 287, loss = 0.34403912\n",
      "Iteration 145, loss = 0.39752901\n",
      "Iteration 288, loss = 0.34386685\n",
      "Iteration 289, loss = 0.34371175\n",
      "Iteration 146, loss = 0.39696982\n",
      "Iteration 1755, loss = 0.20161983\n",
      "Iteration 290, loss = 0.34354839\n",
      "Iteration 291, loss = 0.34337976\n",
      "Iteration 1556, loss = 0.23347825\n",
      "Iteration 292, loss = 0.34322357\n",
      "Iteration 987, loss = 0.27179001\n",
      "Iteration 293, loss = 0.34306011\n",
      "Iteration 870, loss = 0.32159300\n",
      "Iteration 147, loss = 0.39643197\n",
      "Iteration 1032, loss = 0.26882511\n",
      "Iteration 294, loss = 0.34290441\n",
      "Iteration 95, loss = 0.59067172\n",
      "Iteration 148, loss = 0.39594977\n",
      "Iteration 988, loss = 0.27162751\n",
      "Iteration 295, loss = 0.34274691\n",
      "Iteration 1756, loss = 0.20145284\n",
      "Iteration 149, loss = 0.39540303\n",
      "Iteration 150, loss = 0.39493914\n",
      "Iteration 296, loss = 0.34258179\n",
      "Iteration 989, loss = 0.27130689\n",
      "Iteration 151, loss = 0.39440914\n",
      "Iteration 152, loss = 0.39389413\n",
      "Iteration 153, loss = 0.39340621\n",
      "Iteration 154, loss = 0.39295153\n",
      "Iteration 297, loss = 0.34242776Iteration 1557, loss = 0.23324381\n",
      "\n",
      "Iteration 155, loss = 0.39246344\n",
      "Iteration 156, loss = 0.39200570\n",
      "Iteration 1033, loss = 0.26859975\n",
      "Iteration 871, loss = 0.32146689\n",
      "Iteration 157, loss = 0.39153820\n",
      "Iteration 1757, loss = 0.20129296\n",
      "Iteration 990, loss = 0.27125803\n",
      "Iteration 158, loss = 0.39107532\n",
      "Iteration 298, loss = 0.34227571\n",
      "Iteration 159, loss = 0.39063137\n",
      "Iteration 160, loss = 0.39016623\n",
      "Iteration 299, loss = 0.34212191\n",
      "Iteration 161, loss = 0.38971980\n",
      "Iteration 1558, loss = 0.23313611\n",
      "Iteration 162, loss = 0.38927215\n",
      "Iteration 1758, loss = 0.20119037\n",
      "Iteration 300, loss = 0.34195861\n",
      "Iteration 163, loss = 0.38887027\n",
      "Iteration 96, loss = 0.58796377\n",
      "Iteration 164, loss = 0.38841325\n",
      "Iteration 301, loss = 0.34180528\n",
      "Iteration 302, loss = 0.34165268\n",
      "Iteration 872, loss = 0.32136686\n",
      "Iteration 165, loss = 0.38797764\n",
      "Iteration 1034, loss = 0.26836772\n",
      "Iteration 991, loss = 0.27091999\n",
      "Iteration 303, loss = 0.34149732\n",
      "Iteration 166, loss = 0.38757906\n",
      "Iteration 1759, loss = 0.20124078\n",
      "Iteration 304, loss = 0.34135363\n",
      "Iteration 167, loss = 0.38715685\n",
      "Iteration 168, loss = 0.38673013\n",
      "Iteration 169, loss = 0.38632650\n",
      "Iteration 305, loss = 0.34120352\n",
      "Iteration 1559, loss = 0.23300057\n",
      "Iteration 170, loss = 0.38591979\n",
      "Iteration 171, loss = 0.38552538\n",
      "Iteration 306, loss = 0.34105049\n",
      "Iteration 1760, loss = 0.20107983\n",
      "Iteration 992, loss = 0.27069651\n",
      "Iteration 172, loss = 0.38514800\n",
      "Iteration 307, loss = 0.34090171\n",
      "Iteration 173, loss = 0.38473131\n",
      "Iteration 308, loss = 0.34075131\n",
      "Iteration 873, loss = 0.32124120\n",
      "Iteration 309, loss = 0.34059771\n",
      "Iteration 174, loss = 0.38434070\n",
      "Iteration 310, loss = 0.34047225\n",
      "Iteration 311, loss = 0.34031841\n",
      "Iteration 312, loss = 0.34017579\n",
      "Iteration 1761, loss = 0.20088046Iteration 175, loss = 0.38396397\n",
      "\n",
      "Iteration 97, loss = 0.58531012\n",
      "Iteration 176, loss = 0.38360046\n",
      "Iteration 313, loss = 0.34003138\n",
      "Iteration 177, loss = 0.38320016\n",
      "Iteration 314, loss = 0.33987863\n",
      "Iteration 1035, loss = 0.26823807\n",
      "Iteration 178, loss = 0.38282891\n",
      "Iteration 315, loss = 0.33973986\n",
      "Iteration 179, loss = 0.38244243\n",
      "Iteration 316, loss = 0.33960683\n",
      "Iteration 993, loss = 0.27062746\n",
      "Iteration 317, loss = 0.33945782\n",
      "Iteration 1560, loss = 0.23300192\n",
      "Iteration 1762, loss = 0.20075460\n",
      "Iteration 874, loss = 0.32114545\n",
      "Iteration 318, loss = 0.33931780\n",
      "Iteration 180, loss = 0.38206346\n",
      "Iteration 1763, loss = 0.20069613\n",
      "Iteration 98, loss = 0.58249044\n",
      "Iteration 181, loss = 0.38174459\n",
      "Iteration 319, loss = 0.33919909\n",
      "Iteration 1764, loss = 0.20046008\n",
      "Iteration 320, loss = 0.33904169\n",
      "Iteration 182, loss = 0.38137788\n",
      "Iteration 183, loss = 0.38100430\n",
      "Iteration 321, loss = 0.33890285\n",
      "Iteration 184, loss = 0.38064109\n",
      "Iteration 1765, loss = 0.20038276\n",
      "Iteration 185, loss = 0.38029697\n",
      "Iteration 186, loss = 0.37996815\n",
      "Iteration 322, loss = 0.33876963\n",
      "Iteration 994, loss = 0.27031703Iteration 187, loss = 0.37958721\n",
      "\n",
      "Iteration 1766, loss = 0.20030966\n",
      "Iteration 188, loss = 0.37925052\n",
      "Iteration 323, loss = 0.33862814\n",
      "Iteration 875, loss = 0.32096164\n",
      "Iteration 189, loss = 0.37893465\n",
      "Iteration 1036, loss = 0.26792031\n",
      "Iteration 324, loss = 0.33849164\n",
      "Iteration 1561, loss = 0.23279644\n",
      "Iteration 1767, loss = 0.20018992\n",
      "Iteration 325, loss = 0.33836839\n",
      "Iteration 326, loss = 0.33822976\n",
      "Iteration 190, loss = 0.37856596\n",
      "Iteration 191, loss = 0.37825253\n",
      "Iteration 327, loss = 0.33808841\n",
      "Iteration 328, loss = 0.33795718\n",
      "Iteration 99, loss = 0.57975366\n",
      "Iteration 876, loss = 0.32084488\n",
      "Iteration 329, loss = 0.33782365\n",
      "Iteration 192, loss = 0.37792177\n",
      "Iteration 995, loss = 0.27003354\n",
      "Iteration 1768, loss = 0.19997227\n",
      "Iteration 330, loss = 0.33769803\n",
      "Iteration 193, loss = 0.37758613\n",
      "Iteration 1562, loss = 0.23263604\n",
      "Iteration 331, loss = 0.33755663Iteration 194, loss = 0.37726626\n",
      "\n",
      "Iteration 195, loss = 0.37694820\n",
      "Iteration 332, loss = 0.33742585\n",
      "Iteration 1037, loss = 0.26766310\n",
      "Iteration 196, loss = 0.37662084\n",
      "Iteration 1769, loss = 0.19987919\n",
      "Iteration 333, loss = 0.33732369\n",
      "Iteration 996, loss = 0.26977720\n",
      "Iteration 197, loss = 0.37628257\n",
      "Iteration 334, loss = 0.33716494\n",
      "Iteration 1770, loss = 0.19979146\n",
      "Iteration 877, loss = 0.32074956\n",
      "Iteration 198, loss = 0.37599062\n",
      "Iteration 1563, loss = 0.23249223\n",
      "Iteration 335, loss = 0.33703593\n",
      "Iteration 100, loss = 0.57727541\n",
      "Iteration 199, loss = 0.37563846\n",
      "Iteration 1771, loss = 0.19965195\n",
      "Iteration 997, loss = 0.26965110\n",
      "Iteration 336, loss = 0.33690990\n",
      "Iteration 200, loss = 0.37534103\n",
      "Iteration 337, loss = 0.33678930\n",
      "Iteration 1564, loss = 0.23243678\n",
      "Iteration 1772, loss = 0.19959660\n",
      "Iteration 201, loss = 0.37503564\n",
      "Iteration 338, loss = 0.33665394\n",
      "Iteration 339, loss = 0.33653805\n",
      "Iteration 340, loss = 0.33640759\n",
      "Iteration 878, loss = 0.32058049\n",
      "Iteration 1038, loss = 0.26746853\n",
      "Iteration 341, loss = 0.33627944\n",
      "Iteration 202, loss = 0.37471276\n",
      "Iteration 342, loss = 0.33615433\n",
      "Iteration 998, loss = 0.26938377\n",
      "Iteration 1773, loss = 0.19940949\n",
      "Iteration 203, loss = 0.37441625\n",
      "Iteration 343, loss = 0.33602446\n",
      "Iteration 204, loss = 0.37412243\n",
      "Iteration 344, loss = 0.33591098\n",
      "Iteration 1565, loss = 0.23240545\n",
      "Iteration 205, loss = 0.37379967\n",
      "Iteration 101, loss = 0.57464857\n",
      "Iteration 999, loss = 0.26928766\n",
      "Iteration 206, loss = 0.37350013Iteration 1774, loss = 0.19930840\n",
      "\n",
      "Iteration 1039, loss = 0.26727706\n",
      "Iteration 879, loss = 0.32046019\n",
      "Iteration 345, loss = 0.33579084\n",
      "Iteration 1000, loss = 0.26892630\n",
      "Iteration 346, loss = 0.33565661\n",
      "Iteration 207, loss = 0.37319376\n",
      "Iteration 347, loss = 0.33554263\n",
      "Iteration 208, loss = 0.37291281\n",
      "Iteration 1775, loss = 0.19920363\n",
      "Iteration 348, loss = 0.33542640\n",
      "Iteration 1001, loss = 0.26867935\n",
      "Iteration 349, loss = 0.33529857\n",
      "Iteration 350, loss = 0.33517733\n",
      "Iteration 209, loss = 0.37261948\n",
      "Iteration 1566, loss = 0.23212962\n",
      "Iteration 351, loss = 0.33506871\n",
      "Iteration 352, loss = 0.33493832\n",
      "Iteration 210, loss = 0.37233572\n",
      "Iteration 1776, loss = 0.19933149\n",
      "Iteration 211, loss = 0.37204929\n",
      "Iteration 880, loss = 0.32043226\n",
      "Iteration 353, loss = 0.33484173\n",
      "Iteration 354, loss = 0.33472268\n",
      "Iteration 1777, loss = 0.19896154\n",
      "Iteration 212, loss = 0.37173455\n",
      "Iteration 1567, loss = 0.23204206\n",
      "Iteration 102, loss = 0.57214377Iteration 1002, loss = 0.26850934\n",
      "\n",
      "Iteration 213, loss = 0.37147095\n",
      "Iteration 355, loss = 0.33459484\n",
      "Iteration 1040, loss = 0.26702235\n",
      "Iteration 356, loss = 0.33448039\n",
      "Iteration 1778, loss = 0.19880733\n",
      "Iteration 214, loss = 0.37118899\n",
      "Iteration 881, loss = 0.32022227\n",
      "Iteration 357, loss = 0.33435657\n",
      "Iteration 215, loss = 0.37089911\n",
      "Iteration 358, loss = 0.33425332\n",
      "Iteration 216, loss = 0.37063877\n",
      "Iteration 103, loss = 0.56963594\n",
      "Iteration 1003, loss = 0.26835390\n",
      "Iteration 359, loss = 0.33415844\n",
      "Iteration 1568, loss = 0.23192926\n",
      "Iteration 1779, loss = 0.19900129\n",
      "Iteration 217, loss = 0.37033349\n",
      "Iteration 218, loss = 0.37007373\n",
      "Iteration 360, loss = 0.33401904\n",
      "Iteration 219, loss = 0.36978733\n",
      "Iteration 220, loss = 0.36952965\n",
      "Iteration 361, loss = 0.33393123\n",
      "Iteration 221, loss = 0.36925871\n",
      "Iteration 362, loss = 0.33380177\n",
      "Iteration 222, loss = 0.36895659\n",
      "Iteration 363, loss = 0.33368679\n",
      "Iteration 223, loss = 0.36870185\n",
      "Iteration 364, loss = 0.33356981\n",
      "Iteration 224, loss = 0.36843560\n",
      "Iteration 365, loss = 0.33346855\n",
      "Iteration 1780, loss = 0.19871920\n",
      "Iteration 225, loss = 0.36818588\n",
      "Iteration 366, loss = 0.33336308\n",
      "Iteration 882, loss = 0.32016543\n",
      "Iteration 1041, loss = 0.26689988\n",
      "Iteration 226, loss = 0.36790789\n",
      "Iteration 1569, loss = 0.23178357\n",
      "Iteration 367, loss = 0.33323998\n",
      "Iteration 104, loss = 0.56718706\n",
      "Iteration 368, loss = 0.33314621\n",
      "Iteration 227, loss = 0.36764328\n",
      "Iteration 228, loss = 0.36738984\n",
      "Iteration 369, loss = 0.33302821\n",
      "Iteration 229, loss = 0.36711383\n",
      "Iteration 1004, loss = 0.26830867\n",
      "Iteration 1781, loss = 0.19857849\n",
      "Iteration 370, loss = 0.33292408\n",
      "Iteration 230, loss = 0.36686940\n",
      "Iteration 883, loss = 0.31996506\n",
      "Iteration 231, loss = 0.36662993\n",
      "Iteration 371, loss = 0.33281120\n",
      "Iteration 232, loss = 0.36636905\n",
      "Iteration 372, loss = 0.33271343\n",
      "Iteration 1042, loss = 0.26660344\n",
      "Iteration 233, loss = 0.36610793\n",
      "Iteration 373, loss = 0.33259569\n",
      "Iteration 234, loss = 0.36585433\n",
      "Iteration 374, loss = 0.33251095\n",
      "Iteration 375, loss = 0.33239255\n",
      "Iteration 1570, loss = 0.23169367\n",
      "Iteration 235, loss = 0.36559305\n",
      "Iteration 1782, loss = 0.19836813\n",
      "Iteration 376, loss = 0.33227413\n",
      "Iteration 1005, loss = 0.26810463\n",
      "Iteration 236, loss = 0.36535577\n",
      "Iteration 377, loss = 0.33216234\n",
      "Iteration 378, loss = 0.33206864\n",
      "Iteration 884, loss = 0.31984991\n",
      "Iteration 379, loss = 0.33197051\n",
      "Iteration 380, loss = 0.33185810\n",
      "Iteration 237, loss = 0.36508404Iteration 1783, loss = 0.19822138\n",
      "\n",
      "Iteration 105, loss = 0.56475801\n",
      "Iteration 1043, loss = 0.26658280\n",
      "Iteration 381, loss = 0.33175427\n",
      "Iteration 238, loss = 0.36485982\n",
      "Iteration 382, loss = 0.33165584\n",
      "Iteration 383, loss = 0.33155897\n",
      "Iteration 1571, loss = 0.23156772\n",
      "Iteration 239, loss = 0.36459279\n",
      "Iteration 384, loss = 0.33143888\n",
      "Iteration 385, loss = 0.33135026\n",
      "Iteration 1006, loss = 0.26757093Iteration 240, loss = 0.36434891\n",
      "\n",
      "Iteration 386, loss = 0.33124573\n",
      "Iteration 1784, loss = 0.19814626\n",
      "Iteration 241, loss = 0.36410722\n",
      "Iteration 885, loss = 0.31972664\n",
      "Iteration 242, loss = 0.36386627\n",
      "Iteration 387, loss = 0.33114423\n",
      "Iteration 106, loss = 0.56234105\n",
      "Iteration 388, loss = 0.33103750\n",
      "Iteration 1044, loss = 0.26612881\n",
      "Iteration 1572, loss = 0.23145313\n",
      "Iteration 389, loss = 0.33093826\n",
      "Iteration 243, loss = 0.36362726\n",
      "Iteration 390, loss = 0.33082953\n",
      "Iteration 886, loss = 0.31962247\n",
      "Iteration 244, loss = 0.36337157\n",
      "Iteration 391, loss = 0.33073427\n",
      "Iteration 1785, loss = 0.19804219\n",
      "Iteration 392, loss = 0.33064204\n",
      "Iteration 1007, loss = 0.26738556\n",
      "Iteration 245, loss = 0.36314379\n",
      "Iteration 393, loss = 0.33054921\n",
      "Iteration 394, loss = 0.33044595\n",
      "Iteration 246, loss = 0.36290667\n",
      "Iteration 247, loss = 0.36265301\n",
      "Iteration 248, loss = 0.36244492\n",
      "Iteration 887, loss = 0.31947795\n",
      "Iteration 395, loss = 0.33033510\n",
      "Iteration 1008, loss = 0.26712421\n",
      "Iteration 249, loss = 0.36219001\n",
      "Iteration 250, loss = 0.36197168\n",
      "Iteration 251, loss = 0.36174750\n",
      "Iteration 396, loss = 0.33024573\n",
      "Iteration 252, loss = 0.36151139\n",
      "Iteration 253, loss = 0.36128295\n",
      "Iteration 397, loss = 0.33014081\n",
      "Iteration 1573, loss = 0.23131052\n",
      "Iteration 1786, loss = 0.19791314\n",
      "Iteration 254, loss = 0.36104700\n",
      "Iteration 1045, loss = 0.26597194\n",
      "Iteration 255, loss = 0.36083201\n",
      "Iteration 888, loss = 0.31933690\n",
      "Iteration 1787, loss = 0.19776192\n",
      "Iteration 398, loss = 0.33004767\n",
      "Iteration 256, loss = 0.36060638\n",
      "Iteration 399, loss = 0.32994384\n",
      "Iteration 257, loss = 0.36039421\n",
      "Iteration 107, loss = 0.56014926\n",
      "Iteration 400, loss = 0.32984959\n",
      "Iteration 258, loss = 0.36015942\n",
      "Iteration 259, loss = 0.35995215\n",
      "Iteration 260, loss = 0.35971574\n",
      "Iteration 401, loss = 0.32976209\n",
      "Iteration 261, loss = 0.35948296\n",
      "Iteration 402, loss = 0.32966079\n",
      "Iteration 262, loss = 0.35928507\n",
      "Iteration 403, loss = 0.32955651\n",
      "Iteration 1574, loss = 0.23124921\n",
      "Iteration 263, loss = 0.35905484\n",
      "Iteration 1788, loss = 0.19764007\n",
      "Iteration 264, loss = 0.35885178\n",
      "Iteration 1046, loss = 0.26563482\n",
      "Iteration 108, loss = 0.55784770\n",
      "Iteration 265, loss = 0.35863686\n",
      "Iteration 266, loss = 0.35841085\n",
      "Iteration 1009, loss = 0.26706379\n",
      "Iteration 267, loss = 0.35820613\n",
      "Iteration 1789, loss = 0.19760153\n",
      "Iteration 268, loss = 0.35798613\n",
      "Iteration 404, loss = 0.32946275\n",
      "Iteration 269, loss = 0.35776896\n",
      "Iteration 270, loss = 0.35760624\n",
      "Iteration 1790, loss = 0.19747070\n",
      "Iteration 889, loss = 0.31926983\n",
      "Iteration 405, loss = 0.32937724\n",
      "Iteration 271, loss = 0.35733572\n",
      "Iteration 406, loss = 0.32928959\n",
      "Iteration 1010, loss = 0.26670850\n",
      "Iteration 407, loss = 0.32918354\n",
      "Iteration 408, loss = 0.32908478\n",
      "Iteration 409, loss = 0.32899785\n",
      "Iteration 109, loss = 0.55552667\n",
      "Iteration 410, loss = 0.32890354\n",
      "Iteration 411, loss = 0.32881235\n",
      "Iteration 412, loss = 0.32871552\n",
      "Iteration 1791, loss = 0.19730749\n",
      "Iteration 890, loss = 0.31911822\n",
      "Iteration 413, loss = 0.32862014\n",
      "Iteration 1011, loss = 0.26647463\n",
      "Iteration 1575, loss = 0.23111654\n",
      "Iteration 414, loss = 0.32854735\n",
      "Iteration 272, loss = 0.35715510\n",
      "Iteration 110, loss = 0.55338607\n",
      "Iteration 1792, loss = 0.19715303\n",
      "Iteration 415, loss = 0.32844599\n",
      "Iteration 273, loss = 0.35692502\n",
      "Iteration 416, loss = 0.32835674\n",
      "Iteration 1576, loss = 0.23095360\n",
      "Iteration 417, loss = 0.32825871\n",
      "Iteration 274, loss = 0.35672004\n",
      "Iteration 1047, loss = 0.26547344\n",
      "Iteration 1793, loss = 0.19703430\n",
      "Iteration 418, loss = 0.32816980\n",
      "Iteration 275, loss = 0.35651515\n",
      "Iteration 419, loss = 0.32807769\n",
      "Iteration 420, loss = 0.32799476\n",
      "Iteration 276, loss = 0.35631175\n",
      "Iteration 1577, loss = 0.23083471\n",
      "Iteration 277, loss = 0.35610343\n",
      "Iteration 1012, loss = 0.26626983\n",
      "Iteration 421, loss = 0.32790579\n",
      "Iteration 278, loss = 0.35589774\n",
      "Iteration 1794, loss = 0.19706815\n",
      "Iteration 279, loss = 0.35570195\n",
      "Iteration 891, loss = 0.31900045Iteration 111, loss = 0.55127084\n",
      "Iteration 422, loss = 0.32781336\n",
      "\n",
      "Iteration 1795, loss = 0.19682751\n",
      "Iteration 280, loss = 0.35550770\n",
      "Iteration 281, loss = 0.35528746\n",
      "Iteration 423, loss = 0.32771929\n",
      "Iteration 1796, loss = 0.19676242\n",
      "Iteration 282, loss = 0.35509160\n",
      "Iteration 1578, loss = 0.23067734\n",
      "Iteration 283, loss = 0.35490402\n",
      "Iteration 1013, loss = 0.26599008\n",
      "Iteration 424, loss = 0.32763417\n",
      "Iteration 284, loss = 0.35470380\n",
      "Iteration 285, loss = 0.35450532\n",
      "Iteration 1797, loss = 0.19670025\n",
      "Iteration 425, loss = 0.32755422\n",
      "Iteration 286, loss = 0.35429892\n",
      "Iteration 287, loss = 0.35411095\n",
      "Iteration 288, loss = 0.35390482\n",
      "Iteration 426, loss = 0.32746620\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1014, loss = 0.26609019\n",
      "Iteration 1798, loss = 0.19644369\n",
      "Iteration 1048, loss = 0.26532306\n",
      "Iteration 289, loss = 0.35373008\n",
      "Iteration 892, loss = 0.31885023\n",
      "Iteration 290, loss = 0.35352769\n",
      "Iteration 1579, loss = 0.23067425\n",
      "Iteration 1, loss = 0.70176382\n",
      "Iteration 1015, loss = 0.26562115\n",
      "Iteration 291, loss = 0.35330651\n",
      "Iteration 2, loss = 0.70014913\n",
      "Iteration 292, loss = 0.35313584\n",
      "Iteration 293, loss = 0.35295626\n",
      "Iteration 3, loss = 0.69759393\n",
      "Iteration 294, loss = 0.35272485\n",
      "Iteration 4, loss = 0.69447812\n",
      "Iteration 1799, loss = 0.19629762\n",
      "Iteration 295, loss = 0.35254378\n",
      "Iteration 5, loss = 0.69106821\n",
      "Iteration 6, loss = 0.68727267\n",
      "Iteration 296, loss = 0.35237113\n",
      "Iteration 1580, loss = 0.23048691\n",
      "Iteration 112, loss = 0.54911482\n",
      "Iteration 7, loss = 0.68324488\n",
      "Iteration 297, loss = 0.35217789\n",
      "Iteration 1016, loss = 0.26532916\n",
      "Iteration 298, loss = 0.35197111\n",
      "Iteration 299, loss = 0.35178108\n",
      "Iteration 1800, loss = 0.19623024\n",
      "Iteration 8, loss = 0.67909142\n",
      "Iteration 300, loss = 0.35160210\n",
      "Iteration 9, loss = 0.67491943\n",
      "Iteration 893, loss = 0.31876724\n",
      "Iteration 301, loss = 0.35140350\n",
      "Iteration 302, loss = 0.35120840\n",
      "Iteration 1801, loss = 0.19608941\n",
      "Iteration 303, loss = 0.35103148\n",
      "Iteration 10, loss = 0.67076524\n",
      "Iteration 1017, loss = 0.26512138\n",
      "Iteration 304, loss = 0.35083766\n",
      "Iteration 1049, loss = 0.26515922\n",
      "Iteration 305, loss = 0.35065097\n",
      "Iteration 1802, loss = 0.19595832\n",
      "Iteration 306, loss = 0.35047061\n",
      "Iteration 307, loss = 0.35026316\n",
      "Iteration 11, loss = 0.66658590\n",
      "Iteration 1581, loss = 0.23033296\n",
      "Iteration 308, loss = 0.35011178\n",
      "Iteration 113, loss = 0.54718307\n",
      "Iteration 309, loss = 0.34991256\n",
      "Iteration 1803, loss = 0.19588791\n",
      "Iteration 310, loss = 0.34971968\n",
      "Iteration 12, loss = 0.66245578\n",
      "Iteration 311, loss = 0.34953626\n",
      "Iteration 1018, loss = 0.26505564\n",
      "Iteration 312, loss = 0.34935139\n",
      "Iteration 313, loss = 0.34920742\n",
      "Iteration 13, loss = 0.65826485\n",
      "Iteration 314, loss = 0.34898918\n",
      "Iteration 894, loss = 0.31863432\n",
      "Iteration 1804, loss = 0.19575552\n",
      "Iteration 315, loss = 0.34882037\n",
      "Iteration 14, loss = 0.65417335\n",
      "Iteration 316, loss = 0.34863213\n",
      "Iteration 317, loss = 0.34846321\n",
      "Iteration 318, loss = 0.34827793\n",
      "Iteration 319, loss = 0.34810708\n",
      "Iteration 15, loss = 0.65022350\n",
      "Iteration 320, loss = 0.34792558\n",
      "Iteration 1582, loss = 0.23024701\n",
      "Iteration 1805, loss = 0.19561770Iteration 321, loss = 0.34776206\n",
      "\n",
      "Iteration 322, loss = 0.34758158\n",
      "Iteration 16, loss = 0.64618842\n",
      "Iteration 1019, loss = 0.26470453\n",
      "Iteration 323, loss = 0.34741629\n",
      "Iteration 1050, loss = 0.26503262\n",
      "Iteration 324, loss = 0.34726561\n",
      "Iteration 325, loss = 0.34708229\n",
      "Iteration 17, loss = 0.64229432\n",
      "Iteration 326, loss = 0.34690999\n",
      "Iteration 327, loss = 0.34673399\n",
      "Iteration 18, loss = 0.63829860\n",
      "Iteration 328, loss = 0.34659154\n",
      "Iteration 1806, loss = 0.19548179\n",
      "Iteration 1020, loss = 0.26463036\n",
      "Iteration 329, loss = 0.34638649\n",
      "Iteration 114, loss = 0.54514822\n",
      "Iteration 330, loss = 0.34620738\n",
      "Iteration 331, loss = 0.34606004\n",
      "Iteration 19, loss = 0.63444804\n",
      "Iteration 1807, loss = 0.19539147\n",
      "Iteration 332, loss = 0.34587496\n",
      "Iteration 333, loss = 0.34573532\n",
      "Iteration 20, loss = 0.63055706\n",
      "Iteration 334, loss = 0.34554239\n",
      "Iteration 21, loss = 0.62673074\n",
      "Iteration 335, loss = 0.34538607\n",
      "Iteration 1808, loss = 0.19540107\n",
      "Iteration 336, loss = 0.34521821\n",
      "Iteration 1583, loss = 0.23012118\n",
      "Iteration 22, loss = 0.62312397\n",
      "Iteration 337, loss = 0.34503851\n",
      "Iteration 1021, loss = 0.26423946\n",
      "Iteration 1809, loss = 0.19517128\n",
      "Iteration 115, loss = 0.54312724\n",
      "Iteration 23, loss = 0.61941482\n",
      "Iteration 338, loss = 0.34488809\n",
      "Iteration 895, loss = 0.31848298\n",
      "Iteration 1051, loss = 0.26456609\n",
      "Iteration 1810, loss = 0.19506969\n",
      "Iteration 339, loss = 0.34471306\n",
      "Iteration 24, loss = 0.61571716\n",
      "Iteration 1584, loss = 0.23004012\n",
      "Iteration 340, loss = 0.34454936\n",
      "Iteration 1811, loss = 0.19492864\n",
      "Iteration 25, loss = 0.61214028\n",
      "Iteration 341, loss = 0.34439173\n",
      "Iteration 26, loss = 0.60842858\n",
      "Iteration 27, loss = 0.60489045\n",
      "Iteration 342, loss = 0.34424609\n",
      "Iteration 1052, loss = 0.26439937\n",
      "Iteration 1812, loss = 0.19481801\n",
      "Iteration 28, loss = 0.60128241\n",
      "Iteration 1585, loss = 0.22996611\n",
      "Iteration 343, loss = 0.34408223\n",
      "Iteration 29, loss = 0.59779940\n",
      "Iteration 344, loss = 0.34389638\n",
      "Iteration 30, loss = 0.59429931\n",
      "Iteration 116, loss = 0.54115205\n",
      "Iteration 345, loss = 0.34374513\n",
      "Iteration 896, loss = 0.31840708\n",
      "Iteration 31, loss = 0.59075926\n",
      "Iteration 346, loss = 0.34358753\n",
      "Iteration 32, loss = 0.58741478\n",
      "Iteration 347, loss = 0.34342003\n",
      "Iteration 33, loss = 0.58398617Iteration 1022, loss = 0.26399213\n",
      "Iteration 348, loss = 0.34325923\n",
      "Iteration 1813, loss = 0.19470145\n",
      "Iteration 1586, loss = 0.22977198\n",
      "Iteration 349, loss = 0.34310445\n",
      "Iteration 350, loss = 0.34295553\n",
      "\n",
      "Iteration 1053, loss = 0.26420441\n",
      "Iteration 351, loss = 0.34280551\n",
      "Iteration 352, loss = 0.34263320\n",
      "Iteration 1023, loss = 0.26387903\n",
      "Iteration 353, loss = 0.34247988\n",
      "Iteration 354, loss = 0.34231750\n",
      "Iteration 1814, loss = 0.19458847\n",
      "Iteration 355, loss = 0.34217043\n",
      "Iteration 34, loss = 0.58059437\n",
      "Iteration 356, loss = 0.34201034\n",
      "Iteration 357, loss = 0.34186487\n",
      "Iteration 1024, loss = 0.26359233\n",
      "Iteration 1815, loss = 0.19448873\n",
      "Iteration 358, loss = 0.34170565\n",
      "Iteration 359, loss = 0.34154608\n",
      "Iteration 35, loss = 0.57720278\n",
      "Iteration 360, loss = 0.34141005\n",
      "Iteration 36, loss = 0.57388449\n",
      "Iteration 361, loss = 0.34125465\n",
      "Iteration 897, loss = 0.31825979\n",
      "Iteration 1025, loss = 0.26339030\n",
      "Iteration 362, loss = 0.34111624\n",
      "Iteration 37, loss = 0.57049903\n",
      "Iteration 1054, loss = 0.26398919\n",
      "Iteration 363, loss = 0.34096657\n",
      "Iteration 38, loss = 0.56727417\n",
      "Iteration 364, loss = 0.34079538\n",
      "Iteration 365, loss = 0.34064508\n",
      "Iteration 366, loss = 0.34048484\n",
      "Iteration 39, loss = 0.56393602\n",
      "Iteration 117, loss = 0.53937541\n",
      "Iteration 1816, loss = 0.19437656\n",
      "Iteration 40, loss = 0.56069927\n",
      "Iteration 367, loss = 0.34034281\n",
      "Iteration 1026, loss = 0.26307904\n",
      "Iteration 368, loss = 0.34020620\n",
      "Iteration 41, loss = 0.55743737\n",
      "Iteration 369, loss = 0.34006242\n",
      "Iteration 42, loss = 0.55429698\n",
      "Iteration 1587, loss = 0.22969586\n",
      "Iteration 898, loss = 0.31813876\n",
      "Iteration 370, loss = 0.33990021\n",
      "Iteration 43, loss = 0.55099866\n",
      "Iteration 371, loss = 0.33976063\n",
      "Iteration 1817, loss = 0.19419143\n",
      "Iteration 372, loss = 0.33961782\n",
      "Iteration 1027, loss = 0.26293023\n",
      "Iteration 1588, loss = 0.22946914\n",
      "Iteration 118, loss = 0.53747239\n",
      "Iteration 1818, loss = 0.19414421\n",
      "Iteration 44, loss = 0.54786634\n",
      "Iteration 899, loss = 0.31804979\n",
      "Iteration 373, loss = 0.33946270\n",
      "Iteration 45, loss = 0.54469948\n",
      "Iteration 1055, loss = 0.26374672\n",
      "Iteration 46, loss = 0.54160666\n",
      "Iteration 1819, loss = 0.19404694\n",
      "Iteration 1028, loss = 0.26264698\n",
      "Iteration 374, loss = 0.33931833\n",
      "Iteration 47, loss = 0.53858785\n",
      "Iteration 900, loss = 0.31788491\n",
      "Iteration 375, loss = 0.33919562\n",
      "Iteration 1589, loss = 0.22945704\n",
      "Iteration 1820, loss = 0.19392812\n",
      "Iteration 48, loss = 0.53547663\n",
      "Iteration 1821, loss = 0.19384085\n",
      "Iteration 1029, loss = 0.26243422\n",
      "Iteration 49, loss = 0.53248122\n",
      "Iteration 376, loss = 0.33902146\n",
      "Iteration 1822, loss = 0.19361481\n",
      "Iteration 119, loss = 0.53571147\n",
      "Iteration 377, loss = 0.33890832\n",
      "Iteration 378, loss = 0.33874623\n",
      "Iteration 379, loss = 0.33858406\n",
      "Iteration 50, loss = 0.52957542\n",
      "Iteration 380, loss = 0.33845524\n",
      "Iteration 1823, loss = 0.19361882\n",
      "Iteration 381, loss = 0.33831216\n",
      "Iteration 382, loss = 0.33817728\n",
      "Iteration 901, loss = 0.31774899\n",
      "Iteration 383, loss = 0.33802655\n",
      "Iteration 51, loss = 0.52649600\n",
      "Iteration 1824, loss = 0.19347697\n",
      "Iteration 384, loss = 0.33788491\n",
      "Iteration 1030, loss = 0.26235005\n",
      "Iteration 52, loss = 0.52360046\n",
      "Iteration 385, loss = 0.33774862\n",
      "Iteration 1590, loss = 0.22924824\n",
      "Iteration 53, loss = 0.52080730\n",
      "Iteration 386, loss = 0.33761423\n",
      "Iteration 387, loss = 0.33748457\n",
      "Iteration 1056, loss = 0.26357002Iteration 54, loss = 0.51791896\n",
      "\n",
      "Iteration 388, loss = 0.33733364\n",
      "Iteration 1031, loss = 0.26224717\n",
      "Iteration 55, loss = 0.51504521\n",
      "Iteration 389, loss = 0.33722118\n",
      "Iteration 390, loss = 0.33707161\n",
      "Iteration 56, loss = 0.51229990\n",
      "Iteration 1825, loss = 0.19336608\n",
      "Iteration 391, loss = 0.33693438\n",
      "Iteration 57, loss = 0.50954361\n",
      "Iteration 392, loss = 0.33680559\n",
      "Iteration 1032, loss = 0.26191237\n",
      "Iteration 393, loss = 0.33666857\n",
      "Iteration 394, loss = 0.33652581\n",
      "Iteration 120, loss = 0.53394185\n",
      "Iteration 1591, loss = 0.22915921\n",
      "Iteration 395, loss = 0.33639775\n",
      "Iteration 1826, loss = 0.19317413\n",
      "Iteration 902, loss = 0.31768359\n",
      "Iteration 396, loss = 0.33626937\n",
      "Iteration 58, loss = 0.50681761\n",
      "Iteration 397, loss = 0.33614189\n",
      "Iteration 398, loss = 0.33598615\n",
      "Iteration 399, loss = 0.33587301\n",
      "Iteration 400, loss = 0.33572198\n",
      "Iteration 59, loss = 0.50413332\n",
      "Iteration 401, loss = 0.33562025\n",
      "Iteration 1592, loss = 0.22899101\n",
      "Iteration 1033, loss = 0.26169716\n",
      "Iteration 402, loss = 0.33547231\n",
      "Iteration 403, loss = 0.33533021\n",
      "Iteration 60, loss = 0.50145996\n",
      "Iteration 404, loss = 0.33523860\n",
      "Iteration 1827, loss = 0.19305198\n",
      "Iteration 903, loss = 0.31754671\n",
      "Iteration 405, loss = 0.33507933\n",
      "Iteration 61, loss = 0.49892551\n",
      "Iteration 1034, loss = 0.26129975\n",
      "Iteration 62, loss = 0.49629981\n",
      "Iteration 121, loss = 0.53225830\n",
      "Iteration 63, loss = 0.49378243\n",
      "Iteration 904, loss = 0.31744165\n",
      "Iteration 1035, loss = 0.26120796\n",
      "Iteration 64, loss = 0.49130277\n",
      "Iteration 1828, loss = 0.19294973\n",
      "Iteration 406, loss = 0.33495822\n",
      "Iteration 1593, loss = 0.22891302\n",
      "Iteration 65, loss = 0.48885982\n",
      "Iteration 407, loss = 0.33484102\n",
      "Iteration 1036, loss = 0.26086206\n",
      "Iteration 66, loss = 0.48635143\n",
      "Iteration 1057, loss = 0.26324762\n",
      "Iteration 408, loss = 0.33468443\n",
      "Iteration 67, loss = 0.48398420\n",
      "Iteration 1829, loss = 0.19282274\n",
      "Iteration 68, loss = 0.48164118\n",
      "Iteration 409, loss = 0.33457018\n",
      "Iteration 69, loss = 0.47935397\n",
      "Iteration 1037, loss = 0.26062279\n",
      "Iteration 1830, loss = 0.19272959\n",
      "Iteration 70, loss = 0.47714703\n",
      "Iteration 410, loss = 0.33444402\n",
      "Iteration 1594, loss = 0.22877750\n",
      "Iteration 905, loss = 0.31730093\n",
      "Iteration 411, loss = 0.33433654\n",
      "Iteration 71, loss = 0.47484172\n",
      "Iteration 1038, loss = 0.26050752\n",
      "Iteration 72, loss = 0.47265749\n",
      "Iteration 1831, loss = 0.19257722\n",
      "Iteration 906, loss = 0.31714889\n",
      "Iteration 73, loss = 0.47049898\n",
      "Iteration 1832, loss = 0.19246173\n",
      "Iteration 1595, loss = 0.22862827\n",
      "Iteration 412, loss = 0.33418146\n",
      "Iteration 74, loss = 0.46848066\n",
      "Iteration 413, loss = 0.33403745\n",
      "Iteration 1058, loss = 0.26304563\n",
      "Iteration 1039, loss = 0.26025015\n",
      "Iteration 1833, loss = 0.19234342\n",
      "Iteration 122, loss = 0.53053868\n",
      "Iteration 75, loss = 0.46632211\n",
      "Iteration 414, loss = 0.33391788\n",
      "Iteration 76, loss = 0.46434538\n",
      "Iteration 907, loss = 0.31709376\n",
      "Iteration 415, loss = 0.33379816\n",
      "Iteration 77, loss = 0.46231166\n",
      "Iteration 1834, loss = 0.19223308\n",
      "Iteration 416, loss = 0.33365573\n",
      "Iteration 1040, loss = 0.26002692\n",
      "Iteration 1596, loss = 0.22860053\n",
      "Iteration 78, loss = 0.46035787\n",
      "Iteration 417, loss = 0.33353442\n",
      "Iteration 908, loss = 0.31694072Iteration 123, loss = 0.52892697\n",
      "\n",
      "Iteration 1835, loss = 0.19208808\n",
      "Iteration 418, loss = 0.33342818\n",
      "Iteration 1041, loss = 0.25975007\n",
      "Iteration 79, loss = 0.45846941\n",
      "Iteration 419, loss = 0.33327860\n",
      "Iteration 80, loss = 0.45653002\n",
      "Iteration 1059, loss = 0.26291949\n",
      "Iteration 420, loss = 0.33315828\n",
      "Iteration 81, loss = 0.45469145\n",
      "Iteration 1597, loss = 0.22842451\n",
      "Iteration 421, loss = 0.33303770\n",
      "Iteration 1836, loss = 0.19199058\n",
      "Iteration 422, loss = 0.33291178\n",
      "Iteration 423, loss = 0.33279129\n",
      "Iteration 1042, loss = 0.25968445\n",
      "Iteration 82, loss = 0.45285764\n",
      "Iteration 424, loss = 0.33266726\n",
      "Iteration 909, loss = 0.31679778\n",
      "Iteration 1598, loss = 0.22832316\n",
      "Iteration 425, loss = 0.33253174\n",
      "Iteration 83, loss = 0.45101844\n",
      "Iteration 426, loss = 0.33241073\n",
      "Iteration 1837, loss = 0.19187568\n",
      "Iteration 427, loss = 0.33228504\n",
      "Iteration 84, loss = 0.44931796\n",
      "Iteration 428, loss = 0.33216540\n",
      "Iteration 1043, loss = 0.25930780\n",
      "Iteration 429, loss = 0.33204592\n",
      "Iteration 85, loss = 0.44761760\n",
      "Iteration 1599, loss = 0.22815711\n",
      "Iteration 430, loss = 0.33192699\n",
      "Iteration 86, loss = 0.44588332\n",
      "Iteration 431, loss = 0.33181574\n",
      "Iteration 124, loss = 0.52735187\n",
      "Iteration 1838, loss = 0.19179447\n",
      "Iteration 87, loss = 0.44428645\n",
      "Iteration 910, loss = 0.31668549\n",
      "Iteration 432, loss = 0.33168021\n",
      "Iteration 433, loss = 0.33158208\n",
      "Iteration 88, loss = 0.44259520\n",
      "Iteration 1839, loss = 0.19163857\n",
      "Iteration 1044, loss = 0.25912045\n",
      "Iteration 89, loss = 0.44106910\n",
      "Iteration 434, loss = 0.33144334\n",
      "Iteration 1060, loss = 0.26278884\n",
      "Iteration 90, loss = 0.43948691\n",
      "Iteration 435, loss = 0.33133244\n",
      "Iteration 1840, loss = 0.19153945\n",
      "Iteration 91, loss = 0.43792055\n",
      "Iteration 436, loss = 0.33121510\n",
      "Iteration 1841, loss = 0.19143467\n",
      "Iteration 437, loss = 0.33110137\n",
      "Iteration 92, loss = 0.43641753\n",
      "Iteration 438, loss = 0.33097810\n",
      "Iteration 125, loss = 0.52571926\n",
      "Iteration 911, loss = 0.31653368\n",
      "Iteration 1842, loss = 0.19127137\n",
      "Iteration 93, loss = 0.43496358\n",
      "Iteration 439, loss = 0.33085796\n",
      "Iteration 1045, loss = 0.25886213\n",
      "Iteration 1600, loss = 0.22800871\n",
      "Iteration 1843, loss = 0.19121568\n",
      "Iteration 440, loss = 0.33074981\n",
      "Iteration 94, loss = 0.43348348\n",
      "Iteration 95, loss = 0.43204439\n",
      "Iteration 441, loss = 0.33064130\n",
      "Iteration 912, loss = 0.31641304\n",
      "Iteration 1844, loss = 0.19115378\n",
      "Iteration 96, loss = 0.43070559\n",
      "Iteration 1061, loss = 0.26240929\n",
      "Iteration 1046, loss = 0.25879000\n",
      "Iteration 97, loss = 0.42932855\n",
      "Iteration 442, loss = 0.33052005\n",
      "Iteration 443, loss = 0.33041261\n",
      "Iteration 98, loss = 0.42795293\n",
      "Iteration 444, loss = 0.33030196\n",
      "Iteration 445, loss = 0.33018567\n",
      "Iteration 913, loss = 0.31629171\n",
      "Iteration 99, loss = 0.42665383\n",
      "Iteration 446, loss = 0.33006852\n",
      "Iteration 1845, loss = 0.19092308\n",
      "Iteration 100, loss = 0.42538657\n",
      "Iteration 447, loss = 0.32998405\n",
      "Iteration 1047, loss = 0.25867675\n",
      "Iteration 448, loss = 0.32986855\n",
      "Iteration 101, loss = 0.42408581\n",
      "Iteration 449, loss = 0.32973903\n",
      "Iteration 1601, loss = 0.22791522\n",
      "Iteration 126, loss = 0.52418971\n",
      "Iteration 450, loss = 0.32963807\n",
      "Iteration 102, loss = 0.42290346\n",
      "Iteration 1846, loss = 0.19090577\n",
      "Iteration 103, loss = 0.42160652\n",
      "Iteration 451, loss = 0.32952098\n",
      "Iteration 1062, loss = 0.26215361\n",
      "Iteration 452, loss = 0.32942265\n",
      "Iteration 453, loss = 0.32930152\n",
      "Iteration 914, loss = 0.31618863\n",
      "Iteration 104, loss = 0.42043424\n",
      "Iteration 454, loss = 0.32918985\n",
      "Iteration 455, loss = 0.32908602\n",
      "Iteration 1847, loss = 0.19073649\n",
      "Iteration 105, loss = 0.41926363\n",
      "Iteration 1048, loss = 0.25835845\n",
      "Iteration 456, loss = 0.32898409\n",
      "Iteration 106, loss = 0.41812034\n",
      "Iteration 457, loss = 0.32886981\n",
      "Iteration 127, loss = 0.52274480\n",
      "Iteration 458, loss = 0.32875717\n",
      "Iteration 915, loss = 0.31607352\n",
      "Iteration 459, loss = 0.32869072\n",
      "Iteration 1049, loss = 0.25797542\n",
      "Iteration 460, loss = 0.32854959\n",
      "Iteration 461, loss = 0.32844303\n",
      "Iteration 1848, loss = 0.19057935\n",
      "Iteration 462, loss = 0.32833652\n",
      "Iteration 463, loss = 0.32824398\n",
      "Iteration 107, loss = 0.41696778\n",
      "Iteration 1602, loss = 0.22788544\n",
      "Iteration 464, loss = 0.32812514\n",
      "Iteration 108, loss = 0.41589312\n",
      "Iteration 1050, loss = 0.25776884\n",
      "Iteration 1849, loss = 0.19046223\n",
      "Iteration 109, loss = 0.41481986\n",
      "Iteration 465, loss = 0.32804256\n",
      "Iteration 110, loss = 0.41373473\n",
      "Iteration 1063, loss = 0.26213031\n",
      "Iteration 916, loss = 0.31594231\n",
      "Iteration 466, loss = 0.32791879\n",
      "Iteration 1051, loss = 0.25751189\n",
      "Iteration 111, loss = 0.41268374\n",
      "Iteration 128, loss = 0.52124898\n",
      "Iteration 1850, loss = 0.19053621\n",
      "Iteration 112, loss = 0.41167382\n",
      "Iteration 113, loss = 0.41066506\n",
      "Iteration 1603, loss = 0.22770694\n",
      "Iteration 114, loss = 0.40965937\n",
      "Iteration 115, loss = 0.40869851\n",
      "Iteration 1851, loss = 0.19027635\n",
      "Iteration 467, loss = 0.32781640\n",
      "Iteration 468, loss = 0.32773607\n",
      "Iteration 1064, loss = 0.26174308\n",
      "Iteration 469, loss = 0.32760859\n",
      "Iteration 116, loss = 0.40773275\n",
      "Iteration 470, loss = 0.32751257\n",
      "Iteration 1604, loss = 0.22760448\n",
      "Iteration 471, loss = 0.32740720\n",
      "Iteration 1052, loss = 0.25732172\n",
      "Iteration 917, loss = 0.31581823\n",
      "Iteration 472, loss = 0.32730660\n",
      "Iteration 117, loss = 0.40683775Iteration 473, loss = 0.32721012\n",
      "\n",
      "Iteration 474, loss = 0.32710755\n",
      "Iteration 475, loss = 0.32701692\n",
      "Iteration 476, loss = 0.32694537\n",
      "Iteration 477, loss = 0.32681384\n",
      "Iteration 118, loss = 0.40583425\n",
      "Iteration 1852, loss = 0.19009384\n",
      "Iteration 1053, loss = 0.25711006\n",
      "Iteration 129, loss = 0.51981375\n",
      "Iteration 119, loss = 0.40501076\n",
      "Iteration 478, loss = 0.32670111\n",
      "Iteration 1853, loss = 0.19002535\n",
      "Iteration 918, loss = 0.31569013\n",
      "Iteration 479, loss = 0.32660555\n",
      "Iteration 1605, loss = 0.22745340\n",
      "Iteration 120, loss = 0.40409744\n",
      "Iteration 480, loss = 0.32650515\n",
      "Iteration 1065, loss = 0.26153414\n",
      "Iteration 481, loss = 0.32641182\n",
      "Iteration 1854, loss = 0.18986965\n",
      "Iteration 482, loss = 0.32631232\n",
      "Iteration 1054, loss = 0.25695152\n",
      "Iteration 121, loss = 0.40324198\n",
      "Iteration 483, loss = 0.32622477\n",
      "Iteration 1055, loss = 0.25667209\n",
      "Iteration 1606, loss = 0.22729353\n",
      "Iteration 919, loss = 0.31555599\n",
      "Iteration 1855, loss = 0.18975971\n",
      "Iteration 1066, loss = 0.26139071\n",
      "Iteration 1856, loss = 0.18964240\n",
      "Iteration 130, loss = 0.51839502\n",
      "Iteration 1857, loss = 0.18951811\n",
      "Iteration 122, loss = 0.40233044\n",
      "Iteration 920, loss = 0.31545600\n",
      "Iteration 1607, loss = 0.22735761\n",
      "Iteration 484, loss = 0.32610736\n",
      "Iteration 485, loss = 0.32601614\n",
      "Iteration 123, loss = 0.40153856\n",
      "Iteration 1067, loss = 0.26114921\n",
      "Iteration 486, loss = 0.32591588\n",
      "Iteration 124, loss = 0.40073208\n",
      "Iteration 1858, loss = 0.18942479\n",
      "Iteration 1608, loss = 0.22711845\n",
      "Iteration 921, loss = 0.31536757\n",
      "Iteration 1056, loss = 0.25645451\n",
      "Iteration 487, loss = 0.32582334\n",
      "Iteration 125, loss = 0.39992046\n",
      "Iteration 488, loss = 0.32571993\n",
      "Iteration 489, loss = 0.32562012\n",
      "Iteration 131, loss = 0.51700452\n",
      "Iteration 490, loss = 0.32552457\n",
      "Iteration 126, loss = 0.39910429\n",
      "Iteration 491, loss = 0.32541702\n",
      "Iteration 1859, loss = 0.18927699\n",
      "Iteration 1609, loss = 0.22708849\n",
      "Iteration 492, loss = 0.32531967\n",
      "Iteration 127, loss = 0.39830387\n",
      "Iteration 1057, loss = 0.25626354\n",
      "Iteration 1068, loss = 0.26087990\n",
      "Iteration 493, loss = 0.32523186\n",
      "Iteration 494, loss = 0.32513741\n",
      "Iteration 128, loss = 0.39750090\n",
      "Iteration 495, loss = 0.32501868\n",
      "Iteration 129, loss = 0.39680437\n",
      "Iteration 496, loss = 0.32492276\n",
      "Iteration 922, loss = 0.31522343\n",
      "Iteration 497, loss = 0.32482534\n",
      "Iteration 130, loss = 0.39606896\n",
      "Iteration 1610, loss = 0.22691761\n",
      "Iteration 131, loss = 0.39531495\n",
      "Iteration 1860, loss = 0.18916822\n",
      "Iteration 498, loss = 0.32475160\n",
      "Iteration 132, loss = 0.39461574\n",
      "Iteration 133, loss = 0.39390522\n",
      "Iteration 499, loss = 0.32462464\n",
      "Iteration 134, loss = 0.39317926\n",
      "Iteration 1058, loss = 0.25601501\n",
      "Iteration 135, loss = 0.39255528\n",
      "Iteration 500, loss = 0.32453202\n",
      "Iteration 1611, loss = 0.22673483\n",
      "Iteration 136, loss = 0.39183732\n",
      "Iteration 1861, loss = 0.18908399\n",
      "Iteration 501, loss = 0.32446350\n",
      "Iteration 137, loss = 0.39122855\n",
      "Iteration 1069, loss = 0.26081410\n",
      "Iteration 132, loss = 0.51567953\n",
      "Iteration 1059, loss = 0.25578118\n",
      "Iteration 923, loss = 0.31505904\n",
      "Iteration 138, loss = 0.39053830\n",
      "Iteration 502, loss = 0.32434070\n",
      "Iteration 503, loss = 0.32425058\n",
      "Iteration 504, loss = 0.32415011\n",
      "Iteration 1862, loss = 0.18900934\n",
      "Iteration 139, loss = 0.38990685\n",
      "Iteration 505, loss = 0.32408300\n",
      "Iteration 506, loss = 0.32394551\n",
      "Iteration 140, loss = 0.38926857\n",
      "Iteration 1060, loss = 0.25558764\n",
      "Iteration 507, loss = 0.32387472\n",
      "Iteration 1612, loss = 0.22673054\n",
      "Iteration 508, loss = 0.32375529\n",
      "Iteration 509, loss = 0.32368452\n",
      "Iteration 141, loss = 0.38868920\n",
      "Iteration 510, loss = 0.32357661\n",
      "Iteration 924, loss = 0.31495925\n",
      "Iteration 511, loss = 0.32348549\n",
      "Iteration 1863, loss = 0.18882662\n",
      "Iteration 512, loss = 0.32337891\n",
      "Iteration 513, loss = 0.32328809\n",
      "Iteration 514, loss = 0.32320510\n",
      "Iteration 1864, loss = 0.18871155\n",
      "Iteration 1613, loss = 0.22649841\n",
      "Iteration 515, loss = 0.32310056\n",
      "Iteration 516, loss = 0.32301650\n",
      "Iteration 1070, loss = 0.26046003\n",
      "Iteration 517, loss = 0.32293861\n",
      "Iteration 1865, loss = 0.18860459\n",
      "Iteration 518, loss = 0.32281658\n",
      "Iteration 925, loss = 0.31481125\n",
      "Iteration 519, loss = 0.32272515\n",
      "Iteration 142, loss = 0.38804893\n",
      "Iteration 520, loss = 0.32262945\n",
      "Iteration 1866, loss = 0.18849900\n",
      "Iteration 521, loss = 0.32255020\n",
      "Iteration 522, loss = 0.32244004\n",
      "Iteration 1614, loss = 0.22636354\n",
      "Iteration 523, loss = 0.32236467\n",
      "Iteration 524, loss = 0.32226253\n",
      "Iteration 1071, loss = 0.26023966\n",
      "Iteration 525, loss = 0.32217252\n",
      "Iteration 926, loss = 0.31471396\n",
      "Iteration 526, loss = 0.32207657\n",
      "Iteration 1867, loss = 0.18835263\n",
      "Iteration 527, loss = 0.32198486\n",
      "Iteration 143, loss = 0.38742196\n",
      "Iteration 528, loss = 0.32189248\n",
      "Iteration 529, loss = 0.32179826\n",
      "Iteration 1868, loss = 0.18819678\n",
      "Iteration 530, loss = 0.32170709\n",
      "Iteration 1615, loss = 0.22631462\n",
      "Iteration 531, loss = 0.32161217\n",
      "Iteration 532, loss = 0.32157395\n",
      "Iteration 927, loss = 0.31462545\n",
      "Iteration 144, loss = 0.38691209\n",
      "Iteration 533, loss = 0.32143615\n",
      "Iteration 1869, loss = 0.18824921\n",
      "Iteration 534, loss = 0.32133979\n",
      "Iteration 535, loss = 0.32127303\n",
      "Iteration 536, loss = 0.32117726\n",
      "Iteration 145, loss = 0.38627179\n",
      "Iteration 1870, loss = 0.18803027\n",
      "Iteration 133, loss = 0.51427067\n",
      "Iteration 537, loss = 0.32107039\n",
      "Iteration 538, loss = 0.32098142\n",
      "Iteration 146, loss = 0.38566500\n",
      "Iteration 539, loss = 0.32089481\n",
      "Iteration 1616, loss = 0.22609871\n",
      "Iteration 1072, loss = 0.25999861\n",
      "Iteration 1871, loss = 0.18788196\n",
      "Iteration 1061, loss = 0.25542741\n",
      "Iteration 147, loss = 0.38521404\n",
      "Iteration 928, loss = 0.31444279\n",
      "Iteration 540, loss = 0.32081524\n",
      "Iteration 541, loss = 0.32071243\n",
      "Iteration 542, loss = 0.32063388\n",
      "Iteration 148, loss = 0.38461319\n",
      "Iteration 149, loss = 0.38408198\n",
      "Iteration 150, loss = 0.38351046\n",
      "Iteration 543, loss = 0.32053598\n",
      "Iteration 929, loss = 0.31438467\n",
      "Iteration 544, loss = 0.32045880\n",
      "Iteration 134, loss = 0.51305102\n",
      "Iteration 545, loss = 0.32038172\n",
      "Iteration 1872, loss = 0.18796936\n",
      "Iteration 151, loss = 0.38296217\n",
      "Iteration 1617, loss = 0.22602493\n",
      "Iteration 1062, loss = 0.25519509\n",
      "Iteration 546, loss = 0.32028460\n",
      "Iteration 152, loss = 0.38245295\n",
      "Iteration 1618, loss = 0.22591575\n",
      "Iteration 547, loss = 0.32021854\n",
      "Iteration 153, loss = 0.38195302\n",
      "Iteration 930, loss = 0.31422594\n",
      "Iteration 548, loss = 0.32011664\n",
      "Iteration 549, loss = 0.32001303\n",
      "Iteration 1873, loss = 0.18766855\n",
      "Iteration 550, loss = 0.31993772\n",
      "Iteration 135, loss = 0.51178550\n",
      "Iteration 1073, loss = 0.25986153\n",
      "Iteration 154, loss = 0.38142850\n",
      "Iteration 1619, loss = 0.22575619\n",
      "Iteration 551, loss = 0.31984780\n",
      "Iteration 1063, loss = 0.25487765\n",
      "Iteration 1874, loss = 0.18755743\n",
      "Iteration 552, loss = 0.31975606\n",
      "Iteration 553, loss = 0.31966664\n",
      "Iteration 554, loss = 0.31958529\n",
      "Iteration 1875, loss = 0.18747463\n",
      "Iteration 555, loss = 0.31950565\n",
      "Iteration 155, loss = 0.38095291\n",
      "Iteration 1064, loss = 0.25475938\n",
      "Iteration 556, loss = 0.31940965\n",
      "Iteration 1620, loss = 0.22572634\n",
      "Iteration 557, loss = 0.31931998\n",
      "Iteration 1876, loss = 0.18731270\n",
      "Iteration 558, loss = 0.31924011\n",
      "Iteration 559, loss = 0.31915725\n",
      "Iteration 156, loss = 0.38048355\n",
      "Iteration 136, loss = 0.51049827\n",
      "Iteration 1065, loss = 0.25450111\n",
      "Iteration 560, loss = 0.31906908\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1621, loss = 0.22552332\n",
      "Iteration 157, loss = 0.37996409\n",
      "Iteration 1877, loss = 0.18727335\n",
      "Iteration 158, loss = 0.37949509\n",
      "Iteration 159, loss = 0.37902926\n",
      "Iteration 1622, loss = 0.22545768\n",
      "Iteration 160, loss = 0.37855259\n",
      "Iteration 161, loss = 0.37806312\n",
      "Iteration 931, loss = 0.31415728\n",
      "Iteration 1066, loss = 0.25426096\n",
      "Iteration 162, loss = 0.37761268\n",
      "Iteration 1074, loss = 0.25959716\n",
      "Iteration 163, loss = 0.37719250\n",
      "Iteration 1878, loss = 0.18710912\n",
      "Iteration 1, loss = 0.69482592\n",
      "Iteration 137, loss = 0.50929585\n",
      "Iteration 164, loss = 0.37671682\n",
      "Iteration 1623, loss = 0.22532343\n",
      "Iteration 165, loss = 0.37628212\n",
      "Iteration 166, loss = 0.37583519\n",
      "Iteration 2, loss = 0.69326813\n",
      "Iteration 1879, loss = 0.18697582\n",
      "Iteration 167, loss = 0.37542814\n",
      "Iteration 3, loss = 0.69085071\n",
      "Iteration 168, loss = 0.37497805\n",
      "Iteration 4, loss = 0.68794782\n",
      "Iteration 1624, loss = 0.22518077\n",
      "Iteration 5, loss = 0.68460432\n",
      "Iteration 169, loss = 0.37455996\n",
      "Iteration 1880, loss = 0.18691282Iteration 1075, loss = 0.25936057\n",
      "Iteration 6, loss = 0.68095405\n",
      "Iteration 1067, loss = 0.25395836\n",
      "Iteration 170, loss = 0.37417149\n",
      "Iteration 932, loss = 0.31398054\n",
      "Iteration 138, loss = 0.50799088\n",
      "Iteration 7, loss = 0.67714584\n",
      "\n",
      "Iteration 8, loss = 0.67328430\n",
      "Iteration 171, loss = 0.37377660\n",
      "Iteration 9, loss = 0.66943053\n",
      "Iteration 172, loss = 0.37335209\n",
      "Iteration 933, loss = 0.31385426\n",
      "Iteration 1068, loss = 0.25380555\n",
      "Iteration 173, loss = 0.37294853\n",
      "Iteration 139, loss = 0.50687297\n",
      "Iteration 934, loss = 0.31370159\n",
      "Iteration 1076, loss = 0.25935168\n",
      "Iteration 174, loss = 0.37257612\n",
      "Iteration 1069, loss = 0.25359547\n",
      "Iteration 175, loss = 0.37222678\n",
      "Iteration 1070, loss = 0.25332764\n",
      "Iteration 10, loss = 0.66551491Iteration 935, loss = 0.31358900\n",
      "\n",
      "Iteration 140, loss = 0.50573926\n",
      "Iteration 176, loss = 0.37177863\n",
      "Iteration 1881, loss = 0.18672742\n",
      "Iteration 177, loss = 0.37140876\n",
      "Iteration 178, loss = 0.37106651\n",
      "Iteration 11, loss = 0.66152712\n",
      "Iteration 936, loss = 0.31346848\n",
      "Iteration 179, loss = 0.37065246\n",
      "Iteration 1625, loss = 0.22503526Iteration 180, loss = 0.37031194\n",
      "\n",
      "Iteration 141, loss = 0.50455500\n",
      "Iteration 1071, loss = 0.25309020\n",
      "Iteration 181, loss = 0.36993251\n",
      "Iteration 1882, loss = 0.18659959\n",
      "Iteration 1077, loss = 0.25893065\n",
      "Iteration 182, loss = 0.36960903\n",
      "Iteration 12, loss = 0.65754113\n",
      "Iteration 183, loss = 0.36925094\n",
      "Iteration 184, loss = 0.36888819\n",
      "Iteration 1883, loss = 0.18655948\n",
      "Iteration 185, loss = 0.36853742\n",
      "Iteration 13, loss = 0.65364913\n",
      "Iteration 186, loss = 0.36821349\n",
      "Iteration 187, loss = 0.36787453\n",
      "Iteration 937, loss = 0.31335165\n",
      "Iteration 14, loss = 0.64977289\n",
      "Iteration 188, loss = 0.36753307\n",
      "Iteration 189, loss = 0.36722350\n",
      "Iteration 15, loss = 0.64605293\n",
      "Iteration 16, loss = 0.64235518\n",
      "Iteration 1072, loss = 0.25329804\n",
      "Iteration 190, loss = 0.36689098\n",
      "Iteration 142, loss = 0.50340711\n",
      "Iteration 1884, loss = 0.18636514\n",
      "Iteration 191, loss = 0.36656566\n",
      "Iteration 192, loss = 0.36624317\n",
      "Iteration 193, loss = 0.36593898\n",
      "Iteration 1078, loss = 0.25876608\n",
      "Iteration 938, loss = 0.31321001\n",
      "Iteration 194, loss = 0.36561189\n",
      "Iteration 195, loss = 0.36531483\n",
      "Iteration 1885, loss = 0.18624028\n",
      "Iteration 196, loss = 0.36501445\n",
      "Iteration 197, loss = 0.36469759\n",
      "Iteration 939, loss = 0.31309298\n",
      "Iteration 198, loss = 0.36441224\n",
      "Iteration 17, loss = 0.63863811\n",
      "Iteration 1073, loss = 0.25272910\n",
      "Iteration 18, loss = 0.63495618\n",
      "Iteration 199, loss = 0.36411823\n",
      "Iteration 940, loss = 0.31295242\n",
      "Iteration 19, loss = 0.63128104\n",
      "Iteration 20, loss = 0.62776341\n",
      "Iteration 1886, loss = 0.18616108\n",
      "Iteration 21, loss = 0.62427346\n",
      "Iteration 200, loss = 0.36383735\n",
      "Iteration 1626, loss = 0.22492725\n",
      "Iteration 1079, loss = 0.25855129\n",
      "Iteration 22, loss = 0.62071206\n",
      "Iteration 143, loss = 0.50238192\n",
      "Iteration 1074, loss = 0.25254399\n",
      "Iteration 201, loss = 0.36352009\n",
      "Iteration 202, loss = 0.36324273\n",
      "Iteration 203, loss = 0.36295984\n",
      "Iteration 1887, loss = 0.18604538\n",
      "Iteration 204, loss = 0.36266086\n",
      "Iteration 23, loss = 0.61737661\n",
      "Iteration 205, loss = 0.36238979\n",
      "Iteration 206, loss = 0.36212712\n",
      "Iteration 207, loss = 0.36184352\n",
      "Iteration 24, loss = 0.61400140\n",
      "Iteration 208, loss = 0.36155204\n",
      "Iteration 209, loss = 0.36129476\n",
      "Iteration 25, loss = 0.61062076\n",
      "Iteration 1075, loss = 0.25231796\n",
      "Iteration 144, loss = 0.50128792\n",
      "Iteration 1888, loss = 0.18599752\n",
      "Iteration 941, loss = 0.31284082\n",
      "Iteration 1627, loss = 0.22488896\n",
      "Iteration 1080, loss = 0.25850680\n",
      "Iteration 1889, loss = 0.18591120\n",
      "Iteration 210, loss = 0.36102723\n",
      "Iteration 1628, loss = 0.22465928\n",
      "Iteration 26, loss = 0.60731166\n",
      "Iteration 1890, loss = 0.18573498\n",
      "Iteration 1076, loss = 0.25192968\n",
      "Iteration 211, loss = 0.36074550\n",
      "Iteration 27, loss = 0.60402586\n",
      "Iteration 1891, loss = 0.18558285\n",
      "Iteration 145, loss = 0.50017716\n",
      "Iteration 28, loss = 0.60083062\n",
      "Iteration 29, loss = 0.59756310\n",
      "Iteration 212, loss = 0.36049308\n",
      "Iteration 1081, loss = 0.25811174\n",
      "Iteration 1892, loss = 0.18549835\n",
      "Iteration 1629, loss = 0.22502107\n",
      "Iteration 30, loss = 0.59443988\n",
      "Iteration 1077, loss = 0.25172356\n",
      "Iteration 31, loss = 0.59126782\n",
      "Iteration 32, loss = 0.58824582\n",
      "Iteration 1893, loss = 0.18536963\n",
      "Iteration 213, loss = 0.36024311\n",
      "Iteration 214, loss = 0.35999031\n",
      "Iteration 215, loss = 0.35974599\n",
      "Iteration 1630, loss = 0.22444051\n",
      "Iteration 216, loss = 0.35947787\n",
      "Iteration 146, loss = 0.49908114\n",
      "Iteration 942, loss = 0.31271701\n",
      "Iteration 217, loss = 0.35921623\n",
      "Iteration 218, loss = 0.35898141\n",
      "Iteration 33, loss = 0.58519702\n",
      "Iteration 219, loss = 0.35875113\n",
      "Iteration 1631, loss = 0.22433194\n",
      "Iteration 220, loss = 0.35846597\n",
      "Iteration 943, loss = 0.31268072\n",
      "Iteration 221, loss = 0.35824210\n",
      "Iteration 147, loss = 0.49801538\n",
      "Iteration 1632, loss = 0.22427746\n",
      "Iteration 222, loss = 0.35801128\n",
      "Iteration 223, loss = 0.35775305\n",
      "Iteration 944, loss = 0.31249343\n",
      "Iteration 34, loss = 0.58214911\n",
      "Iteration 1894, loss = 0.18525029\n",
      "Iteration 224, loss = 0.35752659\n",
      "Iteration 1633, loss = 0.22408603\n",
      "Iteration 148, loss = 0.49704300\n",
      "Iteration 35, loss = 0.57916960\n",
      "Iteration 225, loss = 0.35727788\n",
      "Iteration 226, loss = 0.35704055\n",
      "Iteration 1895, loss = 0.18515410\n",
      "Iteration 1078, loss = 0.25148216\n",
      "Iteration 1082, loss = 0.25790095\n",
      "Iteration 1634, loss = 0.22409175\n",
      "Iteration 36, loss = 0.57618206\n",
      "Iteration 227, loss = 0.35682497\n",
      "Iteration 945, loss = 0.31242798\n",
      "Iteration 228, loss = 0.35658426\n",
      "Iteration 1079, loss = 0.25125080\n",
      "Iteration 37, loss = 0.57327465\n",
      "Iteration 1896, loss = 0.18505974\n",
      "Iteration 1635, loss = 0.22388130\n",
      "Iteration 229, loss = 0.35637509\n",
      "Iteration 1080, loss = 0.25108613\n",
      "Iteration 946, loss = 0.31227082\n",
      "Iteration 38, loss = 0.57044802\n",
      "Iteration 1897, loss = 0.18491725\n",
      "Iteration 230, loss = 0.35613217\n",
      "Iteration 1083, loss = 0.25776107\n",
      "Iteration 1081, loss = 0.25116645\n",
      "Iteration 39, loss = 0.56756601\n",
      "Iteration 231, loss = 0.35590595\n",
      "Iteration 1898, loss = 0.18474556\n",
      "Iteration 232, loss = 0.35568023\n",
      "Iteration 947, loss = 0.31216680\n",
      "Iteration 233, loss = 0.35547017\n",
      "Iteration 1899, loss = 0.18470026\n",
      "Iteration 40, loss = 0.56465049\n",
      "Iteration 234, loss = 0.35526734\n",
      "Iteration 1082, loss = 0.25066274\n",
      "Iteration 235, loss = 0.35503484\n",
      "Iteration 236, loss = 0.35481431Iteration 1900, loss = 0.18464939\n",
      "\n",
      "Iteration 41, loss = 0.56187342\n",
      "Iteration 237, loss = 0.35459030\n",
      "Iteration 149, loss = 0.49608779\n",
      "Iteration 1083, loss = 0.25065043\n",
      "Iteration 1901, loss = 0.18442872\n",
      "Iteration 1636, loss = 0.22374289\n",
      "Iteration 238, loss = 0.35438781\n",
      "Iteration 42, loss = 0.55911842\n",
      "Iteration 239, loss = 0.35418842\n",
      "Iteration 1902, loss = 0.18435546\n",
      "Iteration 1084, loss = 0.25022398\n",
      "Iteration 240, loss = 0.35396649\n",
      "Iteration 948, loss = 0.31197712\n",
      "Iteration 241, loss = 0.35379695\n",
      "Iteration 43, loss = 0.55628390\n",
      "Iteration 1903, loss = 0.18419051\n",
      "Iteration 242, loss = 0.35357362\n",
      "Iteration 1084, loss = 0.25741802\n",
      "Iteration 243, loss = 0.35338067\n",
      "Iteration 244, loss = 0.35314438\n",
      "Iteration 1904, loss = 0.18419200\n",
      "Iteration 44, loss = 0.55358892\n",
      "Iteration 949, loss = 0.31198790\n",
      "Iteration 245, loss = 0.35293473\n",
      "Iteration 1637, loss = 0.22373734\n",
      "Iteration 45, loss = 0.55088656\n",
      "Iteration 246, loss = 0.35275260\n",
      "Iteration 1085, loss = 0.24996503\n",
      "Iteration 247, loss = 0.35255080\n",
      "Iteration 46, loss = 0.54831192\n",
      "Iteration 1905, loss = 0.18421161\n",
      "Iteration 248, loss = 0.35236657\n",
      "Iteration 47, loss = 0.54570888\n",
      "Iteration 249, loss = 0.35217858\n",
      "Iteration 1085, loss = 0.25731201\n",
      "Iteration 48, loss = 0.54309305\n",
      "Iteration 1638, loss = 0.22348542\n",
      "Iteration 150, loss = 0.49504750\n",
      "Iteration 250, loss = 0.35197713\n",
      "Iteration 49, loss = 0.54046291\n",
      "Iteration 50, loss = 0.53795014\n",
      "Iteration 950, loss = 0.31174232\n",
      "Iteration 251, loss = 0.35178494\n",
      "Iteration 1906, loss = 0.18392396\n",
      "Iteration 51, loss = 0.53536459\n",
      "Iteration 252, loss = 0.35159089\n",
      "Iteration 52, loss = 0.53299936\n",
      "Iteration 253, loss = 0.35140371\n",
      "Iteration 1086, loss = 0.24983525\n",
      "Iteration 1639, loss = 0.22355676\n",
      "Iteration 53, loss = 0.53049142\n",
      "Iteration 1907, loss = 0.18385389\n",
      "Iteration 254, loss = 0.35121900\n",
      "Iteration 54, loss = 0.52808786\n",
      "Iteration 55, loss = 0.52568410\n",
      "Iteration 255, loss = 0.35101742\n",
      "Iteration 1640, loss = 0.22329024\n",
      "Iteration 56, loss = 0.52341868\n",
      "Iteration 57, loss = 0.52106473\n",
      "Iteration 1908, loss = 0.18366651\n",
      "Iteration 1086, loss = 0.25704795\n",
      "Iteration 1087, loss = 0.24950726\n",
      "Iteration 58, loss = 0.51873096\n",
      "Iteration 1641, loss = 0.22313883\n",
      "Iteration 1909, loss = 0.18356378\n",
      "Iteration 59, loss = 0.51650837\n",
      "Iteration 1088, loss = 0.24940674\n",
      "Iteration 60, loss = 0.51423221\n",
      "Iteration 1910, loss = 0.18347776\n",
      "Iteration 951, loss = 0.31162342\n",
      "Iteration 1087, loss = 0.25678515\n",
      "Iteration 256, loss = 0.35083183\n",
      "Iteration 61, loss = 0.51206181\n",
      "Iteration 1911, loss = 0.18335602\n",
      "Iteration 151, loss = 0.49400914\n",
      "Iteration 1089, loss = 0.24925668\n",
      "Iteration 257, loss = 0.35065635\n",
      "Iteration 62, loss = 0.50991844\n",
      "Iteration 1642, loss = 0.22300106\n",
      "Iteration 258, loss = 0.35047193\n",
      "Iteration 259, loss = 0.35029149\n",
      "Iteration 1090, loss = 0.24889641\n",
      "Iteration 1088, loss = 0.25658450\n",
      "Iteration 63, loss = 0.50777840\n",
      "Iteration 260, loss = 0.35012321\n",
      "Iteration 1912, loss = 0.18324081\n",
      "Iteration 952, loss = 0.31152230\n",
      "Iteration 261, loss = 0.34993953\n",
      "Iteration 262, loss = 0.34975815\n",
      "Iteration 64, loss = 0.50566917\n",
      "Iteration 263, loss = 0.34958639\n",
      "Iteration 1091, loss = 0.24866899\n",
      "Iteration 1913, loss = 0.18312637\n",
      "Iteration 1089, loss = 0.25631585\n",
      "Iteration 152, loss = 0.49309533\n",
      "Iteration 264, loss = 0.34943533\n",
      "Iteration 1643, loss = 0.22290008\n",
      "Iteration 1092, loss = 0.24844684\n",
      "Iteration 953, loss = 0.31142669\n",
      "Iteration 65, loss = 0.50365731\n",
      "Iteration 1914, loss = 0.18301711\n",
      "Iteration 66, loss = 0.50154940\n",
      "Iteration 265, loss = 0.34924727\n",
      "Iteration 67, loss = 0.49954014\n",
      "Iteration 266, loss = 0.34907333\n",
      "Iteration 1093, loss = 0.24847679\n",
      "Iteration 267, loss = 0.34891298\n",
      "Iteration 954, loss = 0.31126548\n",
      "Iteration 1090, loss = 0.25620803\n",
      "Iteration 68, loss = 0.49757168\n",
      "Iteration 268, loss = 0.34873957\n",
      "Iteration 153, loss = 0.49217082\n",
      "Iteration 269, loss = 0.34855214\n",
      "Iteration 1915, loss = 0.18288785\n",
      "Iteration 1094, loss = 0.24799006\n",
      "Iteration 69, loss = 0.49555208\n",
      "Iteration 270, loss = 0.34837910\n",
      "Iteration 1916, loss = 0.18284731\n",
      "Iteration 271, loss = 0.34822358\n",
      "Iteration 1095, loss = 0.24778515\n",
      "Iteration 272, loss = 0.34803566\n",
      "Iteration 70, loss = 0.49367294\n",
      "Iteration 273, loss = 0.34787892\n",
      "Iteration 1917, loss = 0.18265071\n",
      "Iteration 1644, loss = 0.22281781\n",
      "Iteration 274, loss = 0.34775773\n",
      "Iteration 71, loss = 0.49179169\n",
      "Iteration 1096, loss = 0.24764545\n",
      "Iteration 275, loss = 0.34756364\n",
      "Iteration 1918, loss = 0.18259310\n",
      "Iteration 72, loss = 0.48984372\n",
      "Iteration 955, loss = 0.31118479\n",
      "Iteration 276, loss = 0.34738835\n",
      "Iteration 73, loss = 0.48797789\n",
      "Iteration 154, loss = 0.49117612\n",
      "Iteration 1097, loss = 0.24730994\n",
      "Iteration 1919, loss = 0.18256570\n",
      "Iteration 74, loss = 0.48620523\n",
      "Iteration 277, loss = 0.34723060\n",
      "Iteration 75, loss = 0.48435307\n",
      "Iteration 278, loss = 0.34715392\n",
      "Iteration 1091, loss = 0.25602111\n",
      "Iteration 1920, loss = 0.18238516\n",
      "Iteration 76, loss = 0.48263828\n",
      "Iteration 956, loss = 0.31105896\n",
      "Iteration 1645, loss = 0.22268557\n",
      "Iteration 279, loss = 0.34692584\n",
      "Iteration 77, loss = 0.48093983\n",
      "Iteration 1098, loss = 0.24737075\n",
      "Iteration 280, loss = 0.34676543\n",
      "Iteration 155, loss = 0.49020999\n",
      "Iteration 1921, loss = 0.18219377\n",
      "Iteration 78, loss = 0.47911348\n",
      "Iteration 281, loss = 0.34659565\n",
      "Iteration 1646, loss = 0.22258937\n",
      "Iteration 957, loss = 0.31091803\n",
      "Iteration 1092, loss = 0.25578772\n",
      "Iteration 282, loss = 0.34646901\n",
      "Iteration 1922, loss = 0.18209780\n",
      "Iteration 79, loss = 0.47743714\n",
      "Iteration 283, loss = 0.34630722\n",
      "Iteration 1099, loss = 0.24689696\n",
      "Iteration 284, loss = 0.34618578\n",
      "Iteration 156, loss = 0.48943097\n",
      "Iteration 1647, loss = 0.22252697\n",
      "Iteration 1923, loss = 0.18198454\n",
      "Iteration 285, loss = 0.34600547\n",
      "Iteration 286, loss = 0.34582283\n",
      "Iteration 80, loss = 0.47580043\n",
      "Iteration 287, loss = 0.34567897\n",
      "Iteration 81, loss = 0.47416805\n",
      "Iteration 1093, loss = 0.25547783\n",
      "Iteration 1924, loss = 0.18188863\n",
      "Iteration 288, loss = 0.34551462\n",
      "Iteration 82, loss = 0.47250149\n",
      "Iteration 1100, loss = 0.24663357\n",
      "Iteration 958, loss = 0.31078055\n",
      "Iteration 1648, loss = 0.22231302\n",
      "Iteration 1925, loss = 0.18178218\n",
      "Iteration 83, loss = 0.47094258\n",
      "Iteration 289, loss = 0.34538195\n",
      "Iteration 84, loss = 0.46941036\n",
      "Iteration 290, loss = 0.34524632\n",
      "Iteration 157, loss = 0.48844425\n",
      "Iteration 291, loss = 0.34508618\n",
      "Iteration 85, loss = 0.46782341\n",
      "Iteration 292, loss = 0.34492614\n",
      "Iteration 86, loss = 0.46634374\n",
      "Iteration 1101, loss = 0.24641810\n",
      "Iteration 293, loss = 0.34478526\n",
      "Iteration 1926, loss = 0.18166504\n",
      "Iteration 1094, loss = 0.25539112\n",
      "Iteration 87, loss = 0.46484459\n",
      "Iteration 294, loss = 0.34465071\n",
      "Iteration 1649, loss = 0.22218827\n",
      "Iteration 295, loss = 0.34449697\n",
      "Iteration 959, loss = 0.31069576\n",
      "Iteration 296, loss = 0.34434948\n",
      "Iteration 88, loss = 0.46339272\n",
      "Iteration 297, loss = 0.34422514\n",
      "Iteration 1927, loss = 0.18167888\n",
      "Iteration 298, loss = 0.34406957\n",
      "Iteration 299, loss = 0.34393854\n",
      "Iteration 1102, loss = 0.24620850\n",
      "Iteration 158, loss = 0.48757581\n",
      "Iteration 960, loss = 0.31052698\n",
      "Iteration 300, loss = 0.34381671\n",
      "Iteration 1650, loss = 0.22208045\n",
      "Iteration 89, loss = 0.46194481\n",
      "Iteration 301, loss = 0.34365952\n",
      "Iteration 1095, loss = 0.25513156\n",
      "Iteration 302, loss = 0.34350969\n",
      "Iteration 303, loss = 0.34338714\n",
      "Iteration 90, loss = 0.46057314\n",
      "Iteration 1103, loss = 0.24599207\n",
      "Iteration 304, loss = 0.34325050\n",
      "Iteration 1928, loss = 0.18155245\n",
      "Iteration 305, loss = 0.34312068\n",
      "Iteration 91, loss = 0.45911832\n",
      "Iteration 306, loss = 0.34296835\n",
      "Iteration 1651, loss = 0.22194061\n",
      "Iteration 92, loss = 0.45774366\n",
      "Iteration 307, loss = 0.34282966\n",
      "Iteration 1929, loss = 0.18130447\n",
      "Iteration 93, loss = 0.45642767\n",
      "Iteration 159, loss = 0.48672767\n",
      "Iteration 308, loss = 0.34270242\n",
      "Iteration 1930, loss = 0.18121397\n",
      "Iteration 309, loss = 0.34256825\n",
      "Iteration 94, loss = 0.45504057\n",
      "Iteration 961, loss = 0.31039348\n",
      "Iteration 1104, loss = 0.24591407\n",
      "Iteration 310, loss = 0.34242943\n",
      "Iteration 311, loss = 0.34234272\n",
      "Iteration 1652, loss = 0.22182528\n",
      "Iteration 95, loss = 0.45374097\n",
      "Iteration 1096, loss = 0.25486825\n",
      "Iteration 1931, loss = 0.18118092\n",
      "Iteration 312, loss = 0.34217879\n",
      "Iteration 1932, loss = 0.18095524\n",
      "Iteration 313, loss = 0.34206350\n",
      "Iteration 962, loss = 0.31028594\n",
      "Iteration 96, loss = 0.45250473\n",
      "Iteration 1653, loss = 0.22171387\n",
      "Iteration 1105, loss = 0.24560996\n",
      "Iteration 1933, loss = 0.18089333\n",
      "Iteration 314, loss = 0.34192834\n",
      "Iteration 97, loss = 0.45117420\n",
      "Iteration 1097, loss = 0.25474575\n",
      "Iteration 1934, loss = 0.18080854\n",
      "Iteration 963, loss = 0.31020397\n",
      "Iteration 315, loss = 0.34177538\n",
      "Iteration 316, loss = 0.34164930\n",
      "Iteration 1935, loss = 0.18061386\n",
      "Iteration 317, loss = 0.34154592\n",
      "Iteration 318, loss = 0.34142801\n",
      "Iteration 1654, loss = 0.22161952\n",
      "Iteration 1106, loss = 0.24530522\n",
      "Iteration 1936, loss = 0.18061270\n",
      "Iteration 319, loss = 0.34128459\n",
      "Iteration 320, loss = 0.34113829\n",
      "Iteration 160, loss = 0.48583482\n",
      "Iteration 98, loss = 0.44990661\n",
      "Iteration 321, loss = 0.34101608\n",
      "Iteration 1098, loss = 0.25446336\n",
      "Iteration 99, loss = 0.44866312\n",
      "Iteration 322, loss = 0.34090337\n",
      "Iteration 964, loss = 0.31002395\n",
      "Iteration 100, loss = 0.44742413\n",
      "Iteration 1937, loss = 0.18045661\n",
      "Iteration 1107, loss = 0.24512644\n",
      "Iteration 101, loss = 0.44624261\n",
      "Iteration 323, loss = 0.34077705\n",
      "Iteration 102, loss = 0.44507352\n",
      "Iteration 1108, loss = 0.24489063\n",
      "Iteration 324, loss = 0.34065260\n",
      "Iteration 1099, loss = 0.25440976\n",
      "Iteration 103, loss = 0.44390070\n",
      "Iteration 1938, loss = 0.18047298\n",
      "Iteration 1655, loss = 0.22149214\n",
      "Iteration 325, loss = 0.34052513\n",
      "Iteration 326, loss = 0.34039743\n",
      "Iteration 161, loss = 0.48500593\n",
      "Iteration 104, loss = 0.44273556\n",
      "Iteration 327, loss = 0.34028177\n",
      "Iteration 328, loss = 0.34022082\n",
      "Iteration 329, loss = 0.34002970\n",
      "Iteration 965, loss = 0.30991698\n",
      "Iteration 1656, loss = 0.22142190\n",
      "Iteration 1939, loss = 0.18023300\n",
      "Iteration 105, loss = 0.44157883\n",
      "Iteration 1100, loss = 0.25405918\n",
      "Iteration 330, loss = 0.33992079\n",
      "Iteration 1940, loss = 0.18010931\n",
      "Iteration 331, loss = 0.33980107\n",
      "Iteration 1109, loss = 0.24471670\n",
      "Iteration 106, loss = 0.44046890\n",
      "Iteration 1657, loss = 0.22124980\n",
      "Iteration 162, loss = 0.48419626\n",
      "Iteration 332, loss = 0.33969264\n",
      "Iteration 107, loss = 0.43934932\n",
      "Iteration 108, loss = 0.43826354\n",
      "Iteration 333, loss = 0.33955558\n",
      "Iteration 1941, loss = 0.17998494\n",
      "Iteration 1101, loss = 0.25372912\n",
      "Iteration 109, loss = 0.43720633\n",
      "Iteration 334, loss = 0.33945025\n",
      "Iteration 1658, loss = 0.22121704\n",
      "Iteration 966, loss = 0.30981113\n",
      "Iteration 1110, loss = 0.24460039\n",
      "Iteration 163, loss = 0.48339362\n",
      "Iteration 335, loss = 0.33932378\n",
      "Iteration 110, loss = 0.43613958\n",
      "Iteration 336, loss = 0.33922146\n",
      "Iteration 1942, loss = 0.17985910\n",
      "Iteration 1659, loss = 0.22102322\n",
      "Iteration 337, loss = 0.33910532\n",
      "Iteration 111, loss = 0.43508610\n",
      "Iteration 338, loss = 0.33900529Iteration 112, loss = 0.43407786\n",
      "\n",
      "Iteration 339, loss = 0.33886095\n",
      "Iteration 1111, loss = 0.24431868\n",
      "Iteration 340, loss = 0.33877116\n",
      "Iteration 113, loss = 0.43306562\n",
      "Iteration 1102, loss = 0.25356237\n",
      "Iteration 341, loss = 0.33866055\n",
      "Iteration 342, loss = 0.33852448\n",
      "Iteration 114, loss = 0.43205060\n",
      "Iteration 1943, loss = 0.17979126\n",
      "Iteration 343, loss = 0.33842694\n",
      "Iteration 1112, loss = 0.24404533\n",
      "Iteration 344, loss = 0.33832532\n",
      "Iteration 164, loss = 0.48251193\n",
      "Iteration 967, loss = 0.30977857\n",
      "Iteration 1944, loss = 0.17967963\n",
      "Iteration 115, loss = 0.43104628\n",
      "Iteration 1660, loss = 0.22097059\n",
      "Iteration 1945, loss = 0.17954184\n",
      "Iteration 116, loss = 0.43005095\n",
      "Iteration 1113, loss = 0.24382364\n",
      "Iteration 345, loss = 0.33821423\n",
      "Iteration 1103, loss = 0.25340523\n",
      "Iteration 1946, loss = 0.17938928\n",
      "Iteration 346, loss = 0.33808938\n",
      "Iteration 117, loss = 0.42904088\n",
      "Iteration 1661, loss = 0.22079755\n",
      "Iteration 347, loss = 0.33797137\n",
      "Iteration 968, loss = 0.30955373\n",
      "Iteration 118, loss = 0.42816911\n",
      "Iteration 165, loss = 0.48169131\n",
      "Iteration 348, loss = 0.33785076\n",
      "Iteration 119, loss = 0.42723846\n",
      "Iteration 349, loss = 0.33774346\n",
      "Iteration 1114, loss = 0.24357344\n",
      "Iteration 1104, loss = 0.25311046\n",
      "Iteration 350, loss = 0.33763778\n",
      "Iteration 1947, loss = 0.17948636\n",
      "Iteration 120, loss = 0.42628360\n",
      "Iteration 351, loss = 0.33753794\n",
      "Iteration 121, loss = 0.42538220\n",
      "Iteration 969, loss = 0.30947632\n",
      "Iteration 1948, loss = 0.17921140\n",
      "Iteration 1662, loss = 0.22064628\n",
      "Iteration 352, loss = 0.33744106\n",
      "Iteration 353, loss = 0.33732535Iteration 1115, loss = 0.24344128\n",
      "\n",
      "Iteration 1949, loss = 0.17912069\n",
      "Iteration 354, loss = 0.33720837\n",
      "Iteration 122, loss = 0.42446528\n",
      "Iteration 1950, loss = 0.17896112\n",
      "Iteration 355, loss = 0.33711167\n",
      "Iteration 356, loss = 0.33700735\n",
      "Iteration 123, loss = 0.42356543\n",
      "Iteration 357, loss = 0.33688779\n",
      "Iteration 1951, loss = 0.17895489\n",
      "Iteration 358, loss = 0.33678670\n",
      "Iteration 1105, loss = 0.25287622\n",
      "Iteration 124, loss = 0.42275308\n",
      "Iteration 359, loss = 0.33667902\n",
      "Iteration 125, loss = 0.42182479\n",
      "Iteration 360, loss = 0.33657348\n",
      "Iteration 1952, loss = 0.17875070\n",
      "Iteration 126, loss = 0.42098117\n",
      "Iteration 361, loss = 0.33648321\n",
      "Iteration 127, loss = 0.42016847\n",
      "Iteration 1953, loss = 0.17867452\n",
      "Iteration 166, loss = 0.48099091\n",
      "Iteration 362, loss = 0.33636518\n",
      "Iteration 363, loss = 0.33627792\n",
      "Iteration 128, loss = 0.41931793\n",
      "Iteration 1106, loss = 0.25267095\n",
      "Iteration 1954, loss = 0.17856993\n",
      "Iteration 970, loss = 0.30930346\n",
      "Iteration 364, loss = 0.33617198\n",
      "Iteration 365, loss = 0.33605853\n",
      "Iteration 129, loss = 0.41850294\n",
      "Iteration 366, loss = 0.33595232\n",
      "Iteration 1663, loss = 0.22057567\n",
      "Iteration 1955, loss = 0.17841692\n",
      "Iteration 367, loss = 0.33585884\n",
      "Iteration 130, loss = 0.41771619\n",
      "Iteration 971, loss = 0.30917966\n",
      "Iteration 1116, loss = 0.24317337\n",
      "Iteration 368, loss = 0.33582102\n",
      "Iteration 369, loss = 0.33566000\n",
      "Iteration 1664, loss = 0.22042744\n",
      "Iteration 370, loss = 0.33557773\n",
      "Iteration 131, loss = 0.41689522\n",
      "Iteration 167, loss = 0.48025245\n",
      "Iteration 371, loss = 0.33545194\n",
      "Iteration 1956, loss = 0.17833083\n",
      "Iteration 372, loss = 0.33536638\n",
      "Iteration 1117, loss = 0.24289767\n",
      "Iteration 373, loss = 0.33527298\n",
      "Iteration 1107, loss = 0.25270957\n",
      "Iteration 132, loss = 0.41610705\n",
      "Iteration 972, loss = 0.30911833\n",
      "Iteration 374, loss = 0.33515852\n",
      "Iteration 375, loss = 0.33507451\n",
      "Iteration 133, loss = 0.41532412\n",
      "Iteration 376, loss = 0.33498701\n",
      "Iteration 377, loss = 0.33486951\n",
      "Iteration 1118, loss = 0.24275993\n",
      "Iteration 378, loss = 0.33477194\n",
      "Iteration 1957, loss = 0.17822305\n",
      "Iteration 379, loss = 0.33466594\n",
      "Iteration 1665, loss = 0.22035133\n",
      "Iteration 134, loss = 0.41457046\n",
      "Iteration 380, loss = 0.33457236\n",
      "Iteration 381, loss = 0.33447301\n",
      "Iteration 973, loss = 0.30896302\n",
      "Iteration 1119, loss = 0.24256147\n",
      "Iteration 382, loss = 0.33437899\n",
      "Iteration 383, loss = 0.33428490\n",
      "Iteration 384, loss = 0.33421785\n",
      "Iteration 385, loss = 0.33408947\n",
      "Iteration 1120, loss = 0.24233019\n",
      "Iteration 135, loss = 0.41379017\n",
      "Iteration 168, loss = 0.47940285\n",
      "Iteration 386, loss = 0.33399259\n",
      "Iteration 974, loss = 0.30878754\n",
      "Iteration 387, loss = 0.33389695\n",
      "Iteration 1108, loss = 0.25228445\n",
      "Iteration 136, loss = 0.41307642\n",
      "Iteration 388, loss = 0.33380801\n",
      "Iteration 1121, loss = 0.24213516\n",
      "Iteration 1958, loss = 0.17812793\n",
      "Iteration 1666, loss = 0.22019708\n",
      "Iteration 389, loss = 0.33372052\n",
      "Iteration 390, loss = 0.33362885\n",
      "Iteration 391, loss = 0.33351286\n",
      "Iteration 392, loss = 0.33347169\n",
      "Iteration 1122, loss = 0.24209389\n",
      "Iteration 393, loss = 0.33333943\n",
      "Iteration 1667, loss = 0.22010543\n",
      "Iteration 975, loss = 0.30872670\n",
      "Iteration 394, loss = 0.33323937\n",
      "Iteration 137, loss = 0.41229245\n",
      "Iteration 395, loss = 0.33314361\n",
      "Iteration 396, loss = 0.33305442\n",
      "Iteration 138, loss = 0.41160055\n",
      "Iteration 1959, loss = 0.17800368\n",
      "Iteration 397, loss = 0.33295858\n",
      "Iteration 398, loss = 0.33287756\n",
      "Iteration 139, loss = 0.41089704\n",
      "Iteration 169, loss = 0.47864133\n",
      "Iteration 399, loss = 0.33279172\n",
      "Iteration 400, loss = 0.33269946\n",
      "Iteration 1123, loss = 0.24159026Iteration 140, loss = 0.41013923\n",
      "Iteration 1668, loss = 0.21997400\n",
      "Iteration 401, loss = 0.33259254\n",
      "Iteration 402, loss = 0.33251525\n",
      "\n",
      "Iteration 1960, loss = 0.17793042\n",
      "Iteration 403, loss = 0.33242286\n",
      "Iteration 976, loss = 0.30854580\n",
      "Iteration 141, loss = 0.40944768\n",
      "Iteration 404, loss = 0.33232864\n",
      "Iteration 1109, loss = 0.25216892\n",
      "Iteration 405, loss = 0.33225633\n",
      "Iteration 142, loss = 0.40872915\n",
      "Iteration 406, loss = 0.33215892\n",
      "Iteration 170, loss = 0.47795535\n",
      "Iteration 1961, loss = 0.17774842\n",
      "Iteration 143, loss = 0.40805001\n",
      "Iteration 407, loss = 0.33206855\n",
      "Iteration 144, loss = 0.40737237\n",
      "Iteration 408, loss = 0.33197002\n",
      "Iteration 145, loss = 0.40673283\n",
      "Iteration 1962, loss = 0.17782130\n",
      "Iteration 409, loss = 0.33187912\n",
      "Iteration 146, loss = 0.40602895\n",
      "Iteration 1124, loss = 0.24143357\n",
      "Iteration 1963, loss = 0.17754035\n",
      "Iteration 410, loss = 0.33179859\n",
      "Iteration 1669, loss = 0.21984763\n",
      "Iteration 147, loss = 0.40539468\n",
      "Iteration 411, loss = 0.33171596\n",
      "Iteration 1964, loss = 0.17742985\n",
      "Iteration 1110, loss = 0.25187493\n",
      "Iteration 977, loss = 0.30847880\n",
      "Iteration 412, loss = 0.33161738\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1125, loss = 0.24123293\n",
      "Iteration 1670, loss = 0.21976667\n",
      "Iteration 1965, loss = 0.17732903\n",
      "Iteration 171, loss = 0.47726674\n",
      "Iteration 148, loss = 0.40469825\n",
      "Iteration 1, loss = 0.74976576\n",
      "Iteration 1966, loss = 0.17728297\n",
      "Iteration 978, loss = 0.30832516\n",
      "Iteration 149, loss = 0.40410321\n",
      "Iteration 2, loss = 0.74636295\n",
      "Iteration 1126, loss = 0.24099001\n",
      "Iteration 150, loss = 0.40344982\n",
      "Iteration 3, loss = 0.74110172\n",
      "Iteration 151, loss = 0.40282898\n",
      "Iteration 1967, loss = 0.17717725\n",
      "Iteration 4, loss = 0.73489092\n",
      "Iteration 979, loss = 0.30820119\n",
      "Iteration 152, loss = 0.40220629\n",
      "Iteration 5, loss = 0.72774960\n",
      "Iteration 153, loss = 0.40156541\n",
      "Iteration 1671, loss = 0.21969952\n",
      "Iteration 154, loss = 0.40098946\n",
      "Iteration 6, loss = 0.72045880\n",
      "Iteration 155, loss = 0.40038026\n",
      "Iteration 1127, loss = 0.24076337\n",
      "Iteration 7, loss = 0.71308307\n",
      "Iteration 980, loss = 0.30806632\n",
      "Iteration 156, loss = 0.39976850\n",
      "Iteration 157, loss = 0.39918743\n",
      "Iteration 1968, loss = 0.17701418\n",
      "Iteration 8, loss = 0.70541742\n",
      "Iteration 1111, loss = 0.25163132\n",
      "Iteration 172, loss = 0.47647208\n",
      "Iteration 1128, loss = 0.24053224\n",
      "Iteration 9, loss = 0.69783158\n",
      "Iteration 1969, loss = 0.17693644\n",
      "Iteration 1672, loss = 0.21946801\n",
      "Iteration 981, loss = 0.30799102\n",
      "Iteration 10, loss = 0.69063951\n",
      "Iteration 158, loss = 0.39858828\n",
      "Iteration 1129, loss = 0.24053360\n",
      "Iteration 1970, loss = 0.17675792\n",
      "Iteration 11, loss = 0.68324967\n",
      "Iteration 982, loss = 0.30782091\n",
      "Iteration 1971, loss = 0.17665098Iteration 12, loss = 0.67636844\n",
      "\n",
      "Iteration 159, loss = 0.39804134\n",
      "Iteration 1112, loss = 0.25144067\n",
      "Iteration 160, loss = 0.39746032\n",
      "Iteration 161, loss = 0.39692216\n",
      "Iteration 13, loss = 0.66974413\n",
      "Iteration 162, loss = 0.39633167\n",
      "Iteration 1673, loss = 0.21936524\n",
      "Iteration 173, loss = 0.47578831\n",
      "Iteration 1130, loss = 0.24012766\n",
      "Iteration 163, loss = 0.39579171\n",
      "Iteration 1972, loss = 0.17673050\n",
      "Iteration 164, loss = 0.39524948\n",
      "Iteration 983, loss = 0.30766837\n",
      "Iteration 14, loss = 0.66329946\n",
      "Iteration 165, loss = 0.39471921\n",
      "Iteration 1113, loss = 0.25126757\n",
      "Iteration 1973, loss = 0.17671807\n",
      "Iteration 1131, loss = 0.23985587Iteration 15, loss = 0.65700731\n",
      "\n",
      "Iteration 166, loss = 0.39418232\n",
      "Iteration 984, loss = 0.30757276\n",
      "Iteration 16, loss = 0.65096255\n",
      "Iteration 1974, loss = 0.17638356\n",
      "Iteration 1674, loss = 0.21925779\n",
      "Iteration 17, loss = 0.64494077\n",
      "Iteration 167, loss = 0.39365769\n",
      "Iteration 1975, loss = 0.17627202\n",
      "Iteration 18, loss = 0.63906087\n",
      "Iteration 168, loss = 0.39314573\n",
      "Iteration 1132, loss = 0.23989632\n",
      "Iteration 19, loss = 0.63374568\n",
      "Iteration 1976, loss = 0.17610745\n",
      "Iteration 1675, loss = 0.21915895\n",
      "Iteration 174, loss = 0.47506743\n",
      "Iteration 169, loss = 0.39262980\n",
      "Iteration 20, loss = 0.62846493\n",
      "Iteration 21, loss = 0.62303288\n",
      "Iteration 1977, loss = 0.17617224\n",
      "Iteration 1676, loss = 0.21900473\n",
      "Iteration 1133, loss = 0.23953546\n",
      "Iteration 985, loss = 0.30743099\n",
      "Iteration 170, loss = 0.39213126\n",
      "Iteration 22, loss = 0.61797234\n",
      "Iteration 1114, loss = 0.25102200\n",
      "Iteration 1134, loss = 0.23931122\n",
      "Iteration 23, loss = 0.61298476\n",
      "Iteration 171, loss = 0.39162391\n",
      "Iteration 1978, loss = 0.17601520\n",
      "Iteration 175, loss = 0.47442675\n",
      "Iteration 1135, loss = 0.23911184\n",
      "Iteration 24, loss = 0.60813421\n",
      "Iteration 172, loss = 0.39113053\n",
      "Iteration 1136, loss = 0.23915897\n",
      "Iteration 1115, loss = 0.25097608\n",
      "Iteration 986, loss = 0.30736308\n",
      "Iteration 1677, loss = 0.21893166\n",
      "Iteration 173, loss = 0.39059728\n",
      "Iteration 25, loss = 0.60330738\n",
      "Iteration 1979, loss = 0.17591770\n",
      "Iteration 174, loss = 0.39016202\n",
      "Iteration 26, loss = 0.59859397\n",
      "Iteration 1137, loss = 0.23874601\n",
      "Iteration 175, loss = 0.38964282\n",
      "Iteration 176, loss = 0.47372330\n",
      "Iteration 1980, loss = 0.17580768\n",
      "Iteration 27, loss = 0.59405228\n",
      "Iteration 1138, loss = 0.23837495\n",
      "Iteration 28, loss = 0.58965261\n",
      "Iteration 987, loss = 0.30719175\n",
      "Iteration 176, loss = 0.38914717\n",
      "Iteration 29, loss = 0.58526719\n",
      "Iteration 1139, loss = 0.23816434\n",
      "Iteration 177, loss = 0.47305973\n",
      "Iteration 30, loss = 0.58095645\n",
      "Iteration 1981, loss = 0.17565492\n",
      "Iteration 177, loss = 0.38868550\n",
      "Iteration 1116, loss = 0.25070951\n",
      "Iteration 31, loss = 0.57669909\n",
      "Iteration 1678, loss = 0.21877271\n",
      "Iteration 1982, loss = 0.17547539\n",
      "Iteration 178, loss = 0.38823790\n",
      "Iteration 32, loss = 0.57258732\n",
      "Iteration 1983, loss = 0.17537307\n",
      "Iteration 33, loss = 0.56848205\n",
      "Iteration 988, loss = 0.30709070\n",
      "Iteration 1117, loss = 0.25054097\n",
      "Iteration 1984, loss = 0.17530594\n",
      "Iteration 1679, loss = 0.21872656\n",
      "Iteration 179, loss = 0.38771444\n",
      "Iteration 34, loss = 0.56446215\n",
      "Iteration 1140, loss = 0.23795281\n",
      "Iteration 180, loss = 0.38728169\n",
      "Iteration 178, loss = 0.47241246\n",
      "Iteration 181, loss = 0.38683473\n",
      "Iteration 989, loss = 0.30696189\n",
      "Iteration 1680, loss = 0.21855257\n",
      "Iteration 182, loss = 0.38640261\n",
      "Iteration 35, loss = 0.56061386\n",
      "Iteration 1985, loss = 0.17533311\n",
      "Iteration 183, loss = 0.38591280\n",
      "Iteration 1141, loss = 0.23770684\n",
      "Iteration 36, loss = 0.55679321\n",
      "Iteration 1986, loss = 0.17504720\n",
      "Iteration 1142, loss = 0.23750614\n",
      "Iteration 37, loss = 0.55308826\n",
      "Iteration 1987, loss = 0.17493854\n",
      "Iteration 990, loss = 0.30685866\n",
      "Iteration 179, loss = 0.47176230\n",
      "Iteration 184, loss = 0.38544448\n",
      "Iteration 38, loss = 0.54928814\n",
      "Iteration 1988, loss = 0.17494874\n",
      "Iteration 185, loss = 0.38503121\n",
      "Iteration 39, loss = 0.54564150\n",
      "Iteration 1681, loss = 0.21842697\n",
      "Iteration 186, loss = 0.38460108\n",
      "Iteration 1143, loss = 0.23740953\n",
      "Iteration 1989, loss = 0.17472437\n",
      "Iteration 187, loss = 0.38415016\n",
      "Iteration 40, loss = 0.54219832\n",
      "Iteration 188, loss = 0.38373104\n",
      "Iteration 1990, loss = 0.17467130\n",
      "Iteration 1682, loss = 0.21831280\n",
      "Iteration 180, loss = 0.47111884\n",
      "Iteration 189, loss = 0.38333388\n",
      "Iteration 1991, loss = 0.17453138\n",
      "Iteration 190, loss = 0.38288861\n",
      "Iteration 191, loss = 0.38245986\n",
      "Iteration 1144, loss = 0.23707868\n",
      "Iteration 1118, loss = 0.25010585\n",
      "Iteration 1683, loss = 0.21827155\n",
      "Iteration 991, loss = 0.30669913\n",
      "Iteration 192, loss = 0.38204523\n",
      "Iteration 41, loss = 0.53865166\n",
      "Iteration 1992, loss = 0.17442528\n",
      "Iteration 1145, loss = 0.23686466\n",
      "Iteration 42, loss = 0.53523216\n",
      "Iteration 193, loss = 0.38163553\n",
      "Iteration 992, loss = 0.30661182\n",
      "Iteration 43, loss = 0.53193950\n",
      "Iteration 1146, loss = 0.23685069\n",
      "Iteration 1993, loss = 0.17435648\n",
      "Iteration 1119, loss = 0.24990415\n",
      "Iteration 194, loss = 0.38123570\n",
      "Iteration 181, loss = 0.47049990\n",
      "Iteration 44, loss = 0.52856334\n",
      "Iteration 195, loss = 0.38084117\n",
      "Iteration 196, loss = 0.38042027\n",
      "Iteration 993, loss = 0.30645910\n",
      "Iteration 1994, loss = 0.17419453\n",
      "Iteration 197, loss = 0.38004661\n",
      "Iteration 45, loss = 0.52537806\n",
      "Iteration 198, loss = 0.37963127\n",
      "Iteration 1120, loss = 0.24977377\n",
      "Iteration 1147, loss = 0.23664889\n",
      "Iteration 199, loss = 0.37927672\n",
      "Iteration 182, loss = 0.46984538\n",
      "Iteration 994, loss = 0.30633376\n",
      "Iteration 46, loss = 0.52219322\n",
      "Iteration 1995, loss = 0.17406481\n",
      "Iteration 1148, loss = 0.23623249\n",
      "Iteration 200, loss = 0.37888162\n",
      "Iteration 47, loss = 0.51919013\n",
      "Iteration 1684, loss = 0.21824636\n",
      "Iteration 995, loss = 0.30627166\n",
      "Iteration 1996, loss = 0.17397774\n",
      "Iteration 48, loss = 0.51611854\n",
      "Iteration 1997, loss = 0.17385367\n",
      "Iteration 1149, loss = 0.23599700\n",
      "Iteration 49, loss = 0.51309566\n",
      "Iteration 201, loss = 0.37849398\n",
      "Iteration 183, loss = 0.46925249\n",
      "Iteration 1998, loss = 0.17383210\n",
      "Iteration 1150, loss = 0.23582534\n",
      "Iteration 202, loss = 0.37810304\n",
      "Iteration 50, loss = 0.51021895\n",
      "Iteration 1685, loss = 0.21796132\n",
      "Iteration 203, loss = 0.37772892\n",
      "Iteration 51, loss = 0.50728250\n",
      "Iteration 1121, loss = 0.24953808\n",
      "Iteration 1151, loss = 0.23557988\n",
      "Iteration 1999, loss = 0.17369668\n",
      "Iteration 204, loss = 0.37735055\n",
      "Iteration 52, loss = 0.50437318\n",
      "Iteration 996, loss = 0.30607419\n",
      "Iteration 53, loss = 0.50169623\n",
      "Iteration 2000, loss = 0.17352433\n",
      "Iteration 1686, loss = 0.21793577\n",
      "Iteration 205, loss = 0.37697904\n",
      "Iteration 54, loss = 0.49900409\n",
      "Iteration 184, loss = 0.46869613\n",
      "Iteration 55, loss = 0.49633294\n",
      "Iteration 1152, loss = 0.23536438\n",
      "Iteration 2001, loss = 0.17337754\n",
      "Iteration 997, loss = 0.30599367\n",
      "Iteration 206, loss = 0.37662810\n",
      "Iteration 1122, loss = 0.24937530\n",
      "Iteration 1687, loss = 0.21786302\n",
      "Iteration 56, loss = 0.49375294\n",
      "Iteration 1153, loss = 0.23512804\n",
      "Iteration 207, loss = 0.37626400\n",
      "Iteration 2002, loss = 0.17329675\n",
      "Iteration 208, loss = 0.37589908\n",
      "Iteration 1688, loss = 0.21765243\n",
      "Iteration 185, loss = 0.46802520\n",
      "Iteration 57, loss = 0.49122668\n",
      "Iteration 1123, loss = 0.24905682\n",
      "Iteration 209, loss = 0.37555161\n",
      "Iteration 2003, loss = 0.17323934\n",
      "Iteration 58, loss = 0.48871513\n",
      "Iteration 1154, loss = 0.23486899\n",
      "Iteration 998, loss = 0.30583242\n",
      "Iteration 210, loss = 0.37516400\n",
      "Iteration 211, loss = 0.37481426\n",
      "Iteration 212, loss = 0.37446884\n",
      "Iteration 999, loss = 0.30571479\n",
      "Iteration 2004, loss = 0.17314228\n",
      "Iteration 213, loss = 0.37410591\n",
      "Iteration 59, loss = 0.48626622\n",
      "Iteration 214, loss = 0.37380136\n",
      "Iteration 1124, loss = 0.24908061\n",
      "Iteration 1155, loss = 0.23466863\n",
      "Iteration 1689, loss = 0.21754300\n",
      "Iteration 215, loss = 0.37344163\n",
      "Iteration 2005, loss = 0.17300515\n",
      "Iteration 216, loss = 0.37311202\n",
      "Iteration 1000, loss = 0.30559636\n",
      "Iteration 60, loss = 0.48385508\n",
      "Iteration 186, loss = 0.46740956\n",
      "Iteration 217, loss = 0.37274335\n",
      "Iteration 1156, loss = 0.23456110\n",
      "Iteration 218, loss = 0.37242517\n",
      "Iteration 2006, loss = 0.17290857\n",
      "Iteration 61, loss = 0.48154623\n",
      "Iteration 219, loss = 0.37209795\n",
      "Iteration 1125, loss = 0.24865044\n",
      "Iteration 187, loss = 0.46685844\n",
      "Iteration 62, loss = 0.47932746\n",
      "Iteration 1001, loss = 0.30548876\n",
      "Iteration 220, loss = 0.37175526\n",
      "Iteration 1690, loss = 0.21738049\n",
      "Iteration 221, loss = 0.37143097\n",
      "Iteration 2007, loss = 0.17277553\n",
      "Iteration 63, loss = 0.47708110\n",
      "Iteration 1157, loss = 0.23434951\n",
      "Iteration 222, loss = 0.37111851\n",
      "Iteration 64, loss = 0.47491314\n",
      "Iteration 1126, loss = 0.24855277\n",
      "Iteration 1158, loss = 0.23405525\n",
      "Iteration 2008, loss = 0.17273309\n",
      "Iteration 223, loss = 0.37081125\n",
      "Iteration 65, loss = 0.47274550\n",
      "Iteration 188, loss = 0.46633365\n",
      "Iteration 66, loss = 0.47067506\n",
      "Iteration 1159, loss = 0.23388460\n",
      "Iteration 67, loss = 0.46864476\n",
      "Iteration 1002, loss = 0.30540701\n",
      "Iteration 1691, loss = 0.21730913\n",
      "Iteration 224, loss = 0.37050727\n",
      "Iteration 2009, loss = 0.17254073\n",
      "Iteration 68, loss = 0.46669763\n",
      "Iteration 1127, loss = 0.24824732\n",
      "Iteration 225, loss = 0.37016458\n",
      "Iteration 1160, loss = 0.23365378\n",
      "Iteration 1003, loss = 0.30526277\n",
      "Iteration 69, loss = 0.46478295\n",
      "Iteration 226, loss = 0.36982726\n",
      "Iteration 2010, loss = 0.17277813Iteration 1692, loss = 0.21722564\n",
      "\n",
      "Iteration 1161, loss = 0.23356660\n",
      "Iteration 70, loss = 0.46283323\n",
      "Iteration 2011, loss = 0.17252485\n",
      "Iteration 71, loss = 0.46093124\n",
      "Iteration 189, loss = 0.46567958\n",
      "Iteration 72, loss = 0.45919430\n",
      "Iteration 2012, loss = 0.17223690\n",
      "Iteration 73, loss = 0.45738202\n",
      "Iteration 2013, loss = 0.17215249\n",
      "Iteration 1128, loss = 0.24798613\n",
      "Iteration 74, loss = 0.45568233\n",
      "Iteration 2014, loss = 0.17203359\n",
      "Iteration 75, loss = 0.45394691\n",
      "Iteration 190, loss = 0.46513164\n",
      "Iteration 76, loss = 0.45228504\n",
      "Iteration 1693, loss = 0.21713367\n",
      "Iteration 1162, loss = 0.23324549\n",
      "Iteration 2015, loss = 0.17197054\n",
      "Iteration 1129, loss = 0.24777166\n",
      "Iteration 77, loss = 0.45071156\n",
      "Iteration 227, loss = 0.36950935Iteration 2016, loss = 0.17186557\n",
      "Iteration 1004, loss = 0.30509468\n",
      "\n",
      "Iteration 1694, loss = 0.21699274\n",
      "Iteration 2017, loss = 0.17175443\n",
      "Iteration 228, loss = 0.36921245\n",
      "Iteration 229, loss = 0.36892388\n",
      "Iteration 2018, loss = 0.17162393\n",
      "Iteration 1163, loss = 0.23310001\n",
      "Iteration 78, loss = 0.44915309\n",
      "Iteration 1695, loss = 0.21682904\n",
      "Iteration 2019, loss = 0.17157437\n",
      "Iteration 191, loss = 0.46458609\n",
      "Iteration 1005, loss = 0.30497023\n",
      "Iteration 230, loss = 0.36861264\n",
      "Iteration 1130, loss = 0.24775744\n",
      "Iteration 2020, loss = 0.17137626\n",
      "Iteration 1164, loss = 0.23286573\n",
      "Iteration 231, loss = 0.36830423\n",
      "Iteration 1006, loss = 0.30489380\n",
      "Iteration 79, loss = 0.44753076\n",
      "Iteration 232, loss = 0.36799508\n",
      "Iteration 1696, loss = 0.21688385\n",
      "Iteration 192, loss = 0.46406469\n",
      "Iteration 1165, loss = 0.23262796\n",
      "Iteration 233, loss = 0.36770585\n",
      "Iteration 80, loss = 0.44601966\n",
      "Iteration 1007, loss = 0.30474140\n",
      "Iteration 1166, loss = 0.23235069\n",
      "Iteration 234, loss = 0.36741126\n",
      "Iteration 2021, loss = 0.17134505\n",
      "Iteration 1131, loss = 0.24745399\n",
      "Iteration 81, loss = 0.44451219\n",
      "Iteration 82, loss = 0.44311075\n",
      "Iteration 1008, loss = 0.30460779\n",
      "Iteration 235, loss = 0.36713972\n",
      "Iteration 1697, loss = 0.21656807\n",
      "Iteration 236, loss = 0.36683406\n",
      "Iteration 83, loss = 0.44171777\n",
      "Iteration 2022, loss = 0.17121467\n",
      "Iteration 1132, loss = 0.24732273\n",
      "Iteration 84, loss = 0.44027644\n",
      "Iteration 237, loss = 0.36656203\n",
      "Iteration 1009, loss = 0.30446227\n",
      "Iteration 193, loss = 0.46349253Iteration 2023, loss = 0.17113965\n",
      "\n",
      "Iteration 1167, loss = 0.23216001\n",
      "Iteration 85, loss = 0.43888151\n",
      "Iteration 238, loss = 0.36625702\n",
      "Iteration 2024, loss = 0.17099133\n",
      "Iteration 239, loss = 0.36596225\n",
      "Iteration 1698, loss = 0.21643246\n",
      "Iteration 1133, loss = 0.24696528\n",
      "Iteration 86, loss = 0.43764351\n",
      "Iteration 1010, loss = 0.30435252\n",
      "Iteration 240, loss = 0.36570125\n",
      "Iteration 1168, loss = 0.23200527\n",
      "Iteration 241, loss = 0.36541457\n",
      "Iteration 2025, loss = 0.17085181\n",
      "Iteration 242, loss = 0.36515394\n",
      "Iteration 87, loss = 0.43624842\n",
      "Iteration 243, loss = 0.36488033\n",
      "Iteration 1134, loss = 0.24677185\n",
      "Iteration 2026, loss = 0.17081217\n",
      "Iteration 1169, loss = 0.23180590\n",
      "Iteration 1011, loss = 0.30440393\n",
      "Iteration 244, loss = 0.36458166\n",
      "Iteration 1699, loss = 0.21643396\n",
      "Iteration 88, loss = 0.43495542\n",
      "Iteration 194, loss = 0.46305503\n",
      "Iteration 2027, loss = 0.17084368\n",
      "Iteration 245, loss = 0.36433425\n",
      "Iteration 1135, loss = 0.24660901\n",
      "Iteration 1012, loss = 0.30412130\n",
      "Iteration 89, loss = 0.43370542\n",
      "Iteration 1170, loss = 0.23149088\n",
      "Iteration 2028, loss = 0.17055885\n",
      "Iteration 1700, loss = 0.21622542\n",
      "Iteration 90, loss = 0.43252817\n",
      "Iteration 246, loss = 0.36402636\n",
      "Iteration 1171, loss = 0.23128771\n",
      "Iteration 247, loss = 0.36376587\n",
      "Iteration 2029, loss = 0.17043654\n",
      "Iteration 248, loss = 0.36351053\n",
      "Iteration 1013, loss = 0.30399705\n",
      "Iteration 91, loss = 0.43133351\n",
      "Iteration 1136, loss = 0.24634968\n",
      "Iteration 249, loss = 0.36324001\n",
      "Iteration 1172, loss = 0.23111308\n",
      "Iteration 195, loss = 0.46243768\n",
      "Iteration 2030, loss = 0.17035958\n",
      "Iteration 250, loss = 0.36298591\n",
      "Iteration 2031, loss = 0.17034441\n",
      "Iteration 251, loss = 0.36273018\n",
      "Iteration 1701, loss = 0.21609877\n",
      "Iteration 1173, loss = 0.23078226\n",
      "Iteration 1014, loss = 0.30386985\n",
      "Iteration 92, loss = 0.43011505\n",
      "Iteration 252, loss = 0.36245096Iteration 196, loss = 0.46189701\n",
      "\n",
      "Iteration 2032, loss = 0.17016619\n",
      "Iteration 1702, loss = 0.21601406\n",
      "Iteration 2033, loss = 0.17003372\n",
      "Iteration 93, loss = 0.42894217\n",
      "Iteration 1174, loss = 0.23063229\n",
      "Iteration 253, loss = 0.36223248\n",
      "Iteration 94, loss = 0.42781672\n",
      "Iteration 1015, loss = 0.30376007\n",
      "Iteration 2034, loss = 0.17012316\n",
      "Iteration 95, loss = 0.42674452\n",
      "Iteration 1137, loss = 0.24614524\n",
      "Iteration 197, loss = 0.46134146\n",
      "Iteration 254, loss = 0.36195747\n",
      "Iteration 1703, loss = 0.21590128\n",
      "Iteration 1175, loss = 0.23074435\n",
      "Iteration 2035, loss = 0.16983440\n",
      "Iteration 96, loss = 0.42565413\n",
      "Iteration 255, loss = 0.36171096\n",
      "Iteration 1016, loss = 0.30370690\n",
      "Iteration 256, loss = 0.36146176\n",
      "Iteration 198, loss = 0.46086898\n",
      "Iteration 97, loss = 0.42456065\n",
      "Iteration 1176, loss = 0.23023305\n",
      "Iteration 2036, loss = 0.16970654\n",
      "Iteration 1138, loss = 0.24592754\n",
      "Iteration 257, loss = 0.36120140\n",
      "Iteration 98, loss = 0.42353264\n",
      "Iteration 1177, loss = 0.23003368\n",
      "Iteration 1017, loss = 0.30352501\n",
      "Iteration 99, loss = 0.42245919\n",
      "Iteration 258, loss = 0.36096654\n",
      "Iteration 1704, loss = 0.21576628\n",
      "Iteration 1178, loss = 0.22990916\n",
      "Iteration 1139, loss = 0.24594573\n",
      "Iteration 1018, loss = 0.30335250\n",
      "Iteration 100, loss = 0.42144180\n",
      "Iteration 259, loss = 0.36073579\n",
      "Iteration 1179, loss = 0.22954644\n",
      "Iteration 199, loss = 0.46033178\n",
      "Iteration 1180, loss = 0.22945576\n",
      "Iteration 1705, loss = 0.21566246\n",
      "Iteration 260, loss = 0.36049562\n",
      "Iteration 101, loss = 0.42047048\n",
      "Iteration 2037, loss = 0.16963536\n",
      "Iteration 1140, loss = 0.24541975\n",
      "Iteration 261, loss = 0.36024237\n",
      "Iteration 1181, loss = 0.22917593\n",
      "Iteration 2038, loss = 0.16950233\n",
      "Iteration 1019, loss = 0.30324911\n",
      "Iteration 200, loss = 0.45986368\n",
      "Iteration 262, loss = 0.36000704\n",
      "Iteration 2039, loss = 0.16939537\n",
      "Iteration 102, loss = 0.41955504\n",
      "Iteration 1706, loss = 0.21558636\n",
      "Iteration 263, loss = 0.35978168\n",
      "Iteration 2040, loss = 0.16924949\n",
      "Iteration 1182, loss = 0.22890119\n",
      "Iteration 1020, loss = 0.30315359\n",
      "Iteration 264, loss = 0.35954003\n",
      "Iteration 2041, loss = 0.16922291\n",
      "Iteration 1141, loss = 0.24568471\n",
      "Iteration 265, loss = 0.35930113\n",
      "Iteration 201, loss = 0.45932023\n",
      "Iteration 103, loss = 0.41857896\n",
      "Iteration 2042, loss = 0.16909894\n",
      "Iteration 1183, loss = 0.22874676\n",
      "Iteration 266, loss = 0.35907022\n",
      "Iteration 1707, loss = 0.21542898\n",
      "Iteration 1021, loss = 0.30299992\n",
      "Iteration 2043, loss = 0.16895500\n",
      "Iteration 267, loss = 0.35883879\n",
      "Iteration 1184, loss = 0.22849222\n",
      "Iteration 268, loss = 0.35860504\n",
      "Iteration 269, loss = 0.35838229\n",
      "Iteration 1142, loss = 0.24508965\n",
      "Iteration 2044, loss = 0.16913566\n",
      "Iteration 270, loss = 0.35814507\n",
      "Iteration 202, loss = 0.45890632\n",
      "Iteration 1185, loss = 0.22837494\n",
      "Iteration 1022, loss = 0.30286557\n",
      "Iteration 1708, loss = 0.21535846\n",
      "Iteration 271, loss = 0.35793096\n",
      "Iteration 272, loss = 0.35771134\n",
      "Iteration 2045, loss = 0.16876737\n",
      "Iteration 273, loss = 0.35749603\n",
      "Iteration 1186, loss = 0.22819189\n",
      "Iteration 274, loss = 0.35729493\n",
      "Iteration 1143, loss = 0.24489333\n",
      "Iteration 1023, loss = 0.30280891\n",
      "Iteration 275, loss = 0.35705089\n",
      "Iteration 1709, loss = 0.21522059\n",
      "Iteration 276, loss = 0.35686459\n",
      "Iteration 1187, loss = 0.22813099\n",
      "Iteration 203, loss = 0.45840831\n",
      "Iteration 2046, loss = 0.16863730\n",
      "Iteration 277, loss = 0.35662805\n",
      "Iteration 278, loss = 0.35642973\n",
      "Iteration 2047, loss = 0.16851983\n",
      "Iteration 1024, loss = 0.30260776\n",
      "Iteration 279, loss = 0.35620453\n",
      "Iteration 1710, loss = 0.21510489\n",
      "Iteration 280, loss = 0.35602339\n",
      "Iteration 204, loss = 0.45789117\n",
      "Iteration 1188, loss = 0.22764015\n",
      "Iteration 2048, loss = 0.16850348\n",
      "Iteration 281, loss = 0.35580587\n",
      "Iteration 1144, loss = 0.24463187\n",
      "Iteration 282, loss = 0.35560270\n",
      "Iteration 1025, loss = 0.30255177\n",
      "Iteration 2049, loss = 0.16830947\n",
      "Iteration 283, loss = 0.35538490\n",
      "Iteration 284, loss = 0.35519564\n",
      "Iteration 1711, loss = 0.21496644\n",
      "Iteration 1189, loss = 0.22778464\n",
      "Iteration 285, loss = 0.35498539\n",
      "Iteration 2050, loss = 0.16814834\n",
      "Iteration 1145, loss = 0.24448431\n",
      "Iteration 205, loss = 0.45743058\n",
      "Iteration 286, loss = 0.35482811\n",
      "Iteration 1190, loss = 0.22722704\n",
      "Iteration 1712, loss = 0.21488065\n",
      "Iteration 2051, loss = 0.16813807\n",
      "Iteration 1191, loss = 0.22724581\n",
      "Iteration 1026, loss = 0.30247844\n",
      "Iteration 1713, loss = 0.21474168\n",
      "Iteration 1146, loss = 0.24426008\n",
      "Iteration 1192, loss = 0.22676909\n",
      "Iteration 287, loss = 0.35459425\n",
      "Iteration 288, loss = 0.35438645\n",
      "Iteration 2052, loss = 0.16796077\n",
      "Iteration 289, loss = 0.35424004\n",
      "Iteration 1193, loss = 0.22660195\n",
      "Iteration 1714, loss = 0.21482402\n",
      "Iteration 290, loss = 0.35400693\n",
      "Iteration 104, loss = 0.41763451Iteration 1027, loss = 0.30224343\n",
      "Iteration 206, loss = 0.45697938\n",
      "Iteration 1194, loss = 0.22638572\n",
      "Iteration 291, loss = 0.35381847\n",
      "Iteration 2053, loss = 0.16792296\n",
      "Iteration 292, loss = 0.35361954\n",
      "Iteration 1195, loss = 0.22612678\n",
      "Iteration 1715, loss = 0.21449140\n",
      "Iteration 293, loss = 0.35342225\n",
      "Iteration 2054, loss = 0.16783275\n",
      "Iteration 1147, loss = 0.24407681\n",
      "Iteration 207, loss = 0.45646499\n",
      "Iteration 294, loss = 0.35324537\n",
      "Iteration 295, loss = 0.35306173\n",
      "Iteration 1028, loss = 0.30225445\n",
      "Iteration 296, loss = 0.35286553\n",
      "Iteration 1196, loss = 0.22594820\n",
      "\n",
      "Iteration 2055, loss = 0.16767952\n",
      "Iteration 297, loss = 0.35268529\n",
      "Iteration 1716, loss = 0.21438818\n",
      "Iteration 298, loss = 0.35249717\n",
      "Iteration 1029, loss = 0.30201881\n",
      "Iteration 2056, loss = 0.16762105\n",
      "Iteration 1148, loss = 0.24381032\n",
      "Iteration 1197, loss = 0.22564110\n",
      "Iteration 208, loss = 0.45599332\n",
      "Iteration 1717, loss = 0.21430039\n",
      "Iteration 299, loss = 0.35232628\n",
      "Iteration 1198, loss = 0.22551843\n",
      "Iteration 300, loss = 0.35211068\n",
      "Iteration 2057, loss = 0.16743867\n",
      "Iteration 1030, loss = 0.30187163\n",
      "Iteration 209, loss = 0.45557467\n",
      "Iteration 2058, loss = 0.16739312\n",
      "Iteration 1199, loss = 0.22532662\n",
      "Iteration 301, loss = 0.35192692\n",
      "Iteration 2059, loss = 0.16727438\n",
      "Iteration 1718, loss = 0.21419029\n",
      "Iteration 1149, loss = 0.24365751\n",
      "Iteration 302, loss = 0.35176436\n",
      "Iteration 1031, loss = 0.30179119\n",
      "Iteration 2060, loss = 0.16716543\n",
      "Iteration 303, loss = 0.35158842\n",
      "Iteration 1200, loss = 0.22526688\n",
      "Iteration 304, loss = 0.35139740\n",
      "Iteration 105, loss = 0.41673517\n",
      "Iteration 2061, loss = 0.16709536\n",
      "Iteration 305, loss = 0.35121889\n",
      "Iteration 1201, loss = 0.22483688\n",
      "Iteration 210, loss = 0.45510082\n",
      "Iteration 1032, loss = 0.30163486\n",
      "Iteration 106, loss = 0.41582255\n",
      "Iteration 1719, loss = 0.21403102\n",
      "Iteration 2062, loss = 0.16704931\n",
      "Iteration 1150, loss = 0.24347340\n",
      "Iteration 306, loss = 0.35105035\n",
      "Iteration 1202, loss = 0.22460888\n",
      "Iteration 307, loss = 0.35087380\n",
      "Iteration 107, loss = 0.41496862\n",
      "Iteration 2063, loss = 0.16680216\n",
      "Iteration 108, loss = 0.41408272\n",
      "Iteration 308, loss = 0.35070646\n",
      "Iteration 2064, loss = 0.16669240\n",
      "Iteration 1033, loss = 0.30149635\n",
      "Iteration 109, loss = 0.41325458\n",
      "Iteration 309, loss = 0.35052608\n",
      "Iteration 1720, loss = 0.21397228\n",
      "Iteration 1151, loss = 0.24324733\n",
      "Iteration 310, loss = 0.35038196\n",
      "Iteration 2065, loss = 0.16674869\n",
      "Iteration 1203, loss = 0.22441154\n",
      "Iteration 1034, loss = 0.30141733\n",
      "Iteration 110, loss = 0.41242987\n",
      "Iteration 111, loss = 0.41165049\n",
      "Iteration 1204, loss = 0.22423530\n",
      "Iteration 311, loss = 0.35018336\n",
      "Iteration 211, loss = 0.45467428\n",
      "Iteration 312, loss = 0.35002697\n",
      "Iteration 1721, loss = 0.21383816\n",
      "Iteration 2066, loss = 0.16649491\n",
      "Iteration 1035, loss = 0.30129392\n",
      "Iteration 112, loss = 0.41080757\n",
      "Iteration 313, loss = 0.34983860\n",
      "Iteration 1205, loss = 0.22395988\n",
      "Iteration 2067, loss = 0.16641209\n",
      "Iteration 314, loss = 0.34967800\n",
      "Iteration 1722, loss = 0.21368118\n",
      "Iteration 113, loss = 0.41003374\n",
      "Iteration 2068, loss = 0.16635369\n",
      "Iteration 1206, loss = 0.22401664\n",
      "Iteration 315, loss = 0.34950568\n",
      "Iteration 1152, loss = 0.24314603\n",
      "Iteration 1036, loss = 0.30116091\n",
      "Iteration 114, loss = 0.40924762\n",
      "Iteration 1723, loss = 0.21357786\n",
      "Iteration 316, loss = 0.34933880\n",
      "Iteration 212, loss = 0.45425170\n",
      "Iteration 2069, loss = 0.16618629\n",
      "Iteration 115, loss = 0.40850815\n",
      "Iteration 317, loss = 0.34919691\n",
      "Iteration 1207, loss = 0.22358724\n",
      "Iteration 1153, loss = 0.24308798\n",
      "Iteration 318, loss = 0.34900960\n",
      "Iteration 116, loss = 0.40775027\n",
      "Iteration 319, loss = 0.34885134\n",
      "Iteration 2070, loss = 0.16609534\n",
      "Iteration 1724, loss = 0.21345334\n",
      "Iteration 320, loss = 0.34868038\n",
      "Iteration 2071, loss = 0.16600992\n",
      "Iteration 1208, loss = 0.22346534\n",
      "Iteration 1037, loss = 0.30109649\n",
      "Iteration 213, loss = 0.45378141\n",
      "Iteration 117, loss = 0.40700571\n",
      "Iteration 321, loss = 0.34851596\n",
      "Iteration 1725, loss = 0.21342599\n",
      "Iteration 1209, loss = 0.22312657\n",
      "Iteration 118, loss = 0.40631665\n",
      "Iteration 322, loss = 0.34835279\n",
      "Iteration 2072, loss = 0.16591793\n",
      "Iteration 323, loss = 0.34820656\n",
      "Iteration 1210, loss = 0.22291113\n",
      "Iteration 2073, loss = 0.16579939\n",
      "Iteration 324, loss = 0.34805604\n",
      "Iteration 214, loss = 0.45337120\n",
      "Iteration 1726, loss = 0.21320970\n",
      "Iteration 325, loss = 0.34789541\n",
      "Iteration 119, loss = 0.40558698\n",
      "Iteration 2074, loss = 0.16576069\n",
      "Iteration 326, loss = 0.34772334\n",
      "Iteration 120, loss = 0.40491917\n",
      "Iteration 327, loss = 0.34755619\n",
      "Iteration 2075, loss = 0.16559074\n",
      "Iteration 328, loss = 0.34740849\n",
      "Iteration 121, loss = 0.40417206\n",
      "Iteration 1154, loss = 0.24260580\n",
      "Iteration 329, loss = 0.34723818\n",
      "Iteration 2076, loss = 0.16556731\n",
      "Iteration 1038, loss = 0.30096207\n",
      "Iteration 122, loss = 0.40359107\n",
      "Iteration 330, loss = 0.34707280\n",
      "Iteration 1727, loss = 0.21315149\n",
      "Iteration 331, loss = 0.34693376\n",
      "Iteration 123, loss = 0.40287147\n",
      "Iteration 215, loss = 0.45291066\n",
      "Iteration 2077, loss = 0.16536568\n",
      "Iteration 332, loss = 0.34677316\n",
      "Iteration 124, loss = 0.40221854\n",
      "Iteration 2078, loss = 0.16529099\n",
      "Iteration 1728, loss = 0.21299532\n",
      "Iteration 125, loss = 0.40160876\n",
      "Iteration 1211, loss = 0.22271241\n",
      "Iteration 2079, loss = 0.16519165\n",
      "Iteration 333, loss = 0.34661774\n",
      "Iteration 1039, loss = 0.30082615\n",
      "Iteration 126, loss = 0.40097189\n",
      "Iteration 1212, loss = 0.22249093\n",
      "Iteration 1155, loss = 0.24234055\n",
      "Iteration 1729, loss = 0.21287473\n",
      "Iteration 334, loss = 0.34646992\n",
      "Iteration 2080, loss = 0.16508401\n",
      "Iteration 1040, loss = 0.30069082\n",
      "Iteration 127, loss = 0.40035830\n",
      "Iteration 1213, loss = 0.22225791\n",
      "Iteration 216, loss = 0.45249124\n",
      "Iteration 335, loss = 0.34631757\n",
      "Iteration 1041, loss = 0.30052888\n",
      "Iteration 2081, loss = 0.16495498\n",
      "Iteration 1214, loss = 0.22217590\n",
      "Iteration 336, loss = 0.34615453\n",
      "Iteration 128, loss = 0.39974553\n",
      "Iteration 1156, loss = 0.24216683\n",
      "Iteration 2082, loss = 0.16490118\n",
      "Iteration 217, loss = 0.45213862\n",
      "Iteration 2083, loss = 0.16471916\n",
      "Iteration 337, loss = 0.34600910\n",
      "Iteration 1730, loss = 0.21273513\n",
      "Iteration 1215, loss = 0.22196350\n",
      "Iteration 129, loss = 0.39910313\n",
      "Iteration 1042, loss = 0.30041033\n",
      "Iteration 338, loss = 0.34590548\n",
      "Iteration 1157, loss = 0.24197078\n",
      "Iteration 1216, loss = 0.22165953\n",
      "Iteration 2084, loss = 0.16461463\n",
      "Iteration 1731, loss = 0.21267563\n",
      "Iteration 218, loss = 0.45161997\n",
      "Iteration 339, loss = 0.34570785\n",
      "Iteration 1217, loss = 0.22164449\n",
      "Iteration 1732, loss = 0.21253689\n",
      "Iteration 340, loss = 0.34557714\n",
      "Iteration 1158, loss = 0.24176583\n",
      "Iteration 2085, loss = 0.16453428\n",
      "Iteration 341, loss = 0.34540762\n",
      "Iteration 1218, loss = 0.22120205\n",
      "Iteration 342, loss = 0.34526711\n",
      "Iteration 2086, loss = 0.16449520\n",
      "Iteration 343, loss = 0.34512127\n",
      "Iteration 1733, loss = 0.21248900\n",
      "Iteration 1043, loss = 0.30026966\n",
      "Iteration 2087, loss = 0.16430587\n",
      "Iteration 344, loss = 0.34498803\n",
      "Iteration 219, loss = 0.45121956\n",
      "Iteration 345, loss = 0.34481638\n",
      "Iteration 2088, loss = 0.16426104\n",
      "Iteration 1159, loss = 0.24153757Iteration 346, loss = 0.34470378\n",
      "\n",
      "Iteration 2089, loss = 0.16411243\n",
      "Iteration 130, loss = 0.39851044\n",
      "Iteration 347, loss = 0.34455078\n",
      "Iteration 1734, loss = 0.21233907\n",
      "Iteration 1219, loss = 0.22114702\n",
      "Iteration 1044, loss = 0.30017434\n",
      "Iteration 2090, loss = 0.16408885\n",
      "Iteration 348, loss = 0.34440205\n",
      "Iteration 349, loss = 0.34425150\n",
      "Iteration 220, loss = 0.45084668\n",
      "Iteration 1220, loss = 0.22090202\n",
      "Iteration 350, loss = 0.34411814\n",
      "Iteration 2091, loss = 0.16398126\n",
      "Iteration 1160, loss = 0.24144318\n",
      "Iteration 351, loss = 0.34399341\n",
      "Iteration 1045, loss = 0.30003862\n",
      "Iteration 2092, loss = 0.16379283\n",
      "Iteration 352, loss = 0.34383294\n",
      "Iteration 1735, loss = 0.21217591\n",
      "Iteration 1221, loss = 0.22063939\n",
      "Iteration 2093, loss = 0.16375518\n",
      "Iteration 353, loss = 0.34368944\n",
      "Iteration 354, loss = 0.34356521\n",
      "Iteration 2094, loss = 0.16368050\n",
      "Iteration 1046, loss = 0.29990974\n",
      "Iteration 1222, loss = 0.22069409\n",
      "Iteration 355, loss = 0.34341085\n",
      "Iteration 356, loss = 0.34327777\n",
      "Iteration 221, loss = 0.45047235\n",
      "Iteration 1736, loss = 0.21208395\n",
      "Iteration 1161, loss = 0.24115815\n",
      "Iteration 1223, loss = 0.22041408\n",
      "Iteration 357, loss = 0.34315118\n",
      "Iteration 2095, loss = 0.16358844\n",
      "Iteration 1047, loss = 0.29977346\n",
      "Iteration 1224, loss = 0.22004931\n",
      "Iteration 358, loss = 0.34300806\n",
      "Iteration 1737, loss = 0.21195756\n",
      "Iteration 222, loss = 0.45002445\n",
      "Iteration 2096, loss = 0.16340244Iteration 131, loss = 0.39794762\n",
      "\n",
      "Iteration 359, loss = 0.34289656\n",
      "Iteration 1048, loss = 0.29965069\n",
      "Iteration 1738, loss = 0.21190096\n",
      "Iteration 1162, loss = 0.24106968\n",
      "Iteration 360, loss = 0.34273332\n",
      "Iteration 2097, loss = 0.16335254\n",
      "Iteration 1225, loss = 0.21978383\n",
      "Iteration 223, loss = 0.44965378\n",
      "Iteration 1739, loss = 0.21177346\n",
      "Iteration 361, loss = 0.34263343\n",
      "Iteration 2098, loss = 0.16327614\n",
      "Iteration 362, loss = 0.34248009\n",
      "Iteration 1049, loss = 0.29952625\n",
      "Iteration 1226, loss = 0.21958162\n",
      "Iteration 1163, loss = 0.24070504\n",
      "Iteration 2099, loss = 0.16306446\n",
      "Iteration 363, loss = 0.34234615\n",
      "Iteration 224, loss = 0.44925440\n",
      "Iteration 1050, loss = 0.29945251\n",
      "Iteration 1227, loss = 0.21948666\n",
      "Iteration 364, loss = 0.34222322\n",
      "Iteration 1740, loss = 0.21162248\n",
      "Iteration 365, loss = 0.34207734\n",
      "Iteration 2100, loss = 0.16304337\n",
      "Iteration 1051, loss = 0.29931915\n",
      "Iteration 1228, loss = 0.21921216\n",
      "Iteration 366, loss = 0.34195704\n",
      "Iteration 225, loss = 0.44883334\n",
      "Iteration 1164, loss = 0.24047711\n",
      "Iteration 2101, loss = 0.16301435\n",
      "Iteration 367, loss = 0.34184946\n",
      "Iteration 368, loss = 0.34169037\n",
      "Iteration 1741, loss = 0.21150117\n",
      "Iteration 2102, loss = 0.16284862\n",
      "Iteration 132, loss = 0.39739679\n",
      "Iteration 1229, loss = 0.21902199\n",
      "Iteration 369, loss = 0.34156393\n",
      "Iteration 1052, loss = 0.29917103\n",
      "Iteration 2103, loss = 0.16270897\n",
      "Iteration 1742, loss = 0.21141183\n",
      "Iteration 370, loss = 0.34145840\n",
      "Iteration 2104, loss = 0.16261063\n",
      "Iteration 1230, loss = 0.21923323\n",
      "Iteration 226, loss = 0.44843744\n",
      "Iteration 1165, loss = 0.24037594Iteration 133, loss = 0.39681395\n",
      "Iteration 1053, loss = 0.29909215\n",
      "Iteration 2105, loss = 0.16249070\n",
      "Iteration 371, loss = 0.34131184\n",
      "Iteration 134, loss = 0.39623216\n",
      "\n",
      "Iteration 2106, loss = 0.16244866\n",
      "Iteration 1743, loss = 0.21127760\n",
      "Iteration 372, loss = 0.34121310\n",
      "Iteration 2107, loss = 0.16236875\n",
      "Iteration 1054, loss = 0.29898234\n",
      "Iteration 373, loss = 0.34105583\n",
      "Iteration 1231, loss = 0.21870249\n",
      "Iteration 374, loss = 0.34094755\n",
      "Iteration 375, loss = 0.34083220\n",
      "Iteration 2108, loss = 0.16220098\n",
      "Iteration 1744, loss = 0.21121851\n",
      "Iteration 1232, loss = 0.21835270\n",
      "Iteration 376, loss = 0.34070556\n",
      "Iteration 135, loss = 0.39571488\n",
      "Iteration 377, loss = 0.34058725\n",
      "Iteration 1055, loss = 0.29878917\n",
      "Iteration 1233, loss = 0.21814922\n",
      "Iteration 2109, loss = 0.16203529\n",
      "Iteration 227, loss = 0.44808305\n",
      "Iteration 1745, loss = 0.21107707\n",
      "Iteration 378, loss = 0.34044781\n",
      "Iteration 1166, loss = 0.24010064\n",
      "Iteration 136, loss = 0.39517733\n",
      "Iteration 379, loss = 0.34034578\n",
      "Iteration 1234, loss = 0.21802253\n",
      "Iteration 1746, loss = 0.21095372\n",
      "Iteration 1056, loss = 0.29874366\n",
      "Iteration 137, loss = 0.39463664\n",
      "Iteration 2110, loss = 0.16201825\n",
      "Iteration 138, loss = 0.39409661\n",
      "Iteration 1747, loss = 0.21077648\n",
      "Iteration 1235, loss = 0.21789465\n",
      "Iteration 380, loss = 0.34020946\n",
      "Iteration 1167, loss = 0.23990691\n",
      "Iteration 381, loss = 0.34010586\n",
      "Iteration 2111, loss = 0.16188794\n",
      "Iteration 228, loss = 0.44775685\n",
      "Iteration 1748, loss = 0.21068144\n",
      "Iteration 1057, loss = 0.29854997\n",
      "Iteration 2112, loss = 0.16174845\n",
      "Iteration 1236, loss = 0.21761154\n",
      "Iteration 139, loss = 0.39359389\n",
      "Iteration 382, loss = 0.33997961\n",
      "Iteration 229, loss = 0.44742112\n",
      "Iteration 1058, loss = 0.29843574\n",
      "Iteration 1237, loss = 0.21735515\n",
      "Iteration 2113, loss = 0.16166048\n",
      "Iteration 383, loss = 0.33986745\n",
      "Iteration 1749, loss = 0.21059712\n",
      "Iteration 1238, loss = 0.21714414\n",
      "Iteration 1168, loss = 0.23975484\n",
      "Iteration 2114, loss = 0.16154112\n",
      "Iteration 384, loss = 0.33976057\n",
      "Iteration 1059, loss = 0.29829629\n",
      "Iteration 385, loss = 0.33966091\n",
      "Iteration 386, loss = 0.33950101\n",
      "Iteration 140, loss = 0.39307370\n",
      "Iteration 1750, loss = 0.21049287\n",
      "Iteration 1239, loss = 0.21694948\n",
      "Iteration 2115, loss = 0.16146971\n",
      "Iteration 230, loss = 0.44695188\n",
      "Iteration 1060, loss = 0.29815150\n",
      "Iteration 387, loss = 0.33939124\n",
      "Iteration 141, loss = 0.39257571\n",
      "Iteration 1751, loss = 0.21040495\n",
      "Iteration 1240, loss = 0.21702390\n",
      "Iteration 142, loss = 0.39206905\n",
      "Iteration 388, loss = 0.33927201\n",
      "Iteration 2116, loss = 0.16134478\n",
      "Iteration 1241, loss = 0.21662339\n",
      "Iteration 143, loss = 0.39157757\n",
      "Iteration 1061, loss = 0.29809811\n",
      "Iteration 1169, loss = 0.23951268\n",
      "Iteration 389, loss = 0.33916014\n",
      "Iteration 1752, loss = 0.21028649\n",
      "Iteration 144, loss = 0.39110742\n",
      "Iteration 390, loss = 0.33904054\n",
      "Iteration 1242, loss = 0.21642354\n",
      "Iteration 391, loss = 0.33894155\n",
      "Iteration 145, loss = 0.39059761Iteration 392, loss = 0.33880558\n",
      "\n",
      "Iteration 2117, loss = 0.16128061\n",
      "Iteration 1243, loss = 0.21621963\n",
      "Iteration 393, loss = 0.33870571\n",
      "Iteration 1753, loss = 0.21010150\n",
      "Iteration 146, loss = 0.39015119\n",
      "Iteration 1244, loss = 0.21627466\n",
      "Iteration 1062, loss = 0.29796147\n",
      "Iteration 147, loss = 0.38966357\n",
      "Iteration 394, loss = 0.33857289\n",
      "Iteration 1754, loss = 0.21004110\n",
      "Iteration 2118, loss = 0.16112316\n",
      "Iteration 395, loss = 0.33847052\n",
      "Iteration 1063, loss = 0.29782748\n",
      "Iteration 148, loss = 0.38919704\n",
      "Iteration 1245, loss = 0.21573665\n",
      "Iteration 231, loss = 0.44659801\n",
      "Iteration 1755, loss = 0.20997700\n",
      "Iteration 149, loss = 0.38875154\n",
      "Iteration 396, loss = 0.33835677\n",
      "Iteration 1170, loss = 0.23922792\n",
      "Iteration 2119, loss = 0.16118496\n",
      "Iteration 1064, loss = 0.29773092\n",
      "Iteration 397, loss = 0.33823905\n",
      "Iteration 398, loss = 0.33813330\n",
      "Iteration 1756, loss = 0.20979885\n",
      "Iteration 150, loss = 0.38831226\n",
      "Iteration 399, loss = 0.33803383\n",
      "Iteration 1171, loss = 0.23913708\n",
      "Iteration 400, loss = 0.33790188\n",
      "Iteration 151, loss = 0.38783549\n",
      "Iteration 1246, loss = 0.21570577\n",
      "Iteration 401, loss = 0.33778536\n",
      "Iteration 1757, loss = 0.20964064\n",
      "Iteration 152, loss = 0.38742309\n",
      "Iteration 402, loss = 0.33767847\n",
      "Iteration 153, loss = 0.38696389\n",
      "Iteration 2120, loss = 0.16106596\n",
      "Iteration 403, loss = 0.33757487\n",
      "Iteration 1172, loss = 0.23905777\n",
      "Iteration 1758, loss = 0.20966056\n",
      "Iteration 404, loss = 0.33747021\n",
      "Iteration 1247, loss = 0.21541893\n",
      "Iteration 2121, loss = 0.16086930\n",
      "Iteration 1065, loss = 0.29756058\n",
      "Iteration 405, loss = 0.33734766\n",
      "Iteration 154, loss = 0.38654359\n",
      "Iteration 1759, loss = 0.20945282\n",
      "Iteration 1248, loss = 0.21513408\n",
      "Iteration 232, loss = 0.44623543\n",
      "Iteration 406, loss = 0.33724501\n",
      "Iteration 1173, loss = 0.23878180\n",
      "Iteration 2122, loss = 0.16080536\n",
      "Iteration 155, loss = 0.38608119\n",
      "Iteration 407, loss = 0.33714897\n",
      "Iteration 2123, loss = 0.16072038\n",
      "Iteration 1760, loss = 0.20929764\n",
      "Iteration 156, loss = 0.38567758\n",
      "Iteration 408, loss = 0.33704303\n",
      "Iteration 157, loss = 0.38525751\n",
      "Iteration 1249, loss = 0.21496239\n",
      "Iteration 409, loss = 0.33691537\n",
      "Iteration 1066, loss = 0.29745838\n",
      "Iteration 2124, loss = 0.16071229\n",
      "Iteration 158, loss = 0.38487327\n",
      "Iteration 159, loss = 0.38444808\n",
      "Iteration 1761, loss = 0.20926237\n",
      "Iteration 410, loss = 0.33681155\n",
      "Iteration 1174, loss = 0.23852539\n",
      "Iteration 2125, loss = 0.16043150\n",
      "Iteration 1250, loss = 0.21480907\n",
      "Iteration 411, loss = 0.33669526\n",
      "Iteration 160, loss = 0.38404991\n",
      "Iteration 412, loss = 0.33659722\n",
      "Iteration 2126, loss = 0.16045486\n",
      "Iteration 233, loss = 0.44586802\n",
      "Iteration 161, loss = 0.38362335\n",
      "Iteration 1251, loss = 0.21460220\n",
      "Iteration 1762, loss = 0.20907398\n",
      "Iteration 413, loss = 0.33648797\n",
      "Iteration 414, loss = 0.33638638\n",
      "Iteration 2127, loss = 0.16023819\n",
      "Iteration 162, loss = 0.38324768\n",
      "Iteration 415, loss = 0.33627806\n",
      "Iteration 163, loss = 0.38283015\n",
      "Iteration 416, loss = 0.33616750\n",
      "Iteration 1175, loss = 0.23831434\n",
      "Iteration 1763, loss = 0.20905535\n",
      "Iteration 1252, loss = 0.21442388\n",
      "Iteration 417, loss = 0.33606151\n",
      "Iteration 164, loss = 0.38245642\n",
      "Iteration 2128, loss = 0.16018336\n",
      "Iteration 418, loss = 0.33595545\n",
      "Iteration 1253, loss = 0.21418793\n",
      "Iteration 234, loss = 0.44552593\n",
      "Iteration 419, loss = 0.33586529\n",
      "Iteration 1764, loss = 0.20885742\n",
      "Iteration 420, loss = 0.33574503\n",
      "Iteration 1067, loss = 0.29731524\n",
      "Iteration 165, loss = 0.38206633\n",
      "Iteration 421, loss = 0.33565647\n",
      "Iteration 1176, loss = 0.23801723\n",
      "Iteration 2129, loss = 0.16021037\n",
      "Iteration 422, loss = 0.33554087\n",
      "Iteration 166, loss = 0.38169038\n",
      "Iteration 423, loss = 0.33543093\n",
      "Iteration 167, loss = 0.38131774\n",
      "Iteration 424, loss = 0.33534458\n",
      "Iteration 425, loss = 0.33523082\n",
      "Iteration 168, loss = 0.38095992\n",
      "Iteration 1068, loss = 0.29722869\n",
      "Iteration 2130, loss = 0.16007471\n",
      "Iteration 169, loss = 0.38058768\n",
      "Iteration 1765, loss = 0.20872634\n",
      "Iteration 170, loss = 0.38021185\n",
      "Iteration 426, loss = 0.33513255\n",
      "Iteration 235, loss = 0.44520302\n",
      "Iteration 427, loss = 0.33503249\n",
      "Iteration 1177, loss = 0.23792197\n",
      "Iteration 428, loss = 0.33493034\n",
      "Iteration 2131, loss = 0.15984170\n",
      "Iteration 1254, loss = 0.21391666\n",
      "Iteration 1069, loss = 0.29705875\n",
      "Iteration 2132, loss = 0.15980976\n",
      "Iteration 429, loss = 0.33482935\n",
      "Iteration 430, loss = 0.33472611\n",
      "Iteration 2133, loss = 0.15960416\n",
      "Iteration 1766, loss = 0.20868534\n",
      "Iteration 431, loss = 0.33465708\n",
      "Iteration 1178, loss = 0.23764028\n",
      "Iteration 432, loss = 0.33453960\n",
      "Iteration 2134, loss = 0.15950669\n",
      "Iteration 171, loss = 0.37986021\n",
      "Iteration 1767, loss = 0.20860338\n",
      "Iteration 1070, loss = 0.29695847\n",
      "Iteration 236, loss = 0.44482804\n",
      "Iteration 2135, loss = 0.15951308\n",
      "Iteration 1255, loss = 0.21382247\n",
      "Iteration 433, loss = 0.33442837\n",
      "Iteration 2136, loss = 0.15931099\n",
      "Iteration 172, loss = 0.37951576\n",
      "Iteration 1071, loss = 0.29685851\n",
      "Iteration 1768, loss = 0.20842361\n",
      "Iteration 173, loss = 0.37913372\n",
      "Iteration 174, loss = 0.37878451\n",
      "Iteration 2137, loss = 0.15921926\n",
      "Iteration 175, loss = 0.37843698\n",
      "Iteration 1072, loss = 0.29670013\n",
      "Iteration 237, loss = 0.44447082\n",
      "Iteration 1256, loss = 0.21354418\n",
      "Iteration 1769, loss = 0.20848423\n",
      "Iteration 434, loss = 0.33432143\n",
      "Iteration 1073, loss = 0.29662536\n",
      "Iteration 2138, loss = 0.15918767\n",
      "Iteration 176, loss = 0.37812694\n",
      "Iteration 1770, loss = 0.20817465\n",
      "Iteration 435, loss = 0.33427154\n",
      "Iteration 238, loss = 0.44414166\n",
      "Iteration 2139, loss = 0.15910144\n",
      "Iteration 1074, loss = 0.29645095\n",
      "Iteration 1179, loss = 0.23741269\n",
      "Iteration 1771, loss = 0.20806710\n",
      "Iteration 436, loss = 0.33412408\n",
      "Iteration 437, loss = 0.33403614Iteration 2140, loss = 0.15899855\n",
      "\n",
      "Iteration 177, loss = 0.37778255\n",
      "Iteration 178, loss = 0.37746999\n",
      "Iteration 1257, loss = 0.21352018\n",
      "Iteration 438, loss = 0.33393080\n",
      "Iteration 179, loss = 0.37711721\n",
      "Iteration 1075, loss = 0.29631632\n",
      "Iteration 439, loss = 0.33383785\n",
      "Iteration 1772, loss = 0.20793493\n",
      "Iteration 2141, loss = 0.15895493\n",
      "Iteration 1180, loss = 0.23726776\n",
      "Iteration 440, loss = 0.33376440\n",
      "Iteration 180, loss = 0.37679171\n",
      "Iteration 239, loss = 0.44386785\n",
      "Iteration 2142, loss = 0.15880047\n",
      "Iteration 441, loss = 0.33365757\n",
      "Iteration 181, loss = 0.37645775\n",
      "Iteration 1258, loss = 0.21314283\n",
      "Iteration 2143, loss = 0.15861863\n",
      "Iteration 182, loss = 0.37615029\n",
      "Iteration 183, loss = 0.37582442\n",
      "Iteration 442, loss = 0.33353508\n",
      "Iteration 184, loss = 0.37549882\n",
      "Iteration 1181, loss = 0.23705770\n",
      "Iteration 1076, loss = 0.29626011\n",
      "Iteration 240, loss = 0.44346401\n",
      "Iteration 443, loss = 0.33343729\n",
      "Iteration 1773, loss = 0.20787605\n",
      "Iteration 2144, loss = 0.15854791\n",
      "Iteration 1259, loss = 0.21291167\n",
      "Iteration 185, loss = 0.37518397\n",
      "Iteration 444, loss = 0.33335858\n",
      "Iteration 1077, loss = 0.29607092\n",
      "Iteration 445, loss = 0.33325298\n",
      "Iteration 1182, loss = 0.23689621\n",
      "Iteration 446, loss = 0.33314259\n",
      "Iteration 2145, loss = 0.15845442\n",
      "Iteration 186, loss = 0.37487505\n",
      "Iteration 1260, loss = 0.21277893\n",
      "Iteration 1774, loss = 0.20782963\n",
      "Iteration 447, loss = 0.33305637\n",
      "Iteration 187, loss = 0.37457132\n",
      "Iteration 1078, loss = 0.29594534\n",
      "Iteration 1261, loss = 0.21248713\n",
      "Iteration 448, loss = 0.33293955\n",
      "Iteration 1183, loss = 0.23709099\n",
      "Iteration 1262, loss = 0.21234172\n",
      "Iteration 1775, loss = 0.20770271\n",
      "Iteration 2146, loss = 0.15829958\n",
      "Iteration 188, loss = 0.37431936\n",
      "Iteration 241, loss = 0.44319050\n",
      "Iteration 449, loss = 0.33285231\n",
      "Iteration 2147, loss = 0.15833303\n",
      "Iteration 1263, loss = 0.21229818\n",
      "Iteration 450, loss = 0.33276442\n",
      "Iteration 2148, loss = 0.15817943\n",
      "Iteration 1079, loss = 0.29598390\n",
      "Iteration 1264, loss = 0.21187122\n",
      "Iteration 451, loss = 0.33268825\n",
      "Iteration 2149, loss = 0.15807314\n",
      "Iteration 452, loss = 0.33258279\n",
      "Iteration 1184, loss = 0.23640588\n",
      "Iteration 189, loss = 0.37396483\n",
      "Iteration 2150, loss = 0.15794458\n",
      "Iteration 1265, loss = 0.21181214\n",
      "Iteration 242, loss = 0.44277433\n",
      "Iteration 453, loss = 0.33247318\n",
      "Iteration 1776, loss = 0.20752090\n",
      "Iteration 2151, loss = 0.15799715\n",
      "Iteration 454, loss = 0.33236703\n",
      "Iteration 190, loss = 0.37365602\n",
      "Iteration 455, loss = 0.33226581\n",
      "Iteration 1266, loss = 0.21160566\n",
      "Iteration 1185, loss = 0.23634432\n",
      "Iteration 191, loss = 0.37336254\n",
      "Iteration 1080, loss = 0.29574011\n",
      "Iteration 2152, loss = 0.15776915\n",
      "Iteration 1267, loss = 0.21140099\n",
      "Iteration 243, loss = 0.44248558\n",
      "Iteration 456, loss = 0.33217160\n",
      "Iteration 1081, loss = 0.29559896\n",
      "Iteration 192, loss = 0.37305626\n",
      "Iteration 1268, loss = 0.21118465\n",
      "Iteration 2153, loss = 0.15767509\n",
      "Iteration 457, loss = 0.33209797\n",
      "Iteration 1186, loss = 0.23600630\n",
      "Iteration 1777, loss = 0.20743719\n",
      "Iteration 2154, loss = 0.15753414\n",
      "Iteration 458, loss = 0.33197727\n",
      "Iteration 2155, loss = 0.15747306\n",
      "Iteration 459, loss = 0.33188313\n",
      "Iteration 193, loss = 0.37278115\n",
      "Iteration 1082, loss = 0.29556063\n",
      "Iteration 1269, loss = 0.21102269\n",
      "Iteration 194, loss = 0.37249465\n",
      "Iteration 244, loss = 0.44213166\n",
      "Iteration 2156, loss = 0.15744989\n",
      "Iteration 460, loss = 0.33179726\n",
      "Iteration 195, loss = 0.37221345\n",
      "Iteration 2157, loss = 0.15722474\n",
      "Iteration 461, loss = 0.33170174\n",
      "Iteration 196, loss = 0.37193312\n",
      "Iteration 462, loss = 0.33160629\n",
      "Iteration 1187, loss = 0.23578654\n",
      "Iteration 1083, loss = 0.29536205\n",
      "Iteration 197, loss = 0.37163960\n",
      "Iteration 463, loss = 0.33151025\n",
      "Iteration 1778, loss = 0.20724422\n",
      "Iteration 464, loss = 0.33141244\n",
      "Iteration 198, loss = 0.37136127\n",
      "Iteration 2158, loss = 0.15714018\n",
      "Iteration 465, loss = 0.33132639\n",
      "Iteration 199, loss = 0.37111356\n",
      "Iteration 1270, loss = 0.21084948\n",
      "Iteration 466, loss = 0.33122349\n",
      "Iteration 467, loss = 0.33113053\n",
      "Iteration 1271, loss = 0.21058617\n",
      "Iteration 245, loss = 0.44181433\n",
      "Iteration 468, loss = 0.33103422\n",
      "Iteration 2159, loss = 0.15713987\n",
      "Iteration 200, loss = 0.37082668\n",
      "Iteration 1084, loss = 0.29524595\n",
      "Iteration 201, loss = 0.37054574\n",
      "Iteration 469, loss = 0.33094335\n",
      "Iteration 1188, loss = 0.23557538\n",
      "Iteration 2160, loss = 0.15690045\n",
      "Iteration 1779, loss = 0.20719356\n",
      "Iteration 202, loss = 0.37030988\n",
      "Iteration 470, loss = 0.33085630\n",
      "Iteration 2161, loss = 0.15687787\n",
      "Iteration 1272, loss = 0.21064984\n",
      "Iteration 471, loss = 0.33075487\n",
      "Iteration 472, loss = 0.33067828\n",
      "Iteration 1085, loss = 0.29508026\n",
      "Iteration 2162, loss = 0.15676320\n",
      "Iteration 473, loss = 0.33057763\n",
      "Iteration 203, loss = 0.37003858\n",
      "Iteration 474, loss = 0.33050268\n",
      "Iteration 1273, loss = 0.21035254\n",
      "Iteration 2163, loss = 0.15668639\n",
      "Iteration 1086, loss = 0.29495434\n",
      "Iteration 204, loss = 0.36975868\n",
      "Iteration 1274, loss = 0.20999368\n",
      "Iteration 475, loss = 0.33039957\n",
      "Iteration 1780, loss = 0.20702944\n",
      "Iteration 2164, loss = 0.15658754\n",
      "Iteration 1087, loss = 0.29488111\n",
      "Iteration 205, loss = 0.36951174\n",
      "Iteration 1275, loss = 0.21003281\n",
      "Iteration 1189, loss = 0.23545532\n",
      "Iteration 2165, loss = 0.15650423\n",
      "Iteration 206, loss = 0.36924387\n",
      "Iteration 1781, loss = 0.20691273\n",
      "Iteration 1276, loss = 0.20963270\n",
      "Iteration 246, loss = 0.44151877\n",
      "Iteration 476, loss = 0.33028767\n",
      "Iteration 207, loss = 0.36899196\n",
      "Iteration 477, loss = 0.33022598\n",
      "Iteration 1782, loss = 0.20682052\n",
      "Iteration 2166, loss = 0.15644462\n",
      "Iteration 208, loss = 0.36873642\n",
      "Iteration 478, loss = 0.33013300\n",
      "Iteration 479, loss = 0.33002955\n",
      "Iteration 1088, loss = 0.29472228\n",
      "Iteration 480, loss = 0.32995986\n",
      "Iteration 209, loss = 0.36846292\n",
      "Iteration 481, loss = 0.32984417\n",
      "Iteration 210, loss = 0.36823015\n",
      "Iteration 247, loss = 0.44123487\n",
      "Iteration 1783, loss = 0.20671753\n",
      "Iteration 1190, loss = 0.23522211\n",
      "Iteration 2167, loss = 0.15624241\n",
      "Iteration 1277, loss = 0.20935727\n",
      "Iteration 211, loss = 0.36797857\n",
      "Iteration 2168, loss = 0.15613042\n",
      "Iteration 1278, loss = 0.20922164\n",
      "Iteration 482, loss = 0.32975616\n",
      "Iteration 2169, loss = 0.15612326\n",
      "Iteration 483, loss = 0.32966470\n",
      "Iteration 1089, loss = 0.29464322\n",
      "Iteration 1191, loss = 0.23505920\n",
      "Iteration 2170, loss = 0.15595198\n",
      "Iteration 484, loss = 0.32962024\n",
      "Iteration 1279, loss = 0.20898843\n",
      "Iteration 2171, loss = 0.15583755\n",
      "Iteration 1784, loss = 0.20663062\n",
      "Iteration 248, loss = 0.44096391\n",
      "Iteration 212, loss = 0.36774151\n",
      "Iteration 2172, loss = 0.15571453\n",
      "Iteration 1280, loss = 0.20872660\n",
      "Iteration 1090, loss = 0.29454896\n",
      "Iteration 2173, loss = 0.15595962\n",
      "Iteration 213, loss = 0.36751265\n",
      "Iteration 485, loss = 0.32953726\n",
      "Iteration 486, loss = 0.32939650\n",
      "Iteration 1281, loss = 0.20850337\n",
      "Iteration 1785, loss = 0.20648691\n",
      "Iteration 487, loss = 0.32930305\n",
      "Iteration 214, loss = 0.36726725\n",
      "Iteration 1091, loss = 0.29434469\n",
      "Iteration 249, loss = 0.44052294\n",
      "Iteration 2174, loss = 0.15558884\n",
      "Iteration 1282, loss = 0.20834049\n",
      "Iteration 488, loss = 0.32922576\n",
      "Iteration 1192, loss = 0.23489004\n",
      "Iteration 215, loss = 0.36700425\n",
      "Iteration 1092, loss = 0.29423864\n",
      "Iteration 2175, loss = 0.15556733\n",
      "Iteration 216, loss = 0.36678840\n",
      "Iteration 489, loss = 0.32912487\n",
      "Iteration 2176, loss = 0.15538117\n",
      "Iteration 1786, loss = 0.20635753\n",
      "Iteration 2177, loss = 0.15530897\n",
      "Iteration 1283, loss = 0.20834128\n",
      "Iteration 2178, loss = 0.15521418\n",
      "Iteration 217, loss = 0.36654345\n",
      "Iteration 490, loss = 0.32905384\n",
      "Iteration 1093, loss = 0.29411363\n",
      "Iteration 2179, loss = 0.15508242\n",
      "Iteration 250, loss = 0.44025601\n",
      "Iteration 491, loss = 0.32894880\n",
      "Iteration 2180, loss = 0.15493287\n",
      "Iteration 218, loss = 0.36629981\n",
      "Iteration 1284, loss = 0.20804449\n",
      "Iteration 2181, loss = 0.15498289\n",
      "Iteration 492, loss = 0.32886150\n",
      "Iteration 219, loss = 0.36606757\n",
      "Iteration 2182, loss = 0.15482447\n",
      "Iteration 1193, loss = 0.23464217\n",
      "Iteration 220, loss = 0.36585308\n",
      "Iteration 493, loss = 0.32879461\n",
      "Iteration 1094, loss = 0.29397079\n",
      "Iteration 2183, loss = 0.15468691\n",
      "Iteration 221, loss = 0.36561878\n",
      "Iteration 494, loss = 0.32870256\n",
      "Iteration 2184, loss = 0.15457991\n",
      "Iteration 495, loss = 0.32860230\n",
      "Iteration 1285, loss = 0.20776526\n",
      "Iteration 222, loss = 0.36540030\n",
      "Iteration 223, loss = 0.36517118\n",
      "Iteration 1787, loss = 0.20622543\n",
      "Iteration 1194, loss = 0.23438825\n",
      "Iteration 224, loss = 0.36493534\n",
      "Iteration 2185, loss = 0.15450688\n",
      "Iteration 1286, loss = 0.20769038\n",
      "Iteration 1095, loss = 0.29383580\n",
      "Iteration 225, loss = 0.36472777\n",
      "Iteration 496, loss = 0.32853293\n",
      "Iteration 1788, loss = 0.20622810\n",
      "Iteration 2186, loss = 0.15443702\n",
      "Iteration 1287, loss = 0.20741085\n",
      "Iteration 226, loss = 0.36451680\n",
      "Iteration 497, loss = 0.32842184\n",
      "Iteration 1195, loss = 0.23425008\n",
      "Iteration 251, loss = 0.43994441\n",
      "Iteration 2187, loss = 0.15434621\n",
      "Iteration 1096, loss = 0.29374142\n",
      "Iteration 498, loss = 0.32833288\n",
      "Iteration 227, loss = 0.36426743\n",
      "Iteration 1789, loss = 0.20615271\n",
      "Iteration 228, loss = 0.36406317\n",
      "Iteration 1288, loss = 0.20714417\n",
      "Iteration 229, loss = 0.36388416\n",
      "Iteration 1289, loss = 0.20694716\n",
      "Iteration 1097, loss = 0.29361952\n",
      "Iteration 2188, loss = 0.15423453\n",
      "Iteration 230, loss = 0.36362008\n",
      "Iteration 231, loss = 0.36341970\n",
      "Iteration 1290, loss = 0.20675693\n",
      "Iteration 2189, loss = 0.15410471\n",
      "Iteration 499, loss = 0.32825080\n",
      "Iteration 2190, loss = 0.15399801\n",
      "Iteration 1790, loss = 0.20591117\n",
      "Iteration 1196, loss = 0.23400303\n",
      "Iteration 500, loss = 0.32816971\n",
      "Iteration 2191, loss = 0.15394306\n",
      "Iteration 232, loss = 0.36321853\n",
      "Iteration 252, loss = 0.43961474\n",
      "Iteration 1098, loss = 0.29349023\n",
      "Iteration 1291, loss = 0.20659753\n",
      "Iteration 1791, loss = 0.20586761\n",
      "Iteration 501, loss = 0.32807111\n",
      "Iteration 502, loss = 0.32798882\n",
      "Iteration 503, loss = 0.32792620\n",
      "Iteration 1292, loss = 0.20636348\n",
      "Iteration 233, loss = 0.36299227\n",
      "Iteration 253, loss = 0.43929432\n",
      "Iteration 2192, loss = 0.15380498\n",
      "Iteration 504, loss = 0.32782473\n",
      "Iteration 1293, loss = 0.20627161\n",
      "Iteration 1099, loss = 0.29336151\n",
      "Iteration 505, loss = 0.32774377\n",
      "Iteration 1792, loss = 0.20567438\n",
      "Iteration 234, loss = 0.36278726\n",
      "Iteration 506, loss = 0.32766090\n",
      "Iteration 2193, loss = 0.15370527\n",
      "Iteration 1197, loss = 0.23393186\n",
      "Iteration 1793, loss = 0.20557323\n",
      "Iteration 254, loss = 0.43900250\n",
      "Iteration 507, loss = 0.32756353\n",
      "Iteration 508, loss = 0.32750607\n",
      "Iteration 2194, loss = 0.15378469\n",
      "Iteration 1100, loss = 0.29323501\n",
      "Iteration 509, loss = 0.32738956\n",
      "Iteration 510, loss = 0.32731724\n",
      "Iteration 255, loss = 0.43874301\n",
      "Iteration 2195, loss = 0.15357807\n",
      "Iteration 511, loss = 0.32722545\n",
      "Iteration 1794, loss = 0.20553257\n",
      "Iteration 235, loss = 0.36255725\n",
      "Iteration 512, loss = 0.32712994\n",
      "Iteration 1101, loss = 0.29308759\n",
      "Iteration 1294, loss = 0.20598624\n",
      "Iteration 513, loss = 0.32707474\n",
      "Iteration 236, loss = 0.36239548\n",
      "Iteration 1198, loss = 0.23363231\n",
      "Iteration 1295, loss = 0.20589348\n",
      "Iteration 237, loss = 0.36216002\n",
      "Iteration 2196, loss = 0.15341343\n",
      "Iteration 514, loss = 0.32696690\n",
      "Iteration 1795, loss = 0.20531622\n",
      "Iteration 256, loss = 0.43844177\n",
      "Iteration 515, loss = 0.32688587\n",
      "Iteration 516, loss = 0.32680540\n",
      "Iteration 238, loss = 0.36196155\n",
      "Iteration 1102, loss = 0.29297469\n",
      "Iteration 1296, loss = 0.20563662\n",
      "Iteration 2197, loss = 0.15333641\n",
      "Iteration 239, loss = 0.36176762\n",
      "Iteration 1199, loss = 0.23350600\n",
      "Iteration 240, loss = 0.36154699\n",
      "Iteration 1297, loss = 0.20547713\n",
      "Iteration 2198, loss = 0.15322646\n",
      "Iteration 241, loss = 0.36136266\n",
      "Iteration 1298, loss = 0.20519376\n",
      "Iteration 2199, loss = 0.15319985\n",
      "Iteration 1103, loss = 0.29285779\n",
      "Iteration 1796, loss = 0.20523219\n",
      "Iteration 1200, loss = 0.23320750\n",
      "Iteration 242, loss = 0.36115640\n",
      "Iteration 517, loss = 0.32673995\n",
      "Iteration 1299, loss = 0.20512094\n",
      "Iteration 1104, loss = 0.29272649\n",
      "Iteration 2200, loss = 0.15303678\n",
      "Iteration 518, loss = 0.32664556\n",
      "Iteration 243, loss = 0.36094772\n",
      "Iteration 257, loss = 0.43809799\n",
      "Iteration 519, loss = 0.32658043\n",
      "Iteration 520, loss = 0.32648854\n",
      "Iteration 244, loss = 0.36077023\n",
      "Iteration 1300, loss = 0.20483030\n",
      "Iteration 521, loss = 0.32639183\n",
      "Iteration 1797, loss = 0.20511950\n",
      "Iteration 522, loss = 0.32633428\n",
      "Iteration 245, loss = 0.36056626\n",
      "Iteration 523, loss = 0.32623832\n",
      "Iteration 1301, loss = 0.20471472\n",
      "Iteration 1201, loss = 0.23312935\n",
      "Iteration 524, loss = 0.32614697\n",
      "Iteration 246, loss = 0.36036993\n",
      "Iteration 1798, loss = 0.20501198\n",
      "Iteration 1105, loss = 0.29264581\n",
      "Iteration 2201, loss = 0.15297039\n",
      "Iteration 258, loss = 0.43782067\n",
      "Iteration 247, loss = 0.36018508\n",
      "Iteration 525, loss = 0.32606222\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1799, loss = 0.20488175\n",
      "Iteration 2202, loss = 0.15288547\n",
      "Iteration 1302, loss = 0.20444204\n",
      "Iteration 1106, loss = 0.29249355\n",
      "Iteration 1303, loss = 0.20422081\n",
      "Iteration 248, loss = 0.36000155\n",
      "Iteration 1202, loss = 0.23281292\n",
      "Iteration 249, loss = 0.35979391\n",
      "Iteration 2203, loss = 0.15303010\n",
      "Iteration 1304, loss = 0.20408126\n",
      "Iteration 250, loss = 0.35960842\n",
      "Iteration 1800, loss = 0.20480607\n",
      "Iteration 1, loss = 0.76085330\n",
      "Iteration 2204, loss = 0.15297275\n",
      "Iteration 259, loss = 0.43755957\n",
      "Iteration 2, loss = 0.75845197\n",
      "Iteration 1305, loss = 0.20414541\n",
      "Iteration 3, loss = 0.75501442\n",
      "Iteration 251, loss = 0.35942397\n",
      "Iteration 4, loss = 0.75077525\n",
      "Iteration 1203, loss = 0.23260295\n",
      "Iteration 5, loss = 0.74620120\n",
      "Iteration 252, loss = 0.35923248\n",
      "Iteration 6, loss = 0.74136324\n",
      "Iteration 7, loss = 0.73645282\n",
      "Iteration 2205, loss = 0.15261911\n",
      "Iteration 8, loss = 0.73167864\n",
      "Iteration 1107, loss = 0.29239766\n",
      "Iteration 1801, loss = 0.20467466\n",
      "Iteration 9, loss = 0.72706781\n",
      "Iteration 1306, loss = 0.20362974\n",
      "Iteration 10, loss = 0.72246496\n",
      "Iteration 11, loss = 0.71818055\n",
      "Iteration 2206, loss = 0.15244850\n",
      "Iteration 253, loss = 0.35908367\n",
      "Iteration 1204, loss = 0.23249576\n",
      "Iteration 12, loss = 0.71405415\n",
      "Iteration 260, loss = 0.43729874\n",
      "Iteration 13, loss = 0.70992060\n",
      "Iteration 1802, loss = 0.20452740\n",
      "Iteration 14, loss = 0.70593906\n",
      "Iteration 254, loss = 0.35888852\n",
      "Iteration 1307, loss = 0.20361140\n",
      "Iteration 15, loss = 0.70230123\n",
      "Iteration 2207, loss = 0.15244926\n",
      "Iteration 1108, loss = 0.29221591\n",
      "Iteration 1205, loss = 0.23225488\n",
      "Iteration 16, loss = 0.69887311\n",
      "Iteration 255, loss = 0.35870640\n",
      "Iteration 17, loss = 0.69526215\n",
      "Iteration 256, loss = 0.35850384\n",
      "Iteration 2208, loss = 0.15230127\n",
      "Iteration 1803, loss = 0.20444600\n",
      "Iteration 18, loss = 0.69191064\n",
      "Iteration 19, loss = 0.68853205\n",
      "Iteration 20, loss = 0.68528613\n",
      "Iteration 261, loss = 0.43696869\n",
      "Iteration 21, loss = 0.68213161\n",
      "Iteration 1308, loss = 0.20337147\n",
      "Iteration 1109, loss = 0.29213142\n",
      "Iteration 22, loss = 0.67909499\n",
      "Iteration 257, loss = 0.35831985\n",
      "Iteration 2209, loss = 0.15222062\n",
      "Iteration 23, loss = 0.67596699\n",
      "Iteration 24, loss = 0.67286924\n",
      "Iteration 25, loss = 0.66992987\n",
      "Iteration 258, loss = 0.35818039\n",
      "Iteration 1206, loss = 0.23204407\n",
      "Iteration 26, loss = 0.66697470\n",
      "Iteration 1804, loss = 0.20435539\n",
      "Iteration 1309, loss = 0.20310960\n",
      "Iteration 27, loss = 0.66392083\n",
      "Iteration 2210, loss = 0.15215078\n",
      "Iteration 259, loss = 0.35797296\n",
      "Iteration 28, loss = 0.66101767\n",
      "Iteration 1110, loss = 0.29202373\n",
      "Iteration 29, loss = 0.65811272\n",
      "Iteration 260, loss = 0.35779770\n",
      "Iteration 30, loss = 0.65520072\n",
      "Iteration 1310, loss = 0.20290751\n",
      "Iteration 31, loss = 0.65225870\n",
      "Iteration 261, loss = 0.35761864\n",
      "Iteration 2211, loss = 0.15198903\n",
      "Iteration 32, loss = 0.64938030\n",
      "Iteration 1111, loss = 0.29183063\n",
      "Iteration 33, loss = 0.64651962\n",
      "Iteration 262, loss = 0.35746451\n",
      "Iteration 1805, loss = 0.20420638\n",
      "Iteration 262, loss = 0.43675057\n",
      "Iteration 34, loss = 0.64357392\n",
      "Iteration 263, loss = 0.35727122\n",
      "Iteration 35, loss = 0.64068137\n",
      "Iteration 1207, loss = 0.23202748\n",
      "Iteration 264, loss = 0.35709822\n",
      "Iteration 36, loss = 0.63780890\n",
      "Iteration 1806, loss = 0.20409723\n",
      "Iteration 1112, loss = 0.29174953\n",
      "Iteration 2212, loss = 0.15197996\n",
      "Iteration 265, loss = 0.35693923\n",
      "Iteration 37, loss = 0.63480947\n",
      "Iteration 1311, loss = 0.20293474\n",
      "Iteration 38, loss = 0.63197243\n",
      "Iteration 266, loss = 0.35676713\n",
      "Iteration 1807, loss = 0.20419986Iteration 2213, loss = 0.15187031\n",
      "\n",
      "Iteration 1312, loss = 0.20251878\n",
      "Iteration 267, loss = 0.35659028\n",
      "Iteration 39, loss = 0.62901010\n",
      "Iteration 1113, loss = 0.29162041\n",
      "Iteration 40, loss = 0.62603546\n",
      "Iteration 268, loss = 0.35644678\n",
      "Iteration 41, loss = 0.62311842\n",
      "Iteration 2214, loss = 0.15173867\n",
      "Iteration 42, loss = 0.62022586\n",
      "Iteration 1313, loss = 0.20228733\n",
      "Iteration 269, loss = 0.35625511\n",
      "Iteration 263, loss = 0.43640583\n",
      "Iteration 43, loss = 0.61723879\n",
      "Iteration 1208, loss = 0.23162227\n",
      "Iteration 2215, loss = 0.15170364\n",
      "Iteration 1314, loss = 0.20216211\n",
      "Iteration 1114, loss = 0.29153363\n",
      "Iteration 270, loss = 0.35610371\n",
      "Iteration 2216, loss = 0.15158376\n",
      "Iteration 44, loss = 0.61426137\n",
      "Iteration 1315, loss = 0.20195823\n",
      "Iteration 1115, loss = 0.29136131\n",
      "Iteration 2217, loss = 0.15141579\n",
      "Iteration 1808, loss = 0.20387199\n",
      "Iteration 45, loss = 0.61140300\n",
      "Iteration 1316, loss = 0.20181280\n",
      "Iteration 271, loss = 0.35592827\n",
      "Iteration 264, loss = 0.43619145\n",
      "Iteration 2218, loss = 0.15137774Iteration 46, loss = 0.60842210\n",
      "\n",
      "Iteration 1116, loss = 0.29125171\n",
      "Iteration 47, loss = 0.60538133\n",
      "Iteration 272, loss = 0.35576710\n",
      "Iteration 48, loss = 0.60249004\n",
      "Iteration 2219, loss = 0.15153923\n",
      "Iteration 49, loss = 0.59949202\n",
      "Iteration 273, loss = 0.35559425\n",
      "Iteration 1209, loss = 0.23147466\n",
      "Iteration 265, loss = 0.43585425\n",
      "Iteration 1809, loss = 0.20380434\n",
      "Iteration 274, loss = 0.35544096\n",
      "Iteration 50, loss = 0.59657143\n",
      "Iteration 1117, loss = 0.29110871\n",
      "Iteration 51, loss = 0.59349684\n",
      "Iteration 275, loss = 0.35529786\n",
      "Iteration 52, loss = 0.59056326\n",
      "Iteration 53, loss = 0.58755854\n",
      "Iteration 54, loss = 0.58453364\n",
      "Iteration 1810, loss = 0.20371631\n",
      "Iteration 1317, loss = 0.20167435\n",
      "Iteration 55, loss = 0.58154123\n",
      "Iteration 1118, loss = 0.29100969\n",
      "Iteration 56, loss = 0.57850427\n",
      "Iteration 2220, loss = 0.15117791\n",
      "Iteration 1318, loss = 0.20140563\n",
      "Iteration 276, loss = 0.35513032\n",
      "Iteration 1811, loss = 0.20362408\n",
      "Iteration 1319, loss = 0.20119644\n",
      "Iteration 1119, loss = 0.29088107\n",
      "Iteration 2221, loss = 0.15110008\n",
      "Iteration 277, loss = 0.35496559\n",
      "Iteration 1812, loss = 0.20354160\n",
      "Iteration 266, loss = 0.43552775\n",
      "Iteration 57, loss = 0.57544869\n",
      "Iteration 1320, loss = 0.20096832\n",
      "Iteration 1210, loss = 0.23118426\n",
      "Iteration 278, loss = 0.35480249\n",
      "Iteration 58, loss = 0.57253988\n",
      "Iteration 59, loss = 0.56946436\n",
      "Iteration 1120, loss = 0.29075052\n",
      "Iteration 2222, loss = 0.15098853\n",
      "Iteration 279, loss = 0.35469948\n",
      "Iteration 60, loss = 0.56645102\n",
      "Iteration 280, loss = 0.35449891\n",
      "Iteration 61, loss = 0.56348061\n",
      "Iteration 62, loss = 0.56053270\n",
      "Iteration 1321, loss = 0.20083031\n",
      "Iteration 1813, loss = 0.20331128\n",
      "Iteration 267, loss = 0.43529004\n",
      "Iteration 63, loss = 0.55751365\n",
      "Iteration 281, loss = 0.35435075\n",
      "Iteration 2223, loss = 0.15090935\n",
      "Iteration 64, loss = 0.55451031\n",
      "Iteration 65, loss = 0.55166226\n",
      "Iteration 1211, loss = 0.23120763\n",
      "Iteration 66, loss = 0.54861903\n",
      "Iteration 67, loss = 0.54583097\n",
      "Iteration 282, loss = 0.35416886\n",
      "Iteration 68, loss = 0.54284298\n",
      "Iteration 2224, loss = 0.15104266\n",
      "Iteration 1322, loss = 0.20097664\n",
      "Iteration 1121, loss = 0.29063266\n",
      "Iteration 268, loss = 0.43502277\n",
      "Iteration 283, loss = 0.35401566\n",
      "Iteration 2225, loss = 0.15074723\n",
      "Iteration 69, loss = 0.54003090\n",
      "Iteration 284, loss = 0.35385334\n",
      "Iteration 2226, loss = 0.15066413\n",
      "Iteration 285, loss = 0.35369991\n",
      "Iteration 1212, loss = 0.23078786\n",
      "Iteration 70, loss = 0.53723267\n",
      "Iteration 1323, loss = 0.20073030\n",
      "Iteration 286, loss = 0.35354761\n",
      "Iteration 71, loss = 0.53437554\n",
      "Iteration 269, loss = 0.43469428\n",
      "Iteration 1324, loss = 0.20022070\n",
      "Iteration 1122, loss = 0.29049552\n",
      "Iteration 2227, loss = 0.15051814\n",
      "Iteration 1814, loss = 0.20326619\n",
      "Iteration 72, loss = 0.53150939\n",
      "Iteration 73, loss = 0.52875471\n",
      "Iteration 74, loss = 0.52603940\n",
      "Iteration 1325, loss = 0.20009385\n",
      "Iteration 287, loss = 0.35340809\n",
      "Iteration 75, loss = 0.52328606\n",
      "Iteration 1123, loss = 0.29037772\n",
      "Iteration 270, loss = 0.43443823\n",
      "Iteration 76, loss = 0.52070142\n",
      "Iteration 2228, loss = 0.15040335\n",
      "Iteration 288, loss = 0.35324093\n",
      "Iteration 77, loss = 0.51798565\n",
      "Iteration 2229, loss = 0.15036271\n",
      "Iteration 1326, loss = 0.20006404\n",
      "Iteration 289, loss = 0.35309710\n",
      "Iteration 1815, loss = 0.20312522\n",
      "Iteration 1213, loss = 0.23076099\n",
      "Iteration 78, loss = 0.51537929\n",
      "Iteration 79, loss = 0.51280601\n",
      "Iteration 1124, loss = 0.29030381\n",
      "Iteration 80, loss = 0.51039743\n",
      "Iteration 2230, loss = 0.15030334\n",
      "Iteration 1327, loss = 0.19968828\n",
      "Iteration 290, loss = 0.35293954\n",
      "Iteration 81, loss = 0.50780453\n",
      "Iteration 2231, loss = 0.15021702\n",
      "Iteration 1816, loss = 0.20297767\n",
      "Iteration 82, loss = 0.50536529\n",
      "Iteration 1328, loss = 0.19974527\n",
      "Iteration 271, loss = 0.43422595\n",
      "Iteration 1214, loss = 0.23050818\n",
      "Iteration 1125, loss = 0.29011419\n",
      "Iteration 2232, loss = 0.15010412\n",
      "Iteration 83, loss = 0.50292028\n",
      "Iteration 291, loss = 0.35280773\n",
      "Iteration 84, loss = 0.50055424\n",
      "Iteration 292, loss = 0.35263569\n",
      "Iteration 85, loss = 0.49822628\n",
      "Iteration 2233, loss = 0.15008355\n",
      "Iteration 1817, loss = 0.20288804\n",
      "Iteration 86, loss = 0.49594526\n",
      "Iteration 293, loss = 0.35248825\n",
      "Iteration 87, loss = 0.49363680\n",
      "Iteration 1215, loss = 0.23043847\n",
      "Iteration 88, loss = 0.49152486\n",
      "Iteration 294, loss = 0.35238251\n",
      "Iteration 89, loss = 0.48937353\n",
      "Iteration 1329, loss = 0.19924833\n",
      "Iteration 90, loss = 0.48720394\n",
      "Iteration 295, loss = 0.35221350\n",
      "Iteration 2234, loss = 0.14996454\n",
      "Iteration 91, loss = 0.48523270\n",
      "Iteration 92, loss = 0.48310389\n",
      "Iteration 272, loss = 0.43393564\n",
      "Iteration 2235, loss = 0.14988174\n",
      "Iteration 1818, loss = 0.20281870\n",
      "Iteration 93, loss = 0.48106686\n",
      "Iteration 1126, loss = 0.29003057\n",
      "Iteration 94, loss = 0.47910461\n",
      "Iteration 95, loss = 0.47720804\n",
      "Iteration 1216, loss = 0.23010021\n",
      "Iteration 2236, loss = 0.14976058\n",
      "Iteration 1330, loss = 0.19917624\n",
      "Iteration 296, loss = 0.35204973\n",
      "Iteration 1819, loss = 0.20267394\n",
      "Iteration 96, loss = 0.47536941\n",
      "Iteration 2237, loss = 0.14960049\n",
      "Iteration 1127, loss = 0.28992929\n",
      "Iteration 2238, loss = 0.14952884\n",
      "Iteration 1217, loss = 0.22981390\n",
      "Iteration 97, loss = 0.47350300\n",
      "Iteration 297, loss = 0.35190772\n",
      "Iteration 2239, loss = 0.14950835\n",
      "Iteration 98, loss = 0.47169905\n",
      "Iteration 273, loss = 0.43365359\n",
      "Iteration 298, loss = 0.35176044\n",
      "Iteration 99, loss = 0.46986180\n",
      "Iteration 1820, loss = 0.20261014\n",
      "Iteration 100, loss = 0.46817515\n",
      "Iteration 299, loss = 0.35160783\n",
      "Iteration 1331, loss = 0.19890414\n",
      "Iteration 101, loss = 0.46651032\n",
      "Iteration 102, loss = 0.46479431\n",
      "Iteration 300, loss = 0.35147265\n",
      "Iteration 103, loss = 0.46319187\n",
      "Iteration 1128, loss = 0.28976621\n",
      "Iteration 1821, loss = 0.20250762\n",
      "Iteration 104, loss = 0.46167171\n",
      "Iteration 2240, loss = 0.14936208\n",
      "Iteration 1332, loss = 0.19876706\n",
      "Iteration 105, loss = 0.46009115\n",
      "Iteration 106, loss = 0.45857402\n",
      "Iteration 301, loss = 0.35132332\n",
      "Iteration 2241, loss = 0.14923580\n",
      "Iteration 1218, loss = 0.22967544\n",
      "Iteration 1333, loss = 0.19867066\n",
      "Iteration 107, loss = 0.45702938\n",
      "Iteration 1129, loss = 0.28962674\n",
      "Iteration 108, loss = 0.45569356\n",
      "Iteration 1334, loss = 0.19852415\n",
      "Iteration 109, loss = 0.45420037\n",
      "Iteration 2242, loss = 0.14917774\n",
      "Iteration 110, loss = 0.45285643\n",
      "Iteration 1822, loss = 0.20241635\n",
      "Iteration 274, loss = 0.43338507\n",
      "Iteration 111, loss = 0.45149931\n",
      "Iteration 1130, loss = 0.28952747\n",
      "Iteration 2243, loss = 0.14904619\n",
      "Iteration 112, loss = 0.45010732\n",
      "Iteration 302, loss = 0.35116759\n",
      "Iteration 2244, loss = 0.14903494\n",
      "Iteration 113, loss = 0.44875468\n",
      "Iteration 1335, loss = 0.19834173\n",
      "Iteration 114, loss = 0.44769376\n",
      "Iteration 303, loss = 0.35103902\n",
      "Iteration 115, loss = 0.44630547\n",
      "Iteration 1219, loss = 0.22953664\n",
      "Iteration 1131, loss = 0.28963729\n",
      "Iteration 1823, loss = 0.20218565\n",
      "Iteration 116, loss = 0.44513393\n",
      "Iteration 2245, loss = 0.14889565\n",
      "Iteration 304, loss = 0.35091985\n",
      "Iteration 1336, loss = 0.19805614\n",
      "Iteration 117, loss = 0.44392375\n",
      "Iteration 118, loss = 0.44271724\n",
      "Iteration 2246, loss = 0.14878463\n",
      "Iteration 305, loss = 0.35074007\n",
      "Iteration 275, loss = 0.43311367\n",
      "Iteration 306, loss = 0.35061716\n",
      "Iteration 2247, loss = 0.14877617\n",
      "Iteration 119, loss = 0.44161371\n",
      "Iteration 307, loss = 0.35047325\n",
      "Iteration 1824, loss = 0.20209644\n",
      "Iteration 1337, loss = 0.19784273\n",
      "Iteration 120, loss = 0.44053380\n",
      "Iteration 2248, loss = 0.14869178\n",
      "Iteration 308, loss = 0.35033061\n",
      "Iteration 121, loss = 0.43937129\n",
      "Iteration 1132, loss = 0.28938433\n",
      "Iteration 122, loss = 0.43830761\n",
      "Iteration 1338, loss = 0.19770663\n",
      "Iteration 1220, loss = 0.22942893\n",
      "Iteration 123, loss = 0.43731857\n",
      "Iteration 124, loss = 0.43628156\n",
      "Iteration 1825, loss = 0.20199994\n",
      "Iteration 1339, loss = 0.19750216\n",
      "Iteration 2249, loss = 0.14854297\n",
      "Iteration 309, loss = 0.35017613\n",
      "Iteration 276, loss = 0.43283973\n",
      "Iteration 1133, loss = 0.28919298\n",
      "Iteration 125, loss = 0.43529322\n",
      "Iteration 126, loss = 0.43426344\n",
      "Iteration 2250, loss = 0.14840688\n",
      "Iteration 1340, loss = 0.19724802\n",
      "Iteration 1826, loss = 0.20192381\n",
      "Iteration 127, loss = 0.43327550\n",
      "Iteration 1221, loss = 0.22941615\n",
      "Iteration 128, loss = 0.43237447\n",
      "Iteration 2251, loss = 0.14843348\n",
      "Iteration 1134, loss = 0.28902265\n",
      "Iteration 310, loss = 0.35003233\n",
      "Iteration 277, loss = 0.43257790\n",
      "Iteration 311, loss = 0.34990892\n",
      "Iteration 129, loss = 0.43139855\n",
      "Iteration 2252, loss = 0.14826650\n",
      "Iteration 312, loss = 0.34978448\n",
      "Iteration 130, loss = 0.43055750\n",
      "Iteration 1222, loss = 0.22885051\n",
      "Iteration 1341, loss = 0.19707965\n",
      "Iteration 1135, loss = 0.28887751\n",
      "Iteration 131, loss = 0.42961336\n",
      "Iteration 313, loss = 0.34962854Iteration 2253, loss = 0.14818608\n",
      "Iteration 132, loss = 0.42871195\n",
      "\n",
      "Iteration 1827, loss = 0.20174513\n",
      "Iteration 278, loss = 0.43235531\n",
      "Iteration 1342, loss = 0.19693810\n",
      "Iteration 314, loss = 0.34949519\n",
      "Iteration 2254, loss = 0.14815962\n",
      "Iteration 133, loss = 0.42784505\n",
      "Iteration 315, loss = 0.34936500\n",
      "Iteration 1828, loss = 0.20165157\n",
      "Iteration 134, loss = 0.42698981\n",
      "Iteration 316, loss = 0.34921413\n",
      "Iteration 135, loss = 0.42617340\n",
      "Iteration 1136, loss = 0.28879977\n",
      "Iteration 136, loss = 0.42536930\n",
      "Iteration 2255, loss = 0.14800314\n",
      "Iteration 317, loss = 0.34907163\n",
      "Iteration 137, loss = 0.42454122\n",
      "Iteration 138, loss = 0.42371003\n",
      "Iteration 1829, loss = 0.20158024\n",
      "Iteration 318, loss = 0.34895408\n",
      "Iteration 1223, loss = 0.22907509\n",
      "Iteration 279, loss = 0.43205775\n",
      "Iteration 1137, loss = 0.28863830\n",
      "Iteration 1343, loss = 0.19659478\n",
      "Iteration 139, loss = 0.42298730\n",
      "Iteration 319, loss = 0.34882271\n",
      "Iteration 2256, loss = 0.14790345\n",
      "Iteration 320, loss = 0.34870362\n",
      "Iteration 140, loss = 0.42217166\n",
      "Iteration 1830, loss = 0.20159099\n",
      "Iteration 1344, loss = 0.19649362\n",
      "Iteration 321, loss = 0.34854262\n",
      "Iteration 2257, loss = 0.14789647\n",
      "Iteration 1224, loss = 0.22849765\n",
      "Iteration 141, loss = 0.42143847\n",
      "Iteration 1345, loss = 0.19631662\n",
      "Iteration 1831, loss = 0.20145078\n",
      "Iteration 142, loss = 0.42063658\n",
      "Iteration 322, loss = 0.34842749\n",
      "Iteration 143, loss = 0.41997979\n",
      "Iteration 1138, loss = 0.28857970\n",
      "Iteration 144, loss = 0.41920606\n",
      "Iteration 323, loss = 0.34827920\n",
      "Iteration 2258, loss = 0.14776094\n",
      "Iteration 145, loss = 0.41850913\n",
      "Iteration 146, loss = 0.41780163\n",
      "Iteration 324, loss = 0.34816747\n",
      "Iteration 280, loss = 0.43178105\n",
      "Iteration 147, loss = 0.41711699\n",
      "Iteration 1346, loss = 0.19616007\n",
      "Iteration 148, loss = 0.41637852\n",
      "Iteration 325, loss = 0.34800711\n",
      "Iteration 1225, loss = 0.22837129\n",
      "Iteration 149, loss = 0.41576591\n",
      "Iteration 326, loss = 0.34788591\n",
      "Iteration 2259, loss = 0.14761340\n",
      "Iteration 150, loss = 0.41504572\n",
      "Iteration 1832, loss = 0.20122109\n",
      "Iteration 327, loss = 0.34775463\n",
      "Iteration 1139, loss = 0.28854848\n",
      "Iteration 151, loss = 0.41444364\n",
      "Iteration 1347, loss = 0.19591587\n",
      "Iteration 2260, loss = 0.14758111\n",
      "Iteration 281, loss = 0.43154136\n",
      "Iteration 328, loss = 0.34762366\n",
      "Iteration 152, loss = 0.41376071\n",
      "Iteration 153, loss = 0.41313950\n",
      "Iteration 2261, loss = 0.14745484\n",
      "Iteration 1226, loss = 0.22824232\n",
      "Iteration 329, loss = 0.34749392\n",
      "Iteration 154, loss = 0.41250195\n",
      "Iteration 1833, loss = 0.20121024\n",
      "Iteration 1348, loss = 0.19570860\n",
      "Iteration 1140, loss = 0.28830843\n",
      "Iteration 155, loss = 0.41189963\n",
      "Iteration 330, loss = 0.34736338\n",
      "Iteration 2262, loss = 0.14746953\n",
      "Iteration 156, loss = 0.41129412\n",
      "Iteration 282, loss = 0.43126834\n",
      "Iteration 2263, loss = 0.14729840\n",
      "Iteration 1349, loss = 0.19555756\n",
      "Iteration 157, loss = 0.41070172\n",
      "Iteration 2264, loss = 0.14725233\n",
      "Iteration 158, loss = 0.41012792\n",
      "Iteration 1141, loss = 0.28811615\n",
      "Iteration 1227, loss = 0.22810122Iteration 1350, loss = 0.19548918\n",
      "Iteration 331, loss = 0.34724069\n",
      "Iteration 159, loss = 0.40954211\n",
      "\n",
      "Iteration 1834, loss = 0.20102026\n",
      "Iteration 160, loss = 0.40895147\n",
      "Iteration 2265, loss = 0.14713876\n",
      "Iteration 161, loss = 0.40840231\n",
      "Iteration 332, loss = 0.34711129\n",
      "Iteration 162, loss = 0.40778517\n",
      "Iteration 1835, loss = 0.20089105\n",
      "Iteration 163, loss = 0.40724327\n",
      "Iteration 333, loss = 0.34698550\n",
      "Iteration 1351, loss = 0.19537391\n",
      "Iteration 2266, loss = 0.14705138\n",
      "Iteration 334, loss = 0.34685200\n",
      "Iteration 1142, loss = 0.28803319\n",
      "Iteration 164, loss = 0.40674476\n",
      "Iteration 1228, loss = 0.22776091\n",
      "Iteration 1836, loss = 0.20077304\n",
      "Iteration 2267, loss = 0.14698877\n",
      "Iteration 335, loss = 0.34672750\n",
      "Iteration 1352, loss = 0.19500789\n",
      "Iteration 165, loss = 0.40618222\n",
      "Iteration 283, loss = 0.43103724\n",
      "Iteration 166, loss = 0.40564493\n",
      "Iteration 2268, loss = 0.14687721Iteration 167, loss = 0.40513901\n",
      "\n",
      "Iteration 336, loss = 0.34660666\n",
      "Iteration 168, loss = 0.40463066\n",
      "Iteration 169, loss = 0.40407919\n",
      "Iteration 337, loss = 0.34648044\n",
      "Iteration 170, loss = 0.40362174\n",
      "Iteration 171, loss = 0.40304494\n",
      "Iteration 1229, loss = 0.22766259\n",
      "Iteration 2269, loss = 0.14681275\n",
      "Iteration 172, loss = 0.40255042\n",
      "Iteration 338, loss = 0.34634970\n",
      "Iteration 173, loss = 0.40207616\n",
      "Iteration 2270, loss = 0.14666282\n",
      "Iteration 1353, loss = 0.19494876\n",
      "Iteration 174, loss = 0.40155373\n",
      "Iteration 284, loss = 0.43091121\n",
      "Iteration 339, loss = 0.34624008\n",
      "Iteration 1143, loss = 0.28788877\n",
      "Iteration 175, loss = 0.40107556\n",
      "Iteration 2271, loss = 0.14660825\n",
      "Iteration 1837, loss = 0.20062898\n",
      "Iteration 1354, loss = 0.19462395\n",
      "Iteration 2272, loss = 0.14653351\n",
      "Iteration 340, loss = 0.34610525\n",
      "Iteration 2273, loss = 0.14641521\n",
      "Iteration 341, loss = 0.34597978\n",
      "Iteration 1355, loss = 0.19440449\n",
      "Iteration 1144, loss = 0.28776267\n",
      "Iteration 176, loss = 0.40060520\n",
      "Iteration 177, loss = 0.40005982\n",
      "Iteration 178, loss = 0.39964207\n",
      "Iteration 1356, loss = 0.19425272\n",
      "Iteration 1230, loss = 0.22737129\n",
      "Iteration 179, loss = 0.39916066\n",
      "Iteration 342, loss = 0.34587400\n",
      "Iteration 180, loss = 0.39865494\n",
      "Iteration 2274, loss = 0.14633602\n",
      "Iteration 181, loss = 0.39823396\n",
      "Iteration 1838, loss = 0.20052913\n",
      "Iteration 182, loss = 0.39776571\n",
      "Iteration 183, loss = 0.39739185\n",
      "Iteration 184, loss = 0.39685507\n",
      "Iteration 343, loss = 0.34573853\n",
      "Iteration 185, loss = 0.39639592\n",
      "Iteration 285, loss = 0.43054441\n",
      "Iteration 1145, loss = 0.28765624\n",
      "Iteration 1839, loss = 0.20048101\n",
      "Iteration 344, loss = 0.34561868\n",
      "Iteration 2275, loss = 0.14633742\n",
      "Iteration 345, loss = 0.34548256\n",
      "Iteration 1357, loss = 0.19400418\n",
      "Iteration 1146, loss = 0.28752474\n",
      "Iteration 286, loss = 0.43029907\n",
      "Iteration 186, loss = 0.39600843\n",
      "Iteration 2276, loss = 0.14620135\n",
      "Iteration 1231, loss = 0.22713291\n",
      "Iteration 346, loss = 0.34537120\n",
      "Iteration 1358, loss = 0.19385620\n",
      "Iteration 1840, loss = 0.20036379\n",
      "Iteration 187, loss = 0.39553161\n",
      "Iteration 1359, loss = 0.19377553\n",
      "Iteration 188, loss = 0.39513494\n",
      "Iteration 1841, loss = 0.20031851\n",
      "Iteration 1147, loss = 0.28743411\n",
      "Iteration 189, loss = 0.39466669\n",
      "Iteration 2277, loss = 0.14617700\n",
      "Iteration 347, loss = 0.34524908\n",
      "Iteration 1360, loss = 0.19350130\n",
      "Iteration 190, loss = 0.39425547\n",
      "Iteration 1361, loss = 0.19349665\n",
      "Iteration 348, loss = 0.34512734\n",
      "Iteration 287, loss = 0.43001386\n",
      "Iteration 1362, loss = 0.19309330\n",
      "Iteration 349, loss = 0.34501115\n",
      "Iteration 1148, loss = 0.28726247\n",
      "Iteration 191, loss = 0.39385144\n",
      "Iteration 192, loss = 0.39339626\n",
      "Iteration 350, loss = 0.34488495\n",
      "Iteration 193, loss = 0.39301810\n",
      "Iteration 1363, loss = 0.19290310\n",
      "Iteration 351, loss = 0.34476833\n",
      "Iteration 194, loss = 0.39255532\n",
      "Iteration 1149, loss = 0.28715551\n",
      "Iteration 195, loss = 0.39218219\n",
      "Iteration 352, loss = 0.34467314\n",
      "Iteration 1364, loss = 0.19279210\n",
      "Iteration 196, loss = 0.39174811\n",
      "Iteration 2278, loss = 0.14594569\n",
      "Iteration 1842, loss = 0.20009760\n",
      "Iteration 2279, loss = 0.14591007\n",
      "Iteration 353, loss = 0.34453502\n",
      "Iteration 1232, loss = 0.22708923\n",
      "Iteration 354, loss = 0.34442127\n",
      "Iteration 288, loss = 0.42977481\n",
      "Iteration 197, loss = 0.39135894\n",
      "Iteration 1150, loss = 0.28702848\n",
      "Iteration 198, loss = 0.39095972\n",
      "Iteration 355, loss = 0.34431040\n",
      "Iteration 199, loss = 0.39059952\n",
      "Iteration 2280, loss = 0.14582659\n",
      "Iteration 200, loss = 0.39016051\n",
      "Iteration 356, loss = 0.34416799\n",
      "Iteration 1843, loss = 0.19999461\n",
      "Iteration 201, loss = 0.38978287\n",
      "Iteration 1365, loss = 0.19260257\n",
      "Iteration 202, loss = 0.38940204\n",
      "Iteration 203, loss = 0.38897745\n",
      "Iteration 2281, loss = 0.14576988\n",
      "Iteration 204, loss = 0.38863278\n",
      "Iteration 205, loss = 0.38827446\n",
      "Iteration 206, loss = 0.38787588\n",
      "Iteration 1233, loss = 0.22690343\n",
      "Iteration 1844, loss = 0.19996268\n",
      "Iteration 1366, loss = 0.19237931\n",
      "Iteration 357, loss = 0.34404556\n",
      "Iteration 207, loss = 0.38749311\n",
      "Iteration 289, loss = 0.42952458\n",
      "Iteration 1151, loss = 0.28692171\n",
      "Iteration 208, loss = 0.38713220\n",
      "Iteration 1367, loss = 0.19220332\n",
      "Iteration 358, loss = 0.34395779\n",
      "Iteration 209, loss = 0.38674468\n",
      "Iteration 1234, loss = 0.22666518\n",
      "Iteration 1368, loss = 0.19199212\n",
      "Iteration 1845, loss = 0.19977532\n",
      "Iteration 210, loss = 0.38645770\n",
      "Iteration 211, loss = 0.38605778\n",
      "Iteration 359, loss = 0.34381804\n",
      "Iteration 212, loss = 0.38569459\n",
      "Iteration 213, loss = 0.38534168\n",
      "Iteration 2282, loss = 0.14563453\n",
      "Iteration 214, loss = 0.38499209\n",
      "Iteration 1846, loss = 0.19965227\n",
      "Iteration 215, loss = 0.38464539\n",
      "Iteration 1235, loss = 0.22639337\n",
      "Iteration 1152, loss = 0.28677090\n",
      "Iteration 290, loss = 0.42924404\n",
      "Iteration 216, loss = 0.38428712\n",
      "Iteration 360, loss = 0.34370109\n",
      "Iteration 1369, loss = 0.19193414\n",
      "Iteration 361, loss = 0.34358361\n",
      "Iteration 1370, loss = 0.19207327\n",
      "Iteration 362, loss = 0.34349635\n",
      "Iteration 217, loss = 0.38396141\n",
      "Iteration 1153, loss = 0.28673074\n",
      "Iteration 1236, loss = 0.22625974\n",
      "Iteration 1847, loss = 0.19954213\n",
      "Iteration 1371, loss = 0.19145384\n",
      "Iteration 218, loss = 0.38360668\n",
      "Iteration 363, loss = 0.34336374\n",
      "Iteration 219, loss = 0.38328305\n",
      "Iteration 1154, loss = 0.28653525\n",
      "Iteration 220, loss = 0.38294187\n",
      "Iteration 291, loss = 0.42902014\n",
      "Iteration 1372, loss = 0.19137588\n",
      "Iteration 221, loss = 0.38265177\n",
      "Iteration 364, loss = 0.34324324\n",
      "Iteration 222, loss = 0.38227773\n",
      "Iteration 1848, loss = 0.19942236\n",
      "Iteration 1155, loss = 0.28641351\n",
      "Iteration 1373, loss = 0.19128061\n",
      "Iteration 223, loss = 0.38197331\n",
      "Iteration 365, loss = 0.34312171\n",
      "Iteration 1237, loss = 0.22622989\n",
      "Iteration 224, loss = 0.38163995\n",
      "Iteration 366, loss = 0.34300769\n",
      "Iteration 1156, loss = 0.28629205\n",
      "Iteration 292, loss = 0.42877169\n",
      "Iteration 225, loss = 0.38130474\n",
      "Iteration 367, loss = 0.34289595\n",
      "Iteration 2283, loss = 0.14558029\n",
      "Iteration 226, loss = 0.38098683\n",
      "Iteration 1374, loss = 0.19089553\n",
      "Iteration 227, loss = 0.38071205\n",
      "Iteration 368, loss = 0.34279123\n",
      "Iteration 228, loss = 0.38035165\n",
      "Iteration 1849, loss = 0.19931718\n",
      "Iteration 229, loss = 0.38004320\n",
      "Iteration 369, loss = 0.34267669\n",
      "Iteration 230, loss = 0.37972547\n",
      "Iteration 1375, loss = 0.19081759\n",
      "Iteration 231, loss = 0.37944519\n",
      "Iteration 370, loss = 0.34256586\n",
      "Iteration 232, loss = 0.37917803\n",
      "Iteration 1238, loss = 0.22582044\n",
      "Iteration 233, loss = 0.37881637\n",
      "Iteration 1157, loss = 0.28617093\n",
      "Iteration 293, loss = 0.42852389\n",
      "Iteration 371, loss = 0.34244693\n",
      "Iteration 1850, loss = 0.19921406\n",
      "Iteration 234, loss = 0.37851036\n",
      "Iteration 1376, loss = 0.19053343\n",
      "Iteration 235, loss = 0.37818033\n",
      "Iteration 372, loss = 0.34234963\n",
      "Iteration 1158, loss = 0.28602291\n",
      "Iteration 236, loss = 0.37790835\n",
      "Iteration 1377, loss = 0.19036669\n",
      "Iteration 237, loss = 0.37760145\n",
      "Iteration 1851, loss = 0.19912153\n",
      "Iteration 238, loss = 0.37729560\n",
      "Iteration 294, loss = 0.42840032\n",
      "Iteration 239, loss = 0.37701177\n",
      "Iteration 1159, loss = 0.28590929\n",
      "Iteration 373, loss = 0.34223806\n",
      "Iteration 1378, loss = 0.19022430\n",
      "Iteration 1239, loss = 0.22587173\n",
      "Iteration 240, loss = 0.37673202\n",
      "Iteration 1852, loss = 0.19896189\n",
      "Iteration 374, loss = 0.34212161\n",
      "Iteration 1379, loss = 0.18996925\n",
      "Iteration 375, loss = 0.34200974\n",
      "Iteration 241, loss = 0.37641073\n",
      "Iteration 1160, loss = 0.28577880\n",
      "Iteration 242, loss = 0.37613748\n",
      "Iteration 376, loss = 0.34191277\n",
      "Iteration 377, loss = 0.34178075\n",
      "Iteration 1380, loss = 0.18982835\n",
      "Iteration 243, loss = 0.37583845\n",
      "Iteration 1853, loss = 0.19896191\n",
      "Iteration 244, loss = 0.37556378\n",
      "Iteration 1161, loss = 0.28565599\n",
      "Iteration 245, loss = 0.37530316\n",
      "Iteration 1381, loss = 0.18960385\n",
      "Iteration 378, loss = 0.34167954\n",
      "Iteration 2284, loss = 0.14548072\n",
      "Iteration 1240, loss = 0.22543716\n",
      "Iteration 246, loss = 0.37499970\n",
      "Iteration 247, loss = 0.37474251\n",
      "Iteration 295, loss = 0.42805739\n",
      "Iteration 248, loss = 0.37445421\n",
      "Iteration 1382, loss = 0.18943554\n",
      "Iteration 379, loss = 0.34157296\n",
      "Iteration 380, loss = 0.34146180Iteration 1162, loss = 0.28555271\n",
      "\n",
      "Iteration 249, loss = 0.37420116\n",
      "Iteration 1854, loss = 0.19880980\n",
      "Iteration 1383, loss = 0.18921026\n",
      "Iteration 381, loss = 0.34136098\n",
      "Iteration 250, loss = 0.37388908\n",
      "Iteration 1241, loss = 0.22528394\n",
      "Iteration 1384, loss = 0.18903648\n",
      "Iteration 382, loss = 0.34125582\n",
      "Iteration 251, loss = 0.37363276\n",
      "Iteration 296, loss = 0.42777313\n",
      "Iteration 1855, loss = 0.19878822\n",
      "Iteration 252, loss = 0.37332876\n",
      "Iteration 253, loss = 0.37305991\n",
      "Iteration 1385, loss = 0.18898475\n",
      "Iteration 383, loss = 0.34112869\n",
      "Iteration 254, loss = 0.37278889\n",
      "Iteration 2285, loss = 0.14535691\n",
      "Iteration 255, loss = 0.37251053\n",
      "Iteration 384, loss = 0.34102539\n",
      "Iteration 1163, loss = 0.28545222\n",
      "Iteration 256, loss = 0.37223146\n",
      "Iteration 1856, loss = 0.19859983\n",
      "Iteration 257, loss = 0.37197223\n",
      "Iteration 385, loss = 0.34090926\n",
      "Iteration 258, loss = 0.37178535\n",
      "Iteration 1386, loss = 0.18873882\n",
      "Iteration 259, loss = 0.37144956\n",
      "Iteration 1242, loss = 0.22507610\n",
      "Iteration 260, loss = 0.37120483\n",
      "Iteration 2286, loss = 0.14529930\n",
      "Iteration 1857, loss = 0.19846599\n",
      "Iteration 261, loss = 0.37089353\n",
      "Iteration 386, loss = 0.34081215\n",
      "Iteration 297, loss = 0.42761229\n",
      "Iteration 1164, loss = 0.28527177\n",
      "Iteration 2287, loss = 0.14521219\n",
      "Iteration 1387, loss = 0.18845257\n",
      "Iteration 262, loss = 0.37064628\n",
      "Iteration 263, loss = 0.37038240\n",
      "Iteration 387, loss = 0.34068449\n",
      "Iteration 1858, loss = 0.19837979\n",
      "Iteration 1388, loss = 0.18831626\n",
      "Iteration 264, loss = 0.37014622\n",
      "Iteration 265, loss = 0.36989928\n",
      "Iteration 388, loss = 0.34061734\n",
      "Iteration 266, loss = 0.36962595\n",
      "Iteration 2288, loss = 0.14510772\n",
      "Iteration 1165, loss = 0.28515680\n",
      "Iteration 267, loss = 0.36938927\n",
      "Iteration 1859, loss = 0.19834223\n",
      "Iteration 268, loss = 0.36914800\n",
      "Iteration 298, loss = 0.42732618\n",
      "Iteration 269, loss = 0.36887698\n",
      "Iteration 1243, loss = 0.22497363\n",
      "Iteration 1389, loss = 0.18819166\n",
      "Iteration 2289, loss = 0.14521358\n",
      "Iteration 270, loss = 0.36862537\n",
      "Iteration 271, loss = 0.36838710\n",
      "Iteration 1166, loss = 0.28504900\n",
      "Iteration 1860, loss = 0.19825052\n",
      "Iteration 1390, loss = 0.18831995\n",
      "Iteration 272, loss = 0.36813418\n",
      "Iteration 2290, loss = 0.14492024\n",
      "Iteration 273, loss = 0.36789582\n",
      "Iteration 1167, loss = 0.28491793\n",
      "Iteration 389, loss = 0.34048699\n",
      "Iteration 1244, loss = 0.22470988\n",
      "Iteration 299, loss = 0.42708872\n",
      "Iteration 274, loss = 0.36770280Iteration 390, loss = 0.34040450\n",
      "\n",
      "Iteration 1861, loss = 0.19800937\n",
      "Iteration 275, loss = 0.36742387\n",
      "Iteration 2291, loss = 0.14484662\n",
      "Iteration 391, loss = 0.34027857\n",
      "Iteration 276, loss = 0.36716346\n",
      "Iteration 277, loss = 0.36690775\n",
      "Iteration 392, loss = 0.34016580\n",
      "Iteration 278, loss = 0.36668771\n",
      "Iteration 1391, loss = 0.18777447\n",
      "Iteration 393, loss = 0.34005034\n",
      "Iteration 2292, loss = 0.14479605\n",
      "Iteration 300, loss = 0.42679061\n",
      "Iteration 1245, loss = 0.22444179\n",
      "Iteration 394, loss = 0.33995822\n",
      "Iteration 279, loss = 0.36644981\n",
      "Iteration 1168, loss = 0.28478382\n",
      "Iteration 1862, loss = 0.19791370\n",
      "Iteration 1392, loss = 0.18751470\n",
      "Iteration 395, loss = 0.33984571\n",
      "Iteration 280, loss = 0.36621593\n",
      "Iteration 2293, loss = 0.14471625\n",
      "Iteration 396, loss = 0.33974814\n",
      "Iteration 281, loss = 0.36597626\n",
      "Iteration 1246, loss = 0.22437484\n",
      "Iteration 1169, loss = 0.28464935\n",
      "Iteration 282, loss = 0.36572996\n",
      "Iteration 301, loss = 0.42657969\n",
      "Iteration 397, loss = 0.33962892\n",
      "Iteration 283, loss = 0.36554395\n",
      "Iteration 2294, loss = 0.14460619\n",
      "Iteration 284, loss = 0.36527785\n",
      "Iteration 398, loss = 0.33954024\n",
      "Iteration 1170, loss = 0.28454005\n",
      "Iteration 1393, loss = 0.18785587\n",
      "Iteration 2295, loss = 0.14454961\n",
      "Iteration 1863, loss = 0.19782733\n",
      "Iteration 285, loss = 0.36505586\n",
      "Iteration 2296, loss = 0.14445174\n",
      "Iteration 1394, loss = 0.18727589\n",
      "Iteration 399, loss = 0.33943041\n",
      "Iteration 302, loss = 0.42638736\n",
      "Iteration 1864, loss = 0.19769714\n",
      "Iteration 1395, loss = 0.18716271\n",
      "Iteration 400, loss = 0.33931970\n",
      "Iteration 1247, loss = 0.22408073\n",
      "Iteration 286, loss = 0.36481833\n",
      "Iteration 1865, loss = 0.19757317\n",
      "Iteration 287, loss = 0.36459062\n",
      "Iteration 288, loss = 0.36436271\n",
      "Iteration 1396, loss = 0.18700105\n",
      "Iteration 401, loss = 0.33925024\n",
      "Iteration 289, loss = 0.36415354\n",
      "Iteration 402, loss = 0.33911866\n",
      "Iteration 1397, loss = 0.18661194\n",
      "Iteration 303, loss = 0.42605818\n",
      "Iteration 290, loss = 0.36393407\n",
      "Iteration 403, loss = 0.33901852\n",
      "Iteration 291, loss = 0.36371593\n",
      "Iteration 404, loss = 0.33891909\n",
      "Iteration 1866, loss = 0.19745369\n",
      "Iteration 292, loss = 0.36350651\n",
      "Iteration 405, loss = 0.33886699\n",
      "Iteration 1248, loss = 0.22403562\n",
      "Iteration 2297, loss = 0.14442793\n",
      "Iteration 1171, loss = 0.28448744\n",
      "Iteration 406, loss = 0.33874851\n",
      "Iteration 293, loss = 0.36326225\n",
      "Iteration 1867, loss = 0.19737591\n",
      "Iteration 1172, loss = 0.28429512\n",
      "Iteration 2298, loss = 0.14425836\n",
      "Iteration 407, loss = 0.33862183\n",
      "Iteration 294, loss = 0.36303590\n",
      "Iteration 304, loss = 0.42587917\n",
      "Iteration 295, loss = 0.36283767\n",
      "Iteration 1398, loss = 0.18668405\n",
      "Iteration 408, loss = 0.33850935\n",
      "Iteration 296, loss = 0.36267202\n",
      "Iteration 1249, loss = 0.22387169\n",
      "Iteration 2299, loss = 0.14415689\n",
      "Iteration 409, loss = 0.33842217\n",
      "Iteration 297, loss = 0.36241851\n",
      "Iteration 1399, loss = 0.18638616\n",
      "Iteration 1173, loss = 0.28415419\n",
      "Iteration 1868, loss = 0.19727379\n",
      "Iteration 298, loss = 0.36218764\n",
      "Iteration 410, loss = 0.33832604\n",
      "Iteration 1400, loss = 0.18612477\n",
      "Iteration 299, loss = 0.36197032\n",
      "Iteration 411, loss = 0.33820971\n",
      "Iteration 2300, loss = 0.14420008\n",
      "Iteration 1869, loss = 0.19717363\n",
      "Iteration 305, loss = 0.42560980\n",
      "Iteration 300, loss = 0.36176605\n",
      "Iteration 301, loss = 0.36155045\n",
      "Iteration 412, loss = 0.33810806\n",
      "Iteration 302, loss = 0.36136503\n",
      "Iteration 303, loss = 0.36114988\n",
      "Iteration 1174, loss = 0.28402981\n",
      "Iteration 304, loss = 0.36094419\n",
      "Iteration 2301, loss = 0.14404125\n",
      "Iteration 413, loss = 0.33801586\n",
      "Iteration 305, loss = 0.36072515\n",
      "Iteration 1401, loss = 0.18606509\n",
      "Iteration 306, loss = 0.36052609\n",
      "Iteration 414, loss = 0.33792729\n",
      "Iteration 1870, loss = 0.19701543\n",
      "Iteration 307, loss = 0.36037000\n",
      "Iteration 306, loss = 0.42537661\n",
      "Iteration 415, loss = 0.33782133\n",
      "Iteration 1175, loss = 0.28392177\n",
      "Iteration 1250, loss = 0.22358109\n",
      "Iteration 2302, loss = 0.14398158\n",
      "Iteration 1402, loss = 0.18586796\n",
      "Iteration 308, loss = 0.36012976\n",
      "Iteration 309, loss = 0.35991897\n",
      "Iteration 2303, loss = 0.14382382\n",
      "Iteration 1871, loss = 0.19694569\n",
      "Iteration 310, loss = 0.35971286\n",
      "Iteration 416, loss = 0.33771829\n",
      "Iteration 311, loss = 0.35952096\n",
      "Iteration 2304, loss = 0.14379297\n",
      "Iteration 312, loss = 0.35932149\n",
      "Iteration 313, loss = 0.35912504\n",
      "Iteration 2305, loss = 0.14375961\n",
      "Iteration 1176, loss = 0.28379875\n",
      "Iteration 1403, loss = 0.18564417\n",
      "Iteration 314, loss = 0.35894570\n",
      "Iteration 417, loss = 0.33764950\n",
      "Iteration 315, loss = 0.35872325\n",
      "Iteration 2306, loss = 0.14362703\n",
      "Iteration 1872, loss = 0.19683889\n",
      "Iteration 307, loss = 0.42515839\n",
      "Iteration 316, loss = 0.35855627\n",
      "Iteration 418, loss = 0.33756871\n",
      "Iteration 1177, loss = 0.28367345\n",
      "Iteration 317, loss = 0.35832047\n",
      "Iteration 2307, loss = 0.14362059\n",
      "Iteration 1873, loss = 0.19675614\n",
      "Iteration 318, loss = 0.35813515\n",
      "Iteration 2308, loss = 0.14340170\n",
      "Iteration 319, loss = 0.35794536\n",
      "Iteration 1404, loss = 0.18537189\n",
      "Iteration 320, loss = 0.35775048\n",
      "Iteration 419, loss = 0.33741880\n",
      "Iteration 321, loss = 0.35754575\n",
      "Iteration 1178, loss = 0.28356909\n",
      "Iteration 2309, loss = 0.14340845\n",
      "Iteration 322, loss = 0.35739524\n",
      "Iteration 323, loss = 0.35716909\n",
      "Iteration 324, loss = 0.35699991\n",
      "Iteration 420, loss = 0.33732130\n",
      "Iteration 1405, loss = 0.18534961\n",
      "Iteration 1874, loss = 0.19661944\n",
      "Iteration 2310, loss = 0.14325264\n",
      "Iteration 1179, loss = 0.28349776\n",
      "Iteration 325, loss = 0.35680199\n",
      "Iteration 421, loss = 0.33722325\n",
      "Iteration 308, loss = 0.42483346\n",
      "Iteration 326, loss = 0.35660945\n",
      "Iteration 2311, loss = 0.14339443\n",
      "Iteration 422, loss = 0.33715158\n",
      "Iteration 1180, loss = 0.28330810\n",
      "Iteration 423, loss = 0.33703465\n",
      "Iteration 1251, loss = 0.22341933\n",
      "Iteration 327, loss = 0.35639914\n",
      "Iteration 2312, loss = 0.14305914\n",
      "Iteration 328, loss = 0.35628782\n",
      "Iteration 1406, loss = 0.18505194\n",
      "Iteration 424, loss = 0.33694249\n",
      "Iteration 329, loss = 0.35603241\n",
      "Iteration 425, loss = 0.33687348\n",
      "Iteration 330, loss = 0.35587700\n",
      "Iteration 1181, loss = 0.28317956\n",
      "Iteration 331, loss = 0.35572109\n",
      "Iteration 309, loss = 0.42465935\n",
      "Iteration 1407, loss = 0.18487745\n",
      "Iteration 2313, loss = 0.14305926\n",
      "Iteration 332, loss = 0.35547710\n",
      "Iteration 333, loss = 0.35533488\n",
      "Iteration 1875, loss = 0.19648439\n",
      "Iteration 1408, loss = 0.18461722\n",
      "Iteration 334, loss = 0.35513847\n",
      "Iteration 426, loss = 0.33675790\n",
      "Iteration 2314, loss = 0.14294354\n",
      "Iteration 1252, loss = 0.22319620\n",
      "Iteration 335, loss = 0.35493844\n",
      "Iteration 310, loss = 0.42437163\n",
      "Iteration 336, loss = 0.35475309\n",
      "Iteration 1182, loss = 0.28308165\n",
      "Iteration 2315, loss = 0.14284308\n",
      "Iteration 427, loss = 0.33666194\n",
      "Iteration 337, loss = 0.35458175\n",
      "Iteration 1409, loss = 0.18452978\n",
      "Iteration 428, loss = 0.33655586\n",
      "Iteration 2316, loss = 0.14280810\n",
      "Iteration 338, loss = 0.35440594\n",
      "Iteration 429, loss = 0.33646819\n",
      "Iteration 1876, loss = 0.19641932\n",
      "Iteration 1253, loss = 0.22318961\n",
      "Iteration 339, loss = 0.35423237\n",
      "Iteration 340, loss = 0.35404999\n",
      "Iteration 2317, loss = 0.14272042\n",
      "Iteration 311, loss = 0.42415827\n",
      "Iteration 341, loss = 0.35390796\n",
      "Iteration 1183, loss = 0.28294138\n",
      "Iteration 430, loss = 0.33635738\n",
      "Iteration 2318, loss = 0.14256473\n",
      "Iteration 1410, loss = 0.18450739\n",
      "Iteration 342, loss = 0.35369435\n",
      "Iteration 431, loss = 0.33627275\n",
      "Iteration 2319, loss = 0.14248810\n",
      "Iteration 343, loss = 0.35353125\n",
      "Iteration 432, loss = 0.33618155\n",
      "Iteration 2320, loss = 0.14262602\n",
      "Iteration 344, loss = 0.35334452\n",
      "Iteration 1877, loss = 0.19626858\n",
      "Iteration 1411, loss = 0.18405910\n",
      "Iteration 345, loss = 0.35320596\n",
      "Iteration 346, loss = 0.35302017\n",
      "Iteration 1254, loss = 0.22280402\n",
      "Iteration 347, loss = 0.35284197\n",
      "Iteration 433, loss = 0.33607594\n",
      "Iteration 2321, loss = 0.14233392\n",
      "Iteration 1412, loss = 0.18391132\n",
      "Iteration 1184, loss = 0.28280934\n",
      "Iteration 348, loss = 0.35265012\n",
      "Iteration 434, loss = 0.33598262\n",
      "Iteration 1878, loss = 0.19618038\n",
      "Iteration 435, loss = 0.33590305\n",
      "Iteration 349, loss = 0.35251218\n",
      "Iteration 1413, loss = 0.18398955\n",
      "Iteration 2322, loss = 0.14229489\n",
      "Iteration 350, loss = 0.35233661\n",
      "Iteration 436, loss = 0.33579942\n",
      "Iteration 351, loss = 0.35217719\n",
      "Iteration 1255, loss = 0.22287814\n",
      "Iteration 1185, loss = 0.28264463\n",
      "Iteration 352, loss = 0.35199591\n",
      "Iteration 1414, loss = 0.18357912\n",
      "Iteration 437, loss = 0.33571503\n",
      "Iteration 1879, loss = 0.19617710\n",
      "Iteration 2323, loss = 0.14217272\n",
      "Iteration 353, loss = 0.35187181\n",
      "Iteration 1186, loss = 0.28253452\n",
      "Iteration 354, loss = 0.35166695\n",
      "Iteration 438, loss = 0.33564155\n",
      "Iteration 2324, loss = 0.14219169\n",
      "Iteration 1415, loss = 0.18346271\n",
      "Iteration 355, loss = 0.35153273\n",
      "Iteration 439, loss = 0.33552793\n",
      "Iteration 1256, loss = 0.22257391\n",
      "Iteration 356, loss = 0.35134807\n",
      "Iteration 440, loss = 0.33542380\n",
      "Iteration 1880, loss = 0.19600220\n",
      "Iteration 2325, loss = 0.14196970\n",
      "Iteration 357, loss = 0.35116151\n",
      "Iteration 1416, loss = 0.18324070\n",
      "Iteration 441, loss = 0.33533104\n",
      "Iteration 358, loss = 0.35101518\n",
      "Iteration 1187, loss = 0.28242643\n",
      "Iteration 2326, loss = 0.14192144\n",
      "Iteration 359, loss = 0.35085129\n",
      "Iteration 1257, loss = 0.22269664\n",
      "Iteration 360, loss = 0.35071083\n",
      "Iteration 442, loss = 0.33525419\n",
      "Iteration 2327, loss = 0.14183168\n",
      "Iteration 361, loss = 0.35057748\n",
      "Iteration 1417, loss = 0.18300368\n",
      "Iteration 1881, loss = 0.19593435\n",
      "Iteration 443, loss = 0.33514828\n",
      "Iteration 362, loss = 0.35037438\n",
      "Iteration 444, loss = 0.33507127\n",
      "Iteration 363, loss = 0.35022075\n",
      "Iteration 2328, loss = 0.14171998\n",
      "Iteration 1258, loss = 0.22210177\n",
      "Iteration 364, loss = 0.35004216\n",
      "Iteration 445, loss = 0.33497813\n",
      "Iteration 365, loss = 0.34991245\n",
      "Iteration 1188, loss = 0.28229895\n",
      "Iteration 1418, loss = 0.18281940\n",
      "Iteration 2329, loss = 0.14168719\n",
      "Iteration 366, loss = 0.34976036\n",
      "Iteration 1882, loss = 0.19578735\n",
      "Iteration 446, loss = 0.33488748\n",
      "Iteration 2330, loss = 0.14162237\n",
      "Iteration 367, loss = 0.34967271\n",
      "Iteration 1419, loss = 0.18282259\n",
      "Iteration 447, loss = 0.33478575\n",
      "Iteration 368, loss = 0.34942886\n",
      "Iteration 2331, loss = 0.14154620\n",
      "Iteration 1189, loss = 0.28215018\n",
      "Iteration 448, loss = 0.33470093\n",
      "Iteration 2332, loss = 0.14143828\n",
      "Iteration 1420, loss = 0.18270412\n",
      "Iteration 369, loss = 0.34928020\n",
      "Iteration 1883, loss = 0.19575842\n",
      "Iteration 449, loss = 0.33460057\n",
      "Iteration 370, loss = 0.34916486\n",
      "Iteration 2333, loss = 0.14134796\n",
      "Iteration 371, loss = 0.34898589\n",
      "Iteration 1259, loss = 0.22188056\n",
      "Iteration 450, loss = 0.33451659\n",
      "Iteration 1421, loss = 0.18260902\n",
      "Iteration 372, loss = 0.34881573\n",
      "Iteration 1190, loss = 0.28211202\n",
      "Iteration 373, loss = 0.34864078\n",
      "Iteration 1884, loss = 0.19558821\n",
      "Iteration 374, loss = 0.34851995\n",
      "Iteration 2334, loss = 0.14123245\n",
      "Iteration 451, loss = 0.33443884\n",
      "Iteration 375, loss = 0.34834541\n",
      "Iteration 1422, loss = 0.18220246\n",
      "Iteration 376, loss = 0.34819955\n",
      "Iteration 2335, loss = 0.14130568\n",
      "Iteration 1191, loss = 0.28195407\n",
      "Iteration 377, loss = 0.34805440\n",
      "Iteration 1885, loss = 0.19547297\n",
      "Iteration 378, loss = 0.34788397\n",
      "Iteration 452, loss = 0.33433765\n",
      "Iteration 1260, loss = 0.22176582\n",
      "Iteration 379, loss = 0.34773909\n",
      "Iteration 380, loss = 0.34760647\n",
      "Iteration 1423, loss = 0.18203478\n",
      "Iteration 2336, loss = 0.14109113\n",
      "Iteration 381, loss = 0.34745571\n",
      "Iteration 453, loss = 0.33425431\n",
      "Iteration 1886, loss = 0.19532219\n",
      "Iteration 382, loss = 0.34729836\n",
      "Iteration 1192, loss = 0.28189349\n",
      "Iteration 383, loss = 0.34714691\n",
      "Iteration 2337, loss = 0.14103334\n",
      "Iteration 454, loss = 0.33418720\n",
      "Iteration 384, loss = 0.34701066\n",
      "Iteration 2338, loss = 0.14099512\n",
      "Iteration 1261, loss = 0.22178520\n",
      "Iteration 455, loss = 0.33406682\n",
      "Iteration 2339, loss = 0.14085758\n",
      "Iteration 456, loss = 0.33398328\n",
      "Iteration 1424, loss = 0.18187129\n",
      "Iteration 385, loss = 0.34683576\n",
      "Iteration 457, loss = 0.33388567\n",
      "Iteration 2340, loss = 0.14076032\n",
      "Iteration 1887, loss = 0.19546627\n",
      "Iteration 386, loss = 0.34669724\n",
      "Iteration 1262, loss = 0.22175373\n",
      "Iteration 2341, loss = 0.14068904\n",
      "Iteration 1193, loss = 0.28176707\n",
      "Iteration 1425, loss = 0.18177280\n",
      "Iteration 387, loss = 0.34654004\n",
      "Iteration 458, loss = 0.33379494\n",
      "Iteration 388, loss = 0.34640471\n",
      "Iteration 2342, loss = 0.14064190\n",
      "Iteration 389, loss = 0.34625769\n",
      "Iteration 390, loss = 0.34614939\n",
      "Iteration 1426, loss = 0.18144468\n",
      "Iteration 1194, loss = 0.28153695\n",
      "Iteration 2343, loss = 0.14055140\n",
      "Iteration 391, loss = 0.34596030\n",
      "Iteration 392, loss = 0.34584846\n",
      "Iteration 459, loss = 0.33370950\n",
      "Iteration 393, loss = 0.34570253\n",
      "Iteration 460, loss = 0.33361426\n",
      "Iteration 1888, loss = 0.19520781\n",
      "Iteration 2344, loss = 0.14039616\n",
      "Iteration 1427, loss = 0.18131783\n",
      "Iteration 394, loss = 0.34554455\n",
      "Iteration 1195, loss = 0.28142652\n",
      "Iteration 395, loss = 0.34537933\n",
      "Iteration 461, loss = 0.33352536\n",
      "Iteration 396, loss = 0.34525213\n",
      "Iteration 1263, loss = 0.22115267\n",
      "Iteration 397, loss = 0.34509849\n",
      "Iteration 2345, loss = 0.14035765\n",
      "Iteration 398, loss = 0.34496700\n",
      "Iteration 1428, loss = 0.18143332\n",
      "Iteration 462, loss = 0.33344181\n",
      "Iteration 399, loss = 0.34479758\n",
      "Iteration 1196, loss = 0.28129683\n",
      "Iteration 1889, loss = 0.19504262\n",
      "Iteration 400, loss = 0.34465166\n",
      "Iteration 1429, loss = 0.18091247\n",
      "Iteration 463, loss = 0.33334115\n",
      "Iteration 2346, loss = 0.14031355\n",
      "Iteration 401, loss = 0.34452688\n",
      "Iteration 312, loss = 0.42390034\n",
      "Iteration 1430, loss = 0.18077686\n",
      "Iteration 402, loss = 0.34436490\n",
      "Iteration 1264, loss = 0.22109854\n",
      "Iteration 464, loss = 0.33327009\n",
      "Iteration 1431, loss = 0.18057573\n",
      "Iteration 2347, loss = 0.14037641\n",
      "Iteration 465, loss = 0.33319434\n",
      "Iteration 403, loss = 0.34423546\n",
      "Iteration 404, loss = 0.34409507\n",
      "Iteration 313, loss = 0.42364662\n",
      "Iteration 1197, loss = 0.28122051\n",
      "Iteration 1890, loss = 0.19492930\n",
      "Iteration 466, loss = 0.33308447\n",
      "Iteration 1432, loss = 0.18045659\n",
      "Iteration 2348, loss = 0.14017197\n",
      "Iteration 405, loss = 0.34395054\n",
      "Iteration 1265, loss = 0.22079137\n",
      "Iteration 467, loss = 0.33298734\n",
      "Iteration 1433, loss = 0.18026136\n",
      "Iteration 406, loss = 0.34380088\n",
      "Iteration 407, loss = 0.34371214\n",
      "Iteration 468, loss = 0.33290041\n",
      "Iteration 2349, loss = 0.14003663\n",
      "Iteration 1891, loss = 0.19479980\n",
      "Iteration 1198, loss = 0.28103504\n",
      "Iteration 408, loss = 0.34353200\n",
      "Iteration 469, loss = 0.33281355\n",
      "Iteration 1434, loss = 0.18002751\n",
      "Iteration 314, loss = 0.42341285\n",
      "Iteration 470, loss = 0.33272547\n",
      "Iteration 1266, loss = 0.22059445\n",
      "Iteration 2350, loss = 0.13994132\n",
      "Iteration 409, loss = 0.34340774\n",
      "Iteration 1892, loss = 0.19473019\n",
      "Iteration 1435, loss = 0.17994073\n",
      "Iteration 410, loss = 0.34324264\n",
      "Iteration 471, loss = 0.33265340\n",
      "Iteration 2351, loss = 0.13988754\n",
      "Iteration 1199, loss = 0.28090452\n",
      "Iteration 1893, loss = 0.19458908\n",
      "Iteration 1436, loss = 0.17968678\n",
      "Iteration 472, loss = 0.33254274\n",
      "Iteration 1267, loss = 0.22044087\n",
      "Iteration 411, loss = 0.34312329\n",
      "Iteration 2352, loss = 0.13979311\n",
      "Iteration 473, loss = 0.33247167\n",
      "Iteration 315, loss = 0.42321900\n",
      "Iteration 2353, loss = 0.13996814\n",
      "Iteration 412, loss = 0.34298648\n",
      "Iteration 474, loss = 0.33239004\n",
      "Iteration 413, loss = 0.34285301\n",
      "Iteration 1200, loss = 0.28082255\n",
      "Iteration 414, loss = 0.34279183\n",
      "Iteration 2354, loss = 0.13961855\n",
      "Iteration 316, loss = 0.42302542\n",
      "Iteration 1437, loss = 0.17954700\n",
      "Iteration 1894, loss = 0.19454665\n",
      "Iteration 475, loss = 0.33229697\n",
      "Iteration 415, loss = 0.34260738\n",
      "Iteration 1201, loss = 0.28066949\n",
      "Iteration 416, loss = 0.34248367\n",
      "Iteration 2355, loss = 0.13953425\n",
      "Iteration 1438, loss = 0.17931784\n",
      "Iteration 1268, loss = 0.22020321\n",
      "Iteration 476, loss = 0.33220525\n",
      "Iteration 417, loss = 0.34231175\n",
      "Iteration 2356, loss = 0.13950829\n",
      "Iteration 1439, loss = 0.17947000\n",
      "Iteration 1895, loss = 0.19445525\n",
      "Iteration 1202, loss = 0.28054852\n",
      "Iteration 418, loss = 0.34217613\n",
      "Iteration 419, loss = 0.34205449\n",
      "Iteration 477, loss = 0.33211839Iteration 2357, loss = 0.13938111\n",
      "\n",
      "Iteration 317, loss = 0.42274185\n",
      "Iteration 420, loss = 0.34190841\n",
      "Iteration 1896, loss = 0.19436651\n",
      "Iteration 1440, loss = 0.17907974\n",
      "Iteration 1269, loss = 0.22009384\n",
      "Iteration 478, loss = 0.33204293\n",
      "Iteration 421, loss = 0.34179636\n",
      "Iteration 2358, loss = 0.13946174\n",
      "Iteration 422, loss = 0.34168102\n",
      "Iteration 479, loss = 0.33194003\n",
      "Iteration 1203, loss = 0.28050213\n",
      "Iteration 2359, loss = 0.13920691\n",
      "Iteration 480, loss = 0.33186047\n",
      "Iteration 423, loss = 0.34152061\n",
      "Iteration 1897, loss = 0.19417160\n",
      "Iteration 424, loss = 0.34138544\n",
      "Iteration 318, loss = 0.42248761\n",
      "Iteration 481, loss = 0.33179066\n",
      "Iteration 425, loss = 0.34126849\n",
      "Iteration 1441, loss = 0.17894684\n",
      "Iteration 2360, loss = 0.13912970\n",
      "Iteration 426, loss = 0.34111323\n",
      "Iteration 427, loss = 0.34104491\n",
      "Iteration 2361, loss = 0.13906189\n",
      "Iteration 428, loss = 0.34088410\n",
      "Iteration 482, loss = 0.33170985\n",
      "Iteration 1442, loss = 0.17865144\n",
      "Iteration 429, loss = 0.34075792\n",
      "Iteration 430, loss = 0.34061971\n",
      "Iteration 1204, loss = 0.28034098\n",
      "Iteration 431, loss = 0.34047414\n",
      "Iteration 1443, loss = 0.17844295\n",
      "Iteration 483, loss = 0.33161245\n",
      "Iteration 1270, loss = 0.21985076\n",
      "Iteration 432, loss = 0.34037175\n",
      "Iteration 484, loss = 0.33153889\n",
      "Iteration 2362, loss = 0.13898798\n",
      "Iteration 1898, loss = 0.19402445\n",
      "Iteration 433, loss = 0.34024058\n",
      "Iteration 434, loss = 0.34014271\n",
      "Iteration 319, loss = 0.42227415\n",
      "Iteration 435, loss = 0.33996984\n",
      "Iteration 1444, loss = 0.17841935\n",
      "Iteration 485, loss = 0.33145391\n",
      "Iteration 436, loss = 0.33988983\n",
      "Iteration 2363, loss = 0.13887101\n",
      "Iteration 1899, loss = 0.19413684\n",
      "Iteration 486, loss = 0.33135152\n",
      "Iteration 437, loss = 0.33972930\n",
      "Iteration 1271, loss = 0.21968906\n",
      "Iteration 2364, loss = 0.13882281\n",
      "Iteration 1205, loss = 0.28016674\n",
      "Iteration 487, loss = 0.33128043\n",
      "Iteration 1445, loss = 0.17817807\n",
      "Iteration 488, loss = 0.33119624\n",
      "Iteration 438, loss = 0.33958902\n",
      "Iteration 320, loss = 0.42203057\n",
      "Iteration 2365, loss = 0.13879114\n",
      "Iteration 439, loss = 0.33947215\n",
      "Iteration 440, loss = 0.33934846\n",
      "Iteration 1900, loss = 0.19391703\n",
      "Iteration 1446, loss = 0.17828942\n",
      "Iteration 489, loss = 0.33110483\n",
      "Iteration 441, loss = 0.33921427\n",
      "Iteration 2366, loss = 0.13875757\n",
      "Iteration 1206, loss = 0.28016992\n",
      "Iteration 442, loss = 0.33910685\n",
      "Iteration 1901, loss = 0.19385950\n",
      "Iteration 443, loss = 0.33898592\n",
      "Iteration 490, loss = 0.33103445\n",
      "Iteration 444, loss = 0.33887372\n",
      "Iteration 321, loss = 0.42183192\n",
      "Iteration 445, loss = 0.33875585\n",
      "Iteration 2367, loss = 0.13875327\n",
      "Iteration 446, loss = 0.33862817\n",
      "Iteration 1207, loss = 0.27997466\n",
      "Iteration 1272, loss = 0.21949922\n",
      "Iteration 447, loss = 0.33848457\n",
      "Iteration 1902, loss = 0.19361721\n",
      "Iteration 2368, loss = 0.13848222\n",
      "Iteration 1447, loss = 0.17771058\n",
      "Iteration 491, loss = 0.33093675\n",
      "Iteration 1208, loss = 0.27984805\n",
      "Iteration 492, loss = 0.33085256\n",
      "Iteration 1448, loss = 0.17756859\n",
      "Iteration 2369, loss = 0.13840169\n",
      "Iteration 448, loss = 0.33836825\n",
      "Iteration 493, loss = 0.33079071\n",
      "Iteration 1449, loss = 0.17739305\n",
      "Iteration 1209, loss = 0.27969648\n",
      "Iteration 2370, loss = 0.13842729\n",
      "Iteration 449, loss = 0.33827176\n",
      "Iteration 494, loss = 0.33070440\n",
      "Iteration 1903, loss = 0.19350293\n",
      "Iteration 2371, loss = 0.13830952\n",
      "Iteration 450, loss = 0.33811664\n",
      "Iteration 495, loss = 0.33060228\n",
      "Iteration 1273, loss = 0.21931672\n",
      "Iteration 322, loss = 0.42159782\n",
      "Iteration 451, loss = 0.33799310\n",
      "Iteration 496, loss = 0.33051763\n",
      "Iteration 452, loss = 0.33789819\n",
      "Iteration 1904, loss = 0.19338356\n",
      "Iteration 453, loss = 0.33777130\n",
      "Iteration 497, loss = 0.33043592\n",
      "Iteration 1450, loss = 0.17755341\n",
      "Iteration 2372, loss = 0.13825884\n",
      "Iteration 454, loss = 0.33770009\n",
      "Iteration 498, loss = 0.33034778\n",
      "Iteration 1210, loss = 0.27960249\n",
      "Iteration 2373, loss = 0.13808721\n",
      "Iteration 455, loss = 0.33751047\n",
      "Iteration 456, loss = 0.33738990\n",
      "Iteration 1451, loss = 0.17709875\n",
      "Iteration 499, loss = 0.33026619\n",
      "Iteration 457, loss = 0.33727995\n",
      "Iteration 323, loss = 0.42136026\n",
      "Iteration 2374, loss = 0.13811247\n",
      "Iteration 458, loss = 0.33717991\n",
      "Iteration 1274, loss = 0.21912191\n",
      "Iteration 500, loss = 0.33019048\n",
      "Iteration 2375, loss = 0.13801177\n",
      "Iteration 459, loss = 0.33705051\n",
      "Iteration 1211, loss = 0.27942815\n",
      "Iteration 501, loss = 0.33013318\n",
      "Iteration 2376, loss = 0.13791646\n",
      "Iteration 460, loss = 0.33694359\n",
      "Iteration 502, loss = 0.33001409\n",
      "Iteration 1452, loss = 0.17695696\n",
      "Iteration 2377, loss = 0.13792320\n",
      "Iteration 1905, loss = 0.19327430\n",
      "Iteration 1275, loss = 0.21935254\n",
      "Iteration 1212, loss = 0.27933013\n",
      "Iteration 461, loss = 0.33685670\n",
      "Iteration 503, loss = 0.33000078\n",
      "Iteration 2378, loss = 0.13777394\n",
      "Iteration 462, loss = 0.33670799\n",
      "Iteration 324, loss = 0.42115836\n",
      "Iteration 463, loss = 0.33663335\n",
      "Iteration 1213, loss = 0.27919803\n",
      "Iteration 504, loss = 0.32986531\n",
      "Iteration 1453, loss = 0.17692377\n",
      "Iteration 464, loss = 0.33645973\n",
      "Iteration 505, loss = 0.32976851\n",
      "Iteration 465, loss = 0.33635560\n",
      "Iteration 2379, loss = 0.13764842\n",
      "Iteration 1276, loss = 0.21878489\n",
      "Iteration 506, loss = 0.32968756\n",
      "Iteration 1906, loss = 0.19321607\n",
      "Iteration 1214, loss = 0.27900149\n",
      "Iteration 466, loss = 0.33622998\n",
      "Iteration 2380, loss = 0.13762134\n",
      "Iteration 507, loss = 0.32960083\n",
      "Iteration 1454, loss = 0.17672852\n",
      "Iteration 2381, loss = 0.13751788\n",
      "Iteration 508, loss = 0.32952175\n",
      "Iteration 467, loss = 0.33610013\n",
      "Iteration 468, loss = 0.33599598\n",
      "Iteration 509, loss = 0.32944477\n",
      "Iteration 1215, loss = 0.27889495\n",
      "Iteration 1455, loss = 0.17654515\n",
      "Iteration 510, loss = 0.32935074\n",
      "Iteration 1277, loss = 0.21861865\n",
      "Iteration 1907, loss = 0.19310141\n",
      "Iteration 511, loss = 0.32927734\n",
      "Iteration 325, loss = 0.42091109\n",
      "Iteration 469, loss = 0.33589381\n",
      "Iteration 1456, loss = 0.17624001\n",
      "Iteration 1216, loss = 0.27887091\n",
      "Iteration 2382, loss = 0.13741215\n",
      "Iteration 470, loss = 0.33576018\n",
      "Iteration 1908, loss = 0.19296837\n",
      "Iteration 512, loss = 0.32920758\n",
      "Iteration 471, loss = 0.33563352\n",
      "Iteration 1217, loss = 0.27868296\n",
      "Iteration 472, loss = 0.33551733\n",
      "Iteration 1278, loss = 0.21837980\n",
      "Iteration 2383, loss = 0.13733318\n",
      "Iteration 326, loss = 0.42065558\n",
      "Iteration 473, loss = 0.33540859\n",
      "Iteration 513, loss = 0.32916300\n",
      "Iteration 474, loss = 0.33530727\n",
      "Iteration 1457, loss = 0.17605547\n",
      "Iteration 475, loss = 0.33518497\n",
      "Iteration 2384, loss = 0.13726906\n",
      "Iteration 476, loss = 0.33506943\n",
      "Iteration 1909, loss = 0.19295413\n",
      "Iteration 1218, loss = 0.27856892\n",
      "Iteration 514, loss = 0.32902719\n",
      "Iteration 477, loss = 0.33495789\n",
      "Iteration 2385, loss = 0.13714281\n",
      "Iteration 478, loss = 0.33486937\n",
      "Iteration 515, loss = 0.32894987\n",
      "Iteration 479, loss = 0.33472629\n",
      "Iteration 1458, loss = 0.17588575\n",
      "Iteration 480, loss = 0.33464632\n",
      "Iteration 1279, loss = 0.21828288\n",
      "Iteration 481, loss = 0.33450855\n",
      "Iteration 1459, loss = 0.17564608\n",
      "Iteration 2386, loss = 0.13705784\n",
      "Iteration 516, loss = 0.32885489\n",
      "Iteration 1910, loss = 0.19277380\n",
      "Iteration 327, loss = 0.42047285\n",
      "Iteration 482, loss = 0.33439098\n",
      "Iteration 1219, loss = 0.27839615\n",
      "Iteration 517, loss = 0.32881238\n",
      "Iteration 2387, loss = 0.13698889\n",
      "Iteration 483, loss = 0.33436126\n",
      "Iteration 1460, loss = 0.17547783\n",
      "Iteration 484, loss = 0.33416579\n",
      "Iteration 518, loss = 0.32872328\n",
      "Iteration 519, loss = 0.32861630\n",
      "Iteration 2388, loss = 0.13689894\n",
      "Iteration 485, loss = 0.33407161\n",
      "Iteration 1280, loss = 0.21811328\n",
      "Iteration 1911, loss = 0.19265854\n",
      "Iteration 1461, loss = 0.17540005\n",
      "Iteration 520, loss = 0.32852834\n",
      "Iteration 486, loss = 0.33395937\n",
      "Iteration 1220, loss = 0.27827545\n",
      "Iteration 328, loss = 0.42020145\n",
      "Iteration 2389, loss = 0.13686702\n",
      "Iteration 487, loss = 0.33386448\n",
      "Iteration 488, loss = 0.33372058\n",
      "Iteration 489, loss = 0.33364727\n",
      "Iteration 2390, loss = 0.13677236\n",
      "Iteration 1462, loss = 0.17517316\n",
      "Iteration 521, loss = 0.32844759\n",
      "Iteration 1912, loss = 0.19265116\n",
      "Iteration 490, loss = 0.33349813\n",
      "Iteration 491, loss = 0.33345387\n",
      "Iteration 2391, loss = 0.13677512\n",
      "Iteration 492, loss = 0.33329333\n",
      "Iteration 522, loss = 0.32836044\n",
      "Iteration 1281, loss = 0.21787108\n",
      "Iteration 1913, loss = 0.19251260\n",
      "Iteration 493, loss = 0.33316369\n",
      "Iteration 523, loss = 0.32828699\n",
      "Iteration 1221, loss = 0.27812635\n",
      "Iteration 1463, loss = 0.17525274\n",
      "Iteration 494, loss = 0.33306516\n",
      "Iteration 329, loss = 0.41997256\n",
      "Iteration 495, loss = 0.33294612\n",
      "Iteration 496, loss = 0.33283114\n",
      "Iteration 2392, loss = 0.13666609\n",
      "Iteration 1914, loss = 0.19230247\n",
      "Iteration 1464, loss = 0.17484877\n",
      "Iteration 497, loss = 0.33273751\n",
      "Iteration 524, loss = 0.32822170\n",
      "Iteration 498, loss = 0.33263999\n",
      "Iteration 499, loss = 0.33253636\n",
      "Iteration 1465, loss = 0.17495783\n",
      "Iteration 2393, loss = 0.13654415\n",
      "Iteration 525, loss = 0.32811603\n",
      "Iteration 1915, loss = 0.19222200\n",
      "Iteration 526, loss = 0.32807603\n",
      "Iteration 1222, loss = 0.27799588\n",
      "Iteration 1282, loss = 0.21767198\n",
      "Iteration 500, loss = 0.33242367\n",
      "Iteration 1466, loss = 0.17442497\n",
      "Iteration 501, loss = 0.33230902\n",
      "Iteration 502, loss = 0.33222207\n",
      "Iteration 527, loss = 0.32795843\n",
      "Iteration 503, loss = 0.33212293\n",
      "Iteration 504, loss = 0.33197789\n",
      "Iteration 528, loss = 0.32788295\n",
      "Iteration 2394, loss = 0.13648128\n",
      "Iteration 505, loss = 0.33192593\n",
      "Iteration 506, loss = 0.33178974\n",
      "Iteration 330, loss = 0.41981737\n",
      "Iteration 529, loss = 0.32780102\n",
      "Iteration 1916, loss = 0.19216315\n",
      "Iteration 507, loss = 0.33168818\n",
      "Iteration 1467, loss = 0.17452668\n",
      "Iteration 1223, loss = 0.27812021\n",
      "Iteration 508, loss = 0.33156185\n",
      "Iteration 1468, loss = 0.17433046\n",
      "Iteration 2395, loss = 0.13640180\n",
      "Iteration 509, loss = 0.33151011\n",
      "Iteration 530, loss = 0.32772953\n",
      "Iteration 1917, loss = 0.19201112\n",
      "Iteration 2396, loss = 0.13630857\n",
      "Iteration 1283, loss = 0.21749590\n",
      "Iteration 1469, loss = 0.17399107\n",
      "Iteration 1224, loss = 0.27776143\n",
      "Iteration 531, loss = 0.32763970\n",
      "Iteration 331, loss = 0.41954521\n",
      "Iteration 510, loss = 0.33136728\n",
      "Iteration 511, loss = 0.33124109\n",
      "Iteration 1470, loss = 0.17373179\n",
      "Iteration 512, loss = 0.33114760\n",
      "Iteration 2397, loss = 0.13623403\n",
      "Iteration 513, loss = 0.33103862\n",
      "Iteration 1225, loss = 0.27762472\n",
      "Iteration 532, loss = 0.32757070\n",
      "Iteration 514, loss = 0.33094711\n",
      "Iteration 1471, loss = 0.17357728\n",
      "Iteration 515, loss = 0.33082934\n",
      "Iteration 516, loss = 0.33074556\n",
      "Iteration 533, loss = 0.32749199\n",
      "Iteration 517, loss = 0.33062520\n",
      "Iteration 2398, loss = 0.13616681\n",
      "Iteration 518, loss = 0.33056566\n",
      "Iteration 519, loss = 0.33044966\n",
      "Iteration 1472, loss = 0.17340872\n",
      "Iteration 2399, loss = 0.13607161\n",
      "Iteration 1226, loss = 0.27752238\n",
      "Iteration 534, loss = 0.32741567\n",
      "Iteration 1918, loss = 0.19195123\n",
      "Iteration 1284, loss = 0.21732564\n",
      "Iteration 520, loss = 0.33033970\n",
      "Iteration 332, loss = 0.41930190\n",
      "Iteration 2400, loss = 0.13603618\n",
      "Iteration 521, loss = 0.33020974\n",
      "Iteration 2401, loss = 0.13595579\n",
      "Iteration 1919, loss = 0.19185922\n",
      "Iteration 535, loss = 0.32731874\n",
      "Iteration 522, loss = 0.33010564\n",
      "Iteration 1473, loss = 0.17324485\n",
      "Iteration 1227, loss = 0.27754719\n",
      "Iteration 523, loss = 0.33002489\n",
      "Iteration 2402, loss = 0.13588177\n",
      "Iteration 1285, loss = 0.21709670\n",
      "Iteration 524, loss = 0.32990962\n",
      "Iteration 1920, loss = 0.19172065\n",
      "Iteration 333, loss = 0.41907317\n",
      "Iteration 536, loss = 0.32725492\n",
      "Iteration 525, loss = 0.32983412\n",
      "Iteration 2403, loss = 0.13575234\n",
      "Iteration 526, loss = 0.32968115\n",
      "Iteration 1228, loss = 0.27725924\n",
      "Iteration 527, loss = 0.32959089\n",
      "Iteration 2404, loss = 0.13572182\n",
      "Iteration 528, loss = 0.32948420\n",
      "Iteration 537, loss = 0.32718344\n",
      "Iteration 334, loss = 0.41885642\n",
      "Iteration 1474, loss = 0.17306548\n",
      "Iteration 529, loss = 0.32940130\n",
      "Iteration 530, loss = 0.32929898\n",
      "Iteration 1921, loss = 0.19162673\n",
      "Iteration 1286, loss = 0.21698038\n",
      "Iteration 538, loss = 0.32708346\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 531, loss = 0.32918439\n",
      "Iteration 532, loss = 0.32907550\n",
      "Iteration 2405, loss = 0.13566443\n",
      "Iteration 533, loss = 0.32900632\n",
      "Iteration 1229, loss = 0.27714281\n",
      "Iteration 534, loss = 0.32886465\n",
      "Iteration 2406, loss = 0.13551745\n",
      "Iteration 535, loss = 0.32880044\n",
      "Iteration 1475, loss = 0.17296037\n",
      "Iteration 536, loss = 0.32865048\n",
      "Iteration 335, loss = 0.41862750\n",
      "Iteration 1922, loss = 0.19154774\n",
      "Iteration 537, loss = 0.32862369\n",
      "Iteration 2407, loss = 0.13553209\n",
      "Iteration 1476, loss = 0.17287362\n",
      "Iteration 538, loss = 0.32844708\n",
      "Iteration 1287, loss = 0.21677789\n",
      "Iteration 1230, loss = 0.27699396\n",
      "Iteration 2408, loss = 0.13546682\n",
      "Iteration 539, loss = 0.32834199\n",
      "Iteration 1477, loss = 0.17269759\n",
      "Iteration 1923, loss = 0.19142314\n",
      "Iteration 540, loss = 0.32824794\n",
      "Iteration 2409, loss = 0.13536190\n",
      "Iteration 1478, loss = 0.17248812\n",
      "Iteration 1, loss = 0.71629864\n",
      "Iteration 541, loss = 0.32814147\n",
      "Iteration 1288, loss = 0.21663818\n",
      "Iteration 336, loss = 0.41841708\n",
      "Iteration 1924, loss = 0.19127307\n",
      "Iteration 542, loss = 0.32804396\n",
      "Iteration 1231, loss = 0.27691900\n",
      "Iteration 1479, loss = 0.17262305\n",
      "Iteration 2, loss = 0.71490462\n",
      "Iteration 543, loss = 0.32795216\n",
      "Iteration 2410, loss = 0.13523885\n",
      "Iteration 1925, loss = 0.19123266\n",
      "Iteration 544, loss = 0.32783857\n",
      "Iteration 545, loss = 0.32774361\n",
      "Iteration 546, loss = 0.32761707\n",
      "Iteration 1232, loss = 0.27682496\n",
      "Iteration 547, loss = 0.32751272\n",
      "Iteration 3, loss = 0.71271713\n",
      "Iteration 548, loss = 0.32742432\n",
      "Iteration 1289, loss = 0.21647188\n",
      "Iteration 337, loss = 0.41822970\n",
      "Iteration 2411, loss = 0.13513762\n",
      "Iteration 549, loss = 0.32730428\n",
      "Iteration 550, loss = 0.32721299\n",
      "Iteration 1480, loss = 0.17223610\n",
      "Iteration 2412, loss = 0.13506904\n",
      "Iteration 4, loss = 0.71010910\n",
      "Iteration 1233, loss = 0.27670801\n",
      "Iteration 551, loss = 0.32710164\n",
      "Iteration 2413, loss = 0.13500818\n",
      "Iteration 5, loss = 0.70704232\n",
      "Iteration 552, loss = 0.32699570\n",
      "Iteration 6, loss = 0.70368928\n",
      "Iteration 1290, loss = 0.21619790\n",
      "Iteration 1481, loss = 0.17208190\n",
      "Iteration 2414, loss = 0.13488663\n",
      "Iteration 338, loss = 0.41796506\n",
      "Iteration 1234, loss = 0.27650974\n",
      "Iteration 553, loss = 0.32690843\n",
      "Iteration 1926, loss = 0.19111879\n",
      "Iteration 7, loss = 0.70023773\n",
      "Iteration 2415, loss = 0.13482936\n",
      "Iteration 1482, loss = 0.17181887\n",
      "Iteration 554, loss = 0.32679266\n",
      "Iteration 8, loss = 0.69660722\n",
      "Iteration 555, loss = 0.32669194\n",
      "Iteration 1927, loss = 0.19110401\n",
      "Iteration 556, loss = 0.32657679\n",
      "Iteration 1483, loss = 0.17166354\n",
      "Iteration 557, loss = 0.32648006\n",
      "Iteration 1235, loss = 0.27640682\n",
      "Iteration 2416, loss = 0.13475538\n",
      "Iteration 1484, loss = 0.17147905\n",
      "Iteration 339, loss = 0.41773775\n",
      "Iteration 1291, loss = 0.21664555\n",
      "Iteration 9, loss = 0.69296573\n",
      "Iteration 2417, loss = 0.13477481\n",
      "Iteration 558, loss = 0.32639149\n",
      "Iteration 559, loss = 0.32627127\n",
      "Iteration 2418, loss = 0.13458574\n",
      "Iteration 10, loss = 0.68924482\n",
      "Iteration 560, loss = 0.32621874\n",
      "Iteration 1928, loss = 0.19096635\n",
      "Iteration 561, loss = 0.32606292\n",
      "Iteration 562, loss = 0.32595983\n",
      "Iteration 11, loss = 0.68564360\n",
      "Iteration 1236, loss = 0.27628714\n",
      "Iteration 563, loss = 0.32586432\n",
      "Iteration 340, loss = 0.41753625\n",
      "Iteration 564, loss = 0.32575376\n",
      "Iteration 2419, loss = 0.13451050\n",
      "Iteration 1485, loss = 0.17132769\n",
      "Iteration 565, loss = 0.32565856\n",
      "Iteration 1929, loss = 0.19081323\n",
      "Iteration 566, loss = 0.32556921\n",
      "Iteration 12, loss = 0.68184551\n",
      "Iteration 567, loss = 0.32544866\n",
      "Iteration 1237, loss = 0.27610892\n",
      "Iteration 568, loss = 0.32535896\n",
      "Iteration 13, loss = 0.67818913\n",
      "Iteration 569, loss = 0.32525000\n",
      "Iteration 2420, loss = 0.13461134\n",
      "Iteration 1486, loss = 0.17113460\n",
      "Iteration 1292, loss = 0.21592338\n",
      "Iteration 1238, loss = 0.27601723\n",
      "Iteration 570, loss = 0.32520523\n",
      "Iteration 14, loss = 0.67440938\n",
      "Iteration 1930, loss = 0.19068157\n",
      "Iteration 571, loss = 0.32505512\n",
      "Iteration 1487, loss = 0.17090980\n",
      "Iteration 15, loss = 0.67078132\n",
      "Iteration 572, loss = 0.32496746\n",
      "Iteration 2421, loss = 0.13440363\n",
      "Iteration 341, loss = 0.41730498\n",
      "Iteration 573, loss = 0.32484659\n",
      "Iteration 574, loss = 0.32477997\n",
      "Iteration 575, loss = 0.32468107\n",
      "Iteration 1239, loss = 0.27585863\n",
      "Iteration 1931, loss = 0.19070767\n",
      "Iteration 16, loss = 0.66698389\n",
      "Iteration 576, loss = 0.32457829\n",
      "Iteration 577, loss = 0.32447425\n",
      "Iteration 2422, loss = 0.13434682\n",
      "Iteration 17, loss = 0.66340630\n",
      "Iteration 1293, loss = 0.21576599\n",
      "Iteration 578, loss = 0.32435736\n",
      "Iteration 1488, loss = 0.17082809\n",
      "Iteration 18, loss = 0.65964789\n",
      "Iteration 342, loss = 0.41713312\n",
      "Iteration 1240, loss = 0.27576830\n",
      "Iteration 579, loss = 0.32426665\n",
      "Iteration 2423, loss = 0.13423825\n",
      "Iteration 580, loss = 0.32417183\n",
      "Iteration 581, loss = 0.32412699\n",
      "Iteration 19, loss = 0.65605951\n",
      "Iteration 2424, loss = 0.13418518\n",
      "Iteration 582, loss = 0.32397341\n",
      "Iteration 1241, loss = 0.27559102\n",
      "Iteration 1932, loss = 0.19053280\n",
      "Iteration 20, loss = 0.65244413\n",
      "Iteration 2425, loss = 0.13408915\n",
      "Iteration 583, loss = 0.32388438\n",
      "Iteration 1489, loss = 0.17069852\n",
      "Iteration 21, loss = 0.64881067\n",
      "Iteration 584, loss = 0.32376193\n",
      "Iteration 2426, loss = 0.13404975\n",
      "Iteration 1933, loss = 0.19032609\n",
      "Iteration 1490, loss = 0.17046742\n",
      "Iteration 585, loss = 0.32367516\n",
      "Iteration 22, loss = 0.64531492\n",
      "Iteration 1294, loss = 0.21551547\n",
      "Iteration 586, loss = 0.32359671\n",
      "Iteration 2427, loss = 0.13392068\n",
      "Iteration 343, loss = 0.41687887\n",
      "Iteration 587, loss = 0.32349493\n",
      "Iteration 23, loss = 0.64163012\n",
      "Iteration 2428, loss = 0.13385292\n",
      "Iteration 588, loss = 0.32337519\n",
      "Iteration 24, loss = 0.63803021\n",
      "Iteration 1934, loss = 0.19030761\n",
      "Iteration 1491, loss = 0.17041746\n",
      "Iteration 589, loss = 0.32331051\n",
      "Iteration 1242, loss = 0.27544553\n",
      "Iteration 590, loss = 0.32319449\n",
      "Iteration 1492, loss = 0.17033407\n",
      "Iteration 591, loss = 0.32310455\n",
      "Iteration 1295, loss = 0.21532024\n",
      "Iteration 344, loss = 0.41664392\n",
      "Iteration 25, loss = 0.63455849\n",
      "Iteration 2429, loss = 0.13386714\n",
      "Iteration 592, loss = 0.32300049\n",
      "Iteration 1493, loss = 0.16998204\n",
      "Iteration 2430, loss = 0.13381375\n",
      "Iteration 1243, loss = 0.27534275\n",
      "Iteration 1935, loss = 0.19018122\n",
      "Iteration 593, loss = 0.32290188Iteration 2431, loss = 0.13362222\n",
      "\n",
      "Iteration 26, loss = 0.63091670\n",
      "Iteration 2432, loss = 0.13353042\n",
      "Iteration 1296, loss = 0.21523387\n",
      "Iteration 1936, loss = 0.19011160\n",
      "Iteration 27, loss = 0.62754988\n",
      "Iteration 1494, loss = 0.16989104\n",
      "Iteration 1244, loss = 0.27518371\n",
      "Iteration 594, loss = 0.32284304\n",
      "Iteration 2433, loss = 0.13347012\n",
      "Iteration 595, loss = 0.32272912\n",
      "Iteration 28, loss = 0.62405426\n",
      "Iteration 345, loss = 0.41647420\n",
      "Iteration 596, loss = 0.32267072\n",
      "Iteration 597, loss = 0.32251578\n",
      "Iteration 29, loss = 0.62054026\n",
      "Iteration 598, loss = 0.32242680\n",
      "Iteration 599, loss = 0.32233230\n",
      "Iteration 2434, loss = 0.13341056\n",
      "Iteration 1937, loss = 0.18998237\n",
      "Iteration 1495, loss = 0.16965089\n",
      "Iteration 2435, loss = 0.13336954\n",
      "Iteration 600, loss = 0.32225629\n",
      "Iteration 1297, loss = 0.21518825\n",
      "Iteration 1245, loss = 0.27509628\n",
      "Iteration 30, loss = 0.61708944\n",
      "Iteration 2436, loss = 0.13338812\n",
      "Iteration 601, loss = 0.32213369\n",
      "Iteration 31, loss = 0.61359490\n",
      "Iteration 2437, loss = 0.13320706\n",
      "Iteration 346, loss = 0.41622083\n",
      "Iteration 602, loss = 0.32203784\n",
      "Iteration 1496, loss = 0.16944918\n",
      "Iteration 2438, loss = 0.13315052\n",
      "Iteration 1497, loss = 0.16936707\n",
      "Iteration 603, loss = 0.32196118\n",
      "Iteration 1246, loss = 0.27497887\n",
      "Iteration 32, loss = 0.61017600\n",
      "Iteration 1938, loss = 0.18984144\n",
      "Iteration 347, loss = 0.41601701\n",
      "Iteration 1298, loss = 0.21504332\n",
      "Iteration 604, loss = 0.32186183\n",
      "Iteration 605, loss = 0.32176576\n",
      "Iteration 606, loss = 0.32167057\n",
      "Iteration 607, loss = 0.32155717\n",
      "Iteration 2439, loss = 0.13309620\n",
      "Iteration 1939, loss = 0.18965841\n",
      "Iteration 608, loss = 0.32151021\n",
      "Iteration 609, loss = 0.32140349\n",
      "Iteration 33, loss = 0.60680269\n",
      "Iteration 2440, loss = 0.13293076\n",
      "Iteration 610, loss = 0.32128362\n",
      "Iteration 611, loss = 0.32125285\n",
      "Iteration 348, loss = 0.41581427\n",
      "Iteration 612, loss = 0.32110095\n",
      "Iteration 2441, loss = 0.13285214\n",
      "Iteration 1299, loss = 0.21469323\n",
      "Iteration 613, loss = 0.32103517\n",
      "Iteration 1940, loss = 0.18954381\n",
      "Iteration 614, loss = 0.32093190\n",
      "Iteration 2442, loss = 0.13278318\n",
      "Iteration 34, loss = 0.60344050\n",
      "Iteration 1247, loss = 0.27484925\n",
      "Iteration 615, loss = 0.32082519\n",
      "Iteration 1498, loss = 0.16935501\n",
      "Iteration 616, loss = 0.32072718\n",
      "Iteration 2443, loss = 0.13270441\n",
      "Iteration 617, loss = 0.32067655\n",
      "Iteration 618, loss = 0.32057554\n",
      "Iteration 619, loss = 0.32045638\n",
      "Iteration 620, loss = 0.32036135\n",
      "Iteration 2444, loss = 0.13267290\n",
      "Iteration 35, loss = 0.60009486\n",
      "Iteration 621, loss = 0.32027735\n",
      "Iteration 1941, loss = 0.18953803\n",
      "Iteration 622, loss = 0.32021550\n",
      "Iteration 623, loss = 0.32010393\n",
      "Iteration 2445, loss = 0.13256759\n",
      "Iteration 36, loss = 0.59672921\n",
      "Iteration 1499, loss = 0.16911162\n",
      "Iteration 1248, loss = 0.27469751\n",
      "Iteration 624, loss = 0.32000360\n",
      "Iteration 1300, loss = 0.21460935\n",
      "Iteration 1942, loss = 0.18933160\n",
      "Iteration 349, loss = 0.41558001\n",
      "Iteration 2446, loss = 0.13254087\n",
      "Iteration 1500, loss = 0.16900856\n",
      "Iteration 625, loss = 0.31996486\n",
      "Iteration 37, loss = 0.59334782\n",
      "Iteration 626, loss = 0.31982185Iteration 2447, loss = 0.13243037\n",
      "Iteration 38, loss = 0.59008179\n",
      "Iteration 1501, loss = 0.16892305\n",
      "Iteration 1249, loss = 0.27459378\n",
      "\n",
      "Iteration 1301, loss = 0.21431292\n",
      "Iteration 39, loss = 0.58678363\n",
      "Iteration 2448, loss = 0.13234060\n",
      "Iteration 1943, loss = 0.18927516\n",
      "Iteration 1502, loss = 0.16850112\n",
      "Iteration 40, loss = 0.58360420\n",
      "Iteration 627, loss = 0.31975266\n",
      "Iteration 1250, loss = 0.27450956\n",
      "Iteration 41, loss = 0.58027621\n",
      "Iteration 1503, loss = 0.16839684Iteration 628, loss = 0.31965379\n",
      "\n",
      "Iteration 2449, loss = 0.13229512\n",
      "Iteration 42, loss = 0.57715761\n",
      "Iteration 629, loss = 0.31957377\n",
      "Iteration 1251, loss = 0.27432734\n",
      "Iteration 1504, loss = 0.16847290\n",
      "Iteration 43, loss = 0.57387666\n",
      "Iteration 630, loss = 0.31947735\n",
      "Iteration 350, loss = 0.41538331\n",
      "Iteration 1302, loss = 0.21418859\n",
      "Iteration 1944, loss = 0.18912346\n",
      "Iteration 1252, loss = 0.27423138\n",
      "Iteration 631, loss = 0.31941305\n",
      "Iteration 44, loss = 0.57075508\n",
      "Iteration 2450, loss = 0.13224727\n",
      "Iteration 632, loss = 0.31930421\n",
      "Iteration 1505, loss = 0.16804389\n",
      "Iteration 633, loss = 0.31922589\n",
      "Iteration 634, loss = 0.31910971\n",
      "Iteration 1253, loss = 0.27412118\n",
      "Iteration 45, loss = 0.56759111\n",
      "Iteration 2451, loss = 0.13228385\n",
      "Iteration 1303, loss = 0.21393825\n",
      "Iteration 1945, loss = 0.18903423\n",
      "Iteration 635, loss = 0.31900922\n",
      "Iteration 46, loss = 0.56449295\n",
      "Iteration 1506, loss = 0.16812331\n",
      "Iteration 2452, loss = 0.13213353\n",
      "Iteration 636, loss = 0.31894608\n",
      "Iteration 1946, loss = 0.18899162\n",
      "Iteration 351, loss = 0.41513118\n",
      "Iteration 47, loss = 0.56147158\n",
      "Iteration 2453, loss = 0.13199907\n",
      "Iteration 1507, loss = 0.16781253\n",
      "Iteration 637, loss = 0.31884547\n",
      "Iteration 48, loss = 0.55834923\n",
      "Iteration 2454, loss = 0.13203376\n",
      "Iteration 638, loss = 0.31876771\n",
      "Iteration 1508, loss = 0.16777094\n",
      "Iteration 1947, loss = 0.18887662\n",
      "Iteration 1254, loss = 0.27397878\n",
      "Iteration 49, loss = 0.55526214\n",
      "Iteration 2455, loss = 0.13184018\n",
      "Iteration 639, loss = 0.31869464\n",
      "Iteration 50, loss = 0.55229822\n",
      "Iteration 2456, loss = 0.13177332\n",
      "Iteration 1509, loss = 0.16742803\n",
      "Iteration 352, loss = 0.41493161\n",
      "Iteration 51, loss = 0.54933189\n",
      "Iteration 640, loss = 0.31857478\n",
      "Iteration 2457, loss = 0.13171214\n",
      "Iteration 1304, loss = 0.21375558\n",
      "Iteration 1255, loss = 0.27385424\n",
      "Iteration 1948, loss = 0.18869735\n",
      "Iteration 2458, loss = 0.13165137\n",
      "Iteration 641, loss = 0.31848807\n",
      "Iteration 642, loss = 0.31844043\n",
      "Iteration 52, loss = 0.54634516\n",
      "Iteration 643, loss = 0.31832059\n",
      "Iteration 1510, loss = 0.16726509\n",
      "Iteration 644, loss = 0.31821961\n",
      "Iteration 353, loss = 0.41475774\n",
      "Iteration 2459, loss = 0.13163309\n",
      "Iteration 1256, loss = 0.27366201\n",
      "Iteration 1949, loss = 0.18869635\n",
      "Iteration 645, loss = 0.31812000\n",
      "Iteration 53, loss = 0.54340728\n",
      "Iteration 1305, loss = 0.21357330\n",
      "Iteration 646, loss = 0.31809697\n",
      "Iteration 54, loss = 0.54046868\n",
      "Iteration 647, loss = 0.31796363\n",
      "Iteration 1950, loss = 0.18854876\n",
      "Iteration 648, loss = 0.31787221\n",
      "Iteration 2460, loss = 0.13147671\n",
      "Iteration 1511, loss = 0.16719291\n",
      "Iteration 649, loss = 0.31782238\n",
      "Iteration 650, loss = 0.31769773\n",
      "Iteration 651, loss = 0.31759842\n",
      "Iteration 1257, loss = 0.27357464\n",
      "Iteration 55, loss = 0.53759335\n",
      "Iteration 652, loss = 0.31760800\n",
      "Iteration 1512, loss = 0.16695459\n",
      "Iteration 354, loss = 0.41449168\n",
      "Iteration 2461, loss = 0.13142619\n",
      "Iteration 1951, loss = 0.18845015\n",
      "Iteration 1306, loss = 0.21341787\n",
      "Iteration 2462, loss = 0.13148311\n",
      "Iteration 653, loss = 0.31741397\n",
      "Iteration 1513, loss = 0.16676689\n",
      "Iteration 56, loss = 0.53472453\n",
      "Iteration 2463, loss = 0.13125070\n",
      "Iteration 654, loss = 0.31734306\n",
      "Iteration 1258, loss = 0.27349352\n",
      "Iteration 1514, loss = 0.16677217\n",
      "Iteration 57, loss = 0.53187842\n",
      "Iteration 1952, loss = 0.18831506\n",
      "Iteration 655, loss = 0.31725261\n",
      "Iteration 2464, loss = 0.13118012\n",
      "Iteration 355, loss = 0.41428000\n",
      "Iteration 58, loss = 0.52921446\n",
      "Iteration 1515, loss = 0.16644023\n",
      "Iteration 2465, loss = 0.13114981\n",
      "Iteration 656, loss = 0.31714320\n",
      "Iteration 1307, loss = 0.21320937\n",
      "Iteration 59, loss = 0.52643212\n",
      "Iteration 2466, loss = 0.13113869\n",
      "Iteration 1953, loss = 0.18825250\n",
      "Iteration 1259, loss = 0.27332713\n",
      "Iteration 60, loss = 0.52365906\n",
      "Iteration 2467, loss = 0.13098115\n",
      "Iteration 356, loss = 0.41410495\n",
      "Iteration 657, loss = 0.31705941\n",
      "Iteration 61, loss = 0.52096815\n",
      "Iteration 1954, loss = 0.18812374\n",
      "Iteration 658, loss = 0.31697871\n",
      "Iteration 2468, loss = 0.13089892\n",
      "Iteration 1516, loss = 0.16631524\n",
      "Iteration 1308, loss = 0.21302297\n",
      "Iteration 2469, loss = 0.13099561\n",
      "Iteration 1260, loss = 0.27323739\n",
      "Iteration 659, loss = 0.31691858\n",
      "Iteration 62, loss = 0.51832907\n",
      "Iteration 357, loss = 0.41395896\n",
      "Iteration 1517, loss = 0.16614864\n",
      "Iteration 660, loss = 0.31682590\n",
      "Iteration 1955, loss = 0.18819102\n",
      "Iteration 1518, loss = 0.16644026\n",
      "Iteration 661, loss = 0.31671017\n",
      "Iteration 2470, loss = 0.13088582\n",
      "Iteration 63, loss = 0.51576630\n",
      "Iteration 1309, loss = 0.21278293\n",
      "Iteration 358, loss = 0.41369626\n",
      "Iteration 662, loss = 0.31661482\n",
      "Iteration 2471, loss = 0.13071112\n",
      "Iteration 1956, loss = 0.18784855\n",
      "Iteration 663, loss = 0.31652796\n",
      "Iteration 1261, loss = 0.27311600\n",
      "Iteration 664, loss = 0.31645537\n",
      "Iteration 64, loss = 0.51321049Iteration 665, loss = 0.31638084\n",
      "Iteration 2472, loss = 0.13060814\n",
      "Iteration 666, loss = 0.31626996\n",
      "\n",
      "Iteration 667, loss = 0.31619085\n",
      "Iteration 1519, loss = 0.16584413\n",
      "Iteration 668, loss = 0.31609864\n",
      "Iteration 1957, loss = 0.18788360\n",
      "Iteration 2473, loss = 0.13053467\n",
      "Iteration 669, loss = 0.31600894\n",
      "Iteration 670, loss = 0.31594858\n",
      "Iteration 359, loss = 0.41348288\n",
      "Iteration 2474, loss = 0.13052063\n",
      "Iteration 65, loss = 0.51058281\n",
      "Iteration 1262, loss = 0.27294350\n",
      "Iteration 671, loss = 0.31580954\n",
      "Iteration 1520, loss = 0.16580991\n",
      "Iteration 672, loss = 0.31572326\n",
      "Iteration 1958, loss = 0.18769332\n",
      "Iteration 673, loss = 0.31564165\n",
      "Iteration 1310, loss = 0.21264511\n",
      "Iteration 674, loss = 0.31555061\n",
      "Iteration 2475, loss = 0.13040067\n",
      "Iteration 675, loss = 0.31548061\n",
      "Iteration 66, loss = 0.50808656\n",
      "Iteration 676, loss = 0.31538982\n",
      "Iteration 1521, loss = 0.16556755\n",
      "Iteration 2476, loss = 0.13032180\n",
      "Iteration 677, loss = 0.31529446\n",
      "Iteration 678, loss = 0.31522389\n",
      "Iteration 1959, loss = 0.18763401\n",
      "Iteration 1263, loss = 0.27280312\n",
      "Iteration 679, loss = 0.31513734\n",
      "Iteration 1522, loss = 0.16563624\n",
      "Iteration 67, loss = 0.50567443\n",
      "Iteration 2477, loss = 0.13028545\n",
      "Iteration 680, loss = 0.31509427\n",
      "Iteration 681, loss = 0.31496877\n",
      "Iteration 1523, loss = 0.16531798\n",
      "Iteration 682, loss = 0.31489750\n",
      "Iteration 360, loss = 0.41329501\n",
      "Iteration 683, loss = 0.31476042\n",
      "Iteration 1264, loss = 0.27265353\n",
      "Iteration 2478, loss = 0.13024057\n",
      "Iteration 1311, loss = 0.21254790\n",
      "Iteration 68, loss = 0.50316386\n",
      "Iteration 1524, loss = 0.16502272\n",
      "Iteration 684, loss = 0.31469364\n",
      "Iteration 2479, loss = 0.13022100\n",
      "Iteration 69, loss = 0.50078328\n",
      "Iteration 2480, loss = 0.13007084\n",
      "Iteration 361, loss = 0.41301105\n",
      "Iteration 70, loss = 0.49843273\n",
      "Iteration 685, loss = 0.31460210\n",
      "Iteration 2481, loss = 0.12995559\n",
      "Iteration 1265, loss = 0.27253724\n",
      "Iteration 1525, loss = 0.16491800\n",
      "Iteration 71, loss = 0.49607645\n",
      "Iteration 1960, loss = 0.18743696\n",
      "Iteration 686, loss = 0.31450853\n",
      "Iteration 2482, loss = 0.12990278\n",
      "Iteration 687, loss = 0.31444428\n",
      "Iteration 1312, loss = 0.21241620\n",
      "Iteration 688, loss = 0.31433873\n",
      "Iteration 362, loss = 0.41285155\n",
      "Iteration 72, loss = 0.49386554\n",
      "Iteration 689, loss = 0.31424537\n",
      "Iteration 1961, loss = 0.18738169\n",
      "Iteration 2483, loss = 0.12994781\n",
      "Iteration 73, loss = 0.49160540\n",
      "Iteration 690, loss = 0.31414042\n",
      "Iteration 691, loss = 0.31406161\n",
      "Iteration 1266, loss = 0.27239689Iteration 2484, loss = 0.12973995\n",
      "\n",
      "Iteration 692, loss = 0.31397987\n",
      "Iteration 693, loss = 0.31387707\n",
      "Iteration 694, loss = 0.31379954\n",
      "Iteration 2485, loss = 0.12979447\n",
      "Iteration 1962, loss = 0.18725944\n",
      "Iteration 695, loss = 0.31371417\n",
      "Iteration 1526, loss = 0.16471387\n",
      "Iteration 74, loss = 0.48938765\n",
      "Iteration 2486, loss = 0.12959952\n",
      "Iteration 696, loss = 0.31362000\n",
      "Iteration 2487, loss = 0.12959997\n",
      "Iteration 1313, loss = 0.21229063\n",
      "Iteration 75, loss = 0.48716121\n",
      "Iteration 1963, loss = 0.18715695\n",
      "Iteration 2488, loss = 0.12950992\n",
      "Iteration 1267, loss = 0.27234983\n",
      "Iteration 697, loss = 0.31354487\n",
      "Iteration 1527, loss = 0.16460381\n",
      "Iteration 698, loss = 0.31345281\n",
      "Iteration 363, loss = 0.41260121\n",
      "Iteration 2489, loss = 0.12939292\n",
      "Iteration 76, loss = 0.48501120\n",
      "Iteration 1314, loss = 0.21194270\n",
      "Iteration 699, loss = 0.31336916\n",
      "Iteration 1528, loss = 0.16445742\n",
      "Iteration 1964, loss = 0.18711701\n",
      "Iteration 2490, loss = 0.12933710\n",
      "Iteration 700, loss = 0.31326597\n",
      "Iteration 77, loss = 0.48294809\n",
      "Iteration 2491, loss = 0.12929879\n",
      "Iteration 1529, loss = 0.16422801\n",
      "Iteration 78, loss = 0.48085530\n",
      "Iteration 1268, loss = 0.27215128\n",
      "Iteration 701, loss = 0.31320810\n",
      "Iteration 2492, loss = 0.12915806\n",
      "Iteration 79, loss = 0.47882792\n",
      "Iteration 2493, loss = 0.12916093\n",
      "Iteration 702, loss = 0.31312068\n",
      "Iteration 1269, loss = 0.27205235\n",
      "Iteration 80, loss = 0.47690541\n",
      "Iteration 2494, loss = 0.12909672\n",
      "Iteration 703, loss = 0.31304057\n",
      "Iteration 1965, loss = 0.18699331\n",
      "Iteration 704, loss = 0.31294249\n",
      "Iteration 2495, loss = 0.12920162\n",
      "Iteration 364, loss = 0.41250101\n",
      "Iteration 1530, loss = 0.16420254\n",
      "Iteration 1270, loss = 0.27200306\n",
      "Iteration 705, loss = 0.31282315\n",
      "Iteration 2496, loss = 0.12896191\n",
      "Iteration 81, loss = 0.47485074\n",
      "Iteration 2497, loss = 0.12896770\n",
      "Iteration 1315, loss = 0.21175564\n",
      "Iteration 1966, loss = 0.18679609\n",
      "Iteration 82, loss = 0.47287674\n",
      "Iteration 1531, loss = 0.16395414\n",
      "Iteration 2498, loss = 0.12884753\n",
      "Iteration 706, loss = 0.31272970\n",
      "Iteration 707, loss = 0.31265821\n",
      "Iteration 1271, loss = 0.27179307\n",
      "Iteration 83, loss = 0.47102733\n",
      "Iteration 708, loss = 0.31258556\n",
      "Iteration 2499, loss = 0.12870432\n",
      "Iteration 709, loss = 0.31248614\n",
      "Iteration 1532, loss = 0.16380623\n",
      "Iteration 365, loss = 0.41224292\n",
      "Iteration 710, loss = 0.31240212\n",
      "Iteration 2500, loss = 0.12861737\n",
      "Iteration 711, loss = 0.31230423\n",
      "Iteration 712, loss = 0.31223258\n",
      "Iteration 2501, loss = 0.12878481\n",
      "Iteration 713, loss = 0.31213235\n",
      "Iteration 1272, loss = 0.27166559\n",
      "Iteration 1967, loss = 0.18672361\n",
      "Iteration 714, loss = 0.31205083\n",
      "Iteration 2502, loss = 0.12857773\n",
      "Iteration 84, loss = 0.46908415\n",
      "Iteration 715, loss = 0.31197968\n",
      "Iteration 716, loss = 0.31188631\n",
      "Iteration 2503, loss = 0.12846896\n",
      "Iteration 85, loss = 0.46730680\n",
      "Iteration 717, loss = 0.31179327\n",
      "Iteration 2504, loss = 0.12845300\n",
      "Iteration 1533, loss = 0.16374829\n",
      "Iteration 86, loss = 0.46557272\n",
      "Iteration 718, loss = 0.31171700\n",
      "Iteration 1316, loss = 0.21159628\n",
      "Iteration 2505, loss = 0.12836893\n",
      "Iteration 1273, loss = 0.27158418\n",
      "Iteration 719, loss = 0.31164410\n",
      "Iteration 87, loss = 0.46380506\n",
      "Iteration 2506, loss = 0.12822320\n",
      "Iteration 720, loss = 0.31157020\n",
      "Iteration 2507, loss = 0.12825512\n",
      "Iteration 1274, loss = 0.27144425\n",
      "Iteration 366, loss = 0.41203031\n",
      "Iteration 1317, loss = 0.21145232\n",
      "Iteration 1534, loss = 0.16361813\n",
      "Iteration 1968, loss = 0.18667184\n",
      "Iteration 721, loss = 0.31148096\n",
      "Iteration 722, loss = 0.31141026\n",
      "Iteration 88, loss = 0.46207388\n",
      "Iteration 2508, loss = 0.12814859\n",
      "Iteration 723, loss = 0.31130728\n",
      "Iteration 724, loss = 0.31121346\n",
      "Iteration 1275, loss = 0.27130755\n",
      "Iteration 725, loss = 0.31119863\n",
      "Iteration 89, loss = 0.46042390\n",
      "Iteration 726, loss = 0.31105731\n",
      "Iteration 727, loss = 0.31097775\n",
      "Iteration 728, loss = 0.31088110\n",
      "Iteration 2509, loss = 0.12802463\n",
      "Iteration 1276, loss = 0.27114178\n",
      "Iteration 729, loss = 0.31080270\n",
      "Iteration 1969, loss = 0.18664193\n",
      "Iteration 367, loss = 0.41178859\n",
      "Iteration 90, loss = 0.45871610\n",
      "Iteration 2510, loss = 0.12795046\n",
      "Iteration 730, loss = 0.31072539\n",
      "Iteration 1318, loss = 0.21131251\n",
      "Iteration 2511, loss = 0.12785262\n",
      "Iteration 1277, loss = 0.27103607\n",
      "Iteration 731, loss = 0.31066405\n",
      "Iteration 732, loss = 0.31056065\n",
      "Iteration 1535, loss = 0.16342375\n",
      "Iteration 733, loss = 0.31047459\n",
      "Iteration 2512, loss = 0.12776985\n",
      "Iteration 734, loss = 0.31040075\n",
      "Iteration 735, loss = 0.31030981\n",
      "Iteration 1278, loss = 0.27088371\n",
      "Iteration 2513, loss = 0.12781753\n",
      "Iteration 736, loss = 0.31023753\n",
      "Iteration 91, loss = 0.45715795\n",
      "Iteration 737, loss = 0.31018429\n",
      "Iteration 368, loss = 0.41157398\n",
      "Iteration 1970, loss = 0.18654733\n",
      "Iteration 738, loss = 0.31004947\n",
      "Iteration 2514, loss = 0.12771369\n",
      "Iteration 739, loss = 0.30997194\n",
      "Iteration 740, loss = 0.30990756\n",
      "Iteration 92, loss = 0.45558535\n",
      "Iteration 741, loss = 0.30982885\n",
      "Iteration 1319, loss = 0.21117992\n",
      "Iteration 742, loss = 0.30973140\n",
      "Iteration 2515, loss = 0.12763007\n",
      "Iteration 1279, loss = 0.27082859\n",
      "Iteration 743, loss = 0.30964976\n",
      "Iteration 369, loss = 0.41134474\n",
      "Iteration 1536, loss = 0.16323137\n",
      "Iteration 744, loss = 0.30957240\n",
      "Iteration 2516, loss = 0.12751716\n",
      "Iteration 1971, loss = 0.18630416\n",
      "Iteration 1280, loss = 0.27065137\n",
      "Iteration 1537, loss = 0.16299381\n",
      "Iteration 2517, loss = 0.12751119\n",
      "Iteration 1538, loss = 0.16285198\n",
      "Iteration 1281, loss = 0.27061199\n",
      "Iteration 745, loss = 0.30949217\n",
      "Iteration 2518, loss = 0.12740125\n",
      "Iteration 1320, loss = 0.21090766\n",
      "Iteration 746, loss = 0.30940147\n",
      "Iteration 1539, loss = 0.16297199\n",
      "Iteration 747, loss = 0.30933439\n",
      "Iteration 748, loss = 0.30925556\n",
      "Iteration 749, loss = 0.30917192\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 2519, loss = 0.12738962\n",
      "Iteration 1282, loss = 0.27040324\n",
      "Iteration 1972, loss = 0.18626434\n",
      "Iteration 370, loss = 0.41115287\n",
      "Iteration 1540, loss = 0.16268615\n",
      "Iteration 1283, loss = 0.27024058\n",
      "Iteration 2520, loss = 0.12725550\n",
      "Iteration 1541, loss = 0.16238469\n",
      "Iteration 1321, loss = 0.21090203\n",
      "Iteration 1973, loss = 0.18615508\n",
      "Iteration 2521, loss = 0.12725445\n",
      "Iteration 371, loss = 0.41091950\n",
      "Iteration 1, loss = 0.79033102\n",
      "Iteration 1284, loss = 0.27019159\n",
      "Iteration 2522, loss = 0.12719729\n",
      "Iteration 1542, loss = 0.16225279\n",
      "Iteration 2523, loss = 0.12701987\n",
      "Iteration 2, loss = 0.78849926\n",
      "Iteration 1285, loss = 0.27002491\n",
      "Iteration 1974, loss = 0.18601142\n",
      "Iteration 93, loss = 0.45401923\n",
      "Iteration 1543, loss = 0.16207904\n",
      "Iteration 1322, loss = 0.21062379\n",
      "Iteration 2524, loss = 0.12697004\n",
      "Iteration 372, loss = 0.41072671\n",
      "Iteration 2525, loss = 0.12701020\n",
      "Iteration 1544, loss = 0.16203224\n",
      "Iteration 1975, loss = 0.18596986\n",
      "Iteration 2526, loss = 0.12688260\n",
      "Iteration 1286, loss = 0.26985551\n",
      "Iteration 3, loss = 0.78580056\n",
      "Iteration 1545, loss = 0.16181758\n",
      "Iteration 2527, loss = 0.12676546\n",
      "Iteration 1976, loss = 0.18580478\n",
      "Iteration 373, loss = 0.41056705\n",
      "Iteration 2528, loss = 0.12672637\n",
      "Iteration 4, loss = 0.78246515\n",
      "Iteration 1287, loss = 0.26974353\n",
      "Iteration 1977, loss = 0.18591782\n",
      "Iteration 1323, loss = 0.21047657\n",
      "Iteration 2529, loss = 0.12659392\n",
      "Iteration 374, loss = 0.41032012\n",
      "Iteration 1288, loss = 0.26963364\n",
      "Iteration 1546, loss = 0.16162046\n",
      "Iteration 1324, loss = 0.21019920\n",
      "Iteration 2530, loss = 0.12664818\n",
      "Iteration 1978, loss = 0.18560611\n",
      "Iteration 5, loss = 0.77860726\n",
      "Iteration 375, loss = 0.41008240\n",
      "Iteration 1547, loss = 0.16157315\n",
      "Iteration 1289, loss = 0.26956847\n",
      "Iteration 1979, loss = 0.18568583\n",
      "Iteration 2531, loss = 0.12649599\n",
      "Iteration 1548, loss = 0.16155371\n",
      "Iteration 1325, loss = 0.21007026\n",
      "Iteration 1549, loss = 0.16136318\n",
      "Iteration 6, loss = 0.77468739\n",
      "Iteration 2532, loss = 0.12641510\n",
      "Iteration 376, loss = 0.40989740\n",
      "Iteration 2533, loss = 0.12636689\n",
      "Iteration 94, loss = 0.45246451\n",
      "Iteration 1980, loss = 0.18548292\n",
      "Iteration 7, loss = 0.77069105\n",
      "Iteration 2534, loss = 0.12630096\n",
      "Iteration 1550, loss = 0.16114281\n",
      "Iteration 1326, loss = 0.20985259\n",
      "Iteration 1290, loss = 0.26940626\n",
      "Iteration 8, loss = 0.76685142\n",
      "Iteration 1551, loss = 0.16098938\n",
      "Iteration 2535, loss = 0.12620432\n",
      "Iteration 1291, loss = 0.26927415\n",
      "Iteration 377, loss = 0.40971116\n",
      "Iteration 95, loss = 0.45102030\n",
      "Iteration 1981, loss = 0.18540890\n",
      "Iteration 9, loss = 0.76259610\n",
      "Iteration 2536, loss = 0.12623152\n",
      "Iteration 1552, loss = 0.16073259\n",
      "Iteration 2537, loss = 0.12616054\n",
      "Iteration 10, loss = 0.75859912\n",
      "Iteration 1327, loss = 0.20973391\n",
      "Iteration 1553, loss = 0.16065996\n",
      "Iteration 1982, loss = 0.18524239\n",
      "Iteration 96, loss = 0.44954269\n",
      "Iteration 11, loss = 0.75465030\n",
      "Iteration 2538, loss = 0.12603923\n",
      "Iteration 378, loss = 0.40948752\n",
      "Iteration 1554, loss = 0.16042689\n",
      "Iteration 1292, loss = 0.26912934\n",
      "Iteration 2539, loss = 0.12595925\n",
      "Iteration 97, loss = 0.44809498\n",
      "Iteration 12, loss = 0.75069285\n",
      "Iteration 2540, loss = 0.12589117\n",
      "Iteration 1983, loss = 0.18524661\n",
      "Iteration 1328, loss = 0.20967658\n",
      "Iteration 98, loss = 0.44667411\n",
      "Iteration 1555, loss = 0.16036143\n",
      "Iteration 13, loss = 0.74700098\n",
      "Iteration 1556, loss = 0.16010776\n",
      "Iteration 1984, loss = 0.18500308\n",
      "Iteration 2541, loss = 0.12579822\n",
      "Iteration 1293, loss = 0.26902463\n",
      "Iteration 99, loss = 0.44539302\n",
      "Iteration 1329, loss = 0.20934664\n",
      "Iteration 1985, loss = 0.18500681\n",
      "Iteration 2542, loss = 0.12576858\n",
      "Iteration 2543, loss = 0.12577165\n",
      "Iteration 1557, loss = 0.16000446\n",
      "Iteration 100, loss = 0.44396663\n",
      "Iteration 1294, loss = 0.26887166\n",
      "Iteration 379, loss = 0.40926776\n",
      "Iteration 2544, loss = 0.12564301\n",
      "Iteration 1558, loss = 0.15988036\n",
      "Iteration 1330, loss = 0.20919933\n",
      "Iteration 14, loss = 0.74307285Iteration 101, loss = 0.44273880\n",
      "Iteration 2545, loss = 0.12565744\n",
      "\n",
      "Iteration 1986, loss = 0.18479523\n",
      "Iteration 2546, loss = 0.12547077\n",
      "Iteration 1331, loss = 0.20936001\n",
      "Iteration 1295, loss = 0.26880382\n",
      "Iteration 102, loss = 0.44135005\n",
      "Iteration 1559, loss = 0.15972149\n",
      "Iteration 380, loss = 0.40912702\n",
      "Iteration 15, loss = 0.73957368\n",
      "Iteration 1560, loss = 0.15958478\n",
      "Iteration 2547, loss = 0.12542610\n",
      "Iteration 103, loss = 0.44012937\n",
      "Iteration 1561, loss = 0.15960196\n",
      "Iteration 1987, loss = 0.18477999\n",
      "Iteration 381, loss = 0.40885733\n",
      "Iteration 2548, loss = 0.12531656\n",
      "Iteration 1296, loss = 0.26860248\n",
      "Iteration 1562, loss = 0.15930983\n",
      "Iteration 104, loss = 0.43888082\n",
      "Iteration 1332, loss = 0.20903000\n",
      "Iteration 16, loss = 0.73611205\n",
      "Iteration 1563, loss = 0.15913058\n",
      "Iteration 2549, loss = 0.12525998\n",
      "Iteration 105, loss = 0.43774514\n",
      "Iteration 2550, loss = 0.12533682\n",
      "Iteration 1988, loss = 0.18477337\n",
      "Iteration 17, loss = 0.73267885\n",
      "Iteration 106, loss = 0.43653104\n",
      "Iteration 1333, loss = 0.20870837\n",
      "Iteration 2551, loss = 0.12512352\n",
      "Iteration 1297, loss = 0.26846812\n",
      "Iteration 1564, loss = 0.15894422\n",
      "Iteration 2552, loss = 0.12506483\n",
      "Iteration 1989, loss = 0.18455721\n",
      "Iteration 107, loss = 0.43534103\n",
      "Iteration 2553, loss = 0.12501354\n",
      "Iteration 382, loss = 0.40864938\n",
      "Iteration 2554, loss = 0.12495189\n",
      "Iteration 18, loss = 0.72909370\n",
      "Iteration 1565, loss = 0.15884131\n",
      "Iteration 1990, loss = 0.18451784\n",
      "Iteration 2555, loss = 0.12505707\n",
      "Iteration 108, loss = 0.43417467\n",
      "Iteration 1298, loss = 0.26840886\n",
      "Iteration 2556, loss = 0.12481706\n",
      "Iteration 109, loss = 0.43307949\n",
      "Iteration 1334, loss = 0.20854836\n",
      "Iteration 19, loss = 0.72576028\n",
      "Iteration 1566, loss = 0.15863643\n",
      "Iteration 383, loss = 0.40841986\n",
      "Iteration 110, loss = 0.43203564\n",
      "Iteration 1991, loss = 0.18445569\n",
      "Iteration 1299, loss = 0.26825124\n",
      "Iteration 2557, loss = 0.12484672\n",
      "Iteration 111, loss = 0.43095651\n",
      "Iteration 1567, loss = 0.15860153\n",
      "Iteration 112, loss = 0.42986323\n",
      "Iteration 1300, loss = 0.26812236\n",
      "Iteration 384, loss = 0.40823842\n",
      "Iteration 20, loss = 0.72237174\n",
      "Iteration 113, loss = 0.42884776\n",
      "Iteration 1568, loss = 0.15856635\n",
      "Iteration 1301, loss = 0.26797723\n",
      "Iteration 114, loss = 0.42780994\n",
      "Iteration 21, loss = 0.71907202\n",
      "Iteration 1992, loss = 0.18435167\n",
      "Iteration 1335, loss = 0.20836134\n",
      "Iteration 385, loss = 0.40804746\n",
      "Iteration 2558, loss = 0.12478246\n",
      "Iteration 1302, loss = 0.26785785\n",
      "Iteration 1569, loss = 0.15820828\n",
      "Iteration 115, loss = 0.42682143\n",
      "Iteration 1993, loss = 0.18411407\n",
      "Iteration 22, loss = 0.71587268\n",
      "Iteration 1570, loss = 0.15813483\n",
      "Iteration 2559, loss = 0.12462039\n",
      "Iteration 1303, loss = 0.26772333\n",
      "Iteration 1571, loss = 0.15796949\n",
      "Iteration 116, loss = 0.42588605\n",
      "Iteration 2560, loss = 0.12459189\n",
      "Iteration 386, loss = 0.40782145\n",
      "Iteration 1994, loss = 0.18419461\n",
      "Iteration 1336, loss = 0.20817335\n",
      "Iteration 1304, loss = 0.26762123\n",
      "Iteration 117, loss = 0.42489961\n",
      "Iteration 2561, loss = 0.12458553\n",
      "Iteration 1572, loss = 0.15777783\n",
      "Iteration 2562, loss = 0.12439956\n",
      "Iteration 23, loss = 0.71269056\n",
      "Iteration 1995, loss = 0.18396940\n",
      "Iteration 1573, loss = 0.15788460\n",
      "Iteration 387, loss = 0.40759367\n",
      "Iteration 118, loss = 0.42397856\n",
      "Iteration 2563, loss = 0.12436671\n",
      "Iteration 1574, loss = 0.15749236\n",
      "Iteration 1305, loss = 0.26747103\n",
      "Iteration 1337, loss = 0.20833070\n",
      "Iteration 1996, loss = 0.18387205\n",
      "Iteration 119, loss = 0.42301570\n",
      "Iteration 24, loss = 0.70942139\n",
      "Iteration 2564, loss = 0.12430689\n",
      "Iteration 1306, loss = 0.26737866\n",
      "Iteration 120, loss = 0.42213003\n",
      "Iteration 1575, loss = 0.15737783\n",
      "Iteration 1997, loss = 0.18388944\n",
      "Iteration 2565, loss = 0.12420851\n",
      "Iteration 1338, loss = 0.20785180\n",
      "Iteration 121, loss = 0.42121984\n",
      "Iteration 388, loss = 0.40739900\n",
      "Iteration 1307, loss = 0.26734002\n",
      "Iteration 122, loss = 0.42033388\n",
      "Iteration 25, loss = 0.70621157\n",
      "Iteration 1576, loss = 0.15727634\n",
      "Iteration 1339, loss = 0.20764233\n",
      "Iteration 123, loss = 0.41949219\n",
      "Iteration 1308, loss = 0.26712815\n",
      "Iteration 124, loss = 0.41865776\n",
      "Iteration 26, loss = 0.70304846\n",
      "Iteration 1998, loss = 0.18376180\n",
      "Iteration 125, loss = 0.41778675\n",
      "Iteration 2566, loss = 0.12441933\n",
      "Iteration 27, loss = 0.69986855\n",
      "Iteration 126, loss = 0.41702482\n",
      "Iteration 1309, loss = 0.26700525\n",
      "Iteration 1999, loss = 0.18349790\n",
      "Iteration 1340, loss = 0.20751553\n",
      "Iteration 389, loss = 0.40723296\n",
      "Iteration 28, loss = 0.69669381\n",
      "Iteration 1577, loss = 0.15725741\n",
      "Iteration 127, loss = 0.41615982\n",
      "Iteration 2567, loss = 0.12416455\n",
      "Iteration 2000, loss = 0.18343972\n",
      "Iteration 1578, loss = 0.15735042\n",
      "Iteration 128, loss = 0.41543083\n",
      "Iteration 390, loss = 0.40704314\n",
      "Iteration 2568, loss = 0.12416874\n",
      "Iteration 129, loss = 0.41464438\n",
      "Iteration 1579, loss = 0.15689538\n",
      "Iteration 29, loss = 0.69348546\n",
      "Iteration 2569, loss = 0.12392922\n",
      "Iteration 130, loss = 0.41390707\n",
      "Iteration 1310, loss = 0.26684723\n",
      "Iteration 2001, loss = 0.18335019\n",
      "Iteration 1341, loss = 0.20737037\n",
      "Iteration 2570, loss = 0.12402577\n",
      "Iteration 30, loss = 0.69033826\n",
      "Iteration 1580, loss = 0.15668579\n",
      "Iteration 2002, loss = 0.18317047\n",
      "Iteration 2571, loss = 0.12388323\n",
      "Iteration 1311, loss = 0.26669885\n",
      "Iteration 2572, loss = 0.12374445\n",
      "Iteration 2573, loss = 0.12372609\n",
      "Iteration 1581, loss = 0.15642981\n",
      "Iteration 31, loss = 0.68722870\n",
      "Iteration 391, loss = 0.40678929\n",
      "Iteration 131, loss = 0.41311201Iteration 1312, loss = 0.26659277\n",
      "Iteration 2574, loss = 0.12364669\n",
      "\n",
      "Iteration 2575, loss = 0.12359428\n",
      "Iteration 132, loss = 0.41244176\n",
      "Iteration 2576, loss = 0.12356258\n",
      "Iteration 32, loss = 0.68400150\n",
      "Iteration 133, loss = 0.41170226\n",
      "Iteration 1582, loss = 0.15668074\n",
      "Iteration 2577, loss = 0.12353547\n",
      "Iteration 2003, loss = 0.18314118\n",
      "Iteration 33, loss = 0.68080808\n",
      "Iteration 134, loss = 0.41100327\n",
      "Iteration 2578, loss = 0.12342284\n",
      "Iteration 1583, loss = 0.15623890\n",
      "Iteration 135, loss = 0.41026478\n",
      "Iteration 1342, loss = 0.20721644\n",
      "Iteration 392, loss = 0.40657760\n",
      "Iteration 1313, loss = 0.26644691\n",
      "Iteration 2004, loss = 0.18301348\n",
      "Iteration 2579, loss = 0.12336294\n",
      "Iteration 34, loss = 0.67769468\n",
      "Iteration 1584, loss = 0.15615502\n",
      "Iteration 136, loss = 0.40964488\n",
      "Iteration 1314, loss = 0.26633852\n",
      "Iteration 1343, loss = 0.20709668\n",
      "Iteration 2005, loss = 0.18296323\n",
      "Iteration 137, loss = 0.40892326\n",
      "Iteration 1585, loss = 0.15601805\n",
      "Iteration 138, loss = 0.40828139\n",
      "Iteration 2006, loss = 0.18282197\n",
      "Iteration 2580, loss = 0.12329785\n",
      "Iteration 35, loss = 0.67457542\n",
      "Iteration 139, loss = 0.40762699\n",
      "Iteration 1344, loss = 0.20687610\n",
      "Iteration 2581, loss = 0.12318468\n",
      "Iteration 1315, loss = 0.26621668\n",
      "Iteration 393, loss = 0.40645231\n",
      "Iteration 2007, loss = 0.18270418\n",
      "Iteration 2582, loss = 0.12320330\n",
      "Iteration 36, loss = 0.67143372\n",
      "Iteration 2583, loss = 0.12305317\n",
      "Iteration 1345, loss = 0.20665450\n",
      "Iteration 394, loss = 0.40616916\n",
      "Iteration 2008, loss = 0.18267060\n",
      "Iteration 1586, loss = 0.15572480\n",
      "Iteration 2584, loss = 0.12304300\n",
      "Iteration 1316, loss = 0.26607116\n",
      "Iteration 140, loss = 0.40695195\n",
      "Iteration 1587, loss = 0.15561049\n",
      "Iteration 2585, loss = 0.12304463\n",
      "Iteration 1317, loss = 0.26595621\n",
      "Iteration 395, loss = 0.40616796\n",
      "Iteration 141, loss = 0.40633384\n",
      "Iteration 1588, loss = 0.15561236\n",
      "Iteration 2009, loss = 0.18248445\n",
      "Iteration 142, loss = 0.40576027\n",
      "Iteration 1589, loss = 0.15539513\n",
      "Iteration 2010, loss = 0.18245149\n",
      "Iteration 1346, loss = 0.20661151\n",
      "Iteration 1590, loss = 0.15552348\n",
      "Iteration 143, loss = 0.40508400\n",
      "Iteration 2586, loss = 0.12286168\n",
      "Iteration 1318, loss = 0.26581107\n",
      "Iteration 37, loss = 0.66820187\n",
      "Iteration 1591, loss = 0.15506304\n",
      "Iteration 1319, loss = 0.26568847\n",
      "Iteration 144, loss = 0.40453972\n",
      "Iteration 2587, loss = 0.12278684\n",
      "Iteration 145, loss = 0.40393260\n",
      "Iteration 396, loss = 0.40588032\n",
      "Iteration 1592, loss = 0.15503879\n",
      "Iteration 2588, loss = 0.12272221\n",
      "Iteration 1320, loss = 0.26558156\n",
      "Iteration 2011, loss = 0.18230047\n",
      "Iteration 38, loss = 0.66503104\n",
      "Iteration 146, loss = 0.40329996\n",
      "Iteration 2589, loss = 0.12266296Iteration 1347, loss = 0.20649277\n",
      "\n",
      "Iteration 1593, loss = 0.15487016\n",
      "Iteration 397, loss = 0.40555576\n",
      "Iteration 1594, loss = 0.15466346\n",
      "Iteration 147, loss = 0.40272315\n",
      "Iteration 1321, loss = 0.26545158\n",
      "Iteration 1595, loss = 0.15462924\n",
      "Iteration 398, loss = 0.40540920\n",
      "Iteration 1348, loss = 0.20621904\n",
      "Iteration 2590, loss = 0.12268798\n",
      "Iteration 148, loss = 0.40230010\n",
      "Iteration 1322, loss = 0.26531451\n",
      "Iteration 1596, loss = 0.15435290\n",
      "Iteration 39, loss = 0.66181207\n",
      "Iteration 2591, loss = 0.12253476\n",
      "Iteration 149, loss = 0.40157033\n",
      "Iteration 2012, loss = 0.18221260\n",
      "Iteration 2592, loss = 0.12246691\n",
      "Iteration 1597, loss = 0.15419721\n",
      "Iteration 399, loss = 0.40514675\n",
      "Iteration 1349, loss = 0.20596029\n",
      "Iteration 40, loss = 0.65866077\n",
      "Iteration 150, loss = 0.40099984\n",
      "Iteration 2593, loss = 0.12239449\n",
      "Iteration 1598, loss = 0.15437416\n",
      "Iteration 1323, loss = 0.26519762\n",
      "Iteration 2594, loss = 0.12238443\n",
      "Iteration 41, loss = 0.65550393\n",
      "Iteration 1599, loss = 0.15392818\n",
      "Iteration 2595, loss = 0.12238469\n",
      "Iteration 151, loss = 0.40047033\n",
      "Iteration 2596, loss = 0.12229232\n",
      "Iteration 400, loss = 0.40497137\n",
      "Iteration 1324, loss = 0.26506294\n",
      "Iteration 152, loss = 0.39994850\n",
      "Iteration 2013, loss = 0.18205586\n",
      "Iteration 153, loss = 0.39935838\n",
      "Iteration 42, loss = 0.65231863\n",
      "Iteration 1600, loss = 0.15396578\n",
      "Iteration 1350, loss = 0.20624396\n",
      "Iteration 154, loss = 0.39886904\n",
      "Iteration 1325, loss = 0.26493434\n",
      "Iteration 2597, loss = 0.12225375\n",
      "Iteration 43, loss = 0.64904996\n",
      "Iteration 2598, loss = 0.12212340\n",
      "Iteration 1351, loss = 0.20580049\n",
      "Iteration 2014, loss = 0.18200322\n",
      "Iteration 1601, loss = 0.15370723\n",
      "Iteration 1326, loss = 0.26480746\n",
      "Iteration 2599, loss = 0.12202912\n",
      "Iteration 2600, loss = 0.12196698\n",
      "Iteration 2015, loss = 0.18208578\n",
      "Iteration 155, loss = 0.39832885\n",
      "Iteration 401, loss = 0.40481297\n",
      "Iteration 1602, loss = 0.15356869\n",
      "Iteration 44, loss = 0.64582609\n",
      "Iteration 2016, loss = 0.18180733\n",
      "Iteration 156, loss = 0.39783993\n",
      "Iteration 1352, loss = 0.20547759\n",
      "Iteration 1603, loss = 0.15339848\n",
      "Iteration 1327, loss = 0.26472064\n",
      "Iteration 157, loss = 0.39734804\n",
      "Iteration 2601, loss = 0.12195226\n",
      "Iteration 402, loss = 0.40457367\n",
      "Iteration 1604, loss = 0.15324347\n",
      "Iteration 158, loss = 0.39681867\n",
      "Iteration 2017, loss = 0.18178282\n",
      "Iteration 45, loss = 0.64264620\n",
      "Iteration 1605, loss = 0.15307327\n",
      "Iteration 159, loss = 0.39634891\n",
      "Iteration 46, loss = 0.63944606\n",
      "Iteration 1353, loss = 0.20541896\n",
      "Iteration 1606, loss = 0.15295671\n",
      "Iteration 1328, loss = 0.26462232\n",
      "Iteration 160, loss = 0.39583975\n",
      "Iteration 2602, loss = 0.12194482\n",
      "Iteration 161, loss = 0.39536974\n",
      "Iteration 2018, loss = 0.18159430\n",
      "Iteration 1607, loss = 0.15281473\n",
      "Iteration 403, loss = 0.40431197\n",
      "Iteration 47, loss = 0.63625040\n",
      "Iteration 1329, loss = 0.26452904\n",
      "Iteration 2603, loss = 0.12184002\n",
      "Iteration 162, loss = 0.39489029\n",
      "Iteration 2019, loss = 0.18150464\n",
      "Iteration 1608, loss = 0.15266184\n",
      "Iteration 163, loss = 0.39439813\n",
      "Iteration 2604, loss = 0.12174165\n",
      "Iteration 48, loss = 0.63300527\n",
      "Iteration 1354, loss = 0.20519560Iteration 164, loss = 0.39393646\n",
      "\n",
      "Iteration 2020, loss = 0.18156691\n",
      "Iteration 1330, loss = 0.26434143\n",
      "Iteration 2605, loss = 0.12168185\n",
      "Iteration 1609, loss = 0.15260994\n",
      "Iteration 2606, loss = 0.12157660\n",
      "Iteration 404, loss = 0.40413451\n",
      "Iteration 1610, loss = 0.15243579\n",
      "Iteration 1331, loss = 0.26417952\n",
      "Iteration 2607, loss = 0.12148464\n",
      "Iteration 165, loss = 0.39352371\n",
      "Iteration 1611, loss = 0.15246419\n",
      "Iteration 49, loss = 0.62986845\n",
      "Iteration 2608, loss = 0.12146805\n",
      "Iteration 166, loss = 0.39305109\n",
      "Iteration 2609, loss = 0.12142829\n",
      "Iteration 2021, loss = 0.18130939\n",
      "Iteration 1355, loss = 0.20503545\n",
      "Iteration 167, loss = 0.39259231\n",
      "Iteration 2610, loss = 0.12136380\n",
      "Iteration 168, loss = 0.39213592\n",
      "Iteration 2611, loss = 0.12125622\n",
      "Iteration 1332, loss = 0.26404729\n",
      "Iteration 405, loss = 0.40395974\n",
      "Iteration 1612, loss = 0.15216655\n",
      "Iteration 50, loss = 0.62656104\n",
      "Iteration 1356, loss = 0.20481080\n",
      "Iteration 169, loss = 0.39171006\n",
      "Iteration 1613, loss = 0.15201093\n",
      "Iteration 1333, loss = 0.26398507\n",
      "Iteration 51, loss = 0.62343096\n",
      "Iteration 2022, loss = 0.18123311\n",
      "Iteration 170, loss = 0.39128843\n",
      "Iteration 2612, loss = 0.12147084\n",
      "Iteration 171, loss = 0.39088628\n",
      "Iteration 1334, loss = 0.26378423\n",
      "Iteration 1614, loss = 0.15191739\n",
      "Iteration 2023, loss = 0.18114935\n",
      "Iteration 172, loss = 0.39045830\n",
      "Iteration 1357, loss = 0.20479020\n",
      "Iteration 2613, loss = 0.12118791\n",
      "Iteration 173, loss = 0.39003816\n",
      "Iteration 406, loss = 0.40372829Iteration 2614, loss = 0.12110041\n",
      "\n",
      "Iteration 52, loss = 0.62015385\n",
      "Iteration 174, loss = 0.38960193\n",
      "Iteration 2615, loss = 0.12125458\n",
      "Iteration 175, loss = 0.38917839\n",
      "Iteration 1335, loss = 0.26366728\n",
      "Iteration 2616, loss = 0.12095543\n",
      "Iteration 1615, loss = 0.15181332\n",
      "Iteration 2024, loss = 0.18110046\n",
      "Iteration 1336, loss = 0.26356525\n",
      "Iteration 53, loss = 0.61705657\n",
      "Iteration 407, loss = 0.40354318\n",
      "Iteration 176, loss = 0.38879512\n",
      "Iteration 1358, loss = 0.20475855\n",
      "Iteration 2617, loss = 0.12095564\n",
      "Iteration 1616, loss = 0.15165326\n",
      "Iteration 1337, loss = 0.26343881\n",
      "Iteration 2618, loss = 0.12087704\n",
      "Iteration 1617, loss = 0.15138945\n",
      "Iteration 2025, loss = 0.18090197\n",
      "Iteration 54, loss = 0.61376450\n",
      "Iteration 1618, loss = 0.15141129\n",
      "Iteration 1359, loss = 0.20432546\n",
      "Iteration 177, loss = 0.38841104\n",
      "Iteration 2619, loss = 0.12077335\n",
      "Iteration 408, loss = 0.40334495\n",
      "Iteration 178, loss = 0.38802959\n",
      "Iteration 1619, loss = 0.15126761\n",
      "Iteration 1338, loss = 0.26330951\n",
      "Iteration 2620, loss = 0.12077095\n",
      "Iteration 179, loss = 0.38760343\n",
      "Iteration 55, loss = 0.61068847\n",
      "Iteration 2026, loss = 0.18098893\n",
      "Iteration 180, loss = 0.38721338\n",
      "Iteration 1620, loss = 0.15113455\n",
      "Iteration 181, loss = 0.38683921\n",
      "Iteration 1360, loss = 0.20428492Iteration 2621, loss = 0.12068360\n",
      "\n",
      "Iteration 182, loss = 0.38645902\n",
      "Iteration 2622, loss = 0.12059365\n",
      "Iteration 183, loss = 0.38612099\n",
      "Iteration 56, loss = 0.60747599\n",
      "Iteration 1339, loss = 0.26319378\n",
      "Iteration 409, loss = 0.40312773\n",
      "Iteration 1621, loss = 0.15107278\n",
      "Iteration 184, loss = 0.38573647\n",
      "Iteration 2027, loss = 0.18077016\n",
      "Iteration 2623, loss = 0.12058190\n",
      "Iteration 185, loss = 0.38536118\n",
      "Iteration 2624, loss = 0.12052164\n",
      "Iteration 1622, loss = 0.15084763\n",
      "Iteration 410, loss = 0.40294408\n",
      "Iteration 2625, loss = 0.12038415\n",
      "Iteration 1361, loss = 0.20419003\n",
      "Iteration 186, loss = 0.38498009\n",
      "Iteration 2626, loss = 0.12036628\n",
      "Iteration 187, loss = 0.38464134\n",
      "Iteration 1623, loss = 0.15069261\n",
      "Iteration 2627, loss = 0.12032671\n",
      "Iteration 57, loss = 0.60436891\n",
      "Iteration 2028, loss = 0.18068186\n",
      "Iteration 1340, loss = 0.26308253\n",
      "Iteration 411, loss = 0.40271730\n",
      "Iteration 2628, loss = 0.12023786\n",
      "Iteration 188, loss = 0.38434513\n",
      "Iteration 2629, loss = 0.12019690\n",
      "Iteration 1624, loss = 0.15049688\n",
      "Iteration 189, loss = 0.38392956\n",
      "Iteration 2630, loss = 0.12019223\n",
      "Iteration 1362, loss = 0.20389000\n",
      "Iteration 190, loss = 0.38360411\n",
      "Iteration 2029, loss = 0.18056307\n",
      "Iteration 58, loss = 0.60108426\n",
      "Iteration 1625, loss = 0.15080774\n",
      "Iteration 191, loss = 0.38323917\n",
      "Iteration 1341, loss = 0.26292651\n",
      "Iteration 2631, loss = 0.12024799\n",
      "Iteration 412, loss = 0.40250742\n",
      "Iteration 1363, loss = 0.20369524\n",
      "Iteration 1626, loss = 0.15031938\n",
      "Iteration 192, loss = 0.38289326\n",
      "Iteration 59, loss = 0.59798919\n",
      "Iteration 1342, loss = 0.26279531\n",
      "Iteration 2030, loss = 0.18043604\n",
      "Iteration 193, loss = 0.38258837\n",
      "Iteration 2632, loss = 0.12008448\n",
      "Iteration 194, loss = 0.38224555\n",
      "Iteration 1627, loss = 0.15014186\n",
      "Iteration 2633, loss = 0.11991371\n",
      "Iteration 2031, loss = 0.18043461\n",
      "Iteration 1364, loss = 0.20357329\n",
      "Iteration 195, loss = 0.38190194\n",
      "Iteration 413, loss = 0.40229757\n",
      "Iteration 1343, loss = 0.26275701\n",
      "Iteration 60, loss = 0.59485885\n",
      "Iteration 1628, loss = 0.15005153\n",
      "Iteration 196, loss = 0.38164553\n",
      "Iteration 2634, loss = 0.11987157\n",
      "Iteration 197, loss = 0.38126169\n",
      "Iteration 2032, loss = 0.18023219\n",
      "Iteration 61, loss = 0.59161552\n",
      "Iteration 198, loss = 0.38090661\n",
      "Iteration 2635, loss = 0.11985876\n",
      "Iteration 1344, loss = 0.26258789\n",
      "Iteration 2636, loss = 0.11979074\n",
      "Iteration 1365, loss = 0.20361548\n",
      "Iteration 414, loss = 0.40208960\n",
      "Iteration 199, loss = 0.38060406\n",
      "Iteration 1629, loss = 0.14996158\n",
      "Iteration 2637, loss = 0.11973259\n",
      "Iteration 1345, loss = 0.26240186\n",
      "Iteration 2033, loss = 0.18013169\n",
      "Iteration 200, loss = 0.38031629\n",
      "Iteration 62, loss = 0.58858894\n",
      "Iteration 1366, loss = 0.20328577\n",
      "Iteration 2638, loss = 0.11965402\n",
      "Iteration 201, loss = 0.37998888\n",
      "Iteration 2639, loss = 0.11959025\n",
      "Iteration 1630, loss = 0.14973188\n",
      "Iteration 415, loss = 0.40192542\n",
      "Iteration 202, loss = 0.37968201\n",
      "Iteration 1346, loss = 0.26245567\n",
      "Iteration 63, loss = 0.58551165\n",
      "Iteration 2640, loss = 0.11954289\n",
      "Iteration 2034, loss = 0.18012024\n",
      "Iteration 203, loss = 0.37943814\n",
      "Iteration 204, loss = 0.37911726\n",
      "Iteration 1367, loss = 0.20302324\n",
      "Iteration 2641, loss = 0.11947234\n",
      "Iteration 1631, loss = 0.14959457\n",
      "Iteration 64, loss = 0.58245208\n",
      "Iteration 2035, loss = 0.18004641\n",
      "Iteration 205, loss = 0.37881823\n",
      "Iteration 1347, loss = 0.26217862\n",
      "Iteration 416, loss = 0.40168914\n",
      "Iteration 2642, loss = 0.11939581\n",
      "Iteration 1632, loss = 0.14944462\n",
      "Iteration 206, loss = 0.37848645\n",
      "Iteration 65, loss = 0.57941574\n",
      "Iteration 1348, loss = 0.26206643\n",
      "Iteration 1633, loss = 0.14942951\n",
      "Iteration 2643, loss = 0.11930944\n",
      "Iteration 207, loss = 0.37825106\n",
      "Iteration 2036, loss = 0.17983331\n",
      "Iteration 1368, loss = 0.20301822\n",
      "Iteration 2644, loss = 0.11927315\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 208, loss = 0.37791262\n",
      "Iteration 1634, loss = 0.14937485\n",
      "Iteration 1349, loss = 0.26190962\n",
      "Iteration 66, loss = 0.57627559\n",
      "Iteration 1635, loss = 0.14915255\n",
      "Iteration 2037, loss = 0.17977532\n",
      "Iteration 209, loss = 0.37766861\n",
      "Iteration 1369, loss = 0.20273463\n",
      "Iteration 417, loss = 0.40164273\n",
      "Iteration 210, loss = 0.37734823\n",
      "Iteration 1350, loss = 0.26177164\n",
      "Iteration 67, loss = 0.57338954\n",
      "Iteration 1636, loss = 0.14910976\n",
      "Iteration 2038, loss = 0.17968931\n",
      "Iteration 1351, loss = 0.26169794\n",
      "Iteration 211, loss = 0.37707789\n",
      "Iteration 418, loss = 0.40129210\n",
      "Iteration 68, loss = 0.57033188\n",
      "Iteration 1637, loss = 0.14882935\n",
      "Iteration 1, loss = 0.77105976\n",
      "Iteration 212, loss = 0.37677568\n",
      "Iteration 1352, loss = 0.26149702\n",
      "Iteration 2039, loss = 0.17970571\n",
      "Iteration 2, loss = 0.76975507\n",
      "Iteration 1370, loss = 0.20263629\n",
      "Iteration 3, loss = 0.76774821\n",
      "Iteration 4, loss = 0.76523640\n",
      "Iteration 69, loss = 0.56751539\n",
      "Iteration 213, loss = 0.37652490\n",
      "Iteration 5, loss = 0.76266520\n",
      "Iteration 1638, loss = 0.14869020\n",
      "Iteration 6, loss = 0.75989963\n",
      "Iteration 214, loss = 0.37628333\n",
      "Iteration 419, loss = 0.40114635\n",
      "Iteration 7, loss = 0.75715876\n",
      "Iteration 1371, loss = 0.20251906\n",
      "Iteration 70, loss = 0.56453440\n",
      "Iteration 1639, loss = 0.14855027\n",
      "Iteration 1353, loss = 0.26146273\n",
      "Iteration 215, loss = 0.37595343\n",
      "Iteration 2040, loss = 0.17966310\n",
      "Iteration 8, loss = 0.75431045\n",
      "Iteration 216, loss = 0.37571735\n",
      "Iteration 1640, loss = 0.14857737\n",
      "Iteration 1641, loss = 0.14831600\n",
      "Iteration 2041, loss = 0.17938439\n",
      "Iteration 71, loss = 0.56160271\n",
      "Iteration 9, loss = 0.75156042\n",
      "Iteration 420, loss = 0.40090704\n",
      "Iteration 217, loss = 0.37542990\n",
      "Iteration 10, loss = 0.74885922\n",
      "Iteration 1372, loss = 0.20228348\n",
      "Iteration 11, loss = 0.74640627\n",
      "Iteration 12, loss = 0.74406901\n",
      "Iteration 1354, loss = 0.26131893\n",
      "Iteration 13, loss = 0.74172368\n",
      "Iteration 1642, loss = 0.14829415\n",
      "Iteration 218, loss = 0.37517161\n",
      "Iteration 14, loss = 0.73947919\n",
      "Iteration 2042, loss = 0.17933260\n",
      "Iteration 15, loss = 0.73735280\n",
      "Iteration 72, loss = 0.55877122\n",
      "Iteration 16, loss = 0.73532450\n",
      "Iteration 421, loss = 0.40073073\n",
      "Iteration 17, loss = 0.73353587\n",
      "Iteration 219, loss = 0.37490413\n",
      "Iteration 1373, loss = 0.20213152\n",
      "Iteration 1355, loss = 0.26116081\n",
      "Iteration 1643, loss = 0.14803629\n",
      "Iteration 73, loss = 0.55595689\n",
      "Iteration 220, loss = 0.37465882\n",
      "Iteration 1356, loss = 0.26103349\n",
      "Iteration 422, loss = 0.40049446\n",
      "Iteration 18, loss = 0.73158995\n",
      "Iteration 19, loss = 0.72986473\n",
      "Iteration 2043, loss = 0.17927043\n",
      "Iteration 20, loss = 0.72807893\n",
      "Iteration 1644, loss = 0.14801283\n",
      "Iteration 21, loss = 0.72649817\n",
      "Iteration 221, loss = 0.37443832\n",
      "Iteration 423, loss = 0.40031062\n",
      "Iteration 22, loss = 0.72486170\n",
      "Iteration 1357, loss = 0.26094986\n",
      "Iteration 1374, loss = 0.20207032\n",
      "Iteration 1645, loss = 0.14778638\n",
      "Iteration 23, loss = 0.72329901\n",
      "Iteration 74, loss = 0.55312322\n",
      "Iteration 222, loss = 0.37414907\n",
      "Iteration 2044, loss = 0.17908400\n",
      "Iteration 24, loss = 0.72185052\n",
      "Iteration 2045, loss = 0.17899261\n",
      "Iteration 75, loss = 0.55037539\n",
      "Iteration 223, loss = 0.37388576\n",
      "Iteration 1375, loss = 0.20176851\n",
      "Iteration 25, loss = 0.72032962\n",
      "Iteration 1646, loss = 0.14770347\n",
      "Iteration 26, loss = 0.71887089Iteration 1358, loss = 0.26079176\n",
      "\n",
      "Iteration 27, loss = 0.71740505\n",
      "Iteration 2046, loss = 0.17889017\n",
      "Iteration 224, loss = 0.37366968\n",
      "Iteration 28, loss = 0.71602171\n",
      "Iteration 424, loss = 0.40023336\n",
      "Iteration 1359, loss = 0.26062450\n",
      "Iteration 1647, loss = 0.14757958\n",
      "Iteration 2047, loss = 0.17876431\n",
      "Iteration 76, loss = 0.54760616\n",
      "Iteration 29, loss = 0.71460004\n",
      "Iteration 30, loss = 0.71330777\n",
      "Iteration 225, loss = 0.37341693\n",
      "Iteration 1648, loss = 0.14753891\n",
      "Iteration 31, loss = 0.71187242\n",
      "Iteration 226, loss = 0.37313784\n",
      "Iteration 1360, loss = 0.26052097\n",
      "Iteration 1649, loss = 0.14725985\n",
      "Iteration 32, loss = 0.71058475\n",
      "Iteration 1376, loss = 0.20173033\n",
      "Iteration 227, loss = 0.37292404\n",
      "Iteration 77, loss = 0.54492749\n",
      "Iteration 228, loss = 0.37274149\n",
      "Iteration 33, loss = 0.70930043\n",
      "Iteration 2048, loss = 0.17871298\n",
      "Iteration 425, loss = 0.39989310\n",
      "Iteration 1650, loss = 0.14729129\n",
      "Iteration 1361, loss = 0.26045391\n",
      "Iteration 34, loss = 0.70798895\n",
      "Iteration 78, loss = 0.54230114\n",
      "Iteration 229, loss = 0.37242013\n",
      "Iteration 35, loss = 0.70670094\n",
      "Iteration 1651, loss = 0.14701141\n",
      "Iteration 1362, loss = 0.26029103\n",
      "Iteration 79, loss = 0.53961043\n",
      "Iteration 36, loss = 0.70537859\n",
      "Iteration 1377, loss = 0.20156725\n",
      "Iteration 230, loss = 0.37220790\n",
      "Iteration 2049, loss = 0.17857988\n",
      "Iteration 37, loss = 0.70407267\n",
      "Iteration 38, loss = 0.70276500\n",
      "Iteration 1652, loss = 0.14693173\n",
      "Iteration 426, loss = 0.39970667\n",
      "Iteration 39, loss = 0.70148865\n",
      "Iteration 231, loss = 0.37196138\n",
      "Iteration 40, loss = 0.70023076\n",
      "Iteration 41, loss = 0.69896769\n",
      "Iteration 1653, loss = 0.14675543\n",
      "Iteration 232, loss = 0.37173367\n",
      "Iteration 42, loss = 0.69767895\n",
      "Iteration 80, loss = 0.53699882\n",
      "Iteration 43, loss = 0.69640213\n",
      "Iteration 427, loss = 0.39952089\n",
      "Iteration 44, loss = 0.69512272\n",
      "Iteration 1378, loss = 0.20127958\n",
      "Iteration 1363, loss = 0.26012145\n",
      "Iteration 45, loss = 0.69377597\n",
      "Iteration 233, loss = 0.37151310\n",
      "Iteration 46, loss = 0.69249799\n",
      "Iteration 81, loss = 0.53445650\n",
      "Iteration 47, loss = 0.69122169\n",
      "Iteration 2050, loss = 0.17866602\n",
      "Iteration 48, loss = 0.68985728\n",
      "Iteration 1379, loss = 0.20107364\n",
      "Iteration 234, loss = 0.37129789\n",
      "Iteration 1364, loss = 0.26000014\n",
      "Iteration 82, loss = 0.53199509\n",
      "Iteration 49, loss = 0.68856297\n",
      "Iteration 1654, loss = 0.14666933\n",
      "Iteration 428, loss = 0.39929557\n",
      "Iteration 50, loss = 0.68721975\n",
      "Iteration 235, loss = 0.37105815\n",
      "Iteration 1380, loss = 0.20094138\n",
      "Iteration 51, loss = 0.68587156\n",
      "Iteration 1655, loss = 0.14648592\n",
      "Iteration 1365, loss = 0.25989589\n",
      "Iteration 236, loss = 0.37083231\n",
      "Iteration 52, loss = 0.68447492\n",
      "Iteration 83, loss = 0.52945632\n",
      "Iteration 1656, loss = 0.14646158\n",
      "Iteration 1366, loss = 0.25974077\n",
      "Iteration 237, loss = 0.37061221\n",
      "Iteration 429, loss = 0.39916102\n",
      "Iteration 53, loss = 0.68313179\n",
      "Iteration 1657, loss = 0.14625841\n",
      "Iteration 54, loss = 0.68170745\n",
      "Iteration 238, loss = 0.37037873\n",
      "Iteration 55, loss = 0.68032036\n",
      "Iteration 239, loss = 0.37015274\n",
      "Iteration 1367, loss = 0.25962456\n",
      "Iteration 1381, loss = 0.20075464\n",
      "Iteration 84, loss = 0.52711045\n",
      "Iteration 56, loss = 0.67886881\n",
      "Iteration 1658, loss = 0.14623881\n",
      "Iteration 57, loss = 0.67737380\n",
      "Iteration 58, loss = 0.67593309\n",
      "Iteration 240, loss = 0.36998993\n",
      "Iteration 1368, loss = 0.25955249\n",
      "Iteration 59, loss = 0.67446571\n",
      "Iteration 2051, loss = 0.17841896\n",
      "Iteration 1659, loss = 0.14600925\n",
      "Iteration 60, loss = 0.67296464\n",
      "Iteration 1382, loss = 0.20061299\n",
      "Iteration 241, loss = 0.36974632\n",
      "Iteration 430, loss = 0.39890607\n",
      "Iteration 85, loss = 0.52467989\n",
      "Iteration 61, loss = 0.67139883\n",
      "Iteration 242, loss = 0.36950492\n",
      "Iteration 1369, loss = 0.25940729\n",
      "Iteration 1660, loss = 0.14599349\n",
      "Iteration 62, loss = 0.66983377\n",
      "Iteration 243, loss = 0.36935138\n",
      "Iteration 63, loss = 0.66827315\n",
      "Iteration 64, loss = 0.66668760\n",
      "Iteration 1383, loss = 0.20045320\n",
      "Iteration 244, loss = 0.36913104\n",
      "Iteration 431, loss = 0.39869780\n",
      "Iteration 86, loss = 0.52227881\n",
      "Iteration 65, loss = 0.66505085\n",
      "Iteration 245, loss = 0.36887143\n",
      "Iteration 1661, loss = 0.14580962\n",
      "Iteration 66, loss = 0.66340322\n",
      "Iteration 1370, loss = 0.25923741\n",
      "Iteration 246, loss = 0.36868479\n",
      "Iteration 67, loss = 0.66169545\n",
      "Iteration 247, loss = 0.36849906\n",
      "Iteration 68, loss = 0.66002199\n",
      "Iteration 87, loss = 0.52004973\n",
      "Iteration 1384, loss = 0.20049413\n",
      "Iteration 69, loss = 0.65830996\n",
      "Iteration 1662, loss = 0.14570788\n",
      "Iteration 70, loss = 0.65654206\n",
      "Iteration 432, loss = 0.39861281\n",
      "Iteration 71, loss = 0.65472512\n",
      "Iteration 248, loss = 0.36826076\n",
      "Iteration 1371, loss = 0.25930187\n",
      "Iteration 72, loss = 0.65297314\n",
      "Iteration 2052, loss = 0.17836533\n",
      "Iteration 88, loss = 0.51770185\n",
      "Iteration 73, loss = 0.65109950\n",
      "Iteration 74, loss = 0.64924781\n",
      "Iteration 1663, loss = 0.14548036\n",
      "Iteration 75, loss = 0.64740178\n",
      "Iteration 2053, loss = 0.17824628\n",
      "Iteration 76, loss = 0.64543296\n",
      "Iteration 1664, loss = 0.14542795\n",
      "Iteration 249, loss = 0.36805596\n",
      "Iteration 1372, loss = 0.25902396\n",
      "Iteration 433, loss = 0.39833333\n",
      "Iteration 1665, loss = 0.14534844\n",
      "Iteration 250, loss = 0.36787397\n",
      "Iteration 89, loss = 0.51544712\n",
      "Iteration 251, loss = 0.36769517\n",
      "Iteration 77, loss = 0.64344517\n",
      "Iteration 1666, loss = 0.14520324\n",
      "Iteration 1385, loss = 0.20026170\n",
      "Iteration 78, loss = 0.64151067\n",
      "Iteration 252, loss = 0.36746623\n",
      "Iteration 79, loss = 0.63949218\n",
      "Iteration 1373, loss = 0.25889062\n",
      "Iteration 1667, loss = 0.14510953\n",
      "Iteration 434, loss = 0.39818972\n",
      "Iteration 80, loss = 0.63749553\n",
      "Iteration 90, loss = 0.51323639\n",
      "Iteration 253, loss = 0.36726415\n",
      "Iteration 81, loss = 0.63536728\n",
      "Iteration 1386, loss = 0.20031015\n",
      "Iteration 82, loss = 0.63326908\n",
      "Iteration 1374, loss = 0.25877091\n",
      "Iteration 2054, loss = 0.17811503\n",
      "Iteration 1668, loss = 0.14487237\n",
      "Iteration 91, loss = 0.51114529\n",
      "Iteration 83, loss = 0.63116686\n",
      "Iteration 84, loss = 0.62900168\n",
      "Iteration 435, loss = 0.39793588\n",
      "Iteration 2055, loss = 0.17798591\n",
      "Iteration 85, loss = 0.62679922\n",
      "Iteration 254, loss = 0.36705271\n",
      "Iteration 1669, loss = 0.14490394\n",
      "Iteration 86, loss = 0.62464290\n",
      "Iteration 1375, loss = 0.25866217\n",
      "Iteration 87, loss = 0.62236631\n",
      "Iteration 1670, loss = 0.14470831\n",
      "Iteration 255, loss = 0.36686562\n",
      "Iteration 1387, loss = 0.19985179\n",
      "Iteration 92, loss = 0.50904162\n",
      "Iteration 2056, loss = 0.17795255\n",
      "Iteration 436, loss = 0.39773188\n",
      "Iteration 88, loss = 0.62014510\n",
      "Iteration 256, loss = 0.36670511\n",
      "Iteration 1671, loss = 0.14489438\n",
      "Iteration 1376, loss = 0.25851760\n",
      "Iteration 257, loss = 0.36648035\n",
      "Iteration 93, loss = 0.50690522\n",
      "Iteration 89, loss = 0.61791147\n",
      "Iteration 1672, loss = 0.14475144\n",
      "Iteration 2057, loss = 0.17783827\n",
      "Iteration 90, loss = 0.61561873\n",
      "Iteration 437, loss = 0.39753331\n",
      "Iteration 91, loss = 0.61326596\n",
      "Iteration 1388, loss = 0.19989031\n",
      "Iteration 92, loss = 0.61096224\n",
      "Iteration 258, loss = 0.36636561\n",
      "Iteration 1377, loss = 0.25833674\n",
      "Iteration 93, loss = 0.60870481\n",
      "Iteration 259, loss = 0.36610209\n",
      "Iteration 94, loss = 0.60634134\n",
      "Iteration 260, loss = 0.36592348\n",
      "Iteration 1673, loss = 0.14434485\n",
      "Iteration 94, loss = 0.50484634\n",
      "Iteration 95, loss = 0.60393787\n",
      "Iteration 2058, loss = 0.17777534\n",
      "Iteration 261, loss = 0.36575310\n",
      "Iteration 96, loss = 0.60150200\n",
      "Iteration 97, loss = 0.59922262\n",
      "Iteration 95, loss = 0.50288787\n",
      "Iteration 438, loss = 0.39734484\n",
      "Iteration 2059, loss = 0.17769011\n",
      "Iteration 1389, loss = 0.19960436\n",
      "Iteration 1378, loss = 0.25832190\n",
      "Iteration 98, loss = 0.59663243\n",
      "Iteration 99, loss = 0.59424790\n",
      "Iteration 100, loss = 0.59183518\n",
      "Iteration 2060, loss = 0.17757102\n",
      "Iteration 101, loss = 0.58940758\n",
      "Iteration 96, loss = 0.50092891\n",
      "Iteration 1379, loss = 0.25809626\n",
      "Iteration 1390, loss = 0.19935193\n",
      "Iteration 1674, loss = 0.14413568\n",
      "Iteration 102, loss = 0.58692038\n",
      "Iteration 103, loss = 0.58441885\n",
      "Iteration 262, loss = 0.36557672\n",
      "Iteration 1380, loss = 0.25801766\n",
      "Iteration 97, loss = 0.49897650\n",
      "Iteration 104, loss = 0.58189819Iteration 263, loss = 0.36537107\n",
      "Iteration 1391, loss = 0.19929694\n",
      "Iteration 1675, loss = 0.14417127\n",
      "Iteration 2061, loss = 0.17747760\n",
      "\n",
      "Iteration 1381, loss = 0.25787694\n",
      "Iteration 1676, loss = 0.14384540\n",
      "Iteration 439, loss = 0.39716347\n",
      "Iteration 264, loss = 0.36518990\n",
      "Iteration 105, loss = 0.57942833\n",
      "Iteration 265, loss = 0.36501077\n",
      "Iteration 106, loss = 0.57694898\n",
      "Iteration 1677, loss = 0.14391406\n",
      "Iteration 1382, loss = 0.25775259\n",
      "Iteration 107, loss = 0.57438017\n",
      "Iteration 266, loss = 0.36484439\n",
      "Iteration 108, loss = 0.57189731\n",
      "Iteration 109, loss = 0.56939769\n",
      "Iteration 1678, loss = 0.14366529\n",
      "Iteration 267, loss = 0.36464262\n",
      "Iteration 110, loss = 0.56699346\n",
      "Iteration 98, loss = 0.49704852\n",
      "Iteration 111, loss = 0.56435062\n",
      "Iteration 1679, loss = 0.14363506\n",
      "Iteration 112, loss = 0.56196817\n",
      "Iteration 440, loss = 0.39696834\n",
      "Iteration 99, loss = 0.49528217\n",
      "Iteration 268, loss = 0.36446693\n",
      "Iteration 2062, loss = 0.17750069\n",
      "Iteration 1383, loss = 0.25757228\n",
      "Iteration 1392, loss = 0.19923385\n",
      "Iteration 113, loss = 0.55941180\n",
      "Iteration 1680, loss = 0.14350656\n",
      "Iteration 269, loss = 0.36430223\n",
      "Iteration 2063, loss = 0.17724327\n",
      "Iteration 114, loss = 0.55690500\n",
      "Iteration 115, loss = 0.55446476\n",
      "Iteration 270, loss = 0.36409275\n",
      "Iteration 116, loss = 0.55201617\n",
      "Iteration 441, loss = 0.39679982\n",
      "Iteration 117, loss = 0.54957403\n",
      "Iteration 271, loss = 0.36393335\n",
      "Iteration 100, loss = 0.49342572\n",
      "Iteration 2064, loss = 0.17721449\n",
      "Iteration 118, loss = 0.54715181\n",
      "Iteration 1393, loss = 0.19892333\n",
      "Iteration 272, loss = 0.36376941\n",
      "Iteration 119, loss = 0.54464746\n",
      "Iteration 120, loss = 0.54227463\n",
      "Iteration 273, loss = 0.36357125\n",
      "Iteration 121, loss = 0.53988803\n",
      "Iteration 1384, loss = 0.25747344\n",
      "Iteration 122, loss = 0.53753927\n",
      "Iteration 1681, loss = 0.14333056\n",
      "Iteration 442, loss = 0.39656368\n",
      "Iteration 123, loss = 0.53522248\n",
      "Iteration 274, loss = 0.36338722\n",
      "Iteration 101, loss = 0.49169442\n",
      "Iteration 2065, loss = 0.17702808\n",
      "Iteration 124, loss = 0.53279649\n",
      "Iteration 1682, loss = 0.14325001\n",
      "Iteration 1394, loss = 0.19869982\n",
      "Iteration 275, loss = 0.36321209\n",
      "Iteration 1385, loss = 0.25739163\n",
      "Iteration 125, loss = 0.53061952\n",
      "Iteration 1683, loss = 0.14330670\n",
      "Iteration 126, loss = 0.52828744\n",
      "Iteration 443, loss = 0.39636285\n",
      "Iteration 102, loss = 0.48995220\n",
      "Iteration 276, loss = 0.36306824\n",
      "Iteration 127, loss = 0.52597382\n",
      "Iteration 2066, loss = 0.17699872\n",
      "Iteration 1684, loss = 0.14309672\n",
      "Iteration 128, loss = 0.52375226\n",
      "Iteration 1386, loss = 0.25730438\n",
      "Iteration 1685, loss = 0.14286802\n",
      "Iteration 2067, loss = 0.17689600\n",
      "Iteration 129, loss = 0.52150272\n",
      "Iteration 1387, loss = 0.25713317\n",
      "Iteration 277, loss = 0.36292354\n",
      "Iteration 103, loss = 0.48821200\n",
      "Iteration 1395, loss = 0.19863311\n",
      "Iteration 1686, loss = 0.14285219\n",
      "Iteration 130, loss = 0.51933227\n",
      "Iteration 1687, loss = 0.14267823\n",
      "Iteration 278, loss = 0.36270934\n",
      "Iteration 2068, loss = 0.17675845\n",
      "Iteration 131, loss = 0.51715926\n",
      "Iteration 279, loss = 0.36253882\n",
      "Iteration 444, loss = 0.39618392\n",
      "Iteration 104, loss = 0.48656330\n",
      "Iteration 280, loss = 0.36236867\n",
      "Iteration 1688, loss = 0.14277809\n",
      "Iteration 1388, loss = 0.25697955Iteration 132, loss = 0.51499050\n",
      "\n",
      "Iteration 281, loss = 0.36221857\n",
      "Iteration 2069, loss = 0.17666572\n",
      "Iteration 1689, loss = 0.14265172\n",
      "Iteration 133, loss = 0.51284736\n",
      "Iteration 282, loss = 0.36208876\n",
      "Iteration 105, loss = 0.48500939\n",
      "Iteration 134, loss = 0.51074549\n",
      "Iteration 1396, loss = 0.19841470\n",
      "Iteration 1389, loss = 0.25683464\n",
      "Iteration 135, loss = 0.50873634\n",
      "Iteration 1690, loss = 0.14228337\n",
      "Iteration 283, loss = 0.36188156\n",
      "Iteration 2070, loss = 0.17658448\n",
      "Iteration 284, loss = 0.36172529\n",
      "Iteration 136, loss = 0.50677162\n",
      "Iteration 445, loss = 0.39608809\n",
      "Iteration 137, loss = 0.50472945\n",
      "Iteration 1691, loss = 0.14237666\n",
      "Iteration 285, loss = 0.36155910\n",
      "Iteration 2071, loss = 0.17654796\n",
      "Iteration 106, loss = 0.48342382\n",
      "Iteration 138, loss = 0.50278640\n",
      "Iteration 1397, loss = 0.19843252Iteration 139, loss = 0.50084238\n",
      "\n",
      "Iteration 1692, loss = 0.14230229\n",
      "Iteration 140, loss = 0.49897793\n",
      "Iteration 286, loss = 0.36143230\n",
      "Iteration 2072, loss = 0.17638221\n",
      "Iteration 1693, loss = 0.14205798\n",
      "Iteration 141, loss = 0.49702324\n",
      "Iteration 142, loss = 0.49520829\n",
      "Iteration 143, loss = 0.49337791\n",
      "Iteration 1390, loss = 0.25674472\n",
      "Iteration 144, loss = 0.49172198\n",
      "Iteration 1694, loss = 0.14178655\n",
      "Iteration 145, loss = 0.48996432\n",
      "Iteration 107, loss = 0.48178151\n",
      "Iteration 1398, loss = 0.19815781\n",
      "Iteration 2073, loss = 0.17627984\n",
      "Iteration 287, loss = 0.36124858\n",
      "Iteration 446, loss = 0.39581400\n",
      "Iteration 146, loss = 0.48814136\n",
      "Iteration 288, loss = 0.36112846\n",
      "Iteration 147, loss = 0.48649676\n",
      "Iteration 108, loss = 0.48034518\n",
      "Iteration 289, loss = 0.36091583\n",
      "Iteration 1391, loss = 0.25658191\n",
      "Iteration 2074, loss = 0.17619201\n",
      "Iteration 1399, loss = 0.19803639\n",
      "Iteration 290, loss = 0.36079583\n",
      "Iteration 148, loss = 0.48479097\n",
      "Iteration 447, loss = 0.39566910\n",
      "Iteration 291, loss = 0.36060044\n",
      "Iteration 109, loss = 0.47884721\n",
      "Iteration 1695, loss = 0.14175228\n",
      "Iteration 2075, loss = 0.17612845\n",
      "Iteration 448, loss = 0.39544732Iteration 1392, loss = 0.25646762\n",
      "\n",
      "Iteration 110, loss = 0.47743497\n",
      "Iteration 1696, loss = 0.14153076\n",
      "Iteration 149, loss = 0.48323629\n",
      "Iteration 1393, loss = 0.25631562\n",
      "Iteration 292, loss = 0.36046451\n",
      "Iteration 150, loss = 0.48161797\n",
      "Iteration 111, loss = 0.47596880\n",
      "Iteration 1697, loss = 0.14140790\n",
      "Iteration 151, loss = 0.48000806\n",
      "Iteration 1400, loss = 0.19787373\n",
      "Iteration 152, loss = 0.47844826\n",
      "Iteration 2076, loss = 0.17601836\n",
      "Iteration 1394, loss = 0.25621940\n",
      "Iteration 293, loss = 0.36032428\n",
      "Iteration 153, loss = 0.47692653\n",
      "Iteration 1698, loss = 0.14139435\n",
      "Iteration 294, loss = 0.36015870\n",
      "Iteration 1395, loss = 0.25608666\n",
      "Iteration 154, loss = 0.47538589\n",
      "Iteration 1699, loss = 0.14152267\n",
      "Iteration 155, loss = 0.47396142\n",
      "Iteration 1401, loss = 0.19768947\n",
      "Iteration 295, loss = 0.36000285\n",
      "Iteration 156, loss = 0.47249881\n",
      "Iteration 449, loss = 0.39519081\n",
      "Iteration 2077, loss = 0.17596471\n",
      "Iteration 112, loss = 0.47467512\n",
      "Iteration 157, loss = 0.47113714\n",
      "Iteration 158, loss = 0.46964260\n",
      "Iteration 1396, loss = 0.25595565\n",
      "Iteration 1700, loss = 0.14105867\n",
      "Iteration 296, loss = 0.35983517\n",
      "Iteration 159, loss = 0.46834364\n",
      "Iteration 160, loss = 0.46697700\n",
      "Iteration 1397, loss = 0.25584929\n",
      "Iteration 297, loss = 0.35970848Iteration 161, loss = 0.46568719\n",
      "\n",
      "Iteration 162, loss = 0.46430673\n",
      "Iteration 113, loss = 0.47328144\n",
      "Iteration 298, loss = 0.35957193\n",
      "Iteration 1398, loss = 0.25570949\n",
      "Iteration 450, loss = 0.39502721\n",
      "Iteration 163, loss = 0.46311008\n",
      "Iteration 299, loss = 0.35938212\n",
      "Iteration 1701, loss = 0.14121296\n",
      "Iteration 2078, loss = 0.17588048\n",
      "Iteration 164, loss = 0.46186035\n",
      "Iteration 165, loss = 0.46063815Iteration 300, loss = 0.35921451\n",
      "Iteration 2079, loss = 0.17580012\n",
      "Iteration 114, loss = 0.47196370\n",
      "Iteration 1399, loss = 0.25560855\n",
      "Iteration 1402, loss = 0.19752254\n",
      "Iteration 1702, loss = 0.14117097\n",
      "Iteration 451, loss = 0.39483906\n",
      "\n",
      "Iteration 1400, loss = 0.25542871\n",
      "Iteration 301, loss = 0.35906640\n",
      "Iteration 115, loss = 0.47067411\n",
      "Iteration 1703, loss = 0.14083960\n",
      "Iteration 2080, loss = 0.17566139\n",
      "Iteration 302, loss = 0.35891044\n",
      "Iteration 1704, loss = 0.14062707\n",
      "Iteration 303, loss = 0.35883316\n",
      "Iteration 116, loss = 0.46940468\n",
      "Iteration 1403, loss = 0.19745520\n",
      "Iteration 2081, loss = 0.17550894\n",
      "Iteration 1401, loss = 0.25532781\n",
      "Iteration 304, loss = 0.35862627\n",
      "Iteration 452, loss = 0.39464269\n",
      "Iteration 1705, loss = 0.14054591\n",
      "Iteration 305, loss = 0.35846364\n",
      "Iteration 306, loss = 0.35830602\n",
      "Iteration 1402, loss = 0.25518264\n",
      "Iteration 117, loss = 0.46815836\n",
      "Iteration 1404, loss = 0.19719249\n",
      "Iteration 307, loss = 0.35819231\n",
      "Iteration 308, loss = 0.35802686\n",
      "Iteration 2082, loss = 0.17544152\n",
      "Iteration 1706, loss = 0.14043158\n",
      "Iteration 118, loss = 0.46691940\n",
      "Iteration 1403, loss = 0.25507078\n",
      "Iteration 309, loss = 0.35787689\n",
      "Iteration 453, loss = 0.39440897\n",
      "Iteration 2083, loss = 0.17531814\n",
      "Iteration 1405, loss = 0.19704163\n",
      "Iteration 310, loss = 0.35773806\n",
      "Iteration 1707, loss = 0.14038252\n",
      "Iteration 1404, loss = 0.25506369\n",
      "Iteration 311, loss = 0.35759317\n",
      "Iteration 119, loss = 0.46573610\n",
      "Iteration 2084, loss = 0.17524958\n",
      "Iteration 1406, loss = 0.19694634\n",
      "Iteration 312, loss = 0.35742180\n",
      "Iteration 454, loss = 0.39432795\n",
      "Iteration 1708, loss = 0.14028468\n",
      "Iteration 313, loss = 0.35728386\n",
      "Iteration 120, loss = 0.46464283\n",
      "Iteration 2085, loss = 0.17516931\n",
      "Iteration 314, loss = 0.35712644\n",
      "Iteration 1407, loss = 0.19680889\n",
      "Iteration 1405, loss = 0.25497718\n",
      "Iteration 1709, loss = 0.14016547\n",
      "Iteration 315, loss = 0.35697996\n",
      "Iteration 455, loss = 0.39403587\n",
      "Iteration 121, loss = 0.46347736\n",
      "Iteration 316, loss = 0.35682763\n",
      "Iteration 1710, loss = 0.14009061\n",
      "Iteration 2086, loss = 0.17507960\n",
      "Iteration 317, loss = 0.35667789\n",
      "Iteration 122, loss = 0.46233392\n",
      "Iteration 1711, loss = 0.13984386\n",
      "Iteration 166, loss = 0.45935700\n",
      "Iteration 318, loss = 0.35653812\n",
      "Iteration 2087, loss = 0.17494917\n",
      "Iteration 1712, loss = 0.13984353\n",
      "Iteration 456, loss = 0.39386423\n",
      "Iteration 1406, loss = 0.25468530\n",
      "Iteration 1408, loss = 0.19695534\n",
      "Iteration 1713, loss = 0.13969796\n",
      "Iteration 319, loss = 0.35640393\n",
      "Iteration 123, loss = 0.46128488\n",
      "Iteration 2088, loss = 0.17488021\n",
      "Iteration 1714, loss = 0.13977091\n",
      "Iteration 320, loss = 0.35629434\n",
      "Iteration 1715, loss = 0.13938396\n",
      "Iteration 1409, loss = 0.19641844\n",
      "Iteration 124, loss = 0.46017988\n",
      "Iteration 1407, loss = 0.25461854\n",
      "Iteration 457, loss = 0.39364697\n",
      "Iteration 321, loss = 0.35611886\n",
      "Iteration 1716, loss = 0.13926689\n",
      "Iteration 2089, loss = 0.17473873\n",
      "Iteration 322, loss = 0.35596304\n",
      "Iteration 2090, loss = 0.17465995\n",
      "Iteration 458, loss = 0.39346966\n",
      "Iteration 1717, loss = 0.13920974\n",
      "Iteration 125, loss = 0.45915789\n",
      "Iteration 323, loss = 0.35582238\n",
      "Iteration 1410, loss = 0.19636345\n",
      "Iteration 1408, loss = 0.25444604\n",
      "Iteration 1718, loss = 0.13909781\n",
      "Iteration 167, loss = 0.45825517\n",
      "Iteration 324, loss = 0.35568914\n",
      "Iteration 126, loss = 0.45810359\n",
      "Iteration 1411, loss = 0.19615372\n",
      "Iteration 325, loss = 0.35552248\n",
      "Iteration 2091, loss = 0.17458122\n",
      "Iteration 326, loss = 0.35542462\n",
      "Iteration 127, loss = 0.45709536\n",
      "Iteration 1412, loss = 0.19624394\n",
      "Iteration 327, loss = 0.35524970\n",
      "Iteration 459, loss = 0.39337419\n",
      "Iteration 168, loss = 0.45709757\n",
      "Iteration 1719, loss = 0.13896669\n",
      "Iteration 328, loss = 0.35510886\n",
      "Iteration 128, loss = 0.45614453\n",
      "Iteration 169, loss = 0.45587985\n",
      "Iteration 1720, loss = 0.13888585\n",
      "Iteration 2092, loss = 0.17447768\n",
      "Iteration 170, loss = 0.45480185\n",
      "Iteration 1409, loss = 0.25433041\n",
      "Iteration 171, loss = 0.45366468\n",
      "Iteration 460, loss = 0.39310363Iteration 329, loss = 0.35496127\n",
      "\n",
      "Iteration 172, loss = 0.45264187\n",
      "Iteration 173, loss = 0.45157220\n",
      "Iteration 174, loss = 0.45047240\n",
      "Iteration 2093, loss = 0.17455582\n",
      "Iteration 1410, loss = 0.25427710\n",
      "Iteration 330, loss = 0.35486996\n",
      "Iteration 1721, loss = 0.13872028\n",
      "Iteration 129, loss = 0.45514905\n",
      "Iteration 1413, loss = 0.19617473\n",
      "Iteration 175, loss = 0.44953561\n",
      "Iteration 2094, loss = 0.17433906\n",
      "Iteration 176, loss = 0.44849754\n",
      "Iteration 1722, loss = 0.13868686\n",
      "Iteration 461, loss = 0.39297872\n",
      "Iteration 177, loss = 0.44744720\n",
      "Iteration 178, loss = 0.44651027\n",
      "Iteration 1411, loss = 0.25408159\n",
      "Iteration 179, loss = 0.44555360\n",
      "Iteration 331, loss = 0.35473044\n",
      "Iteration 180, loss = 0.44461219\n",
      "Iteration 1414, loss = 0.19589281\n",
      "Iteration 181, loss = 0.44367275\n",
      "Iteration 1723, loss = 0.13855455\n",
      "Iteration 130, loss = 0.45421700\n",
      "Iteration 2095, loss = 0.17452760\n",
      "Iteration 1412, loss = 0.25394909\n",
      "Iteration 182, loss = 0.44272529\n",
      "Iteration 332, loss = 0.35454777\n",
      "Iteration 183, loss = 0.44188925\n",
      "Iteration 462, loss = 0.39266927\n",
      "Iteration 184, loss = 0.44093580\n",
      "Iteration 1415, loss = 0.19558790\n",
      "Iteration 333, loss = 0.35442900\n",
      "Iteration 185, loss = 0.44004437\n",
      "Iteration 2096, loss = 0.17413460\n",
      "Iteration 186, loss = 0.43923065\n",
      "Iteration 131, loss = 0.45324522\n",
      "Iteration 1724, loss = 0.13841894\n",
      "Iteration 187, loss = 0.43836676\n",
      "Iteration 334, loss = 0.35428702\n",
      "Iteration 1413, loss = 0.25386956\n",
      "Iteration 188, loss = 0.43748624\n",
      "Iteration 463, loss = 0.39248726Iteration 335, loss = 0.35414179Iteration 189, loss = 0.43662221\n",
      "\n",
      "Iteration 2097, loss = 0.17409548\n",
      "Iteration 1416, loss = 0.19538237\n",
      "\n",
      "Iteration 190, loss = 0.43584245\n",
      "Iteration 336, loss = 0.35399793\n",
      "Iteration 132, loss = 0.45234834\n",
      "Iteration 1725, loss = 0.13850859\n",
      "Iteration 1414, loss = 0.25385442\n",
      "Iteration 191, loss = 0.43502648\n",
      "Iteration 192, loss = 0.43423420\n",
      "Iteration 337, loss = 0.35385903\n",
      "Iteration 2098, loss = 0.17392051\n",
      "Iteration 1726, loss = 0.13834533\n",
      "Iteration 338, loss = 0.35377987\n",
      "Iteration 193, loss = 0.43343178\n",
      "Iteration 339, loss = 0.35359116\n",
      "Iteration 133, loss = 0.45147087\n",
      "Iteration 1415, loss = 0.25359022\n",
      "Iteration 340, loss = 0.35344787\n",
      "Iteration 2099, loss = 0.17389200\n",
      "Iteration 194, loss = 0.43262442\n",
      "Iteration 341, loss = 0.35330668\n",
      "Iteration 464, loss = 0.39231013\n",
      "Iteration 1727, loss = 0.13820563\n",
      "Iteration 134, loss = 0.45058205\n",
      "Iteration 195, loss = 0.43186146\n",
      "Iteration 342, loss = 0.35320951\n",
      "Iteration 1417, loss = 0.19524196\n",
      "Iteration 196, loss = 0.43107295\n",
      "Iteration 343, loss = 0.35303781\n",
      "Iteration 2100, loss = 0.17386712\n",
      "Iteration 197, loss = 0.43034473\n",
      "Iteration 465, loss = 0.39211351\n",
      "Iteration 344, loss = 0.35291545\n",
      "Iteration 198, loss = 0.42954479\n",
      "Iteration 345, loss = 0.35279373\n",
      "Iteration 199, loss = 0.42879831\n",
      "Iteration 1416, loss = 0.25344530\n",
      "Iteration 1728, loss = 0.13795256\n",
      "Iteration 135, loss = 0.44973561\n",
      "Iteration 2101, loss = 0.17375475\n",
      "Iteration 466, loss = 0.39197558\n",
      "Iteration 200, loss = 0.42810163\n",
      "Iteration 346, loss = 0.35264384\n",
      "Iteration 1418, loss = 0.19525754\n",
      "Iteration 1417, loss = 0.25329605\n",
      "Iteration 201, loss = 0.42737676\n",
      "Iteration 136, loss = 0.44892995\n",
      "Iteration 202, loss = 0.42664484\n",
      "Iteration 347, loss = 0.35249967\n",
      "Iteration 203, loss = 0.42594321\n",
      "Iteration 1729, loss = 0.13796280\n",
      "Iteration 2102, loss = 0.17353593\n",
      "Iteration 1418, loss = 0.25315336\n",
      "Iteration 204, loss = 0.42535274\n",
      "Iteration 348, loss = 0.35239261\n",
      "Iteration 137, loss = 0.44805935\n",
      "Iteration 1419, loss = 0.19492183\n",
      "Iteration 349, loss = 0.35225290\n",
      "Iteration 467, loss = 0.39172858\n",
      "Iteration 350, loss = 0.35211512\n",
      "Iteration 205, loss = 0.42453353\n",
      "Iteration 1730, loss = 0.13782669\n",
      "Iteration 2103, loss = 0.17345961\n",
      "Iteration 1419, loss = 0.25305765\n",
      "Iteration 351, loss = 0.35201415\n",
      "Iteration 352, loss = 0.35184402\n",
      "Iteration 206, loss = 0.42383368\n",
      "Iteration 1420, loss = 0.25296506\n",
      "Iteration 353, loss = 0.35171886\n",
      "Iteration 2104, loss = 0.17341601\n",
      "Iteration 1731, loss = 0.13771147\n",
      "Iteration 1420, loss = 0.19488000\n",
      "Iteration 468, loss = 0.39151811\n",
      "Iteration 1421, loss = 0.25278827\n",
      "Iteration 207, loss = 0.42315191\n",
      "Iteration 1732, loss = 0.13757030\n",
      "Iteration 2105, loss = 0.17328624\n",
      "Iteration 138, loss = 0.44727186\n",
      "Iteration 354, loss = 0.35160446\n",
      "Iteration 208, loss = 0.42251548\n",
      "Iteration 1421, loss = 0.19476067\n",
      "Iteration 209, loss = 0.42183477\n",
      "Iteration 210, loss = 0.42119656\n",
      "Iteration 2106, loss = 0.17317850\n",
      "Iteration 211, loss = 0.42052727\n",
      "Iteration 469, loss = 0.39134563\n",
      "Iteration 355, loss = 0.35147279\n",
      "Iteration 212, loss = 0.41996799\n",
      "Iteration 1422, loss = 0.25268346\n",
      "Iteration 1733, loss = 0.13749945\n",
      "Iteration 139, loss = 0.44647230\n",
      "Iteration 356, loss = 0.35140945\n",
      "Iteration 213, loss = 0.41926697\n",
      "Iteration 357, loss = 0.35121983\n",
      "Iteration 1423, loss = 0.25255628\n",
      "Iteration 1734, loss = 0.13741556\n",
      "Iteration 1422, loss = 0.19461029\n",
      "Iteration 214, loss = 0.41863096\n",
      "Iteration 2107, loss = 0.17311848\n",
      "Iteration 358, loss = 0.35108953\n",
      "Iteration 1735, loss = 0.13744877\n",
      "Iteration 1424, loss = 0.25239470\n",
      "Iteration 215, loss = 0.41801726\n",
      "Iteration 140, loss = 0.44570076\n",
      "Iteration 216, loss = 0.41739850\n",
      "Iteration 2108, loss = 0.17302925\n",
      "Iteration 470, loss = 0.39115361\n",
      "Iteration 1736, loss = 0.13707866\n",
      "Iteration 217, loss = 0.41683516\n",
      "Iteration 359, loss = 0.35094727\n",
      "Iteration 141, loss = 0.44490286\n",
      "Iteration 1737, loss = 0.13707822\n",
      "Iteration 218, loss = 0.41615380\n",
      "Iteration 219, loss = 0.41558196\n",
      "Iteration 360, loss = 0.35083079\n",
      "Iteration 1423, loss = 0.19440110\n",
      "Iteration 1738, loss = 0.13691113\n",
      "Iteration 361, loss = 0.35071378\n",
      "Iteration 142, loss = 0.44414392\n",
      "Iteration 2109, loss = 0.17296946\n",
      "Iteration 362, loss = 0.35057593\n",
      "Iteration 1425, loss = 0.25232107\n",
      "Iteration 1739, loss = 0.13700224\n",
      "Iteration 220, loss = 0.41500840\n",
      "Iteration 471, loss = 0.39098322\n",
      "Iteration 221, loss = 0.41440218\n",
      "Iteration 1740, loss = 0.13674975\n",
      "Iteration 363, loss = 0.35049039\n",
      "Iteration 1424, loss = 0.19425550\n",
      "Iteration 143, loss = 0.44341703\n",
      "Iteration 222, loss = 0.41377785\n",
      "Iteration 2110, loss = 0.17301198\n",
      "Iteration 223, loss = 0.41324226\n",
      "Iteration 1426, loss = 0.25214821\n",
      "Iteration 224, loss = 0.41263283\n",
      "Iteration 364, loss = 0.35039090\n",
      "Iteration 1741, loss = 0.13659564\n",
      "Iteration 225, loss = 0.41209990\n",
      "Iteration 226, loss = 0.41151176\n",
      "Iteration 472, loss = 0.39076125\n",
      "Iteration 227, loss = 0.41099661\n",
      "Iteration 2111, loss = 0.17275975\n",
      "Iteration 365, loss = 0.35020214\n",
      "Iteration 144, loss = 0.44273583\n",
      "Iteration 1742, loss = 0.13650222\n",
      "Iteration 1427, loss = 0.25201941\n",
      "Iteration 228, loss = 0.41042094\n",
      "Iteration 366, loss = 0.35011172\n",
      "Iteration 2112, loss = 0.17270473\n",
      "Iteration 1425, loss = 0.19412212\n",
      "Iteration 145, loss = 0.44193028\n",
      "Iteration 367, loss = 0.34995517\n",
      "Iteration 229, loss = 0.40984140\n",
      "Iteration 1743, loss = 0.13633690\n",
      "Iteration 230, loss = 0.40930898\n",
      "Iteration 368, loss = 0.34983730\n",
      "Iteration 231, loss = 0.40873997\n",
      "Iteration 473, loss = 0.39056599\n",
      "Iteration 369, loss = 0.34971092\n",
      "Iteration 1744, loss = 0.13631772\n",
      "Iteration 232, loss = 0.40821430\n",
      "Iteration 370, loss = 0.34960261\n",
      "Iteration 1428, loss = 0.25186923\n",
      "Iteration 146, loss = 0.44129297\n",
      "Iteration 233, loss = 0.40766867\n",
      "Iteration 2113, loss = 0.17274694\n",
      "Iteration 234, loss = 0.40716698\n",
      "Iteration 1745, loss = 0.13621137\n",
      "Iteration 1426, loss = 0.19397864\n",
      "Iteration 147, loss = 0.44056375\n",
      "Iteration 235, loss = 0.40663831\n",
      "Iteration 1746, loss = 0.13625473\n",
      "Iteration 371, loss = 0.34946336\n",
      "Iteration 236, loss = 0.40613513\n",
      "Iteration 474, loss = 0.39039114\n",
      "Iteration 372, loss = 0.34934581\n",
      "Iteration 1747, loss = 0.13588150\n",
      "Iteration 237, loss = 0.40560239\n",
      "Iteration 2114, loss = 0.17243201\n",
      "Iteration 373, loss = 0.34923008\n",
      "Iteration 1429, loss = 0.25176008\n",
      "Iteration 148, loss = 0.43983985\n",
      "Iteration 475, loss = 0.39017769\n",
      "Iteration 1748, loss = 0.13591674\n",
      "Iteration 374, loss = 0.34910539\n",
      "Iteration 149, loss = 0.43919071\n",
      "Iteration 1430, loss = 0.25167129\n",
      "Iteration 238, loss = 0.40516267\n",
      "Iteration 1427, loss = 0.19380696\n",
      "Iteration 2115, loss = 0.17243203\n",
      "Iteration 239, loss = 0.40460112\n",
      "Iteration 1749, loss = 0.13569145\n",
      "Iteration 2116, loss = 0.17246046\n",
      "Iteration 476, loss = 0.38999099Iteration 240, loss = 0.40412772\n",
      "\n",
      "Iteration 375, loss = 0.34898690\n",
      "Iteration 1431, loss = 0.25152080\n",
      "Iteration 150, loss = 0.43853201\n",
      "Iteration 241, loss = 0.40357580\n",
      "Iteration 376, loss = 0.34886279\n",
      "Iteration 242, loss = 0.40311357\n",
      "Iteration 1750, loss = 0.13560917\n",
      "Iteration 1428, loss = 0.19364092\n",
      "Iteration 243, loss = 0.40259856\n",
      "Iteration 377, loss = 0.34887595\n",
      "Iteration 2117, loss = 0.17216657\n",
      "Iteration 151, loss = 0.43789297\n",
      "Iteration 244, loss = 0.40214149\n",
      "Iteration 378, loss = 0.34861739\n",
      "Iteration 1751, loss = 0.13546010\n",
      "Iteration 245, loss = 0.40164464\n",
      "Iteration 1429, loss = 0.19353743\n",
      "Iteration 1432, loss = 0.25138430\n",
      "Iteration 379, loss = 0.34849228\n",
      "Iteration 1752, loss = 0.13536678\n",
      "Iteration 246, loss = 0.40116996\n",
      "Iteration 2118, loss = 0.17215552\n",
      "Iteration 380, loss = 0.34837712\n",
      "Iteration 477, loss = 0.38982326\n",
      "Iteration 247, loss = 0.40073616\n",
      "Iteration 1433, loss = 0.25128701\n",
      "Iteration 248, loss = 0.40027696\n",
      "Iteration 2119, loss = 0.17203844\n",
      "Iteration 381, loss = 0.34825726\n",
      "Iteration 249, loss = 0.39981126\n",
      "Iteration 152, loss = 0.43720391\n",
      "Iteration 1434, loss = 0.25113806\n",
      "Iteration 1753, loss = 0.13551811\n",
      "Iteration 1430, loss = 0.19341791\n",
      "Iteration 250, loss = 0.39934473\n",
      "Iteration 382, loss = 0.34813552\n",
      "Iteration 153, loss = 0.43661931\n",
      "Iteration 251, loss = 0.39889579\n",
      "Iteration 1754, loss = 0.13546428\n",
      "Iteration 252, loss = 0.39842004\n",
      "Iteration 253, loss = 0.39797800\n",
      "Iteration 2120, loss = 0.17194597\n",
      "Iteration 254, loss = 0.39752694\n",
      "Iteration 1755, loss = 0.13510280\n",
      "Iteration 154, loss = 0.43598066\n",
      "Iteration 478, loss = 0.38962163\n",
      "Iteration 255, loss = 0.39705180\n",
      "Iteration 383, loss = 0.34803359\n",
      "Iteration 1435, loss = 0.25107154\n",
      "Iteration 2121, loss = 0.17187897\n",
      "Iteration 1756, loss = 0.13495635\n",
      "Iteration 256, loss = 0.39660462\n",
      "Iteration 257, loss = 0.39618231\n",
      "Iteration 155, loss = 0.43536349\n",
      "Iteration 384, loss = 0.34796575\n",
      "Iteration 2122, loss = 0.17176794\n",
      "Iteration 1436, loss = 0.25092453\n",
      "Iteration 258, loss = 0.39572731\n",
      "Iteration 385, loss = 0.34778414\n",
      "Iteration 1757, loss = 0.13513210\n",
      "Iteration 259, loss = 0.39529838\n",
      "Iteration 479, loss = 0.38939339\n",
      "Iteration 2123, loss = 0.17159116\n",
      "Iteration 156, loss = 0.43473999\n",
      "Iteration 1437, loss = 0.25083134\n",
      "Iteration 260, loss = 0.39484901\n",
      "Iteration 386, loss = 0.34768412\n",
      "Iteration 261, loss = 0.39443485\n",
      "Iteration 387, loss = 0.34757294\n",
      "Iteration 262, loss = 0.39402231\n",
      "Iteration 1758, loss = 0.13482807\n",
      "Iteration 263, loss = 0.39355313\n",
      "Iteration 1438, loss = 0.25064198\n",
      "Iteration 388, loss = 0.34744226\n",
      "Iteration 157, loss = 0.43414205\n",
      "Iteration 2124, loss = 0.17158064\n",
      "Iteration 264, loss = 0.39318488\n",
      "Iteration 480, loss = 0.38921330\n",
      "Iteration 265, loss = 0.39275804\n",
      "Iteration 1759, loss = 0.13474311\n",
      "Iteration 389, loss = 0.34731688\n",
      "Iteration 1439, loss = 0.25050320\n",
      "Iteration 266, loss = 0.39234495\n",
      "Iteration 158, loss = 0.43356633\n",
      "Iteration 2125, loss = 0.17146115\n",
      "Iteration 267, loss = 0.39194154\n",
      "Iteration 390, loss = 0.34730390\n",
      "Iteration 1440, loss = 0.25041971\n",
      "Iteration 1760, loss = 0.13463771\n",
      "Iteration 159, loss = 0.43301458\n",
      "Iteration 268, loss = 0.39157386\n",
      "Iteration 481, loss = 0.38903529\n",
      "Iteration 391, loss = 0.34708552\n",
      "Iteration 2126, loss = 0.17138298\n",
      "Iteration 269, loss = 0.39115157\n",
      "Iteration 1441, loss = 0.25030042\n",
      "Iteration 1761, loss = 0.13476517\n",
      "Iteration 392, loss = 0.34698568\n",
      "Iteration 160, loss = 0.43238878\n",
      "Iteration 270, loss = 0.39074213\n",
      "Iteration 271, loss = 0.39034490\n",
      "Iteration 1762, loss = 0.13432389\n",
      "Iteration 272, loss = 0.38993634\n",
      "Iteration 393, loss = 0.34685752\n",
      "Iteration 273, loss = 0.38957955\n",
      "Iteration 1442, loss = 0.25013861\n",
      "Iteration 394, loss = 0.34678459\n",
      "Iteration 161, loss = 0.43181630\n",
      "Iteration 2127, loss = 0.17129493\n",
      "Iteration 1763, loss = 0.13435229\n",
      "Iteration 482, loss = 0.38900610\n",
      "Iteration 274, loss = 0.38916706\n",
      "Iteration 275, loss = 0.38877939\n",
      "Iteration 276, loss = 0.38840176\n",
      "Iteration 2128, loss = 0.17118382\n",
      "Iteration 1764, loss = 0.13412854\n",
      "Iteration 395, loss = 0.34663839\n",
      "Iteration 1431, loss = 0.19321900\n",
      "Iteration 277, loss = 0.38800249\n",
      "Iteration 162, loss = 0.43124902\n",
      "Iteration 1443, loss = 0.25018272\n",
      "Iteration 278, loss = 0.38765319\n",
      "Iteration 396, loss = 0.34652889\n",
      "Iteration 483, loss = 0.38867865\n",
      "Iteration 279, loss = 0.38726398\n",
      "Iteration 397, loss = 0.34643668Iteration 280, loss = 0.38686826\n",
      "\n",
      "Iteration 281, loss = 0.38649719\n",
      "Iteration 2129, loss = 0.17113705\n",
      "Iteration 163, loss = 0.43070522\n",
      "Iteration 282, loss = 0.38614997\n",
      "Iteration 283, loss = 0.38575843\n",
      "Iteration 1765, loss = 0.13413331\n",
      "Iteration 284, loss = 0.38540745\n",
      "Iteration 164, loss = 0.43012609\n",
      "Iteration 484, loss = 0.38845010\n",
      "Iteration 1432, loss = 0.19317532\n",
      "Iteration 285, loss = 0.38503475\n",
      "Iteration 286, loss = 0.38468763\n",
      "Iteration 287, loss = 0.38432710\n",
      "Iteration 1766, loss = 0.13399194\n",
      "Iteration 1444, loss = 0.24995384\n",
      "Iteration 398, loss = 0.34630461\n",
      "Iteration 165, loss = 0.42960560\n",
      "Iteration 288, loss = 0.38394108\n",
      "Iteration 2130, loss = 0.17098569\n",
      "Iteration 289, loss = 0.38359915\n",
      "Iteration 399, loss = 0.34617361\n",
      "Iteration 290, loss = 0.38327565\n",
      "Iteration 291, loss = 0.38290791\n",
      "Iteration 400, loss = 0.34610484\n",
      "Iteration 1445, loss = 0.24973252\n",
      "Iteration 1767, loss = 0.13388554\n",
      "Iteration 292, loss = 0.38254034\n",
      "Iteration 401, loss = 0.34601851\n",
      "Iteration 293, loss = 0.38218377\n",
      "Iteration 1433, loss = 0.19292161\n",
      "Iteration 402, loss = 0.34587891\n",
      "Iteration 485, loss = 0.38823297\n",
      "Iteration 2131, loss = 0.17086891\n",
      "Iteration 1768, loss = 0.13375246\n",
      "Iteration 166, loss = 0.42905144\n",
      "Iteration 403, loss = 0.34577647\n",
      "Iteration 404, loss = 0.34562565\n",
      "Iteration 1446, loss = 0.24964614\n",
      "Iteration 294, loss = 0.38184650\n",
      "Iteration 1434, loss = 0.19293299\n",
      "Iteration 405, loss = 0.34550455\n",
      "Iteration 295, loss = 0.38150972\n",
      "Iteration 1769, loss = 0.13369092\n",
      "Iteration 2132, loss = 0.17079994\n",
      "Iteration 296, loss = 0.38115318\n",
      "Iteration 167, loss = 0.42852810\n",
      "Iteration 1447, loss = 0.24948588\n",
      "Iteration 406, loss = 0.34539209\n",
      "Iteration 486, loss = 0.38812591\n",
      "Iteration 1770, loss = 0.13349976\n",
      "Iteration 407, loss = 0.34529014\n",
      "Iteration 1435, loss = 0.19271237\n",
      "Iteration 2133, loss = 0.17070594\n",
      "Iteration 408, loss = 0.34517837\n",
      "Iteration 168, loss = 0.42803325\n",
      "Iteration 1771, loss = 0.13342685\n",
      "Iteration 297, loss = 0.38081786\n",
      "Iteration 298, loss = 0.38050230\n",
      "Iteration 169, loss = 0.42745877\n",
      "Iteration 1448, loss = 0.24940015\n",
      "Iteration 1772, loss = 0.13325994\n",
      "Iteration 409, loss = 0.34507460\n",
      "Iteration 299, loss = 0.38018275\n",
      "Iteration 487, loss = 0.38789572\n",
      "Iteration 300, loss = 0.37981336\n",
      "Iteration 301, loss = 0.37949827\n",
      "Iteration 302, loss = 0.37917339\n",
      "Iteration 1773, loss = 0.13315846\n",
      "Iteration 2134, loss = 0.17062727\n",
      "Iteration 410, loss = 0.34494964\n",
      "Iteration 1436, loss = 0.19260092\n",
      "Iteration 170, loss = 0.42696892\n",
      "Iteration 303, loss = 0.37882822\n",
      "Iteration 411, loss = 0.34485387\n",
      "Iteration 1449, loss = 0.24932161\n",
      "Iteration 488, loss = 0.38768226\n",
      "Iteration 171, loss = 0.42650238\n",
      "Iteration 412, loss = 0.34472370\n",
      "Iteration 304, loss = 0.37848413\n",
      "Iteration 1774, loss = 0.13305134\n",
      "Iteration 305, loss = 0.37819834\n",
      "Iteration 413, loss = 0.34464154\n",
      "Iteration 1450, loss = 0.24917488\n",
      "Iteration 306, loss = 0.37784426\n",
      "Iteration 2135, loss = 0.17049530\n",
      "Iteration 414, loss = 0.34450547\n",
      "Iteration 307, loss = 0.37754995\n",
      "Iteration 308, loss = 0.37720805\n",
      "Iteration 415, loss = 0.34438790\n",
      "Iteration 1775, loss = 0.13311238\n",
      "Iteration 489, loss = 0.38751873\n",
      "Iteration 309, loss = 0.37691273\n",
      "Iteration 172, loss = 0.42600700\n",
      "Iteration 416, loss = 0.34428863\n",
      "Iteration 1437, loss = 0.19233702\n",
      "Iteration 417, loss = 0.34415896\n",
      "Iteration 1776, loss = 0.13302378\n",
      "Iteration 1451, loss = 0.24905407\n",
      "Iteration 310, loss = 0.37659163\n",
      "Iteration 311, loss = 0.37626386\n",
      "Iteration 312, loss = 0.37596212\n",
      "Iteration 2136, loss = 0.17057570\n",
      "Iteration 490, loss = 0.38732972\n",
      "Iteration 1777, loss = 0.13286136\n",
      "Iteration 313, loss = 0.37564733\n",
      "Iteration 314, loss = 0.37533684\n",
      "Iteration 315, loss = 0.37503925\n",
      "Iteration 1778, loss = 0.13266875\n",
      "Iteration 418, loss = 0.34407238\n",
      "Iteration 1438, loss = 0.19239087\n",
      "Iteration 2137, loss = 0.17039842\n",
      "Iteration 1452, loss = 0.24888647\n",
      "Iteration 173, loss = 0.42549933\n",
      "Iteration 419, loss = 0.34395647\n",
      "Iteration 316, loss = 0.37478942\n",
      "Iteration 1779, loss = 0.13263214\n",
      "Iteration 317, loss = 0.37443819\n",
      "Iteration 420, loss = 0.34383023\n",
      "Iteration 318, loss = 0.37413629\n",
      "Iteration 319, loss = 0.37380231\n",
      "Iteration 1453, loss = 0.24880624\n",
      "Iteration 421, loss = 0.34373886\n",
      "Iteration 1780, loss = 0.13250043\n",
      "Iteration 320, loss = 0.37351561\n",
      "Iteration 321, loss = 0.37324160\n",
      "Iteration 422, loss = 0.34361585\n",
      "Iteration 322, loss = 0.37293932\n",
      "Iteration 1781, loss = 0.13252937\n",
      "Iteration 1439, loss = 0.19220317\n",
      "Iteration 2138, loss = 0.17027501\n",
      "Iteration 174, loss = 0.42501476\n",
      "Iteration 1454, loss = 0.24861540\n",
      "Iteration 323, loss = 0.37261563\n",
      "Iteration 491, loss = 0.38719473\n",
      "Iteration 423, loss = 0.34352921\n",
      "Iteration 324, loss = 0.37234627\n",
      "Iteration 424, loss = 0.34340432\n",
      "Iteration 2139, loss = 0.17018525\n",
      "Iteration 325, loss = 0.37206678\n",
      "Iteration 175, loss = 0.42453256\n",
      "Iteration 425, loss = 0.34329929\n",
      "Iteration 326, loss = 0.37186009\n",
      "Iteration 1440, loss = 0.19199108\n",
      "Iteration 1455, loss = 0.24851910\n",
      "Iteration 1782, loss = 0.13246179\n",
      "Iteration 426, loss = 0.34322512\n",
      "Iteration 327, loss = 0.37145195\n",
      "Iteration 427, loss = 0.34307055\n",
      "Iteration 1783, loss = 0.13222291\n",
      "Iteration 492, loss = 0.38693791\n",
      "Iteration 328, loss = 0.37116922\n",
      "Iteration 2140, loss = 0.17009254\n",
      "Iteration 428, loss = 0.34299223\n",
      "Iteration 1441, loss = 0.19179105\n",
      "Iteration 329, loss = 0.37087638\n",
      "Iteration 429, loss = 0.34285006\n",
      "Iteration 1456, loss = 0.24839966\n",
      "Iteration 2141, loss = 0.16995958\n",
      "Iteration 330, loss = 0.37064203\n",
      "Iteration 430, loss = 0.34277870\n",
      "Iteration 176, loss = 0.42402274\n",
      "Iteration 1784, loss = 0.13220171\n",
      "Iteration 331, loss = 0.37030184\n",
      "Iteration 431, loss = 0.34263895\n",
      "Iteration 332, loss = 0.37001493\n",
      "Iteration 493, loss = 0.38670190\n",
      "Iteration 1785, loss = 0.13197788\n",
      "Iteration 333, loss = 0.36979036\n",
      "Iteration 334, loss = 0.36944280\n",
      "Iteration 2142, loss = 0.17002185\n",
      "Iteration 335, loss = 0.36923863\n",
      "Iteration 1786, loss = 0.13185108\n",
      "Iteration 336, loss = 0.36888187\n",
      "Iteration 432, loss = 0.34255017\n",
      "Iteration 1457, loss = 0.24824360\n",
      "Iteration 177, loss = 0.42355177\n",
      "Iteration 494, loss = 0.38652374\n",
      "Iteration 337, loss = 0.36860467\n",
      "Iteration 1442, loss = 0.19162096\n",
      "Iteration 178, loss = 0.42312134\n",
      "Iteration 2143, loss = 0.16983195\n",
      "Iteration 433, loss = 0.34246488\n",
      "Iteration 338, loss = 0.36834422\n",
      "Iteration 495, loss = 0.38636213\n",
      "Iteration 339, loss = 0.36807265\n",
      "Iteration 434, loss = 0.34233345\n",
      "Iteration 1458, loss = 0.24811612\n",
      "Iteration 1787, loss = 0.13183036\n",
      "Iteration 340, loss = 0.36779481\n",
      "Iteration 1443, loss = 0.19174889\n",
      "Iteration 179, loss = 0.42265076\n",
      "Iteration 341, loss = 0.36750306\n",
      "Iteration 435, loss = 0.34223725\n",
      "Iteration 342, loss = 0.36727648\n",
      "Iteration 496, loss = 0.38617662\n",
      "Iteration 343, loss = 0.36698029\n",
      "Iteration 436, loss = 0.34212999\n",
      "Iteration 2144, loss = 0.16982609\n",
      "Iteration 1444, loss = 0.19136149\n",
      "Iteration 344, loss = 0.36674604\n",
      "Iteration 437, loss = 0.34199334\n",
      "Iteration 180, loss = 0.42216445\n",
      "Iteration 1788, loss = 0.13166127\n",
      "Iteration 438, loss = 0.34189291\n",
      "Iteration 1459, loss = 0.24798742\n",
      "Iteration 345, loss = 0.36644963\n",
      "Iteration 497, loss = 0.38596724\n",
      "Iteration 2145, loss = 0.16963609\n",
      "Iteration 346, loss = 0.36621963\n",
      "Iteration 439, loss = 0.34181433\n",
      "Iteration 181, loss = 0.42174568\n",
      "Iteration 347, loss = 0.36591267\n",
      "Iteration 1789, loss = 0.13161981\n",
      "Iteration 1460, loss = 0.24786137\n",
      "Iteration 1445, loss = 0.19120388\n",
      "Iteration 348, loss = 0.36568442Iteration 440, loss = 0.34166767\n",
      "\n",
      "Iteration 349, loss = 0.36544448\n",
      "Iteration 182, loss = 0.42125861\n",
      "Iteration 1790, loss = 0.13158762\n",
      "Iteration 350, loss = 0.36519694\n",
      "Iteration 498, loss = 0.38572855\n",
      "Iteration 1461, loss = 0.24774964\n",
      "Iteration 441, loss = 0.34156599\n",
      "Iteration 2146, loss = 0.16951286\n",
      "Iteration 351, loss = 0.36490493\n",
      "Iteration 442, loss = 0.34148116\n",
      "Iteration 1462, loss = 0.24770873\n",
      "Iteration 1446, loss = 0.19111222\n",
      "Iteration 352, loss = 0.36465002\n",
      "Iteration 183, loss = 0.42081661\n",
      "Iteration 1791, loss = 0.13154045\n",
      "Iteration 443, loss = 0.34140604\n",
      "Iteration 353, loss = 0.36438827\n",
      "Iteration 2147, loss = 0.16947880\n",
      "Iteration 499, loss = 0.38557706\n",
      "Iteration 1447, loss = 0.19092529\n",
      "Iteration 1463, loss = 0.24752780\n",
      "Iteration 444, loss = 0.34127777\n",
      "Iteration 1792, loss = 0.13141806\n",
      "Iteration 354, loss = 0.36418051\n",
      "Iteration 1793, loss = 0.13130474\n",
      "Iteration 184, loss = 0.42042264\n",
      "Iteration 1464, loss = 0.24742251\n",
      "Iteration 355, loss = 0.36391428\n",
      "Iteration 1448, loss = 0.19097062\n",
      "Iteration 1794, loss = 0.13109350\n",
      "Iteration 445, loss = 0.34115756\n",
      "Iteration 2148, loss = 0.16937125\n",
      "Iteration 356, loss = 0.36366912\n",
      "Iteration 185, loss = 0.41994190\n",
      "Iteration 357, loss = 0.36344357\n",
      "Iteration 500, loss = 0.38535408\n",
      "Iteration 446, loss = 0.34104237\n",
      "Iteration 358, loss = 0.36316750\n",
      "Iteration 1795, loss = 0.13135970\n",
      "Iteration 447, loss = 0.34092162\n",
      "Iteration 1465, loss = 0.24726868\n",
      "Iteration 359, loss = 0.36291574\n",
      "Iteration 448, loss = 0.34082115\n",
      "Iteration 360, loss = 0.36267716\n",
      "Iteration 1449, loss = 0.19076053\n",
      "Iteration 361, loss = 0.36243215\n",
      "Iteration 449, loss = 0.34073671\n",
      "Iteration 2149, loss = 0.16925805\n",
      "Iteration 1796, loss = 0.13088512\n",
      "Iteration 186, loss = 0.41953772\n",
      "Iteration 362, loss = 0.36220231\n",
      "Iteration 450, loss = 0.34059993\n",
      "Iteration 1797, loss = 0.13082756\n",
      "Iteration 451, loss = 0.34051217\n",
      "Iteration 363, loss = 0.36197324\n",
      "Iteration 501, loss = 0.38519911\n",
      "Iteration 364, loss = 0.36172598\n",
      "Iteration 1798, loss = 0.13068568\n",
      "Iteration 452, loss = 0.34040677\n",
      "Iteration 2150, loss = 0.16934937\n",
      "Iteration 187, loss = 0.41909501\n",
      "Iteration 1466, loss = 0.24712281\n",
      "Iteration 1799, loss = 0.13063813\n",
      "Iteration 453, loss = 0.34029531\n",
      "Iteration 365, loss = 0.36147222\n",
      "Iteration 366, loss = 0.36126743\n",
      "Iteration 1450, loss = 0.19065251\n",
      "Iteration 188, loss = 0.41866619\n",
      "Iteration 2151, loss = 0.16922434\n",
      "Iteration 1800, loss = 0.13048071\n",
      "Iteration 367, loss = 0.36099925\n",
      "Iteration 502, loss = 0.38498780\n",
      "Iteration 368, loss = 0.36074050\n",
      "Iteration 454, loss = 0.34021004\n",
      "Iteration 369, loss = 0.36052433\n",
      "Iteration 189, loss = 0.41825775\n",
      "Iteration 1467, loss = 0.24700505\n",
      "Iteration 1801, loss = 0.13042040\n",
      "Iteration 370, loss = 0.36027234\n",
      "Iteration 2152, loss = 0.16905460\n",
      "Iteration 371, loss = 0.36005267\n",
      "Iteration 1802, loss = 0.13037740\n",
      "Iteration 372, loss = 0.35981884\n",
      "Iteration 455, loss = 0.34007890\n",
      "Iteration 373, loss = 0.35959116\n",
      "Iteration 2153, loss = 0.16885208\n",
      "Iteration 374, loss = 0.35938272\n",
      "Iteration 503, loss = 0.38480478\n",
      "Iteration 1803, loss = 0.13025462\n",
      "Iteration 375, loss = 0.35912777\n",
      "Iteration 1468, loss = 0.24688284\n",
      "Iteration 376, loss = 0.35888882\n",
      "Iteration 1451, loss = 0.19042167\n",
      "Iteration 377, loss = 0.35863882\n",
      "Iteration 1804, loss = 0.13042712\n",
      "Iteration 456, loss = 0.34001091\n",
      "Iteration 190, loss = 0.41784855\n",
      "Iteration 2154, loss = 0.16885947\n",
      "Iteration 378, loss = 0.35847061\n",
      "Iteration 379, loss = 0.35818591\n",
      "Iteration 457, loss = 0.33987609\n",
      "Iteration 380, loss = 0.35795150\n",
      "Iteration 381, loss = 0.35773219\n",
      "Iteration 1469, loss = 0.24678165\n",
      "Iteration 2155, loss = 0.16871271\n",
      "Iteration 191, loss = 0.41748268\n",
      "Iteration 504, loss = 0.38465631\n",
      "Iteration 382, loss = 0.35753695\n",
      "Iteration 1805, loss = 0.13016559\n",
      "Iteration 383, loss = 0.35726010\n",
      "Iteration 458, loss = 0.33977543\n",
      "Iteration 384, loss = 0.35704511\n",
      "Iteration 385, loss = 0.35681534\n",
      "Iteration 1470, loss = 0.24662329\n",
      "Iteration 192, loss = 0.41705368\n",
      "Iteration 1806, loss = 0.12993763\n",
      "Iteration 459, loss = 0.33965769\n",
      "Iteration 2156, loss = 0.16872500\n",
      "Iteration 386, loss = 0.35668663\n",
      "Iteration 460, loss = 0.33955647\n",
      "Iteration 387, loss = 0.35632974\n",
      "Iteration 1471, loss = 0.24660447\n",
      "Iteration 505, loss = 0.38438360\n",
      "Iteration 1807, loss = 0.12989798\n",
      "Iteration 193, loss = 0.41664744\n",
      "Iteration 388, loss = 0.35612479\n",
      "Iteration 2157, loss = 0.16860069\n",
      "Iteration 461, loss = 0.33947205\n",
      "Iteration 1472, loss = 0.24640119\n",
      "Iteration 389, loss = 0.35590816\n",
      "Iteration 462, loss = 0.33933612\n",
      "Iteration 1808, loss = 0.12983086\n",
      "Iteration 506, loss = 0.38421048\n",
      "Iteration 390, loss = 0.35567662\n",
      "Iteration 463, loss = 0.33928394\n",
      "Iteration 2158, loss = 0.16849854\n",
      "Iteration 194, loss = 0.41630768\n",
      "Iteration 391, loss = 0.35549227\n",
      "Iteration 1473, loss = 0.24624915\n",
      "Iteration 1809, loss = 0.12965160\n",
      "Iteration 392, loss = 0.35523716\n",
      "Iteration 393, loss = 0.35501970\n",
      "Iteration 195, loss = 0.41586366\n",
      "Iteration 394, loss = 0.35479368\n",
      "Iteration 464, loss = 0.33911689\n",
      "Iteration 395, loss = 0.35455612\n",
      "Iteration 1810, loss = 0.12959416\n",
      "Iteration 2159, loss = 0.16832726\n",
      "Iteration 1474, loss = 0.24614230\n",
      "Iteration 507, loss = 0.38402243\n",
      "Iteration 1811, loss = 0.12946961\n",
      "Iteration 1452, loss = 0.19020480\n",
      "Iteration 396, loss = 0.35435868\n",
      "Iteration 465, loss = 0.33900955\n",
      "Iteration 2160, loss = 0.16828178\n",
      "Iteration 1812, loss = 0.12937156\n",
      "Iteration 397, loss = 0.35414465\n",
      "Iteration 398, loss = 0.35395656\n",
      "Iteration 508, loss = 0.38380500\n",
      "Iteration 2161, loss = 0.16819704\n",
      "Iteration 466, loss = 0.33892928\n",
      "Iteration 196, loss = 0.41546892\n",
      "Iteration 1475, loss = 0.24600987\n",
      "Iteration 1453, loss = 0.19024279\n",
      "Iteration 399, loss = 0.35371954\n",
      "Iteration 1813, loss = 0.12929217\n",
      "Iteration 467, loss = 0.33881152\n",
      "Iteration 1476, loss = 0.24615113\n",
      "Iteration 400, loss = 0.35352392\n",
      "Iteration 1814, loss = 0.12923680Iteration 509, loss = 0.38364609\n",
      "Iteration 197, loss = 0.41509218\n",
      "Iteration 401, loss = 0.35327704\n",
      "Iteration 2162, loss = 0.16806219\n",
      "Iteration 468, loss = 0.33870966\n",
      "\n",
      "Iteration 402, loss = 0.35305417\n",
      "Iteration 469, loss = 0.33861689\n",
      "Iteration 403, loss = 0.35285594\n",
      "Iteration 470, loss = 0.33848782\n",
      "Iteration 198, loss = 0.41471085\n",
      "Iteration 404, loss = 0.35259662\n",
      "Iteration 2163, loss = 0.16800075\n",
      "Iteration 1815, loss = 0.12909091\n",
      "Iteration 405, loss = 0.35239409\n",
      "Iteration 1454, loss = 0.18991940\n",
      "Iteration 471, loss = 0.33837657\n",
      "Iteration 1816, loss = 0.12904763\n",
      "Iteration 406, loss = 0.35218052\n",
      "Iteration 472, loss = 0.33826993\n",
      "Iteration 2164, loss = 0.16797052\n",
      "Iteration 473, loss = 0.33815967\n",
      "Iteration 1817, loss = 0.12888317\n",
      "Iteration 407, loss = 0.35193552\n",
      "Iteration 1477, loss = 0.24577513\n",
      "Iteration 199, loss = 0.41433874\n",
      "Iteration 510, loss = 0.38344435\n",
      "Iteration 1455, loss = 0.18996889\n",
      "Iteration 408, loss = 0.35171874\n",
      "Iteration 409, loss = 0.35151406\n",
      "Iteration 474, loss = 0.33807244\n",
      "Iteration 2165, loss = 0.16789111\n",
      "Iteration 410, loss = 0.35131988\n",
      "Iteration 1478, loss = 0.24569680\n",
      "Iteration 475, loss = 0.33802534\n",
      "Iteration 411, loss = 0.35108079\n",
      "Iteration 412, loss = 0.35086065\n",
      "Iteration 476, loss = 0.33786154\n",
      "Iteration 413, loss = 0.35069348\n",
      "Iteration 1456, loss = 0.18974922\n",
      "Iteration 414, loss = 0.35049522\n",
      "Iteration 1818, loss = 0.12880181\n",
      "Iteration 477, loss = 0.33774637\n",
      "Iteration 511, loss = 0.38322495\n",
      "Iteration 415, loss = 0.35031176\n",
      "Iteration 1479, loss = 0.24551827\n",
      "Iteration 416, loss = 0.35004436\n",
      "Iteration 478, loss = 0.33764914\n",
      "Iteration 200, loss = 0.41396644\n",
      "Iteration 417, loss = 0.34981585\n",
      "Iteration 1457, loss = 0.18953519\n",
      "Iteration 1819, loss = 0.12876759\n",
      "Iteration 479, loss = 0.33762946\n",
      "Iteration 418, loss = 0.34967020\n",
      "Iteration 480, loss = 0.33750334\n",
      "Iteration 419, loss = 0.34939837\n",
      "Iteration 1480, loss = 0.24542795\n",
      "Iteration 2166, loss = 0.16782345\n",
      "Iteration 481, loss = 0.33733019\n",
      "Iteration 420, loss = 0.34920155\n",
      "Iteration 201, loss = 0.41360307\n",
      "Iteration 482, loss = 0.33725584\n",
      "Iteration 512, loss = 0.38315186\n",
      "Iteration 1820, loss = 0.12863375\n",
      "Iteration 1458, loss = 0.18961737\n",
      "Iteration 1481, loss = 0.24526692\n",
      "Iteration 421, loss = 0.34898660\n",
      "Iteration 483, loss = 0.33712871\n",
      "Iteration 422, loss = 0.34876092\n",
      "Iteration 202, loss = 0.41323064\n",
      "Iteration 484, loss = 0.33707953\n",
      "Iteration 2167, loss = 0.16778049\n",
      "Iteration 423, loss = 0.34856478\n",
      "Iteration 1482, loss = 0.24515533\n",
      "Iteration 1821, loss = 0.12852600\n",
      "Iteration 1459, loss = 0.18926888\n",
      "Iteration 424, loss = 0.34838910\n",
      "Iteration 485, loss = 0.33691008\n",
      "Iteration 513, loss = 0.38285879\n",
      "Iteration 425, loss = 0.34817283\n",
      "Iteration 203, loss = 0.41285998\n",
      "Iteration 426, loss = 0.34802160\n",
      "Iteration 1822, loss = 0.12858682\n",
      "Iteration 1483, loss = 0.24502705\n",
      "Iteration 427, loss = 0.34777743\n",
      "Iteration 2168, loss = 0.16759597\n",
      "Iteration 486, loss = 0.33680138\n",
      "Iteration 1460, loss = 0.18915245\n",
      "Iteration 1823, loss = 0.12861938\n",
      "Iteration 428, loss = 0.34755941\n",
      "Iteration 429, loss = 0.34733257\n",
      "Iteration 487, loss = 0.33670536\n",
      "Iteration 1484, loss = 0.24492888\n",
      "Iteration 1824, loss = 0.12828904\n",
      "Iteration 204, loss = 0.41252746\n",
      "Iteration 430, loss = 0.34713535\n",
      "Iteration 2169, loss = 0.16743804\n",
      "Iteration 1461, loss = 0.18906621\n",
      "Iteration 431, loss = 0.34692218\n",
      "Iteration 1485, loss = 0.24479790\n",
      "Iteration 514, loss = 0.38267956\n",
      "Iteration 488, loss = 0.33658885\n",
      "Iteration 432, loss = 0.34679840\n",
      "Iteration 1825, loss = 0.12817851\n",
      "Iteration 205, loss = 0.41216199\n",
      "Iteration 433, loss = 0.34651375\n",
      "Iteration 489, loss = 0.33650437\n",
      "Iteration 2170, loss = 0.16743705\n",
      "Iteration 1826, loss = 0.12816591\n",
      "Iteration 434, loss = 0.34633954\n",
      "Iteration 435, loss = 0.34610784\n",
      "Iteration 490, loss = 0.33640010\n",
      "Iteration 1462, loss = 0.18884826\n",
      "Iteration 1486, loss = 0.24462546\n",
      "Iteration 436, loss = 0.34589516\n",
      "Iteration 491, loss = 0.33631767\n",
      "Iteration 515, loss = 0.38250449\n",
      "Iteration 1827, loss = 0.12798491\n",
      "Iteration 437, loss = 0.34570217\n",
      "Iteration 2171, loss = 0.16732281\n",
      "Iteration 206, loss = 0.41183115\n",
      "Iteration 438, loss = 0.34551326\n",
      "Iteration 1487, loss = 0.24452669\n",
      "Iteration 492, loss = 0.33618643\n",
      "Iteration 439, loss = 0.34528641\n",
      "Iteration 440, loss = 0.34507789\n",
      "Iteration 207, loss = 0.41146729\n",
      "Iteration 493, loss = 0.33607261\n",
      "Iteration 1828, loss = 0.12790709\n",
      "Iteration 1463, loss = 0.18873167\n",
      "Iteration 441, loss = 0.34490171\n",
      "Iteration 516, loss = 0.38228960\n",
      "Iteration 208, loss = 0.41110467\n",
      "Iteration 494, loss = 0.33596756\n",
      "Iteration 1829, loss = 0.12796480\n",
      "Iteration 2172, loss = 0.16731982\n",
      "Iteration 442, loss = 0.34467044\n",
      "Iteration 443, loss = 0.34451709\n",
      "Iteration 1488, loss = 0.24444193\n",
      "Iteration 517, loss = 0.38207697\n",
      "Iteration 1464, loss = 0.18859413\n",
      "Iteration 444, loss = 0.34436300\n",
      "Iteration 495, loss = 0.33589419\n",
      "Iteration 1830, loss = 0.12770386\n",
      "Iteration 445, loss = 0.34407940\n",
      "Iteration 2173, loss = 0.16715407\n",
      "Iteration 446, loss = 0.34387449\n",
      "Iteration 1489, loss = 0.24439109\n",
      "Iteration 496, loss = 0.33577897\n",
      "Iteration 447, loss = 0.34366421\n",
      "Iteration 518, loss = 0.38192742\n",
      "Iteration 209, loss = 0.41076528\n",
      "Iteration 1831, loss = 0.12764591\n",
      "Iteration 448, loss = 0.34347921\n",
      "Iteration 1465, loss = 0.18871740\n",
      "Iteration 449, loss = 0.34325962\n",
      "Iteration 450, loss = 0.34312378\n",
      "Iteration 451, loss = 0.34287209\n",
      "Iteration 1832, loss = 0.12753799\n",
      "Iteration 1490, loss = 0.24412265\n",
      "Iteration 452, loss = 0.34275639\n",
      "Iteration 2174, loss = 0.16731818\n",
      "Iteration 497, loss = 0.33566409\n",
      "Iteration 453, loss = 0.34251765\n",
      "Iteration 1833, loss = 0.12745868\n",
      "Iteration 454, loss = 0.34230079\n",
      "Iteration 498, loss = 0.33563288\n",
      "Iteration 210, loss = 0.41041665\n",
      "Iteration 519, loss = 0.38172186\n",
      "Iteration 499, loss = 0.33546040\n",
      "Iteration 2175, loss = 0.16707705\n",
      "Iteration 455, loss = 0.34212540\n",
      "Iteration 1834, loss = 0.12734119\n",
      "Iteration 456, loss = 0.34193534\n",
      "Iteration 1491, loss = 0.24402744\n",
      "Iteration 1466, loss = 0.18847952\n",
      "Iteration 457, loss = 0.34175635\n",
      "Iteration 500, loss = 0.33537060\n",
      "Iteration 1835, loss = 0.12755254\n",
      "Iteration 211, loss = 0.41009724\n",
      "Iteration 458, loss = 0.34155108\n",
      "Iteration 459, loss = 0.34136551\n",
      "Iteration 1492, loss = 0.24390150\n",
      "Iteration 501, loss = 0.33524599\n",
      "Iteration 520, loss = 0.38148240\n",
      "Iteration 2176, loss = 0.16682207\n",
      "Iteration 460, loss = 0.34120679\n",
      "Iteration 1836, loss = 0.12717581\n",
      "Iteration 1493, loss = 0.24378291\n",
      "Iteration 502, loss = 0.33517873\n",
      "Iteration 2177, loss = 0.16672998\n",
      "Iteration 461, loss = 0.34102910\n",
      "Iteration 521, loss = 0.38130960\n",
      "Iteration 212, loss = 0.40978750\n",
      "Iteration 1467, loss = 0.18816867\n",
      "Iteration 503, loss = 0.33503801\n",
      "Iteration 462, loss = 0.34080716\n",
      "Iteration 463, loss = 0.34060841\n",
      "Iteration 1837, loss = 0.12705312\n",
      "Iteration 464, loss = 0.34041799\n",
      "Iteration 1494, loss = 0.24366091\n",
      "Iteration 504, loss = 0.33496290\n",
      "Iteration 2178, loss = 0.16675017\n",
      "Iteration 213, loss = 0.40940346\n",
      "Iteration 465, loss = 0.34022640\n",
      "Iteration 1468, loss = 0.18811772\n",
      "Iteration 1838, loss = 0.12700952\n",
      "Iteration 505, loss = 0.33485540\n",
      "Iteration 466, loss = 0.34003744\n",
      "Iteration 1495, loss = 0.24356504\n",
      "Iteration 2179, loss = 0.16663622\n",
      "Iteration 467, loss = 0.33985461\n",
      "Iteration 522, loss = 0.38110575\n",
      "Iteration 1469, loss = 0.18791566\n",
      "Iteration 506, loss = 0.33475205\n",
      "Iteration 468, loss = 0.33966997\n",
      "Iteration 1839, loss = 0.12697912\n",
      "Iteration 507, loss = 0.33464627\n",
      "Iteration 2180, loss = 0.16646027\n",
      "Iteration 1496, loss = 0.24341744\n",
      "Iteration 214, loss = 0.40908832\n",
      "Iteration 508, loss = 0.33455172\n",
      "Iteration 509, loss = 0.33446387\n",
      "Iteration 1840, loss = 0.12686127\n",
      "Iteration 469, loss = 0.33949166\n",
      "Iteration 1497, loss = 0.24327526\n",
      "Iteration 523, loss = 0.38091264\n",
      "Iteration 470, loss = 0.33935874\n",
      "Iteration 471, loss = 0.33913828\n",
      "Iteration 2181, loss = 0.16647484\n",
      "Iteration 472, loss = 0.33900403\n",
      "Iteration 510, loss = 0.33432902\n",
      "Iteration 215, loss = 0.40875555\n",
      "Iteration 1470, loss = 0.18772763\n",
      "Iteration 1841, loss = 0.12691911\n",
      "Iteration 2182, loss = 0.16633982\n",
      "Iteration 511, loss = 0.33423086\n",
      "Iteration 1498, loss = 0.24323713\n",
      "Iteration 216, loss = 0.40841499\n",
      "Iteration 473, loss = 0.33878488\n",
      "Iteration 1471, loss = 0.18769208\n",
      "Iteration 1842, loss = 0.12675621\n",
      "Iteration 512, loss = 0.33411895\n",
      "Iteration 474, loss = 0.33860246\n",
      "Iteration 475, loss = 0.33846625\n",
      "Iteration 524, loss = 0.38075298\n",
      "Iteration 1843, loss = 0.12659323\n",
      "Iteration 1499, loss = 0.24309803\n",
      "Iteration 1472, loss = 0.18751068\n",
      "Iteration 217, loss = 0.40809369\n",
      "Iteration 513, loss = 0.33404929\n",
      "Iteration 476, loss = 0.33821823\n",
      "Iteration 2183, loss = 0.16623618\n",
      "Iteration 477, loss = 0.33802854\n",
      "Iteration 478, loss = 0.33788129\n",
      "Iteration 218, loss = 0.40778606\n",
      "Iteration 479, loss = 0.33771959\n",
      "Iteration 514, loss = 0.33397239\n",
      "Iteration 1844, loss = 0.12646065\n",
      "Iteration 525, loss = 0.38052936\n",
      "Iteration 1500, loss = 0.24290225\n",
      "Iteration 480, loss = 0.33758865\n",
      "Iteration 2184, loss = 0.16615138\n",
      "Iteration 481, loss = 0.33734600\n",
      "Iteration 515, loss = 0.33383719\n",
      "Iteration 219, loss = 0.40745144\n",
      "Iteration 482, loss = 0.33715841\n",
      "Iteration 1845, loss = 0.12637950\n",
      "Iteration 483, loss = 0.33697903\n",
      "Iteration 526, loss = 0.38033167\n",
      "Iteration 516, loss = 0.33382865Iteration 1473, loss = 0.18743501\n",
      "\n",
      "Iteration 484, loss = 0.33680678\n",
      "Iteration 220, loss = 0.40714201\n",
      "Iteration 2185, loss = 0.16612242\n",
      "Iteration 485, loss = 0.33663409\n",
      "Iteration 1846, loss = 0.12629794\n",
      "Iteration 1501, loss = 0.24280804\n",
      "Iteration 486, loss = 0.33648263\n",
      "Iteration 527, loss = 0.38022773\n",
      "Iteration 487, loss = 0.33629194\n",
      "Iteration 488, loss = 0.33610461\n",
      "Iteration 489, loss = 0.33594115\n",
      "Iteration 517, loss = 0.33364128\n",
      "Iteration 2186, loss = 0.16602291\n",
      "Iteration 490, loss = 0.33574033\n",
      "Iteration 1502, loss = 0.24273025\n",
      "Iteration 528, loss = 0.37996213\n",
      "Iteration 491, loss = 0.33559522\n",
      "Iteration 221, loss = 0.40681328\n",
      "Iteration 518, loss = 0.33352898\n",
      "Iteration 1847, loss = 0.12621835\n",
      "Iteration 492, loss = 0.33540922\n",
      "Iteration 519, loss = 0.33342756\n",
      "Iteration 2187, loss = 0.16586133\n",
      "Iteration 493, loss = 0.33523036\n",
      "Iteration 222, loss = 0.40649100\n",
      "Iteration 1848, loss = 0.12610147\n",
      "Iteration 1474, loss = 0.18717558\n",
      "Iteration 520, loss = 0.33334117\n",
      "Iteration 494, loss = 0.33509078\n",
      "Iteration 1849, loss = 0.12612409\n",
      "Iteration 495, loss = 0.33493252\n",
      "Iteration 496, loss = 0.33473949\n",
      "Iteration 521, loss = 0.33324415\n",
      "Iteration 2188, loss = 0.16580998\n",
      "Iteration 1503, loss = 0.24260276\n",
      "Iteration 497, loss = 0.33458812\n",
      "Iteration 529, loss = 0.37990377\n",
      "Iteration 223, loss = 0.40624857\n",
      "Iteration 1850, loss = 0.12598992\n",
      "Iteration 522, loss = 0.33309578\n",
      "Iteration 2189, loss = 0.16570906\n",
      "Iteration 1504, loss = 0.24250871\n",
      "Iteration 1475, loss = 0.18710430\n",
      "Iteration 1851, loss = 0.12588804\n",
      "Iteration 523, loss = 0.33300542\n",
      "Iteration 498, loss = 0.33439376\n",
      "Iteration 1505, loss = 0.24232004\n",
      "Iteration 224, loss = 0.40585350\n",
      "Iteration 2190, loss = 0.16566570\n",
      "Iteration 524, loss = 0.33290026\n",
      "Iteration 525, loss = 0.33286076\n",
      "Iteration 530, loss = 0.37966962\n",
      "Iteration 1852, loss = 0.12579841\n",
      "Iteration 1506, loss = 0.24217369\n",
      "Iteration 2191, loss = 0.16556085\n",
      "Iteration 499, loss = 0.33422311\n",
      "Iteration 526, loss = 0.33271078\n",
      "Iteration 500, loss = 0.33404310\n",
      "Iteration 1476, loss = 0.18698091\n",
      "Iteration 225, loss = 0.40556332\n",
      "Iteration 501, loss = 0.33388537\n",
      "Iteration 527, loss = 0.33259849\n",
      "Iteration 502, loss = 0.33371732\n",
      "Iteration 531, loss = 0.37934544\n",
      "Iteration 1507, loss = 0.24203593\n",
      "Iteration 503, loss = 0.33360100\n",
      "Iteration 1853, loss = 0.12581160\n",
      "Iteration 504, loss = 0.33339021\n",
      "Iteration 528, loss = 0.33250851\n",
      "Iteration 505, loss = 0.33320362\n",
      "Iteration 2192, loss = 0.16545161\n",
      "Iteration 506, loss = 0.33305201\n",
      "Iteration 507, loss = 0.33287124\n",
      "Iteration 529, loss = 0.33239936\n",
      "Iteration 532, loss = 0.37919568\n",
      "Iteration 508, loss = 0.33274563\n",
      "Iteration 509, loss = 0.33258422\n",
      "Iteration 1477, loss = 0.18682379\n",
      "Iteration 510, loss = 0.33245484\n",
      "Iteration 530, loss = 0.33229695\n",
      "Iteration 226, loss = 0.40527578\n",
      "Iteration 2193, loss = 0.16535246\n",
      "Iteration 511, loss = 0.33227133\n",
      "Iteration 1508, loss = 0.24203895\n",
      "Iteration 531, loss = 0.33219313\n",
      "Iteration 512, loss = 0.33206244\n",
      "Iteration 513, loss = 0.33192079\n",
      "Iteration 533, loss = 0.37901782\n",
      "Iteration 1854, loss = 0.12557285\n",
      "Iteration 1478, loss = 0.18663479\n",
      "Iteration 532, loss = 0.33209913\n",
      "Iteration 514, loss = 0.33173629\n",
      "Iteration 227, loss = 0.40496773\n",
      "Iteration 1509, loss = 0.24180877\n",
      "Iteration 533, loss = 0.33201134\n",
      "Iteration 2194, loss = 0.16533355\n",
      "Iteration 1855, loss = 0.12553931\n",
      "Iteration 1479, loss = 0.18667434\n",
      "Iteration 515, loss = 0.33159488\n",
      "Iteration 516, loss = 0.33142495\n",
      "Iteration 228, loss = 0.40464237\n",
      "Iteration 1856, loss = 0.12548438\n",
      "Iteration 534, loss = 0.33191106\n",
      "Iteration 517, loss = 0.33127757\n",
      "Iteration 1510, loss = 0.24167415\n",
      "Iteration 1480, loss = 0.18648770\n",
      "Iteration 534, loss = 0.37881166\n",
      "Iteration 518, loss = 0.33111171\n",
      "Iteration 2195, loss = 0.16518581\n",
      "Iteration 535, loss = 0.33180479\n",
      "Iteration 519, loss = 0.33093969\n",
      "Iteration 1857, loss = 0.12544819\n",
      "Iteration 1511, loss = 0.24152948\n",
      "Iteration 520, loss = 0.33076516\n",
      "Iteration 1481, loss = 0.18638847\n",
      "Iteration 521, loss = 0.33062994\n",
      "Iteration 522, loss = 0.33044665\n",
      "Iteration 1858, loss = 0.12524765\n",
      "Iteration 536, loss = 0.33172514\n",
      "Iteration 523, loss = 0.33028734\n",
      "Iteration 524, loss = 0.33018553\n",
      "Iteration 1512, loss = 0.24153726\n",
      "Iteration 537, loss = 0.33158734\n",
      "Iteration 535, loss = 0.37860109\n",
      "Iteration 1859, loss = 0.12518469\n",
      "Iteration 229, loss = 0.40437004\n",
      "Iteration 1482, loss = 0.18616367\n",
      "Iteration 1513, loss = 0.24133235\n",
      "Iteration 2196, loss = 0.16516476\n",
      "Iteration 536, loss = 0.37839416\n",
      "Iteration 538, loss = 0.33148634\n",
      "Iteration 525, loss = 0.32998229\n",
      "Iteration 1860, loss = 0.12513416\n",
      "Iteration 230, loss = 0.40406295\n",
      "Iteration 526, loss = 0.32988965\n",
      "Iteration 1514, loss = 0.24117360\n",
      "Iteration 527, loss = 0.32965796\n",
      "Iteration 539, loss = 0.33138002\n",
      "Iteration 1861, loss = 0.12507799\n",
      "Iteration 528, loss = 0.32949201\n",
      "Iteration 2197, loss = 0.16503869\n",
      "Iteration 529, loss = 0.32935037\n",
      "Iteration 1862, loss = 0.12496849\n",
      "Iteration 530, loss = 0.32920264\n",
      "Iteration 540, loss = 0.33138760\n",
      "Iteration 1483, loss = 0.18597897\n",
      "Iteration 231, loss = 0.40375249\n",
      "Iteration 531, loss = 0.32912461\n",
      "Iteration 541, loss = 0.33117319\n",
      "Iteration 537, loss = 0.37825437\n",
      "Iteration 1515, loss = 0.24108624\n",
      "Iteration 532, loss = 0.32892158\n",
      "Iteration 1863, loss = 0.12498291\n",
      "Iteration 2198, loss = 0.16495900\n",
      "Iteration 542, loss = 0.33107063\n",
      "Iteration 1864, loss = 0.12476667\n",
      "Iteration 533, loss = 0.32870839\n",
      "Iteration 232, loss = 0.40347469\n",
      "Iteration 534, loss = 0.32858139\n",
      "Iteration 1865, loss = 0.12489304\n",
      "Iteration 538, loss = 0.37806686\n",
      "Iteration 1484, loss = 0.18600413\n",
      "Iteration 535, loss = 0.32839951\n",
      "Iteration 536, loss = 0.32827282\n",
      "Iteration 1866, loss = 0.12484786\n",
      "Iteration 537, loss = 0.32808599\n",
      "Iteration 538, loss = 0.32794033\n",
      "Iteration 1867, loss = 0.12457308\n",
      "Iteration 543, loss = 0.33099902\n",
      "Iteration 539, loss = 0.32782806\n",
      "Iteration 539, loss = 0.37784162\n",
      "Iteration 1868, loss = 0.12455075\n",
      "Iteration 540, loss = 0.32764550\n",
      "Iteration 1516, loss = 0.24097942\n",
      "Iteration 1485, loss = 0.18572225\n",
      "Iteration 233, loss = 0.40319027\n",
      "Iteration 544, loss = 0.33088638\n",
      "Iteration 2199, loss = 0.16494038\n",
      "Iteration 541, loss = 0.32748486\n",
      "Iteration 1869, loss = 0.12439862\n",
      "Iteration 545, loss = 0.33080227\n",
      "Iteration 1517, loss = 0.24079433\n",
      "Iteration 542, loss = 0.32735347\n",
      "Iteration 2200, loss = 0.16480908\n",
      "Iteration 234, loss = 0.40288571\n",
      "Iteration 546, loss = 0.33071179\n",
      "Iteration 543, loss = 0.32718518\n",
      "Iteration 1518, loss = 0.24068871\n",
      "Iteration 544, loss = 0.32702552\n",
      "Iteration 2201, loss = 0.16470403\n",
      "Iteration 545, loss = 0.32691132\n",
      "Iteration 1519, loss = 0.24062573\n",
      "Iteration 547, loss = 0.33057293\n",
      "Iteration 540, loss = 0.37761993\n",
      "Iteration 1486, loss = 0.18603262\n",
      "Iteration 546, loss = 0.32674687\n",
      "Iteration 1870, loss = 0.12428508\n",
      "Iteration 548, loss = 0.33051562\n",
      "Iteration 2202, loss = 0.16482176\n",
      "Iteration 235, loss = 0.40265221\n",
      "Iteration 549, loss = 0.33037536\n",
      "Iteration 1871, loss = 0.12428195\n",
      "Iteration 1520, loss = 0.24043499\n",
      "Iteration 236, loss = 0.40231009\n",
      "Iteration 541, loss = 0.37739172\n",
      "Iteration 1872, loss = 0.12460383\n",
      "Iteration 2203, loss = 0.16452456\n",
      "Iteration 550, loss = 0.33027735\n",
      "Iteration 547, loss = 0.32664348\n",
      "Iteration 548, loss = 0.32644913\n",
      "Iteration 551, loss = 0.33016661\n",
      "Iteration 237, loss = 0.40203825\n",
      "Iteration 549, loss = 0.32630804\n",
      "Iteration 552, loss = 0.33005892\n",
      "Iteration 550, loss = 0.32614106\n",
      "Iteration 1521, loss = 0.24031846\n",
      "Iteration 2204, loss = 0.16440953\n",
      "Iteration 551, loss = 0.32601212\n",
      "Iteration 553, loss = 0.32994782\n",
      "Iteration 552, loss = 0.32586616\n",
      "Iteration 1487, loss = 0.18561068\n",
      "Iteration 1873, loss = 0.12410355\n",
      "Iteration 1522, loss = 0.24025179\n",
      "Iteration 553, loss = 0.32570624\n",
      "Iteration 238, loss = 0.40178043\n",
      "Iteration 2205, loss = 0.16439985\n",
      "Iteration 554, loss = 0.32553230\n",
      "Iteration 1488, loss = 0.18533738\n",
      "Iteration 555, loss = 0.32543736\n",
      "Iteration 556, loss = 0.32523087\n",
      "Iteration 557, loss = 0.32505737\n",
      "Iteration 1874, loss = 0.12396616\n",
      "Iteration 554, loss = 0.32987149\n",
      "Iteration 1523, loss = 0.24013939\n",
      "Iteration 1489, loss = 0.18532443\n",
      "Iteration 558, loss = 0.32493937\n",
      "Iteration 542, loss = 0.37719773\n",
      "Iteration 559, loss = 0.32474869\n",
      "Iteration 239, loss = 0.40146735\n",
      "Iteration 1524, loss = 0.23999749\n",
      "Iteration 2206, loss = 0.16432544\n",
      "Iteration 1875, loss = 0.12389000\n",
      "Iteration 240, loss = 0.40118721\n",
      "Iteration 1876, loss = 0.12384280\n",
      "Iteration 1525, loss = 0.23982203\n",
      "Iteration 560, loss = 0.32461803\n",
      "Iteration 555, loss = 0.32977041\n",
      "Iteration 1490, loss = 0.18509485\n",
      "Iteration 241, loss = 0.40092948\n",
      "Iteration 543, loss = 0.37704326\n",
      "Iteration 561, loss = 0.32444748\n",
      "Iteration 1877, loss = 0.12384768\n",
      "Iteration 2207, loss = 0.16419775\n",
      "Iteration 1526, loss = 0.23982073\n",
      "Iteration 562, loss = 0.32429173\n",
      "Iteration 556, loss = 0.32968202\n",
      "Iteration 563, loss = 0.32419092\n",
      "Iteration 564, loss = 0.32401127\n",
      "Iteration 557, loss = 0.32956362\n",
      "Iteration 2208, loss = 0.16412544\n",
      "Iteration 1878, loss = 0.12367082\n",
      "Iteration 565, loss = 0.32385092\n",
      "Iteration 242, loss = 0.40062629\n",
      "Iteration 558, loss = 0.32947497\n",
      "Iteration 566, loss = 0.32368020\n",
      "Iteration 544, loss = 0.37686928\n",
      "Iteration 1527, loss = 0.23961640\n",
      "Iteration 567, loss = 0.32355650\n",
      "Iteration 1491, loss = 0.18494797\n",
      "Iteration 2209, loss = 0.16407134\n",
      "Iteration 1879, loss = 0.12356297\n",
      "Iteration 568, loss = 0.32338174\n",
      "Iteration 569, loss = 0.32323459\n",
      "Iteration 559, loss = 0.32934889\n",
      "Iteration 570, loss = 0.32308809\n",
      "Iteration 1880, loss = 0.12350010\n",
      "Iteration 571, loss = 0.32296713\n",
      "Iteration 243, loss = 0.40035278\n",
      "Iteration 1528, loss = 0.23944125\n",
      "Iteration 572, loss = 0.32281986\n",
      "Iteration 560, loss = 0.32929500\n",
      "Iteration 545, loss = 0.37663553\n",
      "Iteration 1881, loss = 0.12340197\n",
      "Iteration 573, loss = 0.32265137\n",
      "Iteration 561, loss = 0.32916003\n",
      "Iteration 1492, loss = 0.18488343\n",
      "Iteration 574, loss = 0.32251342\n",
      "Iteration 2210, loss = 0.16392255\n",
      "Iteration 244, loss = 0.40009985\n",
      "Iteration 575, loss = 0.32237907\n",
      "Iteration 1882, loss = 0.12380158\n",
      "Iteration 576, loss = 0.32227604\n",
      "Iteration 562, loss = 0.32904957\n",
      "Iteration 245, loss = 0.39980147\n",
      "Iteration 1529, loss = 0.23940951\n",
      "Iteration 2211, loss = 0.16387849\n",
      "Iteration 577, loss = 0.32204632\n",
      "Iteration 563, loss = 0.32897871\n",
      "Iteration 546, loss = 0.37648535\n",
      "Iteration 578, loss = 0.32189738\n",
      "Iteration 1883, loss = 0.12347347\n",
      "Iteration 1493, loss = 0.18479690\n",
      "Iteration 564, loss = 0.32886532\n",
      "Iteration 579, loss = 0.32174334\n",
      "Iteration 1884, loss = 0.12317973\n",
      "Iteration 246, loss = 0.39950937\n",
      "Iteration 580, loss = 0.32162193\n",
      "Iteration 1530, loss = 0.23920137\n",
      "Iteration 565, loss = 0.32877753\n",
      "Iteration 581, loss = 0.32147729\n",
      "Iteration 1494, loss = 0.18484598\n",
      "Iteration 247, loss = 0.39925605\n",
      "Iteration 1885, loss = 0.12318987\n",
      "Iteration 1531, loss = 0.23916020\n",
      "Iteration 2212, loss = 0.16393056\n",
      "Iteration 566, loss = 0.32868235\n",
      "Iteration 582, loss = 0.32132442\n",
      "Iteration 567, loss = 0.32857905\n",
      "Iteration 248, loss = 0.39899231\n",
      "Iteration 583, loss = 0.32117843\n",
      "Iteration 2213, loss = 0.16370847\n",
      "Iteration 568, loss = 0.32847827\n",
      "Iteration 584, loss = 0.32102012\n",
      "Iteration 547, loss = 0.37625062\n",
      "Iteration 1886, loss = 0.12322660\n",
      "Iteration 569, loss = 0.32835131\n",
      "Iteration 585, loss = 0.32089020\n",
      "Iteration 1532, loss = 0.23899265\n",
      "Iteration 586, loss = 0.32071458\n",
      "Iteration 2214, loss = 0.16360138\n",
      "Iteration 1495, loss = 0.18444095\n",
      "Iteration 587, loss = 0.32057875\n",
      "Iteration 570, loss = 0.32828558\n",
      "Iteration 1887, loss = 0.12301942\n",
      "Iteration 588, loss = 0.32051897\n",
      "Iteration 249, loss = 0.39869838\n",
      "Iteration 548, loss = 0.37601598\n",
      "Iteration 1888, loss = 0.12295683\n",
      "Iteration 589, loss = 0.32026382\n",
      "Iteration 590, loss = 0.32010885\n",
      "Iteration 571, loss = 0.32816476\n",
      "Iteration 1533, loss = 0.23884392\n",
      "Iteration 1889, loss = 0.12281724\n",
      "Iteration 2215, loss = 0.16349364\n",
      "Iteration 572, loss = 0.32804775\n",
      "Iteration 591, loss = 0.32000643\n",
      "Iteration 1496, loss = 0.18436039\n",
      "Iteration 592, loss = 0.31983493\n",
      "Iteration 250, loss = 0.39842488\n",
      "Iteration 573, loss = 0.32795745\n",
      "Iteration 549, loss = 0.37581025\n",
      "Iteration 593, loss = 0.31967629Iteration 2216, loss = 0.16348214\n",
      "\n",
      "Iteration 574, loss = 0.32785023\n",
      "Iteration 594, loss = 0.31954619\n",
      "Iteration 595, loss = 0.31937654\n",
      "Iteration 251, loss = 0.39817747\n",
      "Iteration 575, loss = 0.32782588\n",
      "Iteration 596, loss = 0.31924920\n",
      "Iteration 1534, loss = 0.23868889\n",
      "Iteration 2217, loss = 0.16334255Iteration 1890, loss = 0.12271567\n",
      "\n",
      "Iteration 597, loss = 0.31910799\n",
      "Iteration 576, loss = 0.32764618\n",
      "Iteration 252, loss = 0.39793858\n",
      "Iteration 577, loss = 0.32761676\n",
      "Iteration 1891, loss = 0.12265408\n",
      "Iteration 598, loss = 0.31895246\n",
      "Iteration 1535, loss = 0.23866007\n",
      "Iteration 578, loss = 0.32746888\n",
      "Iteration 2218, loss = 0.16326606\n",
      "Iteration 1892, loss = 0.12265095\n",
      "Iteration 599, loss = 0.31886109Iteration 579, loss = 0.32740881\n",
      "Iteration 550, loss = 0.37560871\n",
      "\n",
      "Iteration 253, loss = 0.39764987\n",
      "Iteration 600, loss = 0.31866445\n",
      "Iteration 1536, loss = 0.23862507\n",
      "Iteration 601, loss = 0.31854410\n",
      "Iteration 602, loss = 0.31839473\n",
      "Iteration 580, loss = 0.32728528\n",
      "Iteration 2219, loss = 0.16318140\n",
      "Iteration 1893, loss = 0.12256297\n",
      "Iteration 603, loss = 0.31827123\n",
      "Iteration 604, loss = 0.31809941\n",
      "Iteration 1497, loss = 0.18426790\n",
      "Iteration 254, loss = 0.39738624\n",
      "Iteration 581, loss = 0.32719269\n",
      "Iteration 605, loss = 0.31800047\n",
      "Iteration 551, loss = 0.37552845\n",
      "Iteration 1894, loss = 0.12243140Iteration 606, loss = 0.31781773\n",
      "\n",
      "Iteration 2220, loss = 0.16312703\n",
      "Iteration 607, loss = 0.31765979\n",
      "Iteration 608, loss = 0.31753616\n",
      "Iteration 582, loss = 0.32707240\n",
      "Iteration 609, loss = 0.31737828\n",
      "Iteration 1498, loss = 0.18405386\n",
      "Iteration 610, loss = 0.31731781\n",
      "Iteration 255, loss = 0.39718714\n",
      "Iteration 2221, loss = 0.16304217\n",
      "Iteration 1895, loss = 0.12230742\n",
      "Iteration 583, loss = 0.32699500\n",
      "Iteration 1537, loss = 0.23833387\n",
      "Iteration 1896, loss = 0.12247923\n",
      "Iteration 256, loss = 0.39689239\n",
      "Iteration 1499, loss = 0.18389114\n",
      "Iteration 552, loss = 0.37529932\n",
      "Iteration 2222, loss = 0.16299701\n",
      "Iteration 584, loss = 0.32688372\n",
      "Iteration 1538, loss = 0.23828491\n",
      "Iteration 611, loss = 0.31710305\n",
      "Iteration 612, loss = 0.31703771\n",
      "Iteration 613, loss = 0.31688875\n",
      "Iteration 1897, loss = 0.12214287\n",
      "Iteration 614, loss = 0.31671370\n",
      "Iteration 1500, loss = 0.18377516\n",
      "Iteration 1539, loss = 0.23811458\n",
      "Iteration 615, loss = 0.31660858\n",
      "Iteration 553, loss = 0.37500718\n",
      "Iteration 1898, loss = 0.12206236\n",
      "Iteration 1540, loss = 0.23806302\n",
      "Iteration 2223, loss = 0.16287188\n",
      "Iteration 616, loss = 0.31638711\n",
      "Iteration 585, loss = 0.32677898\n",
      "Iteration 554, loss = 0.37485124\n",
      "Iteration 617, loss = 0.31625477\n",
      "Iteration 1899, loss = 0.12205092\n",
      "Iteration 618, loss = 0.31611157\n",
      "Iteration 586, loss = 0.32668801\n",
      "Iteration 1501, loss = 0.18378472\n",
      "Iteration 619, loss = 0.31595438\n",
      "Iteration 1541, loss = 0.23788272\n",
      "Iteration 587, loss = 0.32659144\n",
      "Iteration 1900, loss = 0.12194086\n",
      "Iteration 2224, loss = 0.16286144\n",
      "Iteration 588, loss = 0.32648724\n",
      "Iteration 620, loss = 0.31580813\n",
      "Iteration 589, loss = 0.32638979\n",
      "Iteration 1542, loss = 0.23777695\n",
      "Iteration 621, loss = 0.31565118\n",
      "Iteration 590, loss = 0.32629430\n",
      "Iteration 1502, loss = 0.18352350\n",
      "Iteration 1901, loss = 0.12183387\n",
      "Iteration 622, loss = 0.31550181\n",
      "Iteration 555, loss = 0.37472134\n",
      "Iteration 2225, loss = 0.16275261\n",
      "Iteration 623, loss = 0.31542482\n",
      "Iteration 591, loss = 0.32619459\n",
      "Iteration 1902, loss = 0.12175267\n",
      "Iteration 1543, loss = 0.23764080\n",
      "Iteration 624, loss = 0.31523087\n",
      "Iteration 556, loss = 0.37442550\n",
      "Iteration 592, loss = 0.32608518\n",
      "Iteration 1903, loss = 0.12168049\n",
      "Iteration 1503, loss = 0.18339001\n",
      "Iteration 2226, loss = 0.16270005\n",
      "Iteration 625, loss = 0.31511672\n",
      "Iteration 593, loss = 0.32598521\n",
      "Iteration 626, loss = 0.31494381\n",
      "Iteration 594, loss = 0.32590597\n",
      "Iteration 1904, loss = 0.12166712\n",
      "Iteration 257, loss = 0.39664165\n",
      "Iteration 2227, loss = 0.16253071\n",
      "Iteration 1544, loss = 0.23748913\n",
      "Iteration 627, loss = 0.31482287Iteration 1504, loss = 0.18324410\n",
      "Iteration 595, loss = 0.32579696\n",
      "Iteration 258, loss = 0.39637896\n",
      "Iteration 1905, loss = 0.12165808\n",
      "\n",
      "Iteration 2228, loss = 0.16244441\n",
      "Iteration 557, loss = 0.37423674\n",
      "Iteration 1545, loss = 0.23734089\n",
      "Iteration 1505, loss = 0.18316825\n",
      "Iteration 628, loss = 0.31473344\n",
      "Iteration 1906, loss = 0.12149520\n",
      "Iteration 629, loss = 0.31451229\n",
      "Iteration 596, loss = 0.32572472\n",
      "Iteration 630, loss = 0.31435754\n",
      "Iteration 631, loss = 0.31422560\n",
      "Iteration 1907, loss = 0.12135755\n",
      "Iteration 259, loss = 0.39615799\n",
      "Iteration 632, loss = 0.31416935\n",
      "Iteration 2229, loss = 0.16240029\n",
      "Iteration 1908, loss = 0.12131345\n",
      "Iteration 1506, loss = 0.18322781\n",
      "Iteration 1909, loss = 0.12145516\n",
      "Iteration 597, loss = 0.32557918\n",
      "Iteration 2230, loss = 0.16224191\n",
      "Iteration 633, loss = 0.31396127\n",
      "Iteration 1546, loss = 0.23726238\n",
      "Iteration 558, loss = 0.37404380\n",
      "Iteration 634, loss = 0.31380994\n",
      "Iteration 1910, loss = 0.12125580\n",
      "Iteration 2231, loss = 0.16226784\n",
      "Iteration 260, loss = 0.39589743\n",
      "Iteration 635, loss = 0.31369206\n",
      "Iteration 598, loss = 0.32550065\n",
      "Iteration 1507, loss = 0.18286333\n",
      "Iteration 1911, loss = 0.12120370\n",
      "Iteration 559, loss = 0.37395842\n",
      "Iteration 2232, loss = 0.16216816\n",
      "Iteration 1547, loss = 0.23719502\n",
      "Iteration 636, loss = 0.31352036\n",
      "Iteration 599, loss = 0.32541796\n",
      "Iteration 1912, loss = 0.12098019\n",
      "Iteration 637, loss = 0.31337338\n",
      "Iteration 638, loss = 0.31325691\n",
      "Iteration 261, loss = 0.39565078\n",
      "Iteration 1508, loss = 0.18285657\n",
      "Iteration 639, loss = 0.31311561\n",
      "Iteration 600, loss = 0.32528670\n",
      "Iteration 560, loss = 0.37367085\n",
      "Iteration 640, loss = 0.31299661\n",
      "Iteration 1913, loss = 0.12093506\n",
      "Iteration 1548, loss = 0.23699724\n",
      "Iteration 641, loss = 0.31283739\n",
      "Iteration 2233, loss = 0.16202399\n",
      "Iteration 642, loss = 0.31273990\n",
      "Iteration 1914, loss = 0.12084554\n",
      "Iteration 262, loss = 0.39541768\n",
      "Iteration 643, loss = 0.31257527\n",
      "Iteration 644, loss = 0.31243375\n",
      "Iteration 645, loss = 0.31231123\n",
      "Iteration 1509, loss = 0.18265766\n",
      "Iteration 601, loss = 0.32522978\n",
      "Iteration 263, loss = 0.39518680\n",
      "Iteration 646, loss = 0.31219288\n",
      "Iteration 2234, loss = 0.16196521\n",
      "Iteration 647, loss = 0.31206308\n",
      "Iteration 648, loss = 0.31191622\n",
      "Iteration 602, loss = 0.32509062\n",
      "Iteration 1549, loss = 0.23691379\n",
      "Iteration 649, loss = 0.31178440\n",
      "Iteration 2235, loss = 0.16184510\n",
      "Iteration 603, loss = 0.32500280\n",
      "Iteration 264, loss = 0.39493981\n",
      "Iteration 1915, loss = 0.12083658\n",
      "Iteration 561, loss = 0.37343204\n",
      "Iteration 650, loss = 0.31164249\n",
      "Iteration 651, loss = 0.31154254\n",
      "Iteration 604, loss = 0.32490474\n",
      "Iteration 2236, loss = 0.16181035\n",
      "Iteration 1510, loss = 0.18265429\n",
      "Iteration 1550, loss = 0.23675874\n",
      "Iteration 605, loss = 0.32480910\n",
      "Iteration 652, loss = 0.31138423\n",
      "Iteration 265, loss = 0.39471228\n",
      "Iteration 606, loss = 0.32475372\n",
      "Iteration 653, loss = 0.31124044\n",
      "Iteration 654, loss = 0.31110815\n",
      "Iteration 655, loss = 0.31096757Iteration 607, loss = 0.32462324\n",
      "\n",
      "Iteration 562, loss = 0.37327414\n",
      "Iteration 1551, loss = 0.23663244\n",
      "Iteration 266, loss = 0.39449219\n",
      "Iteration 656, loss = 0.31088638\n",
      "Iteration 657, loss = 0.31072544\n",
      "Iteration 2237, loss = 0.16166440\n",
      "Iteration 1916, loss = 0.12081579\n",
      "Iteration 658, loss = 0.31059698\n",
      "Iteration 1511, loss = 0.18239150\n",
      "Iteration 1552, loss = 0.23650496\n",
      "Iteration 659, loss = 0.31043854\n",
      "Iteration 267, loss = 0.39423844\n",
      "Iteration 660, loss = 0.31031022\n",
      "Iteration 2238, loss = 0.16162257\n",
      "Iteration 661, loss = 0.31021534\n",
      "Iteration 1917, loss = 0.12067180\n",
      "Iteration 563, loss = 0.37301414\n",
      "Iteration 608, loss = 0.32450623\n",
      "Iteration 662, loss = 0.31006586\n",
      "Iteration 663, loss = 0.30992064\n",
      "Iteration 268, loss = 0.39399079\n",
      "Iteration 1553, loss = 0.23639044\n",
      "Iteration 1512, loss = 0.18242318\n",
      "Iteration 1918, loss = 0.12076462\n",
      "Iteration 2239, loss = 0.16153317\n",
      "Iteration 609, loss = 0.32442322\n",
      "Iteration 664, loss = 0.30980107\n",
      "Iteration 665, loss = 0.30968376\n",
      "Iteration 564, loss = 0.37284473\n",
      "Iteration 269, loss = 0.39378486\n",
      "Iteration 610, loss = 0.32434704\n",
      "Iteration 611, loss = 0.32420936\n",
      "Iteration 666, loss = 0.30953465\n",
      "Iteration 270, loss = 0.39354295\n",
      "Iteration 2240, loss = 0.16148142\n",
      "Iteration 612, loss = 0.32410789\n",
      "Iteration 1554, loss = 0.23639742\n",
      "Iteration 667, loss = 0.30938952\n",
      "Iteration 565, loss = 0.37268205\n",
      "Iteration 668, loss = 0.30925966\n",
      "Iteration 613, loss = 0.32400774\n",
      "Iteration 271, loss = 0.39330355\n",
      "Iteration 669, loss = 0.30915986\n",
      "Iteration 2241, loss = 0.16135600\n",
      "Iteration 1919, loss = 0.12055264\n",
      "Iteration 1513, loss = 0.18225810\n",
      "Iteration 614, loss = 0.32391382\n",
      "Iteration 615, loss = 0.32380800\n",
      "Iteration 670, loss = 0.30897020Iteration 272, loss = 0.39310191\n",
      "\n",
      "Iteration 2242, loss = 0.16130770\n",
      "Iteration 616, loss = 0.32372075\n",
      "Iteration 1555, loss = 0.23624220\n",
      "Iteration 671, loss = 0.30890355\n",
      "Iteration 672, loss = 0.30878412\n",
      "Iteration 1920, loss = 0.12037877\n",
      "Iteration 1514, loss = 0.18209281\n",
      "Iteration 673, loss = 0.30861547\n",
      "Iteration 566, loss = 0.37246050\n",
      "Iteration 1556, loss = 0.23601335\n",
      "Iteration 674, loss = 0.30847766\n",
      "Iteration 675, loss = 0.30832009\n",
      "Iteration 617, loss = 0.32363149\n",
      "Iteration 676, loss = 0.30821577\n",
      "Iteration 1921, loss = 0.12030765\n",
      "Iteration 2243, loss = 0.16122262\n",
      "Iteration 618, loss = 0.32356593\n",
      "Iteration 677, loss = 0.30808519\n",
      "Iteration 1922, loss = 0.12031903\n",
      "Iteration 678, loss = 0.30796510\n",
      "Iteration 1557, loss = 0.23591493\n",
      "Iteration 619, loss = 0.32341613\n",
      "Iteration 1515, loss = 0.18200064\n",
      "Iteration 679, loss = 0.30785738\n",
      "Iteration 567, loss = 0.37228919\n",
      "Iteration 620, loss = 0.32332705\n",
      "Iteration 680, loss = 0.30771815\n",
      "Iteration 2244, loss = 0.16110708\n",
      "Iteration 1923, loss = 0.12019258\n",
      "Iteration 681, loss = 0.30767140\n",
      "Iteration 621, loss = 0.32321714\n",
      "Iteration 1924, loss = 0.12008097\n",
      "Iteration 682, loss = 0.30743436\n",
      "Iteration 273, loss = 0.39287960\n",
      "Iteration 622, loss = 0.32314033\n",
      "Iteration 683, loss = 0.30732480\n",
      "Iteration 1516, loss = 0.18189972\n",
      "Iteration 1925, loss = 0.11999266\n",
      "Iteration 684, loss = 0.30717202\n",
      "Iteration 2245, loss = 0.16109525\n",
      "Iteration 623, loss = 0.32303484\n",
      "Iteration 685, loss = 0.30704537\n",
      "Iteration 568, loss = 0.37204562\n",
      "Iteration 624, loss = 0.32293544\n",
      "Iteration 1926, loss = 0.12021974\n",
      "Iteration 274, loss = 0.39265768\n",
      "Iteration 1517, loss = 0.18170115\n",
      "Iteration 2246, loss = 0.16101883\n",
      "Iteration 686, loss = 0.30692183\n",
      "Iteration 625, loss = 0.32283572\n",
      "Iteration 687, loss = 0.30681298\n",
      "Iteration 1927, loss = 0.11984151\n",
      "Iteration 569, loss = 0.37194175\n",
      "Iteration 626, loss = 0.32275826\n",
      "Iteration 688, loss = 0.30666068\n",
      "Iteration 275, loss = 0.39242802\n",
      "Iteration 627, loss = 0.32271059\n",
      "Iteration 689, loss = 0.30664881\n",
      "Iteration 1928, loss = 0.11987069\n",
      "Iteration 2247, loss = 0.16088649\n",
      "Iteration 690, loss = 0.30638055\n",
      "Iteration 628, loss = 0.32254754\n",
      "Iteration 1518, loss = 0.18170994\n",
      "Iteration 691, loss = 0.30634061\n",
      "Iteration 570, loss = 0.37166666\n",
      "Iteration 1929, loss = 0.11977328\n",
      "Iteration 692, loss = 0.30612067\n",
      "Iteration 1558, loss = 0.23581936\n",
      "Iteration 276, loss = 0.39220649\n",
      "Iteration 629, loss = 0.32245353\n",
      "Iteration 693, loss = 0.30597776\n",
      "Iteration 694, loss = 0.30585155\n",
      "Iteration 630, loss = 0.32234708\n",
      "Iteration 1519, loss = 0.18174011\n",
      "Iteration 1559, loss = 0.23564274\n",
      "Iteration 1930, loss = 0.11976760\n",
      "Iteration 695, loss = 0.30572105\n",
      "Iteration 696, loss = 0.30559541\n",
      "Iteration 697, loss = 0.30556394\n",
      "Iteration 277, loss = 0.39199015\n",
      "Iteration 631, loss = 0.32226506\n",
      "Iteration 1931, loss = 0.11960678\n",
      "Iteration 632, loss = 0.32219482\n",
      "Iteration 698, loss = 0.30531170\n",
      "Iteration 278, loss = 0.39176408\n",
      "Iteration 699, loss = 0.30518407\n",
      "Iteration 700, loss = 0.30510074\n",
      "Iteration 633, loss = 0.32212483\n",
      "Iteration 571, loss = 0.37145589\n",
      "Iteration 1932, loss = 0.11954245\n",
      "Iteration 1560, loss = 0.23565077\n",
      "Iteration 2248, loss = 0.16086859\n",
      "Iteration 279, loss = 0.39158130\n",
      "Iteration 1561, loss = 0.23546941\n",
      "Iteration 1520, loss = 0.18141390\n",
      "Iteration 634, loss = 0.32197930\n",
      "Iteration 2249, loss = 0.16071034\n",
      "Iteration 280, loss = 0.39133768\n",
      "Iteration 1933, loss = 0.11945609\n",
      "Iteration 701, loss = 0.30493493\n",
      "Iteration 635, loss = 0.32187223\n",
      "Iteration 702, loss = 0.30479051\n",
      "Iteration 1562, loss = 0.23533643\n",
      "Iteration 1934, loss = 0.11940870\n",
      "Iteration 2250, loss = 0.16067641\n",
      "Iteration 703, loss = 0.30466023\n",
      "Iteration 2251, loss = 0.16063035\n",
      "Iteration 704, loss = 0.30454705\n",
      "Iteration 636, loss = 0.32176978\n",
      "Iteration 1935, loss = 0.11940476\n",
      "Iteration 705, loss = 0.30443422\n",
      "Iteration 1521, loss = 0.18116426\n",
      "Iteration 706, loss = 0.30427710\n",
      "Iteration 281, loss = 0.39114060\n",
      "Iteration 707, loss = 0.30413489\n",
      "Iteration 637, loss = 0.32168628\n",
      "Iteration 572, loss = 0.37125744\n",
      "Iteration 1563, loss = 0.23520297\n",
      "Iteration 1936, loss = 0.11919765\n",
      "Iteration 638, loss = 0.32157291\n",
      "Iteration 708, loss = 0.30400673\n",
      "Iteration 639, loss = 0.32152466\n",
      "Iteration 709, loss = 0.30392488\n",
      "Iteration 1937, loss = 0.11936162\n",
      "Iteration 1522, loss = 0.18100079\n",
      "Iteration 2252, loss = 0.16050471\n",
      "Iteration 710, loss = 0.30374175\n",
      "Iteration 282, loss = 0.39091126\n",
      "Iteration 640, loss = 0.32140732Iteration 573, loss = 0.37107187\n",
      "\n",
      "Iteration 1938, loss = 0.11911894\n",
      "Iteration 1564, loss = 0.23508360\n",
      "Iteration 2253, loss = 0.16042124\n",
      "Iteration 1939, loss = 0.11907218\n",
      "Iteration 1523, loss = 0.18099174\n",
      "Iteration 283, loss = 0.39074079\n",
      "Iteration 2254, loss = 0.16032588\n",
      "Iteration 1940, loss = 0.11897797\n",
      "Iteration 641, loss = 0.32129952\n",
      "Iteration 1941, loss = 0.11900145\n",
      "Iteration 284, loss = 0.39047957\n",
      "Iteration 1524, loss = 0.18089961\n",
      "Iteration 2255, loss = 0.16021029\n",
      "Iteration 711, loss = 0.30362632\n",
      "Iteration 1565, loss = 0.23504565\n",
      "Iteration 712, loss = 0.30347648\n",
      "Iteration 1942, loss = 0.11888111\n",
      "Iteration 713, loss = 0.30335552\n",
      "Iteration 285, loss = 0.39027969\n",
      "Iteration 642, loss = 0.32119710\n",
      "Iteration 714, loss = 0.30321367\n",
      "Iteration 2256, loss = 0.16025328\n",
      "Iteration 715, loss = 0.30308518\n",
      "Iteration 1566, loss = 0.23479976\n",
      "Iteration 716, loss = 0.30295313\n",
      "Iteration 574, loss = 0.37085891\n",
      "Iteration 1525, loss = 0.18068811\n",
      "Iteration 717, loss = 0.30280926\n",
      "Iteration 643, loss = 0.32110318\n",
      "Iteration 1943, loss = 0.11874445\n",
      "Iteration 718, loss = 0.30267718\n",
      "Iteration 286, loss = 0.39007442\n",
      "Iteration 644, loss = 0.32100389\n",
      "Iteration 719, loss = 0.30255123\n",
      "Iteration 720, loss = 0.30243524\n",
      "Iteration 1526, loss = 0.18056234\n",
      "Iteration 1567, loss = 0.23473226\n",
      "Iteration 721, loss = 0.30229430\n",
      "Iteration 287, loss = 0.38988428\n",
      "Iteration 722, loss = 0.30215382\n",
      "Iteration 575, loss = 0.37071260\n",
      "Iteration 723, loss = 0.30203092\n",
      "Iteration 724, loss = 0.30187871\n",
      "Iteration 645, loss = 0.32090873\n",
      "Iteration 1944, loss = 0.11868558\n",
      "Iteration 725, loss = 0.30177860\n",
      "Iteration 1527, loss = 0.18061301\n",
      "Iteration 2257, loss = 0.16015542\n",
      "Iteration 646, loss = 0.32080812\n",
      "Iteration 726, loss = 0.30165878\n",
      "Iteration 288, loss = 0.38964797\n",
      "Iteration 1568, loss = 0.23462426\n",
      "Iteration 727, loss = 0.30154607\n",
      "Iteration 647, loss = 0.32069233\n",
      "Iteration 1528, loss = 0.18031036\n",
      "Iteration 728, loss = 0.30141272\n",
      "Iteration 1945, loss = 0.11859974\n",
      "Iteration 648, loss = 0.32061387\n",
      "Iteration 729, loss = 0.30129268\n",
      "Iteration 576, loss = 0.37046515\n",
      "Iteration 730, loss = 0.30114302\n",
      "Iteration 289, loss = 0.38945496\n",
      "Iteration 731, loss = 0.30102723\n",
      "Iteration 649, loss = 0.32049864\n",
      "Iteration 1529, loss = 0.18026338\n",
      "Iteration 2258, loss = 0.16002294\n",
      "Iteration 1946, loss = 0.11856948\n",
      "Iteration 1569, loss = 0.23444364\n",
      "Iteration 650, loss = 0.32041062\n",
      "Iteration 732, loss = 0.30087185\n",
      "Iteration 1947, loss = 0.11846717\n",
      "Iteration 290, loss = 0.38927284\n",
      "Iteration 577, loss = 0.37027269\n",
      "Iteration 2259, loss = 0.15995829\n",
      "Iteration 651, loss = 0.32033059\n",
      "Iteration 733, loss = 0.30070326\n",
      "Iteration 1948, loss = 0.11845189\n",
      "Iteration 1570, loss = 0.23436591\n",
      "Iteration 734, loss = 0.30057336\n",
      "Iteration 652, loss = 0.32021161\n",
      "Iteration 1530, loss = 0.18010813\n",
      "Iteration 735, loss = 0.30054888\n",
      "Iteration 291, loss = 0.38904285\n",
      "Iteration 1949, loss = 0.11846909\n",
      "Iteration 1571, loss = 0.23421437\n",
      "Iteration 2260, loss = 0.15986640\n",
      "Iteration 653, loss = 0.32015040\n",
      "Iteration 736, loss = 0.30034799\n",
      "Iteration 578, loss = 0.37005266\n",
      "Iteration 292, loss = 0.38883558\n",
      "Iteration 737, loss = 0.30020613\n",
      "Iteration 738, loss = 0.30006314\n",
      "Iteration 1572, loss = 0.23417035\n",
      "Iteration 739, loss = 0.29993177\n",
      "Iteration 654, loss = 0.32002936\n",
      "Iteration 1950, loss = 0.11830539\n",
      "Iteration 2261, loss = 0.15973618\n",
      "Iteration 740, loss = 0.29981582\n",
      "Iteration 1531, loss = 0.18021595\n",
      "Iteration 293, loss = 0.38867816\n",
      "Iteration 1951, loss = 0.11816789\n",
      "Iteration 655, loss = 0.31991720\n",
      "Iteration 741, loss = 0.29968593\n",
      "Iteration 656, loss = 0.31987391\n",
      "Iteration 742, loss = 0.29952663\n",
      "Iteration 2262, loss = 0.15974400\n",
      "Iteration 1573, loss = 0.23400113\n",
      "Iteration 743, loss = 0.29941574\n",
      "Iteration 657, loss = 0.31984785\n",
      "Iteration 744, loss = 0.29934205\n",
      "Iteration 579, loss = 0.36985771\n",
      "Iteration 658, loss = 0.31963706\n",
      "Iteration 2263, loss = 0.15958476\n",
      "Iteration 1574, loss = 0.23384810\n",
      "Iteration 745, loss = 0.29918457\n",
      "Iteration 1532, loss = 0.17996143\n",
      "Iteration 659, loss = 0.31953481\n",
      "Iteration 294, loss = 0.38846605\n",
      "Iteration 660, loss = 0.31948448\n",
      "Iteration 746, loss = 0.29902228\n",
      "Iteration 1575, loss = 0.23376354\n",
      "Iteration 2264, loss = 0.15960824\n",
      "Iteration 1952, loss = 0.11814180\n",
      "Iteration 1953, loss = 0.11807905Iteration 295, loss = 0.38824009\n",
      "\n",
      "Iteration 747, loss = 0.29889663\n",
      "Iteration 580, loss = 0.36970589\n",
      "Iteration 1954, loss = 0.11797163\n",
      "Iteration 748, loss = 0.29882244\n",
      "Iteration 661, loss = 0.31942188\n",
      "Iteration 1533, loss = 0.17972597\n",
      "Iteration 296, loss = 0.38805524\n",
      "Iteration 1955, loss = 0.11815865\n",
      "Iteration 2265, loss = 0.15945140\n",
      "Iteration 662, loss = 0.31924721\n",
      "Iteration 749, loss = 0.29865979\n",
      "Iteration 750, loss = 0.29849908\n",
      "Iteration 1956, loss = 0.11788579\n",
      "Iteration 751, loss = 0.29839064\n",
      "Iteration 752, loss = 0.29828420\n",
      "Iteration 663, loss = 0.31916657\n",
      "Iteration 1576, loss = 0.23363037\n",
      "Iteration 581, loss = 0.36952050\n",
      "Iteration 1534, loss = 0.17963367\n",
      "Iteration 664, loss = 0.31910354\n",
      "Iteration 297, loss = 0.38787266\n",
      "Iteration 1957, loss = 0.11782378\n",
      "Iteration 753, loss = 0.29816222\n",
      "Iteration 665, loss = 0.31895549\n",
      "Iteration 2266, loss = 0.15941533\n",
      "Iteration 1958, loss = 0.11785344\n",
      "Iteration 754, loss = 0.29800729\n",
      "Iteration 1959, loss = 0.11770574\n",
      "Iteration 298, loss = 0.38775206\n",
      "Iteration 666, loss = 0.31884623\n",
      "Iteration 755, loss = 0.29789995\n",
      "Iteration 1577, loss = 0.23352586\n",
      "Iteration 1960, loss = 0.11767314\n",
      "Iteration 756, loss = 0.29777108\n",
      "Iteration 582, loss = 0.36925282\n",
      "Iteration 757, loss = 0.29760739\n",
      "Iteration 667, loss = 0.31876858\n",
      "Iteration 2267, loss = 0.15929002\n",
      "Iteration 758, loss = 0.29751468\n",
      "Iteration 1961, loss = 0.11752563\n",
      "Iteration 299, loss = 0.38746810\n",
      "Iteration 759, loss = 0.29740081\n",
      "Iteration 668, loss = 0.31872562\n",
      "Iteration 1578, loss = 0.23336135\n",
      "Iteration 760, loss = 0.29724090\n",
      "Iteration 1962, loss = 0.11744960\n",
      "Iteration 669, loss = 0.31857838\n",
      "Iteration 761, loss = 0.29713623\n",
      "Iteration 300, loss = 0.38728974\n",
      "Iteration 670, loss = 0.31847385\n",
      "Iteration 762, loss = 0.29698331\n",
      "Iteration 2268, loss = 0.15919710\n",
      "Iteration 583, loss = 0.36904162\n",
      "Iteration 1963, loss = 0.11749218\n",
      "Iteration 1579, loss = 0.23331451\n",
      "Iteration 1535, loss = 0.17945200\n",
      "Iteration 763, loss = 0.29692440\n",
      "Iteration 671, loss = 0.31835760\n",
      "Iteration 1964, loss = 0.11734713\n",
      "Iteration 764, loss = 0.29672882\n",
      "Iteration 2269, loss = 0.15917721\n",
      "Iteration 301, loss = 0.38710051\n",
      "Iteration 672, loss = 0.31830132\n",
      "Iteration 1580, loss = 0.23318498\n",
      "Iteration 765, loss = 0.29665703\n",
      "Iteration 766, loss = 0.29647075\n",
      "Iteration 2270, loss = 0.15904523\n",
      "Iteration 767, loss = 0.29639928\n",
      "Iteration 1581, loss = 0.23300077\n",
      "Iteration 1965, loss = 0.11727433\n",
      "Iteration 302, loss = 0.38689658\n",
      "Iteration 768, loss = 0.29625828\n",
      "Iteration 1536, loss = 0.17939757\n",
      "Iteration 673, loss = 0.31816801\n",
      "Iteration 584, loss = 0.36882235\n",
      "Iteration 2271, loss = 0.15897550\n",
      "Iteration 769, loss = 0.29610865\n",
      "Iteration 674, loss = 0.31809585\n",
      "Iteration 770, loss = 0.29604780\n",
      "Iteration 1966, loss = 0.11717894\n",
      "Iteration 1582, loss = 0.23289993\n",
      "Iteration 675, loss = 0.31797636Iteration 771, loss = 0.29585439\n",
      "\n",
      "Iteration 1967, loss = 0.11715854\n",
      "Iteration 303, loss = 0.38669669\n",
      "Iteration 676, loss = 0.31785833\n",
      "Iteration 772, loss = 0.29571048\n",
      "Iteration 1968, loss = 0.11707662\n",
      "Iteration 585, loss = 0.36862621\n",
      "Iteration 773, loss = 0.29557614\n",
      "Iteration 1583, loss = 0.23279372\n",
      "Iteration 1537, loss = 0.17921310\n",
      "Iteration 677, loss = 0.31779464\n",
      "Iteration 304, loss = 0.38660315\n",
      "Iteration 774, loss = 0.29546272\n",
      "Iteration 1969, loss = 0.11700053\n",
      "Iteration 775, loss = 0.29530059\n",
      "Iteration 776, loss = 0.29517085\n",
      "Iteration 678, loss = 0.31765510\n",
      "Iteration 777, loss = 0.29510479\n",
      "Iteration 305, loss = 0.38635730\n",
      "Iteration 1584, loss = 0.23277068\n",
      "Iteration 778, loss = 0.29490536\n",
      "Iteration 679, loss = 0.31765261\n",
      "Iteration 779, loss = 0.29479115\n",
      "Iteration 680, loss = 0.31745536\n",
      "Iteration 1970, loss = 0.11707082\n",
      "Iteration 780, loss = 0.29468958\n",
      "Iteration 2272, loss = 0.15890413\n",
      "Iteration 586, loss = 0.36845146\n",
      "Iteration 781, loss = 0.29455738\n",
      "Iteration 681, loss = 0.31737349\n",
      "Iteration 782, loss = 0.29439580\n",
      "Iteration 682, loss = 0.31728337\n",
      "Iteration 1538, loss = 0.17911466\n",
      "Iteration 306, loss = 0.38618894\n",
      "Iteration 1585, loss = 0.23255903\n",
      "Iteration 1971, loss = 0.11697568\n",
      "Iteration 2273, loss = 0.15903625\n",
      "Iteration 783, loss = 0.29425487\n",
      "Iteration 683, loss = 0.31716826\n",
      "Iteration 587, loss = 0.36822201\n",
      "Iteration 307, loss = 0.38595868\n",
      "Iteration 1539, loss = 0.17894884\n",
      "Iteration 1972, loss = 0.11681431\n",
      "Iteration 1586, loss = 0.23241177\n",
      "Iteration 2274, loss = 0.15874226\n",
      "Iteration 308, loss = 0.38579307\n",
      "Iteration 684, loss = 0.31712161\n",
      "Iteration 784, loss = 0.29416294\n",
      "Iteration 685, loss = 0.31699711\n",
      "Iteration 588, loss = 0.36805499\n",
      "Iteration 1540, loss = 0.17893742\n",
      "Iteration 1973, loss = 0.11677697\n",
      "Iteration 1587, loss = 0.23232349\n",
      "Iteration 686, loss = 0.31691278\n",
      "Iteration 309, loss = 0.38556985\n",
      "Iteration 785, loss = 0.29404859\n",
      "Iteration 2275, loss = 0.15869669\n",
      "Iteration 687, loss = 0.31678671\n",
      "Iteration 786, loss = 0.29395471\n",
      "Iteration 310, loss = 0.38541918\n",
      "Iteration 787, loss = 0.29374537\n",
      "Iteration 2276, loss = 0.15869698\n",
      "Iteration 688, loss = 0.31667822\n",
      "Iteration 788, loss = 0.29363837\n",
      "Iteration 1541, loss = 0.17871784\n",
      "Iteration 1974, loss = 0.11678766\n",
      "Iteration 689, loss = 0.31657113\n",
      "Iteration 589, loss = 0.36785896\n",
      "Iteration 1588, loss = 0.23216512\n",
      "Iteration 789, loss = 0.29351684\n",
      "Iteration 790, loss = 0.29338107\n",
      "Iteration 690, loss = 0.31652115\n",
      "Iteration 791, loss = 0.29327720\n",
      "Iteration 691, loss = 0.31637912\n",
      "Iteration 792, loss = 0.29308910\n",
      "Iteration 1542, loss = 0.17865075\n",
      "Iteration 1975, loss = 0.11669795\n",
      "Iteration 311, loss = 0.38521157\n",
      "Iteration 2277, loss = 0.15857614Iteration 793, loss = 0.29300779\n",
      "Iteration 1589, loss = 0.23216276\n",
      "\n",
      "Iteration 692, loss = 0.31631180\n",
      "Iteration 794, loss = 0.29282655\n",
      "Iteration 1976, loss = 0.11669890\n",
      "Iteration 590, loss = 0.36770813\n",
      "Iteration 1543, loss = 0.17861123\n",
      "Iteration 795, loss = 0.29271892\n",
      "Iteration 693, loss = 0.31625761\n",
      "Iteration 2278, loss = 0.15850515\n",
      "Iteration 1590, loss = 0.23198341\n",
      "Iteration 796, loss = 0.29256943\n",
      "Iteration 694, loss = 0.31608337\n",
      "Iteration 312, loss = 0.38507949\n",
      "Iteration 1977, loss = 0.11652634\n",
      "Iteration 797, loss = 0.29243754\n",
      "Iteration 695, loss = 0.31601132\n",
      "Iteration 798, loss = 0.29232221\n",
      "Iteration 2279, loss = 0.15845009\n",
      "Iteration 1978, loss = 0.11652255\n",
      "Iteration 696, loss = 0.31588606\n",
      "Iteration 313, loss = 0.38495754\n",
      "Iteration 799, loss = 0.29216674\n",
      "Iteration 697, loss = 0.31584759\n",
      "Iteration 2280, loss = 0.15834828\n",
      "Iteration 591, loss = 0.36743241\n",
      "Iteration 1591, loss = 0.23186624\n",
      "Iteration 800, loss = 0.29204459\n",
      "Iteration 1544, loss = 0.17840465\n",
      "Iteration 801, loss = 0.29191562\n",
      "Iteration 698, loss = 0.31571881\n",
      "Iteration 314, loss = 0.38469921\n",
      "Iteration 1979, loss = 0.11644519\n",
      "Iteration 802, loss = 0.29179106\n",
      "Iteration 803, loss = 0.29175918\n",
      "Iteration 699, loss = 0.31562224\n",
      "Iteration 804, loss = 0.29155718\n",
      "Iteration 1545, loss = 0.17834221\n",
      "Iteration 2281, loss = 0.15819542\n",
      "Iteration 315, loss = 0.38448829\n",
      "Iteration 700, loss = 0.31553611\n",
      "Iteration 805, loss = 0.29140418\n",
      "Iteration 1980, loss = 0.11649985\n",
      "Iteration 1592, loss = 0.23170533\n",
      "Iteration 806, loss = 0.29124472\n",
      "Iteration 701, loss = 0.31538721\n",
      "Iteration 592, loss = 0.36726715\n",
      "Iteration 702, loss = 0.31530311\n",
      "Iteration 2282, loss = 0.15814092\n",
      "Iteration 807, loss = 0.29116530\n",
      "Iteration 1981, loss = 0.11626247\n",
      "Iteration 703, loss = 0.31519033\n",
      "Iteration 808, loss = 0.29098091\n",
      "Iteration 1546, loss = 0.17821127\n",
      "Iteration 704, loss = 0.31509146\n",
      "Iteration 809, loss = 0.29095502\n",
      "Iteration 316, loss = 0.38432325\n",
      "Iteration 1593, loss = 0.23157351\n",
      "Iteration 810, loss = 0.29075076\n",
      "Iteration 2283, loss = 0.15804167\n",
      "Iteration 811, loss = 0.29064713\n",
      "Iteration 1594, loss = 0.23145278\n",
      "Iteration 705, loss = 0.31499024\n",
      "Iteration 317, loss = 0.38413517\n",
      "Iteration 593, loss = 0.36717044\n",
      "Iteration 1547, loss = 0.17822469\n",
      "Iteration 706, loss = 0.31489709\n",
      "Iteration 812, loss = 0.29049945\n",
      "Iteration 1982, loss = 0.11632303\n",
      "Iteration 813, loss = 0.29040255\n",
      "Iteration 707, loss = 0.31480031\n",
      "Iteration 814, loss = 0.29018740\n",
      "Iteration 318, loss = 0.38396580\n",
      "Iteration 815, loss = 0.29008849\n",
      "Iteration 708, loss = 0.31470532\n",
      "Iteration 2284, loss = 0.15804710\n",
      "Iteration 1595, loss = 0.23133773\n",
      "Iteration 709, loss = 0.31459319\n",
      "Iteration 594, loss = 0.36690390\n",
      "Iteration 319, loss = 0.38378730\n",
      "Iteration 816, loss = 0.28998257\n",
      "Iteration 1548, loss = 0.17798291\n",
      "Iteration 1983, loss = 0.11614681\n",
      "Iteration 817, loss = 0.28984429\n",
      "Iteration 710, loss = 0.31450594\n",
      "Iteration 2285, loss = 0.15790481\n",
      "Iteration 320, loss = 0.38363114\n",
      "Iteration 711, loss = 0.31451270\n",
      "Iteration 1596, loss = 0.23133206\n",
      "Iteration 818, loss = 0.28971418\n",
      "Iteration 1549, loss = 0.17787711\n",
      "Iteration 712, loss = 0.31432599\n",
      "Iteration 819, loss = 0.28953706\n",
      "Iteration 321, loss = 0.38346966\n",
      "Iteration 1984, loss = 0.11612283\n",
      "Iteration 820, loss = 0.28939678\n",
      "Iteration 595, loss = 0.36662421\n",
      "Iteration 1597, loss = 0.23115132\n",
      "Iteration 821, loss = 0.28925783\n",
      "Iteration 713, loss = 0.31421915\n",
      "Iteration 1550, loss = 0.17772972\n",
      "Iteration 2286, loss = 0.15778328\n",
      "Iteration 822, loss = 0.28915656\n",
      "Iteration 714, loss = 0.31411388\n",
      "Iteration 322, loss = 0.38325878\n",
      "Iteration 823, loss = 0.28903969\n",
      "Iteration 1985, loss = 0.11599714\n",
      "Iteration 824, loss = 0.28888656\n",
      "Iteration 715, loss = 0.31405099\n",
      "Iteration 2287, loss = 0.15775805\n",
      "Iteration 825, loss = 0.28874379\n",
      "Iteration 1598, loss = 0.23101237\n",
      "Iteration 596, loss = 0.36645916\n",
      "Iteration 323, loss = 0.38309618\n",
      "Iteration 716, loss = 0.31395443\n",
      "Iteration 826, loss = 0.28862217\n",
      "Iteration 1986, loss = 0.11598107\n",
      "Iteration 717, loss = 0.31383633\n",
      "Iteration 827, loss = 0.28848079\n",
      "Iteration 2288, loss = 0.15762431\n",
      "Iteration 1987, loss = 0.11587934\n",
      "Iteration 324, loss = 0.38293147\n",
      "Iteration 1551, loss = 0.17783314\n",
      "Iteration 828, loss = 0.28842222\n",
      "Iteration 1988, loss = 0.11581237\n",
      "Iteration 1599, loss = 0.23086322\n",
      "Iteration 325, loss = 0.38276246\n",
      "Iteration 829, loss = 0.28821994\n",
      "Iteration 2289, loss = 0.15754533\n",
      "Iteration 830, loss = 0.28810950\n",
      "Iteration 1552, loss = 0.17762199\n",
      "Iteration 718, loss = 0.31372341\n",
      "Iteration 831, loss = 0.28795415\n",
      "Iteration 1989, loss = 0.11578944\n",
      "Iteration 597, loss = 0.36629967\n",
      "Iteration 832, loss = 0.28786933\n",
      "Iteration 1600, loss = 0.23076966\n",
      "Iteration 719, loss = 0.31364004\n",
      "Iteration 1553, loss = 0.17745007\n",
      "Iteration 326, loss = 0.38260821\n",
      "Iteration 2290, loss = 0.15754284\n",
      "Iteration 1990, loss = 0.11570423\n",
      "Iteration 833, loss = 0.28773306\n",
      "Iteration 720, loss = 0.31358356\n",
      "Iteration 327, loss = 0.38242083\n",
      "Iteration 834, loss = 0.28764604\n",
      "Iteration 2291, loss = 0.15736819\n",
      "Iteration 1601, loss = 0.23062915\n",
      "Iteration 598, loss = 0.36600039\n",
      "Iteration 721, loss = 0.31342788\n",
      "Iteration 835, loss = 0.28744176\n",
      "Iteration 1991, loss = 0.11565351\n",
      "Iteration 328, loss = 0.38226185\n",
      "Iteration 836, loss = 0.28736855\n",
      "Iteration 1554, loss = 0.17732051\n",
      "Iteration 2292, loss = 0.15732501\n",
      "Iteration 837, loss = 0.28717693\n",
      "Iteration 599, loss = 0.36584286\n",
      "Iteration 838, loss = 0.28710434\n",
      "Iteration 839, loss = 0.28701104\n",
      "Iteration 722, loss = 0.31335386\n",
      "Iteration 1992, loss = 0.11559011\n",
      "Iteration 1555, loss = 0.17728178\n",
      "Iteration 1602, loss = 0.23053388\n",
      "Iteration 329, loss = 0.38210395\n",
      "Iteration 840, loss = 0.28678594\n",
      "Iteration 1993, loss = 0.11555362\n",
      "Iteration 841, loss = 0.28670423\n",
      "Iteration 723, loss = 0.31324793\n",
      "Iteration 2293, loss = 0.15727073\n",
      "Iteration 724, loss = 0.31326639\n",
      "Iteration 600, loss = 0.36563430\n",
      "Iteration 330, loss = 0.38191377\n",
      "Iteration 842, loss = 0.28648844\n",
      "Iteration 2294, loss = 0.15717355\n",
      "Iteration 1603, loss = 0.23039611\n",
      "Iteration 1994, loss = 0.11549912\n",
      "Iteration 1556, loss = 0.17699287\n",
      "Iteration 725, loss = 0.31305913\n",
      "Iteration 843, loss = 0.28643772\n",
      "Iteration 331, loss = 0.38184660\n",
      "Iteration 844, loss = 0.28642371\n",
      "Iteration 1604, loss = 0.23026858\n",
      "Iteration 726, loss = 0.31301388\n",
      "Iteration 332, loss = 0.38157971\n",
      "Iteration 2295, loss = 0.15709364\n",
      "Iteration 845, loss = 0.28612027\n",
      "Iteration 1557, loss = 0.17692261\n",
      "Iteration 1995, loss = 0.11537970\n",
      "Iteration 846, loss = 0.28600999\n",
      "Iteration 727, loss = 0.31284432\n",
      "Iteration 601, loss = 0.36544358\n",
      "Iteration 333, loss = 0.38151605\n",
      "Iteration 1996, loss = 0.11543417\n",
      "Iteration 1558, loss = 0.17679742\n",
      "Iteration 1997, loss = 0.11530291\n",
      "Iteration 334, loss = 0.38125722\n",
      "Iteration 847, loss = 0.28592491\n",
      "Iteration 728, loss = 0.31275864\n",
      "Iteration 1605, loss = 0.23012092\n",
      "Iteration 848, loss = 0.28575760\n",
      "Iteration 602, loss = 0.36522613\n",
      "Iteration 1998, loss = 0.11526827\n",
      "Iteration 849, loss = 0.28557789\n",
      "Iteration 2296, loss = 0.15700140\n",
      "Iteration 729, loss = 0.31270110\n",
      "Iteration 1999, loss = 0.11519924\n",
      "Iteration 730, loss = 0.31257471\n",
      "Iteration 850, loss = 0.28547035\n",
      "Iteration 2297, loss = 0.15695206\n",
      "Iteration 335, loss = 0.38109707\n",
      "Iteration 851, loss = 0.28551518\n",
      "Iteration 1559, loss = 0.17665552\n",
      "Iteration 1606, loss = 0.23001877\n",
      "Iteration 852, loss = 0.28529868\n",
      "Iteration 853, loss = 0.28521066\n",
      "Iteration 603, loss = 0.36503830\n",
      "Iteration 336, loss = 0.38095793\n",
      "Iteration 854, loss = 0.28495623\n",
      "Iteration 855, loss = 0.28499219\n",
      "Iteration 1560, loss = 0.17662677\n",
      "Iteration 856, loss = 0.28465262\n",
      "Iteration 337, loss = 0.38076424\n",
      "Iteration 731, loss = 0.31246826\n",
      "Iteration 2298, loss = 0.15696581\n",
      "Iteration 857, loss = 0.28452761\n",
      "Iteration 2000, loss = 0.11520648\n",
      "Iteration 732, loss = 0.31235941\n",
      "Iteration 1561, loss = 0.17652459\n",
      "Iteration 604, loss = 0.36484056\n",
      "Iteration 2001, loss = 0.11512300\n",
      "Iteration 858, loss = 0.28442366\n",
      "Iteration 859, loss = 0.28428701\n",
      "Iteration 860, loss = 0.28413682\n",
      "Iteration 2002, loss = 0.11504538\n",
      "Iteration 861, loss = 0.28404774\n",
      "Iteration 605, loss = 0.36461396\n",
      "Iteration 1607, loss = 0.22996862\n",
      "Iteration 733, loss = 0.31226912\n",
      "Iteration 2299, loss = 0.15679873\n",
      "Iteration 862, loss = 0.28385937\n",
      "Iteration 863, loss = 0.28376161\n",
      "Iteration 1562, loss = 0.17643874\n",
      "Iteration 864, loss = 0.28361723\n",
      "Iteration 865, loss = 0.28352011\n",
      "Iteration 338, loss = 0.38063586\n",
      "Iteration 866, loss = 0.28332521\n",
      "Iteration 734, loss = 0.31216746\n",
      "Iteration 867, loss = 0.28320323\n",
      "Iteration 868, loss = 0.28309814\n",
      "Iteration 2003, loss = 0.11500869\n",
      "Iteration 735, loss = 0.31205784\n",
      "Iteration 869, loss = 0.28293455\n",
      "Iteration 1608, loss = 0.22976885\n",
      "Iteration 339, loss = 0.38044294\n",
      "Iteration 606, loss = 0.36444404\n",
      "Iteration 2300, loss = 0.15670391\n",
      "Iteration 736, loss = 0.31203998\n",
      "Iteration 870, loss = 0.28288907\n",
      "Iteration 871, loss = 0.28265237\n",
      "Iteration 340, loss = 0.38029091\n",
      "Iteration 1563, loss = 0.17623309\n",
      "Iteration 872, loss = 0.28253775\n",
      "Iteration 2004, loss = 0.11489254\n",
      "Iteration 737, loss = 0.31186069\n",
      "Iteration 1609, loss = 0.22973993\n",
      "Iteration 2301, loss = 0.15661640\n",
      "Iteration 738, loss = 0.31179599\n",
      "Iteration 873, loss = 0.28241725\n",
      "Iteration 2005, loss = 0.11491819\n",
      "Iteration 874, loss = 0.28230306\n",
      "Iteration 607, loss = 0.36431379\n",
      "Iteration 1564, loss = 0.17608899\n",
      "Iteration 341, loss = 0.38013169\n",
      "Iteration 875, loss = 0.28211503\n",
      "Iteration 739, loss = 0.31166246\n",
      "Iteration 1610, loss = 0.22961370\n",
      "Iteration 2302, loss = 0.15660338\n",
      "Iteration 876, loss = 0.28202185\n",
      "Iteration 2006, loss = 0.11497316\n",
      "Iteration 342, loss = 0.37997102\n",
      "Iteration 740, loss = 0.31157947\n",
      "Iteration 1611, loss = 0.22945894\n",
      "Iteration 2303, loss = 0.15646838\n",
      "Iteration 741, loss = 0.31154024\n",
      "Iteration 2007, loss = 0.11474052\n",
      "Iteration 877, loss = 0.28191431\n",
      "Iteration 343, loss = 0.37983298\n",
      "Iteration 742, loss = 0.31137163\n",
      "Iteration 1612, loss = 0.22935950\n",
      "Iteration 2008, loss = 0.11483071\n",
      "Iteration 743, loss = 0.31128982\n",
      "Iteration 2304, loss = 0.15646264\n",
      "Iteration 344, loss = 0.37963267\n",
      "Iteration 744, loss = 0.31120012\n",
      "Iteration 1613, loss = 0.22933810\n",
      "Iteration 878, loss = 0.28171813\n",
      "Iteration 1565, loss = 0.17606177\n",
      "Iteration 2009, loss = 0.11463151\n",
      "Iteration 879, loss = 0.28159097\n",
      "Iteration 2305, loss = 0.15632300\n",
      "Iteration 745, loss = 0.31109389\n",
      "Iteration 608, loss = 0.36417759\n",
      "Iteration 2010, loss = 0.11452660\n",
      "Iteration 746, loss = 0.31099466\n",
      "Iteration 1614, loss = 0.22910948\n",
      "Iteration 345, loss = 0.37950581\n",
      "Iteration 880, loss = 0.28157162\n",
      "Iteration 2011, loss = 0.11464433\n",
      "Iteration 747, loss = 0.31090282\n",
      "Iteration 2306, loss = 0.15627501\n",
      "Iteration 881, loss = 0.28138104\n",
      "Iteration 346, loss = 0.37944156\n",
      "Iteration 1615, loss = 0.22894274\n",
      "Iteration 2307, loss = 0.15620688\n",
      "Iteration 748, loss = 0.31079568\n",
      "Iteration 882, loss = 0.28119660\n",
      "Iteration 609, loss = 0.36377972\n",
      "Iteration 1566, loss = 0.17591537\n",
      "Iteration 347, loss = 0.37918386\n",
      "Iteration 883, loss = 0.28117880\n",
      "Iteration 1616, loss = 0.22883708\n",
      "Iteration 884, loss = 0.28094722\n",
      "Iteration 2012, loss = 0.11455146\n",
      "Iteration 2308, loss = 0.15613923\n",
      "Iteration 885, loss = 0.28078458\n",
      "Iteration 749, loss = 0.31070927\n",
      "Iteration 2013, loss = 0.11451458\n",
      "Iteration 750, loss = 0.31063725\n",
      "Iteration 2309, loss = 0.15607973\n",
      "Iteration 886, loss = 0.28066470\n",
      "Iteration 2014, loss = 0.11430705\n",
      "Iteration 610, loss = 0.36358185\n",
      "Iteration 751, loss = 0.31054249\n",
      "Iteration 348, loss = 0.37907217\n",
      "Iteration 2310, loss = 0.15608238\n",
      "Iteration 2015, loss = 0.11428598\n",
      "Iteration 887, loss = 0.28053493\n",
      "Iteration 1567, loss = 0.17580921\n",
      "Iteration 752, loss = 0.31041824\n",
      "Iteration 1617, loss = 0.22874144\n",
      "Iteration 2016, loss = 0.11419818\n",
      "Iteration 888, loss = 0.28041505\n",
      "Iteration 2311, loss = 0.15590478\n",
      "Iteration 611, loss = 0.36340027\n",
      "Iteration 753, loss = 0.31030671\n",
      "Iteration 889, loss = 0.28030011\n",
      "Iteration 349, loss = 0.37887622\n",
      "Iteration 1618, loss = 0.22861181\n",
      "Iteration 890, loss = 0.28015781\n",
      "Iteration 754, loss = 0.31020901\n",
      "Iteration 891, loss = 0.28002220\n",
      "Iteration 892, loss = 0.27990381\n",
      "Iteration 2017, loss = 0.11415189\n",
      "Iteration 755, loss = 0.31012448\n",
      "Iteration 1568, loss = 0.17594652\n",
      "Iteration 893, loss = 0.27973193\n",
      "Iteration 1619, loss = 0.22852109\n",
      "Iteration 350, loss = 0.37872572\n",
      "Iteration 894, loss = 0.27959142\n",
      "Iteration 756, loss = 0.31000563\n",
      "Iteration 895, loss = 0.27953011\n",
      "Iteration 2312, loss = 0.15580348\n",
      "Iteration 896, loss = 0.27932667\n",
      "Iteration 2018, loss = 0.11413854\n",
      "Iteration 1569, loss = 0.17561340\n",
      "Iteration 612, loss = 0.36318251\n",
      "Iteration 757, loss = 0.30992995\n",
      "Iteration 1620, loss = 0.22836539\n",
      "Iteration 758, loss = 0.30993725\n",
      "Iteration 897, loss = 0.27917053\n",
      "Iteration 2313, loss = 0.15574714\n",
      "Iteration 759, loss = 0.30978937\n",
      "Iteration 351, loss = 0.37856635\n",
      "Iteration 1570, loss = 0.17554805\n",
      "Iteration 1621, loss = 0.22824942\n",
      "Iteration 760, loss = 0.30963889\n",
      "Iteration 2019, loss = 0.11404573\n",
      "Iteration 898, loss = 0.27909059\n",
      "Iteration 2314, loss = 0.15567725\n",
      "Iteration 761, loss = 0.30956808\n",
      "Iteration 1622, loss = 0.22811930\n",
      "Iteration 2020, loss = 0.11397629\n",
      "Iteration 899, loss = 0.27895032\n",
      "Iteration 1571, loss = 0.17534054\n",
      "Iteration 2315, loss = 0.15554849\n",
      "Iteration 2021, loss = 0.11401279\n",
      "Iteration 613, loss = 0.36298287\n",
      "Iteration 900, loss = 0.27878486\n",
      "Iteration 762, loss = 0.30941703\n",
      "Iteration 1623, loss = 0.22803017\n",
      "Iteration 901, loss = 0.27871249\n",
      "Iteration 2022, loss = 0.11392963\n",
      "Iteration 902, loss = 0.27850736\n",
      "Iteration 352, loss = 0.37845938\n",
      "Iteration 1572, loss = 0.17526352\n",
      "Iteration 763, loss = 0.30933157\n",
      "Iteration 2023, loss = 0.11384279\n",
      "Iteration 903, loss = 0.27835557\n",
      "Iteration 1624, loss = 0.22789017\n",
      "Iteration 764, loss = 0.30924234\n",
      "Iteration 2024, loss = 0.11376082\n",
      "Iteration 2316, loss = 0.15554283\n",
      "Iteration 904, loss = 0.27828329\n",
      "Iteration 905, loss = 0.27820508\n",
      "Iteration 353, loss = 0.37828956\n",
      "Iteration 614, loss = 0.36274690\n",
      "Iteration 765, loss = 0.30913844\n",
      "Iteration 906, loss = 0.27794824\n",
      "Iteration 907, loss = 0.27782671\n",
      "Iteration 766, loss = 0.30903429\n",
      "Iteration 908, loss = 0.27768442\n",
      "Iteration 1625, loss = 0.22775645\n",
      "Iteration 2025, loss = 0.11380787\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 909, loss = 0.27756371\n",
      "Iteration 767, loss = 0.30896130\n",
      "Iteration 2317, loss = 0.15545455\n",
      "Iteration 910, loss = 0.27747220\n",
      "Iteration 911, loss = 0.27734367\n",
      "Iteration 354, loss = 0.37810452\n",
      "Iteration 912, loss = 0.27715891\n",
      "Iteration 1626, loss = 0.22766615\n",
      "Iteration 1573, loss = 0.17511202\n",
      "Iteration 615, loss = 0.36263307\n",
      "Iteration 768, loss = 0.30888513\n",
      "Iteration 913, loss = 0.27718125\n",
      "Iteration 914, loss = 0.27690090\n",
      "Iteration 915, loss = 0.27683961\n",
      "Iteration 916, loss = 0.27663123\n",
      "Iteration 769, loss = 0.30880179\n",
      "Iteration 2318, loss = 0.15534327\n",
      "Iteration 917, loss = 0.27646238\n",
      "Iteration 355, loss = 0.37793888\n",
      "Iteration 918, loss = 0.27634681\n",
      "Iteration 1, loss = 0.78183973\n",
      "Iteration 919, loss = 0.27624391\n",
      "Iteration 1574, loss = 0.17517537\n",
      "Iteration 770, loss = 0.30866066\n",
      "Iteration 920, loss = 0.27608535\n",
      "Iteration 616, loss = 0.36246007\n",
      "Iteration 1627, loss = 0.22758184\n",
      "Iteration 921, loss = 0.27597237\n",
      "Iteration 922, loss = 0.27582564\n",
      "Iteration 2319, loss = 0.15527310\n",
      "Iteration 771, loss = 0.30856922\n",
      "Iteration 2, loss = 0.78048434\n",
      "Iteration 356, loss = 0.37783230\n",
      "Iteration 617, loss = 0.36215625\n",
      "Iteration 923, loss = 0.27570366\n",
      "Iteration 3, loss = 0.77846626\n",
      "Iteration 772, loss = 0.30847170\n",
      "Iteration 1575, loss = 0.17497017\n",
      "Iteration 2320, loss = 0.15522833\n",
      "Iteration 924, loss = 0.27559312\n",
      "Iteration 1628, loss = 0.22739374Iteration 357, loss = 0.37763207\n",
      "Iteration 773, loss = 0.30836465\n",
      "Iteration 4, loss = 0.77603009\n",
      "Iteration 925, loss = 0.27541967\n",
      "\n",
      "Iteration 774, loss = 0.30826586\n",
      "Iteration 926, loss = 0.27525408\n",
      "Iteration 1576, loss = 0.17485357\n",
      "Iteration 1629, loss = 0.22744543\n",
      "Iteration 775, loss = 0.30822827\n",
      "Iteration 2321, loss = 0.15515213\n",
      "Iteration 5, loss = 0.77318412\n",
      "Iteration 358, loss = 0.37754889\n",
      "Iteration 927, loss = 0.27515882\n",
      "Iteration 618, loss = 0.36204095\n",
      "Iteration 776, loss = 0.30807032\n",
      "Iteration 928, loss = 0.27500018\n",
      "Iteration 777, loss = 0.30798383\n",
      "Iteration 929, loss = 0.27487771\n",
      "Iteration 359, loss = 0.37734744\n",
      "Iteration 930, loss = 0.27471682\n",
      "Iteration 2322, loss = 0.15514821\n",
      "Iteration 1577, loss = 0.17475197\n",
      "Iteration 778, loss = 0.30790090\n",
      "Iteration 6, loss = 0.77020698\n",
      "Iteration 931, loss = 0.27465360\n",
      "Iteration 360, loss = 0.37720875\n",
      "Iteration 619, loss = 0.36188047\n",
      "Iteration 7, loss = 0.76722904\n",
      "Iteration 932, loss = 0.27446027\n",
      "Iteration 779, loss = 0.30779869\n",
      "Iteration 1630, loss = 0.22718868\n",
      "Iteration 933, loss = 0.27431818\n",
      "Iteration 8, loss = 0.76412176\n",
      "Iteration 2323, loss = 0.15496034\n",
      "Iteration 780, loss = 0.30768646\n",
      "Iteration 934, loss = 0.27417135\n",
      "Iteration 1631, loss = 0.22707044\n",
      "Iteration 1578, loss = 0.17459892\n",
      "Iteration 935, loss = 0.27412696\n",
      "Iteration 781, loss = 0.30760221\n",
      "Iteration 361, loss = 0.37705985\n",
      "Iteration 9, loss = 0.76107306\n",
      "Iteration 936, loss = 0.27393662\n",
      "Iteration 782, loss = 0.30749060\n",
      "Iteration 620, loss = 0.36153543\n",
      "Iteration 2324, loss = 0.15495216\n",
      "Iteration 937, loss = 0.27379325\n",
      "Iteration 10, loss = 0.75802990\n",
      "Iteration 783, loss = 0.30741069\n",
      "Iteration 1579, loss = 0.17441560\n",
      "Iteration 938, loss = 0.27362188\n",
      "Iteration 939, loss = 0.27350533\n",
      "Iteration 1632, loss = 0.22696795\n",
      "Iteration 362, loss = 0.37690192\n",
      "Iteration 11, loss = 0.75509958\n",
      "Iteration 940, loss = 0.27340130\n",
      "Iteration 2325, loss = 0.15482680\n",
      "Iteration 941, loss = 0.27323354\n",
      "Iteration 12, loss = 0.75218993\n",
      "Iteration 621, loss = 0.36133802\n",
      "Iteration 942, loss = 0.27306761\n",
      "Iteration 943, loss = 0.27299951\n",
      "Iteration 784, loss = 0.30730940\n",
      "Iteration 2326, loss = 0.15470869\n",
      "Iteration 13, loss = 0.74932050\n",
      "Iteration 944, loss = 0.27281503\n",
      "Iteration 945, loss = 0.27271019\n",
      "Iteration 363, loss = 0.37675906\n",
      "Iteration 1580, loss = 0.17464000\n",
      "Iteration 946, loss = 0.27257518\n",
      "Iteration 622, loss = 0.36115919\n",
      "Iteration 14, loss = 0.74657624\n",
      "Iteration 1633, loss = 0.22684499\n",
      "Iteration 785, loss = 0.30722671\n",
      "Iteration 947, loss = 0.27243645\n",
      "Iteration 15, loss = 0.74385429\n",
      "Iteration 786, loss = 0.30710918\n",
      "Iteration 948, loss = 0.27226283\n",
      "Iteration 2327, loss = 0.15464991\n",
      "Iteration 949, loss = 0.27218979\n",
      "Iteration 787, loss = 0.30706476\n",
      "Iteration 950, loss = 0.27196881\n",
      "Iteration 16, loss = 0.74116724\n",
      "Iteration 364, loss = 0.37664205\n",
      "Iteration 951, loss = 0.27190806\n",
      "Iteration 952, loss = 0.27175925\n",
      "Iteration 788, loss = 0.30692518\n",
      "Iteration 17, loss = 0.73845444\n",
      "Iteration 2328, loss = 0.15461509\n",
      "Iteration 953, loss = 0.27202691\n",
      "Iteration 1634, loss = 0.22675541\n",
      "Iteration 954, loss = 0.27163032\n",
      "Iteration 789, loss = 0.30682213\n",
      "Iteration 1581, loss = 0.17429988\n",
      "Iteration 955, loss = 0.27138461\n",
      "Iteration 2329, loss = 0.15448640\n",
      "Iteration 956, loss = 0.27128259\n",
      "Iteration 18, loss = 0.73589963\n",
      "Iteration 623, loss = 0.36095684\n",
      "Iteration 957, loss = 0.27110087\n",
      "Iteration 365, loss = 0.37649835\n",
      "Iteration 1635, loss = 0.22658193\n",
      "Iteration 2330, loss = 0.15443961\n",
      "Iteration 958, loss = 0.27089725\n",
      "Iteration 790, loss = 0.30677950\n",
      "Iteration 19, loss = 0.73328771\n",
      "Iteration 959, loss = 0.27075230\n",
      "Iteration 960, loss = 0.27067295\n",
      "Iteration 624, loss = 0.36072251\n",
      "Iteration 961, loss = 0.27051508\n",
      "Iteration 2331, loss = 0.15439637\n",
      "Iteration 1582, loss = 0.17418091\n",
      "Iteration 962, loss = 0.27049475\n",
      "Iteration 1636, loss = 0.22651001\n",
      "Iteration 963, loss = 0.27025132\n",
      "Iteration 366, loss = 0.37634524\n",
      "Iteration 964, loss = 0.27012230\n",
      "Iteration 791, loss = 0.30661599\n",
      "Iteration 965, loss = 0.27000787\n",
      "Iteration 20, loss = 0.73078757\n",
      "Iteration 625, loss = 0.36060013\n",
      "Iteration 966, loss = 0.26996469\n",
      "Iteration 2332, loss = 0.15433390\n",
      "Iteration 967, loss = 0.26983061\n",
      "Iteration 792, loss = 0.30653452\n",
      "Iteration 968, loss = 0.26955322\n",
      "Iteration 1637, loss = 0.22640046\n",
      "Iteration 367, loss = 0.37619045\n",
      "Iteration 1583, loss = 0.17405285\n",
      "Iteration 969, loss = 0.26946604\n",
      "Iteration 793, loss = 0.30645135\n",
      "Iteration 970, loss = 0.26927763\n",
      "Iteration 794, loss = 0.30634232\n",
      "Iteration 21, loss = 0.72814658\n",
      "Iteration 2333, loss = 0.15419693\n",
      "Iteration 626, loss = 0.36034242\n",
      "Iteration 971, loss = 0.26916247\n",
      "Iteration 368, loss = 0.37602822\n",
      "Iteration 972, loss = 0.26902527\n",
      "Iteration 795, loss = 0.30625615\n",
      "Iteration 22, loss = 0.72568551\n",
      "Iteration 973, loss = 0.26895050Iteration 1638, loss = 0.22625752\n",
      "\n",
      "Iteration 2334, loss = 0.15414572\n",
      "Iteration 627, loss = 0.36012094\n",
      "Iteration 1584, loss = 0.17394296\n",
      "Iteration 974, loss = 0.26880192\n",
      "Iteration 975, loss = 0.26861443\n",
      "Iteration 369, loss = 0.37586618\n",
      "Iteration 1639, loss = 0.22622858\n",
      "Iteration 796, loss = 0.30614859\n",
      "Iteration 2335, loss = 0.15409885\n",
      "Iteration 797, loss = 0.30604698\n",
      "Iteration 798, loss = 0.30595383\n",
      "Iteration 370, loss = 0.37571792\n",
      "Iteration 2336, loss = 0.15403167\n",
      "Iteration 1585, loss = 0.17383832\n",
      "Iteration 799, loss = 0.30589540\n",
      "Iteration 800, loss = 0.30575098\n",
      "Iteration 2337, loss = 0.15392943\n",
      "Iteration 371, loss = 0.37558569\n",
      "Iteration 801, loss = 0.30564123\n",
      "Iteration 628, loss = 0.35993151\n",
      "Iteration 802, loss = 0.30556192\n",
      "Iteration 1586, loss = 0.17367232\n",
      "Iteration 372, loss = 0.37544284\n",
      "Iteration 2338, loss = 0.15384714\n",
      "Iteration 803, loss = 0.30547973\n",
      "Iteration 23, loss = 0.72323580\n",
      "Iteration 976, loss = 0.26845901\n",
      "Iteration 977, loss = 0.26835883\n",
      "Iteration 1640, loss = 0.22602869\n",
      "Iteration 24, loss = 0.72087533\n",
      "Iteration 978, loss = 0.26818868\n",
      "Iteration 804, loss = 0.30538717\n",
      "Iteration 1587, loss = 0.17358962\n",
      "Iteration 25, loss = 0.71829571\n",
      "Iteration 979, loss = 0.26814923\n",
      "Iteration 1641, loss = 0.22593937\n",
      "Iteration 980, loss = 0.26791188\n",
      "Iteration 373, loss = 0.37529293\n",
      "Iteration 981, loss = 0.26779985\n",
      "Iteration 26, loss = 0.71595709\n",
      "Iteration 982, loss = 0.26766725\n",
      "Iteration 1642, loss = 0.22574566\n",
      "Iteration 983, loss = 0.26753798\n",
      "Iteration 374, loss = 0.37516827\n",
      "Iteration 984, loss = 0.26738386\n",
      "Iteration 27, loss = 0.71346132\n",
      "Iteration 805, loss = 0.30527653\n",
      "Iteration 629, loss = 0.35973640\n",
      "Iteration 985, loss = 0.26724999\n",
      "Iteration 806, loss = 0.30519408\n",
      "Iteration 2339, loss = 0.15385595\n",
      "Iteration 28, loss = 0.71104399\n",
      "Iteration 986, loss = 0.26718642\n",
      "Iteration 1643, loss = 0.22562960\n",
      "Iteration 987, loss = 0.26699548\n",
      "Iteration 29, loss = 0.70872848\n",
      "Iteration 2340, loss = 0.15368048\n",
      "Iteration 1588, loss = 0.17345937\n",
      "Iteration 988, loss = 0.26704357\n",
      "Iteration 1644, loss = 0.22555368\n",
      "Iteration 630, loss = 0.35947593\n",
      "Iteration 375, loss = 0.37501292\n",
      "Iteration 30, loss = 0.70624623\n",
      "Iteration 989, loss = 0.26670773\n",
      "Iteration 1645, loss = 0.22542394\n",
      "Iteration 1589, loss = 0.17338865\n",
      "Iteration 990, loss = 0.26660481\n",
      "Iteration 631, loss = 0.35929926\n",
      "Iteration 376, loss = 0.37487473\n",
      "Iteration 807, loss = 0.30508177\n",
      "Iteration 991, loss = 0.26643532\n",
      "Iteration 1590, loss = 0.17326436\n",
      "Iteration 992, loss = 0.26640054\n",
      "Iteration 1646, loss = 0.22527216\n",
      "Iteration 31, loss = 0.70379836\n",
      "Iteration 993, loss = 0.26620306\n",
      "Iteration 994, loss = 0.26602736\n",
      "Iteration 808, loss = 0.30497048\n",
      "Iteration 2341, loss = 0.15363970\n",
      "Iteration 377, loss = 0.37475312\n",
      "Iteration 1647, loss = 0.22535221\n",
      "Iteration 32, loss = 0.70139050\n",
      "Iteration 1591, loss = 0.17313193\n",
      "Iteration 2342, loss = 0.15358082\n",
      "Iteration 995, loss = 0.26588821\n",
      "Iteration 996, loss = 0.26578176\n",
      "Iteration 33, loss = 0.69893306\n",
      "Iteration 997, loss = 0.26581757\n",
      "Iteration 998, loss = 0.26552978\n",
      "Iteration 378, loss = 0.37456818\n",
      "Iteration 809, loss = 0.30489160\n",
      "Iteration 999, loss = 0.26536457\n",
      "Iteration 2343, loss = 0.15346820\n",
      "Iteration 1000, loss = 0.26520989\n",
      "Iteration 632, loss = 0.35909015\n",
      "Iteration 810, loss = 0.30479558\n",
      "Iteration 34, loss = 0.69657616\n",
      "Iteration 1001, loss = 0.26516140\n",
      "Iteration 1592, loss = 0.17317037\n",
      "Iteration 1002, loss = 0.26501661\n",
      "Iteration 811, loss = 0.30467668\n",
      "Iteration 1003, loss = 0.26489532\n",
      "Iteration 1004, loss = 0.26472479\n",
      "Iteration 35, loss = 0.69403001\n",
      "Iteration 812, loss = 0.30458773\n",
      "Iteration 1005, loss = 0.26455821\n",
      "Iteration 813, loss = 0.30448539\n",
      "Iteration 379, loss = 0.37447021\n",
      "Iteration 1006, loss = 0.26451508\n",
      "Iteration 814, loss = 0.30438090\n",
      "Iteration 36, loss = 0.69154338\n",
      "Iteration 1007, loss = 0.26433561\n",
      "Iteration 2344, loss = 0.15341418\n",
      "Iteration 633, loss = 0.35885205\n",
      "Iteration 815, loss = 0.30430676\n",
      "Iteration 1008, loss = 0.26415195\n",
      "Iteration 380, loss = 0.37432271\n",
      "Iteration 1009, loss = 0.26402166\n",
      "Iteration 816, loss = 0.30420418\n",
      "Iteration 1648, loss = 0.22504947\n",
      "Iteration 37, loss = 0.68904584\n",
      "Iteration 817, loss = 0.30408841\n",
      "Iteration 1010, loss = 0.26386129\n",
      "Iteration 1011, loss = 0.26380085\n",
      "Iteration 1012, loss = 0.26363176\n",
      "Iteration 1593, loss = 0.17309984\n",
      "Iteration 2345, loss = 0.15329725\n",
      "Iteration 1649, loss = 0.22500652\n",
      "Iteration 818, loss = 0.30399702\n",
      "Iteration 1013, loss = 0.26351467\n",
      "Iteration 634, loss = 0.35867271\n",
      "Iteration 38, loss = 0.68664048\n",
      "Iteration 1014, loss = 0.26337792\n",
      "Iteration 1015, loss = 0.26320014\n",
      "Iteration 381, loss = 0.37425415\n",
      "Iteration 1016, loss = 0.26308217\n",
      "Iteration 39, loss = 0.68405713\n",
      "Iteration 819, loss = 0.30391309\n",
      "Iteration 1650, loss = 0.22491767\n",
      "Iteration 1017, loss = 0.26297086\n",
      "Iteration 40, loss = 0.68147349\n",
      "Iteration 2346, loss = 0.15321984\n",
      "Iteration 1594, loss = 0.17279995\n",
      "Iteration 820, loss = 0.30383212\n",
      "Iteration 1018, loss = 0.26282915\n",
      "Iteration 41, loss = 0.67904701\n",
      "Iteration 1019, loss = 0.26269909\n",
      "Iteration 1651, loss = 0.22468728\n",
      "Iteration 2347, loss = 0.15323302\n",
      "Iteration 1020, loss = 0.26259724\n",
      "Iteration 821, loss = 0.30369161\n",
      "Iteration 1595, loss = 0.17276205\n",
      "Iteration 42, loss = 0.67644145\n",
      "Iteration 1021, loss = 0.26238605\n",
      "Iteration 1022, loss = 0.26231835\n",
      "Iteration 822, loss = 0.30359110\n",
      "Iteration 43, loss = 0.67387798\n",
      "Iteration 635, loss = 0.35851342\n",
      "Iteration 1652, loss = 0.22458766\n",
      "Iteration 1023, loss = 0.26215305\n",
      "Iteration 1024, loss = 0.26202273\n",
      "Iteration 823, loss = 0.30352147\n",
      "Iteration 1025, loss = 0.26193838\n",
      "Iteration 2348, loss = 0.15312663\n",
      "Iteration 44, loss = 0.67131550\n",
      "Iteration 1596, loss = 0.17265458\n",
      "Iteration 824, loss = 0.30341447\n",
      "Iteration 1026, loss = 0.26171738\n",
      "Iteration 636, loss = 0.35825683\n",
      "Iteration 1027, loss = 0.26163970\n",
      "Iteration 825, loss = 0.30332242\n",
      "Iteration 45, loss = 0.66873360\n",
      "Iteration 1028, loss = 0.26147058\n",
      "Iteration 1653, loss = 0.22453838\n",
      "Iteration 1029, loss = 0.26139578\n",
      "Iteration 826, loss = 0.30321470\n",
      "Iteration 1030, loss = 0.26121488\n",
      "Iteration 1031, loss = 0.26106396\n",
      "Iteration 827, loss = 0.30310606\n",
      "Iteration 1032, loss = 0.26094103\n",
      "Iteration 46, loss = 0.66611528\n",
      "Iteration 1033, loss = 0.26087096\n",
      "Iteration 828, loss = 0.30305931\n",
      "Iteration 1034, loss = 0.26066872\n",
      "Iteration 1597, loss = 0.17251817\n",
      "Iteration 637, loss = 0.35802293\n",
      "Iteration 1654, loss = 0.22432389\n",
      "Iteration 2349, loss = 0.15314129\n",
      "Iteration 47, loss = 0.66351257\n",
      "Iteration 829, loss = 0.30289963\n",
      "Iteration 1035, loss = 0.26053138\n",
      "Iteration 1036, loss = 0.26039380\n",
      "Iteration 830, loss = 0.30282152\n",
      "Iteration 2350, loss = 0.15299916\n",
      "Iteration 48, loss = 0.66086990\n",
      "Iteration 1037, loss = 0.26045596\n",
      "Iteration 638, loss = 0.35785977\n",
      "Iteration 831, loss = 0.30280978\n",
      "Iteration 1038, loss = 0.26012738\n",
      "Iteration 2351, loss = 0.15285336\n",
      "Iteration 1655, loss = 0.22426825\n",
      "Iteration 832, loss = 0.30263189\n",
      "Iteration 49, loss = 0.65816577\n",
      "Iteration 1039, loss = 0.26004381\n",
      "Iteration 1040, loss = 0.25987939\n",
      "Iteration 382, loss = 0.37402970\n",
      "Iteration 2352, loss = 0.15288633\n",
      "Iteration 1041, loss = 0.25976520\n",
      "Iteration 50, loss = 0.65552008\n",
      "Iteration 1042, loss = 0.25964166\n",
      "Iteration 639, loss = 0.35761824\n",
      "Iteration 1656, loss = 0.22416514\n",
      "Iteration 833, loss = 0.30253994\n",
      "Iteration 1043, loss = 0.25945783\n",
      "Iteration 1598, loss = 0.17256688\n",
      "Iteration 51, loss = 0.65284652\n",
      "Iteration 1044, loss = 0.25940160\n",
      "Iteration 834, loss = 0.30244071\n",
      "Iteration 1045, loss = 0.25929066\n",
      "Iteration 1657, loss = 0.22402993\n",
      "Iteration 1046, loss = 0.25909013\n",
      "Iteration 835, loss = 0.30233925\n",
      "Iteration 1047, loss = 0.25895306\n",
      "Iteration 2353, loss = 0.15273347\n",
      "Iteration 52, loss = 0.65016695\n",
      "Iteration 383, loss = 0.37389097\n",
      "Iteration 1599, loss = 0.17227934\n",
      "Iteration 53, loss = 0.64742655\n",
      "Iteration 1048, loss = 0.25880653\n",
      "Iteration 836, loss = 0.30226104\n",
      "Iteration 1049, loss = 0.25871682\n",
      "Iteration 1050, loss = 0.25857002\n",
      "Iteration 1051, loss = 0.25840890\n",
      "Iteration 54, loss = 0.64469314\n",
      "Iteration 1052, loss = 0.25831861\n",
      "Iteration 2354, loss = 0.15270467\n",
      "Iteration 1600, loss = 0.17217720\n",
      "Iteration 640, loss = 0.35743274\n",
      "Iteration 1053, loss = 0.25817605\n",
      "Iteration 1054, loss = 0.25801726\n",
      "Iteration 1658, loss = 0.22402249\n",
      "Iteration 384, loss = 0.37375949\n",
      "Iteration 837, loss = 0.30215731\n",
      "Iteration 1055, loss = 0.25791232\n",
      "Iteration 55, loss = 0.64196170\n",
      "Iteration 2355, loss = 0.15269440\n",
      "Iteration 1659, loss = 0.22372592\n",
      "Iteration 1056, loss = 0.25773448\n",
      "Iteration 838, loss = 0.30204466\n",
      "Iteration 56, loss = 0.63925719\n",
      "Iteration 1057, loss = 0.25768850\n",
      "Iteration 839, loss = 0.30197825\n",
      "Iteration 1058, loss = 0.25748091\n",
      "Iteration 57, loss = 0.63640243\n",
      "Iteration 1660, loss = 0.22372665\n",
      "Iteration 1059, loss = 0.25733606\n",
      "Iteration 840, loss = 0.30186361\n",
      "Iteration 641, loss = 0.35731351\n",
      "Iteration 2356, loss = 0.15251216\n",
      "Iteration 58, loss = 0.63368162\n",
      "Iteration 1060, loss = 0.25721482\n",
      "Iteration 1661, loss = 0.22357253\n",
      "Iteration 59, loss = 0.63086950\n",
      "Iteration 1601, loss = 0.17209242\n",
      "Iteration 385, loss = 0.37359901\n",
      "Iteration 1061, loss = 0.25708615\n",
      "Iteration 841, loss = 0.30174600\n",
      "Iteration 2357, loss = 0.15249251\n",
      "Iteration 1062, loss = 0.25692666\n",
      "Iteration 842, loss = 0.30165646\n",
      "Iteration 642, loss = 0.35699744\n",
      "Iteration 1662, loss = 0.22348565\n",
      "Iteration 1063, loss = 0.25679669\n",
      "Iteration 60, loss = 0.62811807\n",
      "Iteration 843, loss = 0.30155306\n",
      "Iteration 2358, loss = 0.15241893\n",
      "Iteration 386, loss = 0.37346577\n",
      "Iteration 844, loss = 0.30148084\n",
      "Iteration 1064, loss = 0.25717149\n",
      "Iteration 1663, loss = 0.22325400\n",
      "Iteration 1065, loss = 0.25653532\n",
      "Iteration 845, loss = 0.30137233\n",
      "Iteration 643, loss = 0.35681038\n",
      "Iteration 1066, loss = 0.25652633\n",
      "Iteration 1602, loss = 0.17207289\n",
      "Iteration 2359, loss = 0.15227252\n",
      "Iteration 1067, loss = 0.25626305\n",
      "Iteration 846, loss = 0.30127684\n",
      "Iteration 1664, loss = 0.22311758\n",
      "Iteration 1068, loss = 0.25615802\n",
      "Iteration 2360, loss = 0.15219104\n",
      "Iteration 387, loss = 0.37331714\n",
      "Iteration 644, loss = 0.35660366\n",
      "Iteration 1603, loss = 0.17195320\n",
      "Iteration 1069, loss = 0.25601896\n",
      "Iteration 61, loss = 0.62529345\n",
      "Iteration 847, loss = 0.30120086\n",
      "Iteration 388, loss = 0.37323801\n",
      "Iteration 2361, loss = 0.15226883\n",
      "Iteration 848, loss = 0.30106865\n",
      "Iteration 1665, loss = 0.22304064\n",
      "Iteration 1070, loss = 0.25590575\n",
      "Iteration 389, loss = 0.37304141\n",
      "Iteration 1071, loss = 0.25585801\n",
      "Iteration 62, loss = 0.62250815\n",
      "Iteration 1072, loss = 0.25559279\n",
      "Iteration 1073, loss = 0.25553047\n",
      "Iteration 1604, loss = 0.17182467\n",
      "Iteration 645, loss = 0.35638811\n",
      "Iteration 849, loss = 0.30098399\n",
      "Iteration 1074, loss = 0.25532119\n",
      "Iteration 390, loss = 0.37290290\n",
      "Iteration 1075, loss = 0.25520330\n",
      "Iteration 2362, loss = 0.15207414\n",
      "Iteration 1076, loss = 0.25515593\n",
      "Iteration 63, loss = 0.61961509\n",
      "Iteration 1666, loss = 0.22290555\n",
      "Iteration 1077, loss = 0.25506914\n",
      "Iteration 1605, loss = 0.17171231\n",
      "Iteration 850, loss = 0.30095335\n",
      "Iteration 1078, loss = 0.25477878\n",
      "Iteration 1079, loss = 0.25466646\n",
      "Iteration 646, loss = 0.35622398\n",
      "Iteration 1080, loss = 0.25453219\n",
      "Iteration 391, loss = 0.37277225\n",
      "Iteration 851, loss = 0.30080052\n",
      "Iteration 64, loss = 0.61685374\n",
      "Iteration 1081, loss = 0.25440981\n",
      "Iteration 852, loss = 0.30076750\n",
      "Iteration 647, loss = 0.35596631\n",
      "Iteration 1667, loss = 0.22278089\n",
      "Iteration 1082, loss = 0.25431065\n",
      "Iteration 65, loss = 0.61404868\n",
      "Iteration 2363, loss = 0.15200300\n",
      "Iteration 853, loss = 0.30060150\n",
      "Iteration 1083, loss = 0.25413871\n",
      "Iteration 392, loss = 0.37270759\n",
      "Iteration 66, loss = 0.61118823\n",
      "Iteration 1606, loss = 0.17174367\n",
      "Iteration 1084, loss = 0.25395223\n",
      "Iteration 854, loss = 0.30051016\n",
      "Iteration 67, loss = 0.60829180\n",
      "Iteration 1085, loss = 0.25388725\n",
      "Iteration 1668, loss = 0.22264851\n",
      "Iteration 393, loss = 0.37249368\n",
      "Iteration 855, loss = 0.30041839\n",
      "Iteration 2364, loss = 0.15208463\n",
      "Iteration 1086, loss = 0.25373061\n",
      "Iteration 68, loss = 0.60551391\n",
      "Iteration 856, loss = 0.30030395\n",
      "Iteration 1607, loss = 0.17161531\n",
      "Iteration 1087, loss = 0.25357547\n",
      "Iteration 648, loss = 0.35580161\n",
      "Iteration 1088, loss = 0.25340167\n",
      "Iteration 69, loss = 0.60263519\n",
      "Iteration 857, loss = 0.30024293\n",
      "Iteration 394, loss = 0.37235816\n",
      "Iteration 2365, loss = 0.15196617\n",
      "Iteration 1669, loss = 0.22255240\n",
      "Iteration 1089, loss = 0.25339831\n",
      "Iteration 70, loss = 0.59987148\n",
      "Iteration 858, loss = 0.30010796\n",
      "Iteration 1090, loss = 0.25314936\n",
      "Iteration 859, loss = 0.30000350\n",
      "Iteration 71, loss = 0.59696415\n",
      "Iteration 649, loss = 0.35558506\n",
      "Iteration 2366, loss = 0.15179555\n",
      "Iteration 395, loss = 0.37221072\n",
      "Iteration 860, loss = 0.29994509\n",
      "Iteration 1091, loss = 0.25299999\n",
      "Iteration 1608, loss = 0.17148185\n",
      "Iteration 1670, loss = 0.22246454\n",
      "Iteration 1092, loss = 0.25293336\n",
      "Iteration 861, loss = 0.29982124\n",
      "Iteration 72, loss = 0.59423623\n",
      "Iteration 2367, loss = 0.15172689\n",
      "Iteration 862, loss = 0.29972108\n",
      "Iteration 1093, loss = 0.25283170\n",
      "Iteration 73, loss = 0.59138806\n",
      "Iteration 396, loss = 0.37208726\n",
      "Iteration 1671, loss = 0.22236059\n",
      "Iteration 2368, loss = 0.15160945\n",
      "Iteration 1094, loss = 0.25266325\n",
      "Iteration 863, loss = 0.29963275\n",
      "Iteration 1609, loss = 0.17123444\n",
      "Iteration 1095, loss = 0.25255997\n",
      "Iteration 864, loss = 0.29954446\n",
      "Iteration 2369, loss = 0.15170771\n",
      "Iteration 1672, loss = 0.22219370\n",
      "Iteration 74, loss = 0.58847717\n",
      "Iteration 1096, loss = 0.25236995\n",
      "Iteration 397, loss = 0.37194603\n",
      "Iteration 650, loss = 0.35544597\n",
      "Iteration 75, loss = 0.58574857\n",
      "Iteration 1610, loss = 0.17113078\n",
      "Iteration 1097, loss = 0.25223180\n",
      "Iteration 1673, loss = 0.22209381\n",
      "Iteration 865, loss = 0.29941866\n",
      "Iteration 1098, loss = 0.25206424\n",
      "Iteration 2370, loss = 0.15151329\n",
      "Iteration 398, loss = 0.37187308\n",
      "Iteration 1099, loss = 0.25208649\n",
      "Iteration 651, loss = 0.35523467\n",
      "Iteration 1611, loss = 0.17112889\n",
      "Iteration 1674, loss = 0.22198068\n",
      "Iteration 1100, loss = 0.25181101\n",
      "Iteration 1101, loss = 0.25182708\n",
      "Iteration 866, loss = 0.29937256\n",
      "Iteration 76, loss = 0.58302850\n",
      "Iteration 1675, loss = 0.22185037\n",
      "Iteration 2371, loss = 0.15149244\n",
      "Iteration 1102, loss = 0.25154781\n",
      "Iteration 652, loss = 0.35496579\n",
      "Iteration 399, loss = 0.37168163\n",
      "Iteration 1103, loss = 0.25146179\n",
      "Iteration 77, loss = 0.58010767\n",
      "Iteration 2372, loss = 0.15134702\n",
      "Iteration 867, loss = 0.29923891\n",
      "Iteration 1612, loss = 0.17095874\n",
      "Iteration 1104, loss = 0.25133600\n",
      "Iteration 400, loss = 0.37158705\n",
      "Iteration 1105, loss = 0.25116414\n",
      "Iteration 653, loss = 0.35481383\n",
      "Iteration 1676, loss = 0.22176261\n",
      "Iteration 868, loss = 0.29914653\n",
      "Iteration 1106, loss = 0.25101372\n",
      "Iteration 78, loss = 0.57732547\n",
      "Iteration 1613, loss = 0.17083488\n",
      "Iteration 1107, loss = 0.25089086\n",
      "Iteration 2373, loss = 0.15131564\n",
      "Iteration 1108, loss = 0.25073188\n",
      "Iteration 869, loss = 0.29903097\n",
      "Iteration 1109, loss = 0.25067217\n",
      "Iteration 401, loss = 0.37140863\n",
      "Iteration 1110, loss = 0.25049295\n",
      "Iteration 79, loss = 0.57455096\n",
      "Iteration 1677, loss = 0.22165740\n",
      "Iteration 1111, loss = 0.25038167\n",
      "Iteration 1112, loss = 0.25025344\n",
      "Iteration 2374, loss = 0.15123029\n",
      "Iteration 402, loss = 0.37127484\n",
      "Iteration 1113, loss = 0.25007841\n",
      "Iteration 870, loss = 0.29903794\n",
      "Iteration 1114, loss = 0.24991749\n",
      "Iteration 1614, loss = 0.17073755\n",
      "Iteration 80, loss = 0.57182477\n",
      "Iteration 1115, loss = 0.24983258\n",
      "Iteration 1116, loss = 0.24966278\n",
      "Iteration 654, loss = 0.35469536\n",
      "Iteration 81, loss = 0.56917933\n",
      "Iteration 871, loss = 0.29887813\n",
      "Iteration 872, loss = 0.29880237\n",
      "Iteration 82, loss = 0.56645906\n",
      "Iteration 873, loss = 0.29867161\n",
      "Iteration 2375, loss = 0.15110390\n",
      "Iteration 1117, loss = 0.24953011\n",
      "Iteration 403, loss = 0.37113494\n",
      "Iteration 83, loss = 0.56378460\n",
      "Iteration 874, loss = 0.29860135\n",
      "Iteration 1118, loss = 0.24937273\n",
      "Iteration 1615, loss = 0.17063940\n",
      "Iteration 1119, loss = 0.24927845\n",
      "Iteration 875, loss = 0.29848449\n",
      "Iteration 1120, loss = 0.24908124\n",
      "Iteration 1678, loss = 0.22152117\n",
      "Iteration 84, loss = 0.56109773\n",
      "Iteration 876, loss = 0.29836569\n",
      "Iteration 1121, loss = 0.24898149\n",
      "Iteration 877, loss = 0.29827545\n",
      "Iteration 1122, loss = 0.24882197\n",
      "Iteration 2376, loss = 0.15104854\n",
      "Iteration 1679, loss = 0.22136759\n",
      "Iteration 404, loss = 0.37103342\n",
      "Iteration 1123, loss = 0.24870208\n",
      "Iteration 655, loss = 0.35436289\n",
      "Iteration 2377, loss = 0.15102446\n",
      "Iteration 1124, loss = 0.24858139\n",
      "Iteration 1680, loss = 0.22128517\n",
      "Iteration 1616, loss = 0.17053919\n",
      "Iteration 2378, loss = 0.15099549\n",
      "Iteration 1125, loss = 0.24845545\n",
      "Iteration 878, loss = 0.29824799\n",
      "Iteration 879, loss = 0.29807843\n",
      "Iteration 656, loss = 0.35414110\n",
      "Iteration 1681, loss = 0.22125059\n",
      "Iteration 405, loss = 0.37087196\n",
      "Iteration 85, loss = 0.55842550\n",
      "Iteration 880, loss = 0.29802275\n",
      "Iteration 1126, loss = 0.24833298\n",
      "Iteration 881, loss = 0.29799036\n",
      "Iteration 1127, loss = 0.24818157\n",
      "Iteration 406, loss = 0.37073977\n",
      "Iteration 1617, loss = 0.17066849\n",
      "Iteration 1682, loss = 0.22107726\n",
      "Iteration 2379, loss = 0.15078371Iteration 1128, loss = 0.24801731\n",
      "Iteration 86, loss = 0.55570840\n",
      "\n",
      "Iteration 882, loss = 0.29782969\n",
      "Iteration 1129, loss = 0.24785094\n",
      "Iteration 883, loss = 0.29771184\n",
      "Iteration 1130, loss = 0.24775343\n",
      "Iteration 884, loss = 0.29770427\n",
      "Iteration 1618, loss = 0.17040322\n",
      "Iteration 1131, loss = 0.24771779\n",
      "Iteration 657, loss = 0.35397139\n",
      "Iteration 1683, loss = 0.22095190\n",
      "Iteration 87, loss = 0.55310858\n",
      "Iteration 407, loss = 0.37060813\n",
      "Iteration 1132, loss = 0.24752753\n",
      "Iteration 885, loss = 0.29751179\n",
      "Iteration 88, loss = 0.55050146\n",
      "Iteration 1619, loss = 0.17019623\n",
      "Iteration 886, loss = 0.29741169\n",
      "Iteration 1133, loss = 0.24732017\n",
      "Iteration 887, loss = 0.29731576\n",
      "Iteration 658, loss = 0.35371010\n",
      "Iteration 2380, loss = 0.15076600\n",
      "Iteration 89, loss = 0.54792425\n",
      "Iteration 888, loss = 0.29723234\n",
      "Iteration 1134, loss = 0.24719790\n",
      "Iteration 408, loss = 0.37049409\n",
      "Iteration 1684, loss = 0.22083283\n",
      "Iteration 1135, loss = 0.24707855\n",
      "Iteration 889, loss = 0.29711440\n",
      "Iteration 409, loss = 0.37036959\n",
      "Iteration 1136, loss = 0.24693248\n",
      "Iteration 90, loss = 0.54538902\n",
      "Iteration 2381, loss = 0.15068328\n",
      "Iteration 1685, loss = 0.22068088\n",
      "Iteration 890, loss = 0.29710414\n",
      "Iteration 659, loss = 0.35353142\n",
      "Iteration 1137, loss = 0.24681208\n",
      "Iteration 1138, loss = 0.24666775\n",
      "Iteration 891, loss = 0.29696750\n",
      "Iteration 1139, loss = 0.24647484\n",
      "Iteration 1620, loss = 0.17016886\n",
      "Iteration 91, loss = 0.54293029\n",
      "Iteration 1140, loss = 0.24646415\n",
      "Iteration 892, loss = 0.29691923\n",
      "Iteration 1141, loss = 0.24629818\n",
      "Iteration 1686, loss = 0.22066733\n",
      "Iteration 893, loss = 0.29674716\n",
      "Iteration 1142, loss = 0.24609839\n",
      "Iteration 2382, loss = 0.15070343\n",
      "Iteration 660, loss = 0.35327459\n",
      "Iteration 1143, loss = 0.24602370\n",
      "Iteration 894, loss = 0.29676170\n",
      "Iteration 1144, loss = 0.24581944\n",
      "Iteration 1687, loss = 0.22060337\n",
      "Iteration 410, loss = 0.37020587\n",
      "Iteration 92, loss = 0.54043169\n",
      "Iteration 2383, loss = 0.15060021\n",
      "Iteration 1145, loss = 0.24565570\n",
      "Iteration 1146, loss = 0.24552212\n",
      "Iteration 1147, loss = 0.24539979\n",
      "Iteration 1621, loss = 0.17033238\n",
      "Iteration 2384, loss = 0.15057821\n",
      "Iteration 411, loss = 0.37011781\n",
      "Iteration 1148, loss = 0.24532406\n",
      "Iteration 895, loss = 0.29654979\n",
      "Iteration 93, loss = 0.53800885\n",
      "Iteration 1149, loss = 0.24518137\n",
      "Iteration 1688, loss = 0.22038767\n",
      "Iteration 1150, loss = 0.24502022\n",
      "Iteration 94, loss = 0.53557004\n",
      "Iteration 896, loss = 0.29649882\n",
      "Iteration 1151, loss = 0.24482196\n",
      "Iteration 661, loss = 0.35329986\n",
      "Iteration 2385, loss = 0.15045555\n",
      "Iteration 1152, loss = 0.24491861\n",
      "Iteration 95, loss = 0.53314360\n",
      "Iteration 1689, loss = 0.22023915\n",
      "Iteration 1153, loss = 0.24451093\n",
      "Iteration 412, loss = 0.36997310\n",
      "Iteration 897, loss = 0.29635007\n",
      "Iteration 1154, loss = 0.24437789\n",
      "Iteration 898, loss = 0.29628362\n",
      "Iteration 96, loss = 0.53079436\n",
      "Iteration 662, loss = 0.35293094\n",
      "Iteration 1155, loss = 0.24425514\n",
      "Iteration 1690, loss = 0.22014271\n",
      "Iteration 1622, loss = 0.16994023\n",
      "Iteration 413, loss = 0.36979273\n",
      "Iteration 1156, loss = 0.24409025\n",
      "Iteration 899, loss = 0.29615772\n",
      "Iteration 1691, loss = 0.22001743\n",
      "Iteration 2386, loss = 0.15035714\n",
      "Iteration 414, loss = 0.36969563\n",
      "Iteration 1157, loss = 0.24397122\n",
      "Iteration 97, loss = 0.52847889\n",
      "Iteration 1158, loss = 0.24379570\n",
      "Iteration 1623, loss = 0.16998278\n",
      "Iteration 900, loss = 0.29609676\n",
      "Iteration 663, loss = 0.35265776\n",
      "Iteration 1159, loss = 0.24363619\n",
      "Iteration 98, loss = 0.52624737\n",
      "Iteration 2387, loss = 0.15034175\n",
      "Iteration 1160, loss = 0.24358489\n",
      "Iteration 901, loss = 0.29599301\n",
      "Iteration 415, loss = 0.36954537\n",
      "Iteration 1161, loss = 0.24334629\n",
      "Iteration 99, loss = 0.52395011Iteration 1624, loss = 0.16970376\n",
      "\n",
      "Iteration 902, loss = 0.29591143\n",
      "Iteration 903, loss = 0.29578774\n",
      "Iteration 1162, loss = 0.24334309\n",
      "Iteration 100, loss = 0.52178928\n",
      "Iteration 416, loss = 0.36950694\n",
      "Iteration 1163, loss = 0.24326284\n",
      "Iteration 1692, loss = 0.21997543\n",
      "Iteration 664, loss = 0.35245243\n",
      "Iteration 417, loss = 0.36931515\n",
      "Iteration 2388, loss = 0.15020682\n",
      "Iteration 1693, loss = 0.21983575\n",
      "Iteration 418, loss = 0.36916493\n",
      "Iteration 1164, loss = 0.24294526\n",
      "Iteration 1625, loss = 0.16962592\n",
      "Iteration 1694, loss = 0.21972554\n",
      "Iteration 904, loss = 0.29567969\n",
      "Iteration 905, loss = 0.29558916\n",
      "Iteration 101, loss = 0.51957941\n",
      "Iteration 906, loss = 0.29548624\n",
      "Iteration 419, loss = 0.36910044\n",
      "Iteration 1165, loss = 0.24280014\n",
      "Iteration 1695, loss = 0.21955548\n",
      "Iteration 907, loss = 0.29540113\n",
      "Iteration 665, loss = 0.35236791\n",
      "Iteration 1166, loss = 0.24264046\n",
      "Iteration 908, loss = 0.29532503\n",
      "Iteration 102, loss = 0.51737112\n",
      "Iteration 2389, loss = 0.15010493\n",
      "Iteration 1167, loss = 0.24249725\n",
      "Iteration 909, loss = 0.29518971\n",
      "Iteration 103, loss = 0.51526144\n",
      "Iteration 1696, loss = 0.21954212\n",
      "Iteration 1168, loss = 0.24236500\n",
      "Iteration 910, loss = 0.29514379\n",
      "Iteration 104, loss = 0.51316107\n",
      "Iteration 1169, loss = 0.24224366\n",
      "Iteration 911, loss = 0.29501219\n",
      "Iteration 1170, loss = 0.24207785\n",
      "Iteration 1171, loss = 0.24200560\n",
      "Iteration 1697, loss = 0.21931039\n",
      "Iteration 1172, loss = 0.24186946\n",
      "Iteration 105, loss = 0.51105503\n",
      "Iteration 912, loss = 0.29494849\n",
      "Iteration 1626, loss = 0.16957759\n",
      "Iteration 2390, loss = 0.15007707\n",
      "Iteration 106, loss = 0.50908086\n",
      "Iteration 913, loss = 0.29485249\n",
      "Iteration 1173, loss = 0.24167318\n",
      "Iteration 666, loss = 0.35205265\n",
      "Iteration 1174, loss = 0.24168149\n",
      "Iteration 2391, loss = 0.14992362\n",
      "Iteration 1175, loss = 0.24133916\n",
      "Iteration 420, loss = 0.36890653\n",
      "Iteration 107, loss = 0.50699421\n",
      "Iteration 2392, loss = 0.15017328\n",
      "Iteration 914, loss = 0.29474125\n",
      "Iteration 667, loss = 0.35186517\n",
      "Iteration 915, loss = 0.29463334\n",
      "Iteration 1627, loss = 0.16961214\n",
      "Iteration 916, loss = 0.29455451\n",
      "Iteration 1176, loss = 0.24119206\n",
      "Iteration 2393, loss = 0.14974961\n",
      "Iteration 1698, loss = 0.21918359\n",
      "Iteration 668, loss = 0.35167672\n",
      "Iteration 917, loss = 0.29446016\n",
      "Iteration 1177, loss = 0.24105029\n",
      "Iteration 108, loss = 0.50499528\n",
      "Iteration 1178, loss = 0.24100119\n",
      "Iteration 918, loss = 0.29434260\n",
      "Iteration 421, loss = 0.36875146\n",
      "Iteration 1628, loss = 0.16950470\n",
      "Iteration 1699, loss = 0.21908787\n",
      "Iteration 669, loss = 0.35137950\n",
      "Iteration 2394, loss = 0.14971440\n",
      "Iteration 109, loss = 0.50310515\n",
      "Iteration 919, loss = 0.29425348\n",
      "Iteration 1179, loss = 0.24077266\n",
      "Iteration 1180, loss = 0.24066410\n",
      "Iteration 2395, loss = 0.14963135\n",
      "Iteration 1700, loss = 0.21894378\n",
      "Iteration 920, loss = 0.29416688\n",
      "Iteration 110, loss = 0.50120216\n",
      "Iteration 422, loss = 0.36862453\n",
      "Iteration 2396, loss = 0.14967368\n",
      "Iteration 111, loss = 0.49929379\n",
      "Iteration 1629, loss = 0.16926818\n",
      "Iteration 1701, loss = 0.21881377\n",
      "Iteration 1181, loss = 0.24047753\n",
      "Iteration 921, loss = 0.29404954\n",
      "Iteration 670, loss = 0.35127195\n",
      "Iteration 922, loss = 0.29397063\n",
      "Iteration 1182, loss = 0.24033941\n",
      "Iteration 423, loss = 0.36863105\n",
      "Iteration 923, loss = 0.29386033\n",
      "Iteration 2397, loss = 0.14950841\n",
      "Iteration 1630, loss = 0.16911955\n",
      "Iteration 1183, loss = 0.24019163\n",
      "Iteration 112, loss = 0.49742887\n",
      "Iteration 1184, loss = 0.24020128\n",
      "Iteration 1702, loss = 0.21884573\n",
      "Iteration 1185, loss = 0.23990674\n",
      "Iteration 671, loss = 0.35101205\n",
      "Iteration 1186, loss = 0.23975938\n",
      "Iteration 113, loss = 0.49564562\n",
      "Iteration 424, loss = 0.36839776\n",
      "Iteration 1187, loss = 0.23964612\n",
      "Iteration 924, loss = 0.29376305\n",
      "Iteration 1703, loss = 0.21870431\n",
      "Iteration 2398, loss = 0.14941401\n",
      "Iteration 925, loss = 0.29366571\n",
      "Iteration 114, loss = 0.49388876\n",
      "Iteration 1188, loss = 0.23963029\n",
      "Iteration 926, loss = 0.29356755\n",
      "Iteration 1189, loss = 0.23943357\n",
      "Iteration 1190, loss = 0.23922034\n",
      "Iteration 1704, loss = 0.21853802\n",
      "Iteration 1191, loss = 0.23901730\n",
      "Iteration 1631, loss = 0.16918039\n",
      "Iteration 1192, loss = 0.23883552\n",
      "Iteration 115, loss = 0.49220818\n",
      "Iteration 927, loss = 0.29345933\n",
      "Iteration 672, loss = 0.35086212\n",
      "Iteration 1193, loss = 0.23873685\n",
      "Iteration 928, loss = 0.29339721\n",
      "Iteration 425, loss = 0.36825764\n",
      "Iteration 116, loss = 0.49043483\n",
      "Iteration 2399, loss = 0.14949975\n",
      "Iteration 1705, loss = 0.21835725\n",
      "Iteration 1194, loss = 0.23865020\n",
      "Iteration 929, loss = 0.29327675\n",
      "Iteration 1706, loss = 0.21828821\n",
      "Iteration 1195, loss = 0.23842655\n",
      "Iteration 2400, loss = 0.14925851\n",
      "Iteration 426, loss = 0.36810608\n",
      "Iteration 1196, loss = 0.23826845\n",
      "Iteration 930, loss = 0.29319355\n",
      "Iteration 1197, loss = 0.23817100\n",
      "Iteration 673, loss = 0.35055530\n",
      "Iteration 117, loss = 0.48871857\n",
      "Iteration 1198, loss = 0.23797718\n",
      "Iteration 931, loss = 0.29309439\n",
      "Iteration 1199, loss = 0.23783496\n",
      "Iteration 2401, loss = 0.14919661\n",
      "Iteration 1200, loss = 0.23767899\n",
      "Iteration 932, loss = 0.29298689\n",
      "Iteration 1201, loss = 0.23758064\n",
      "Iteration 427, loss = 0.36803310\n",
      "Iteration 1202, loss = 0.23750033\n",
      "Iteration 1632, loss = 0.16891529\n",
      "Iteration 1203, loss = 0.23728313\n",
      "Iteration 1707, loss = 0.21813457\n",
      "Iteration 933, loss = 0.29290109\n",
      "Iteration 2402, loss = 0.14913817\n",
      "Iteration 934, loss = 0.29282888\n",
      "Iteration 674, loss = 0.35035170\n",
      "Iteration 1204, loss = 0.23707592\n",
      "Iteration 118, loss = 0.48706271\n",
      "Iteration 1205, loss = 0.23693365\n",
      "Iteration 428, loss = 0.36785209\n",
      "Iteration 935, loss = 0.29271675\n",
      "Iteration 1206, loss = 0.23681122\n",
      "Iteration 1708, loss = 0.21804985\n",
      "Iteration 1207, loss = 0.23661420\n",
      "Iteration 119, loss = 0.48539893\n",
      "Iteration 1208, loss = 0.23646744\n",
      "Iteration 1209, loss = 0.23636278\n",
      "Iteration 1210, loss = 0.23617587\n",
      "Iteration 936, loss = 0.29261494\n",
      "Iteration 2403, loss = 0.14909352\n",
      "Iteration 1633, loss = 0.16888357\n",
      "Iteration 429, loss = 0.36775627\n",
      "Iteration 1709, loss = 0.21790518\n",
      "Iteration 1211, loss = 0.23600909\n",
      "Iteration 1212, loss = 0.23588672\n",
      "Iteration 675, loss = 0.35013126\n",
      "Iteration 120, loss = 0.48387547\n",
      "Iteration 937, loss = 0.29253236\n",
      "Iteration 430, loss = 0.36762464\n",
      "Iteration 121, loss = 0.48226059\n",
      "Iteration 2404, loss = 0.14899048\n",
      "Iteration 1634, loss = 0.16874103\n",
      "Iteration 431, loss = 0.36750386\n",
      "Iteration 1213, loss = 0.23584556\n",
      "Iteration 1710, loss = 0.21786110\n",
      "Iteration 676, loss = 0.34996946\n",
      "Iteration 2405, loss = 0.14899964\n",
      "Iteration 938, loss = 0.29241834\n",
      "Iteration 432, loss = 0.36736941\n",
      "Iteration 939, loss = 0.29233912\n",
      "Iteration 2406, loss = 0.14883628\n",
      "Iteration 940, loss = 0.29223904\n",
      "Iteration 1711, loss = 0.21769388\n",
      "Iteration 433, loss = 0.36726652\n",
      "Iteration 1214, loss = 0.23556403\n",
      "Iteration 1215, loss = 0.23543676\n",
      "Iteration 677, loss = 0.34967551\n",
      "Iteration 1216, loss = 0.23524803\n",
      "Iteration 941, loss = 0.29213993\n",
      "Iteration 1217, loss = 0.23517238\n",
      "Iteration 1712, loss = 0.21756167\n",
      "Iteration 1218, loss = 0.23497760\n",
      "Iteration 1219, loss = 0.23483391\n",
      "Iteration 122, loss = 0.48075233\n",
      "Iteration 1220, loss = 0.23465741\n",
      "Iteration 942, loss = 0.29204773\n",
      "Iteration 434, loss = 0.36713385\n",
      "Iteration 678, loss = 0.34956152\n",
      "Iteration 123, loss = 0.47917529\n",
      "Iteration 1221, loss = 0.23451355\n",
      "Iteration 2407, loss = 0.14877504\n",
      "Iteration 1635, loss = 0.16872232\n",
      "Iteration 943, loss = 0.29196337\n",
      "Iteration 124, loss = 0.47775180\n",
      "Iteration 1222, loss = 0.23439148\n",
      "Iteration 944, loss = 0.29188003\n",
      "Iteration 1223, loss = 0.23418794\n",
      "Iteration 679, loss = 0.34927012\n",
      "Iteration 2408, loss = 0.14879184\n",
      "Iteration 945, loss = 0.29181608\n",
      "Iteration 435, loss = 0.36701614\n",
      "Iteration 1224, loss = 0.23412148\n",
      "Iteration 125, loss = 0.47620725\n",
      "Iteration 1225, loss = 0.23390249\n",
      "Iteration 1713, loss = 0.21743728\n",
      "Iteration 1636, loss = 0.16853972\n",
      "Iteration 1226, loss = 0.23375317\n",
      "Iteration 1227, loss = 0.23359309\n",
      "Iteration 946, loss = 0.29170232\n",
      "Iteration 126, loss = 0.47479983\n",
      "Iteration 1228, loss = 0.23347636\n",
      "Iteration 680, loss = 0.34910420\n",
      "Iteration 1229, loss = 0.23334636\n",
      "Iteration 1637, loss = 0.16854759\n",
      "Iteration 947, loss = 0.29157269\n",
      "Iteration 1230, loss = 0.23311710\n",
      "Iteration 1714, loss = 0.21741038\n",
      "Iteration 1231, loss = 0.23313361\n",
      "Iteration 436, loss = 0.36693163\n",
      "Iteration 127, loss = 0.47343846\n",
      "Iteration 1232, loss = 0.23284495\n",
      "Iteration 2409, loss = 0.14863646\n",
      "Iteration 1233, loss = 0.23267285\n",
      "Iteration 1234, loss = 0.23252071\n",
      "Iteration 948, loss = 0.29154297\n",
      "Iteration 1235, loss = 0.23248058\n",
      "Iteration 1715, loss = 0.21724061\n",
      "Iteration 1236, loss = 0.23221448\n",
      "Iteration 2410, loss = 0.14853780\n",
      "Iteration 1237, loss = 0.23212367\n",
      "Iteration 128, loss = 0.47200501\n",
      "Iteration 1638, loss = 0.16839505\n",
      "Iteration 1238, loss = 0.23190950\n",
      "Iteration 437, loss = 0.36676803\n",
      "Iteration 949, loss = 0.29138431\n",
      "Iteration 1239, loss = 0.23183365\n",
      "Iteration 2411, loss = 0.14849542\n",
      "Iteration 129, loss = 0.47072764\n",
      "Iteration 1240, loss = 0.23163257\n",
      "Iteration 438, loss = 0.36662221\n",
      "Iteration 1241, loss = 0.23150212\n",
      "Iteration 1716, loss = 0.21711737\n",
      "Iteration 1242, loss = 0.23132684\n",
      "Iteration 130, loss = 0.46926534\n",
      "Iteration 681, loss = 0.34891005\n",
      "Iteration 1243, loss = 0.23113798\n",
      "Iteration 950, loss = 0.29128620\n",
      "Iteration 2412, loss = 0.14843628\n",
      "Iteration 131, loss = 0.46804406\n",
      "Iteration 1717, loss = 0.21702880\n",
      "Iteration 1244, loss = 0.23097098\n",
      "Iteration 2413, loss = 0.14835110\n",
      "Iteration 951, loss = 0.29121171\n",
      "Iteration 1639, loss = 0.16838671\n",
      "Iteration 439, loss = 0.36649399\n",
      "Iteration 1245, loss = 0.23083236\n",
      "Iteration 1246, loss = 0.23068869\n",
      "Iteration 2414, loss = 0.14830767\n",
      "Iteration 1718, loss = 0.21688294\n",
      "Iteration 1247, loss = 0.23049875\n",
      "Iteration 952, loss = 0.29110642\n",
      "Iteration 132, loss = 0.46670132\n",
      "Iteration 1640, loss = 0.16832537\n",
      "Iteration 1248, loss = 0.23043073\n",
      "Iteration 953, loss = 0.29100718\n",
      "Iteration 682, loss = 0.34875497\n",
      "Iteration 1719, loss = 0.21676493\n",
      "Iteration 2415, loss = 0.14825311\n",
      "Iteration 440, loss = 0.36638704\n",
      "Iteration 133, loss = 0.46540053\n",
      "Iteration 1249, loss = 0.23019407\n",
      "Iteration 954, loss = 0.29093368\n",
      "Iteration 441, loss = 0.36623654\n",
      "Iteration 2416, loss = 0.14811456\n",
      "Iteration 134, loss = 0.46415924\n",
      "Iteration 1641, loss = 0.16838559\n",
      "Iteration 1250, loss = 0.23006020\n",
      "Iteration 955, loss = 0.29081703\n",
      "Iteration 1720, loss = 0.21673966\n",
      "Iteration 683, loss = 0.34846121\n",
      "Iteration 1251, loss = 0.22998459\n",
      "Iteration 442, loss = 0.36619115\n",
      "Iteration 956, loss = 0.29074158\n",
      "Iteration 1252, loss = 0.22976585\n",
      "Iteration 135, loss = 0.46294351Iteration 1253, loss = 0.22961783\n",
      "\n",
      "Iteration 957, loss = 0.29072547\n",
      "Iteration 443, loss = 0.36610288\n",
      "Iteration 1254, loss = 0.22947749\n",
      "Iteration 1721, loss = 0.21654949\n",
      "Iteration 684, loss = 0.34825930\n",
      "Iteration 958, loss = 0.29054338\n",
      "Iteration 136, loss = 0.46172347\n",
      "Iteration 1255, loss = 0.22929445\n",
      "Iteration 2417, loss = 0.14835890\n",
      "Iteration 1256, loss = 0.22914550\n",
      "Iteration 137, loss = 0.46054491\n",
      "Iteration 959, loss = 0.29044898\n",
      "Iteration 1257, loss = 0.22896605\n",
      "Iteration 2418, loss = 0.14814322\n",
      "Iteration 138, loss = 0.45934595\n",
      "Iteration 685, loss = 0.34799308\n",
      "Iteration 1642, loss = 0.16808353\n",
      "Iteration 960, loss = 0.29034194\n",
      "Iteration 1258, loss = 0.22877750\n",
      "Iteration 139, loss = 0.45818456\n",
      "Iteration 1259, loss = 0.22861915\n",
      "Iteration 961, loss = 0.29028024\n",
      "Iteration 1722, loss = 0.21647502\n",
      "Iteration 444, loss = 0.36587551\n",
      "Iteration 962, loss = 0.29015464\n",
      "Iteration 2419, loss = 0.14798411\n",
      "Iteration 1260, loss = 0.22852738\n",
      "Iteration 686, loss = 0.34782274\n",
      "Iteration 963, loss = 0.29008064\n",
      "Iteration 1723, loss = 0.21630726\n",
      "Iteration 1643, loss = 0.16816774\n",
      "Iteration 1261, loss = 0.22835407\n",
      "Iteration 140, loss = 0.45700642\n",
      "Iteration 964, loss = 0.29000697\n",
      "Iteration 1262, loss = 0.22822653\n",
      "Iteration 965, loss = 0.28987952\n",
      "Iteration 2420, loss = 0.14785016\n",
      "Iteration 1263, loss = 0.22802517\n",
      "Iteration 1724, loss = 0.21633252\n",
      "Iteration 1264, loss = 0.22791900\n",
      "Iteration 445, loss = 0.36574470\n",
      "Iteration 1265, loss = 0.22767612\n",
      "Iteration 141, loss = 0.45597856\n",
      "Iteration 687, loss = 0.34764837\n",
      "Iteration 1266, loss = 0.22750084\n",
      "Iteration 2421, loss = 0.14780530\n",
      "Iteration 1267, loss = 0.22739996\n",
      "Iteration 446, loss = 0.36562148\n",
      "Iteration 1268, loss = 0.22718725\n",
      "Iteration 966, loss = 0.28976486\n",
      "Iteration 1644, loss = 0.16786953\n",
      "Iteration 1269, loss = 0.22706606\n",
      "Iteration 1270, loss = 0.22689312\n",
      "Iteration 1725, loss = 0.21617138\n",
      "Iteration 967, loss = 0.28973007\n",
      "Iteration 142, loss = 0.45481611\n",
      "Iteration 968, loss = 0.28959899\n",
      "Iteration 2422, loss = 0.14772584\n",
      "Iteration 1645, loss = 0.16773665\n",
      "Iteration 1271, loss = 0.22673119\n",
      "Iteration 143, loss = 0.45378274\n",
      "Iteration 1726, loss = 0.21593637\n",
      "Iteration 688, loss = 0.34737536\n",
      "Iteration 1272, loss = 0.22659601\n",
      "Iteration 969, loss = 0.28956381\n",
      "Iteration 1273, loss = 0.22665262\n",
      "Iteration 970, loss = 0.28940881\n",
      "Iteration 1727, loss = 0.21593712\n",
      "Iteration 144, loss = 0.45265322\n",
      "Iteration 1274, loss = 0.22628390\n",
      "Iteration 447, loss = 0.36551038\n",
      "Iteration 2423, loss = 0.14763412\n",
      "Iteration 971, loss = 0.28934579\n",
      "Iteration 1275, loss = 0.22613914\n",
      "Iteration 1646, loss = 0.16762230\n",
      "Iteration 145, loss = 0.45167343\n",
      "Iteration 1276, loss = 0.22596504\n",
      "Iteration 972, loss = 0.28920246\n",
      "Iteration 1277, loss = 0.22595862\n",
      "Iteration 1728, loss = 0.21573714\n",
      "Iteration 1278, loss = 0.22563518\n",
      "Iteration 448, loss = 0.36538040\n",
      "Iteration 2424, loss = 0.14757411\n",
      "Iteration 146, loss = 0.45054612\n",
      "Iteration 689, loss = 0.34715364\n",
      "Iteration 1279, loss = 0.22558284\n",
      "Iteration 973, loss = 0.28912967\n",
      "Iteration 147, loss = 0.44958466\n",
      "Iteration 2425, loss = 0.14751945\n",
      "Iteration 1280, loss = 0.22532262\n",
      "Iteration 1281, loss = 0.22513381\n",
      "Iteration 974, loss = 0.28907382\n",
      "Iteration 148, loss = 0.44857484\n",
      "Iteration 1282, loss = 0.22509905\n",
      "Iteration 1647, loss = 0.16759778\n",
      "Iteration 2426, loss = 0.14747432\n",
      "Iteration 1729, loss = 0.21564256\n",
      "Iteration 1283, loss = 0.22488839\n",
      "Iteration 149, loss = 0.44760376\n",
      "Iteration 1284, loss = 0.22466567\n",
      "Iteration 975, loss = 0.28891862\n",
      "Iteration 449, loss = 0.36524655\n",
      "Iteration 1285, loss = 0.22453460\n",
      "Iteration 1648, loss = 0.16751953\n",
      "Iteration 1286, loss = 0.22440002\n",
      "Iteration 690, loss = 0.34696702\n",
      "Iteration 1730, loss = 0.21553132\n",
      "Iteration 1287, loss = 0.22421828\n",
      "Iteration 2427, loss = 0.14735717\n",
      "Iteration 450, loss = 0.36519617\n",
      "Iteration 1288, loss = 0.22410597\n",
      "Iteration 976, loss = 0.28881270\n",
      "Iteration 1289, loss = 0.22403661\n",
      "Iteration 1290, loss = 0.22373751\n",
      "Iteration 1731, loss = 0.21544299\n",
      "Iteration 1291, loss = 0.22355826\n",
      "Iteration 1649, loss = 0.16731871\n",
      "Iteration 150, loss = 0.44668353\n",
      "Iteration 1292, loss = 0.22341929\n",
      "Iteration 451, loss = 0.36503801\n",
      "Iteration 2428, loss = 0.14737064\n",
      "Iteration 977, loss = 0.28882274\n",
      "Iteration 978, loss = 0.28864493\n",
      "Iteration 151, loss = 0.44564750\n",
      "Iteration 1293, loss = 0.22327556\n",
      "Iteration 452, loss = 0.36488500\n",
      "Iteration 2429, loss = 0.14742253\n",
      "Iteration 691, loss = 0.34683646\n",
      "Iteration 1294, loss = 0.22316129\n",
      "Iteration 1732, loss = 0.21525967\n",
      "Iteration 1295, loss = 0.22298301\n",
      "Iteration 979, loss = 0.28855386\n",
      "Iteration 1296, loss = 0.22283527\n",
      "Iteration 1297, loss = 0.22266188\n",
      "Iteration 152, loss = 0.44466782\n",
      "Iteration 453, loss = 0.36478032\n",
      "Iteration 1298, loss = 0.22258299\n",
      "Iteration 1733, loss = 0.21516127\n",
      "Iteration 1299, loss = 0.22235510\n",
      "Iteration 980, loss = 0.28846689\n",
      "Iteration 1650, loss = 0.16731952\n",
      "Iteration 1300, loss = 0.22217954\n",
      "Iteration 454, loss = 0.36465461\n",
      "Iteration 692, loss = 0.34650924\n",
      "Iteration 2430, loss = 0.14720837\n",
      "Iteration 153, loss = 0.44378534\n",
      "Iteration 455, loss = 0.36453744\n",
      "Iteration 1734, loss = 0.21505435\n",
      "Iteration 981, loss = 0.28836536\n",
      "Iteration 1301, loss = 0.22210533\n",
      "Iteration 1651, loss = 0.16711996\n",
      "Iteration 1302, loss = 0.22197181\n",
      "Iteration 1303, loss = 0.22181448\n",
      "Iteration 456, loss = 0.36440809\n",
      "Iteration 1304, loss = 0.22155330\n",
      "Iteration 2431, loss = 0.14711628\n",
      "Iteration 982, loss = 0.28829164\n",
      "Iteration 154, loss = 0.44283331\n",
      "Iteration 693, loss = 0.34629830\n",
      "Iteration 1305, loss = 0.22139736\n",
      "Iteration 1735, loss = 0.21494274\n",
      "Iteration 983, loss = 0.28818628\n",
      "Iteration 1306, loss = 0.22122802\n",
      "Iteration 984, loss = 0.28812816\n",
      "Iteration 1652, loss = 0.16704405\n",
      "Iteration 694, loss = 0.34614772\n",
      "Iteration 457, loss = 0.36428872\n",
      "Iteration 1736, loss = 0.21497101\n",
      "Iteration 985, loss = 0.28799469\n",
      "Iteration 1307, loss = 0.22107960\n",
      "Iteration 155, loss = 0.44188121\n",
      "Iteration 2432, loss = 0.14703227\n",
      "Iteration 1308, loss = 0.22105571\n",
      "Iteration 986, loss = 0.28793089\n",
      "Iteration 1653, loss = 0.16707049\n",
      "Iteration 695, loss = 0.34584874\n",
      "Iteration 987, loss = 0.28779303\n",
      "Iteration 458, loss = 0.36416269\n",
      "Iteration 1309, loss = 0.22082402\n",
      "Iteration 1737, loss = 0.21474010\n",
      "Iteration 1310, loss = 0.22077327\n",
      "Iteration 988, loss = 0.28771608\n",
      "Iteration 1311, loss = 0.22045683\n",
      "Iteration 2433, loss = 0.14696246\n",
      "Iteration 1654, loss = 0.16686025\n",
      "Iteration 696, loss = 0.34573050\n",
      "Iteration 1312, loss = 0.22036440\n",
      "Iteration 459, loss = 0.36405582\n",
      "Iteration 989, loss = 0.28766698\n",
      "Iteration 1313, loss = 0.22020068\n",
      "Iteration 156, loss = 0.44103476\n",
      "Iteration 2434, loss = 0.14711556\n",
      "Iteration 1314, loss = 0.22016473\n",
      "Iteration 460, loss = 0.36395912\n",
      "Iteration 990, loss = 0.28751438\n",
      "Iteration 157, loss = 0.44015361\n",
      "Iteration 1315, loss = 0.21989654\n",
      "Iteration 991, loss = 0.28743375\n",
      "Iteration 1655, loss = 0.16680063\n",
      "Iteration 1316, loss = 0.21970427\n",
      "Iteration 2435, loss = 0.14682167\n",
      "Iteration 1317, loss = 0.21961519\n",
      "Iteration 992, loss = 0.28734075\n",
      "Iteration 1738, loss = 0.21467899\n",
      "Iteration 1318, loss = 0.21933431\n",
      "Iteration 158, loss = 0.43927996\n",
      "Iteration 993, loss = 0.28725075\n",
      "Iteration 1319, loss = 0.21920581\n",
      "Iteration 994, loss = 0.28716451\n",
      "Iteration 1320, loss = 0.21907942\n",
      "Iteration 995, loss = 0.28703220\n",
      "Iteration 1321, loss = 0.21895660\n",
      "Iteration 697, loss = 0.34561492\n",
      "Iteration 159, loss = 0.43845151\n",
      "Iteration 996, loss = 0.28695852\n",
      "Iteration 1739, loss = 0.21450372\n",
      "Iteration 461, loss = 0.36379572\n",
      "Iteration 1322, loss = 0.21881137\n",
      "Iteration 997, loss = 0.28690231\n",
      "Iteration 1656, loss = 0.16673636\n",
      "Iteration 2436, loss = 0.14680181\n",
      "Iteration 998, loss = 0.28675133\n",
      "Iteration 462, loss = 0.36367000\n",
      "Iteration 1323, loss = 0.21855340\n",
      "Iteration 160, loss = 0.43760238\n",
      "Iteration 1740, loss = 0.21434458\n",
      "Iteration 1324, loss = 0.21859385\n",
      "Iteration 999, loss = 0.28671165\n",
      "Iteration 698, loss = 0.34523056\n",
      "Iteration 1325, loss = 0.21828790\n",
      "Iteration 1000, loss = 0.28661797\n",
      "Iteration 2437, loss = 0.14684006\n",
      "Iteration 1741, loss = 0.21424815\n",
      "Iteration 161, loss = 0.43669760\n",
      "Iteration 1657, loss = 0.16663694\n",
      "Iteration 1001, loss = 0.28650244\n",
      "Iteration 1326, loss = 0.21818099\n",
      "Iteration 1327, loss = 0.21803125\n",
      "Iteration 1002, loss = 0.28647652\n",
      "Iteration 2438, loss = 0.14661465\n",
      "Iteration 1328, loss = 0.21781351\n",
      "Iteration 463, loss = 0.36354388\n",
      "Iteration 1003, loss = 0.28631163\n",
      "Iteration 1329, loss = 0.21763103\n",
      "Iteration 162, loss = 0.43593252\n",
      "Iteration 1330, loss = 0.21741434\n",
      "Iteration 1004, loss = 0.28622639\n",
      "Iteration 2439, loss = 0.14658146\n",
      "Iteration 1331, loss = 0.21728032\n",
      "Iteration 1742, loss = 0.21414036\n",
      "Iteration 1658, loss = 0.16651405\n",
      "Iteration 1005, loss = 0.28610841\n",
      "Iteration 1332, loss = 0.21719692\n",
      "Iteration 1006, loss = 0.28604798\n",
      "Iteration 1333, loss = 0.21695247\n",
      "Iteration 699, loss = 0.34497872\n",
      "Iteration 163, loss = 0.43514050\n",
      "Iteration 2440, loss = 0.14653097\n",
      "Iteration 1334, loss = 0.21693103\n",
      "Iteration 1007, loss = 0.28592728\n",
      "Iteration 464, loss = 0.36342413\n",
      "Iteration 1008, loss = 0.28584305\n",
      "Iteration 1335, loss = 0.21672009\n",
      "Iteration 1336, loss = 0.21651001\n",
      "Iteration 1337, loss = 0.21630477\n",
      "Iteration 2441, loss = 0.14640920\n",
      "Iteration 164, loss = 0.43436671\n",
      "Iteration 1659, loss = 0.16643394\n",
      "Iteration 1743, loss = 0.21401344\n",
      "Iteration 1338, loss = 0.21621763\n",
      "Iteration 1339, loss = 0.21604049\n",
      "Iteration 465, loss = 0.36329304\n",
      "Iteration 1009, loss = 0.28577313\n",
      "Iteration 165, loss = 0.43353816\n",
      "Iteration 700, loss = 0.34471417\n",
      "Iteration 2442, loss = 0.14633375\n",
      "Iteration 1744, loss = 0.21391463\n",
      "Iteration 1340, loss = 0.21585977\n",
      "Iteration 166, loss = 0.43279213\n",
      "Iteration 1660, loss = 0.16632298\n",
      "Iteration 1010, loss = 0.28564860\n",
      "Iteration 1341, loss = 0.21575661\n",
      "Iteration 1342, loss = 0.21559727\n",
      "Iteration 2443, loss = 0.14634971\n",
      "Iteration 1011, loss = 0.28554999\n",
      "Iteration 466, loss = 0.36320776\n",
      "Iteration 1745, loss = 0.21380556\n",
      "Iteration 167, loss = 0.43205855\n",
      "Iteration 701, loss = 0.34473338\n",
      "Iteration 1343, loss = 0.21534580\n",
      "Iteration 168, loss = 0.43133737\n",
      "Iteration 1661, loss = 0.16633533\n",
      "Iteration 1012, loss = 0.28544226\n",
      "Iteration 1344, loss = 0.21521818\n",
      "Iteration 467, loss = 0.36305188\n",
      "Iteration 1345, loss = 0.21512584\n",
      "Iteration 2444, loss = 0.14624556\n",
      "Iteration 1746, loss = 0.21371650\n",
      "Iteration 702, loss = 0.34430402\n",
      "Iteration 1346, loss = 0.21487090\n",
      "Iteration 1013, loss = 0.28536897\n",
      "Iteration 169, loss = 0.43056061\n",
      "Iteration 1747, loss = 0.21352922\n",
      "Iteration 1347, loss = 0.21473410\n",
      "Iteration 1662, loss = 0.16610731\n",
      "Iteration 703, loss = 0.34408709\n",
      "Iteration 1348, loss = 0.21455950\n",
      "Iteration 1014, loss = 0.28526630\n",
      "Iteration 468, loss = 0.36295094\n",
      "Iteration 2445, loss = 0.14620130\n",
      "Iteration 1349, loss = 0.21440455\n",
      "Iteration 1748, loss = 0.21341852\n",
      "Iteration 170, loss = 0.42986307\n",
      "Iteration 1015, loss = 0.28516896\n",
      "Iteration 1350, loss = 0.21432103\n",
      "Iteration 2446, loss = 0.14610230\n",
      "Iteration 1351, loss = 0.21407463\n",
      "Iteration 1352, loss = 0.21392738\n",
      "Iteration 469, loss = 0.36281437\n",
      "Iteration 1353, loss = 0.21378893\n",
      "Iteration 1749, loss = 0.21333429\n",
      "Iteration 1354, loss = 0.21359302\n",
      "Iteration 704, loss = 0.34383838\n",
      "Iteration 171, loss = 0.42908757\n",
      "Iteration 1355, loss = 0.21340057\n",
      "Iteration 470, loss = 0.36271347\n",
      "Iteration 1356, loss = 0.21330634\n",
      "Iteration 1357, loss = 0.21309929\n",
      "Iteration 1016, loss = 0.28507993\n",
      "Iteration 2447, loss = 0.14601455\n",
      "Iteration 1663, loss = 0.16608963\n",
      "Iteration 1358, loss = 0.21294000\n",
      "Iteration 172, loss = 0.42841416\n",
      "Iteration 1750, loss = 0.21321179\n",
      "Iteration 2448, loss = 0.14591601\n",
      "Iteration 1359, loss = 0.21291415\n",
      "Iteration 173, loss = 0.42769017\n",
      "Iteration 1751, loss = 0.21308461\n",
      "Iteration 471, loss = 0.36260177\n",
      "Iteration 1360, loss = 0.21265198\n",
      "Iteration 1017, loss = 0.28497512\n",
      "Iteration 705, loss = 0.34371379\n",
      "Iteration 2449, loss = 0.14583924\n",
      "Iteration 1361, loss = 0.21261346\n",
      "Iteration 174, loss = 0.42706448\n",
      "Iteration 1018, loss = 0.28497209\n",
      "Iteration 1362, loss = 0.21243622\n",
      "Iteration 472, loss = 0.36251854\n",
      "Iteration 1664, loss = 0.16594245\n",
      "Iteration 2450, loss = 0.14585153\n",
      "Iteration 1752, loss = 0.21297247\n",
      "Iteration 1019, loss = 0.28477680\n",
      "Iteration 1363, loss = 0.21212736\n",
      "Iteration 706, loss = 0.34339025\n",
      "Iteration 175, loss = 0.42637917\n",
      "Iteration 473, loss = 0.36235103\n",
      "Iteration 1364, loss = 0.21209139\n",
      "Iteration 1365, loss = 0.21183222\n",
      "Iteration 1753, loss = 0.21286788\n",
      "Iteration 1665, loss = 0.16587023\n",
      "Iteration 1020, loss = 0.28471227\n",
      "Iteration 2451, loss = 0.14579250\n",
      "Iteration 1366, loss = 0.21187977\n",
      "Iteration 1367, loss = 0.21154361\n",
      "Iteration 176, loss = 0.42568303\n",
      "Iteration 1368, loss = 0.21133060\n",
      "Iteration 474, loss = 0.36222318\n",
      "Iteration 1369, loss = 0.21119130\n",
      "Iteration 1021, loss = 0.28461784\n",
      "Iteration 2452, loss = 0.14563131\n",
      "Iteration 1370, loss = 0.21107083\n",
      "Iteration 1666, loss = 0.16578145\n",
      "Iteration 1371, loss = 0.21090671\n",
      "Iteration 1372, loss = 0.21077632\n",
      "Iteration 177, loss = 0.42500729\n",
      "Iteration 475, loss = 0.36214529\n",
      "Iteration 1022, loss = 0.28452104\n",
      "Iteration 1373, loss = 0.21059840\n",
      "Iteration 1374, loss = 0.21036955\n",
      "Iteration 178, loss = 0.42438954\n",
      "Iteration 1375, loss = 0.21028223\n",
      "Iteration 1667, loss = 0.16569174\n",
      "Iteration 476, loss = 0.36198297\n",
      "Iteration 1376, loss = 0.21025230\n",
      "Iteration 2453, loss = 0.14555789\n",
      "Iteration 1377, loss = 0.20995884\n",
      "Iteration 1023, loss = 0.28442949\n",
      "Iteration 1754, loss = 0.21276514\n",
      "Iteration 707, loss = 0.34320631\n",
      "Iteration 1378, loss = 0.20975179\n",
      "Iteration 179, loss = 0.42371541\n",
      "Iteration 1024, loss = 0.28435998\n",
      "Iteration 2454, loss = 0.14551176\n",
      "Iteration 1025, loss = 0.28423399\n",
      "Iteration 477, loss = 0.36187975Iteration 1379, loss = 0.20961422\n",
      "Iteration 180, loss = 0.42305877\n",
      "Iteration 1668, loss = 0.16568295\n",
      "Iteration 2455, loss = 0.14544705\n",
      "\n",
      "Iteration 1755, loss = 0.21271813\n",
      "Iteration 708, loss = 0.34295789\n",
      "Iteration 1026, loss = 0.28412378\n",
      "Iteration 1380, loss = 0.20947359\n",
      "Iteration 1027, loss = 0.28403088\n",
      "Iteration 478, loss = 0.36176734\n",
      "Iteration 1381, loss = 0.20931548\n",
      "Iteration 2456, loss = 0.14539474\n",
      "Iteration 1382, loss = 0.20930402\n",
      "Iteration 181, loss = 0.42242510\n",
      "Iteration 1756, loss = 0.21250127\n",
      "Iteration 1383, loss = 0.20918435\n",
      "Iteration 1028, loss = 0.28393006\n",
      "Iteration 1384, loss = 0.20886480\n",
      "Iteration 1385, loss = 0.20875638\n",
      "Iteration 1386, loss = 0.20855109\n",
      "Iteration 2457, loss = 0.14527753\n",
      "Iteration 1029, loss = 0.28391006\n",
      "Iteration 709, loss = 0.34274430\n",
      "Iteration 1387, loss = 0.20841528\n",
      "Iteration 479, loss = 0.36166258\n",
      "Iteration 182, loss = 0.42184109\n",
      "Iteration 1388, loss = 0.20817740\n",
      "Iteration 1030, loss = 0.28377022\n",
      "Iteration 1757, loss = 0.21241860\n",
      "Iteration 1669, loss = 0.16547261\n",
      "Iteration 1389, loss = 0.20806635\n",
      "Iteration 183, loss = 0.42120981\n",
      "Iteration 1390, loss = 0.20788416\n",
      "Iteration 480, loss = 0.36152646\n",
      "Iteration 1391, loss = 0.20776592\n",
      "Iteration 184, loss = 0.42057227\n",
      "Iteration 2458, loss = 0.14528681\n",
      "Iteration 710, loss = 0.34248343\n",
      "Iteration 1758, loss = 0.21244306\n",
      "Iteration 481, loss = 0.36139682\n",
      "Iteration 1392, loss = 0.20762247\n",
      "Iteration 185, loss = 0.42001459\n",
      "Iteration 1393, loss = 0.20744128\n",
      "Iteration 1031, loss = 0.28364163\n",
      "Iteration 2459, loss = 0.14517452\n",
      "Iteration 1394, loss = 0.20735600\n",
      "Iteration 1395, loss = 0.20724881\n",
      "Iteration 1670, loss = 0.16551382\n",
      "Iteration 711, loss = 0.34250967\n",
      "Iteration 1759, loss = 0.21215573\n",
      "Iteration 1396, loss = 0.20721705\n",
      "Iteration 1032, loss = 0.28355447\n",
      "Iteration 482, loss = 0.36128091\n",
      "Iteration 186, loss = 0.41940662\n",
      "Iteration 1397, loss = 0.20692117\n",
      "Iteration 2460, loss = 0.14515937\n",
      "Iteration 1033, loss = 0.28345411\n",
      "Iteration 483, loss = 0.36123286\n",
      "Iteration 1671, loss = 0.16528481\n",
      "Iteration 1760, loss = 0.21202909\n",
      "Iteration 1398, loss = 0.20666192\n",
      "Iteration 187, loss = 0.41882349\n",
      "Iteration 1034, loss = 0.28334118\n",
      "Iteration 2461, loss = 0.14500425\n",
      "Iteration 712, loss = 0.34204921\n",
      "Iteration 1035, loss = 0.28326702\n",
      "Iteration 1399, loss = 0.20653992\n",
      "Iteration 188, loss = 0.41823116\n",
      "Iteration 1400, loss = 0.20632576\n",
      "Iteration 484, loss = 0.36107102\n",
      "Iteration 1401, loss = 0.20620177\n",
      "Iteration 2462, loss = 0.14496930\n",
      "Iteration 1761, loss = 0.21204209\n",
      "Iteration 1402, loss = 0.20599419\n",
      "Iteration 1036, loss = 0.28317763\n",
      "Iteration 1403, loss = 0.20590346\n",
      "Iteration 1404, loss = 0.20570562\n",
      "Iteration 189, loss = 0.41769628\n",
      "Iteration 713, loss = 0.34191116\n",
      "Iteration 1405, loss = 0.20560726\n",
      "Iteration 1672, loss = 0.16520980\n",
      "Iteration 1037, loss = 0.28308104\n",
      "Iteration 1406, loss = 0.20536891\n",
      "Iteration 1762, loss = 0.21183188\n",
      "Iteration 485, loss = 0.36097695\n",
      "Iteration 2463, loss = 0.14501759\n",
      "Iteration 1407, loss = 0.20522510\n",
      "Iteration 1408, loss = 0.20502788\n",
      "Iteration 1038, loss = 0.28297873\n",
      "Iteration 1409, loss = 0.20484984\n",
      "Iteration 190, loss = 0.41711835\n",
      "Iteration 2464, loss = 0.14489098\n",
      "Iteration 1763, loss = 0.21170373\n",
      "Iteration 1410, loss = 0.20469973\n",
      "Iteration 486, loss = 0.36084216\n",
      "Iteration 1039, loss = 0.28291516\n",
      "Iteration 1673, loss = 0.16512285\n",
      "Iteration 714, loss = 0.34175085\n",
      "Iteration 1411, loss = 0.20465525\n",
      "Iteration 2465, loss = 0.14477763\n",
      "Iteration 1412, loss = 0.20446588\n",
      "Iteration 487, loss = 0.36071986\n",
      "Iteration 1413, loss = 0.20423378\n",
      "Iteration 1040, loss = 0.28279902\n",
      "Iteration 191, loss = 0.41659124\n",
      "Iteration 1764, loss = 0.21166089\n",
      "Iteration 1414, loss = 0.20403343\n",
      "Iteration 1041, loss = 0.28270699\n",
      "Iteration 2466, loss = 0.14470630\n",
      "Iteration 1415, loss = 0.20399493\n",
      "Iteration 488, loss = 0.36064316\n",
      "Iteration 1042, loss = 0.28258558\n",
      "Iteration 1674, loss = 0.16503538\n",
      "Iteration 1416, loss = 0.20366195\n",
      "Iteration 192, loss = 0.41603956\n",
      "Iteration 1043, loss = 0.28250004\n",
      "Iteration 1417, loss = 0.20357833\n",
      "Iteration 1418, loss = 0.20335384\n",
      "Iteration 1765, loss = 0.21145491\n",
      "Iteration 715, loss = 0.34156283\n",
      "Iteration 1419, loss = 0.20339644\n",
      "Iteration 1044, loss = 0.28240406\n",
      "Iteration 2467, loss = 0.14467326\n",
      "Iteration 489, loss = 0.36050804\n",
      "Iteration 1420, loss = 0.20301457\n",
      "Iteration 193, loss = 0.41551026\n",
      "Iteration 1421, loss = 0.20279837\n",
      "Iteration 1766, loss = 0.21136777\n",
      "Iteration 1422, loss = 0.20265950\n",
      "Iteration 1045, loss = 0.28234421\n",
      "Iteration 1675, loss = 0.16498205\n",
      "Iteration 490, loss = 0.36038604\n",
      "Iteration 1423, loss = 0.20277587\n",
      "Iteration 2468, loss = 0.14459884\n",
      "Iteration 194, loss = 0.41495141\n",
      "Iteration 716, loss = 0.34122040\n",
      "Iteration 1046, loss = 0.28223511\n",
      "Iteration 1767, loss = 0.21123433\n",
      "Iteration 491, loss = 0.36024782\n",
      "Iteration 1424, loss = 0.20228984\n",
      "Iteration 2469, loss = 0.14448616\n",
      "Iteration 195, loss = 0.41445422\n",
      "Iteration 1768, loss = 0.21113029\n",
      "Iteration 717, loss = 0.34093164\n",
      "Iteration 1425, loss = 0.20221377\n",
      "Iteration 1047, loss = 0.28213231\n",
      "Iteration 1676, loss = 0.16509582\n",
      "Iteration 1426, loss = 0.20195313\n",
      "Iteration 492, loss = 0.36013716\n",
      "Iteration 1427, loss = 0.20184214\n",
      "Iteration 196, loss = 0.41405367\n",
      "Iteration 1769, loss = 0.21124671\n",
      "Iteration 2470, loss = 0.14446780\n",
      "Iteration 197, loss = 0.41345263\n",
      "Iteration 1048, loss = 0.28203838\n",
      "Iteration 1428, loss = 0.20169951\n",
      "Iteration 2471, loss = 0.14432397\n",
      "Iteration 1429, loss = 0.20155834\n",
      "Iteration 493, loss = 0.36001019\n",
      "Iteration 718, loss = 0.34072811\n",
      "Iteration 1430, loss = 0.20130458\n",
      "Iteration 1431, loss = 0.20118272\n",
      "Iteration 1049, loss = 0.28192817\n",
      "Iteration 198, loss = 0.41295984\n",
      "Iteration 1432, loss = 0.20096250\n",
      "Iteration 494, loss = 0.35993212\n",
      "Iteration 1677, loss = 0.16492723\n",
      "Iteration 1433, loss = 0.20075486\n",
      "Iteration 1434, loss = 0.20069468\n",
      "Iteration 1770, loss = 0.21090076\n",
      "Iteration 199, loss = 0.41245963\n",
      "Iteration 1435, loss = 0.20065903\n",
      "Iteration 1050, loss = 0.28184804\n",
      "Iteration 1436, loss = 0.20035578\n",
      "Iteration 2472, loss = 0.14436188\n",
      "Iteration 495, loss = 0.35978490\n",
      "Iteration 1437, loss = 0.20031116\n",
      "Iteration 719, loss = 0.34050647\n",
      "Iteration 200, loss = 0.41196231\n",
      "Iteration 1438, loss = 0.20003039\n",
      "Iteration 1439, loss = 0.19981666\n",
      "Iteration 1051, loss = 0.28182847\n",
      "Iteration 496, loss = 0.35966819\n",
      "Iteration 1771, loss = 0.21077581\n",
      "Iteration 201, loss = 0.41152250\n",
      "Iteration 2473, loss = 0.14423368\n",
      "Iteration 1440, loss = 0.19968890\n",
      "Iteration 1678, loss = 0.16477672\n",
      "Iteration 720, loss = 0.34042408\n",
      "Iteration 1441, loss = 0.19948414\n",
      "Iteration 497, loss = 0.35955247\n",
      "Iteration 1442, loss = 0.19934704\n",
      "Iteration 1772, loss = 0.21066884\n",
      "Iteration 1052, loss = 0.28167149\n",
      "Iteration 1443, loss = 0.19914216\n",
      "Iteration 1444, loss = 0.19896560\n",
      "Iteration 1445, loss = 0.19890951\n",
      "Iteration 202, loss = 0.41106650\n",
      "Iteration 1446, loss = 0.19867214\n",
      "Iteration 1679, loss = 0.16478583\n",
      "Iteration 498, loss = 0.35944129\n",
      "Iteration 1447, loss = 0.19850002\n",
      "Iteration 1053, loss = 0.28157846\n",
      "Iteration 1448, loss = 0.19836873\n",
      "Iteration 1773, loss = 0.21059031\n",
      "Iteration 499, loss = 0.35939034\n",
      "Iteration 721, loss = 0.34009358\n",
      "Iteration 1449, loss = 0.19826949\n",
      "Iteration 2474, loss = 0.14411494\n",
      "Iteration 203, loss = 0.41055010\n",
      "Iteration 1450, loss = 0.19798606\n",
      "Iteration 1054, loss = 0.28144919\n",
      "Iteration 1774, loss = 0.21043712\n",
      "Iteration 500, loss = 0.35921651\n",
      "Iteration 1680, loss = 0.16453870\n",
      "Iteration 1451, loss = 0.19780688\n",
      "Iteration 1055, loss = 0.28135137\n",
      "Iteration 1452, loss = 0.19767199\n",
      "Iteration 204, loss = 0.41017482\n",
      "Iteration 1056, loss = 0.28125221\n",
      "Iteration 722, loss = 0.33980450\n",
      "Iteration 1453, loss = 0.19757420\n",
      "Iteration 1775, loss = 0.21045931\n",
      "Iteration 1681, loss = 0.16463362\n",
      "Iteration 1057, loss = 0.28121047\n",
      "Iteration 1454, loss = 0.19753153\n",
      "Iteration 2475, loss = 0.14414841\n",
      "Iteration 1455, loss = 0.19724503\n",
      "Iteration 1058, loss = 0.28106890\n",
      "Iteration 205, loss = 0.40974936\n",
      "Iteration 501, loss = 0.35908651\n",
      "Iteration 1456, loss = 0.19701864\n",
      "Iteration 1776, loss = 0.21028111\n",
      "Iteration 1457, loss = 0.19681695\n",
      "Iteration 723, loss = 0.33958422\n",
      "Iteration 1682, loss = 0.16442626\n",
      "Iteration 206, loss = 0.40920244\n",
      "Iteration 1059, loss = 0.28102664\n",
      "Iteration 1777, loss = 0.21010652\n",
      "Iteration 1458, loss = 0.19665000\n",
      "Iteration 502, loss = 0.35898439\n",
      "Iteration 2476, loss = 0.14407280\n",
      "Iteration 1060, loss = 0.28089254\n",
      "Iteration 1459, loss = 0.19649750\n",
      "Iteration 503, loss = 0.35887598\n",
      "Iteration 207, loss = 0.40875693\n",
      "Iteration 2477, loss = 0.14409841\n",
      "Iteration 1778, loss = 0.21003761\n",
      "Iteration 1061, loss = 0.28080721\n",
      "Iteration 1460, loss = 0.19635707\n",
      "Iteration 724, loss = 0.33934156\n",
      "Iteration 1461, loss = 0.19630510\n",
      "Iteration 208, loss = 0.40834046\n",
      "Iteration 1462, loss = 0.19621515\n",
      "Iteration 1062, loss = 0.28070633\n",
      "Iteration 1463, loss = 0.19590540\n",
      "Iteration 209, loss = 0.40790334\n",
      "Iteration 1464, loss = 0.19578224\n",
      "Iteration 504, loss = 0.35876146\n",
      "Iteration 1465, loss = 0.19564655\n",
      "Iteration 1063, loss = 0.28066381\n",
      "Iteration 1466, loss = 0.19538217\n",
      "Iteration 1779, loss = 0.20987496\n",
      "Iteration 725, loss = 0.33927699\n",
      "Iteration 1467, loss = 0.19536840\n",
      "Iteration 1683, loss = 0.16427953\n",
      "Iteration 1064, loss = 0.28049796\n",
      "Iteration 1468, loss = 0.19503886\n",
      "Iteration 1065, loss = 0.28041703\n",
      "Iteration 210, loss = 0.40749692\n",
      "Iteration 1469, loss = 0.19513116\n",
      "Iteration 1470, loss = 0.19468027\n",
      "Iteration 1066, loss = 0.28034574\n",
      "Iteration 2478, loss = 0.14389382\n",
      "Iteration 1684, loss = 0.16425764\n",
      "Iteration 1067, loss = 0.28025864\n",
      "Iteration 726, loss = 0.33896560\n",
      "Iteration 2479, loss = 0.14384703\n",
      "Iteration 1068, loss = 0.28012008\n",
      "Iteration 1471, loss = 0.19461499\n",
      "Iteration 1780, loss = 0.20977008\n",
      "Iteration 211, loss = 0.40706337\n",
      "Iteration 1472, loss = 0.19432503\n",
      "Iteration 1069, loss = 0.28005080\n",
      "Iteration 1473, loss = 0.19417398\n",
      "Iteration 505, loss = 0.35862996\n",
      "Iteration 1474, loss = 0.19401250\n",
      "Iteration 2480, loss = 0.14373045\n",
      "Iteration 1070, loss = 0.27992233\n",
      "Iteration 1475, loss = 0.19383126\n",
      "Iteration 727, loss = 0.33867215\n",
      "Iteration 212, loss = 0.40668117\n",
      "Iteration 1781, loss = 0.20966828\n",
      "Iteration 1476, loss = 0.19377630\n",
      "Iteration 1071, loss = 0.27991015\n",
      "Iteration 1685, loss = 0.16406510\n",
      "Iteration 506, loss = 0.35863876\n",
      "Iteration 1477, loss = 0.19353958\n",
      "Iteration 1072, loss = 0.27975272\n",
      "Iteration 1478, loss = 0.19331544\n",
      "Iteration 1782, loss = 0.20957125\n",
      "Iteration 213, loss = 0.40625481\n",
      "Iteration 1073, loss = 0.27963231\n",
      "Iteration 507, loss = 0.35843256\n",
      "Iteration 2481, loss = 0.14365920\n",
      "Iteration 1479, loss = 0.19328356\n",
      "Iteration 1480, loss = 0.19313505\n",
      "Iteration 1686, loss = 0.16410656\n",
      "Iteration 508, loss = 0.35827348\n",
      "Iteration 1481, loss = 0.19286346\n",
      "Iteration 1074, loss = 0.27957691\n",
      "Iteration 728, loss = 0.33846224\n",
      "Iteration 1783, loss = 0.20940013\n",
      "Iteration 214, loss = 0.40590747\n",
      "Iteration 1482, loss = 0.19275132\n",
      "Iteration 1483, loss = 0.19259851\n",
      "Iteration 1075, loss = 0.27945344\n",
      "Iteration 729, loss = 0.33823205\n",
      "Iteration 1076, loss = 0.27940222\n",
      "Iteration 1484, loss = 0.19231636\n",
      "Iteration 215, loss = 0.40545408\n",
      "Iteration 509, loss = 0.35817496\n",
      "Iteration 2482, loss = 0.14367458\n",
      "Iteration 1485, loss = 0.19213962\n",
      "Iteration 1687, loss = 0.16405490\n",
      "Iteration 1486, loss = 0.19198232\n",
      "Iteration 1077, loss = 0.27924206\n",
      "Iteration 1487, loss = 0.19194073\n",
      "Iteration 510, loss = 0.35806601\n",
      "Iteration 730, loss = 0.33804176\n",
      "Iteration 1784, loss = 0.20931731\n",
      "Iteration 1078, loss = 0.27916259\n",
      "Iteration 216, loss = 0.40506133\n",
      "Iteration 1488, loss = 0.19169980\n",
      "Iteration 2483, loss = 0.14355725\n",
      "Iteration 1489, loss = 0.19153364\n",
      "Iteration 1785, loss = 0.20918709\n",
      "Iteration 1490, loss = 0.19133670\n",
      "Iteration 1688, loss = 0.16383408\n",
      "Iteration 1079, loss = 0.27907915\n",
      "Iteration 511, loss = 0.35799757\n",
      "Iteration 217, loss = 0.40463217\n",
      "Iteration 1491, loss = 0.19126070\n",
      "Iteration 1492, loss = 0.19103428\n",
      "Iteration 731, loss = 0.33779455\n",
      "Iteration 2484, loss = 0.14351189\n",
      "Iteration 1493, loss = 0.19087318\n",
      "Iteration 1494, loss = 0.19085318\n",
      "Iteration 1786, loss = 0.20910828\n",
      "Iteration 1080, loss = 0.27900395\n",
      "Iteration 512, loss = 0.35792062\n",
      "Iteration 218, loss = 0.40432765\n",
      "Iteration 1689, loss = 0.16387787\n",
      "Iteration 1495, loss = 0.19063191\n",
      "Iteration 1081, loss = 0.27888023\n",
      "Iteration 2485, loss = 0.14344102\n",
      "Iteration 1496, loss = 0.19045677\n",
      "Iteration 1787, loss = 0.20895560\n",
      "Iteration 1082, loss = 0.27881028\n",
      "Iteration 513, loss = 0.35775731\n",
      "Iteration 1497, loss = 0.19025078\n",
      "Iteration 219, loss = 0.40394794\n",
      "Iteration 732, loss = 0.33762706\n",
      "Iteration 1498, loss = 0.19011121\n",
      "Iteration 1788, loss = 0.20890857\n",
      "Iteration 514, loss = 0.35760446\n",
      "Iteration 1690, loss = 0.16367480\n",
      "Iteration 1499, loss = 0.18982307\n",
      "Iteration 1083, loss = 0.27869261\n",
      "Iteration 220, loss = 0.40354146\n",
      "Iteration 733, loss = 0.33740204\n",
      "Iteration 1500, loss = 0.18965487\n",
      "Iteration 1789, loss = 0.20874977\n",
      "Iteration 2486, loss = 0.14339281\n",
      "Iteration 1084, loss = 0.27858328\n",
      "Iteration 515, loss = 0.35752603\n",
      "Iteration 1501, loss = 0.18951585\n",
      "Iteration 1502, loss = 0.18947496\n",
      "Iteration 221, loss = 0.40312626\n",
      "Iteration 1503, loss = 0.18923933\n",
      "Iteration 1085, loss = 0.27859136\n",
      "Iteration 734, loss = 0.33712081\n",
      "Iteration 1691, loss = 0.16359091\n",
      "Iteration 1790, loss = 0.20867144\n",
      "Iteration 1504, loss = 0.18904026\n",
      "Iteration 1086, loss = 0.27840455\n",
      "Iteration 516, loss = 0.35739747\n",
      "Iteration 2487, loss = 0.14330032\n",
      "Iteration 1505, loss = 0.18888923\n",
      "Iteration 222, loss = 0.40279540\n",
      "Iteration 1087, loss = 0.27832376\n",
      "Iteration 1506, loss = 0.18864255\n",
      "Iteration 1088, loss = 0.27826404\n",
      "Iteration 1507, loss = 0.18853968\n",
      "Iteration 735, loss = 0.33691295\n",
      "Iteration 1508, loss = 0.18839311\n",
      "Iteration 1791, loss = 0.20855472\n",
      "Iteration 1509, loss = 0.18817969\n",
      "Iteration 1089, loss = 0.27811802\n",
      "Iteration 517, loss = 0.35725977\n",
      "Iteration 1510, loss = 0.18809971\n",
      "Iteration 223, loss = 0.40243372\n",
      "Iteration 1792, loss = 0.20846900\n",
      "Iteration 1692, loss = 0.16351357\n",
      "Iteration 224, loss = 0.40206012\n",
      "Iteration 1793, loss = 0.20841435\n",
      "Iteration 518, loss = 0.35716017Iteration 2488, loss = 0.14323397\n",
      "\n",
      "Iteration 225, loss = 0.40171374\n",
      "Iteration 1511, loss = 0.18792846\n",
      "Iteration 1090, loss = 0.27803040\n",
      "Iteration 226, loss = 0.40134222\n",
      "Iteration 1512, loss = 0.18787163\n",
      "Iteration 2489, loss = 0.14322660\n",
      "Iteration 1091, loss = 0.27793121\n",
      "Iteration 1513, loss = 0.18749021\n",
      "Iteration 736, loss = 0.33669397\n",
      "Iteration 1514, loss = 0.18734525\n",
      "Iteration 1092, loss = 0.27782912Iteration 1794, loss = 0.20841332\n",
      "\n",
      "Iteration 2490, loss = 0.14316131\n",
      "Iteration 1093, loss = 0.27776177\n",
      "Iteration 1515, loss = 0.18712794\n",
      "Iteration 1795, loss = 0.20810083\n",
      "Iteration 227, loss = 0.40107492\n",
      "Iteration 1693, loss = 0.16337105\n",
      "Iteration 519, loss = 0.35709004\n",
      "Iteration 2491, loss = 0.14316954\n",
      "Iteration 1516, loss = 0.18701539\n",
      "Iteration 1094, loss = 0.27763989\n",
      "Iteration 228, loss = 0.40066753\n",
      "Iteration 1095, loss = 0.27753477\n",
      "Iteration 1517, loss = 0.18696479\n",
      "Iteration 2492, loss = 0.14303648\n",
      "Iteration 229, loss = 0.40030312\n",
      "Iteration 737, loss = 0.33643806\n",
      "Iteration 1796, loss = 0.20795247\n",
      "Iteration 1096, loss = 0.27745845\n",
      "Iteration 1518, loss = 0.18667024\n",
      "Iteration 520, loss = 0.35691150\n",
      "Iteration 2493, loss = 0.14289447\n",
      "Iteration 1097, loss = 0.27738494\n",
      "Iteration 1519, loss = 0.18663575\n",
      "Iteration 1797, loss = 0.20796607\n",
      "Iteration 2494, loss = 0.14288094\n",
      "Iteration 1098, loss = 0.27729209\n",
      "Iteration 1694, loss = 0.16328660\n",
      "Iteration 1099, loss = 0.27720999\n",
      "Iteration 2495, loss = 0.14282098\n",
      "Iteration 1520, loss = 0.18631124\n",
      "Iteration 230, loss = 0.39996991\n",
      "Iteration 1798, loss = 0.20773106\n",
      "Iteration 738, loss = 0.33624628\n",
      "Iteration 1521, loss = 0.18644515\n",
      "Iteration 1100, loss = 0.27704775\n",
      "Iteration 1695, loss = 0.16331936\n",
      "Iteration 1101, loss = 0.27703626\n",
      "Iteration 1522, loss = 0.18608504\n",
      "Iteration 1523, loss = 0.18586599\n",
      "Iteration 1102, loss = 0.27690965\n",
      "Iteration 231, loss = 0.39962896\n",
      "Iteration 1799, loss = 0.20763828\n",
      "Iteration 521, loss = 0.35679322\n",
      "Iteration 2496, loss = 0.14278176\n",
      "Iteration 1524, loss = 0.18568931\n",
      "Iteration 1103, loss = 0.27675619\n",
      "Iteration 1525, loss = 0.18567498\n",
      "Iteration 739, loss = 0.33601756\n",
      "Iteration 232, loss = 0.39930649\n",
      "Iteration 1800, loss = 0.20757938\n",
      "Iteration 522, loss = 0.35672712\n",
      "Iteration 1526, loss = 0.18551317\n",
      "Iteration 1696, loss = 0.16328705\n",
      "Iteration 1104, loss = 0.27666663\n",
      "Iteration 1801, loss = 0.20743134\n",
      "Iteration 233, loss = 0.39899756\n",
      "Iteration 1527, loss = 0.18521322\n",
      "Iteration 1528, loss = 0.18503697\n",
      "Iteration 2497, loss = 0.14274402\n",
      "Iteration 1105, loss = 0.27658708\n",
      "Iteration 1529, loss = 0.18483414\n",
      "Iteration 523, loss = 0.35658637\n",
      "Iteration 1530, loss = 0.18471736\n",
      "Iteration 234, loss = 0.39863958\n",
      "Iteration 1531, loss = 0.18454802\n",
      "Iteration 1802, loss = 0.20726725\n",
      "Iteration 524, loss = 0.35646526\n",
      "Iteration 1532, loss = 0.18439633\n",
      "Iteration 235, loss = 0.39836054\n",
      "Iteration 1697, loss = 0.16310323\n",
      "Iteration 1106, loss = 0.27649015\n",
      "Iteration 1533, loss = 0.18418930\n",
      "Iteration 2498, loss = 0.14266575\n",
      "Iteration 740, loss = 0.33576695\n",
      "Iteration 1534, loss = 0.18415531\n",
      "Iteration 1535, loss = 0.18394787\n",
      "Iteration 1536, loss = 0.18371609\n",
      "Iteration 1803, loss = 0.20718042\n",
      "Iteration 1107, loss = 0.27639090\n",
      "Iteration 2499, loss = 0.14254908\n",
      "Iteration 1537, loss = 0.18356253\n",
      "Iteration 525, loss = 0.35633631\n",
      "Iteration 1538, loss = 0.18335982\n",
      "Iteration 1108, loss = 0.27632799\n",
      "Iteration 1539, loss = 0.18346417\n",
      "Iteration 236, loss = 0.39798549\n",
      "Iteration 1540, loss = 0.18301812\n",
      "Iteration 1698, loss = 0.16300288\n",
      "Iteration 526, loss = 0.35624658\n",
      "Iteration 2500, loss = 0.14246622\n",
      "Iteration 1541, loss = 0.18298519\n",
      "Iteration 1804, loss = 0.20709465\n",
      "Iteration 1542, loss = 0.18273872\n",
      "Iteration 1543, loss = 0.18250186\n",
      "Iteration 741, loss = 0.33555557\n",
      "Iteration 1544, loss = 0.18232652\n",
      "Iteration 1109, loss = 0.27618805\n",
      "Iteration 527, loss = 0.35617110\n",
      "Iteration 2501, loss = 0.14240222\n",
      "Iteration 237, loss = 0.39765851\n",
      "Iteration 1545, loss = 0.18228270\n",
      "Iteration 1546, loss = 0.18216729\n",
      "Iteration 2502, loss = 0.14244054\n",
      "Iteration 1805, loss = 0.20697933\n",
      "Iteration 1110, loss = 0.27610301\n",
      "Iteration 1699, loss = 0.16304470\n",
      "Iteration 238, loss = 0.39737518\n",
      "Iteration 1547, loss = 0.18198470\n",
      "Iteration 528, loss = 0.35597774\n",
      "Iteration 1111, loss = 0.27606417\n",
      "Iteration 1548, loss = 0.18180513\n",
      "Iteration 1806, loss = 0.20692070\n",
      "Iteration 742, loss = 0.33529788\n",
      "Iteration 1549, loss = 0.18149851\n",
      "Iteration 2503, loss = 0.14226122\n",
      "Iteration 1550, loss = 0.18150535\n",
      "Iteration 529, loss = 0.35590240\n",
      "Iteration 239, loss = 0.39708166\n",
      "Iteration 1551, loss = 0.18120467\n",
      "Iteration 1552, loss = 0.18110035\n",
      "Iteration 240, loss = 0.39679094\n",
      "Iteration 1807, loss = 0.20692940\n",
      "Iteration 1112, loss = 0.27591334\n",
      "Iteration 1553, loss = 0.18083316\n",
      "Iteration 1700, loss = 0.16281690\n",
      "Iteration 1554, loss = 0.18069386\n",
      "Iteration 241, loss = 0.39646314\n",
      "Iteration 1113, loss = 0.27581596\n",
      "Iteration 2504, loss = 0.14220553\n",
      "Iteration 1555, loss = 0.18057667\n",
      "Iteration 1556, loss = 0.18039668\n",
      "Iteration 1114, loss = 0.27571861\n",
      "Iteration 743, loss = 0.33509143\n",
      "Iteration 1557, loss = 0.18023842\n",
      "Iteration 530, loss = 0.35575020\n",
      "Iteration 1558, loss = 0.18002898\n",
      "Iteration 242, loss = 0.39613650\n",
      "Iteration 1559, loss = 0.17988269\n",
      "Iteration 1560, loss = 0.17973770\n",
      "Iteration 2505, loss = 0.14213753\n",
      "Iteration 1808, loss = 0.20662498\n",
      "Iteration 1115, loss = 0.27565730\n",
      "Iteration 1561, loss = 0.17955290\n",
      "Iteration 531, loss = 0.35567885\n",
      "Iteration 1562, loss = 0.17944621\n",
      "Iteration 1701, loss = 0.16276398\n",
      "Iteration 1116, loss = 0.27555222\n",
      "Iteration 1563, loss = 0.17923641\n",
      "Iteration 1117, loss = 0.27546090\n",
      "Iteration 243, loss = 0.39582012\n",
      "Iteration 744, loss = 0.33488560\n",
      "Iteration 1118, loss = 0.27537920\n",
      "Iteration 1809, loss = 0.20655035\n",
      "Iteration 244, loss = 0.39557618\n",
      "Iteration 1564, loss = 0.17921219\n",
      "Iteration 2506, loss = 0.14210058\n",
      "Iteration 1119, loss = 0.27523382\n",
      "Iteration 1565, loss = 0.17890751\n",
      "Iteration 245, loss = 0.39526854\n",
      "Iteration 532, loss = 0.35554840\n",
      "Iteration 1120, loss = 0.27514693\n",
      "Iteration 1566, loss = 0.17874459\n",
      "Iteration 1810, loss = 0.20643029\n",
      "Iteration 745, loss = 0.33464233\n",
      "Iteration 1702, loss = 0.16261822\n",
      "Iteration 2507, loss = 0.14203857\n",
      "Iteration 1567, loss = 0.17861152\n",
      "Iteration 246, loss = 0.39506725\n",
      "Iteration 1121, loss = 0.27506707\n",
      "Iteration 1568, loss = 0.17842737\n",
      "Iteration 247, loss = 0.39466869\n",
      "Iteration 2508, loss = 0.14198926\n",
      "Iteration 1569, loss = 0.17838218\n",
      "Iteration 746, loss = 0.33439920\n",
      "Iteration 1122, loss = 0.27493237\n",
      "Iteration 1570, loss = 0.17831718\n",
      "Iteration 1811, loss = 0.20633309\n",
      "Iteration 248, loss = 0.39437534\n",
      "Iteration 533, loss = 0.35540778\n",
      "Iteration 1123, loss = 0.27485849\n",
      "Iteration 1571, loss = 0.17821121\n",
      "Iteration 1124, loss = 0.27474294\n",
      "Iteration 747, loss = 0.33435600\n",
      "Iteration 2509, loss = 0.14201252\n",
      "Iteration 1572, loss = 0.17776286\n",
      "Iteration 1703, loss = 0.16261217\n",
      "Iteration 1125, loss = 0.27466651\n",
      "Iteration 1573, loss = 0.17778621\n",
      "Iteration 1574, loss = 0.17747706\n",
      "Iteration 534, loss = 0.35535991\n",
      "Iteration 249, loss = 0.39411624\n",
      "Iteration 1812, loss = 0.20624046\n",
      "Iteration 250, loss = 0.39383811\n",
      "Iteration 2510, loss = 0.14187664\n",
      "Iteration 1704, loss = 0.16259319\n",
      "Iteration 535, loss = 0.35522346\n",
      "Iteration 1126, loss = 0.27455471\n",
      "Iteration 1575, loss = 0.17732137\n",
      "Iteration 1127, loss = 0.27444754\n",
      "Iteration 2511, loss = 0.14183300\n",
      "Iteration 1576, loss = 0.17718305\n",
      "Iteration 251, loss = 0.39353556\n",
      "Iteration 748, loss = 0.33399072\n",
      "Iteration 1128, loss = 0.27436790\n",
      "Iteration 536, loss = 0.35510613\n",
      "Iteration 1577, loss = 0.17700212\n",
      "Iteration 1813, loss = 0.20610909\n",
      "Iteration 2512, loss = 0.14172540\n",
      "Iteration 1578, loss = 0.17687038\n",
      "Iteration 252, loss = 0.39325103\n",
      "Iteration 749, loss = 0.33374532\n",
      "Iteration 1129, loss = 0.27430866\n",
      "Iteration 537, loss = 0.35493070\n",
      "Iteration 1579, loss = 0.17672785\n",
      "Iteration 1705, loss = 0.16276844\n",
      "Iteration 253, loss = 0.39298113\n",
      "Iteration 2513, loss = 0.14175852\n",
      "Iteration 1580, loss = 0.17654883\n",
      "Iteration 1814, loss = 0.20600412\n",
      "Iteration 1581, loss = 0.17640141\n",
      "Iteration 1130, loss = 0.27417410\n",
      "Iteration 1582, loss = 0.17623856\n",
      "Iteration 1583, loss = 0.17605927\n",
      "Iteration 538, loss = 0.35490130\n",
      "Iteration 1706, loss = 0.16232612\n",
      "Iteration 254, loss = 0.39272681\n",
      "Iteration 1584, loss = 0.17601220\n",
      "Iteration 1131, loss = 0.27406960\n",
      "Iteration 2514, loss = 0.14158307\n",
      "Iteration 1815, loss = 0.20592420\n",
      "Iteration 1585, loss = 0.17573278\n",
      "Iteration 539, loss = 0.35477442\n",
      "Iteration 1132, loss = 0.27398620\n",
      "Iteration 255, loss = 0.39242213\n",
      "Iteration 750, loss = 0.33354221\n",
      "Iteration 1586, loss = 0.17553896\n",
      "Iteration 1707, loss = 0.16222931\n",
      "Iteration 1816, loss = 0.20574414\n",
      "Iteration 1587, loss = 0.17553758\n",
      "Iteration 540, loss = 0.35465325\n",
      "Iteration 1133, loss = 0.27393150\n",
      "Iteration 1588, loss = 0.17529136\n",
      "Iteration 256, loss = 0.39214060\n",
      "Iteration 2515, loss = 0.14150796\n",
      "Iteration 1134, loss = 0.27385114\n",
      "Iteration 1589, loss = 0.17514922\n",
      "Iteration 1817, loss = 0.20571587\n",
      "Iteration 1135, loss = 0.27370840\n",
      "Iteration 2516, loss = 0.14144596\n",
      "Iteration 1136, loss = 0.27365300\n",
      "Iteration 1590, loss = 0.17493348\n",
      "Iteration 1708, loss = 0.16212494\n",
      "Iteration 257, loss = 0.39185444\n",
      "Iteration 1591, loss = 0.17485274\n",
      "Iteration 1592, loss = 0.17462256\n",
      "Iteration 751, loss = 0.33339684\n",
      "Iteration 1137, loss = 0.27349406\n",
      "Iteration 2517, loss = 0.14139614\n",
      "Iteration 541, loss = 0.35451511\n",
      "Iteration 1138, loss = 0.27342535\n",
      "Iteration 258, loss = 0.39159234\n",
      "Iteration 1818, loss = 0.20552996\n",
      "Iteration 1593, loss = 0.17442209\n",
      "Iteration 1139, loss = 0.27330804\n",
      "Iteration 1140, loss = 0.27319943\n",
      "Iteration 1594, loss = 0.17427893\n",
      "Iteration 542, loss = 0.35439546\n",
      "Iteration 1595, loss = 0.17411533\n",
      "Iteration 1141, loss = 0.27313931\n",
      "Iteration 1596, loss = 0.17391120\n",
      "Iteration 259, loss = 0.39132831\n",
      "Iteration 1597, loss = 0.17375571\n",
      "Iteration 1709, loss = 0.16208665\n",
      "Iteration 2518, loss = 0.14133813\n",
      "Iteration 1598, loss = 0.17363316\n",
      "Iteration 1142, loss = 0.27302692Iteration 1599, loss = 0.17337477\n",
      "\n",
      "Iteration 752, loss = 0.33315432\n",
      "Iteration 543, loss = 0.35427868\n",
      "Iteration 1600, loss = 0.17328754\n",
      "Iteration 2519, loss = 0.14134631\n",
      "Iteration 1819, loss = 0.20548036\n",
      "Iteration 1601, loss = 0.17311901\n",
      "Iteration 1143, loss = 0.27291161\n",
      "Iteration 1602, loss = 0.17294806\n",
      "Iteration 1603, loss = 0.17292288\n",
      "Iteration 1604, loss = 0.17259214\n",
      "Iteration 1710, loss = 0.16217346\n",
      "Iteration 260, loss = 0.39121206\n",
      "Iteration 1605, loss = 0.17244069\n",
      "Iteration 1820, loss = 0.20541131\n",
      "Iteration 2520, loss = 0.14125973\n",
      "Iteration 1606, loss = 0.17227201\n",
      "Iteration 753, loss = 0.33285391\n",
      "Iteration 1144, loss = 0.27283713\n",
      "Iteration 1607, loss = 0.17211024\n",
      "Iteration 1821, loss = 0.20523162\n",
      "Iteration 1608, loss = 0.17189370\n",
      "Iteration 1711, loss = 0.16217597\n",
      "Iteration 2521, loss = 0.14112036\n",
      "Iteration 1609, loss = 0.17188729\n",
      "Iteration 1610, loss = 0.17170720\n",
      "Iteration 261, loss = 0.39082208\n",
      "Iteration 1145, loss = 0.27272015\n",
      "Iteration 544, loss = 0.35414393\n",
      "Iteration 1822, loss = 0.20511221\n",
      "Iteration 2522, loss = 0.14120851\n",
      "Iteration 1146, loss = 0.27263639\n",
      "Iteration 262, loss = 0.39050116\n",
      "Iteration 1611, loss = 0.17144187\n",
      "Iteration 754, loss = 0.33271476\n",
      "Iteration 1612, loss = 0.17130164\n",
      "Iteration 263, loss = 0.39029418\n",
      "Iteration 1147, loss = 0.27258870\n",
      "Iteration 1613, loss = 0.17109448\n",
      "Iteration 545, loss = 0.35404824\n",
      "Iteration 1823, loss = 0.20501022\n",
      "Iteration 1148, loss = 0.27244924\n",
      "Iteration 264, loss = 0.39002368\n",
      "Iteration 1614, loss = 0.17099611\n",
      "Iteration 2523, loss = 0.14099848\n",
      "Iteration 1712, loss = 0.16190096\n",
      "Iteration 1615, loss = 0.17085535\n",
      "Iteration 1616, loss = 0.17062319\n",
      "Iteration 1149, loss = 0.27233613\n",
      "Iteration 1824, loss = 0.20493015\n",
      "Iteration 755, loss = 0.33240837\n",
      "Iteration 1150, loss = 0.27226459\n",
      "Iteration 1617, loss = 0.17048555\n",
      "Iteration 265, loss = 0.38986320\n",
      "Iteration 1713, loss = 0.16179295\n",
      "Iteration 1151, loss = 0.27215124\n",
      "Iteration 546, loss = 0.35390514\n",
      "Iteration 1825, loss = 0.20475597\n",
      "Iteration 266, loss = 0.38944611\n",
      "Iteration 1618, loss = 0.17043894\n",
      "Iteration 2524, loss = 0.14096382\n",
      "Iteration 1619, loss = 0.17010617\n",
      "Iteration 1152, loss = 0.27207774\n",
      "Iteration 1620, loss = 0.16999052\n",
      "Iteration 267, loss = 0.38923387\n",
      "Iteration 756, loss = 0.33220558\n",
      "Iteration 547, loss = 0.35376758\n",
      "Iteration 2525, loss = 0.14091452\n",
      "Iteration 1826, loss = 0.20469239\n",
      "Iteration 268, loss = 0.38893448\n",
      "Iteration 1714, loss = 0.16168254\n",
      "Iteration 1621, loss = 0.16979143\n",
      "Iteration 1153, loss = 0.27195388\n",
      "Iteration 1622, loss = 0.16971569\n",
      "Iteration 269, loss = 0.38872227\n",
      "Iteration 1623, loss = 0.16946327\n",
      "Iteration 548, loss = 0.35370272\n",
      "Iteration 1827, loss = 0.20457104\n",
      "Iteration 1154, loss = 0.27188758\n",
      "Iteration 1624, loss = 0.16942773\n",
      "Iteration 270, loss = 0.38848013\n",
      "Iteration 1625, loss = 0.16931361\n",
      "Iteration 1155, loss = 0.27181271\n",
      "Iteration 2526, loss = 0.14090666\n",
      "Iteration 1626, loss = 0.16898870\n",
      "Iteration 757, loss = 0.33200228\n",
      "Iteration 1828, loss = 0.20447684\n",
      "Iteration 549, loss = 0.35355686\n",
      "Iteration 271, loss = 0.38817200\n",
      "Iteration 1627, loss = 0.16881274\n",
      "Iteration 1156, loss = 0.27173616\n",
      "Iteration 1715, loss = 0.16156217\n",
      "Iteration 1628, loss = 0.16871985\n",
      "Iteration 1157, loss = 0.27156100\n",
      "Iteration 550, loss = 0.35348798\n",
      "Iteration 1629, loss = 0.16862198\n",
      "Iteration 272, loss = 0.38796703\n",
      "Iteration 2527, loss = 0.14077526\n",
      "Iteration 1630, loss = 0.16868187\n",
      "Iteration 758, loss = 0.33182331\n",
      "Iteration 551, loss = 0.35342970\n",
      "Iteration 1158, loss = 0.27148985\n",
      "Iteration 273, loss = 0.38775991\n",
      "Iteration 1631, loss = 0.16824263\n",
      "Iteration 2528, loss = 0.14072463\n",
      "Iteration 1632, loss = 0.16811787\n",
      "Iteration 552, loss = 0.35321180\n",
      "Iteration 1159, loss = 0.27147324\n",
      "Iteration 1829, loss = 0.20432197\n",
      "Iteration 274, loss = 0.38742972\n",
      "Iteration 1633, loss = 0.16786346\n",
      "Iteration 1160, loss = 0.27131785\n",
      "Iteration 1716, loss = 0.16155356\n",
      "Iteration 1634, loss = 0.16768695\n",
      "Iteration 275, loss = 0.38721800\n",
      "Iteration 553, loss = 0.35310516\n",
      "Iteration 759, loss = 0.33154411\n",
      "Iteration 1635, loss = 0.16756108\n",
      "Iteration 276, loss = 0.38694809\n",
      "Iteration 1161, loss = 0.27120827\n",
      "Iteration 1636, loss = 0.16749240\n",
      "Iteration 277, loss = 0.38671244\n",
      "Iteration 1637, loss = 0.16725281\n",
      "Iteration 554, loss = 0.35296627\n",
      "Iteration 2529, loss = 0.14072462\n",
      "Iteration 1830, loss = 0.20424378\n",
      "Iteration 1717, loss = 0.16143307\n",
      "Iteration 1162, loss = 0.27114018\n",
      "Iteration 1638, loss = 0.16712104\n",
      "Iteration 278, loss = 0.38645792\n",
      "Iteration 555, loss = 0.35285366\n",
      "Iteration 760, loss = 0.33132157\n",
      "Iteration 1163, loss = 0.27112811\n",
      "Iteration 1639, loss = 0.16692668\n",
      "Iteration 2530, loss = 0.14061329\n",
      "Iteration 1164, loss = 0.27091047\n",
      "Iteration 279, loss = 0.38622238\n",
      "Iteration 1831, loss = 0.20420878\n",
      "Iteration 1640, loss = 0.16674945\n",
      "Iteration 1641, loss = 0.16663021\n",
      "Iteration 2531, loss = 0.14050473\n",
      "Iteration 1642, loss = 0.16639338\n",
      "Iteration 1643, loss = 0.16636538\n",
      "Iteration 1165, loss = 0.27082656\n",
      "Iteration 1832, loss = 0.20410842\n",
      "Iteration 1644, loss = 0.16615963\n",
      "Iteration 2532, loss = 0.14048058\n",
      "Iteration 280, loss = 0.38597996\n",
      "Iteration 1645, loss = 0.16590532\n",
      "Iteration 1166, loss = 0.27071435\n",
      "Iteration 556, loss = 0.35276810\n",
      "Iteration 1646, loss = 0.16584369\n",
      "Iteration 761, loss = 0.33108083\n",
      "Iteration 1647, loss = 0.16559832\n",
      "Iteration 1167, loss = 0.27067541\n",
      "Iteration 2533, loss = 0.14040634\n",
      "Iteration 1718, loss = 0.16138121\n",
      "Iteration 1648, loss = 0.16551679\n",
      "Iteration 1833, loss = 0.20398432\n",
      "Iteration 1649, loss = 0.16530388\n",
      "Iteration 1650, loss = 0.16526798\n",
      "Iteration 281, loss = 0.38581298\n",
      "Iteration 1651, loss = 0.16495755\n",
      "Iteration 1168, loss = 0.27055379\n",
      "Iteration 557, loss = 0.35261960\n",
      "Iteration 1652, loss = 0.16498948\n",
      "Iteration 1834, loss = 0.20382402\n",
      "Iteration 282, loss = 0.38552283\n",
      "Iteration 1653, loss = 0.16471359\n",
      "Iteration 1169, loss = 0.27043088\n",
      "Iteration 2534, loss = 0.14036265\n",
      "Iteration 1654, loss = 0.16466828\n",
      "Iteration 283, loss = 0.38529773\n",
      "Iteration 1835, loss = 0.20367744\n",
      "Iteration 1170, loss = 0.27036761\n",
      "Iteration 762, loss = 0.33109988\n",
      "Iteration 558, loss = 0.35255841\n",
      "Iteration 1655, loss = 0.16454946\n",
      "Iteration 1719, loss = 0.16125023\n",
      "Iteration 1656, loss = 0.16430177\n",
      "Iteration 1171, loss = 0.27023808\n",
      "Iteration 2535, loss = 0.14027497\n",
      "Iteration 284, loss = 0.38503817\n",
      "Iteration 1836, loss = 0.20359627\n",
      "Iteration 1657, loss = 0.16406522\n",
      "Iteration 559, loss = 0.35240742\n",
      "Iteration 1172, loss = 0.27015497\n",
      "Iteration 1658, loss = 0.16398878\n",
      "Iteration 285, loss = 0.38485753\n",
      "Iteration 1720, loss = 0.16126515\n",
      "Iteration 1837, loss = 0.20345032\n",
      "Iteration 1659, loss = 0.16378013\n",
      "Iteration 1173, loss = 0.27009903\n",
      "Iteration 2536, loss = 0.14020292\n",
      "Iteration 1660, loss = 0.16370366\n",
      "Iteration 763, loss = 0.33074834\n",
      "Iteration 286, loss = 0.38457205\n",
      "Iteration 1661, loss = 0.16344291\n",
      "Iteration 560, loss = 0.35228558\n",
      "Iteration 1838, loss = 0.20336180\n",
      "Iteration 1662, loss = 0.16358932\n",
      "Iteration 1663, loss = 0.16316277\n",
      "Iteration 1174, loss = 0.26999111\n",
      "Iteration 287, loss = 0.38437568\n",
      "Iteration 1664, loss = 0.16297044\n",
      "Iteration 1839, loss = 0.20327635\n",
      "Iteration 2537, loss = 0.14017097\n",
      "Iteration 1665, loss = 0.16283551\n",
      "Iteration 1175, loss = 0.26988289\n",
      "Iteration 1666, loss = 0.16279971\n",
      "Iteration 1721, loss = 0.16118887\n",
      "Iteration 288, loss = 0.38411379\n",
      "Iteration 1176, loss = 0.26980177\n",
      "Iteration 2538, loss = 0.14011366\n",
      "Iteration 1667, loss = 0.16263896\n",
      "Iteration 764, loss = 0.33048122\n",
      "Iteration 561, loss = 0.35217238\n",
      "Iteration 1840, loss = 0.20331109\n",
      "Iteration 1668, loss = 0.16240513\n",
      "Iteration 1177, loss = 0.26970121\n",
      "Iteration 1669, loss = 0.16222395\n",
      "Iteration 1178, loss = 0.26958951\n",
      "Iteration 1670, loss = 0.16203291\n",
      "Iteration 1671, loss = 0.16190716\n",
      "Iteration 1841, loss = 0.20311958\n",
      "Iteration 1179, loss = 0.26950533\n",
      "Iteration 289, loss = 0.38388454\n",
      "Iteration 1672, loss = 0.16175276\n",
      "Iteration 1673, loss = 0.16158270\n",
      "Iteration 1180, loss = 0.26939278\n",
      "Iteration 2539, loss = 0.14007863\n",
      "Iteration 765, loss = 0.33016650\n",
      "Iteration 1181, loss = 0.26935681\n",
      "Iteration 1722, loss = 0.16103323\n",
      "Iteration 1842, loss = 0.20301099\n",
      "Iteration 2540, loss = 0.14001791\n",
      "Iteration 1674, loss = 0.16150812\n",
      "Iteration 1182, loss = 0.26929227Iteration 290, loss = 0.38363128\n",
      "Iteration 562, loss = 0.35204785\n",
      "\n",
      "Iteration 1843, loss = 0.20284658\n",
      "Iteration 1675, loss = 0.16144425\n",
      "Iteration 1183, loss = 0.26913242\n",
      "Iteration 1723, loss = 0.16102487\n",
      "Iteration 291, loss = 0.38342344\n",
      "Iteration 1676, loss = 0.16117441\n",
      "Iteration 1184, loss = 0.26906402\n",
      "Iteration 1677, loss = 0.16096070\n",
      "Iteration 563, loss = 0.35200446\n",
      "Iteration 1844, loss = 0.20272170\n",
      "Iteration 1678, loss = 0.16089977\n",
      "Iteration 292, loss = 0.38318367\n",
      "Iteration 1185, loss = 0.26893686\n",
      "Iteration 766, loss = 0.32997691\n",
      "Iteration 1679, loss = 0.16070133\n",
      "Iteration 1680, loss = 0.16055852\n",
      "Iteration 2541, loss = 0.13991653\n",
      "Iteration 1681, loss = 0.16058642\n",
      "Iteration 293, loss = 0.38297838\n",
      "Iteration 1845, loss = 0.20265070\n",
      "Iteration 1724, loss = 0.16086608\n",
      "Iteration 1186, loss = 0.26887952\n",
      "Iteration 294, loss = 0.38277543\n",
      "Iteration 767, loss = 0.32980317\n",
      "Iteration 564, loss = 0.35181787\n",
      "Iteration 1682, loss = 0.16028127\n",
      "Iteration 2542, loss = 0.13987962\n",
      "Iteration 295, loss = 0.38254873\n",
      "Iteration 1683, loss = 0.16009936\n",
      "Iteration 1187, loss = 0.26880307\n",
      "Iteration 1684, loss = 0.16008361\n",
      "Iteration 1846, loss = 0.20262142\n",
      "Iteration 1685, loss = 0.15983181\n",
      "Iteration 296, loss = 0.38230061\n",
      "Iteration 1686, loss = 0.15965829\n",
      "Iteration 1725, loss = 0.16080253\n",
      "Iteration 297, loss = 0.38209907\n",
      "Iteration 1847, loss = 0.20239270\n",
      "Iteration 2543, loss = 0.13977604\n",
      "Iteration 1687, loss = 0.15954476\n",
      "Iteration 1188, loss = 0.26866265\n",
      "Iteration 565, loss = 0.35169120\n",
      "Iteration 768, loss = 0.32949629\n",
      "Iteration 1189, loss = 0.26857411\n",
      "Iteration 1688, loss = 0.15940482\n",
      "Iteration 1848, loss = 0.20225798\n",
      "Iteration 298, loss = 0.38185222\n",
      "Iteration 566, loss = 0.35159112\n",
      "Iteration 1689, loss = 0.15920675\n",
      "Iteration 2544, loss = 0.13979933\n",
      "Iteration 1190, loss = 0.26846630\n",
      "Iteration 1849, loss = 0.20219872\n",
      "Iteration 1690, loss = 0.15907082\n",
      "Iteration 299, loss = 0.38171502\n",
      "Iteration 1191, loss = 0.26839829\n",
      "Iteration 769, loss = 0.32937194\n",
      "Iteration 1691, loss = 0.15889595\n",
      "Iteration 1850, loss = 0.20221471\n",
      "Iteration 1726, loss = 0.16073070\n",
      "Iteration 300, loss = 0.38141681\n",
      "Iteration 1692, loss = 0.15874311\n",
      "Iteration 2545, loss = 0.13967197\n",
      "Iteration 1693, loss = 0.15873947\n",
      "Iteration 1192, loss = 0.26825384\n",
      "Iteration 567, loss = 0.35147934\n",
      "Iteration 301, loss = 0.38121806\n",
      "Iteration 1727, loss = 0.16065235\n",
      "Iteration 1694, loss = 0.15857742\n",
      "Iteration 1851, loss = 0.20195112\n",
      "Iteration 1193, loss = 0.26818681\n",
      "Iteration 1695, loss = 0.15837963\n",
      "Iteration 1194, loss = 0.26807302\n",
      "Iteration 1696, loss = 0.15812273\n",
      "Iteration 568, loss = 0.35134597\n",
      "Iteration 1195, loss = 0.26799144\n",
      "Iteration 770, loss = 0.32908008\n",
      "Iteration 1697, loss = 0.15803075\n",
      "Iteration 1728, loss = 0.16056391\n",
      "Iteration 1698, loss = 0.15791454\n",
      "Iteration 1196, loss = 0.26793949\n",
      "Iteration 2546, loss = 0.13961983\n",
      "Iteration 1852, loss = 0.20182728\n",
      "Iteration 1699, loss = 0.15779363\n",
      "Iteration 302, loss = 0.38100466\n",
      "Iteration 1700, loss = 0.15773171\n",
      "Iteration 569, loss = 0.35124768\n",
      "Iteration 1701, loss = 0.15745022\n",
      "Iteration 1702, loss = 0.15724519\n",
      "Iteration 1853, loss = 0.20177260\n",
      "Iteration 1197, loss = 0.26782888\n",
      "Iteration 570, loss = 0.35112323\n",
      "Iteration 2547, loss = 0.13955620\n",
      "Iteration 1703, loss = 0.15729453\n",
      "Iteration 303, loss = 0.38080291\n",
      "Iteration 771, loss = 0.32885353\n",
      "Iteration 1729, loss = 0.16051476\n",
      "Iteration 1704, loss = 0.15695591\n",
      "Iteration 1198, loss = 0.26768161\n",
      "Iteration 571, loss = 0.35101284\n",
      "Iteration 304, loss = 0.38063009\n",
      "Iteration 1854, loss = 0.20162982\n",
      "Iteration 2548, loss = 0.13954337\n",
      "Iteration 1730, loss = 0.16043830\n",
      "Iteration 1705, loss = 0.15684437\n",
      "Iteration 1199, loss = 0.26759302\n",
      "Iteration 1855, loss = 0.20164260\n",
      "Iteration 572, loss = 0.35092678\n",
      "Iteration 1200, loss = 0.26757297\n",
      "Iteration 772, loss = 0.32857833\n",
      "Iteration 1706, loss = 0.15659590\n",
      "Iteration 1707, loss = 0.15654484\n",
      "Iteration 305, loss = 0.38034195\n",
      "Iteration 1201, loss = 0.26741029\n",
      "Iteration 1708, loss = 0.15634517\n",
      "Iteration 1202, loss = 0.26732749\n",
      "Iteration 1856, loss = 0.20138837\n",
      "Iteration 306, loss = 0.38018650\n",
      "Iteration 1709, loss = 0.15624813\n",
      "Iteration 1731, loss = 0.16029449\n",
      "Iteration 573, loss = 0.35078312\n",
      "Iteration 307, loss = 0.37989888\n",
      "Iteration 1857, loss = 0.20131259\n",
      "Iteration 1203, loss = 0.26723662\n",
      "Iteration 1710, loss = 0.15605449\n",
      "Iteration 2549, loss = 0.13955605\n",
      "Iteration 773, loss = 0.32847792\n",
      "Iteration 574, loss = 0.35066886\n",
      "Iteration 1711, loss = 0.15601104\n",
      "Iteration 1204, loss = 0.26711267\n",
      "Iteration 1712, loss = 0.15574791\n",
      "Iteration 1205, loss = 0.26703374\n",
      "Iteration 2550, loss = 0.13937643\n",
      "Iteration 1732, loss = 0.16030502\n",
      "Iteration 1713, loss = 0.15563862\n",
      "Iteration 1714, loss = 0.15551906\n",
      "Iteration 1206, loss = 0.26694398\n",
      "Iteration 308, loss = 0.37971118\n",
      "Iteration 1715, loss = 0.15541070\n",
      "Iteration 2551, loss = 0.13928719\n",
      "Iteration 575, loss = 0.35053205\n",
      "Iteration 1858, loss = 0.20121269\n",
      "Iteration 1207, loss = 0.26683921\n",
      "Iteration 309, loss = 0.37951835\n",
      "Iteration 1716, loss = 0.15541680\n",
      "Iteration 1717, loss = 0.15507040\n",
      "Iteration 2552, loss = 0.13932801\n",
      "Iteration 310, loss = 0.37934398\n",
      "Iteration 1733, loss = 0.16014384\n",
      "Iteration 774, loss = 0.32813064\n",
      "Iteration 311, loss = 0.37912667\n",
      "Iteration 1208, loss = 0.26675999\n",
      "Iteration 1718, loss = 0.15491375\n",
      "Iteration 2553, loss = 0.13916836\n",
      "Iteration 1859, loss = 0.20116468\n",
      "Iteration 1209, loss = 0.26664234\n",
      "Iteration 1719, loss = 0.15482946\n",
      "Iteration 1210, loss = 0.26659803\n",
      "Iteration 312, loss = 0.37893300\n",
      "Iteration 1720, loss = 0.15487076\n",
      "Iteration 576, loss = 0.35041564\n",
      "Iteration 1734, loss = 0.16016275\n",
      "Iteration 2554, loss = 0.13917371\n",
      "Iteration 1860, loss = 0.20105418\n",
      "Iteration 1721, loss = 0.15450764\n",
      "Iteration 1722, loss = 0.15447040\n",
      "Iteration 2555, loss = 0.13914242\n",
      "Iteration 1723, loss = 0.15432811\n",
      "Iteration 1861, loss = 0.20087422\n",
      "Iteration 775, loss = 0.32811904\n",
      "Iteration 1724, loss = 0.15412612\n",
      "Iteration 313, loss = 0.37865396\n",
      "Iteration 577, loss = 0.35033338\n",
      "Iteration 1725, loss = 0.15386486\n",
      "Iteration 1211, loss = 0.26648897\n",
      "Iteration 1735, loss = 0.16015292\n",
      "Iteration 1726, loss = 0.15392614\n",
      "Iteration 1212, loss = 0.26635332\n",
      "Iteration 776, loss = 0.32792050\n",
      "Iteration 2556, loss = 0.13901428\n",
      "Iteration 1213, loss = 0.26627778\n",
      "Iteration 1727, loss = 0.15359949\n",
      "Iteration 314, loss = 0.37849767\n",
      "Iteration 1728, loss = 0.15345939\n",
      "Iteration 1736, loss = 0.16000795\n",
      "Iteration 1862, loss = 0.20076858\n",
      "Iteration 1214, loss = 0.26618310\n",
      "Iteration 1215, loss = 0.26607995\n",
      "Iteration 315, loss = 0.37825470\n",
      "Iteration 1729, loss = 0.15340446\n",
      "Iteration 2557, loss = 0.13909307\n",
      "Iteration 777, loss = 0.32759708\n",
      "Iteration 1863, loss = 0.20067404\n",
      "Iteration 1216, loss = 0.26598695\n",
      "Iteration 316, loss = 0.37808645\n",
      "Iteration 1730, loss = 0.15314010\n",
      "Iteration 1217, loss = 0.26593822\n",
      "Iteration 1731, loss = 0.15324120\n",
      "Iteration 2558, loss = 0.13891356\n",
      "Iteration 578, loss = 0.35020174\n",
      "Iteration 317, loss = 0.37782980\n",
      "Iteration 1732, loss = 0.15281971\n",
      "Iteration 1218, loss = 0.26581665\n",
      "Iteration 1864, loss = 0.20056918\n",
      "Iteration 1733, loss = 0.15273583\n",
      "Iteration 1737, loss = 0.15991161\n",
      "Iteration 778, loss = 0.32724045\n",
      "Iteration 1219, loss = 0.26573716\n",
      "Iteration 579, loss = 0.35006648\n",
      "Iteration 1220, loss = 0.26563732\n",
      "Iteration 1734, loss = 0.15259992\n",
      "Iteration 318, loss = 0.37768608\n",
      "Iteration 1735, loss = 0.15244856\n",
      "Iteration 2559, loss = 0.13882743\n",
      "Iteration 1736, loss = 0.15237991\n",
      "Iteration 1221, loss = 0.26550882\n",
      "Iteration 1737, loss = 0.15249505\n",
      "Iteration 1865, loss = 0.20044900\n",
      "Iteration 319, loss = 0.37745431\n",
      "Iteration 580, loss = 0.34996847\n",
      "Iteration 1738, loss = 0.15979631\n",
      "Iteration 1738, loss = 0.15210644\n",
      "Iteration 779, loss = 0.32703637\n",
      "Iteration 320, loss = 0.37730991\n",
      "Iteration 1866, loss = 0.20040618\n",
      "Iteration 2560, loss = 0.13879742\n",
      "Iteration 1222, loss = 0.26541433\n",
      "Iteration 581, loss = 0.34983828\n",
      "Iteration 1739, loss = 0.15188368\n",
      "Iteration 1223, loss = 0.26535826\n",
      "Iteration 321, loss = 0.37703262\n",
      "Iteration 1739, loss = 0.15974571\n",
      "Iteration 780, loss = 0.32687307\n",
      "Iteration 1740, loss = 0.15179060\n",
      "Iteration 322, loss = 0.37692090\n",
      "Iteration 1867, loss = 0.20028247\n",
      "Iteration 2561, loss = 0.13872688\n",
      "Iteration 1741, loss = 0.15168454\n",
      "Iteration 1742, loss = 0.15162497\n",
      "Iteration 582, loss = 0.34973540\n",
      "Iteration 1224, loss = 0.26539182\n",
      "Iteration 1743, loss = 0.15132343\n",
      "Iteration 1744, loss = 0.15127383\n",
      "Iteration 781, loss = 0.32654514\n",
      "Iteration 1740, loss = 0.15974571\n",
      "Iteration 1745, loss = 0.15113300\n",
      "Iteration 1746, loss = 0.15086511\n",
      "Iteration 1225, loss = 0.26512721\n",
      "Iteration 583, loss = 0.34962274\n",
      "Iteration 1868, loss = 0.20020196\n",
      "Iteration 1747, loss = 0.15085218\n",
      "Iteration 323, loss = 0.37663421\n",
      "Iteration 1226, loss = 0.26504736\n",
      "Iteration 2562, loss = 0.13862981\n",
      "Iteration 1748, loss = 0.15058909\n",
      "Iteration 324, loss = 0.37650191\n",
      "Iteration 1227, loss = 0.26498396\n",
      "Iteration 1741, loss = 0.15962260\n",
      "Iteration 1749, loss = 0.15044724\n",
      "Iteration 1228, loss = 0.26485734\n",
      "Iteration 2563, loss = 0.13864345\n",
      "Iteration 325, loss = 0.37624997\n",
      "Iteration 584, loss = 0.34953105\n",
      "Iteration 1869, loss = 0.20008331\n",
      "Iteration 1229, loss = 0.26476051\n",
      "Iteration 2564, loss = 0.13853565\n",
      "Iteration 326, loss = 0.37607650\n",
      "Iteration 1750, loss = 0.15032004\n",
      "Iteration 1751, loss = 0.15014535\n",
      "Iteration 782, loss = 0.32647157\n",
      "Iteration 1230, loss = 0.26467396\n",
      "Iteration 1742, loss = 0.15951566\n",
      "Iteration 1752, loss = 0.15001794\n",
      "Iteration 327, loss = 0.37587084\n",
      "Iteration 585, loss = 0.34944875\n",
      "Iteration 1870, loss = 0.19997683\n",
      "Iteration 2565, loss = 0.13853720\n",
      "Iteration 1753, loss = 0.14988353\n",
      "Iteration 328, loss = 0.37566187\n",
      "Iteration 1754, loss = 0.14979482\n",
      "Iteration 2566, loss = 0.13841993\n",
      "Iteration 1231, loss = 0.26455766\n",
      "Iteration 1755, loss = 0.14964623\n",
      "Iteration 329, loss = 0.37545228\n",
      "Iteration 1756, loss = 0.14947581\n",
      "Iteration 1757, loss = 0.14957495\n",
      "Iteration 586, loss = 0.34935984\n",
      "Iteration 330, loss = 0.37522914\n",
      "Iteration 1758, loss = 0.14919930\n",
      "Iteration 1871, loss = 0.19983123\n",
      "Iteration 1232, loss = 0.26448841\n",
      "Iteration 1759, loss = 0.14909169\n",
      "Iteration 1760, loss = 0.14889090\n",
      "Iteration 1761, loss = 0.14880855\n",
      "Iteration 2567, loss = 0.13845673\n",
      "Iteration 1762, loss = 0.14870578\n",
      "Iteration 1233, loss = 0.26437986\n",
      "Iteration 783, loss = 0.32611560\n",
      "Iteration 1763, loss = 0.14851245\n",
      "Iteration 1872, loss = 0.19977962\n",
      "Iteration 1234, loss = 0.26435564\n",
      "Iteration 1764, loss = 0.14843302\n",
      "Iteration 587, loss = 0.34916063\n",
      "Iteration 1235, loss = 0.26417014\n",
      "Iteration 1765, loss = 0.14826952\n",
      "Iteration 1743, loss = 0.15948755\n",
      "Iteration 1766, loss = 0.14806131\n",
      "Iteration 1873, loss = 0.19974577\n",
      "Iteration 1236, loss = 0.26409017\n",
      "Iteration 1767, loss = 0.14792059\n",
      "Iteration 331, loss = 0.37505638\n",
      "Iteration 1768, loss = 0.14786767\n",
      "Iteration 1237, loss = 0.26410462\n",
      "Iteration 1769, loss = 0.14765389\n",
      "Iteration 588, loss = 0.34903022\n",
      "Iteration 1770, loss = 0.14761863\n",
      "Iteration 1238, loss = 0.26391106\n",
      "Iteration 332, loss = 0.37486934\n",
      "Iteration 1771, loss = 0.14738191\n",
      "Iteration 1874, loss = 0.19951974Iteration 2568, loss = 0.13834429\n",
      "\n",
      "Iteration 333, loss = 0.37466869\n",
      "Iteration 1772, loss = 0.14728223\n",
      "Iteration 589, loss = 0.34894548\n",
      "Iteration 1773, loss = 0.14724106\n",
      "Iteration 784, loss = 0.32589658\n",
      "Iteration 1774, loss = 0.14706827\n",
      "Iteration 1775, loss = 0.14686449\n",
      "Iteration 2569, loss = 0.13828874\n",
      "Iteration 334, loss = 0.37447362\n",
      "Iteration 1776, loss = 0.14699534\n",
      "Iteration 1744, loss = 0.15933734\n",
      "Iteration 1777, loss = 0.14665682\n",
      "Iteration 1239, loss = 0.26379670\n",
      "Iteration 1778, loss = 0.14644447\n",
      "Iteration 590, loss = 0.34888078\n",
      "Iteration 2570, loss = 0.13820540\n",
      "Iteration 1779, loss = 0.14637898\n",
      "Iteration 1875, loss = 0.19941022\n",
      "Iteration 1240, loss = 0.26371632\n",
      "Iteration 785, loss = 0.32570536\n",
      "Iteration 335, loss = 0.37424957\n",
      "Iteration 1241, loss = 0.26367088\n",
      "Iteration 1780, loss = 0.14632621Iteration 2571, loss = 0.13817838\n",
      "\n",
      "Iteration 336, loss = 0.37407401\n",
      "Iteration 591, loss = 0.34868731\n",
      "Iteration 1781, loss = 0.14612019\n",
      "Iteration 786, loss = 0.32541678\n",
      "Iteration 1782, loss = 0.14598519\n",
      "Iteration 1745, loss = 0.15932832\n",
      "Iteration 1242, loss = 0.26351558\n",
      "Iteration 337, loss = 0.37386646\n",
      "Iteration 1876, loss = 0.19931977\n",
      "Iteration 2572, loss = 0.13810876\n",
      "Iteration 1783, loss = 0.14580306\n",
      "Iteration 1243, loss = 0.26346330\n",
      "Iteration 592, loss = 0.34863393\n",
      "Iteration 338, loss = 0.37372510\n",
      "Iteration 787, loss = 0.32516153\n",
      "Iteration 1784, loss = 0.14574078\n",
      "Iteration 1877, loss = 0.19920258\n",
      "Iteration 1244, loss = 0.26334200\n",
      "Iteration 1746, loss = 0.15926607\n",
      "Iteration 2573, loss = 0.13797632\n",
      "Iteration 1785, loss = 0.14570422\n",
      "Iteration 339, loss = 0.37351556\n",
      "Iteration 1786, loss = 0.14543138\n",
      "Iteration 1878, loss = 0.19907893\n",
      "Iteration 1245, loss = 0.26325505\n",
      "Iteration 593, loss = 0.34853884\n",
      "Iteration 1787, loss = 0.14526418\n",
      "Iteration 1246, loss = 0.26315325\n",
      "Iteration 340, loss = 0.37331390\n",
      "Iteration 1247, loss = 0.26308149\n",
      "Iteration 788, loss = 0.32499935\n",
      "Iteration 1788, loss = 0.14520462\n",
      "Iteration 2574, loss = 0.13794058\n",
      "Iteration 1789, loss = 0.14501117\n",
      "Iteration 1248, loss = 0.26296810\n",
      "Iteration 1747, loss = 0.15913826\n",
      "Iteration 1790, loss = 0.14491163\n",
      "Iteration 1249, loss = 0.26286960\n",
      "Iteration 341, loss = 0.37312237\n",
      "Iteration 1879, loss = 0.19901462\n",
      "Iteration 594, loss = 0.34836561\n",
      "Iteration 1791, loss = 0.14487110\n",
      "Iteration 1250, loss = 0.26276214\n",
      "Iteration 2575, loss = 0.13786020\n",
      "Iteration 1792, loss = 0.14499644\n",
      "Iteration 1251, loss = 0.26271200\n",
      "Iteration 1748, loss = 0.15905899\n",
      "Iteration 342, loss = 0.37289527\n",
      "Iteration 1793, loss = 0.14456823\n",
      "Iteration 1880, loss = 0.19907364\n",
      "Iteration 789, loss = 0.32478886\n",
      "Iteration 595, loss = 0.34825503\n",
      "Iteration 1794, loss = 0.14428872\n",
      "Iteration 1795, loss = 0.14422960\n",
      "Iteration 1796, loss = 0.14404141\n",
      "Iteration 343, loss = 0.37279855\n",
      "Iteration 1252, loss = 0.26259734\n",
      "Iteration 1797, loss = 0.14394431\n",
      "Iteration 1253, loss = 0.26248480\n",
      "Iteration 2576, loss = 0.13781322\n",
      "Iteration 1798, loss = 0.14388781\n",
      "Iteration 596, loss = 0.34815941\n",
      "Iteration 1254, loss = 0.26243894\n",
      "Iteration 1799, loss = 0.14372324\n",
      "Iteration 1749, loss = 0.15899368\n",
      "Iteration 1881, loss = 0.19880430\n",
      "Iteration 1800, loss = 0.14365942\n",
      "Iteration 1801, loss = 0.14355605\n",
      "Iteration 790, loss = 0.32453752\n",
      "Iteration 344, loss = 0.37251709\n",
      "Iteration 1802, loss = 0.14339158\n",
      "Iteration 1803, loss = 0.14312759\n",
      "Iteration 2577, loss = 0.13778415\n",
      "Iteration 345, loss = 0.37233303\n",
      "Iteration 597, loss = 0.34800133\n",
      "Iteration 1882, loss = 0.19875969\n",
      "Iteration 1804, loss = 0.14296518\n",
      "Iteration 1805, loss = 0.14283246\n",
      "Iteration 2578, loss = 0.13768091Iteration 1806, loss = 0.14291489\n",
      "\n",
      "Iteration 1807, loss = 0.14267765\n",
      "Iteration 1808, loss = 0.14247921\n",
      "Iteration 598, loss = 0.34792175\n",
      "Iteration 791, loss = 0.32426736\n",
      "Iteration 1750, loss = 0.15889153\n",
      "Iteration 1883, loss = 0.19858114\n",
      "Iteration 346, loss = 0.37212744\n",
      "Iteration 2579, loss = 0.13769422\n",
      "Iteration 1809, loss = 0.14239071\n",
      "Iteration 1255, loss = 0.26233337\n",
      "Iteration 1810, loss = 0.14226605\n",
      "Iteration 347, loss = 0.37191736\n",
      "Iteration 1811, loss = 0.14213249\n",
      "Iteration 1812, loss = 0.14201481\n",
      "Iteration 792, loss = 0.32410385\n",
      "Iteration 2580, loss = 0.13762092\n",
      "Iteration 348, loss = 0.37172515\n",
      "Iteration 1813, loss = 0.14193406\n",
      "Iteration 599, loss = 0.34780796\n",
      "Iteration 1884, loss = 0.19848697\n",
      "Iteration 1751, loss = 0.15889888\n",
      "Iteration 349, loss = 0.37155122\n",
      "Iteration 2581, loss = 0.13754702\n",
      "Iteration 1885, loss = 0.19837753\n",
      "Iteration 350, loss = 0.37141507\n",
      "Iteration 1814, loss = 0.14171938\n",
      "Iteration 2582, loss = 0.13745223\n",
      "Iteration 1815, loss = 0.14160460\n",
      "Iteration 600, loss = 0.34769289\n",
      "Iteration 1752, loss = 0.15884881\n",
      "Iteration 793, loss = 0.32382450\n",
      "Iteration 351, loss = 0.37117107\n",
      "Iteration 1886, loss = 0.19837180\n",
      "Iteration 1816, loss = 0.14147605\n",
      "Iteration 601, loss = 0.34760390\n",
      "Iteration 2583, loss = 0.13740900\n",
      "Iteration 352, loss = 0.37095647\n",
      "Iteration 1817, loss = 0.14134427\n",
      "Iteration 1818, loss = 0.14121768\n",
      "Iteration 602, loss = 0.34748065\n",
      "Iteration 1887, loss = 0.19826972\n",
      "Iteration 1819, loss = 0.14109311\n",
      "Iteration 353, loss = 0.37080915\n",
      "Iteration 2584, loss = 0.13739034\n",
      "Iteration 1820, loss = 0.14117456\n",
      "Iteration 1256, loss = 0.26221069\n",
      "Iteration 794, loss = 0.32359337\n",
      "Iteration 1821, loss = 0.14092903\n",
      "Iteration 1753, loss = 0.15863933\n",
      "Iteration 354, loss = 0.37056328\n",
      "Iteration 1822, loss = 0.14070128\n",
      "Iteration 1823, loss = 0.14055493\n",
      "Iteration 2585, loss = 0.13731830\n",
      "Iteration 1824, loss = 0.14039553\n",
      "Iteration 1825, loss = 0.14050552\n",
      "Iteration 1826, loss = 0.14014515\n",
      "Iteration 355, loss = 0.37036821\n",
      "Iteration 1888, loss = 0.19803871\n",
      "Iteration 1754, loss = 0.15870471\n",
      "Iteration 603, loss = 0.34733674\n",
      "Iteration 2586, loss = 0.13723660\n",
      "Iteration 795, loss = 0.32333234\n",
      "Iteration 1827, loss = 0.14024793\n",
      "Iteration 604, loss = 0.34725718\n",
      "Iteration 1828, loss = 0.13991104\n",
      "Iteration 356, loss = 0.37020997\n",
      "Iteration 1829, loss = 0.13984591\n",
      "Iteration 1830, loss = 0.13968213\n",
      "Iteration 1831, loss = 0.13960848\n",
      "Iteration 1889, loss = 0.19798576\n",
      "Iteration 357, loss = 0.37000349\n",
      "Iteration 2587, loss = 0.13723058\n",
      "Iteration 1832, loss = 0.13946560\n",
      "Iteration 796, loss = 0.32330702\n",
      "Iteration 1833, loss = 0.13939102\n",
      "Iteration 358, loss = 0.36982374\n",
      "Iteration 1834, loss = 0.13922480\n",
      "Iteration 1755, loss = 0.15866068\n",
      "Iteration 1257, loss = 0.26210832\n",
      "Iteration 359, loss = 0.36962300\n",
      "Iteration 1835, loss = 0.13906994\n",
      "Iteration 1836, loss = 0.13891381\n",
      "Iteration 1837, loss = 0.13886746\n",
      "Iteration 2588, loss = 0.13720703\n",
      "Iteration 360, loss = 0.36942135\n",
      "Iteration 1890, loss = 0.19785073\n",
      "Iteration 1258, loss = 0.26203923\n",
      "Iteration 1838, loss = 0.13866481\n",
      "Iteration 1259, loss = 0.26191799\n",
      "Iteration 1891, loss = 0.19774929\n",
      "Iteration 797, loss = 0.32289646\n",
      "Iteration 605, loss = 0.34713752\n",
      "Iteration 1839, loss = 0.13858191\n",
      "Iteration 361, loss = 0.36922973Iteration 1260, loss = 0.26183047\n",
      "\n",
      "Iteration 1840, loss = 0.13852087\n",
      "Iteration 2589, loss = 0.13713226\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1261, loss = 0.26172852\n",
      "Iteration 1841, loss = 0.13841888\n",
      "Iteration 606, loss = 0.34701242\n",
      "Iteration 1756, loss = 0.15847246\n",
      "Iteration 1892, loss = 0.19762383\n",
      "Iteration 1262, loss = 0.26167322\n",
      "Iteration 1842, loss = 0.13835838\n",
      "Iteration 607, loss = 0.34694684\n",
      "Iteration 1843, loss = 0.13816722\n",
      "Iteration 1263, loss = 0.26156787\n",
      "Iteration 1844, loss = 0.13805845\n",
      "Iteration 1845, loss = 0.13789988\n",
      "Iteration 1893, loss = 0.19751540\n",
      "Iteration 798, loss = 0.32270566\n",
      "Iteration 608, loss = 0.34681128\n",
      "Iteration 362, loss = 0.36905046\n",
      "Iteration 1264, loss = 0.26145041\n",
      "Iteration 1846, loss = 0.13772530\n",
      "Iteration 1757, loss = 0.15844687\n",
      "Iteration 1265, loss = 0.26135238\n",
      "Iteration 609, loss = 0.34670624\n",
      "Iteration 1, loss = 0.80744357\n",
      "Iteration 1266, loss = 0.26126420\n",
      "Iteration 1847, loss = 0.13761260\n",
      "Iteration 1848, loss = 0.13744841\n",
      "Iteration 1267, loss = 0.26134324\n",
      "Iteration 1849, loss = 0.13734400\n",
      "Iteration 1758, loss = 0.15834065\n",
      "Iteration 1268, loss = 0.26108265\n",
      "Iteration 1850, loss = 0.13729409\n",
      "Iteration 1851, loss = 0.13702066\n",
      "Iteration 1269, loss = 0.26101226\n",
      "Iteration 610, loss = 0.34652667\n",
      "Iteration 1852, loss = 0.13703245\n",
      "Iteration 2, loss = 0.80678851\n",
      "Iteration 1270, loss = 0.26088940\n",
      "Iteration 363, loss = 0.36887795\n",
      "Iteration 1759, loss = 0.15827315\n",
      "Iteration 799, loss = 0.32244618\n",
      "Iteration 1271, loss = 0.26080247\n",
      "Iteration 1853, loss = 0.13688128\n",
      "Iteration 1894, loss = 0.19748661\n",
      "Iteration 364, loss = 0.36871418\n",
      "Iteration 611, loss = 0.34644013\n",
      "Iteration 1854, loss = 0.13675385\n",
      "Iteration 1272, loss = 0.26074684\n",
      "Iteration 1855, loss = 0.13654472\n",
      "Iteration 365, loss = 0.36849914\n",
      "Iteration 1895, loss = 0.19733194\n",
      "Iteration 612, loss = 0.34631448\n",
      "Iteration 800, loss = 0.32236431\n",
      "Iteration 1856, loss = 0.13644757\n",
      "Iteration 366, loss = 0.36829844\n",
      "Iteration 1896, loss = 0.19722481\n",
      "Iteration 1857, loss = 0.13640200\n",
      "Iteration 1760, loss = 0.15826852\n",
      "Iteration 1858, loss = 0.13618239\n",
      "Iteration 1273, loss = 0.26060723\n",
      "Iteration 1859, loss = 0.13620003\n",
      "Iteration 3, loss = 0.80582213\n",
      "Iteration 613, loss = 0.34625342\n",
      "Iteration 367, loss = 0.36811177\n",
      "Iteration 1897, loss = 0.19717883\n",
      "Iteration 1761, loss = 0.15812217\n",
      "Iteration 1860, loss = 0.13601216\n",
      "Iteration 1861, loss = 0.13586042\n",
      "Iteration 1274, loss = 0.26053190\n",
      "Iteration 801, loss = 0.32199274\n",
      "Iteration 1862, loss = 0.13583667\n",
      "Iteration 368, loss = 0.36795410\n",
      "Iteration 614, loss = 0.34607730\n",
      "Iteration 1898, loss = 0.19701330\n",
      "Iteration 1863, loss = 0.13558837\n",
      "Iteration 1275, loss = 0.26041402\n",
      "Iteration 1864, loss = 0.13544488\n",
      "Iteration 369, loss = 0.36777221\n",
      "Iteration 1276, loss = 0.26030963\n",
      "Iteration 1865, loss = 0.13538590\n",
      "Iteration 4, loss = 0.80456559\n",
      "Iteration 1866, loss = 0.13520322\n",
      "Iteration 1762, loss = 0.15806301\n",
      "Iteration 1867, loss = 0.13502301\n",
      "Iteration 370, loss = 0.36761446\n",
      "Iteration 1277, loss = 0.26025042\n",
      "Iteration 802, loss = 0.32179088\n",
      "Iteration 1868, loss = 0.13497357\n",
      "Iteration 615, loss = 0.34598172\n",
      "Iteration 1899, loss = 0.19690428\n",
      "Iteration 1869, loss = 0.13488000\n",
      "Iteration 371, loss = 0.36737430\n",
      "Iteration 1870, loss = 0.13485843\n",
      "Iteration 1278, loss = 0.26014048\n",
      "Iteration 5, loss = 0.80311477\n",
      "Iteration 1279, loss = 0.26004425\n",
      "Iteration 1871, loss = 0.13480471\n",
      "Iteration 372, loss = 0.36719676\n",
      "Iteration 616, loss = 0.34585381\n",
      "Iteration 1763, loss = 0.15813785\n",
      "Iteration 803, loss = 0.32156875\n",
      "Iteration 1900, loss = 0.19679063\n",
      "Iteration 1280, loss = 0.25993350\n",
      "Iteration 1872, loss = 0.13456582\n",
      "Iteration 1873, loss = 0.13438518\n",
      "Iteration 1901, loss = 0.19673717\n",
      "Iteration 617, loss = 0.34574644\n",
      "Iteration 373, loss = 0.36703120\n",
      "Iteration 1281, loss = 0.25986065\n",
      "Iteration 1874, loss = 0.13430061\n",
      "Iteration 1282, loss = 0.25976491\n",
      "Iteration 1902, loss = 0.19664000\n",
      "Iteration 6, loss = 0.80159610\n",
      "Iteration 1764, loss = 0.15804217\n",
      "Iteration 804, loss = 0.32134548\n",
      "Iteration 1875, loss = 0.13421968\n",
      "Iteration 1283, loss = 0.25976138\n",
      "Iteration 1876, loss = 0.13396936\n",
      "Iteration 1903, loss = 0.19657022\n",
      "Iteration 374, loss = 0.36682575\n",
      "Iteration 1877, loss = 0.13395061\n",
      "Iteration 618, loss = 0.34563805\n",
      "Iteration 1284, loss = 0.25961395\n",
      "Iteration 375, loss = 0.36667185\n",
      "Iteration 1765, loss = 0.15777126\n",
      "Iteration 1878, loss = 0.13381524\n",
      "Iteration 1879, loss = 0.13376575\n",
      "Iteration 1285, loss = 0.25953631\n",
      "Iteration 1904, loss = 0.19637633\n",
      "Iteration 1880, loss = 0.13355740\n",
      "Iteration 1881, loss = 0.13339991\n",
      "Iteration 376, loss = 0.36648104\n",
      "Iteration 1882, loss = 0.13326665\n",
      "Iteration 1286, loss = 0.25941488\n",
      "Iteration 1883, loss = 0.13324512\n",
      "Iteration 377, loss = 0.36629018\n",
      "Iteration 1884, loss = 0.13304857\n",
      "Iteration 1905, loss = 0.19630130\n",
      "Iteration 619, loss = 0.34549026\n",
      "Iteration 1885, loss = 0.13294956\n",
      "Iteration 7, loss = 0.79995168\n",
      "Iteration 1287, loss = 0.25938072\n",
      "Iteration 378, loss = 0.36616058\n",
      "Iteration 1886, loss = 0.13289846\n",
      "Iteration 805, loss = 0.32111640\n",
      "Iteration 379, loss = 0.36590829\n",
      "Iteration 1887, loss = 0.13262721\n",
      "Iteration 1766, loss = 0.15779593\n",
      "Iteration 1288, loss = 0.25922081\n",
      "Iteration 1906, loss = 0.19618751\n",
      "Iteration 620, loss = 0.34538815\n",
      "Iteration 1289, loss = 0.25907304\n",
      "Iteration 1888, loss = 0.13258434\n",
      "Iteration 806, loss = 0.32089128\n",
      "Iteration 380, loss = 0.36573641\n",
      "Iteration 1889, loss = 0.13248907\n",
      "Iteration 1290, loss = 0.25899969\n",
      "Iteration 1291, loss = 0.25890285\n",
      "Iteration 1890, loss = 0.13231474\n",
      "Iteration 8, loss = 0.79831116\n",
      "Iteration 1767, loss = 0.15764005\n",
      "Iteration 1891, loss = 0.13221485\n",
      "Iteration 1292, loss = 0.25880643\n",
      "Iteration 807, loss = 0.32081711\n",
      "Iteration 381, loss = 0.36562086\n",
      "Iteration 1907, loss = 0.19610006\n",
      "Iteration 1892, loss = 0.13203232\n",
      "Iteration 621, loss = 0.34530611\n",
      "Iteration 1893, loss = 0.13193438\n",
      "Iteration 382, loss = 0.36536942\n",
      "Iteration 1894, loss = 0.13183627\n",
      "Iteration 1293, loss = 0.25879277\n",
      "Iteration 1908, loss = 0.19601817\n",
      "Iteration 383, loss = 0.36521817\n",
      "Iteration 1895, loss = 0.13173829\n",
      "Iteration 1768, loss = 0.15766027\n",
      "Iteration 1896, loss = 0.13157164\n",
      "Iteration 1294, loss = 0.25860534\n",
      "Iteration 1897, loss = 0.13152927\n",
      "Iteration 808, loss = 0.32048276\n",
      "Iteration 1898, loss = 0.13137840\n",
      "Iteration 9, loss = 0.79664884\n",
      "Iteration 1909, loss = 0.19588463\n",
      "Iteration 384, loss = 0.36511219\n",
      "Iteration 1899, loss = 0.13139501\n",
      "Iteration 1900, loss = 0.13106318\n",
      "Iteration 622, loss = 0.34516590\n",
      "Iteration 1295, loss = 0.25850044\n",
      "Iteration 1901, loss = 0.13106721\n",
      "Iteration 1902, loss = 0.13087304\n",
      "Iteration 623, loss = 0.34504117\n",
      "Iteration 1296, loss = 0.25845742\n",
      "Iteration 10, loss = 0.79485371\n",
      "Iteration 385, loss = 0.36485401\n",
      "Iteration 1903, loss = 0.13073973\n",
      "Iteration 1769, loss = 0.15755435\n",
      "Iteration 809, loss = 0.32028459\n",
      "Iteration 1904, loss = 0.13062140\n",
      "Iteration 386, loss = 0.36467169\n",
      "Iteration 624, loss = 0.34491321\n",
      "Iteration 1297, loss = 0.25835237\n",
      "Iteration 387, loss = 0.36459214\n",
      "Iteration 810, loss = 0.32002010\n",
      "Iteration 1910, loss = 0.19584924\n",
      "Iteration 1905, loss = 0.13052678\n",
      "Iteration 625, loss = 0.34480089\n",
      "Iteration 1298, loss = 0.25826413\n",
      "Iteration 1906, loss = 0.13042146\n",
      "Iteration 1907, loss = 0.13043066\n",
      "Iteration 388, loss = 0.36441697\n",
      "Iteration 1908, loss = 0.13016647\n",
      "Iteration 11, loss = 0.79322265\n",
      "Iteration 1770, loss = 0.15745754\n",
      "Iteration 1909, loss = 0.13004245\n",
      "Iteration 811, loss = 0.31976082\n",
      "Iteration 1910, loss = 0.12990095\n",
      "Iteration 1299, loss = 0.25818756\n",
      "Iteration 1911, loss = 0.12988913\n",
      "Iteration 626, loss = 0.34474131\n",
      "Iteration 389, loss = 0.36440083\n",
      "Iteration 1912, loss = 0.12963380\n",
      "Iteration 1300, loss = 0.25805912\n",
      "Iteration 1913, loss = 0.12975154\n",
      "Iteration 12, loss = 0.79143782\n",
      "Iteration 1301, loss = 0.25796706\n",
      "Iteration 1914, loss = 0.12950308\n",
      "Iteration 1771, loss = 0.15752210\n",
      "Iteration 1915, loss = 0.12943927\n",
      "Iteration 1916, loss = 0.12921809\n",
      "Iteration 1302, loss = 0.25789777\n",
      "Iteration 390, loss = 0.36396052\n",
      "Iteration 627, loss = 0.34457882\n",
      "Iteration 1917, loss = 0.12908701\n",
      "Iteration 1918, loss = 0.12899663\n",
      "Iteration 812, loss = 0.31951772\n",
      "Iteration 13, loss = 0.78964951\n",
      "Iteration 1919, loss = 0.12888410\n",
      "Iteration 391, loss = 0.36376301\n",
      "Iteration 1920, loss = 0.12875634\n",
      "Iteration 1303, loss = 0.25781570\n",
      "Iteration 1921, loss = 0.12884839\n",
      "Iteration 392, loss = 0.36358850\n",
      "Iteration 1772, loss = 0.15733266\n",
      "Iteration 628, loss = 0.34451833\n",
      "Iteration 1304, loss = 0.25766216\n",
      "Iteration 1922, loss = 0.12856006\n",
      "Iteration 813, loss = 0.31943515\n",
      "Iteration 14, loss = 0.78797315\n",
      "Iteration 1305, loss = 0.25760746\n",
      "Iteration 1923, loss = 0.12848086\n",
      "Iteration 629, loss = 0.34436380\n",
      "Iteration 1306, loss = 0.25748403\n",
      "Iteration 393, loss = 0.36339443\n",
      "Iteration 1924, loss = 0.12828761\n",
      "Iteration 1773, loss = 0.15727246\n",
      "Iteration 1925, loss = 0.12823644\n",
      "Iteration 1926, loss = 0.12804028\n",
      "Iteration 630, loss = 0.34423015\n",
      "Iteration 1927, loss = 0.12799408\n",
      "Iteration 15, loss = 0.78635002\n",
      "Iteration 1928, loss = 0.12794368\n",
      "Iteration 1307, loss = 0.25740796\n",
      "Iteration 814, loss = 0.31914567\n",
      "Iteration 394, loss = 0.36323265\n",
      "Iteration 1929, loss = 0.12783230\n",
      "Iteration 631, loss = 0.34411430\n",
      "Iteration 1308, loss = 0.25730697\n",
      "Iteration 1930, loss = 0.12759719\n",
      "Iteration 1774, loss = 0.15727764\n",
      "Iteration 1931, loss = 0.12748950\n",
      "Iteration 16, loss = 0.78454630\n",
      "Iteration 1309, loss = 0.25723877\n",
      "Iteration 395, loss = 0.36309262\n",
      "Iteration 1932, loss = 0.12740994\n",
      "Iteration 815, loss = 0.31894009\n",
      "Iteration 1933, loss = 0.12725335\n",
      "Iteration 1310, loss = 0.25713340\n",
      "Iteration 632, loss = 0.34399882\n",
      "Iteration 1934, loss = 0.12715183\n",
      "Iteration 1935, loss = 0.12704420\n",
      "Iteration 1775, loss = 0.15716516\n",
      "Iteration 1936, loss = 0.12701964\n",
      "Iteration 633, loss = 0.34390038\n",
      "Iteration 1937, loss = 0.12688008\n",
      "Iteration 396, loss = 0.36289958\n",
      "Iteration 816, loss = 0.31869453\n",
      "Iteration 17, loss = 0.78285779\n",
      "Iteration 1938, loss = 0.12668116\n",
      "Iteration 1311, loss = 0.25701239\n",
      "Iteration 1939, loss = 0.12662610\n",
      "Iteration 1312, loss = 0.25694693\n",
      "Iteration 1313, loss = 0.25681454\n",
      "Iteration 1776, loss = 0.15720497\n",
      "Iteration 18, loss = 0.78114331\n",
      "Iteration 397, loss = 0.36271771\n",
      "Iteration 1940, loss = 0.12657370\n",
      "Iteration 634, loss = 0.34381859\n",
      "Iteration 1314, loss = 0.25674977\n",
      "Iteration 1941, loss = 0.12658454\n",
      "Iteration 1942, loss = 0.12639649\n",
      "Iteration 398, loss = 0.36262179\n",
      "Iteration 817, loss = 0.31848303\n",
      "Iteration 1315, loss = 0.25665304\n",
      "Iteration 1943, loss = 0.12624141\n",
      "Iteration 635, loss = 0.34367858\n",
      "Iteration 1944, loss = 0.12628603\n",
      "Iteration 1777, loss = 0.15693688\n",
      "Iteration 1316, loss = 0.25651805\n",
      "Iteration 19, loss = 0.77948580\n",
      "Iteration 1945, loss = 0.12592993\n",
      "Iteration 399, loss = 0.36235285\n",
      "Iteration 1946, loss = 0.12596190\n",
      "Iteration 818, loss = 0.31833077\n",
      "Iteration 1317, loss = 0.25642430\n",
      "Iteration 636, loss = 0.34352906\n",
      "Iteration 400, loss = 0.36217911\n",
      "Iteration 1947, loss = 0.12586658\n",
      "Iteration 1778, loss = 0.15694197\n",
      "Iteration 1948, loss = 0.12565324\n",
      "Iteration 20, loss = 0.77780416\n",
      "Iteration 637, loss = 0.34342243\n",
      "Iteration 1949, loss = 0.12561660\n",
      "Iteration 1318, loss = 0.25633203\n",
      "Iteration 1911, loss = 0.19566988\n",
      "Iteration 1950, loss = 0.12545641\n",
      "Iteration 401, loss = 0.36202175\n",
      "Iteration 1951, loss = 0.12531189\n",
      "Iteration 819, loss = 0.31805363\n",
      "Iteration 1319, loss = 0.25627803\n",
      "Iteration 1952, loss = 0.12525244\n",
      "Iteration 1779, loss = 0.15683870\n",
      "Iteration 1953, loss = 0.12504999\n",
      "Iteration 638, loss = 0.34331613\n",
      "Iteration 1954, loss = 0.12494051\n",
      "Iteration 402, loss = 0.36187214\n",
      "Iteration 1912, loss = 0.19560162\n",
      "Iteration 1955, loss = 0.12501695\n",
      "Iteration 1320, loss = 0.25615273\n",
      "Iteration 21, loss = 0.77610977\n",
      "Iteration 1913, loss = 0.19553129\n",
      "Iteration 1956, loss = 0.12475615\n",
      "Iteration 639, loss = 0.34318622\n",
      "Iteration 820, loss = 0.31798320\n",
      "Iteration 403, loss = 0.36167002\n",
      "Iteration 1957, loss = 0.12460753\n",
      "Iteration 1780, loss = 0.15678361\n",
      "Iteration 1958, loss = 0.12452564\n",
      "Iteration 404, loss = 0.36154060\n",
      "Iteration 1321, loss = 0.25609785\n",
      "Iteration 1959, loss = 0.12499425\n",
      "Iteration 1914, loss = 0.19533501\n",
      "Iteration 1960, loss = 0.12443179\n",
      "Iteration 405, loss = 0.36130695\n",
      "Iteration 1961, loss = 0.12428678\n",
      "Iteration 640, loss = 0.34310880\n",
      "Iteration 1322, loss = 0.25596477\n",
      "Iteration 1781, loss = 0.15669217\n",
      "Iteration 1962, loss = 0.12436637\n",
      "Iteration 1323, loss = 0.25585527\n",
      "Iteration 406, loss = 0.36115741\n",
      "Iteration 1963, loss = 0.12421237\n",
      "Iteration 1324, loss = 0.25579222\n",
      "Iteration 641, loss = 0.34293825\n",
      "Iteration 22, loss = 0.77440136\n",
      "Iteration 1964, loss = 0.12414432\n",
      "Iteration 407, loss = 0.36103520\n",
      "Iteration 1965, loss = 0.12383287\n",
      "Iteration 821, loss = 0.31763076\n",
      "Iteration 1966, loss = 0.12382608\n",
      "Iteration 1325, loss = 0.25570540\n",
      "Iteration 1915, loss = 0.19523228\n",
      "Iteration 1967, loss = 0.12368045\n",
      "Iteration 408, loss = 0.36079922\n",
      "Iteration 1916, loss = 0.19511923\n",
      "Iteration 1782, loss = 0.15662008\n",
      "Iteration 409, loss = 0.36064343\n",
      "Iteration 642, loss = 0.34285489\n",
      "Iteration 1326, loss = 0.25556239\n",
      "Iteration 1968, loss = 0.12346749\n",
      "Iteration 1969, loss = 0.12332440\n",
      "Iteration 1970, loss = 0.12340126\n",
      "Iteration 23, loss = 0.77268734\n",
      "Iteration 410, loss = 0.36049165\n",
      "Iteration 643, loss = 0.34272010\n",
      "Iteration 1917, loss = 0.19503129\n",
      "Iteration 1327, loss = 0.25548562\n",
      "Iteration 822, loss = 0.31735773\n",
      "Iteration 1783, loss = 0.15657008\n",
      "Iteration 411, loss = 0.36030629\n",
      "Iteration 1971, loss = 0.12309702\n",
      "Iteration 1972, loss = 0.12317571\n",
      "Iteration 412, loss = 0.36013445\n",
      "Iteration 1918, loss = 0.19493582\n",
      "Iteration 823, loss = 0.31718217\n",
      "Iteration 1328, loss = 0.25547633\n",
      "Iteration 644, loss = 0.34262025\n",
      "Iteration 1329, loss = 0.25533797\n",
      "Iteration 24, loss = 0.77099139\n",
      "Iteration 1919, loss = 0.19482049\n",
      "Iteration 1330, loss = 0.25518753\n",
      "Iteration 413, loss = 0.35995088\n",
      "Iteration 645, loss = 0.34251092\n",
      "Iteration 824, loss = 0.31700223\n",
      "Iteration 1784, loss = 0.15648396\n",
      "Iteration 1973, loss = 0.12299208\n",
      "Iteration 1331, loss = 0.25513793\n",
      "Iteration 1974, loss = 0.12308844\n",
      "Iteration 414, loss = 0.35978752\n",
      "Iteration 1920, loss = 0.19485141\n",
      "Iteration 1975, loss = 0.12268123\n",
      "Iteration 1332, loss = 0.25500840\n",
      "Iteration 1976, loss = 0.12254747\n",
      "Iteration 646, loss = 0.34236806\n",
      "Iteration 25, loss = 0.76930793\n",
      "Iteration 415, loss = 0.35962786\n",
      "Iteration 1977, loss = 0.12270041\n",
      "Iteration 1921, loss = 0.19470491\n",
      "Iteration 825, loss = 0.31667011\n",
      "Iteration 1333, loss = 0.25494643\n",
      "Iteration 647, loss = 0.34233855\n",
      "Iteration 1785, loss = 0.15641225\n",
      "Iteration 1978, loss = 0.12241308\n",
      "Iteration 1334, loss = 0.25481365\n",
      "Iteration 1979, loss = 0.12233714\n",
      "Iteration 416, loss = 0.35941303\n",
      "Iteration 1335, loss = 0.25469293\n",
      "Iteration 648, loss = 0.34218577\n",
      "Iteration 417, loss = 0.35930314Iteration 1786, loss = 0.15633922\n",
      "\n",
      "Iteration 1336, loss = 0.25466279\n",
      "Iteration 1922, loss = 0.19451737\n",
      "Iteration 826, loss = 0.31640183\n",
      "Iteration 1980, loss = 0.12221524\n",
      "Iteration 26, loss = 0.76758239\n",
      "Iteration 649, loss = 0.34206352\n",
      "Iteration 1981, loss = 0.12216427\n",
      "Iteration 1337, loss = 0.25452328\n",
      "Iteration 418, loss = 0.35910147\n",
      "Iteration 1982, loss = 0.12195348\n",
      "Iteration 1338, loss = 0.25442199\n",
      "Iteration 419, loss = 0.35890684\n",
      "Iteration 1983, loss = 0.12191245\n",
      "Iteration 1787, loss = 0.15630813\n",
      "Iteration 1923, loss = 0.19441345\n",
      "Iteration 1984, loss = 0.12187833\n",
      "Iteration 1985, loss = 0.12186460\n",
      "Iteration 420, loss = 0.35876523\n",
      "Iteration 650, loss = 0.34192437\n",
      "Iteration 1986, loss = 0.12167273\n",
      "Iteration 1339, loss = 0.25432436\n",
      "Iteration 1987, loss = 0.12147263\n",
      "Iteration 827, loss = 0.31630992\n",
      "Iteration 27, loss = 0.76578837\n",
      "Iteration 1988, loss = 0.12138775\n",
      "Iteration 1788, loss = 0.15623619\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1989, loss = 0.12132570\n",
      "Iteration 1340, loss = 0.25424559\n",
      "Iteration 651, loss = 0.34180717\n",
      "Iteration 1924, loss = 0.19428003\n",
      "Iteration 1990, loss = 0.12118263\n",
      "Iteration 421, loss = 0.35865330\n",
      "Iteration 828, loss = 0.31601541\n",
      "Iteration 1341, loss = 0.25416503\n",
      "Iteration 1991, loss = 0.12106567\n",
      "Iteration 28, loss = 0.76403311\n",
      "Iteration 1992, loss = 0.12105063\n",
      "Iteration 1925, loss = 0.19425031\n",
      "Iteration 422, loss = 0.35840340\n",
      "Iteration 1993, loss = 0.12087039\n",
      "Iteration 652, loss = 0.34175742\n",
      "Iteration 1994, loss = 0.12069082\n",
      "Iteration 1342, loss = 0.25401148\n",
      "Iteration 829, loss = 0.31586300\n",
      "Iteration 423, loss = 0.35818377\n",
      "Iteration 1343, loss = 0.25392217\n",
      "Iteration 1995, loss = 0.12073805\n",
      "Iteration 653, loss = 0.34160269\n",
      "Iteration 1996, loss = 0.12049724\n",
      "Iteration 29, loss = 0.76225427Iteration 1, loss = 0.77779924\n",
      "\n",
      "Iteration 2, loss = 0.77294746\n",
      "Iteration 1926, loss = 0.19416368\n",
      "Iteration 3, loss = 0.76539991\n",
      "Iteration 424, loss = 0.35806941\n",
      "Iteration 4, loss = 0.75641899\n",
      "Iteration 5, loss = 0.74635603\n",
      "Iteration 1997, loss = 0.12048233\n",
      "Iteration 6, loss = 0.73593732\n",
      "Iteration 7, loss = 0.72500690\n",
      "Iteration 8, loss = 0.71442420\n",
      "Iteration 9, loss = 0.70385171\n",
      "Iteration 10, loss = 0.69356308\n",
      "Iteration 1344, loss = 0.25394178\n",
      "Iteration 11, loss = 0.68386327\n",
      "Iteration 425, loss = 0.35783173\n",
      "Iteration 12, loss = 0.67451862\n",
      "Iteration 1998, loss = 0.12033136\n",
      "Iteration 13, loss = 0.66525498\n",
      "Iteration 14, loss = 0.65672354\n",
      "Iteration 1345, loss = 0.25371825\n",
      "Iteration 1999, loss = 0.12022137\n",
      "Iteration 1927, loss = 0.19417022\n",
      "Iteration 426, loss = 0.35765394\n",
      "Iteration 15, loss = 0.64848245\n",
      "Iteration 1346, loss = 0.25368294\n",
      "Iteration 16, loss = 0.64054470\n",
      "Iteration 17, loss = 0.63327549\n",
      "Iteration 654, loss = 0.34146693\n",
      "Iteration 18, loss = 0.62578664\n",
      "Iteration 19, loss = 0.61895401\n",
      "Iteration 2000, loss = 0.12009013\n",
      "Iteration 20, loss = 0.61228447\n",
      "Iteration 21, loss = 0.60601935\n",
      "Iteration 30, loss = 0.76037273\n",
      "Iteration 22, loss = 0.60005836\n",
      "Iteration 1347, loss = 0.25357209\n",
      "Iteration 23, loss = 0.59421523\n",
      "Iteration 24, loss = 0.58866856\n",
      "Iteration 427, loss = 0.35754830\n",
      "Iteration 25, loss = 0.58318922\n",
      "Iteration 26, loss = 0.57808142\n",
      "Iteration 830, loss = 0.31584993\n",
      "Iteration 27, loss = 0.57319409\n",
      "Iteration 28, loss = 0.56828678\n",
      "Iteration 29, loss = 0.56368817\n",
      "Iteration 30, loss = 0.55915792\n",
      "Iteration 1348, loss = 0.25344013\n",
      "Iteration 31, loss = 0.55484930\n",
      "Iteration 2001, loss = 0.12003688\n",
      "Iteration 32, loss = 0.55072168\n",
      "Iteration 33, loss = 0.54660851\n",
      "Iteration 34, loss = 0.54267181\n",
      "Iteration 31, loss = 0.75856653\n",
      "Iteration 1928, loss = 0.19391151\n",
      "Iteration 2002, loss = 0.12026493\n",
      "Iteration 428, loss = 0.35733251\n",
      "Iteration 35, loss = 0.53886615\n",
      "Iteration 36, loss = 0.53514048\n",
      "Iteration 37, loss = 0.53156441\n",
      "Iteration 38, loss = 0.52809785\n",
      "Iteration 39, loss = 0.52474723\n",
      "Iteration 2003, loss = 0.11977643\n",
      "Iteration 40, loss = 0.52145650\n",
      "Iteration 2004, loss = 0.11990431\n",
      "Iteration 41, loss = 0.51825254\n",
      "Iteration 1349, loss = 0.25336865\n",
      "Iteration 42, loss = 0.51517071\n",
      "Iteration 43, loss = 0.51204273\n",
      "Iteration 44, loss = 0.50915395\n",
      "Iteration 45, loss = 0.50624266\n",
      "Iteration 46, loss = 0.50342940\n",
      "Iteration 47, loss = 0.50070883\n",
      "Iteration 48, loss = 0.49801849\n",
      "Iteration 49, loss = 0.49548258\n",
      "Iteration 1929, loss = 0.19381251\n",
      "Iteration 50, loss = 0.49297674\n",
      "Iteration 51, loss = 0.49049306\n",
      "Iteration 2005, loss = 0.11978048\n",
      "Iteration 429, loss = 0.35715687\n",
      "Iteration 52, loss = 0.48806179\n",
      "Iteration 32, loss = 0.75667181\n",
      "Iteration 53, loss = 0.48576271\n",
      "Iteration 655, loss = 0.34134760\n",
      "Iteration 54, loss = 0.48350094\n",
      "Iteration 1350, loss = 0.25326870\n",
      "Iteration 55, loss = 0.48120018\n",
      "Iteration 56, loss = 0.47902623\n",
      "Iteration 2006, loss = 0.11955970\n",
      "Iteration 57, loss = 0.47689239\n",
      "Iteration 58, loss = 0.47485478\n",
      "Iteration 59, loss = 0.47277630\n",
      "Iteration 60, loss = 0.47078617\n",
      "Iteration 831, loss = 0.31528227\n",
      "Iteration 430, loss = 0.35698313\n",
      "Iteration 61, loss = 0.46884643\n",
      "Iteration 62, loss = 0.46687933\n",
      "Iteration 2007, loss = 0.11945925\n",
      "Iteration 2008, loss = 0.11929076\n",
      "Iteration 63, loss = 0.46505426\n",
      "Iteration 656, loss = 0.34126554\n",
      "Iteration 2009, loss = 0.11943311\n",
      "Iteration 1351, loss = 0.25316660\n",
      "Iteration 64, loss = 0.46322782\n",
      "Iteration 33, loss = 0.75483054\n",
      "Iteration 2010, loss = 0.11911713\n",
      "Iteration 65, loss = 0.46139393\n",
      "Iteration 66, loss = 0.45968669\n",
      "Iteration 2011, loss = 0.11932018\n",
      "Iteration 431, loss = 0.35690883\n",
      "Iteration 1352, loss = 0.25307263\n",
      "Iteration 1930, loss = 0.19379995\n",
      "Iteration 67, loss = 0.45794573\n",
      "Iteration 68, loss = 0.45631117\n",
      "Iteration 657, loss = 0.34118503\n",
      "Iteration 1353, loss = 0.25300505\n",
      "Iteration 832, loss = 0.31511829\n",
      "Iteration 69, loss = 0.45461084Iteration 432, loss = 0.35673946\n",
      "Iteration 2012, loss = 0.11899643\n",
      "\n",
      "Iteration 2013, loss = 0.11881256\n",
      "Iteration 1354, loss = 0.25287264\n",
      "Iteration 70, loss = 0.45300264\n",
      "Iteration 2014, loss = 0.11863235\n",
      "Iteration 71, loss = 0.45144516\n",
      "Iteration 433, loss = 0.35647634\n",
      "Iteration 72, loss = 0.44986706\n",
      "Iteration 1355, loss = 0.25277628\n",
      "Iteration 2015, loss = 0.11858772\n",
      "Iteration 73, loss = 0.44837182\n",
      "Iteration 74, loss = 0.44684898\n",
      "Iteration 2016, loss = 0.11854395\n",
      "Iteration 75, loss = 0.44541489\n",
      "Iteration 76, loss = 0.44393642\n",
      "Iteration 1356, loss = 0.25272232\n",
      "Iteration 77, loss = 0.44255044\n",
      "Iteration 434, loss = 0.35630110\n",
      "Iteration 78, loss = 0.44114244\n",
      "Iteration 34, loss = 0.75289349\n",
      "Iteration 79, loss = 0.43976361\n",
      "Iteration 80, loss = 0.43847637\n",
      "Iteration 1357, loss = 0.25265956\n",
      "Iteration 81, loss = 0.43718302\n",
      "Iteration 658, loss = 0.34107125\n",
      "Iteration 82, loss = 0.43585740\n",
      "Iteration 2017, loss = 0.11836542\n",
      "Iteration 1931, loss = 0.19378749\n",
      "Iteration 83, loss = 0.43457403\n",
      "Iteration 833, loss = 0.31486768\n",
      "Iteration 84, loss = 0.43338337\n",
      "Iteration 85, loss = 0.43215714\n",
      "Iteration 1358, loss = 0.25249543\n",
      "Iteration 86, loss = 0.43096032\n",
      "Iteration 87, loss = 0.42979109\n",
      "Iteration 88, loss = 0.42862284\n",
      "Iteration 659, loss = 0.34098032\n",
      "Iteration 89, loss = 0.42749004\n",
      "Iteration 90, loss = 0.42637437\n",
      "Iteration 91, loss = 0.42529384\n",
      "Iteration 1932, loss = 0.19357078\n",
      "Iteration 92, loss = 0.42421162\n",
      "Iteration 1359, loss = 0.25240773\n",
      "Iteration 93, loss = 0.42314796\n",
      "Iteration 94, loss = 0.42212612\n",
      "Iteration 35, loss = 0.75094885\n",
      "Iteration 95, loss = 0.42109011\n",
      "Iteration 2018, loss = 0.11828421\n",
      "Iteration 1360, loss = 0.25231010\n",
      "Iteration 96, loss = 0.42007783\n",
      "Iteration 97, loss = 0.41910583\n",
      "Iteration 2019, loss = 0.11826744\n",
      "Iteration 98, loss = 0.41814427\n",
      "Iteration 1361, loss = 0.25232799\n",
      "Iteration 99, loss = 0.41717630\n",
      "Iteration 2020, loss = 0.11823040\n",
      "Iteration 834, loss = 0.31463554\n",
      "Iteration 2021, loss = 0.11812798\n",
      "Iteration 100, loss = 0.41623081\n",
      "Iteration 101, loss = 0.41529120\n",
      "Iteration 102, loss = 0.41441515\n",
      "Iteration 435, loss = 0.35611625\n",
      "Iteration 1362, loss = 0.25215143\n",
      "Iteration 2022, loss = 0.11791505\n",
      "Iteration 36, loss = 0.74895812\n",
      "Iteration 103, loss = 0.41351534\n",
      "Iteration 660, loss = 0.34080178\n",
      "Iteration 104, loss = 0.41260582\n",
      "Iteration 105, loss = 0.41174317\n",
      "Iteration 1933, loss = 0.19337759\n",
      "Iteration 106, loss = 0.41092213\n",
      "Iteration 107, loss = 0.41006456\n",
      "Iteration 108, loss = 0.40924908\n",
      "Iteration 436, loss = 0.35594961\n",
      "Iteration 109, loss = 0.40842542\n",
      "Iteration 110, loss = 0.40761408\n",
      "Iteration 2023, loss = 0.11775216\n",
      "Iteration 111, loss = 0.40682459\n",
      "Iteration 1363, loss = 0.25199810\n",
      "Iteration 112, loss = 0.40602900\n",
      "Iteration 113, loss = 0.40527960\n",
      "Iteration 114, loss = 0.40453732\n",
      "Iteration 115, loss = 0.40383417\n",
      "Iteration 2024, loss = 0.11770737\n",
      "Iteration 437, loss = 0.35583091\n",
      "Iteration 661, loss = 0.34069830\n",
      "Iteration 116, loss = 0.40303344\n",
      "Iteration 117, loss = 0.40233073\n",
      "Iteration 118, loss = 0.40161424\n",
      "Iteration 1364, loss = 0.25196596\n",
      "Iteration 119, loss = 0.40091388\n",
      "Iteration 120, loss = 0.40021010\n",
      "Iteration 121, loss = 0.39955949\n",
      "Iteration 122, loss = 0.39888558\n",
      "Iteration 123, loss = 0.39824932\n",
      "Iteration 2025, loss = 0.11763277\n",
      "Iteration 124, loss = 0.39757803\n",
      "Iteration 125, loss = 0.39692431\n",
      "Iteration 1934, loss = 0.19337885\n",
      "Iteration 126, loss = 0.39626737\n",
      "Iteration 37, loss = 0.74698558\n",
      "Iteration 127, loss = 0.39564296\n",
      "Iteration 438, loss = 0.35561256\n",
      "Iteration 128, loss = 0.39502393Iteration 1365, loss = 0.25185551\n",
      "\n",
      "Iteration 129, loss = 0.39439770\n",
      "Iteration 130, loss = 0.39381173\n",
      "Iteration 2026, loss = 0.11741168\n",
      "Iteration 131, loss = 0.39320786\n",
      "Iteration 132, loss = 0.39263111\n",
      "Iteration 1935, loss = 0.19336257\n",
      "Iteration 133, loss = 0.39202401\n",
      "Iteration 134, loss = 0.39145198\n",
      "Iteration 2027, loss = 0.11757999\n",
      "Iteration 135, loss = 0.39087583\n",
      "Iteration 439, loss = 0.35548028\n",
      "Iteration 136, loss = 0.39030894\n",
      "Iteration 1366, loss = 0.25172462\n",
      "Iteration 137, loss = 0.38975623\n",
      "Iteration 138, loss = 0.38922763\n",
      "Iteration 835, loss = 0.31457048\n",
      "Iteration 2028, loss = 0.11725036\n",
      "Iteration 139, loss = 0.38864455\n",
      "Iteration 140, loss = 0.38811584\n",
      "Iteration 141, loss = 0.38759387\n",
      "Iteration 2029, loss = 0.11724996\n",
      "Iteration 142, loss = 0.38707575\n",
      "Iteration 143, loss = 0.38655644\n",
      "Iteration 38, loss = 0.74494346\n",
      "Iteration 144, loss = 0.38604145\n",
      "Iteration 145, loss = 0.38552842\n",
      "Iteration 2030, loss = 0.11710935\n",
      "Iteration 146, loss = 0.38505207\n",
      "Iteration 1367, loss = 0.25167690\n",
      "Iteration 147, loss = 0.38455075\n",
      "Iteration 148, loss = 0.38404635\n",
      "Iteration 440, loss = 0.35524717\n",
      "Iteration 1936, loss = 0.19309409\n",
      "Iteration 149, loss = 0.38355534\n",
      "Iteration 2031, loss = 0.11705236\n",
      "Iteration 150, loss = 0.38308005\n",
      "Iteration 151, loss = 0.38260601\n",
      "Iteration 152, loss = 0.38217007\n",
      "Iteration 1368, loss = 0.25154385\n",
      "Iteration 662, loss = 0.34059549\n",
      "Iteration 153, loss = 0.38169344\n",
      "Iteration 154, loss = 0.38120710\n",
      "Iteration 441, loss = 0.35506734Iteration 155, loss = 0.38076963\n",
      "\n",
      "Iteration 1369, loss = 0.25143498\n",
      "Iteration 156, loss = 0.38032557\n",
      "Iteration 2032, loss = 0.11709197\n",
      "Iteration 1937, loss = 0.19297471\n",
      "Iteration 836, loss = 0.31420872\n",
      "Iteration 157, loss = 0.37988944\n",
      "Iteration 158, loss = 0.37945251\n",
      "Iteration 2033, loss = 0.11682793\n",
      "Iteration 159, loss = 0.37902447\n",
      "Iteration 39, loss = 0.74291337\n",
      "Iteration 1370, loss = 0.25137835\n",
      "Iteration 160, loss = 0.37858545\n",
      "Iteration 663, loss = 0.34047315\n",
      "Iteration 161, loss = 0.37816246\n",
      "Iteration 162, loss = 0.37774792\n",
      "Iteration 163, loss = 0.37731552\n",
      "Iteration 442, loss = 0.35492438\n",
      "Iteration 1371, loss = 0.25123312\n",
      "Iteration 1938, loss = 0.19285981\n",
      "Iteration 164, loss = 0.37689039\n",
      "Iteration 165, loss = 0.37648062\n",
      "Iteration 166, loss = 0.37611038\n",
      "Iteration 664, loss = 0.34038924\n",
      "Iteration 167, loss = 0.37569928\n",
      "Iteration 168, loss = 0.37528427\n",
      "Iteration 169, loss = 0.37489451\n",
      "Iteration 170, loss = 0.37450994\n",
      "Iteration 443, loss = 0.35475735\n",
      "Iteration 171, loss = 0.37413299\n",
      "Iteration 172, loss = 0.37373552\n",
      "Iteration 2034, loss = 0.11677234\n",
      "Iteration 173, loss = 0.37337696\n",
      "Iteration 1372, loss = 0.25117360\n",
      "Iteration 174, loss = 0.37299867\n",
      "Iteration 175, loss = 0.37260448\n",
      "Iteration 176, loss = 0.37223603\n",
      "Iteration 2035, loss = 0.11669706\n",
      "Iteration 1939, loss = 0.19276315\n",
      "Iteration 177, loss = 0.37188821\n",
      "Iteration 2036, loss = 0.11645672\n",
      "Iteration 178, loss = 0.37153634\n",
      "Iteration 665, loss = 0.34026434\n",
      "Iteration 40, loss = 0.74083645\n",
      "Iteration 2037, loss = 0.11632759\n",
      "Iteration 179, loss = 0.37116955\n",
      "Iteration 444, loss = 0.35457850\n",
      "Iteration 180, loss = 0.37081496\n",
      "Iteration 2038, loss = 0.11633977\n",
      "Iteration 837, loss = 0.31398654\n",
      "Iteration 1373, loss = 0.25112490\n",
      "Iteration 2039, loss = 0.11618743\n",
      "Iteration 181, loss = 0.37046669\n",
      "Iteration 2040, loss = 0.11605501\n",
      "Iteration 445, loss = 0.35439783\n",
      "Iteration 182, loss = 0.37010130\n",
      "Iteration 183, loss = 0.36977638\n",
      "Iteration 2041, loss = 0.11601871\n",
      "Iteration 184, loss = 0.36941797\n",
      "Iteration 1940, loss = 0.19267306\n",
      "Iteration 185, loss = 0.36908337\n",
      "Iteration 1374, loss = 0.25098496\n",
      "Iteration 2042, loss = 0.11583339\n",
      "Iteration 186, loss = 0.36871777\n",
      "Iteration 187, loss = 0.36840623\n",
      "Iteration 188, loss = 0.36807448\n",
      "Iteration 446, loss = 0.35425040\n",
      "Iteration 666, loss = 0.34014492\n",
      "Iteration 189, loss = 0.36775206\n",
      "Iteration 2043, loss = 0.11572985\n",
      "Iteration 190, loss = 0.36742323\n",
      "Iteration 838, loss = 0.31375622\n",
      "Iteration 191, loss = 0.36710173\n",
      "Iteration 192, loss = 0.36678321\n",
      "Iteration 2044, loss = 0.11561690\n",
      "Iteration 1941, loss = 0.19264566\n",
      "Iteration 193, loss = 0.36645842\n",
      "Iteration 194, loss = 0.36614031\n",
      "Iteration 195, loss = 0.36583182\n",
      "Iteration 196, loss = 0.36551986\n",
      "Iteration 197, loss = 0.36522632\n",
      "Iteration 198, loss = 0.36492011\n",
      "Iteration 199, loss = 0.36461132\n",
      "Iteration 200, loss = 0.36433385\n",
      "Iteration 201, loss = 0.36401092\n",
      "Iteration 202, loss = 0.36370599\n",
      "Iteration 203, loss = 0.36341793\n",
      "Iteration 447, loss = 0.35405524\n",
      "Iteration 204, loss = 0.36313249\n",
      "Iteration 205, loss = 0.36284548\n",
      "Iteration 206, loss = 0.36254562\n",
      "Iteration 207, loss = 0.36225760\n",
      "Iteration 208, loss = 0.36199088\n",
      "Iteration 209, loss = 0.36169503\n",
      "Iteration 667, loss = 0.34002306\n",
      "Iteration 1375, loss = 0.25083788\n",
      "Iteration 210, loss = 0.36141796\n",
      "Iteration 839, loss = 0.31363865\n",
      "Iteration 1942, loss = 0.19259327\n",
      "Iteration 2045, loss = 0.11557042\n",
      "Iteration 211, loss = 0.36114286\n",
      "Iteration 2046, loss = 0.11547449\n",
      "Iteration 41, loss = 0.73874551\n",
      "Iteration 1376, loss = 0.25074066\n",
      "Iteration 1943, loss = 0.19251540\n",
      "Iteration 212, loss = 0.36088063\n",
      "Iteration 2047, loss = 0.11539896\n",
      "Iteration 213, loss = 0.36059296\n",
      "Iteration 668, loss = 0.33991953\n",
      "Iteration 448, loss = 0.35389746\n",
      "Iteration 2048, loss = 0.11524097\n",
      "Iteration 840, loss = 0.31332028\n",
      "Iteration 1377, loss = 0.25065905\n",
      "Iteration 214, loss = 0.36032551\n",
      "Iteration 215, loss = 0.36008209\n",
      "Iteration 2049, loss = 0.11552948\n",
      "Iteration 2050, loss = 0.11508914\n",
      "Iteration 1378, loss = 0.25054334\n",
      "Iteration 216, loss = 0.35980038\n",
      "Iteration 449, loss = 0.35370385\n",
      "Iteration 1944, loss = 0.19226770\n",
      "Iteration 217, loss = 0.35953286\n",
      "Iteration 2051, loss = 0.11514096\n",
      "Iteration 669, loss = 0.33981044\n",
      "Iteration 218, loss = 0.35926002\n",
      "Iteration 219, loss = 0.35900194\n",
      "Iteration 1379, loss = 0.25045034\n",
      "Iteration 42, loss = 0.73661135\n",
      "Iteration 220, loss = 0.35875175\n",
      "Iteration 450, loss = 0.35353281\n",
      "Iteration 221, loss = 0.35849594\n",
      "Iteration 222, loss = 0.35823464\n",
      "Iteration 1945, loss = 0.19223617\n",
      "Iteration 223, loss = 0.35798461\n",
      "Iteration 224, loss = 0.35773992\n",
      "Iteration 451, loss = 0.35338676\n",
      "Iteration 225, loss = 0.35747401\n",
      "Iteration 226, loss = 0.35723900\n",
      "Iteration 227, loss = 0.35699109\n",
      "Iteration 2052, loss = 0.11491857\n",
      "Iteration 43, loss = 0.73440617\n",
      "Iteration 841, loss = 0.31316707\n",
      "Iteration 1946, loss = 0.19209752\n",
      "Iteration 1380, loss = 0.25037342\n",
      "Iteration 228, loss = 0.35676039\n",
      "Iteration 229, loss = 0.35649662\n",
      "Iteration 452, loss = 0.35326065\n",
      "Iteration 230, loss = 0.35625491\n",
      "Iteration 231, loss = 0.35601182\n",
      "Iteration 232, loss = 0.35577614\n",
      "Iteration 2053, loss = 0.11499767\n",
      "Iteration 233, loss = 0.35555053\n",
      "Iteration 1381, loss = 0.25026322\n",
      "Iteration 234, loss = 0.35529398\n",
      "Iteration 235, loss = 0.35507242\n",
      "Iteration 236, loss = 0.35483562\n",
      "Iteration 237, loss = 0.35461118\n",
      "Iteration 238, loss = 0.35436299\n",
      "Iteration 44, loss = 0.73219279\n",
      "Iteration 2054, loss = 0.11476023\n",
      "Iteration 239, loss = 0.35413012\n",
      "Iteration 670, loss = 0.33983042\n",
      "Iteration 240, loss = 0.35390044\n",
      "Iteration 1947, loss = 0.19197544\n",
      "Iteration 241, loss = 0.35368471\n",
      "Iteration 2055, loss = 0.11464312\n",
      "Iteration 1382, loss = 0.25018613\n",
      "Iteration 242, loss = 0.35347200\n",
      "Iteration 243, loss = 0.35323198\n",
      "Iteration 244, loss = 0.35301189\n",
      "Iteration 2056, loss = 0.11448210\n",
      "Iteration 245, loss = 0.35280562\n",
      "Iteration 246, loss = 0.35257070\n",
      "Iteration 247, loss = 0.35234668\n",
      "Iteration 453, loss = 0.35308309\n",
      "Iteration 248, loss = 0.35213181\n",
      "Iteration 2057, loss = 0.11441038\n",
      "Iteration 249, loss = 0.35191731\n",
      "Iteration 250, loss = 0.35170569\n",
      "Iteration 671, loss = 0.33961077\n",
      "Iteration 251, loss = 0.35148041\n",
      "Iteration 842, loss = 0.31298067\n",
      "Iteration 2058, loss = 0.11442649\n",
      "Iteration 252, loss = 0.35129006\n",
      "Iteration 253, loss = 0.35105735\n",
      "Iteration 254, loss = 0.35087408\n",
      "Iteration 255, loss = 0.35065070\n",
      "Iteration 2059, loss = 0.11427409\n",
      "Iteration 256, loss = 0.35044421\n",
      "Iteration 257, loss = 0.35023956\n",
      "Iteration 258, loss = 0.35002812\n",
      "Iteration 1383, loss = 0.25011372\n",
      "Iteration 259, loss = 0.34985230\n",
      "Iteration 2060, loss = 0.11412230\n",
      "Iteration 260, loss = 0.34962983\n",
      "Iteration 261, loss = 0.34944117\n",
      "Iteration 2061, loss = 0.11399882\n",
      "Iteration 45, loss = 0.72998978\n",
      "Iteration 262, loss = 0.34925129\n",
      "Iteration 263, loss = 0.34906681\n",
      "Iteration 672, loss = 0.33948857\n",
      "Iteration 454, loss = 0.35287626\n",
      "Iteration 264, loss = 0.34884322\n",
      "Iteration 1384, loss = 0.25003915\n",
      "Iteration 2062, loss = 0.11401508\n",
      "Iteration 265, loss = 0.34864772\n",
      "Iteration 2063, loss = 0.11397489\n",
      "Iteration 266, loss = 0.34845712\n",
      "Iteration 1385, loss = 0.24987540\n",
      "Iteration 2064, loss = 0.11386075\n",
      "Iteration 267, loss = 0.34827779\n",
      "Iteration 843, loss = 0.31268932\n",
      "Iteration 1948, loss = 0.19183335\n",
      "Iteration 268, loss = 0.34806993\n",
      "Iteration 2065, loss = 0.11362338\n",
      "Iteration 673, loss = 0.33945454\n",
      "Iteration 269, loss = 0.34789775\n",
      "Iteration 1386, loss = 0.24982165\n",
      "Iteration 2066, loss = 0.11365926\n",
      "Iteration 270, loss = 0.34771141\n",
      "Iteration 2067, loss = 0.11363922\n",
      "Iteration 271, loss = 0.34751625\n",
      "Iteration 272, loss = 0.34731192\n",
      "Iteration 273, loss = 0.34714075\n",
      "Iteration 274, loss = 0.34694200\n",
      "Iteration 1387, loss = 0.24971215\n",
      "Iteration 275, loss = 0.34677456\n",
      "Iteration 276, loss = 0.34658746\n",
      "Iteration 277, loss = 0.34641144\n",
      "Iteration 278, loss = 0.34621495\n",
      "Iteration 674, loss = 0.33927441Iteration 279, loss = 0.34604628\n",
      "Iteration 2068, loss = 0.11336876\n",
      "Iteration 844, loss = 0.31245353\n",
      "Iteration 280, loss = 0.34586005\n",
      "\n",
      "Iteration 281, loss = 0.34568754\n",
      "Iteration 455, loss = 0.35269295\n",
      "Iteration 282, loss = 0.34551040\n",
      "Iteration 283, loss = 0.34533584\n",
      "Iteration 284, loss = 0.34515598\n",
      "Iteration 285, loss = 0.34498841\n",
      "Iteration 286, loss = 0.34481957\n",
      "Iteration 287, loss = 0.34465034\n",
      "Iteration 288, loss = 0.34447454\n",
      "Iteration 289, loss = 0.34431978\n",
      "Iteration 2069, loss = 0.11348000\n",
      "Iteration 290, loss = 0.34412162\n",
      "Iteration 291, loss = 0.34396940\n",
      "Iteration 292, loss = 0.34379909\n",
      "Iteration 456, loss = 0.35256222\n",
      "Iteration 293, loss = 0.34363396\n",
      "Iteration 294, loss = 0.34348325\n",
      "Iteration 295, loss = 0.34330247\n",
      "Iteration 1388, loss = 0.24959449\n",
      "Iteration 296, loss = 0.34313966\n",
      "Iteration 2070, loss = 0.11323426\n",
      "Iteration 297, loss = 0.34297956\n",
      "Iteration 1949, loss = 0.19174265\n",
      "Iteration 46, loss = 0.72771520\n",
      "Iteration 2071, loss = 0.11310641\n",
      "Iteration 298, loss = 0.34282181\n",
      "Iteration 299, loss = 0.34266014\n",
      "Iteration 2072, loss = 0.11297198\n",
      "Iteration 675, loss = 0.33917650\n",
      "Iteration 300, loss = 0.34249708\n",
      "Iteration 2073, loss = 0.11286621\n",
      "Iteration 301, loss = 0.34233441\n",
      "Iteration 1389, loss = 0.24955992\n",
      "Iteration 2074, loss = 0.11285977\n",
      "Iteration 302, loss = 0.34220961\n",
      "Iteration 457, loss = 0.35238805\n",
      "Iteration 845, loss = 0.31231958\n",
      "Iteration 303, loss = 0.34201874\n",
      "Iteration 1950, loss = 0.19167391\n",
      "Iteration 304, loss = 0.34186760\n",
      "Iteration 676, loss = 0.33902984\n",
      "Iteration 305, loss = 0.34171786\n",
      "Iteration 306, loss = 0.34155734\n",
      "Iteration 2075, loss = 0.11261840\n",
      "Iteration 307, loss = 0.34141227\n",
      "Iteration 1390, loss = 0.24942852\n",
      "Iteration 308, loss = 0.34125221\n",
      "Iteration 2076, loss = 0.11277923\n",
      "Iteration 47, loss = 0.72539333\n",
      "Iteration 309, loss = 0.34109820\n",
      "Iteration 310, loss = 0.34094482\n",
      "Iteration 2077, loss = 0.11250552\n",
      "Iteration 311, loss = 0.34080341Iteration 1951, loss = 0.19156381\n",
      "\n",
      "Iteration 677, loss = 0.33902494\n",
      "Iteration 2078, loss = 0.11243124\n",
      "Iteration 312, loss = 0.34065571\n",
      "Iteration 2079, loss = 0.11236142\n",
      "Iteration 313, loss = 0.34050415Iteration 1391, loss = 0.24928178\n",
      "\n",
      "Iteration 458, loss = 0.35222946\n",
      "Iteration 314, loss = 0.34035151\n",
      "Iteration 315, loss = 0.34019228\n",
      "Iteration 2080, loss = 0.11240243\n",
      "Iteration 316, loss = 0.34005696\n",
      "Iteration 846, loss = 0.31199795\n",
      "Iteration 317, loss = 0.33990504\n",
      "Iteration 318, loss = 0.33976141\n",
      "Iteration 319, loss = 0.33961373\n",
      "Iteration 2081, loss = 0.11219245\n",
      "Iteration 320, loss = 0.33946616\n",
      "Iteration 321, loss = 0.33932866\n",
      "Iteration 322, loss = 0.33919298\n",
      "Iteration 323, loss = 0.33904036\n",
      "Iteration 2082, loss = 0.11238083\n",
      "Iteration 324, loss = 0.33889436\n",
      "Iteration 1952, loss = 0.19146060\n",
      "Iteration 48, loss = 0.72305627\n",
      "Iteration 325, loss = 0.33876063\n",
      "Iteration 326, loss = 0.33861279\n",
      "Iteration 327, loss = 0.33846995\n",
      "Iteration 328, loss = 0.33834323\n",
      "Iteration 1392, loss = 0.24928759\n",
      "Iteration 329, loss = 0.33819394\n",
      "Iteration 2083, loss = 0.11215550\n",
      "Iteration 330, loss = 0.33806936\n",
      "Iteration 1393, loss = 0.24915136\n",
      "Iteration 1953, loss = 0.19139731\n",
      "Iteration 331, loss = 0.33791241\n",
      "Iteration 678, loss = 0.33881905\n",
      "Iteration 332, loss = 0.33778767\n",
      "Iteration 2084, loss = 0.11185650\n",
      "Iteration 459, loss = 0.35202738\n",
      "Iteration 333, loss = 0.33767134\n",
      "Iteration 847, loss = 0.31178782\n",
      "Iteration 2085, loss = 0.11177898\n",
      "Iteration 334, loss = 0.33750696\n",
      "Iteration 335, loss = 0.33738140\n",
      "Iteration 336, loss = 0.33725473\n",
      "Iteration 337, loss = 0.33711805\n",
      "Iteration 338, loss = 0.33698494\n",
      "Iteration 339, loss = 0.33685989\n",
      "Iteration 340, loss = 0.33671443\n",
      "Iteration 1394, loss = 0.24900043\n",
      "Iteration 341, loss = 0.33659376\n",
      "Iteration 342, loss = 0.33646374\n",
      "Iteration 343, loss = 0.33632742\n",
      "Iteration 344, loss = 0.33620287\n",
      "Iteration 2086, loss = 0.11171045\n",
      "Iteration 49, loss = 0.72064447\n",
      "Iteration 345, loss = 0.33607314\n",
      "Iteration 1954, loss = 0.19124958\n",
      "Iteration 346, loss = 0.33594253\n",
      "Iteration 347, loss = 0.33581302\n",
      "Iteration 348, loss = 0.33570328\n",
      "Iteration 349, loss = 0.33556982\n",
      "Iteration 350, loss = 0.33544541\n",
      "Iteration 351, loss = 0.33532195\n",
      "Iteration 352, loss = 0.33519680\n",
      "Iteration 2087, loss = 0.11163205\n",
      "Iteration 679, loss = 0.33870285\n",
      "Iteration 353, loss = 0.33507275\n",
      "Iteration 354, loss = 0.33494582\n",
      "Iteration 355, loss = 0.33482689\n",
      "Iteration 356, loss = 0.33469750\n",
      "Iteration 2088, loss = 0.11161112\n",
      "Iteration 460, loss = 0.35192823\n",
      "Iteration 357, loss = 0.33459112\n",
      "Iteration 1395, loss = 0.24892639\n",
      "Iteration 2089, loss = 0.11141543\n",
      "Iteration 2090, loss = 0.11127612\n",
      "Iteration 461, loss = 0.35168820\n",
      "Iteration 358, loss = 0.33446479\n",
      "Iteration 680, loss = 0.33863507\n",
      "Iteration 2091, loss = 0.11116885\n",
      "Iteration 50, loss = 0.71821844\n",
      "Iteration 2092, loss = 0.11106620\n",
      "Iteration 462, loss = 0.35149504\n",
      "Iteration 1396, loss = 0.24896866\n",
      "Iteration 2093, loss = 0.11098649\n",
      "Iteration 359, loss = 0.33434918\n",
      "Iteration 360, loss = 0.33421707\n",
      "Iteration 2094, loss = 0.11097646\n",
      "Iteration 361, loss = 0.33410615\n",
      "Iteration 362, loss = 0.33398006\n",
      "Iteration 363, loss = 0.33386761\n",
      "Iteration 2095, loss = 0.11104209\n",
      "Iteration 681, loss = 0.33849061\n",
      "Iteration 364, loss = 0.33374535\n",
      "Iteration 1397, loss = 0.24871327\n",
      "Iteration 2096, loss = 0.11067621\n",
      "Iteration 1955, loss = 0.19130566\n",
      "Iteration 365, loss = 0.33364337\n",
      "Iteration 848, loss = 0.31157063\n",
      "Iteration 366, loss = 0.33351171\n",
      "Iteration 367, loss = 0.33338991\n",
      "Iteration 51, loss = 0.71580103\n",
      "Iteration 463, loss = 0.35133542\n",
      "Iteration 1398, loss = 0.24861182\n",
      "Iteration 368, loss = 0.33326836\n",
      "Iteration 369, loss = 0.33316178\n",
      "Iteration 1956, loss = 0.19105647\n",
      "Iteration 370, loss = 0.33304416\n",
      "Iteration 1399, loss = 0.24855698\n",
      "Iteration 371, loss = 0.33292919\n",
      "Iteration 372, loss = 0.33282036\n",
      "Iteration 373, loss = 0.33270300\n",
      "Iteration 1400, loss = 0.24842652\n",
      "Iteration 374, loss = 0.33259508\n",
      "Iteration 682, loss = 0.33837571\n",
      "Iteration 375, loss = 0.33247221\n",
      "Iteration 376, loss = 0.33239570\n",
      "Iteration 1401, loss = 0.24839304\n",
      "Iteration 377, loss = 0.33226978\n",
      "Iteration 378, loss = 0.33214895\n",
      "Iteration 2097, loss = 0.11071074\n",
      "Iteration 849, loss = 0.31134955\n",
      "Iteration 52, loss = 0.71323866\n",
      "Iteration 379, loss = 0.33202862\n",
      "Iteration 380, loss = 0.33191902\n",
      "Iteration 464, loss = 0.35117357\n",
      "Iteration 1957, loss = 0.19098036\n",
      "Iteration 381, loss = 0.33181082\n",
      "Iteration 382, loss = 0.33169145\n",
      "Iteration 383, loss = 0.33160490\n",
      "Iteration 1402, loss = 0.24831397\n",
      "Iteration 2098, loss = 0.11061541\n",
      "Iteration 384, loss = 0.33147830\n",
      "Iteration 385, loss = 0.33136741\n",
      "Iteration 386, loss = 0.33126153\n",
      "Iteration 387, loss = 0.33116259\n",
      "Iteration 2099, loss = 0.11046105\n",
      "Iteration 388, loss = 0.33103914\n",
      "Iteration 389, loss = 0.33093543\n",
      "Iteration 390, loss = 0.33083115\n",
      "Iteration 391, loss = 0.33074088\n",
      "Iteration 2100, loss = 0.11034835\n",
      "Iteration 392, loss = 0.33061867\n",
      "Iteration 393, loss = 0.33051830\n",
      "Iteration 1958, loss = 0.19089225\n",
      "Iteration 394, loss = 0.33039790\n",
      "Iteration 683, loss = 0.33827391\n",
      "Iteration 2101, loss = 0.11023547\n",
      "Iteration 1403, loss = 0.24813710\n",
      "Iteration 850, loss = 0.31112634\n",
      "Iteration 395, loss = 0.33031479\n",
      "Iteration 465, loss = 0.35100668\n",
      "Iteration 396, loss = 0.33020392\n",
      "Iteration 2102, loss = 0.11030588\n",
      "Iteration 397, loss = 0.33008790\n",
      "Iteration 2103, loss = 0.11021996\n",
      "Iteration 53, loss = 0.71080361\n",
      "Iteration 2104, loss = 0.10995379\n",
      "Iteration 398, loss = 0.32999075\n",
      "Iteration 2105, loss = 0.11007245\n",
      "Iteration 399, loss = 0.32988390\n",
      "Iteration 851, loss = 0.31105015\n",
      "Iteration 400, loss = 0.32978409\n",
      "Iteration 401, loss = 0.32968216\n",
      "Iteration 466, loss = 0.35087353\n",
      "Iteration 2106, loss = 0.10991304\n",
      "Iteration 402, loss = 0.32958699\n",
      "Iteration 403, loss = 0.32947817\n",
      "Iteration 404, loss = 0.32938758\n",
      "Iteration 405, loss = 0.32927845\n",
      "Iteration 406, loss = 0.32918272\n",
      "Iteration 2107, loss = 0.10976482\n",
      "Iteration 407, loss = 0.32907324\n",
      "Iteration 408, loss = 0.32898524\n",
      "Iteration 409, loss = 0.32888128\n",
      "Iteration 467, loss = 0.35074924\n",
      "Iteration 410, loss = 0.32879306\n",
      "Iteration 411, loss = 0.32868834\n",
      "Iteration 2108, loss = 0.10964424\n",
      "Iteration 412, loss = 0.32858468\n",
      "Iteration 1959, loss = 0.19084251\n",
      "Iteration 413, loss = 0.32849062\n",
      "Iteration 414, loss = 0.32839690\n",
      "Iteration 54, loss = 0.70821387\n",
      "Iteration 415, loss = 0.32830965\n",
      "Iteration 416, loss = 0.32821076\n",
      "Iteration 684, loss = 0.33819900\n",
      "Iteration 468, loss = 0.35054931\n",
      "Iteration 2109, loss = 0.10977597\n",
      "Iteration 417, loss = 0.32810978\n",
      "Iteration 418, loss = 0.32801968\n",
      "Iteration 2110, loss = 0.10938876\n",
      "Iteration 419, loss = 0.32792728\n",
      "Iteration 469, loss = 0.35032116\n",
      "Iteration 420, loss = 0.32783951\n",
      "Iteration 2111, loss = 0.10933154\n",
      "Iteration 1404, loss = 0.24808447\n",
      "Iteration 421, loss = 0.32773567\n",
      "Iteration 422, loss = 0.32764853\n",
      "Iteration 852, loss = 0.31077727\n",
      "Iteration 423, loss = 0.32757290Iteration 470, loss = 0.35021874\n",
      "\n",
      "Iteration 1405, loss = 0.24797117\n",
      "Iteration 424, loss = 0.32745863\n",
      "Iteration 2112, loss = 0.10941832\n",
      "Iteration 425, loss = 0.32737187\n",
      "Iteration 1960, loss = 0.19066201\n",
      "Iteration 426, loss = 0.32729376\n",
      "Iteration 471, loss = 0.35006362\n",
      "Iteration 427, loss = 0.32719263\n",
      "Iteration 2113, loss = 0.10923539\n",
      "Iteration 428, loss = 0.32710472\n",
      "Iteration 685, loss = 0.33807166\n",
      "Iteration 429, loss = 0.32701387\n",
      "Iteration 430, loss = 0.32692652\n",
      "Iteration 2114, loss = 0.10911232\n",
      "Iteration 1406, loss = 0.24790599\n",
      "Iteration 431, loss = 0.32683409\n",
      "Iteration 432, loss = 0.32675298\n",
      "Iteration 433, loss = 0.32665633\n",
      "Iteration 472, loss = 0.34998252\n",
      "Iteration 1407, loss = 0.24771603\n",
      "Iteration 434, loss = 0.32656695\n",
      "Iteration 435, loss = 0.32648352\n",
      "Iteration 2115, loss = 0.10902036\n",
      "Iteration 436, loss = 0.32639615\n",
      "Iteration 1961, loss = 0.19056032\n",
      "Iteration 1408, loss = 0.24763224\n",
      "Iteration 437, loss = 0.32630602\n",
      "Iteration 438, loss = 0.32622400\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1409, loss = 0.24754595\n",
      "Iteration 853, loss = 0.31058910\n",
      "Iteration 473, loss = 0.34967413\n",
      "Iteration 1410, loss = 0.24744620\n",
      "Iteration 2116, loss = 0.10898863\n",
      "Iteration 474, loss = 0.34963092\n",
      "Iteration 2117, loss = 0.10882696\n",
      "Iteration 2118, loss = 0.10881875\n",
      "Iteration 55, loss = 0.70564274\n",
      "Iteration 1, loss = 0.68502223\n",
      "Iteration 1411, loss = 0.24735675\n",
      "Iteration 2, loss = 0.68203827\n",
      "Iteration 686, loss = 0.33795386\n",
      "Iteration 3, loss = 0.67746946\n",
      "Iteration 2119, loss = 0.10861824\n",
      "Iteration 1962, loss = 0.19046833\n",
      "Iteration 4, loss = 0.67194392\n",
      "Iteration 2120, loss = 0.10872662\n",
      "Iteration 2121, loss = 0.10856172\n",
      "Iteration 56, loss = 0.70300649\n",
      "Iteration 5, loss = 0.66583088\n",
      "Iteration 6, loss = 0.65936781\n",
      "Iteration 2122, loss = 0.10836526\n",
      "Iteration 475, loss = 0.34936273\n",
      "Iteration 7, loss = 0.65266833\n",
      "Iteration 8, loss = 0.64575007\n",
      "Iteration 1412, loss = 0.24726489\n",
      "Iteration 9, loss = 0.63893379\n",
      "Iteration 10, loss = 0.63241167\n",
      "Iteration 1963, loss = 0.19033837\n",
      "Iteration 11, loss = 0.62595702\n",
      "Iteration 2123, loss = 0.10847496\n",
      "Iteration 12, loss = 0.61954001\n",
      "Iteration 13, loss = 0.61344434\n",
      "Iteration 1413, loss = 0.24715742\n",
      "Iteration 14, loss = 0.60759429\n",
      "Iteration 687, loss = 0.33781959\n",
      "Iteration 15, loss = 0.60200707\n",
      "Iteration 476, loss = 0.34916772\n",
      "Iteration 16, loss = 0.59641882\n",
      "Iteration 854, loss = 0.31035101\n",
      "Iteration 17, loss = 0.59087108\n",
      "Iteration 2124, loss = 0.10823110\n",
      "Iteration 688, loss = 0.33773907\n",
      "Iteration 18, loss = 0.58591694\n",
      "Iteration 1414, loss = 0.24706936\n",
      "Iteration 19, loss = 0.58062223\n",
      "Iteration 2125, loss = 0.10819694\n",
      "Iteration 20, loss = 0.57591845\n",
      "Iteration 21, loss = 0.57113772\n",
      "Iteration 1415, loss = 0.24699435\n",
      "Iteration 2126, loss = 0.10796591\n",
      "Iteration 22, loss = 0.56649390\n",
      "Iteration 23, loss = 0.56221239\n",
      "Iteration 2127, loss = 0.10786543\n",
      "Iteration 689, loss = 0.33761120\n",
      "Iteration 477, loss = 0.34900143\n",
      "Iteration 2128, loss = 0.10781436\n",
      "Iteration 1964, loss = 0.19026062\n",
      "Iteration 1416, loss = 0.24684627\n",
      "Iteration 24, loss = 0.55788299\n",
      "Iteration 690, loss = 0.33749024\n",
      "Iteration 25, loss = 0.55374369\n",
      "Iteration 1965, loss = 0.19017860\n",
      "Iteration 26, loss = 0.54960676\n",
      "Iteration 1417, loss = 0.24675851\n",
      "Iteration 27, loss = 0.54564148\n",
      "Iteration 691, loss = 0.33743388\n",
      "Iteration 855, loss = 0.31008116\n",
      "Iteration 28, loss = 0.54181176\n",
      "Iteration 57, loss = 0.70029308\n",
      "Iteration 29, loss = 0.53810603\n",
      "Iteration 30, loss = 0.53440219\n",
      "Iteration 2129, loss = 0.10774138\n",
      "Iteration 31, loss = 0.53092732\n",
      "Iteration 2130, loss = 0.10770572\n",
      "Iteration 32, loss = 0.52739992\n",
      "Iteration 2131, loss = 0.10752998\n",
      "Iteration 33, loss = 0.52401346\n",
      "Iteration 2132, loss = 0.10749019\n",
      "Iteration 34, loss = 0.52070262\n",
      "Iteration 2133, loss = 0.10736128\n",
      "Iteration 35, loss = 0.51749809\n",
      "Iteration 478, loss = 0.34885223\n",
      "Iteration 2134, loss = 0.10735182\n",
      "Iteration 36, loss = 0.51428337\n",
      "Iteration 37, loss = 0.51128140\n",
      "Iteration 1966, loss = 0.19005291\n",
      "Iteration 38, loss = 0.50823018\n",
      "Iteration 1418, loss = 0.24667636\n",
      "Iteration 39, loss = 0.50536658\n",
      "Iteration 692, loss = 0.33726317\n",
      "Iteration 479, loss = 0.34870249\n",
      "Iteration 40, loss = 0.50247013\n",
      "Iteration 2135, loss = 0.10721047\n",
      "Iteration 41, loss = 0.49972157\n",
      "Iteration 58, loss = 0.69759600\n",
      "Iteration 42, loss = 0.49695502\n",
      "Iteration 2136, loss = 0.10722414\n",
      "Iteration 43, loss = 0.49419502\n",
      "Iteration 44, loss = 0.49169605\n",
      "Iteration 45, loss = 0.48905585\n",
      "Iteration 46, loss = 0.48661311\n",
      "Iteration 2137, loss = 0.10705558\n",
      "Iteration 1967, loss = 0.19004603\n",
      "Iteration 47, loss = 0.48410174\n",
      "Iteration 2138, loss = 0.10699614\n",
      "Iteration 480, loss = 0.34851445\n",
      "Iteration 48, loss = 0.48171083\n",
      "Iteration 2139, loss = 0.10679352\n",
      "Iteration 49, loss = 0.47936755\n",
      "Iteration 50, loss = 0.47707700\n",
      "Iteration 1419, loss = 0.24661071\n",
      "Iteration 2140, loss = 0.10681881\n",
      "Iteration 51, loss = 0.47482081\n",
      "Iteration 52, loss = 0.47258029\n",
      "Iteration 53, loss = 0.47049040\n",
      "Iteration 693, loss = 0.33715337\n",
      "Iteration 1968, loss = 0.18988227\n",
      "Iteration 54, loss = 0.46831689\n",
      "Iteration 2141, loss = 0.10682694\n",
      "Iteration 55, loss = 0.46628632\n",
      "Iteration 56, loss = 0.46425752\n",
      "Iteration 57, loss = 0.46221566\n",
      "Iteration 58, loss = 0.46028948\n",
      "Iteration 856, loss = 0.30976245\n",
      "Iteration 59, loss = 0.45838094\n",
      "Iteration 481, loss = 0.34838811\n",
      "Iteration 1420, loss = 0.24646809\n",
      "Iteration 60, loss = 0.45652009\n",
      "Iteration 61, loss = 0.45464738\n",
      "Iteration 694, loss = 0.33706042\n",
      "Iteration 59, loss = 0.69480637\n",
      "Iteration 2142, loss = 0.10668635\n",
      "Iteration 2143, loss = 0.10645801\n",
      "Iteration 1969, loss = 0.18985053\n",
      "Iteration 482, loss = 0.34821277\n",
      "Iteration 62, loss = 0.45286428\n",
      "Iteration 1421, loss = 0.24637642\n",
      "Iteration 63, loss = 0.45110119\n",
      "Iteration 2144, loss = 0.10639587\n",
      "Iteration 64, loss = 0.44936186\n",
      "Iteration 2145, loss = 0.10644337\n",
      "Iteration 2146, loss = 0.10626223\n",
      "Iteration 65, loss = 0.44764132\n",
      "Iteration 1422, loss = 0.24626594\n",
      "Iteration 2147, loss = 0.10621643\n",
      "Iteration 66, loss = 0.44599382\n",
      "Iteration 483, loss = 0.34803893\n",
      "Iteration 67, loss = 0.44437593\n",
      "Iteration 857, loss = 0.30961968\n",
      "Iteration 1970, loss = 0.18969592\n",
      "Iteration 60, loss = 0.69212502\n",
      "Iteration 68, loss = 0.44274192\n",
      "Iteration 69, loss = 0.44116305\n",
      "Iteration 70, loss = 0.43961711\n",
      "Iteration 2148, loss = 0.10611997\n",
      "Iteration 71, loss = 0.43805276\n",
      "Iteration 72, loss = 0.43661159\n",
      "Iteration 73, loss = 0.43516892\n",
      "Iteration 2149, loss = 0.10603360\n",
      "Iteration 1423, loss = 0.24618209\n",
      "Iteration 74, loss = 0.43367084\n",
      "Iteration 75, loss = 0.43226574\n",
      "Iteration 695, loss = 0.33693577\n",
      "Iteration 76, loss = 0.43094972\n",
      "Iteration 2150, loss = 0.10594964\n",
      "Iteration 77, loss = 0.42956322\n",
      "Iteration 484, loss = 0.34786420\n",
      "Iteration 78, loss = 0.42822307\n",
      "Iteration 2151, loss = 0.10581489\n",
      "Iteration 79, loss = 0.42686404\n",
      "Iteration 80, loss = 0.42557630\n",
      "Iteration 81, loss = 0.42428135\n",
      "Iteration 2152, loss = 0.10567502\n",
      "Iteration 82, loss = 0.42301640\n",
      "Iteration 83, loss = 0.42181146\n",
      "Iteration 84, loss = 0.42064684\n",
      "Iteration 2153, loss = 0.10565743\n",
      "Iteration 1424, loss = 0.24607897\n",
      "Iteration 85, loss = 0.41936510\n",
      "Iteration 86, loss = 0.41826620\n",
      "Iteration 87, loss = 0.41712906\n",
      "Iteration 88, loss = 0.41594772\n",
      "Iteration 485, loss = 0.34768299\n",
      "Iteration 89, loss = 0.41484711\n",
      "Iteration 1425, loss = 0.24602337\n",
      "Iteration 90, loss = 0.41371847\n",
      "Iteration 696, loss = 0.33684153\n",
      "Iteration 1971, loss = 0.18957447\n",
      "Iteration 2154, loss = 0.10566157\n",
      "Iteration 61, loss = 0.68923318\n",
      "Iteration 858, loss = 0.30940617\n",
      "Iteration 91, loss = 0.41268821\n",
      "Iteration 486, loss = 0.34754055\n",
      "Iteration 92, loss = 0.41162794\n",
      "Iteration 93, loss = 0.41057729\n",
      "Iteration 1426, loss = 0.24589018\n",
      "Iteration 94, loss = 0.40958454\n",
      "Iteration 95, loss = 0.40854366\n",
      "Iteration 2155, loss = 0.10547328\n",
      "Iteration 96, loss = 0.40754146\n",
      "Iteration 97, loss = 0.40657745\n",
      "Iteration 98, loss = 0.40559304\n",
      "Iteration 487, loss = 0.34738268\n",
      "Iteration 859, loss = 0.30914670\n",
      "Iteration 99, loss = 0.40468463\n",
      "Iteration 100, loss = 0.40372271\n",
      "Iteration 2156, loss = 0.10541518\n",
      "Iteration 101, loss = 0.40282905\n",
      "Iteration 1427, loss = 0.24584765\n",
      "Iteration 102, loss = 0.40190323\n",
      "Iteration 697, loss = 0.33672232\n",
      "Iteration 2157, loss = 0.10535613\n",
      "Iteration 1972, loss = 0.18949509\n",
      "Iteration 488, loss = 0.34721758\n",
      "Iteration 2158, loss = 0.10543765\n",
      "Iteration 103, loss = 0.40098893\n",
      "Iteration 62, loss = 0.68636148\n",
      "Iteration 104, loss = 0.40011158\n",
      "Iteration 1428, loss = 0.24585134\n",
      "Iteration 2159, loss = 0.10512441\n",
      "Iteration 105, loss = 0.39924317\n",
      "Iteration 106, loss = 0.39840646\n",
      "Iteration 489, loss = 0.34702584\n",
      "Iteration 107, loss = 0.39756294\n",
      "Iteration 698, loss = 0.33662143\n",
      "Iteration 2160, loss = 0.10511094\n",
      "Iteration 108, loss = 0.39674104\n",
      "Iteration 860, loss = 0.30895852\n",
      "Iteration 109, loss = 0.39590245\n",
      "Iteration 2161, loss = 0.10500366\n",
      "Iteration 110, loss = 0.39509011\n",
      "Iteration 2162, loss = 0.10493190\n",
      "Iteration 2163, loss = 0.10486744\n",
      "Iteration 1429, loss = 0.24560574\n",
      "Iteration 63, loss = 0.68352712\n",
      "Iteration 2164, loss = 0.10486246Iteration 111, loss = 0.39430543\n",
      "Iteration 1973, loss = 0.18934605\n",
      "Iteration 112, loss = 0.39356340\n",
      "Iteration 113, loss = 0.39276579\n",
      "\n",
      "Iteration 699, loss = 0.33651458\n",
      "Iteration 114, loss = 0.39198941\n",
      "Iteration 115, loss = 0.39124804\n",
      "Iteration 1430, loss = 0.24550399\n",
      "Iteration 116, loss = 0.39048973\n",
      "Iteration 117, loss = 0.38976929\n",
      "Iteration 2165, loss = 0.10462155\n",
      "Iteration 118, loss = 0.38905702\n",
      "Iteration 119, loss = 0.38835373\n",
      "Iteration 1431, loss = 0.24538581\n",
      "Iteration 120, loss = 0.38766822\n",
      "Iteration 121, loss = 0.38694863\n",
      "Iteration 122, loss = 0.38627005\n",
      "Iteration 490, loss = 0.34701649\n",
      "Iteration 2166, loss = 0.10464666\n",
      "Iteration 123, loss = 0.38560998\n",
      "Iteration 124, loss = 0.38494214\n",
      "Iteration 1432, loss = 0.24528416\n",
      "Iteration 125, loss = 0.38427976\n",
      "Iteration 2167, loss = 0.10441803\n",
      "Iteration 2168, loss = 0.10437356\n",
      "Iteration 861, loss = 0.30884176\n",
      "Iteration 126, loss = 0.38367446\n",
      "Iteration 2169, loss = 0.10437483\n",
      "Iteration 491, loss = 0.34670937\n",
      "Iteration 1974, loss = 0.18936535\n",
      "Iteration 2170, loss = 0.10438844\n",
      "Iteration 700, loss = 0.33642005\n",
      "Iteration 127, loss = 0.38302931\n",
      "Iteration 2171, loss = 0.10417281\n",
      "Iteration 492, loss = 0.34653811\n",
      "Iteration 128, loss = 0.38238295\n",
      "Iteration 1433, loss = 0.24520078\n",
      "Iteration 2172, loss = 0.10403048\n",
      "Iteration 1975, loss = 0.18923417\n",
      "Iteration 129, loss = 0.38177392\n",
      "Iteration 1434, loss = 0.24511294\n",
      "Iteration 2173, loss = 0.10391949\n",
      "Iteration 130, loss = 0.38115546\n",
      "Iteration 131, loss = 0.38054754\n",
      "Iteration 2174, loss = 0.10391366\n",
      "Iteration 132, loss = 0.37996150\n",
      "Iteration 64, loss = 0.68054016\n",
      "Iteration 133, loss = 0.37938617\n",
      "Iteration 493, loss = 0.34637447\n",
      "Iteration 2175, loss = 0.10381978\n",
      "Iteration 862, loss = 0.30854850\n",
      "Iteration 1435, loss = 0.24499915\n",
      "Iteration 2176, loss = 0.10364916\n",
      "Iteration 701, loss = 0.33634378\n",
      "Iteration 1976, loss = 0.18908166\n",
      "Iteration 494, loss = 0.34618579\n",
      "Iteration 134, loss = 0.37879746\n",
      "Iteration 1436, loss = 0.24490956\n",
      "Iteration 135, loss = 0.37826431\n",
      "Iteration 136, loss = 0.37765500\n",
      "Iteration 137, loss = 0.37710855\n",
      "Iteration 2177, loss = 0.10357841\n",
      "Iteration 65, loss = 0.67777068\n",
      "Iteration 138, loss = 0.37654149\n",
      "Iteration 139, loss = 0.37602594\n",
      "Iteration 1437, loss = 0.24483275\n",
      "Iteration 702, loss = 0.33618703\n",
      "Iteration 495, loss = 0.34604829\n",
      "Iteration 2178, loss = 0.10351768\n",
      "Iteration 140, loss = 0.37547275\n",
      "Iteration 1977, loss = 0.18900602\n",
      "Iteration 141, loss = 0.37492942\n",
      "Iteration 863, loss = 0.30835200\n",
      "Iteration 142, loss = 0.37441201\n",
      "Iteration 1438, loss = 0.24471106\n",
      "Iteration 2179, loss = 0.10358883\n",
      "Iteration 143, loss = 0.37392917\n",
      "Iteration 496, loss = 0.34590636\n",
      "Iteration 144, loss = 0.37340271\n",
      "Iteration 145, loss = 0.37288901\n",
      "Iteration 2180, loss = 0.10334040\n",
      "Iteration 146, loss = 0.37240923\n",
      "Iteration 147, loss = 0.37189843\n",
      "Iteration 148, loss = 0.37142752\n",
      "Iteration 149, loss = 0.37092402\n",
      "Iteration 150, loss = 0.37046992\n",
      "Iteration 2181, loss = 0.10325586\n",
      "Iteration 151, loss = 0.37001886\n",
      "Iteration 66, loss = 0.67474056Iteration 152, loss = 0.36953249\n",
      "\n",
      "Iteration 497, loss = 0.34572759\n",
      "Iteration 153, loss = 0.36907042\n",
      "Iteration 1439, loss = 0.24459907Iteration 703, loss = 0.33608886\n",
      "\n",
      "Iteration 154, loss = 0.36860933\n",
      "Iteration 2182, loss = 0.10328198\n",
      "Iteration 155, loss = 0.36817736\n",
      "Iteration 864, loss = 0.30809730\n",
      "Iteration 156, loss = 0.36772716\n",
      "Iteration 498, loss = 0.34557144\n",
      "Iteration 1440, loss = 0.24449578\n",
      "Iteration 1978, loss = 0.18887841\n",
      "Iteration 704, loss = 0.33595451\n",
      "Iteration 67, loss = 0.67179693\n",
      "Iteration 157, loss = 0.36728503\n",
      "Iteration 2183, loss = 0.10310545\n",
      "Iteration 158, loss = 0.36685599\n",
      "Iteration 1441, loss = 0.24442942\n",
      "Iteration 159, loss = 0.36642568\n",
      "Iteration 1442, loss = 0.24441803\n",
      "Iteration 68, loss = 0.66886008\n",
      "Iteration 1979, loss = 0.18881604\n",
      "Iteration 1443, loss = 0.24419320\n",
      "Iteration 160, loss = 0.36599169\n",
      "Iteration 161, loss = 0.36558857\n",
      "Iteration 499, loss = 0.34541142\n",
      "Iteration 162, loss = 0.36518115\n",
      "Iteration 163, loss = 0.36476361\n",
      "Iteration 865, loss = 0.30789412\n",
      "Iteration 164, loss = 0.36434559\n",
      "Iteration 165, loss = 0.36394292\n",
      "Iteration 705, loss = 0.33583444\n",
      "Iteration 166, loss = 0.36356252\n",
      "Iteration 1444, loss = 0.24410211\n",
      "Iteration 500, loss = 0.34530568\n",
      "Iteration 167, loss = 0.36316036\n",
      "Iteration 168, loss = 0.36276983\n",
      "Iteration 2184, loss = 0.10309429\n",
      "Iteration 169, loss = 0.36239143\n",
      "Iteration 170, loss = 0.36199535\n",
      "Iteration 171, loss = 0.36161970\n",
      "Iteration 1980, loss = 0.18868110\n",
      "Iteration 172, loss = 0.36125759\n",
      "Iteration 173, loss = 0.36088589\n",
      "Iteration 174, loss = 0.36053222\n",
      "Iteration 175, loss = 0.36018859\n",
      "Iteration 2185, loss = 0.10292993\n",
      "Iteration 706, loss = 0.33581476\n",
      "Iteration 1445, loss = 0.24403610\n",
      "Iteration 2186, loss = 0.10288029\n",
      "Iteration 176, loss = 0.35980288\n",
      "Iteration 177, loss = 0.35946007\n",
      "Iteration 866, loss = 0.30772209\n",
      "Iteration 1981, loss = 0.18861127\n",
      "Iteration 178, loss = 0.35911265\n",
      "Iteration 501, loss = 0.34513568\n",
      "Iteration 2187, loss = 0.10277109\n",
      "Iteration 1446, loss = 0.24393664\n",
      "Iteration 179, loss = 0.35875393\n",
      "Iteration 2188, loss = 0.10267534\n",
      "Iteration 180, loss = 0.35842611\n",
      "Iteration 181, loss = 0.35808960\n",
      "Iteration 707, loss = 0.33566193\n",
      "Iteration 182, loss = 0.35774624\n",
      "Iteration 2189, loss = 0.10262632\n",
      "Iteration 183, loss = 0.35741828\n",
      "Iteration 184, loss = 0.35708774\n",
      "Iteration 69, loss = 0.66578189\n",
      "Iteration 185, loss = 0.35675557\n",
      "Iteration 2190, loss = 0.10253501\n",
      "Iteration 186, loss = 0.35643015\n",
      "Iteration 2191, loss = 0.10236412\n",
      "Iteration 708, loss = 0.33555390\n",
      "Iteration 1447, loss = 0.24382080\n",
      "Iteration 187, loss = 0.35612549\n",
      "Iteration 188, loss = 0.35582030\n",
      "Iteration 1982, loss = 0.18846369\n",
      "Iteration 189, loss = 0.35548549\n",
      "Iteration 2192, loss = 0.10230869\n",
      "Iteration 502, loss = 0.34491382\n",
      "Iteration 1448, loss = 0.24373793\n",
      "Iteration 190, loss = 0.35519358\n",
      "Iteration 2193, loss = 0.10236297\n",
      "Iteration 191, loss = 0.35488349\n",
      "Iteration 2194, loss = 0.10219601\n",
      "Iteration 867, loss = 0.30739274\n",
      "Iteration 70, loss = 0.66272816\n",
      "Iteration 2195, loss = 0.10215096\n",
      "Iteration 192, loss = 0.35455845\n",
      "Iteration 193, loss = 0.35428396\n",
      "Iteration 2196, loss = 0.10196629\n",
      "Iteration 194, loss = 0.35397799\n",
      "Iteration 503, loss = 0.34477168\n",
      "Iteration 2197, loss = 0.10192309\n",
      "Iteration 1449, loss = 0.24360371\n",
      "Iteration 2198, loss = 0.10200669\n",
      "Iteration 195, loss = 0.35366908\n",
      "Iteration 196, loss = 0.35338010\n",
      "Iteration 2199, loss = 0.10184956\n",
      "Iteration 71, loss = 0.65967980\n",
      "Iteration 2200, loss = 0.10175570\n",
      "Iteration 504, loss = 0.34459388\n",
      "Iteration 197, loss = 0.35309791\n",
      "Iteration 198, loss = 0.35281506\n",
      "Iteration 1983, loss = 0.18841401\n",
      "Iteration 2201, loss = 0.10170888\n",
      "Iteration 1450, loss = 0.24353296\n",
      "Iteration 2202, loss = 0.10150992\n",
      "Iteration 868, loss = 0.30721212\n",
      "Iteration 199, loss = 0.35252341\n",
      "Iteration 200, loss = 0.35225214\n",
      "Iteration 2203, loss = 0.10142620\n",
      "Iteration 709, loss = 0.33546482\n",
      "Iteration 201, loss = 0.35196174\n",
      "Iteration 2204, loss = 0.10138365\n",
      "Iteration 1984, loss = 0.18831684\n",
      "Iteration 1451, loss = 0.24343113\n",
      "Iteration 2205, loss = 0.10131501\n",
      "Iteration 202, loss = 0.35167821\n",
      "Iteration 505, loss = 0.34451193\n",
      "Iteration 1985, loss = 0.18822001\n",
      "Iteration 2206, loss = 0.10117962\n",
      "Iteration 1452, loss = 0.24332064\n",
      "Iteration 869, loss = 0.30693710\n",
      "Iteration 203, loss = 0.35142225\n",
      "Iteration 204, loss = 0.35113947\n",
      "Iteration 2207, loss = 0.10134462\n",
      "Iteration 205, loss = 0.35087065\n",
      "Iteration 206, loss = 0.35061822\n",
      "Iteration 1453, loss = 0.24322147\n",
      "Iteration 207, loss = 0.35035279\n",
      "Iteration 2208, loss = 0.10105657\n",
      "Iteration 208, loss = 0.35008823\n",
      "Iteration 2209, loss = 0.10095082\n",
      "Iteration 1454, loss = 0.24313190\n",
      "Iteration 209, loss = 0.34982872\n",
      "Iteration 710, loss = 0.33530544\n",
      "Iteration 2210, loss = 0.10090143\n",
      "Iteration 210, loss = 0.34956687\n",
      "Iteration 1455, loss = 0.24305701\n",
      "Iteration 506, loss = 0.34428505\n",
      "Iteration 2211, loss = 0.10089693\n",
      "Iteration 72, loss = 0.65665205\n",
      "Iteration 2212, loss = 0.10070085\n",
      "Iteration 1986, loss = 0.18810982\n",
      "Iteration 211, loss = 0.34932602\n",
      "Iteration 212, loss = 0.34906960\n",
      "Iteration 1456, loss = 0.24296687\n",
      "Iteration 870, loss = 0.30676387\n",
      "Iteration 213, loss = 0.34882557\n",
      "Iteration 214, loss = 0.34855979\n",
      "Iteration 2213, loss = 0.10058453\n",
      "Iteration 2214, loss = 0.10054693\n",
      "Iteration 1457, loss = 0.24290874\n",
      "Iteration 215, loss = 0.34833510\n",
      "Iteration 2215, loss = 0.10046549\n",
      "Iteration 216, loss = 0.34807626\n",
      "Iteration 217, loss = 0.34783166\n",
      "Iteration 711, loss = 0.33518127\n",
      "Iteration 2216, loss = 0.10045508\n",
      "Iteration 1458, loss = 0.24274621\n",
      "Iteration 218, loss = 0.34761355\n",
      "Iteration 507, loss = 0.34412703\n",
      "Iteration 219, loss = 0.34736786\n",
      "Iteration 73, loss = 0.65359139\n",
      "Iteration 220, loss = 0.34712177\n",
      "Iteration 2217, loss = 0.10038827\n",
      "Iteration 221, loss = 0.34690132\n",
      "Iteration 1987, loss = 0.18800560\n",
      "Iteration 2218, loss = 0.10026975\n",
      "Iteration 222, loss = 0.34667326\n",
      "Iteration 712, loss = 0.33513476\n",
      "Iteration 871, loss = 0.30652292\n",
      "Iteration 2219, loss = 0.10013903\n",
      "Iteration 223, loss = 0.34644203\n",
      "Iteration 224, loss = 0.34620511\n",
      "Iteration 225, loss = 0.34599793\n",
      "Iteration 226, loss = 0.34577206\n",
      "Iteration 1459, loss = 0.24261457\n",
      "Iteration 227, loss = 0.34553855\n",
      "Iteration 74, loss = 0.65038918\n",
      "Iteration 228, loss = 0.34535062\n",
      "Iteration 229, loss = 0.34512265\n",
      "Iteration 2220, loss = 0.10006885\n",
      "Iteration 230, loss = 0.34489658\n",
      "Iteration 508, loss = 0.34397380\n",
      "Iteration 231, loss = 0.34468740\n",
      "Iteration 1988, loss = 0.18789783\n",
      "Iteration 1460, loss = 0.24262877\n",
      "Iteration 232, loss = 0.34448494\n",
      "Iteration 713, loss = 0.33496813\n",
      "Iteration 233, loss = 0.34426855\n",
      "Iteration 2221, loss = 0.09999925\n",
      "Iteration 872, loss = 0.30641336\n",
      "Iteration 2222, loss = 0.09999077\n",
      "Iteration 234, loss = 0.34406410\n",
      "Iteration 2223, loss = 0.09982649\n",
      "Iteration 75, loss = 0.64731279\n",
      "Iteration 1461, loss = 0.24243449\n",
      "Iteration 714, loss = 0.33487232\n",
      "Iteration 2224, loss = 0.09981323\n",
      "Iteration 509, loss = 0.34382541\n",
      "Iteration 2225, loss = 0.09969890\n",
      "Iteration 2226, loss = 0.09956388\n",
      "Iteration 235, loss = 0.34386950\n",
      "Iteration 236, loss = 0.34365703\n",
      "Iteration 237, loss = 0.34343977\n",
      "Iteration 238, loss = 0.34325595\n",
      "Iteration 873, loss = 0.30608711\n",
      "Iteration 715, loss = 0.33473538\n",
      "Iteration 510, loss = 0.34366222\n",
      "Iteration 239, loss = 0.34304165\n",
      "Iteration 76, loss = 0.64423082\n",
      "Iteration 240, loss = 0.34285361\n",
      "Iteration 241, loss = 0.34266230\n",
      "Iteration 1989, loss = 0.18792049\n",
      "Iteration 1462, loss = 0.24231626\n",
      "Iteration 242, loss = 0.34245209\n",
      "Iteration 716, loss = 0.33464951\n",
      "Iteration 243, loss = 0.34226883\n",
      "Iteration 244, loss = 0.34208233\n",
      "Iteration 511, loss = 0.34358638\n",
      "Iteration 245, loss = 0.34189059\n",
      "Iteration 246, loss = 0.34168622\n",
      "Iteration 247, loss = 0.34152530\n",
      "Iteration 1463, loss = 0.24224708\n",
      "Iteration 248, loss = 0.34132140\n",
      "Iteration 2227, loss = 0.09953311\n",
      "Iteration 512, loss = 0.34337212\n",
      "Iteration 249, loss = 0.34114047\n",
      "Iteration 77, loss = 0.64101194\n",
      "Iteration 1464, loss = 0.24213100\n",
      "Iteration 1990, loss = 0.18768211\n",
      "Iteration 250, loss = 0.34096142\n",
      "Iteration 874, loss = 0.30589333\n",
      "Iteration 2228, loss = 0.09968827\n",
      "Iteration 513, loss = 0.34317324\n",
      "Iteration 251, loss = 0.34078216\n",
      "Iteration 1465, loss = 0.24203505\n",
      "Iteration 252, loss = 0.34060917\n",
      "Iteration 253, loss = 0.34041769\n",
      "Iteration 254, loss = 0.34023775\n",
      "Iteration 717, loss = 0.33454771\n",
      "Iteration 2229, loss = 0.09935491\n",
      "Iteration 255, loss = 0.34005776\n",
      "Iteration 514, loss = 0.34308028\n",
      "Iteration 256, loss = 0.33989495\n",
      "Iteration 2230, loss = 0.09925226\n",
      "Iteration 257, loss = 0.33972309Iteration 1466, loss = 0.24195222\n",
      "\n",
      "Iteration 258, loss = 0.33954962\n",
      "Iteration 2231, loss = 0.09941909\n",
      "Iteration 78, loss = 0.63782889\n",
      "Iteration 259, loss = 0.33937870\n",
      "Iteration 1991, loss = 0.18783550\n",
      "Iteration 875, loss = 0.30560065\n",
      "Iteration 260, loss = 0.33919879\n",
      "Iteration 2232, loss = 0.09912668\n",
      "Iteration 515, loss = 0.34293418\n",
      "Iteration 261, loss = 0.33904023\n",
      "Iteration 262, loss = 0.33887500\n",
      "Iteration 263, loss = 0.33870342\n",
      "Iteration 264, loss = 0.33854005\n",
      "Iteration 265, loss = 0.33837946\n",
      "Iteration 718, loss = 0.33445412\n",
      "Iteration 266, loss = 0.33822192\n",
      "Iteration 267, loss = 0.33805183\n",
      "Iteration 2233, loss = 0.09906484\n",
      "Iteration 1467, loss = 0.24183655\n",
      "Iteration 268, loss = 0.33789636\n",
      "Iteration 269, loss = 0.33774278\n",
      "Iteration 270, loss = 0.33759035\n",
      "Iteration 271, loss = 0.33741732\n",
      "Iteration 272, loss = 0.33726743\n",
      "Iteration 1992, loss = 0.18752666\n",
      "Iteration 273, loss = 0.33712084\n",
      "Iteration 876, loss = 0.30551243\n",
      "Iteration 274, loss = 0.33698108\n",
      "Iteration 79, loss = 0.63473429\n",
      "Iteration 2234, loss = 0.09898621\n",
      "Iteration 275, loss = 0.33681223\n",
      "Iteration 276, loss = 0.33666279\n",
      "Iteration 516, loss = 0.34269154\n",
      "Iteration 2235, loss = 0.09894160\n",
      "Iteration 277, loss = 0.33651860\n",
      "Iteration 1468, loss = 0.24172313\n",
      "Iteration 278, loss = 0.33635978\n",
      "Iteration 719, loss = 0.33431936\n",
      "Iteration 279, loss = 0.33623807\n",
      "Iteration 517, loss = 0.34255504\n",
      "Iteration 280, loss = 0.33608813\n",
      "Iteration 1993, loss = 0.18741210\n",
      "Iteration 281, loss = 0.33593578\n",
      "Iteration 282, loss = 0.33580172\n",
      "Iteration 283, loss = 0.33565254\n",
      "Iteration 1469, loss = 0.24172837\n",
      "Iteration 284, loss = 0.33550235\n",
      "Iteration 285, loss = 0.33537780\n",
      "Iteration 720, loss = 0.33421047\n",
      "Iteration 286, loss = 0.33522335\n",
      "Iteration 518, loss = 0.34241268\n",
      "Iteration 287, loss = 0.33509479\n",
      "Iteration 288, loss = 0.33494580\n",
      "Iteration 289, loss = 0.33482761\n",
      "Iteration 1994, loss = 0.18737760\n",
      "Iteration 290, loss = 0.33468232\n",
      "Iteration 291, loss = 0.33454513\n",
      "Iteration 2236, loss = 0.09927197\n",
      "Iteration 292, loss = 0.33440624\n",
      "Iteration 1470, loss = 0.24156422\n",
      "Iteration 293, loss = 0.33429352\n",
      "Iteration 294, loss = 0.33413709\n",
      "Iteration 295, loss = 0.33401196\n",
      "Iteration 519, loss = 0.34222919\n",
      "Iteration 296, loss = 0.33387837\n",
      "Iteration 1471, loss = 0.24145467\n",
      "Iteration 2237, loss = 0.09888596\n",
      "Iteration 721, loss = 0.33413040\n",
      "Iteration 297, loss = 0.33376285\n",
      "Iteration 2238, loss = 0.09874993\n",
      "Iteration 80, loss = 0.63152970\n",
      "Iteration 520, loss = 0.34210912\n",
      "Iteration 1472, loss = 0.24135746\n",
      "Iteration 298, loss = 0.33361404\n",
      "Iteration 877, loss = 0.30523595\n",
      "Iteration 299, loss = 0.33348922\n",
      "Iteration 2239, loss = 0.09864893\n",
      "Iteration 300, loss = 0.33336027\n",
      "Iteration 301, loss = 0.33322651\n",
      "Iteration 521, loss = 0.34197581\n",
      "Iteration 302, loss = 0.33310678\n",
      "Iteration 1473, loss = 0.24129010\n",
      "Iteration 2240, loss = 0.09859076\n",
      "Iteration 303, loss = 0.33298938\n",
      "Iteration 1995, loss = 0.18717421\n",
      "Iteration 304, loss = 0.33285001\n",
      "Iteration 305, loss = 0.33273557\n",
      "Iteration 522, loss = 0.34179892\n",
      "Iteration 306, loss = 0.33260329\n",
      "Iteration 878, loss = 0.30499118\n",
      "Iteration 2241, loss = 0.09842469\n",
      "Iteration 307, loss = 0.33249138\n",
      "Iteration 1474, loss = 0.24118053\n",
      "Iteration 1996, loss = 0.18713275\n",
      "Iteration 308, loss = 0.33236013\n",
      "Iteration 722, loss = 0.33397893\n",
      "Iteration 523, loss = 0.34167868\n",
      "Iteration 309, loss = 0.33225264\n",
      "Iteration 2242, loss = 0.09835318\n",
      "Iteration 310, loss = 0.33211654\n",
      "Iteration 311, loss = 0.33200983\n",
      "Iteration 2243, loss = 0.09826308\n",
      "Iteration 1475, loss = 0.24110238\n",
      "Iteration 81, loss = 0.62836819\n",
      "Iteration 312, loss = 0.33187431\n",
      "Iteration 313, loss = 0.33176667\n",
      "Iteration 1997, loss = 0.18697997\n",
      "Iteration 314, loss = 0.33167840\n",
      "Iteration 315, loss = 0.33153862\n",
      "Iteration 316, loss = 0.33140693\n",
      "Iteration 317, loss = 0.33129738\n",
      "Iteration 1476, loss = 0.24097668\n",
      "Iteration 318, loss = 0.33120011\n",
      "Iteration 723, loss = 0.33387056\n",
      "Iteration 319, loss = 0.33108875\n",
      "Iteration 320, loss = 0.33095934\n",
      "Iteration 1477, loss = 0.24094524\n",
      "Iteration 321, loss = 0.33084698\n",
      "Iteration 1998, loss = 0.18703058\n",
      "Iteration 322, loss = 0.33074377\n",
      "Iteration 524, loss = 0.34143557\n",
      "Iteration 323, loss = 0.33063305\n",
      "Iteration 1478, loss = 0.24077683\n",
      "Iteration 324, loss = 0.33051868\n",
      "Iteration 1999, loss = 0.18678581\n",
      "Iteration 1479, loss = 0.24081187\n",
      "Iteration 325, loss = 0.33040575\n",
      "Iteration 326, loss = 0.33029445\n",
      "Iteration 2244, loss = 0.09837740\n",
      "Iteration 327, loss = 0.33019027\n",
      "Iteration 328, loss = 0.33009732\n",
      "Iteration 329, loss = 0.32997331\n",
      "Iteration 1480, loss = 0.24062159\n",
      "Iteration 330, loss = 0.32987395\n",
      "Iteration 331, loss = 0.32976679\n",
      "Iteration 82, loss = 0.62525344\n",
      "Iteration 724, loss = 0.33378183Iteration 2245, loss = 0.09823004\n",
      "Iteration 879, loss = 0.30473607\n",
      "Iteration 332, loss = 0.32965830\n",
      "Iteration 333, loss = 0.32958569\n",
      "Iteration 334, loss = 0.32943929\n",
      "Iteration 335, loss = 0.32935267\n",
      "Iteration 2246, loss = 0.09801328\n",
      "\n",
      "Iteration 336, loss = 0.32923749\n",
      "Iteration 525, loss = 0.34132401\n",
      "Iteration 1481, loss = 0.24046411\n",
      "Iteration 337, loss = 0.32912832\n",
      "Iteration 338, loss = 0.32903651\n",
      "Iteration 339, loss = 0.32895209\n",
      "Iteration 340, loss = 0.32883054\n",
      "Iteration 341, loss = 0.32873378\n",
      "Iteration 342, loss = 0.32862198\n",
      "Iteration 2247, loss = 0.09794070\n",
      "Iteration 526, loss = 0.34113767\n",
      "Iteration 343, loss = 0.32853182\n",
      "Iteration 344, loss = 0.32842925\n",
      "Iteration 345, loss = 0.32832755\n",
      "Iteration 346, loss = 0.32823806\n",
      "Iteration 2248, loss = 0.09802927\n",
      "Iteration 527, loss = 0.34100317\n",
      "Iteration 1482, loss = 0.24040101\n",
      "Iteration 2249, loss = 0.09793610\n",
      "Iteration 725, loss = 0.33362886\n",
      "Iteration 2000, loss = 0.18678500\n",
      "Iteration 880, loss = 0.30456732\n",
      "Iteration 2250, loss = 0.09778908\n",
      "Iteration 83, loss = 0.62213284\n",
      "Iteration 347, loss = 0.32813671\n",
      "Iteration 348, loss = 0.32803513\n",
      "Iteration 2251, loss = 0.09775067\n",
      "Iteration 349, loss = 0.32795285\n",
      "Iteration 2252, loss = 0.09757492\n",
      "Iteration 1483, loss = 0.24030346\n",
      "Iteration 350, loss = 0.32783659\n",
      "Iteration 2253, loss = 0.09760776\n",
      "Iteration 351, loss = 0.32774432\n",
      "Iteration 726, loss = 0.33352388\n",
      "Iteration 352, loss = 0.32765026\n",
      "Iteration 528, loss = 0.34085747\n",
      "Iteration 2254, loss = 0.09747675\n",
      "Iteration 353, loss = 0.32755207\n",
      "Iteration 354, loss = 0.32745510\n",
      "Iteration 2001, loss = 0.18664674\n",
      "Iteration 2255, loss = 0.09746067\n",
      "Iteration 355, loss = 0.32735712\n",
      "Iteration 84, loss = 0.61899951\n",
      "Iteration 727, loss = 0.33342425\n",
      "Iteration 2256, loss = 0.09729990\n",
      "Iteration 1484, loss = 0.24018231\n",
      "Iteration 356, loss = 0.32727759\n",
      "Iteration 357, loss = 0.32717935\n",
      "Iteration 2257, loss = 0.09727728\n",
      "Iteration 529, loss = 0.34066483\n",
      "Iteration 358, loss = 0.32708976\n",
      "Iteration 2258, loss = 0.09708876\n",
      "Iteration 881, loss = 0.30435302\n",
      "Iteration 359, loss = 0.32697722\n",
      "Iteration 360, loss = 0.32690371\n",
      "Iteration 361, loss = 0.32680096\n",
      "Iteration 1485, loss = 0.24009654\n",
      "Iteration 362, loss = 0.32670920\n",
      "Iteration 2259, loss = 0.09701708\n",
      "Iteration 728, loss = 0.33330363\n",
      "Iteration 530, loss = 0.34052898\n",
      "Iteration 2260, loss = 0.09700210\n",
      "Iteration 1486, loss = 0.24001793\n",
      "Iteration 85, loss = 0.61590588\n",
      "Iteration 531, loss = 0.34038177\n",
      "Iteration 363, loss = 0.32662673\n",
      "Iteration 1487, loss = 0.23987508\n",
      "Iteration 2261, loss = 0.09693505\n",
      "Iteration 882, loss = 0.30418792\n",
      "Iteration 2262, loss = 0.09700977\n",
      "Iteration 2263, loss = 0.09677024\n",
      "Iteration 1488, loss = 0.23985231\n",
      "Iteration 532, loss = 0.34025657\n",
      "Iteration 2264, loss = 0.09671795\n",
      "Iteration 364, loss = 0.32653466\n",
      "Iteration 86, loss = 0.61273764\n",
      "Iteration 1489, loss = 0.23976787\n",
      "Iteration 365, loss = 0.32643853\n",
      "Iteration 2265, loss = 0.09659208\n",
      "Iteration 366, loss = 0.32634967\n",
      "Iteration 367, loss = 0.32626571\n",
      "Iteration 2002, loss = 0.18654989\n",
      "Iteration 368, loss = 0.32617658\n",
      "Iteration 1490, loss = 0.23961290\n",
      "Iteration 369, loss = 0.32609075\n",
      "Iteration 370, loss = 0.32600743\n",
      "Iteration 2266, loss = 0.09656202\n",
      "Iteration 533, loss = 0.34005313\n",
      "Iteration 2267, loss = 0.09643882\n",
      "Iteration 371, loss = 0.32590803\n",
      "Iteration 372, loss = 0.32581020\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1491, loss = 0.23949026\n",
      "Iteration 2268, loss = 0.09639947\n",
      "Iteration 534, loss = 0.33993802\n",
      "Iteration 2003, loss = 0.18643472\n",
      "Iteration 2269, loss = 0.09628780\n",
      "Iteration 883, loss = 0.30387516\n",
      "Iteration 729, loss = 0.33319829\n",
      "Iteration 1, loss = 0.73546372\n",
      "Iteration 2, loss = 0.73033927\n",
      "Iteration 3, loss = 0.72273182\n",
      "Iteration 535, loss = 0.33973114\n",
      "Iteration 4, loss = 0.71339571\n",
      "Iteration 1492, loss = 0.23940170\n",
      "Iteration 87, loss = 0.60957545\n",
      "Iteration 2270, loss = 0.09629717\n",
      "Iteration 5, loss = 0.70296374\n",
      "Iteration 6, loss = 0.69266181\n",
      "Iteration 2004, loss = 0.18636868\n",
      "Iteration 7, loss = 0.68157463\n",
      "Iteration 8, loss = 0.67072620\n",
      "Iteration 730, loss = 0.33308350\n",
      "Iteration 9, loss = 0.66067706\n",
      "Iteration 10, loss = 0.65093224\n",
      "Iteration 1493, loss = 0.23931662\n",
      "Iteration 2271, loss = 0.09616694\n",
      "Iteration 11, loss = 0.64134931\n",
      "Iteration 2272, loss = 0.09602168\n",
      "Iteration 884, loss = 0.30372393\n",
      "Iteration 2005, loss = 0.18624859\n",
      "Iteration 12, loss = 0.63220461Iteration 731, loss = 0.33298485\n",
      "\n",
      "Iteration 1494, loss = 0.23923446\n",
      "Iteration 2273, loss = 0.09599490\n",
      "Iteration 13, loss = 0.62357808\n",
      "Iteration 536, loss = 0.33958051\n",
      "Iteration 88, loss = 0.60652210\n",
      "Iteration 14, loss = 0.61559990\n",
      "Iteration 1495, loss = 0.23907872\n",
      "Iteration 2274, loss = 0.09586745\n",
      "Iteration 15, loss = 0.60798144\n",
      "Iteration 2006, loss = 0.18620640\n",
      "Iteration 1496, loss = 0.23899768\n",
      "Iteration 537, loss = 0.33944675\n",
      "Iteration 2275, loss = 0.09591942\n",
      "Iteration 16, loss = 0.60054068\n",
      "Iteration 2276, loss = 0.09589132\n",
      "Iteration 89, loss = 0.60348383\n",
      "Iteration 17, loss = 0.59350067\n",
      "Iteration 18, loss = 0.58683472\n",
      "Iteration 2007, loss = 0.18602105\n",
      "Iteration 885, loss = 0.30347478\n",
      "Iteration 1497, loss = 0.23887246\n",
      "Iteration 19, loss = 0.58043853\n",
      "Iteration 20, loss = 0.57463429\n",
      "Iteration 732, loss = 0.33288341\n",
      "Iteration 2277, loss = 0.09570499\n",
      "Iteration 21, loss = 0.56893036\n",
      "Iteration 22, loss = 0.56329821\n",
      "Iteration 23, loss = 0.55801040\n",
      "Iteration 1498, loss = 0.23880943\n",
      "Iteration 2278, loss = 0.09570688\n",
      "Iteration 733, loss = 0.33275031\n",
      "Iteration 24, loss = 0.55290300\n",
      "Iteration 90, loss = 0.60047516\n",
      "Iteration 538, loss = 0.33932279\n",
      "Iteration 1499, loss = 0.23872165\n",
      "Iteration 2279, loss = 0.09562515\n",
      "Iteration 2008, loss = 0.18595095\n",
      "Iteration 1500, loss = 0.23875163\n",
      "Iteration 539, loss = 0.33912807\n",
      "Iteration 2280, loss = 0.09539841\n",
      "Iteration 2009, loss = 0.18584185\n",
      "Iteration 1501, loss = 0.23854077\n",
      "Iteration 540, loss = 0.33896840\n",
      "Iteration 2281, loss = 0.09537452\n",
      "Iteration 1502, loss = 0.23845490\n",
      "Iteration 734, loss = 0.33266827\n",
      "Iteration 2282, loss = 0.09524775\n",
      "Iteration 25, loss = 0.54792230\n",
      "Iteration 541, loss = 0.33885541\n",
      "Iteration 26, loss = 0.54327945\n",
      "Iteration 886, loss = 0.30322097\n",
      "Iteration 2283, loss = 0.09532971\n",
      "Iteration 27, loss = 0.53874598\n",
      "Iteration 2010, loss = 0.18575641\n",
      "Iteration 2284, loss = 0.09540584\n",
      "Iteration 28, loss = 0.53444279\n",
      "Iteration 2285, loss = 0.09517896\n",
      "Iteration 29, loss = 0.53013893\n",
      "Iteration 1503, loss = 0.23832826\n",
      "Iteration 2286, loss = 0.09496188\n",
      "Iteration 30, loss = 0.52608551\n",
      "Iteration 2287, loss = 0.09488210\n",
      "Iteration 542, loss = 0.33869541\n",
      "Iteration 31, loss = 0.52220923\n",
      "Iteration 91, loss = 0.59757949\n",
      "Iteration 887, loss = 0.30297545\n",
      "Iteration 1504, loss = 0.23821390\n",
      "Iteration 735, loss = 0.33258030\n",
      "Iteration 32, loss = 0.51830669\n",
      "Iteration 2288, loss = 0.09484432\n",
      "Iteration 33, loss = 0.51465152\n",
      "Iteration 34, loss = 0.51100384\n",
      "Iteration 2011, loss = 0.18561392\n",
      "Iteration 35, loss = 0.50755343\n",
      "Iteration 1505, loss = 0.23812235\n",
      "Iteration 2289, loss = 0.09488697\n",
      "Iteration 36, loss = 0.50421258\n",
      "Iteration 543, loss = 0.33850083\n",
      "Iteration 37, loss = 0.50086760\n",
      "Iteration 736, loss = 0.33242842\n",
      "Iteration 888, loss = 0.30280923\n",
      "Iteration 2290, loss = 0.09468331\n",
      "Iteration 38, loss = 0.49772707\n",
      "Iteration 92, loss = 0.59442973\n",
      "Iteration 2291, loss = 0.09457103\n",
      "Iteration 39, loss = 0.49463555\n",
      "Iteration 2292, loss = 0.09453679\n",
      "Iteration 1506, loss = 0.23804940\n",
      "Iteration 40, loss = 0.49158961\n",
      "Iteration 737, loss = 0.33230556\n",
      "Iteration 2293, loss = 0.09439966\n",
      "Iteration 41, loss = 0.48864459\n",
      "Iteration 1507, loss = 0.23791220\n",
      "Iteration 2012, loss = 0.18550884\n",
      "Iteration 42, loss = 0.48579843\n",
      "Iteration 43, loss = 0.48297983\n",
      "Iteration 2294, loss = 0.09435644\n",
      "Iteration 44, loss = 0.48033569\n",
      "Iteration 889, loss = 0.30262088\n",
      "Iteration 544, loss = 0.33840666\n",
      "Iteration 45, loss = 0.47762053\n",
      "Iteration 46, loss = 0.47510785\n",
      "Iteration 2295, loss = 0.09421968\n",
      "Iteration 47, loss = 0.47252256\n",
      "Iteration 1508, loss = 0.23779833\n",
      "Iteration 48, loss = 0.47014268\n",
      "Iteration 738, loss = 0.33220572\n",
      "Iteration 49, loss = 0.46774593\n",
      "Iteration 50, loss = 0.46541896\n",
      "Iteration 2013, loss = 0.18549147\n",
      "Iteration 51, loss = 0.46320933\n",
      "Iteration 52, loss = 0.46096064\n",
      "Iteration 2296, loss = 0.09418249\n",
      "Iteration 93, loss = 0.59151598\n",
      "Iteration 53, loss = 0.45869990\n",
      "Iteration 1509, loss = 0.23776324\n",
      "Iteration 54, loss = 0.45667136\n",
      "Iteration 545, loss = 0.33817087\n",
      "Iteration 55, loss = 0.45461987\n",
      "Iteration 1510, loss = 0.23761777\n",
      "Iteration 2297, loss = 0.09409499\n",
      "Iteration 739, loss = 0.33207536\n",
      "Iteration 2298, loss = 0.09406282\n",
      "Iteration 56, loss = 0.45261291\n",
      "Iteration 2299, loss = 0.09393873\n",
      "Iteration 57, loss = 0.45054500\n",
      "Iteration 2014, loss = 0.18532985\n",
      "Iteration 1511, loss = 0.23750653\n",
      "Iteration 2300, loss = 0.09405613\n",
      "Iteration 58, loss = 0.44870044\n",
      "Iteration 546, loss = 0.33803313\n",
      "Iteration 59, loss = 0.44676662\n",
      "Iteration 2301, loss = 0.09379845\n",
      "Iteration 890, loss = 0.30256369\n",
      "Iteration 60, loss = 0.44494181\n",
      "Iteration 2302, loss = 0.09365223\n",
      "Iteration 61, loss = 0.44317366\n",
      "Iteration 2303, loss = 0.09360625\n",
      "Iteration 1512, loss = 0.23740322\n",
      "Iteration 62, loss = 0.44142262\n",
      "Iteration 547, loss = 0.33789272\n",
      "Iteration 2304, loss = 0.09350508\n",
      "Iteration 740, loss = 0.33197341\n",
      "Iteration 63, loss = 0.43964843\n",
      "Iteration 1513, loss = 0.23731463\n",
      "Iteration 2015, loss = 0.18538180\n",
      "Iteration 94, loss = 0.58863787\n",
      "Iteration 548, loss = 0.33771432\n",
      "Iteration 64, loss = 0.43798703\n",
      "Iteration 2305, loss = 0.09349219\n",
      "Iteration 65, loss = 0.43634743\n",
      "Iteration 66, loss = 0.43468206\n",
      "Iteration 1514, loss = 0.23723862\n",
      "Iteration 549, loss = 0.33755515\n",
      "Iteration 67, loss = 0.43317930\n",
      "Iteration 2306, loss = 0.09332667\n",
      "Iteration 2016, loss = 0.18512895\n",
      "Iteration 68, loss = 0.43156268\n",
      "Iteration 741, loss = 0.33186869\n",
      "Iteration 891, loss = 0.30227474\n",
      "Iteration 2307, loss = 0.09328988\n",
      "Iteration 69, loss = 0.43004967\n",
      "Iteration 2308, loss = 0.09328505\n",
      "Iteration 1515, loss = 0.23711238\n",
      "Iteration 70, loss = 0.42858478\n",
      "Iteration 2309, loss = 0.09310372\n",
      "Iteration 71, loss = 0.42717853\n",
      "Iteration 550, loss = 0.33741800\n",
      "Iteration 2310, loss = 0.09323122\n",
      "Iteration 95, loss = 0.58569759\n",
      "Iteration 2017, loss = 0.18518623\n",
      "Iteration 2311, loss = 0.09299748\n",
      "Iteration 72, loss = 0.42573414\n",
      "Iteration 2312, loss = 0.09320685\n",
      "Iteration 551, loss = 0.33735140\n",
      "Iteration 1516, loss = 0.23702852\n",
      "Iteration 2313, loss = 0.09279581\n",
      "Iteration 73, loss = 0.42432944\n",
      "Iteration 74, loss = 0.42297949\n",
      "Iteration 2314, loss = 0.09299811\n",
      "Iteration 75, loss = 0.42162757\n",
      "Iteration 742, loss = 0.33175276\n",
      "Iteration 2018, loss = 0.18496599\n",
      "Iteration 892, loss = 0.30205111\n",
      "Iteration 552, loss = 0.33712865\n",
      "Iteration 76, loss = 0.42030171\n",
      "Iteration 743, loss = 0.33169940\n",
      "Iteration 77, loss = 0.41907249\n",
      "Iteration 553, loss = 0.33696541\n",
      "Iteration 2019, loss = 0.18483616\n",
      "Iteration 1517, loss = 0.23691015\n",
      "Iteration 78, loss = 0.41777519\n",
      "Iteration 893, loss = 0.30168999\n",
      "Iteration 96, loss = 0.58282413\n",
      "Iteration 79, loss = 0.41651698\n",
      "Iteration 2315, loss = 0.09277406\n",
      "Iteration 2020, loss = 0.18472612\n",
      "Iteration 554, loss = 0.33678931\n",
      "Iteration 80, loss = 0.41537444\n",
      "Iteration 2316, loss = 0.09263366\n",
      "Iteration 81, loss = 0.41414764\n",
      "Iteration 744, loss = 0.33153605\n",
      "Iteration 82, loss = 0.41298860\n",
      "Iteration 1518, loss = 0.23682049\n",
      "Iteration 2317, loss = 0.09269905\n",
      "Iteration 894, loss = 0.30156933\n",
      "Iteration 83, loss = 0.41183446\n",
      "Iteration 84, loss = 0.41073117\n",
      "Iteration 85, loss = 0.40958171\n",
      "Iteration 86, loss = 0.40851289\n",
      "Iteration 1519, loss = 0.23681033\n",
      "Iteration 87, loss = 0.40749392\n",
      "Iteration 2318, loss = 0.09246874\n",
      "Iteration 88, loss = 0.40645287\n",
      "Iteration 89, loss = 0.40542761\n",
      "Iteration 2021, loss = 0.18463023\n",
      "Iteration 90, loss = 0.40442979\n",
      "Iteration 1520, loss = 0.23659408\n",
      "Iteration 91, loss = 0.40340357\n",
      "Iteration 97, loss = 0.58012988\n",
      "Iteration 2319, loss = 0.09241537\n",
      "Iteration 92, loss = 0.40244304\n",
      "Iteration 745, loss = 0.33143406\n",
      "Iteration 1521, loss = 0.23655027\n",
      "Iteration 2320, loss = 0.09263868\n",
      "Iteration 93, loss = 0.40149414\n",
      "Iteration 555, loss = 0.33668462\n",
      "Iteration 94, loss = 0.40058142\n",
      "Iteration 2321, loss = 0.09226510\n",
      "Iteration 95, loss = 0.39969600\n",
      "Iteration 1522, loss = 0.23641853\n",
      "Iteration 96, loss = 0.39871312\n",
      "Iteration 2322, loss = 0.09220756\n",
      "Iteration 97, loss = 0.39786351\n",
      "Iteration 1523, loss = 0.23633877\n",
      "Iteration 98, loss = 0.39698276\n",
      "Iteration 556, loss = 0.33645015\n",
      "Iteration 895, loss = 0.30128914\n",
      "Iteration 99, loss = 0.39614120\n",
      "Iteration 100, loss = 0.39526065\n",
      "Iteration 746, loss = 0.33130514\n",
      "Iteration 1524, loss = 0.23625519\n",
      "Iteration 101, loss = 0.39446787\n",
      "Iteration 2022, loss = 0.18459776\n",
      "Iteration 102, loss = 0.39366153\n",
      "Iteration 557, loss = 0.33635368\n",
      "Iteration 2323, loss = 0.09214153\n",
      "Iteration 1525, loss = 0.23610666\n",
      "Iteration 2324, loss = 0.09199287\n",
      "Iteration 558, loss = 0.33615611\n",
      "Iteration 2325, loss = 0.09194038\n",
      "Iteration 103, loss = 0.39283170\n",
      "Iteration 2326, loss = 0.09186700\n",
      "Iteration 104, loss = 0.39207170\n",
      "Iteration 105, loss = 0.39129446\n",
      "Iteration 559, loss = 0.33600833\n",
      "Iteration 98, loss = 0.57716656\n",
      "Iteration 1526, loss = 0.23604746\n",
      "Iteration 747, loss = 0.33121207\n",
      "Iteration 106, loss = 0.39054112\n",
      "Iteration 896, loss = 0.30107030\n",
      "Iteration 2327, loss = 0.09180722\n",
      "Iteration 107, loss = 0.38976012\n",
      "Iteration 1527, loss = 0.23593566\n",
      "Iteration 108, loss = 0.38901210\n",
      "Iteration 109, loss = 0.38829164\n",
      "Iteration 2328, loss = 0.09181939\n",
      "Iteration 560, loss = 0.33585599\n",
      "Iteration 110, loss = 0.38758045\n",
      "Iteration 2329, loss = 0.09169801\n",
      "Iteration 111, loss = 0.38688614\n",
      "Iteration 1528, loss = 0.23585224\n",
      "Iteration 2330, loss = 0.09182764\n",
      "Iteration 748, loss = 0.33118242\n",
      "Iteration 112, loss = 0.38619206\n",
      "Iteration 2331, loss = 0.09188325\n",
      "Iteration 2023, loss = 0.18447530\n",
      "Iteration 897, loss = 0.30090006\n",
      "Iteration 113, loss = 0.38549464\n",
      "Iteration 99, loss = 0.57440251\n",
      "Iteration 1529, loss = 0.23574236\n",
      "Iteration 2332, loss = 0.09166244\n",
      "Iteration 561, loss = 0.33573181\n",
      "Iteration 114, loss = 0.38484096\n",
      "Iteration 2333, loss = 0.09136722\n",
      "Iteration 115, loss = 0.38415789\n",
      "Iteration 116, loss = 0.38351860\n",
      "Iteration 2334, loss = 0.09146399\n",
      "Iteration 898, loss = 0.30056764\n",
      "Iteration 562, loss = 0.33554604\n",
      "Iteration 117, loss = 0.38284930\n",
      "Iteration 118, loss = 0.38220016\n",
      "Iteration 1530, loss = 0.23561619\n",
      "Iteration 749, loss = 0.33098851\n",
      "Iteration 119, loss = 0.38159470\n",
      "Iteration 2024, loss = 0.18445855\n",
      "Iteration 2335, loss = 0.09122979\n",
      "Iteration 1531, loss = 0.23550618\n",
      "Iteration 120, loss = 0.38098034\n",
      "Iteration 100, loss = 0.57185196\n",
      "Iteration 2336, loss = 0.09118917\n",
      "Iteration 563, loss = 0.33543308\n",
      "Iteration 121, loss = 0.38038093\n",
      "Iteration 2337, loss = 0.09115018\n",
      "Iteration 899, loss = 0.30037158\n",
      "Iteration 2338, loss = 0.09102828\n",
      "Iteration 122, loss = 0.37975543\n",
      "Iteration 2339, loss = 0.09099856\n",
      "Iteration 123, loss = 0.37918614\n",
      "Iteration 1532, loss = 0.23544802\n",
      "Iteration 124, loss = 0.37859589\n",
      "Iteration 2025, loss = 0.18427465\n",
      "Iteration 125, loss = 0.37800810\n",
      "Iteration 564, loss = 0.33530239\n",
      "Iteration 2340, loss = 0.09088119\n",
      "Iteration 126, loss = 0.37744801\n",
      "Iteration 1533, loss = 0.23532371\n",
      "Iteration 750, loss = 0.33095526\n",
      "Iteration 565, loss = 0.33509333Iteration 127, loss = 0.37689343\n",
      "Iteration 2341, loss = 0.09081575\n",
      "Iteration 128, loss = 0.37634483\n",
      "Iteration 101, loss = 0.56908698\n",
      "\n",
      "Iteration 2342, loss = 0.09100519\n",
      "Iteration 129, loss = 0.37579482\n",
      "Iteration 900, loss = 0.30021272\n",
      "Iteration 130, loss = 0.37526147\n",
      "Iteration 751, loss = 0.33079082\n",
      "Iteration 131, loss = 0.37473795\n",
      "Iteration 1534, loss = 0.23526741\n",
      "Iteration 2343, loss = 0.09065712\n",
      "Iteration 132, loss = 0.37422030\n",
      "Iteration 133, loss = 0.37369120\n",
      "Iteration 134, loss = 0.37317816\n",
      "Iteration 135, loss = 0.37269002\n",
      "Iteration 2344, loss = 0.09060058\n",
      "Iteration 136, loss = 0.37218392\n",
      "Iteration 137, loss = 0.37172492\n",
      "Iteration 138, loss = 0.37121818\n",
      "Iteration 1535, loss = 0.23530573\n",
      "Iteration 901, loss = 0.30017711\n",
      "Iteration 139, loss = 0.37073099\n",
      "Iteration 752, loss = 0.33064880\n",
      "Iteration 140, loss = 0.37028659\n",
      "Iteration 566, loss = 0.33492674\n",
      "Iteration 2345, loss = 0.09066297\n",
      "Iteration 141, loss = 0.36979132\n",
      "Iteration 142, loss = 0.36934203\n",
      "Iteration 2346, loss = 0.09045644\n",
      "Iteration 1536, loss = 0.23508585\n",
      "Iteration 143, loss = 0.36890380\n",
      "Iteration 2026, loss = 0.18421441\n",
      "Iteration 753, loss = 0.33054172\n",
      "Iteration 144, loss = 0.36844968\n",
      "Iteration 145, loss = 0.36799635\n",
      "Iteration 1537, loss = 0.23494950\n",
      "Iteration 2347, loss = 0.09043909\n",
      "Iteration 146, loss = 0.36757137\n",
      "Iteration 2348, loss = 0.09032657\n",
      "Iteration 567, loss = 0.33478714\n",
      "Iteration 147, loss = 0.36714787\n",
      "Iteration 2349, loss = 0.09042924\n",
      "Iteration 754, loss = 0.33044789\n",
      "Iteration 148, loss = 0.36669993\n",
      "Iteration 2027, loss = 0.18408849\n",
      "Iteration 2350, loss = 0.09019900\n",
      "Iteration 149, loss = 0.36628019\n",
      "Iteration 102, loss = 0.56651292\n",
      "Iteration 1538, loss = 0.23482443\n",
      "Iteration 150, loss = 0.36584544\n",
      "Iteration 568, loss = 0.33465237\n",
      "Iteration 2351, loss = 0.09010845\n",
      "Iteration 902, loss = 0.29973144Iteration 1539, loss = 0.23472806\n",
      "\n",
      "Iteration 151, loss = 0.36543294\n",
      "Iteration 152, loss = 0.36506533\n",
      "Iteration 153, loss = 0.36464426\n",
      "Iteration 2352, loss = 0.09018215\n",
      "Iteration 154, loss = 0.36425850\n",
      "Iteration 569, loss = 0.33445261\n",
      "Iteration 155, loss = 0.36385005\n",
      "Iteration 103, loss = 0.56394853\n",
      "Iteration 156, loss = 0.36344675\n",
      "Iteration 2353, loss = 0.09005344\n",
      "Iteration 2028, loss = 0.18404609\n",
      "Iteration 157, loss = 0.36307517\n",
      "Iteration 1540, loss = 0.23467174\n",
      "Iteration 755, loss = 0.33038386\n",
      "Iteration 2354, loss = 0.08988490\n",
      "Iteration 158, loss = 0.36268491\n",
      "Iteration 570, loss = 0.33431451\n",
      "Iteration 159, loss = 0.36232947\n",
      "Iteration 2355, loss = 0.08987278\n",
      "Iteration 2029, loss = 0.18390288\n",
      "Iteration 160, loss = 0.36194097\n",
      "Iteration 1541, loss = 0.23461116\n",
      "Iteration 2356, loss = 0.08977496\n",
      "Iteration 903, loss = 0.29948620\n",
      "Iteration 2357, loss = 0.08977231\n",
      "Iteration 161, loss = 0.36157957\n",
      "Iteration 1542, loss = 0.23442870\n",
      "Iteration 162, loss = 0.36122974\n",
      "Iteration 163, loss = 0.36084770\n",
      "Iteration 104, loss = 0.56136239\n",
      "Iteration 2030, loss = 0.18377095\n",
      "Iteration 571, loss = 0.33421623\n",
      "Iteration 2358, loss = 0.08964163\n",
      "Iteration 1543, loss = 0.23438528\n",
      "Iteration 756, loss = 0.33023757\n",
      "Iteration 164, loss = 0.36050493\n",
      "Iteration 2359, loss = 0.08972368\n",
      "Iteration 904, loss = 0.29926737\n",
      "Iteration 165, loss = 0.36014867\n",
      "Iteration 2031, loss = 0.18372530\n",
      "Iteration 166, loss = 0.35979483\n",
      "Iteration 167, loss = 0.35945654\n",
      "Iteration 572, loss = 0.33405948\n",
      "Iteration 168, loss = 0.35911459\n",
      "Iteration 1544, loss = 0.23426771\n",
      "Iteration 2360, loss = 0.08956569\n",
      "Iteration 169, loss = 0.35877918\n",
      "Iteration 170, loss = 0.35845661\n",
      "Iteration 573, loss = 0.33388010\n",
      "Iteration 757, loss = 0.33010810\n",
      "Iteration 2361, loss = 0.08960679\n",
      "Iteration 2032, loss = 0.18358710\n",
      "Iteration 171, loss = 0.35812934\n",
      "Iteration 105, loss = 0.55886103\n",
      "Iteration 1545, loss = 0.23417732\n",
      "Iteration 172, loss = 0.35780568\n",
      "Iteration 173, loss = 0.35746455\n",
      "Iteration 1546, loss = 0.23407072\n",
      "Iteration 174, loss = 0.35714326\n",
      "Iteration 175, loss = 0.35683349\n",
      "Iteration 2362, loss = 0.08952554\n",
      "Iteration 758, loss = 0.32998603\n",
      "Iteration 176, loss = 0.35651340\n",
      "Iteration 177, loss = 0.35620422\n",
      "Iteration 574, loss = 0.33365892\n",
      "Iteration 905, loss = 0.29940323\n",
      "Iteration 178, loss = 0.35590967\n",
      "Iteration 179, loss = 0.35559978\n",
      "Iteration 1547, loss = 0.23392944\n",
      "Iteration 2363, loss = 0.08940547\n",
      "Iteration 180, loss = 0.35529857\n",
      "Iteration 106, loss = 0.55638073\n",
      "Iteration 181, loss = 0.35498786\n",
      "Iteration 2364, loss = 0.08924230\n",
      "Iteration 2365, loss = 0.08935203\n",
      "Iteration 2033, loss = 0.18346205\n",
      "Iteration 575, loss = 0.33351534\n",
      "Iteration 182, loss = 0.35471401\n",
      "Iteration 1548, loss = 0.23395098\n",
      "Iteration 906, loss = 0.29884990\n",
      "Iteration 2366, loss = 0.08909830\n",
      "Iteration 183, loss = 0.35441791\n",
      "Iteration 107, loss = 0.55411523\n",
      "Iteration 2367, loss = 0.08912593\n",
      "Iteration 184, loss = 0.35413255\n",
      "Iteration 759, loss = 0.32988947\n",
      "Iteration 2034, loss = 0.18338438\n",
      "Iteration 2368, loss = 0.08902359\n",
      "Iteration 1549, loss = 0.23377898\n",
      "Iteration 2369, loss = 0.08885310\n",
      "Iteration 185, loss = 0.35384180\n",
      "Iteration 2370, loss = 0.08906969\n",
      "Iteration 186, loss = 0.35356029\n",
      "Iteration 760, loss = 0.32987337\n",
      "Iteration 576, loss = 0.33336101\n",
      "Iteration 1550, loss = 0.23370283\n",
      "Iteration 187, loss = 0.35326228\n",
      "Iteration 1551, loss = 0.23358109\n",
      "Iteration 188, loss = 0.35298951\n",
      "Iteration 577, loss = 0.33338528\n",
      "Iteration 2371, loss = 0.08883191\n",
      "Iteration 907, loss = 0.29862902\n",
      "Iteration 1552, loss = 0.23345375\n",
      "Iteration 2035, loss = 0.18333926\n",
      "Iteration 189, loss = 0.35272003\n",
      "Iteration 108, loss = 0.55176151\n",
      "Iteration 578, loss = 0.33308827\n",
      "Iteration 1553, loss = 0.23336560\n",
      "Iteration 190, loss = 0.35244587\n",
      "Iteration 761, loss = 0.32967833\n",
      "Iteration 191, loss = 0.35217597\n",
      "Iteration 1554, loss = 0.23329661\n",
      "Iteration 192, loss = 0.35190331\n",
      "Iteration 579, loss = 0.33291872\n",
      "Iteration 193, loss = 0.35164759\n",
      "Iteration 194, loss = 0.35137074\n",
      "Iteration 908, loss = 0.29850024\n",
      "Iteration 195, loss = 0.35112652\n",
      "Iteration 1555, loss = 0.23320769\n",
      "Iteration 762, loss = 0.32958246\n",
      "Iteration 196, loss = 0.35086640\n",
      "Iteration 197, loss = 0.35059624\n",
      "Iteration 198, loss = 0.35037367\n",
      "Iteration 199, loss = 0.35010219\n",
      "Iteration 1556, loss = 0.23323722\n",
      "Iteration 200, loss = 0.34984201\n",
      "Iteration 580, loss = 0.33277211\n",
      "Iteration 2036, loss = 0.18322477\n",
      "Iteration 201, loss = 0.34961817\n",
      "Iteration 202, loss = 0.34935707\n",
      "Iteration 1557, loss = 0.23295875\n",
      "Iteration 109, loss = 0.54939830\n",
      "Iteration 203, loss = 0.34910924\n",
      "Iteration 909, loss = 0.29824370\n",
      "Iteration 763, loss = 0.32944024\n",
      "Iteration 204, loss = 0.34888693\n",
      "Iteration 1558, loss = 0.23289600\n",
      "Iteration 581, loss = 0.33270964\n",
      "Iteration 205, loss = 0.34863666\n",
      "Iteration 1559, loss = 0.23276405\n",
      "Iteration 2037, loss = 0.18317369\n",
      "Iteration 206, loss = 0.34841530\n",
      "Iteration 207, loss = 0.34817358\n",
      "Iteration 764, loss = 0.32936528\n",
      "Iteration 208, loss = 0.34793890\n",
      "Iteration 582, loss = 0.33255682\n",
      "Iteration 209, loss = 0.34769707\n",
      "Iteration 1560, loss = 0.23270362\n",
      "Iteration 210, loss = 0.34749256\n",
      "Iteration 910, loss = 0.29795205\n",
      "Iteration 211, loss = 0.34725351\n",
      "Iteration 583, loss = 0.33233641\n",
      "Iteration 2038, loss = 0.18305192\n",
      "Iteration 765, loss = 0.32923453\n",
      "Iteration 110, loss = 0.54717630\n",
      "Iteration 212, loss = 0.34703443\n",
      "Iteration 213, loss = 0.34681374\n",
      "Iteration 1561, loss = 0.23262143\n",
      "Iteration 584, loss = 0.33216046\n",
      "Iteration 214, loss = 0.34659543\n",
      "Iteration 2039, loss = 0.18305472\n",
      "Iteration 215, loss = 0.34636718\n",
      "Iteration 216, loss = 0.34616237\n",
      "Iteration 911, loss = 0.29774716\n",
      "Iteration 217, loss = 0.34591662\n",
      "Iteration 585, loss = 0.33213734\n",
      "Iteration 218, loss = 0.34572330\n",
      "Iteration 219, loss = 0.34551907\n",
      "Iteration 1562, loss = 0.23248815\n",
      "Iteration 220, loss = 0.34529739\n",
      "Iteration 2040, loss = 0.18288319\n",
      "Iteration 221, loss = 0.34508446\n",
      "Iteration 222, loss = 0.34488798\n",
      "Iteration 766, loss = 0.32914310\n",
      "Iteration 223, loss = 0.34467175\n",
      "Iteration 1563, loss = 0.23239244\n",
      "Iteration 111, loss = 0.54500508\n",
      "Iteration 586, loss = 0.33181598\n",
      "Iteration 224, loss = 0.34448030\n",
      "Iteration 225, loss = 0.34426791\n",
      "Iteration 912, loss = 0.29754498\n",
      "Iteration 226, loss = 0.34409014\n",
      "Iteration 1564, loss = 0.23230738\n",
      "Iteration 227, loss = 0.34387437\n",
      "Iteration 2041, loss = 0.18275228\n",
      "Iteration 587, loss = 0.33171965\n",
      "Iteration 2372, loss = 0.08877691\n",
      "Iteration 228, loss = 0.34367393\n",
      "Iteration 229, loss = 0.34347489\n",
      "Iteration 230, loss = 0.34328405\n",
      "Iteration 767, loss = 0.32898789\n",
      "Iteration 231, loss = 0.34309371\n",
      "Iteration 112, loss = 0.54278463\n",
      "Iteration 232, loss = 0.34288826\n",
      "Iteration 1565, loss = 0.23219081\n",
      "Iteration 233, loss = 0.34269777\n",
      "Iteration 588, loss = 0.33151607\n",
      "Iteration 913, loss = 0.29753198\n",
      "Iteration 234, loss = 0.34252151\n",
      "Iteration 2042, loss = 0.18264762\n",
      "Iteration 235, loss = 0.34232871\n",
      "Iteration 768, loss = 0.32889143\n",
      "Iteration 589, loss = 0.33139920\n",
      "Iteration 1566, loss = 0.23214936\n",
      "Iteration 236, loss = 0.34214587\n",
      "Iteration 113, loss = 0.54074999\n",
      "Iteration 2373, loss = 0.08881522\n",
      "Iteration 1567, loss = 0.23199234\n",
      "Iteration 237, loss = 0.34196416\n",
      "Iteration 590, loss = 0.33122764\n",
      "Iteration 238, loss = 0.34176690\n",
      "Iteration 1568, loss = 0.23201944\n",
      "Iteration 239, loss = 0.34159319\n",
      "Iteration 769, loss = 0.32881139\n",
      "Iteration 591, loss = 0.33107890\n",
      "Iteration 1569, loss = 0.23179747\n",
      "Iteration 240, loss = 0.34141146\n",
      "Iteration 2043, loss = 0.18258631\n",
      "Iteration 241, loss = 0.34122778\n",
      "Iteration 114, loss = 0.53866094\n",
      "Iteration 2374, loss = 0.08866790\n",
      "Iteration 1570, loss = 0.23167635\n",
      "Iteration 242, loss = 0.34103680\n",
      "Iteration 914, loss = 0.29709980\n",
      "Iteration 243, loss = 0.34087045\n",
      "Iteration 592, loss = 0.33091437\n",
      "Iteration 244, loss = 0.34069598\n",
      "Iteration 2044, loss = 0.18243776\n",
      "Iteration 245, loss = 0.34052460\n",
      "Iteration 246, loss = 0.34034425\n",
      "Iteration 1571, loss = 0.23162917\n",
      "Iteration 247, loss = 0.34017749\n",
      "Iteration 770, loss = 0.32871177\n",
      "Iteration 115, loss = 0.53665548\n",
      "Iteration 248, loss = 0.33998747\n",
      "Iteration 249, loss = 0.33983585\n",
      "Iteration 250, loss = 0.33968440\n",
      "Iteration 251, loss = 0.33949776\n",
      "Iteration 252, loss = 0.33931489\n",
      "Iteration 593, loss = 0.33086358\n",
      "Iteration 253, loss = 0.33915222\n",
      "Iteration 915, loss = 0.29687490\n",
      "Iteration 254, loss = 0.33898460\n",
      "Iteration 1572, loss = 0.23153442\n",
      "Iteration 255, loss = 0.33882632\n",
      "Iteration 256, loss = 0.33866461\n",
      "Iteration 116, loss = 0.53461809\n",
      "Iteration 594, loss = 0.33062249\n",
      "Iteration 257, loss = 0.33850216\n",
      "Iteration 258, loss = 0.33834210\n",
      "Iteration 771, loss = 0.32856675\n",
      "Iteration 259, loss = 0.33818001\n",
      "Iteration 2045, loss = 0.18242310\n",
      "Iteration 260, loss = 0.33802432\n",
      "Iteration 261, loss = 0.33786150\n",
      "Iteration 1573, loss = 0.23142513\n",
      "Iteration 262, loss = 0.33769494\n",
      "Iteration 595, loss = 0.33045634\n",
      "Iteration 916, loss = 0.29667497\n",
      "Iteration 117, loss = 0.53278675\n",
      "Iteration 263, loss = 0.33753536\n",
      "Iteration 2046, loss = 0.18227317\n",
      "Iteration 264, loss = 0.33738945\n",
      "Iteration 1574, loss = 0.23132446\n",
      "Iteration 265, loss = 0.33721976\n",
      "Iteration 772, loss = 0.32847319\n",
      "Iteration 1575, loss = 0.23119051\n",
      "Iteration 266, loss = 0.33708639\n",
      "Iteration 2375, loss = 0.08857977\n",
      "Iteration 2047, loss = 0.18212550\n",
      "Iteration 1576, loss = 0.23117232\n",
      "Iteration 267, loss = 0.33692680\n",
      "Iteration 596, loss = 0.33037529\n",
      "Iteration 2376, loss = 0.08853490\n",
      "Iteration 268, loss = 0.33677034\n",
      "Iteration 1577, loss = 0.23101895\n",
      "Iteration 269, loss = 0.33664197\n",
      "Iteration 270, loss = 0.33649466\n",
      "Iteration 271, loss = 0.33633484\n",
      "Iteration 917, loss = 0.29646459\n",
      "Iteration 1578, loss = 0.23088878\n",
      "Iteration 272, loss = 0.33618245\n",
      "Iteration 773, loss = 0.32836618\n",
      "Iteration 273, loss = 0.33602171\n",
      "Iteration 2377, loss = 0.08840586\n",
      "Iteration 274, loss = 0.33588869\n",
      "Iteration 597, loss = 0.33016811\n",
      "Iteration 118, loss = 0.53085626\n",
      "Iteration 1579, loss = 0.23082741\n",
      "Iteration 2378, loss = 0.08841476\n",
      "Iteration 275, loss = 0.33574301\n",
      "Iteration 1580, loss = 0.23073350\n",
      "Iteration 2379, loss = 0.08826211\n",
      "Iteration 598, loss = 0.32997903\n",
      "Iteration 2048, loss = 0.18207445\n",
      "Iteration 276, loss = 0.33559267\n",
      "Iteration 2380, loss = 0.08826466\n",
      "Iteration 277, loss = 0.33545137\n",
      "Iteration 774, loss = 0.32824005\n",
      "Iteration 599, loss = 0.32999144\n",
      "Iteration 278, loss = 0.33531486\n",
      "Iteration 119, loss = 0.52903238\n",
      "Iteration 1581, loss = 0.23060714\n",
      "Iteration 2381, loss = 0.08815359\n",
      "Iteration 279, loss = 0.33519265\n",
      "Iteration 2049, loss = 0.18198950\n",
      "Iteration 918, loss = 0.29619740\n",
      "Iteration 280, loss = 0.33503413\n",
      "Iteration 281, loss = 0.33488136\n",
      "Iteration 282, loss = 0.33476466\n",
      "Iteration 1582, loss = 0.23050524\n",
      "Iteration 2382, loss = 0.08827433\n",
      "Iteration 283, loss = 0.33462048\n",
      "Iteration 284, loss = 0.33448204\n",
      "Iteration 2050, loss = 0.18200857\n",
      "Iteration 1583, loss = 0.23054619\n",
      "Iteration 775, loss = 0.32812853\n",
      "Iteration 2383, loss = 0.08800537\n",
      "Iteration 285, loss = 0.33434312\n",
      "Iteration 286, loss = 0.33420480\n",
      "Iteration 600, loss = 0.32972326\n",
      "Iteration 120, loss = 0.52723105\n",
      "Iteration 2384, loss = 0.08795777\n",
      "Iteration 287, loss = 0.33407474\n",
      "Iteration 2385, loss = 0.08800439\n",
      "Iteration 288, loss = 0.33393632\n",
      "Iteration 289, loss = 0.33380350\n",
      "Iteration 290, loss = 0.33368306\n",
      "Iteration 2386, loss = 0.08778659\n",
      "Iteration 2051, loss = 0.18176047\n",
      "Iteration 776, loss = 0.32800970\n",
      "Iteration 1584, loss = 0.23030754\n",
      "Iteration 601, loss = 0.32955316\n",
      "Iteration 291, loss = 0.33355078\n",
      "Iteration 919, loss = 0.29602980\n",
      "Iteration 2387, loss = 0.08785437\n",
      "Iteration 292, loss = 0.33341575\n",
      "Iteration 293, loss = 0.33328649\n",
      "Iteration 602, loss = 0.32939752\n",
      "Iteration 294, loss = 0.33316589\n",
      "Iteration 295, loss = 0.33303580\n",
      "Iteration 121, loss = 0.52551837\n",
      "Iteration 2052, loss = 0.18172023\n",
      "Iteration 1585, loss = 0.23023655\n",
      "Iteration 777, loss = 0.32789997\n",
      "Iteration 296, loss = 0.33290950\n",
      "Iteration 603, loss = 0.32927406\n",
      "Iteration 297, loss = 0.33276801\n",
      "Iteration 298, loss = 0.33265282\n",
      "Iteration 299, loss = 0.33252353\n",
      "Iteration 1586, loss = 0.23015865\n",
      "Iteration 300, loss = 0.33240689\n",
      "Iteration 2053, loss = 0.18161688\n",
      "Iteration 301, loss = 0.33227192\n",
      "Iteration 1587, loss = 0.23007453\n",
      "Iteration 778, loss = 0.32780727\n",
      "Iteration 2388, loss = 0.08780535\n",
      "Iteration 302, loss = 0.33214932\n",
      "Iteration 604, loss = 0.32910313\n",
      "Iteration 303, loss = 0.33202826\n",
      "Iteration 304, loss = 0.33191370\n",
      "Iteration 2054, loss = 0.18147777\n",
      "Iteration 2389, loss = 0.08776089\n",
      "Iteration 1588, loss = 0.22995590\n",
      "Iteration 305, loss = 0.33179445\n",
      "Iteration 2390, loss = 0.08760790\n",
      "Iteration 920, loss = 0.29584629\n",
      "Iteration 306, loss = 0.33167346\n",
      "Iteration 2391, loss = 0.08758953\n",
      "Iteration 779, loss = 0.32766808\n",
      "Iteration 122, loss = 0.52381490\n",
      "Iteration 307, loss = 0.33155737\n",
      "Iteration 2055, loss = 0.18137671\n",
      "Iteration 2392, loss = 0.08744657\n",
      "Iteration 308, loss = 0.33144151\n",
      "Iteration 2393, loss = 0.08735863\n",
      "Iteration 1589, loss = 0.22985544\n",
      "Iteration 309, loss = 0.33131528\n",
      "Iteration 605, loss = 0.32894424\n",
      "Iteration 2394, loss = 0.08742076\n",
      "Iteration 310, loss = 0.33120037\n",
      "Iteration 2395, loss = 0.08734883\n",
      "Iteration 2056, loss = 0.18131159\n",
      "Iteration 2396, loss = 0.08719114\n",
      "Iteration 311, loss = 0.33107764\n",
      "Iteration 2397, loss = 0.08724465\n",
      "Iteration 1590, loss = 0.22972759\n",
      "Iteration 312, loss = 0.33097014\n",
      "Iteration 780, loss = 0.32758132\n",
      "Iteration 606, loss = 0.32877111\n",
      "Iteration 2398, loss = 0.08737886\n",
      "Iteration 313, loss = 0.33085670\n",
      "Iteration 921, loss = 0.29563946\n",
      "Iteration 1591, loss = 0.22972090\n",
      "Iteration 2057, loss = 0.18127592\n",
      "Iteration 2399, loss = 0.08702429\n",
      "Iteration 314, loss = 0.33072989\n",
      "Iteration 2400, loss = 0.08690960\n",
      "Iteration 1592, loss = 0.22955917\n",
      "Iteration 315, loss = 0.33062306\n",
      "Iteration 2401, loss = 0.08687443\n",
      "Iteration 316, loss = 0.33051159Iteration 123, loss = 0.52212835\n",
      "\n",
      "Iteration 2402, loss = 0.08693621\n",
      "Iteration 607, loss = 0.32862748\n",
      "Iteration 781, loss = 0.32749105\n",
      "Iteration 317, loss = 0.33040781\n",
      "Iteration 922, loss = 0.29534148\n",
      "Iteration 2403, loss = 0.08672085\n",
      "Iteration 318, loss = 0.33028554\n",
      "Iteration 1593, loss = 0.22943000\n",
      "Iteration 319, loss = 0.33018317\n",
      "Iteration 2058, loss = 0.18113341\n",
      "Iteration 2404, loss = 0.08668787\n",
      "Iteration 124, loss = 0.52046541\n",
      "Iteration 320, loss = 0.33006905\n",
      "Iteration 321, loss = 0.32994509\n",
      "Iteration 2405, loss = 0.08665071\n",
      "Iteration 322, loss = 0.32985229\n",
      "Iteration 2406, loss = 0.08655311\n",
      "Iteration 608, loss = 0.32847979\n",
      "Iteration 782, loss = 0.32738989\n",
      "Iteration 323, loss = 0.32976010\n",
      "Iteration 324, loss = 0.32962705\n",
      "Iteration 1594, loss = 0.22934038\n",
      "Iteration 325, loss = 0.32952651\n",
      "Iteration 923, loss = 0.29516100\n",
      "Iteration 2407, loss = 0.08649414\n",
      "Iteration 326, loss = 0.32943033\n",
      "Iteration 609, loss = 0.32834154\n",
      "Iteration 2408, loss = 0.08645009\n",
      "Iteration 327, loss = 0.32930681\n",
      "Iteration 1595, loss = 0.22925504\n",
      "Iteration 2409, loss = 0.08642970\n",
      "Iteration 783, loss = 0.32725538\n",
      "Iteration 2410, loss = 0.08634791\n",
      "Iteration 1596, loss = 0.22914260\n",
      "Iteration 328, loss = 0.32920889\n",
      "Iteration 2059, loss = 0.18107144\n",
      "Iteration 329, loss = 0.32910300\n",
      "Iteration 2411, loss = 0.08631617\n",
      "Iteration 125, loss = 0.51884708\n",
      "Iteration 330, loss = 0.32899695\n",
      "Iteration 1597, loss = 0.22906136\n",
      "Iteration 331, loss = 0.32889355\n",
      "Iteration 2412, loss = 0.08634595\n",
      "Iteration 332, loss = 0.32878727\n",
      "Iteration 333, loss = 0.32869903\n",
      "Iteration 2413, loss = 0.08614415\n",
      "Iteration 334, loss = 0.32858136\n",
      "Iteration 1598, loss = 0.22897177\n",
      "Iteration 335, loss = 0.32848044\n",
      "Iteration 2414, loss = 0.08619090\n",
      "Iteration 336, loss = 0.32838715\n",
      "Iteration 2415, loss = 0.08600563Iteration 924, loss = 0.29491268\n",
      "Iteration 337, loss = 0.32829549\n",
      "Iteration 1599, loss = 0.22888670\n",
      "\n",
      "Iteration 784, loss = 0.32713031\n",
      "Iteration 2060, loss = 0.18090634\n",
      "Iteration 338, loss = 0.32817788\n",
      "Iteration 610, loss = 0.32816314\n",
      "Iteration 339, loss = 0.32809161\n",
      "Iteration 2416, loss = 0.08599177\n",
      "Iteration 340, loss = 0.32798422\n",
      "Iteration 341, loss = 0.32788813\n",
      "Iteration 2417, loss = 0.08591517\n",
      "Iteration 342, loss = 0.32777805\n",
      "Iteration 343, loss = 0.32769551\n",
      "Iteration 2061, loss = 0.18091368\n",
      "Iteration 1600, loss = 0.22872513\n",
      "Iteration 925, loss = 0.29510367\n",
      "Iteration 344, loss = 0.32760179\n",
      "Iteration 345, loss = 0.32748981\n",
      "Iteration 2418, loss = 0.08591309\n",
      "Iteration 611, loss = 0.32820576\n",
      "Iteration 346, loss = 0.32739009\n",
      "Iteration 1601, loss = 0.22863191\n",
      "Iteration 347, loss = 0.32729877\n",
      "Iteration 348, loss = 0.32721693\n",
      "Iteration 785, loss = 0.32699965\n",
      "Iteration 349, loss = 0.32710572\n",
      "Iteration 2419, loss = 0.08588040\n",
      "Iteration 612, loss = 0.32787738\n",
      "Iteration 350, loss = 0.32701387\n",
      "Iteration 1602, loss = 0.22854297\n",
      "Iteration 126, loss = 0.51731601\n",
      "Iteration 351, loss = 0.32691922\n",
      "Iteration 2420, loss = 0.08578747\n",
      "Iteration 2421, loss = 0.08576853\n",
      "Iteration 352, loss = 0.32682731\n",
      "Iteration 1603, loss = 0.22851237\n",
      "Iteration 613, loss = 0.32771635\n",
      "Iteration 2422, loss = 0.08563154\n",
      "Iteration 2062, loss = 0.18080781\n",
      "Iteration 353, loss = 0.32675147\n",
      "Iteration 1604, loss = 0.22833185\n",
      "Iteration 2423, loss = 0.08557453\n",
      "Iteration 354, loss = 0.32665272\n",
      "Iteration 2424, loss = 0.08561192\n",
      "Iteration 127, loss = 0.51581885\n",
      "Iteration 355, loss = 0.32655231\n",
      "Iteration 356, loss = 0.32646073\n",
      "Iteration 614, loss = 0.32754785\n",
      "Iteration 1605, loss = 0.22839926\n",
      "Iteration 926, loss = 0.29441883\n",
      "Iteration 357, loss = 0.32635544\n",
      "Iteration 2425, loss = 0.08574272\n",
      "Iteration 2063, loss = 0.18069473\n",
      "Iteration 358, loss = 0.32627509\n",
      "Iteration 2426, loss = 0.08534743\n",
      "Iteration 615, loss = 0.32744651\n",
      "Iteration 359, loss = 0.32619100\n",
      "Iteration 786, loss = 0.32688023\n",
      "Iteration 2427, loss = 0.08538386\n",
      "Iteration 2428, loss = 0.08547854\n",
      "Iteration 360, loss = 0.32609121\n",
      "Iteration 2429, loss = 0.08518023\n",
      "Iteration 1606, loss = 0.22818024\n",
      "Iteration 361, loss = 0.32601294\n",
      "Iteration 2430, loss = 0.08512150\n",
      "Iteration 616, loss = 0.32729744\n",
      "Iteration 2064, loss = 0.18064184\n",
      "Iteration 1607, loss = 0.22809160\n",
      "Iteration 2431, loss = 0.08508565\n",
      "Iteration 362, loss = 0.32593282\n",
      "Iteration 363, loss = 0.32582212\n",
      "Iteration 787, loss = 0.32680012\n",
      "Iteration 364, loss = 0.32574132\n",
      "Iteration 1608, loss = 0.22793327\n",
      "Iteration 365, loss = 0.32563727\n",
      "Iteration 128, loss = 0.51432027\n",
      "Iteration 2065, loss = 0.18046359\n",
      "Iteration 366, loss = 0.32555559\n",
      "Iteration 367, loss = 0.32546910\n",
      "Iteration 927, loss = 0.29434170\n",
      "Iteration 617, loss = 0.32713544\n",
      "Iteration 2432, loss = 0.08506400\n",
      "Iteration 368, loss = 0.32538325\n",
      "Iteration 1609, loss = 0.22786346\n",
      "Iteration 369, loss = 0.32529025\n",
      "Iteration 2433, loss = 0.08501595\n",
      "Iteration 370, loss = 0.32519939\n",
      "Iteration 618, loss = 0.32696311\n",
      "Iteration 371, loss = 0.32512124\n",
      "Iteration 372, loss = 0.32503943\n",
      "Iteration 1610, loss = 0.22776499\n",
      "Iteration 373, loss = 0.32494555\n",
      "Iteration 788, loss = 0.32674362\n",
      "Iteration 374, loss = 0.32487329\n",
      "Iteration 2434, loss = 0.08491553\n",
      "Iteration 375, loss = 0.32479034\n",
      "Iteration 2066, loss = 0.18039662\n",
      "Iteration 1611, loss = 0.22762593\n",
      "Iteration 376, loss = 0.32468475\n",
      "Iteration 377, loss = 0.32460056\n",
      "Iteration 378, loss = 0.32452501\n",
      "Iteration 1612, loss = 0.22754343\n",
      "Iteration 379, loss = 0.32444122\n",
      "Iteration 129, loss = 0.51287424\n",
      "Iteration 2435, loss = 0.08498004\n",
      "Iteration 380, loss = 0.32435445\n",
      "Iteration 789, loss = 0.32656337\n",
      "Iteration 1613, loss = 0.22746037\n",
      "Iteration 381, loss = 0.32427384\n",
      "Iteration 2067, loss = 0.18028757\n",
      "Iteration 928, loss = 0.29427843\n",
      "Iteration 2436, loss = 0.08482859\n",
      "Iteration 619, loss = 0.32680752\n",
      "Iteration 382, loss = 0.32419058\n",
      "Iteration 1614, loss = 0.22734194\n",
      "Iteration 383, loss = 0.32411278\n",
      "Iteration 130, loss = 0.51144309\n",
      "Iteration 2437, loss = 0.08471572\n",
      "Iteration 384, loss = 0.32402373\n",
      "Iteration 1615, loss = 0.22727624\n",
      "Iteration 385, loss = 0.32394366\n",
      "Iteration 386, loss = 0.32387964\n",
      "Iteration 790, loss = 0.32645701\n",
      "Iteration 387, loss = 0.32378364\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 2438, loss = 0.08469717\n",
      "Iteration 1616, loss = 0.22722414Iteration 620, loss = 0.32668585\n",
      "Iteration 2068, loss = 0.18018494\n",
      "\n",
      "Iteration 2439, loss = 0.08468785\n",
      "Iteration 929, loss = 0.29381736\n",
      "Iteration 2440, loss = 0.08455223\n",
      "Iteration 2441, loss = 0.08455205\n",
      "Iteration 621, loss = 0.32655377\n",
      "Iteration 2442, loss = 0.08459402\n",
      "Iteration 2443, loss = 0.08439384\n",
      "Iteration 131, loss = 0.50999914\n",
      "Iteration 791, loss = 0.32635413\n",
      "Iteration 2069, loss = 0.18008058\n",
      "Iteration 1617, loss = 0.22706497\n",
      "Iteration 2444, loss = 0.08439126\n",
      "Iteration 2445, loss = 0.08425494\n",
      "Iteration 622, loss = 0.32638740\n",
      "Iteration 930, loss = 0.29356501\n",
      "Iteration 2446, loss = 0.08426750\n",
      "Iteration 792, loss = 0.32622033\n",
      "Iteration 2447, loss = 0.08422189\n",
      "Iteration 1618, loss = 0.22693609\n",
      "Iteration 623, loss = 0.32619305\n",
      "Iteration 1, loss = 0.77901412\n",
      "Iteration 2448, loss = 0.08418140\n",
      "Iteration 2070, loss = 0.17994761\n",
      "Iteration 2449, loss = 0.08411206\n",
      "Iteration 2, loss = 0.77468883\n",
      "Iteration 2450, loss = 0.08416812\n",
      "Iteration 3, loss = 0.76835876\n",
      "Iteration 793, loss = 0.32613702\n",
      "Iteration 132, loss = 0.50867666\n",
      "Iteration 1619, loss = 0.22687246\n",
      "Iteration 4, loss = 0.76058543\n",
      "Iteration 2451, loss = 0.08405437\n",
      "Iteration 931, loss = 0.29335859\n",
      "Iteration 624, loss = 0.32604413\n",
      "Iteration 5, loss = 0.75192809\n",
      "Iteration 2452, loss = 0.08397970\n",
      "Iteration 6, loss = 0.74300386\n",
      "Iteration 7, loss = 0.73386440\n",
      "Iteration 8, loss = 0.72501227\n",
      "Iteration 1620, loss = 0.22675971\n",
      "Iteration 9, loss = 0.71613951\n",
      "Iteration 2071, loss = 0.17988620\n",
      "Iteration 794, loss = 0.32601154\n",
      "Iteration 625, loss = 0.32592281\n",
      "Iteration 10, loss = 0.70741656\n",
      "Iteration 11, loss = 0.69953326\n",
      "Iteration 2453, loss = 0.08393904\n",
      "Iteration 12, loss = 0.69150170\n",
      "Iteration 626, loss = 0.32577761\n",
      "Iteration 1621, loss = 0.22665360\n",
      "Iteration 133, loss = 0.50725014\n",
      "Iteration 13, loss = 0.68393639\n",
      "Iteration 2454, loss = 0.08402791\n",
      "Iteration 14, loss = 0.67662184\n",
      "Iteration 15, loss = 0.66950954\n",
      "Iteration 2455, loss = 0.08387238\n",
      "Iteration 1622, loss = 0.22665082\n",
      "Iteration 16, loss = 0.66277842\n",
      "Iteration 17, loss = 0.65613433\n",
      "Iteration 2456, loss = 0.08378754\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 18, loss = 0.65003518\n",
      "Iteration 795, loss = 0.32589602\n",
      "Iteration 627, loss = 0.32559603Iteration 932, loss = 0.29318381\n",
      "\n",
      "Iteration 2072, loss = 0.17981280\n",
      "Iteration 19, loss = 0.64375626\n",
      "Iteration 1623, loss = 0.22646723\n",
      "Iteration 20, loss = 0.63787778\n",
      "Iteration 21, loss = 0.63204935\n",
      "Iteration 22, loss = 0.62646224\n",
      "Iteration 23, loss = 0.62093353\n",
      "Iteration 24, loss = 0.61579335\n",
      "Iteration 25, loss = 0.61055225\n",
      "Iteration 26, loss = 0.60551775\n",
      "Iteration 933, loss = 0.29305116\n",
      "Iteration 1624, loss = 0.22637531\n",
      "Iteration 27, loss = 0.60067028\n",
      "Iteration 2073, loss = 0.17970670\n",
      "Iteration 1, loss = 0.69930646\n",
      "Iteration 28, loss = 0.59574794\n",
      "Iteration 29, loss = 0.59104979\n",
      "Iteration 134, loss = 0.50601208\n",
      "Iteration 1625, loss = 0.22627893\n",
      "Iteration 2, loss = 0.69757414\n",
      "Iteration 30, loss = 0.58638630\n",
      "Iteration 796, loss = 0.32582381\n",
      "Iteration 3, loss = 0.69492295\n",
      "Iteration 628, loss = 0.32547782\n",
      "Iteration 4, loss = 0.69152695\n",
      "Iteration 5, loss = 0.68789964\n",
      "Iteration 6, loss = 0.68378430\n",
      "Iteration 31, loss = 0.58203109\n",
      "Iteration 7, loss = 0.67953222\n",
      "Iteration 32, loss = 0.57750678\n",
      "Iteration 8, loss = 0.67515676\n",
      "Iteration 1626, loss = 0.22620147\n",
      "Iteration 2074, loss = 0.17965669\n",
      "Iteration 9, loss = 0.67073893\n",
      "Iteration 10, loss = 0.66621860\n",
      "Iteration 33, loss = 0.57309429\n",
      "Iteration 629, loss = 0.32532722\n",
      "Iteration 11, loss = 0.66183211\n",
      "Iteration 34, loss = 0.56891167\n",
      "Iteration 135, loss = 0.50474592\n",
      "Iteration 35, loss = 0.56454155\n",
      "Iteration 797, loss = 0.32568809\n",
      "Iteration 934, loss = 0.29271416\n",
      "Iteration 36, loss = 0.56037402\n",
      "Iteration 37, loss = 0.55633893\n",
      "Iteration 630, loss = 0.32515263\n",
      "Iteration 38, loss = 0.55240788\n",
      "Iteration 39, loss = 0.54843965\n",
      "Iteration 40, loss = 0.54453630\n",
      "Iteration 2075, loss = 0.17953755\n",
      "Iteration 41, loss = 0.54084328\n",
      "Iteration 631, loss = 0.32508000\n",
      "Iteration 42, loss = 0.53699862\n",
      "Iteration 136, loss = 0.50349659\n",
      "Iteration 43, loss = 0.53328684\n",
      "Iteration 44, loss = 0.52973178\n",
      "Iteration 45, loss = 0.52625918\n",
      "Iteration 46, loss = 0.52270588\n",
      "Iteration 1627, loss = 0.22610142\n",
      "Iteration 2076, loss = 0.17948797\n",
      "Iteration 47, loss = 0.51931454\n",
      "Iteration 12, loss = 0.65739885\n",
      "Iteration 632, loss = 0.32488531\n",
      "Iteration 48, loss = 0.51590050\n",
      "Iteration 1628, loss = 0.22601885\n",
      "Iteration 13, loss = 0.65302314\n",
      "Iteration 49, loss = 0.51257020\n",
      "Iteration 633, loss = 0.32473125\n",
      "Iteration 14, loss = 0.64866311\n",
      "Iteration 50, loss = 0.50935893\n",
      "Iteration 51, loss = 0.50615396\n",
      "Iteration 1629, loss = 0.22583134\n",
      "Iteration 52, loss = 0.50307163\n",
      "Iteration 15, loss = 0.64450018\n",
      "Iteration 935, loss = 0.29275258\n",
      "Iteration 53, loss = 0.50005375\n",
      "Iteration 16, loss = 0.64024003\n",
      "Iteration 54, loss = 0.49710699\n",
      "Iteration 2077, loss = 0.17944507\n",
      "Iteration 55, loss = 0.49414501\n",
      "Iteration 634, loss = 0.32477640\n",
      "Iteration 17, loss = 0.63610592\n",
      "Iteration 137, loss = 0.50220264\n",
      "Iteration 1630, loss = 0.22575502\n",
      "Iteration 56, loss = 0.49121939\n",
      "Iteration 18, loss = 0.63190457\n",
      "Iteration 57, loss = 0.48839399\n",
      "Iteration 635, loss = 0.32444805\n",
      "Iteration 58, loss = 0.48561808\n",
      "Iteration 1631, loss = 0.22568703\n",
      "Iteration 19, loss = 0.62777849\n",
      "Iteration 20, loss = 0.62371008\n",
      "Iteration 798, loss = 0.32557883\n",
      "Iteration 59, loss = 0.48290273\n",
      "Iteration 1632, loss = 0.22561386\n",
      "Iteration 936, loss = 0.29225182\n",
      "Iteration 21, loss = 0.61973740\n",
      "Iteration 636, loss = 0.32430606\n",
      "Iteration 60, loss = 0.48017730\n",
      "Iteration 22, loss = 0.61579642\n",
      "Iteration 61, loss = 0.47753520\n",
      "Iteration 23, loss = 0.61192355\n",
      "Iteration 1633, loss = 0.22544972\n",
      "Iteration 24, loss = 0.60801842\n",
      "Iteration 2078, loss = 0.17925586\n",
      "Iteration 62, loss = 0.47499884\n",
      "Iteration 25, loss = 0.60418212\n",
      "Iteration 637, loss = 0.32412089\n",
      "Iteration 63, loss = 0.47250291\n",
      "Iteration 1634, loss = 0.22543367\n",
      "Iteration 799, loss = 0.32545688\n",
      "Iteration 26, loss = 0.60031540\n",
      "Iteration 64, loss = 0.47008117\n",
      "Iteration 138, loss = 0.50095760\n",
      "Iteration 27, loss = 0.59653118\n",
      "Iteration 1635, loss = 0.22529958\n",
      "Iteration 28, loss = 0.59271107\n",
      "Iteration 65, loss = 0.46761457\n",
      "Iteration 638, loss = 0.32396529\n",
      "Iteration 29, loss = 0.58903448\n",
      "Iteration 30, loss = 0.58530606\n",
      "Iteration 1636, loss = 0.22519021\n",
      "Iteration 66, loss = 0.46523656\n",
      "Iteration 2079, loss = 0.17913994\n",
      "Iteration 31, loss = 0.58156308\n",
      "Iteration 67, loss = 0.46299689\n",
      "Iteration 937, loss = 0.29200514\n",
      "Iteration 800, loss = 0.32534132\n",
      "Iteration 68, loss = 0.46068502\n",
      "Iteration 1637, loss = 0.22509300\n",
      "Iteration 32, loss = 0.57799564\n",
      "Iteration 69, loss = 0.45841716\n",
      "Iteration 139, loss = 0.49978129\n",
      "Iteration 639, loss = 0.32379910\n",
      "Iteration 1638, loss = 0.22500870\n",
      "Iteration 2080, loss = 0.17900404\n",
      "Iteration 33, loss = 0.57433798\n",
      "Iteration 70, loss = 0.45623089\n",
      "Iteration 34, loss = 0.57079154\n",
      "Iteration 35, loss = 0.56717357\n",
      "Iteration 71, loss = 0.45415138\n",
      "Iteration 36, loss = 0.56363825\n",
      "Iteration 72, loss = 0.45205910\n",
      "Iteration 140, loss = 0.49873094\n",
      "Iteration 1639, loss = 0.22489588\n",
      "Iteration 73, loss = 0.44998936\n",
      "Iteration 801, loss = 0.32526746\n",
      "Iteration 74, loss = 0.44804277\n",
      "Iteration 37, loss = 0.56012212\n",
      "Iteration 2081, loss = 0.17894037\n",
      "Iteration 75, loss = 0.44605483\n",
      "Iteration 640, loss = 0.32365710\n",
      "Iteration 76, loss = 0.44409908\n",
      "Iteration 77, loss = 0.44227510\n",
      "Iteration 38, loss = 0.55662945\n",
      "Iteration 78, loss = 0.44047980\n",
      "Iteration 1640, loss = 0.22477753\n",
      "Iteration 79, loss = 0.43875006\n",
      "Iteration 39, loss = 0.55320165\n",
      "Iteration 141, loss = 0.49749271\n",
      "Iteration 80, loss = 0.43700219\n",
      "Iteration 40, loss = 0.54980511\n",
      "Iteration 802, loss = 0.32512244\n",
      "Iteration 81, loss = 0.43529865\n",
      "Iteration 2082, loss = 0.17887874\n",
      "Iteration 41, loss = 0.54634307\n",
      "Iteration 641, loss = 0.32361822\n",
      "Iteration 1641, loss = 0.22475098\n",
      "Iteration 82, loss = 0.43357882\n",
      "Iteration 42, loss = 0.54302349\n",
      "Iteration 43, loss = 0.53966492\n",
      "Iteration 83, loss = 0.43203563\n",
      "Iteration 1642, loss = 0.22460981\n",
      "Iteration 44, loss = 0.53637580\n",
      "Iteration 938, loss = 0.29182216\n",
      "Iteration 642, loss = 0.32350320\n",
      "Iteration 45, loss = 0.53309943\n",
      "Iteration 84, loss = 0.43046252\n",
      "Iteration 46, loss = 0.52991582\n",
      "Iteration 85, loss = 0.42889219\n",
      "Iteration 2083, loss = 0.17874699\n",
      "Iteration 1643, loss = 0.22447941\n",
      "Iteration 86, loss = 0.42739780\n",
      "Iteration 87, loss = 0.42593122\n",
      "Iteration 803, loss = 0.32502258\n",
      "Iteration 47, loss = 0.52675625\n",
      "Iteration 88, loss = 0.42451093\n",
      "Iteration 1644, loss = 0.22437233\n",
      "Iteration 89, loss = 0.42301976\n",
      "Iteration 142, loss = 0.49636243\n",
      "Iteration 90, loss = 0.42163352\n",
      "Iteration 91, loss = 0.42030975\n",
      "Iteration 939, loss = 0.29160010\n",
      "Iteration 48, loss = 0.52357957\n",
      "Iteration 1645, loss = 0.22431587\n",
      "Iteration 92, loss = 0.41901144\n",
      "Iteration 93, loss = 0.41770817\n",
      "Iteration 94, loss = 0.41645702\n",
      "Iteration 49, loss = 0.52045657\n",
      "Iteration 50, loss = 0.51737729\n",
      "Iteration 643, loss = 0.32320018\n",
      "Iteration 95, loss = 0.41517943\n",
      "Iteration 2084, loss = 0.17865692\n",
      "Iteration 1646, loss = 0.22417831\n",
      "Iteration 96, loss = 0.41407248\n",
      "Iteration 51, loss = 0.51422942\n",
      "Iteration 97, loss = 0.41285456\n",
      "Iteration 52, loss = 0.51123325\n",
      "Iteration 98, loss = 0.41169378\n",
      "Iteration 804, loss = 0.32493615\n",
      "Iteration 143, loss = 0.49532431\n",
      "Iteration 99, loss = 0.41061961\n",
      "Iteration 53, loss = 0.50834613\n",
      "Iteration 100, loss = 0.40945608\n",
      "Iteration 940, loss = 0.29137705\n",
      "Iteration 54, loss = 0.50539908\n",
      "Iteration 101, loss = 0.40841365\n",
      "Iteration 102, loss = 0.40732395\n",
      "Iteration 55, loss = 0.50240714\n",
      "Iteration 103, loss = 0.40627629\n",
      "Iteration 56, loss = 0.49958936\n",
      "Iteration 104, loss = 0.40531612\n",
      "Iteration 105, loss = 0.40429666\n",
      "Iteration 57, loss = 0.49676290\n",
      "Iteration 1647, loss = 0.22412849\n",
      "Iteration 106, loss = 0.40335527\n",
      "Iteration 58, loss = 0.49391688\n",
      "Iteration 107, loss = 0.40236385\n",
      "Iteration 644, loss = 0.32305243\n",
      "Iteration 59, loss = 0.49127092\n",
      "Iteration 60, loss = 0.48853182\n",
      "Iteration 108, loss = 0.40142860\n",
      "Iteration 805, loss = 0.32480814\n",
      "Iteration 61, loss = 0.48586328\n",
      "Iteration 1648, loss = 0.22397792\n",
      "Iteration 109, loss = 0.40052591\n",
      "Iteration 645, loss = 0.32296153\n",
      "Iteration 62, loss = 0.48331137\n",
      "Iteration 2085, loss = 0.17860461\n",
      "Iteration 63, loss = 0.48069086\n",
      "Iteration 144, loss = 0.49421501\n",
      "Iteration 110, loss = 0.39964415\n",
      "Iteration 1649, loss = 0.22391311\n",
      "Iteration 941, loss = 0.29118105\n",
      "Iteration 64, loss = 0.47821305\n",
      "Iteration 111, loss = 0.39875414\n",
      "Iteration 65, loss = 0.47570203\n",
      "Iteration 646, loss = 0.32277259\n",
      "Iteration 112, loss = 0.39787234\n",
      "Iteration 66, loss = 0.47319756\n",
      "Iteration 67, loss = 0.47079326\n",
      "Iteration 113, loss = 0.39706136\n",
      "Iteration 1650, loss = 0.22381076\n",
      "Iteration 68, loss = 0.46843509\n",
      "Iteration 145, loss = 0.49303458\n",
      "Iteration 69, loss = 0.46613365\n",
      "Iteration 114, loss = 0.39622164\n",
      "Iteration 2086, loss = 0.17851354\n",
      "Iteration 115, loss = 0.39538131\n",
      "Iteration 647, loss = 0.32262528\n",
      "Iteration 70, loss = 0.46386050\n",
      "Iteration 116, loss = 0.39459049\n",
      "Iteration 71, loss = 0.46158402\n",
      "Iteration 1651, loss = 0.22365354\n",
      "Iteration 72, loss = 0.45934635\n",
      "Iteration 942, loss = 0.29094525\n",
      "Iteration 2087, loss = 0.17836783\n",
      "Iteration 117, loss = 0.39382581\n",
      "Iteration 73, loss = 0.45718470\n",
      "Iteration 806, loss = 0.32468225\n",
      "Iteration 118, loss = 0.39308975\n",
      "Iteration 119, loss = 0.39232312\n",
      "Iteration 120, loss = 0.39160650\n",
      "Iteration 74, loss = 0.45519192\n",
      "Iteration 1652, loss = 0.22358692\n",
      "Iteration 121, loss = 0.39086725\n",
      "Iteration 122, loss = 0.39014138\n",
      "Iteration 648, loss = 0.32256276\n",
      "Iteration 123, loss = 0.38946650\n",
      "Iteration 124, loss = 0.38879809\n",
      "Iteration 807, loss = 0.32456356\n",
      "Iteration 75, loss = 0.45298500\n",
      "Iteration 943, loss = 0.29078418\n",
      "Iteration 125, loss = 0.38811661\n",
      "Iteration 126, loss = 0.38747100\n",
      "Iteration 127, loss = 0.38682443\n",
      "Iteration 146, loss = 0.49202250\n",
      "Iteration 2088, loss = 0.17833846\n",
      "Iteration 76, loss = 0.45095641\n",
      "Iteration 1653, loss = 0.22347427\n",
      "Iteration 128, loss = 0.38619330\n",
      "Iteration 649, loss = 0.32239732\n",
      "Iteration 129, loss = 0.38555614\n",
      "Iteration 808, loss = 0.32446734\n",
      "Iteration 130, loss = 0.38494615\n",
      "Iteration 77, loss = 0.44897058\n",
      "Iteration 131, loss = 0.38431168\n",
      "Iteration 2089, loss = 0.17817059\n",
      "Iteration 132, loss = 0.38375568\n",
      "Iteration 78, loss = 0.44698677\n",
      "Iteration 1654, loss = 0.22339047\n",
      "Iteration 944, loss = 0.29052664\n",
      "Iteration 809, loss = 0.32434655\n",
      "Iteration 133, loss = 0.38315055\n",
      "Iteration 79, loss = 0.44504306\n",
      "Iteration 1655, loss = 0.22327141\n",
      "Iteration 134, loss = 0.38255923\n",
      "Iteration 135, loss = 0.38202006\n",
      "Iteration 80, loss = 0.44312787\n",
      "Iteration 136, loss = 0.38146714\n",
      "Iteration 650, loss = 0.32221423\n",
      "Iteration 81, loss = 0.44126995\n",
      "Iteration 137, loss = 0.38094654\n",
      "Iteration 138, loss = 0.38037047\n",
      "Iteration 2090, loss = 0.17809968\n",
      "Iteration 82, loss = 0.43942154\n",
      "Iteration 147, loss = 0.49098718\n",
      "Iteration 139, loss = 0.37984181\n",
      "Iteration 83, loss = 0.43765309\n",
      "Iteration 140, loss = 0.37931569\n",
      "Iteration 141, loss = 0.37879870\n",
      "Iteration 84, loss = 0.43588824\n",
      "Iteration 142, loss = 0.37827487\n",
      "Iteration 85, loss = 0.43420261\n",
      "Iteration 945, loss = 0.29033778\n",
      "Iteration 1656, loss = 0.22319594\n",
      "Iteration 651, loss = 0.32201880\n",
      "Iteration 143, loss = 0.37781296\n",
      "Iteration 810, loss = 0.32425962\n",
      "Iteration 86, loss = 0.43247952\n",
      "Iteration 144, loss = 0.37727623\n",
      "Iteration 87, loss = 0.43080461\n",
      "Iteration 1657, loss = 0.22312206\n",
      "Iteration 145, loss = 0.37685173\n",
      "Iteration 88, loss = 0.42923757\n",
      "Iteration 652, loss = 0.32187206\n",
      "Iteration 146, loss = 0.37634307\n",
      "Iteration 2091, loss = 0.17822551\n",
      "Iteration 148, loss = 0.49001407\n",
      "Iteration 147, loss = 0.37585156\n",
      "Iteration 89, loss = 0.42765835\n",
      "Iteration 946, loss = 0.29011444\n",
      "Iteration 1658, loss = 0.22297129\n",
      "Iteration 148, loss = 0.37542173\n",
      "Iteration 653, loss = 0.32174616\n",
      "Iteration 90, loss = 0.42605788\n",
      "Iteration 149, loss = 0.37492166Iteration 811, loss = 0.32415093\n",
      "\n",
      "Iteration 1659, loss = 0.22288196\n",
      "Iteration 150, loss = 0.37450472\n",
      "Iteration 654, loss = 0.32156437\n",
      "Iteration 1660, loss = 0.22277815\n",
      "Iteration 91, loss = 0.42451530\n",
      "Iteration 151, loss = 0.37402064\n",
      "Iteration 152, loss = 0.37357443\n",
      "Iteration 92, loss = 0.42304327\n",
      "Iteration 149, loss = 0.48909936\n",
      "Iteration 93, loss = 0.42157638\n",
      "Iteration 2092, loss = 0.17792452\n",
      "Iteration 153, loss = 0.37313098\n",
      "Iteration 154, loss = 0.37271495\n",
      "Iteration 94, loss = 0.42007730\n",
      "Iteration 155, loss = 0.37229518\n",
      "Iteration 95, loss = 0.41872597\n",
      "Iteration 812, loss = 0.32402019\n",
      "Iteration 1661, loss = 0.22271559\n",
      "Iteration 96, loss = 0.41735381\n",
      "Iteration 947, loss = 0.28987316\n",
      "Iteration 655, loss = 0.32144255\n",
      "Iteration 1662, loss = 0.22258990\n",
      "Iteration 156, loss = 0.37188841\n",
      "Iteration 97, loss = 0.41604150\n",
      "Iteration 98, loss = 0.41469890\n",
      "Iteration 2093, loss = 0.17788222\n",
      "Iteration 157, loss = 0.37147133\n",
      "Iteration 99, loss = 0.41338135\n",
      "Iteration 1663, loss = 0.22249705Iteration 100, loss = 0.41216402\n",
      "\n",
      "Iteration 813, loss = 0.32391890\n",
      "Iteration 158, loss = 0.37105580\n",
      "Iteration 150, loss = 0.48804458\n",
      "Iteration 656, loss = 0.32125824\n",
      "Iteration 159, loss = 0.37064845\n",
      "Iteration 160, loss = 0.37022082\n",
      "Iteration 101, loss = 0.41086642\n",
      "Iteration 161, loss = 0.36981264\n",
      "Iteration 2094, loss = 0.17774634\n",
      "Iteration 948, loss = 0.28986667\n",
      "Iteration 102, loss = 0.40972956\n",
      "Iteration 1664, loss = 0.22235869\n",
      "Iteration 814, loss = 0.32390897\n",
      "Iteration 103, loss = 0.40847914\n",
      "Iteration 162, loss = 0.36942321\n",
      "Iteration 163, loss = 0.36905846\n",
      "Iteration 104, loss = 0.40733827\n",
      "Iteration 164, loss = 0.36867306\n",
      "Iteration 165, loss = 0.36828584\n",
      "Iteration 166, loss = 0.36792556\n",
      "Iteration 2095, loss = 0.17777084\n",
      "Iteration 105, loss = 0.40619343\n",
      "Iteration 657, loss = 0.32123042\n",
      "Iteration 167, loss = 0.36752353\n",
      "Iteration 106, loss = 0.40505455\n",
      "Iteration 168, loss = 0.36716027\n",
      "Iteration 1665, loss = 0.22232739\n",
      "Iteration 151, loss = 0.48703593\n",
      "Iteration 169, loss = 0.36678853\n",
      "Iteration 2096, loss = 0.17756811\n",
      "Iteration 170, loss = 0.36641806\n",
      "Iteration 815, loss = 0.32371257\n",
      "Iteration 171, loss = 0.36608372\n",
      "Iteration 172, loss = 0.36571548\n",
      "Iteration 949, loss = 0.28945138\n",
      "Iteration 173, loss = 0.36535414\n",
      "Iteration 174, loss = 0.36498551\n",
      "Iteration 175, loss = 0.36466792\n",
      "Iteration 176, loss = 0.36433649\n",
      "Iteration 2097, loss = 0.17746165\n",
      "Iteration 177, loss = 0.36397424\n",
      "Iteration 816, loss = 0.32362758\n",
      "Iteration 178, loss = 0.36366028\n",
      "Iteration 179, loss = 0.36330887\n",
      "Iteration 152, loss = 0.48616183\n",
      "Iteration 180, loss = 0.36297330\n",
      "Iteration 107, loss = 0.40398948\n",
      "Iteration 108, loss = 0.40287149\n",
      "Iteration 181, loss = 0.36268590\n",
      "Iteration 2098, loss = 0.17735579\n",
      "Iteration 1666, loss = 0.22226295\n",
      "Iteration 109, loss = 0.40183844\n",
      "Iteration 950, loss = 0.28936107\n",
      "Iteration 182, loss = 0.36235185Iteration 817, loss = 0.32350248\n",
      "Iteration 110, loss = 0.40082182\n",
      "Iteration 658, loss = 0.32102551\n",
      "Iteration 111, loss = 0.39980651\n",
      "\n",
      "Iteration 112, loss = 0.39880855\n",
      "Iteration 1667, loss = 0.22209629\n",
      "Iteration 113, loss = 0.39784915\n",
      "Iteration 183, loss = 0.36201349\n",
      "Iteration 114, loss = 0.39685977\n",
      "Iteration 153, loss = 0.48521560\n",
      "Iteration 659, loss = 0.32084675\n",
      "Iteration 115, loss = 0.39593991\n",
      "Iteration 184, loss = 0.36171318\n",
      "Iteration 185, loss = 0.36140142\n",
      "Iteration 186, loss = 0.36111291\n",
      "Iteration 1668, loss = 0.22199932\n",
      "Iteration 116, loss = 0.39502642\n",
      "Iteration 660, loss = 0.32074629\n",
      "Iteration 187, loss = 0.36078275\n",
      "Iteration 188, loss = 0.36048818\n",
      "Iteration 189, loss = 0.36021670\n",
      "Iteration 1669, loss = 0.22188674\n",
      "Iteration 190, loss = 0.35987068\n",
      "Iteration 951, loss = 0.28906703\n",
      "Iteration 117, loss = 0.39416319\n",
      "Iteration 191, loss = 0.35960399\n",
      "Iteration 192, loss = 0.35931387\n",
      "Iteration 118, loss = 0.39323996\n",
      "Iteration 818, loss = 0.32342005\n",
      "Iteration 193, loss = 0.35902122\n",
      "Iteration 661, loss = 0.32054919\n",
      "Iteration 194, loss = 0.35875863\n",
      "Iteration 1670, loss = 0.22182686\n",
      "Iteration 119, loss = 0.39247231\n",
      "Iteration 662, loss = 0.32038152\n",
      "Iteration 195, loss = 0.35844640\n",
      "Iteration 819, loss = 0.32327278\n",
      "Iteration 120, loss = 0.39155755\n",
      "Iteration 1671, loss = 0.22170290\n",
      "Iteration 2099, loss = 0.17746604\n",
      "Iteration 121, loss = 0.39072975\n",
      "Iteration 663, loss = 0.32028439\n",
      "Iteration 1672, loss = 0.22167422\n",
      "Iteration 122, loss = 0.38983530\n",
      "Iteration 196, loss = 0.35819454\n",
      "Iteration 197, loss = 0.35789404\n",
      "Iteration 123, loss = 0.38909993\n",
      "Iteration 952, loss = 0.28900612\n",
      "Iteration 664, loss = 0.32030147\n",
      "Iteration 124, loss = 0.38832189\n",
      "Iteration 2100, loss = 0.17718386\n",
      "Iteration 1673, loss = 0.22147382\n",
      "Iteration 125, loss = 0.38756112\n",
      "Iteration 126, loss = 0.38680930\n",
      "Iteration 1674, loss = 0.22145156\n",
      "Iteration 127, loss = 0.38601681\n",
      "Iteration 128, loss = 0.38525487\n",
      "Iteration 665, loss = 0.32000054\n",
      "Iteration 129, loss = 0.38458358\n",
      "Iteration 2101, loss = 0.17712880\n",
      "Iteration 1675, loss = 0.22134562\n",
      "Iteration 953, loss = 0.28859507\n",
      "Iteration 130, loss = 0.38390376\n",
      "Iteration 131, loss = 0.38320357\n",
      "Iteration 1676, loss = 0.22118764\n",
      "Iteration 666, loss = 0.31978392\n",
      "Iteration 132, loss = 0.38254990\n",
      "Iteration 1677, loss = 0.22109719\n",
      "Iteration 2102, loss = 0.17698149\n",
      "Iteration 133, loss = 0.38185875\n",
      "Iteration 820, loss = 0.32318986\n",
      "Iteration 667, loss = 0.31966479\n",
      "Iteration 134, loss = 0.38118857\n",
      "Iteration 154, loss = 0.48427048\n",
      "Iteration 135, loss = 0.38061801\n",
      "Iteration 198, loss = 0.35764281\n",
      "Iteration 136, loss = 0.37992402\n",
      "Iteration 199, loss = 0.35734050\n",
      "Iteration 137, loss = 0.37930854\n",
      "Iteration 668, loss = 0.31952167\n",
      "Iteration 138, loss = 0.37870371\n",
      "Iteration 200, loss = 0.35709288\n",
      "Iteration 139, loss = 0.37810269\n",
      "Iteration 2103, loss = 0.17696583\n",
      "Iteration 1678, loss = 0.22099495\n",
      "Iteration 140, loss = 0.37750961\n",
      "Iteration 201, loss = 0.35681260\n",
      "Iteration 669, loss = 0.31937276\n",
      "Iteration 141, loss = 0.37695116\n",
      "Iteration 202, loss = 0.35655905\n",
      "Iteration 203, loss = 0.35627336\n",
      "Iteration 142, loss = 0.37637078\n",
      "Iteration 821, loss = 0.32303127\n",
      "Iteration 143, loss = 0.37575932\n",
      "Iteration 204, loss = 0.35601957\n",
      "Iteration 954, loss = 0.28839615\n",
      "Iteration 670, loss = 0.31918756\n",
      "Iteration 205, loss = 0.35576219\n",
      "Iteration 1679, loss = 0.22090892\n",
      "Iteration 206, loss = 0.35547312\n",
      "Iteration 207, loss = 0.35521050\n",
      "Iteration 208, loss = 0.35496954\n",
      "Iteration 155, loss = 0.48333409\n",
      "Iteration 209, loss = 0.35471499\n",
      "Iteration 144, loss = 0.37527385\n",
      "Iteration 2104, loss = 0.17683632\n",
      "Iteration 210, loss = 0.35444975\n",
      "Iteration 211, loss = 0.35420258\n",
      "Iteration 212, loss = 0.35394094\n",
      "Iteration 213, loss = 0.35369273\n",
      "Iteration 214, loss = 0.35345466\n",
      "Iteration 1680, loss = 0.22080240\n",
      "Iteration 215, loss = 0.35318644\n",
      "Iteration 671, loss = 0.31902380\n",
      "Iteration 145, loss = 0.37468725\n",
      "Iteration 216, loss = 0.35296081\n",
      "Iteration 2105, loss = 0.17673047\n",
      "Iteration 955, loss = 0.28817036\n",
      "Iteration 822, loss = 0.32298966\n",
      "Iteration 146, loss = 0.37412300\n",
      "Iteration 1681, loss = 0.22070148\n",
      "Iteration 147, loss = 0.37367905\n",
      "Iteration 217, loss = 0.35269839\n",
      "Iteration 148, loss = 0.37315108\n",
      "Iteration 218, loss = 0.35246638\n",
      "Iteration 1682, loss = 0.22067172\n",
      "Iteration 219, loss = 0.35223426\n",
      "Iteration 220, loss = 0.35203852\n",
      "Iteration 221, loss = 0.35176490\n",
      "Iteration 149, loss = 0.37262987\n",
      "Iteration 672, loss = 0.31890644\n",
      "Iteration 2106, loss = 0.17667545\n",
      "Iteration 222, loss = 0.35151900\n",
      "Iteration 1683, loss = 0.22050217\n",
      "Iteration 223, loss = 0.35130174\n",
      "Iteration 224, loss = 0.35107146\n",
      "Iteration 225, loss = 0.35086431\n",
      "Iteration 150, loss = 0.37214747\n",
      "Iteration 226, loss = 0.35064569\n",
      "Iteration 156, loss = 0.48257257\n",
      "Iteration 823, loss = 0.32298869\n",
      "Iteration 227, loss = 0.35040615\n",
      "Iteration 1684, loss = 0.22037566\n",
      "Iteration 228, loss = 0.35018689\n",
      "Iteration 229, loss = 0.34995872\n",
      "Iteration 151, loss = 0.37161801\n",
      "Iteration 230, loss = 0.34976223\n",
      "Iteration 231, loss = 0.34952687\n",
      "Iteration 673, loss = 0.31875893\n",
      "Iteration 152, loss = 0.37115560\n",
      "Iteration 232, loss = 0.34933859\n",
      "Iteration 956, loss = 0.28799026\n",
      "Iteration 233, loss = 0.34910220\n",
      "Iteration 1685, loss = 0.22044559\n",
      "Iteration 234, loss = 0.34889733\n",
      "Iteration 824, loss = 0.32274044\n",
      "Iteration 235, loss = 0.34868338\n",
      "Iteration 153, loss = 0.37070346\n",
      "Iteration 154, loss = 0.37019562\n",
      "Iteration 674, loss = 0.31858791\n",
      "Iteration 236, loss = 0.34848032\n",
      "Iteration 237, loss = 0.34826006\n",
      "Iteration 155, loss = 0.36976399\n",
      "Iteration 675, loss = 0.31848058\n",
      "Iteration 2107, loss = 0.17661437\n",
      "Iteration 238, loss = 0.34806980\n",
      "Iteration 156, loss = 0.36928671\n",
      "Iteration 239, loss = 0.34785226\n",
      "Iteration 157, loss = 0.48163115\n",
      "Iteration 957, loss = 0.28769603\n",
      "Iteration 157, loss = 0.36883700\n",
      "Iteration 240, loss = 0.34764752\n",
      "Iteration 825, loss = 0.32264164\n",
      "Iteration 158, loss = 0.36841938\n",
      "Iteration 159, loss = 0.36798590\n",
      "Iteration 241, loss = 0.34745769\n",
      "Iteration 242, loss = 0.34724909\n",
      "Iteration 243, loss = 0.34706467\n",
      "Iteration 244, loss = 0.34684063\n",
      "Iteration 1686, loss = 0.22021960\n",
      "Iteration 676, loss = 0.31838314\n",
      "Iteration 245, loss = 0.34664237\n",
      "Iteration 160, loss = 0.36754909Iteration 158, loss = 0.48076347\n",
      "\n",
      "Iteration 826, loss = 0.32257186\n",
      "Iteration 2108, loss = 0.17651826\n",
      "Iteration 246, loss = 0.34645500\n",
      "Iteration 1687, loss = 0.22011871\n",
      "Iteration 161, loss = 0.36711478\n",
      "Iteration 247, loss = 0.34624639\n",
      "Iteration 162, loss = 0.36670151\n",
      "Iteration 248, loss = 0.34608295\n",
      "Iteration 827, loss = 0.32247872\n",
      "Iteration 1688, loss = 0.22007223\n",
      "Iteration 159, loss = 0.47998122\n",
      "Iteration 249, loss = 0.34585787\n",
      "Iteration 163, loss = 0.36630677\n",
      "Iteration 677, loss = 0.31816974\n",
      "Iteration 1689, loss = 0.21989244\n",
      "Iteration 164, loss = 0.36590441\n",
      "Iteration 250, loss = 0.34568130\n",
      "Iteration 165, loss = 0.36547507\n",
      "Iteration 166, loss = 0.36509118\n",
      "Iteration 1690, loss = 0.21984971\n",
      "Iteration 251, loss = 0.34548897\n",
      "Iteration 2109, loss = 0.17639542\n",
      "Iteration 252, loss = 0.34528478\n",
      "Iteration 678, loss = 0.31798478\n",
      "Iteration 253, loss = 0.34509660\n",
      "Iteration 958, loss = 0.28752529\n",
      "Iteration 828, loss = 0.32231537\n",
      "Iteration 167, loss = 0.36470686\n",
      "Iteration 254, loss = 0.34490636\n",
      "Iteration 168, loss = 0.36433523\n",
      "Iteration 1691, loss = 0.21968186\n",
      "Iteration 255, loss = 0.34473609\n",
      "Iteration 2110, loss = 0.17635974\n",
      "Iteration 256, loss = 0.34453690\n",
      "Iteration 257, loss = 0.34435688\n",
      "Iteration 1692, loss = 0.21965539\n",
      "Iteration 169, loss = 0.36392779\n",
      "Iteration 258, loss = 0.34416903\n",
      "Iteration 160, loss = 0.47911363\n",
      "Iteration 259, loss = 0.34399688\n",
      "Iteration 170, loss = 0.36355602\n",
      "Iteration 829, loss = 0.32217524\n",
      "Iteration 260, loss = 0.34379195\n",
      "Iteration 679, loss = 0.31786878\n",
      "Iteration 2111, loss = 0.17620674\n",
      "Iteration 261, loss = 0.34362710\n",
      "Iteration 1693, loss = 0.21950933\n",
      "Iteration 171, loss = 0.36322592\n",
      "Iteration 262, loss = 0.34345785\n",
      "Iteration 959, loss = 0.28730823\n",
      "Iteration 172, loss = 0.36282041\n",
      "Iteration 2112, loss = 0.17611046\n",
      "Iteration 830, loss = 0.32218287\n",
      "Iteration 263, loss = 0.34326879\n",
      "Iteration 264, loss = 0.34308196\n",
      "Iteration 265, loss = 0.34290860\n",
      "Iteration 266, loss = 0.34275584\n",
      "Iteration 1694, loss = 0.21940621\n",
      "Iteration 680, loss = 0.31775836\n",
      "Iteration 267, loss = 0.34256964\n",
      "Iteration 173, loss = 0.36247913\n",
      "Iteration 268, loss = 0.34238775\n",
      "Iteration 269, loss = 0.34222202\n",
      "Iteration 270, loss = 0.34206824\n",
      "Iteration 831, loss = 0.32202059\n",
      "Iteration 271, loss = 0.34187114\n",
      "Iteration 161, loss = 0.47829879\n",
      "Iteration 272, loss = 0.34172321\n",
      "Iteration 273, loss = 0.34154091\n",
      "Iteration 174, loss = 0.36213390\n",
      "Iteration 274, loss = 0.34136222\n",
      "Iteration 681, loss = 0.31758952\n",
      "Iteration 1695, loss = 0.21930402\n",
      "Iteration 275, loss = 0.34120004\n",
      "Iteration 175, loss = 0.36176681\n",
      "Iteration 832, loss = 0.32185793\n",
      "Iteration 960, loss = 0.28705316\n",
      "Iteration 276, loss = 0.34103057\n",
      "Iteration 277, loss = 0.34088586\n",
      "Iteration 682, loss = 0.31740699\n",
      "Iteration 2113, loss = 0.17600752\n",
      "Iteration 176, loss = 0.36141367\n",
      "Iteration 278, loss = 0.34069890\n",
      "Iteration 279, loss = 0.34053100\n",
      "Iteration 177, loss = 0.36108853\n",
      "Iteration 1696, loss = 0.21930334\n",
      "Iteration 280, loss = 0.34037997\n",
      "Iteration 178, loss = 0.36076567\n",
      "Iteration 281, loss = 0.34020480\n",
      "Iteration 179, loss = 0.36039435\n",
      "Iteration 282, loss = 0.34005233\n",
      "Iteration 961, loss = 0.28690506\n",
      "Iteration 833, loss = 0.32178692\n",
      "Iteration 162, loss = 0.47749661\n",
      "Iteration 283, loss = 0.33989535\n",
      "Iteration 180, loss = 0.36009386\n",
      "Iteration 284, loss = 0.33976402\n",
      "Iteration 285, loss = 0.33958403\n",
      "Iteration 2114, loss = 0.17600102\n",
      "Iteration 1697, loss = 0.21909086\n",
      "Iteration 286, loss = 0.33941616\n",
      "Iteration 683, loss = 0.31724258\n",
      "Iteration 287, loss = 0.33926783\n",
      "Iteration 288, loss = 0.33909675\n",
      "Iteration 289, loss = 0.33895187\n",
      "Iteration 962, loss = 0.28681527\n",
      "Iteration 1698, loss = 0.21900446\n",
      "Iteration 181, loss = 0.35972898\n",
      "Iteration 290, loss = 0.33880186\n",
      "Iteration 291, loss = 0.33861987\n",
      "Iteration 182, loss = 0.35944481\n",
      "Iteration 292, loss = 0.33848697\n",
      "Iteration 1699, loss = 0.21892948\n",
      "Iteration 183, loss = 0.35912655\n",
      "Iteration 293, loss = 0.33833133\n",
      "Iteration 294, loss = 0.33816232\n",
      "Iteration 184, loss = 0.35879872\n",
      "Iteration 185, loss = 0.35849596\n",
      "Iteration 834, loss = 0.32162761\n",
      "Iteration 1700, loss = 0.21879635\n",
      "Iteration 163, loss = 0.47673484\n",
      "Iteration 186, loss = 0.35820177\n",
      "Iteration 684, loss = 0.31719847\n",
      "Iteration 2115, loss = 0.17589335\n",
      "Iteration 187, loss = 0.35787962\n",
      "Iteration 1701, loss = 0.21871458\n",
      "Iteration 188, loss = 0.35758236\n",
      "Iteration 295, loss = 0.33801766\n",
      "Iteration 189, loss = 0.35728486\n",
      "Iteration 2116, loss = 0.17579648\n",
      "Iteration 685, loss = 0.31699553\n",
      "Iteration 296, loss = 0.33786403\n",
      "Iteration 1702, loss = 0.21864935\n",
      "Iteration 835, loss = 0.32155285\n",
      "Iteration 297, loss = 0.33771654\n",
      "Iteration 190, loss = 0.35698670\n",
      "Iteration 298, loss = 0.33756056\n",
      "Iteration 686, loss = 0.31682400\n",
      "Iteration 299, loss = 0.33741354\n",
      "Iteration 300, loss = 0.33726636\n",
      "Iteration 191, loss = 0.35670596\n",
      "Iteration 301, loss = 0.33712039\n",
      "Iteration 1703, loss = 0.21854930\n",
      "Iteration 963, loss = 0.28648076\n",
      "Iteration 192, loss = 0.35641914\n",
      "Iteration 164, loss = 0.47585595\n",
      "Iteration 302, loss = 0.33696643\n",
      "Iteration 193, loss = 0.35614839\n",
      "Iteration 194, loss = 0.35584856\n",
      "Iteration 1704, loss = 0.21843043\n",
      "Iteration 836, loss = 0.32145100\n",
      "Iteration 195, loss = 0.35560091\n",
      "Iteration 687, loss = 0.31666925\n",
      "Iteration 303, loss = 0.33681852\n",
      "Iteration 196, loss = 0.35532333\n",
      "Iteration 197, loss = 0.35503613\n",
      "Iteration 2117, loss = 0.17570368\n",
      "Iteration 198, loss = 0.35476630\n",
      "Iteration 1705, loss = 0.21827747\n",
      "Iteration 165, loss = 0.47508327\n",
      "Iteration 304, loss = 0.33668357\n",
      "Iteration 199, loss = 0.35452001\n",
      "Iteration 305, loss = 0.33654177\n",
      "Iteration 1706, loss = 0.21819305\n",
      "Iteration 688, loss = 0.31649847\n",
      "Iteration 306, loss = 0.33639935\n",
      "Iteration 964, loss = 0.28628817\n",
      "Iteration 200, loss = 0.35426993\n",
      "Iteration 2118, loss = 0.17569551\n",
      "Iteration 307, loss = 0.33623971\n",
      "Iteration 837, loss = 0.32131690\n",
      "Iteration 1707, loss = 0.21810218\n",
      "Iteration 166, loss = 0.47437317\n",
      "Iteration 201, loss = 0.35398212\n",
      "Iteration 689, loss = 0.31640334\n",
      "Iteration 308, loss = 0.33610638\n",
      "Iteration 309, loss = 0.33596297\n",
      "Iteration 202, loss = 0.35373442\n",
      "Iteration 203, loss = 0.35348450\n",
      "Iteration 204, loss = 0.35322888\n",
      "Iteration 205, loss = 0.35299456\n",
      "Iteration 310, loss = 0.33583892\n",
      "Iteration 690, loss = 0.31621892\n",
      "Iteration 311, loss = 0.33567791\n",
      "Iteration 206, loss = 0.35273742\n",
      "Iteration 1708, loss = 0.21796250\n",
      "Iteration 2119, loss = 0.17545496\n",
      "Iteration 312, loss = 0.33553720\n",
      "Iteration 207, loss = 0.35250024\n",
      "Iteration 313, loss = 0.33540009\n",
      "Iteration 965, loss = 0.28608031\n",
      "Iteration 314, loss = 0.33525346\n",
      "Iteration 838, loss = 0.32128162\n",
      "Iteration 315, loss = 0.33511864\n",
      "Iteration 691, loss = 0.31602468\n",
      "Iteration 316, loss = 0.33497253\n",
      "Iteration 167, loss = 0.47361393\n",
      "Iteration 208, loss = 0.35226420\n",
      "Iteration 317, loss = 0.33484903\n",
      "Iteration 318, loss = 0.33468623\n",
      "Iteration 209, loss = 0.35203368Iteration 1709, loss = 0.21787375\n",
      "\n",
      "Iteration 319, loss = 0.33456675\n",
      "Iteration 320, loss = 0.33442382\n",
      "Iteration 210, loss = 0.35178066\n",
      "Iteration 321, loss = 0.33427188\n",
      "Iteration 211, loss = 0.35154976\n",
      "Iteration 322, loss = 0.33415164\n",
      "Iteration 2120, loss = 0.17538167\n",
      "Iteration 692, loss = 0.31591734\n",
      "Iteration 1710, loss = 0.21779957\n",
      "Iteration 212, loss = 0.35132240\n",
      "Iteration 323, loss = 0.33403158\n",
      "Iteration 324, loss = 0.33387995\n",
      "Iteration 325, loss = 0.33373982\n",
      "Iteration 1711, loss = 0.21771066\n",
      "Iteration 326, loss = 0.33361774\n",
      "Iteration 213, loss = 0.35111363\n",
      "Iteration 693, loss = 0.31591642\n",
      "Iteration 327, loss = 0.33349960\n",
      "Iteration 214, loss = 0.35088374\n",
      "Iteration 328, loss = 0.33334537\n",
      "Iteration 839, loss = 0.32109245\n",
      "Iteration 329, loss = 0.33323083\n",
      "Iteration 168, loss = 0.47286338\n",
      "Iteration 215, loss = 0.35064543\n",
      "Iteration 966, loss = 0.28585318\n",
      "Iteration 330, loss = 0.33307598\n",
      "Iteration 216, loss = 0.35045695\n",
      "Iteration 331, loss = 0.33295180\n",
      "Iteration 332, loss = 0.33283931\n",
      "Iteration 217, loss = 0.35021027\n",
      "Iteration 333, loss = 0.33271779\n",
      "Iteration 218, loss = 0.34999907\n",
      "Iteration 694, loss = 0.31564035\n",
      "Iteration 2121, loss = 0.17527128\n",
      "Iteration 334, loss = 0.33256456\n",
      "Iteration 1712, loss = 0.21759165\n",
      "Iteration 335, loss = 0.33244150\n",
      "Iteration 219, loss = 0.34980219\n",
      "Iteration 336, loss = 0.33230964\n",
      "Iteration 337, loss = 0.33217468\n",
      "Iteration 338, loss = 0.33205209\n",
      "Iteration 1713, loss = 0.21748116\n",
      "Iteration 339, loss = 0.33192197\n",
      "Iteration 340, loss = 0.33179913\n",
      "Iteration 2122, loss = 0.17517278\n",
      "Iteration 220, loss = 0.34958623\n",
      "Iteration 341, loss = 0.33166423\n",
      "Iteration 840, loss = 0.32100152\n",
      "Iteration 967, loss = 0.28575903\n",
      "Iteration 342, loss = 0.33156054\n",
      "Iteration 343, loss = 0.33143916\n",
      "Iteration 695, loss = 0.31562855\n",
      "Iteration 344, loss = 0.33129487\n",
      "Iteration 221, loss = 0.34936763\n",
      "Iteration 345, loss = 0.33117684\n",
      "Iteration 2123, loss = 0.17512622\n",
      "Iteration 696, loss = 0.31534232\n",
      "Iteration 346, loss = 0.33105182\n",
      "Iteration 222, loss = 0.34915380\n",
      "Iteration 1714, loss = 0.21740400\n",
      "Iteration 347, loss = 0.33093284\n",
      "Iteration 223, loss = 0.34896120\n",
      "Iteration 348, loss = 0.33081514\n",
      "Iteration 841, loss = 0.32091828\n",
      "Iteration 169, loss = 0.47214516\n",
      "Iteration 349, loss = 0.33067799\n",
      "Iteration 224, loss = 0.34873846\n",
      "Iteration 350, loss = 0.33056525\n",
      "Iteration 225, loss = 0.34854123\n",
      "Iteration 351, loss = 0.33044298\n",
      "Iteration 968, loss = 0.28539622\n",
      "Iteration 352, loss = 0.33032183\n",
      "Iteration 1715, loss = 0.21742439\n",
      "Iteration 353, loss = 0.33020478\n",
      "Iteration 354, loss = 0.33010298\n",
      "Iteration 226, loss = 0.34833618\n",
      "Iteration 355, loss = 0.32997589\n",
      "Iteration 356, loss = 0.32985114\n",
      "Iteration 1716, loss = 0.21726482\n",
      "Iteration 697, loss = 0.31522881\n",
      "Iteration 357, loss = 0.32977634\n",
      "Iteration 227, loss = 0.34813983\n",
      "Iteration 358, loss = 0.32962180\n",
      "Iteration 228, loss = 0.34794484\n",
      "Iteration 359, loss = 0.32949906\n",
      "Iteration 1717, loss = 0.21710393\n",
      "Iteration 360, loss = 0.32940044\n",
      "Iteration 2124, loss = 0.17505666\n",
      "Iteration 229, loss = 0.34774314\n",
      "Iteration 698, loss = 0.31501237\n",
      "Iteration 1718, loss = 0.21695882\n",
      "Iteration 361, loss = 0.32928652\n",
      "Iteration 842, loss = 0.32078340\n",
      "Iteration 170, loss = 0.47143972\n",
      "Iteration 1719, loss = 0.21686513\n",
      "Iteration 2125, loss = 0.17487419\n",
      "Iteration 362, loss = 0.32917740\n",
      "Iteration 699, loss = 0.31486996\n",
      "Iteration 230, loss = 0.34756654\n",
      "Iteration 969, loss = 0.28523489\n",
      "Iteration 363, loss = 0.32908113\n",
      "Iteration 364, loss = 0.32895051\n",
      "Iteration 231, loss = 0.34735749\n",
      "Iteration 365, loss = 0.32883372\n",
      "Iteration 232, loss = 0.34718388\n",
      "Iteration 1720, loss = 0.21676853\n",
      "Iteration 700, loss = 0.31473729\n",
      "Iteration 366, loss = 0.32872184\n",
      "Iteration 233, loss = 0.34699141\n",
      "Iteration 1721, loss = 0.21666294\n",
      "Iteration 234, loss = 0.34680886\n",
      "Iteration 843, loss = 0.32071417\n",
      "Iteration 2126, loss = 0.17479147\n",
      "Iteration 367, loss = 0.32862574\n",
      "Iteration 235, loss = 0.34660739\n",
      "Iteration 368, loss = 0.32851349\n",
      "Iteration 1722, loss = 0.21657774\n",
      "Iteration 369, loss = 0.32841096\n",
      "Iteration 171, loss = 0.47071079\n",
      "Iteration 370, loss = 0.32829287\n",
      "Iteration 970, loss = 0.28491090\n",
      "Iteration 701, loss = 0.31459987\n",
      "Iteration 371, loss = 0.32818935\n",
      "Iteration 372, loss = 0.32807563\n",
      "Iteration 1723, loss = 0.21651946\n",
      "Iteration 373, loss = 0.32797388\n",
      "Iteration 374, loss = 0.32787425\n",
      "Iteration 236, loss = 0.34643403\n",
      "Iteration 375, loss = 0.32776842\n",
      "Iteration 1724, loss = 0.21645386\n",
      "Iteration 376, loss = 0.32765818\n",
      "Iteration 377, loss = 0.32756540\n",
      "Iteration 378, loss = 0.32746673\n",
      "Iteration 1725, loss = 0.21642745\n",
      "Iteration 702, loss = 0.31452072\n",
      "Iteration 379, loss = 0.32734211\n",
      "Iteration 237, loss = 0.34624660\n",
      "Iteration 380, loss = 0.32724216\n",
      "Iteration 844, loss = 0.32059226\n",
      "Iteration 381, loss = 0.32713367\n",
      "Iteration 172, loss = 0.46996905\n",
      "Iteration 382, loss = 0.32704118\n",
      "Iteration 2127, loss = 0.17474065\n",
      "Iteration 1726, loss = 0.21624228\n",
      "Iteration 238, loss = 0.34607502\n",
      "Iteration 383, loss = 0.32692742\n",
      "Iteration 239, loss = 0.34590088\n",
      "Iteration 384, loss = 0.32682030\n",
      "Iteration 385, loss = 0.32672422\n",
      "Iteration 240, loss = 0.34572053\n",
      "Iteration 386, loss = 0.32662727\n",
      "Iteration 387, loss = 0.32655411\n",
      "Iteration 241, loss = 0.34555982\n",
      "Iteration 1727, loss = 0.21607527\n",
      "Iteration 242, loss = 0.34538099\n",
      "Iteration 703, loss = 0.31425793\n",
      "Iteration 388, loss = 0.32640795\n",
      "Iteration 845, loss = 0.32046166\n",
      "Iteration 1728, loss = 0.21596115\n",
      "Iteration 389, loss = 0.32632199\n",
      "Iteration 243, loss = 0.34520983\n",
      "Iteration 390, loss = 0.32622454\n",
      "Iteration 391, loss = 0.32611415\n",
      "Iteration 392, loss = 0.32603067\n",
      "Iteration 244, loss = 0.34503350\n",
      "Iteration 393, loss = 0.32591272\n",
      "Iteration 2128, loss = 0.17470901\n",
      "Iteration 1729, loss = 0.21585582\n",
      "Iteration 245, loss = 0.34483738\n",
      "Iteration 394, loss = 0.32580602\n",
      "Iteration 846, loss = 0.32036179\n",
      "Iteration 704, loss = 0.31406036\n",
      "Iteration 971, loss = 0.28489153\n",
      "Iteration 246, loss = 0.34468415\n",
      "Iteration 395, loss = 0.32571778\n",
      "Iteration 1730, loss = 0.21577839\n",
      "Iteration 396, loss = 0.32564066\n",
      "Iteration 247, loss = 0.34452669\n",
      "Iteration 397, loss = 0.32553301\n",
      "Iteration 398, loss = 0.32540211\n",
      "Iteration 248, loss = 0.34437141\n",
      "Iteration 1731, loss = 0.21561952\n",
      "Iteration 399, loss = 0.32533625\n",
      "Iteration 705, loss = 0.31396131\n",
      "Iteration 400, loss = 0.32521741\n",
      "Iteration 249, loss = 0.34418609\n",
      "Iteration 401, loss = 0.32512986\n",
      "Iteration 706, loss = 0.31380475\n",
      "Iteration 402, loss = 0.32502471\n",
      "Iteration 250, loss = 0.34403764\n",
      "Iteration 2129, loss = 0.17467829\n",
      "Iteration 403, loss = 0.32492240\n",
      "Iteration 404, loss = 0.32483300\n",
      "Iteration 1732, loss = 0.21556065\n",
      "Iteration 847, loss = 0.32024297\n",
      "Iteration 707, loss = 0.31380856\n",
      "Iteration 251, loss = 0.34386446\n",
      "Iteration 173, loss = 0.46932716\n",
      "Iteration 252, loss = 0.34368783\n",
      "Iteration 405, loss = 0.32472734\n",
      "Iteration 253, loss = 0.34354084\n",
      "Iteration 2130, loss = 0.17446364\n",
      "Iteration 406, loss = 0.32463521\n",
      "Iteration 254, loss = 0.34340308\n",
      "Iteration 1733, loss = 0.21544464\n",
      "Iteration 407, loss = 0.32454945\n",
      "Iteration 708, loss = 0.31353291\n",
      "Iteration 408, loss = 0.32444168\n",
      "Iteration 1734, loss = 0.21541286\n",
      "Iteration 255, loss = 0.34322818\n",
      "Iteration 2131, loss = 0.17435677\n",
      "Iteration 174, loss = 0.46858508\n",
      "Iteration 256, loss = 0.34304600\n",
      "Iteration 972, loss = 0.28456272\n",
      "Iteration 409, loss = 0.32434930\n",
      "Iteration 848, loss = 0.32013659\n",
      "Iteration 257, loss = 0.34290472\n",
      "Iteration 410, loss = 0.32424542\n",
      "Iteration 258, loss = 0.34275760\n",
      "Iteration 709, loss = 0.31332518\n",
      "Iteration 411, loss = 0.32417813\n",
      "Iteration 849, loss = 0.32003091\n",
      "Iteration 412, loss = 0.32406710\n",
      "Iteration 973, loss = 0.28426889\n",
      "Iteration 259, loss = 0.34260268\n",
      "Iteration 260, loss = 0.34245231\n",
      "Iteration 413, loss = 0.32397441\n",
      "Iteration 261, loss = 0.34229841\n",
      "Iteration 262, loss = 0.34214833\n",
      "Iteration 263, loss = 0.34199796\n",
      "Iteration 414, loss = 0.32388533\n",
      "Iteration 710, loss = 0.31316867\n",
      "Iteration 2132, loss = 0.17432602\n",
      "Iteration 264, loss = 0.34186014\n",
      "Iteration 1735, loss = 0.21528854\n",
      "Iteration 415, loss = 0.32377836\n",
      "Iteration 265, loss = 0.34170969\n",
      "Iteration 175, loss = 0.46795432\n",
      "Iteration 266, loss = 0.34158447\n",
      "Iteration 416, loss = 0.32368067\n",
      "Iteration 267, loss = 0.34143282\n",
      "Iteration 417, loss = 0.32360142\n",
      "Iteration 268, loss = 0.34128971\n",
      "Iteration 418, loss = 0.32350828\n",
      "Iteration 2133, loss = 0.17416665\n",
      "Iteration 269, loss = 0.34114334\n",
      "Iteration 1736, loss = 0.21519067\n",
      "Iteration 270, loss = 0.34099717\n",
      "Iteration 419, loss = 0.32340451\n",
      "Iteration 271, loss = 0.34086133\n",
      "Iteration 420, loss = 0.32332092\n",
      "Iteration 850, loss = 0.31991911\n",
      "Iteration 421, loss = 0.32322990\n",
      "Iteration 974, loss = 0.28423981\n",
      "Iteration 272, loss = 0.34071477\n",
      "Iteration 1737, loss = 0.21502047\n",
      "Iteration 711, loss = 0.31303733\n",
      "Iteration 273, loss = 0.34058432\n",
      "Iteration 422, loss = 0.32314875\n",
      "Iteration 274, loss = 0.34046802\n",
      "Iteration 423, loss = 0.32305739\n",
      "Iteration 275, loss = 0.34032143\n",
      "Iteration 424, loss = 0.32295446\n",
      "Iteration 425, loss = 0.32286171\n",
      "Iteration 176, loss = 0.46730221\n",
      "Iteration 2134, loss = 0.17429886\n",
      "Iteration 426, loss = 0.32277660\n",
      "Iteration 1738, loss = 0.21495235\n",
      "Iteration 427, loss = 0.32267866\n",
      "Iteration 428, loss = 0.32259530\n",
      "Iteration 429, loss = 0.32250437\n",
      "Iteration 276, loss = 0.34017356\n",
      "Iteration 975, loss = 0.28386364\n",
      "Iteration 430, loss = 0.32240699\n",
      "Iteration 431, loss = 0.32232395\n",
      "Iteration 1739, loss = 0.21483381\n",
      "Iteration 432, loss = 0.32222989\n",
      "Iteration 277, loss = 0.34004443\n",
      "Iteration 278, loss = 0.33994614\n",
      "Iteration 433, loss = 0.32214292\n",
      "Iteration 434, loss = 0.32205512\n",
      "Iteration 279, loss = 0.33977741\n",
      "Iteration 2135, loss = 0.17399966\n",
      "Iteration 712, loss = 0.31288147\n",
      "Iteration 1740, loss = 0.21475780\n",
      "Iteration 851, loss = 0.31981722\n",
      "Iteration 435, loss = 0.32196884\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 177, loss = 0.46661879\n",
      "Iteration 976, loss = 0.28363600\n",
      "Iteration 280, loss = 0.33964434\n",
      "Iteration 713, loss = 0.31278722\n",
      "Iteration 1741, loss = 0.21462480\n",
      "Iteration 1, loss = 0.68618894\n",
      "Iteration 281, loss = 0.33952092\n",
      "Iteration 1742, loss = 0.21461624\n",
      "Iteration 282, loss = 0.33938107\n",
      "Iteration 852, loss = 0.31974049\n",
      "Iteration 2, loss = 0.68454783\n",
      "Iteration 283, loss = 0.33925831\n",
      "Iteration 284, loss = 0.33916236\n",
      "Iteration 285, loss = 0.33900806\n",
      "Iteration 714, loss = 0.31262860\n",
      "Iteration 1743, loss = 0.21455291\n",
      "Iteration 286, loss = 0.33886625\n",
      "Iteration 977, loss = 0.28344172\n",
      "Iteration 178, loss = 0.46597295\n",
      "Iteration 3, loss = 0.68201082\n",
      "Iteration 287, loss = 0.33874313\n",
      "Iteration 288, loss = 0.33861268\n",
      "Iteration 1744, loss = 0.21434144\n",
      "Iteration 2136, loss = 0.17391061\n",
      "Iteration 289, loss = 0.33850279\n",
      "Iteration 290, loss = 0.33837262\n",
      "Iteration 1745, loss = 0.21430682\n",
      "Iteration 291, loss = 0.33825650\n",
      "Iteration 292, loss = 0.33811922\n",
      "Iteration 4, loss = 0.67886645\n",
      "Iteration 1746, loss = 0.21420758\n",
      "Iteration 715, loss = 0.31242798\n",
      "Iteration 293, loss = 0.33800180\n",
      "Iteration 853, loss = 0.31959451\n",
      "Iteration 2137, loss = 0.17389404\n",
      "Iteration 294, loss = 0.33792668\n",
      "Iteration 1747, loss = 0.21406422\n",
      "Iteration 5, loss = 0.67535090\n",
      "Iteration 295, loss = 0.33776457\n",
      "Iteration 296, loss = 0.33764324\n",
      "Iteration 179, loss = 0.46532609\n",
      "Iteration 297, loss = 0.33753920\n",
      "Iteration 298, loss = 0.33741060\n",
      "Iteration 6, loss = 0.67146234\n",
      "Iteration 978, loss = 0.28336906\n",
      "Iteration 299, loss = 0.33729377\n",
      "Iteration 716, loss = 0.31228241\n",
      "Iteration 300, loss = 0.33717639\n",
      "Iteration 7, loss = 0.66744471\n",
      "Iteration 2138, loss = 0.17373856\n",
      "Iteration 1748, loss = 0.21393962\n",
      "Iteration 301, loss = 0.33706567\n",
      "Iteration 717, loss = 0.31210487\n",
      "Iteration 8, loss = 0.66329007Iteration 302, loss = 0.33694797\n",
      "\n",
      "Iteration 2139, loss = 0.17369438\n",
      "Iteration 303, loss = 0.33683354\n",
      "Iteration 718, loss = 0.31196804\n",
      "Iteration 854, loss = 0.31949409\n",
      "Iteration 304, loss = 0.33671982\n",
      "Iteration 9, loss = 0.65922833\n",
      "Iteration 1749, loss = 0.21385227\n",
      "Iteration 180, loss = 0.46466425\n",
      "Iteration 10, loss = 0.65503034\n",
      "Iteration 719, loss = 0.31180680\n",
      "Iteration 1750, loss = 0.21373070\n",
      "Iteration 305, loss = 0.33659895\n",
      "Iteration 11, loss = 0.65081153\n",
      "Iteration 855, loss = 0.31938837\n",
      "Iteration 1751, loss = 0.21370679\n",
      "Iteration 979, loss = 0.28301810\n",
      "Iteration 306, loss = 0.33648758\n",
      "Iteration 2140, loss = 0.17358503\n",
      "Iteration 1752, loss = 0.21350278\n",
      "Iteration 720, loss = 0.31167326\n",
      "Iteration 307, loss = 0.33637879\n",
      "Iteration 856, loss = 0.31928673\n",
      "Iteration 721, loss = 0.31152195\n",
      "Iteration 1753, loss = 0.21341641\n",
      "Iteration 181, loss = 0.46406495\n",
      "Iteration 12, loss = 0.64658491\n",
      "Iteration 308, loss = 0.33625390\n",
      "Iteration 980, loss = 0.28283254\n",
      "Iteration 722, loss = 0.31144266\n",
      "Iteration 309, loss = 0.33615161\n",
      "Iteration 857, loss = 0.31920444\n",
      "Iteration 2141, loss = 0.17356962\n",
      "Iteration 13, loss = 0.64246191\n",
      "Iteration 723, loss = 0.31118741\n",
      "Iteration 310, loss = 0.33603123\n",
      "Iteration 14, loss = 0.63831943\n",
      "Iteration 311, loss = 0.33600094\n",
      "Iteration 15, loss = 0.63433580\n",
      "Iteration 1754, loss = 0.21330667\n",
      "Iteration 312, loss = 0.33581136\n",
      "Iteration 858, loss = 0.31914652\n",
      "Iteration 981, loss = 0.28259739\n",
      "Iteration 313, loss = 0.33571546\n",
      "Iteration 2142, loss = 0.17355809\n",
      "Iteration 16, loss = 0.63031424\n",
      "Iteration 724, loss = 0.31105809\n",
      "Iteration 1755, loss = 0.21326164\n",
      "Iteration 17, loss = 0.62645916\n",
      "Iteration 314, loss = 0.33560055\n",
      "Iteration 725, loss = 0.31099986\n",
      "Iteration 315, loss = 0.33548749\n",
      "Iteration 18, loss = 0.62241948\n",
      "Iteration 182, loss = 0.46344736\n",
      "Iteration 19, loss = 0.61852984\n",
      "Iteration 20, loss = 0.61475380\n",
      "Iteration 316, loss = 0.33537898\n",
      "Iteration 1756, loss = 0.21311394\n",
      "Iteration 726, loss = 0.31082416\n",
      "Iteration 317, loss = 0.33528170\n",
      "Iteration 21, loss = 0.61099366\n",
      "Iteration 2143, loss = 0.17333862\n",
      "Iteration 318, loss = 0.33517692\n",
      "Iteration 859, loss = 0.31897140\n",
      "Iteration 22, loss = 0.60725236\n",
      "Iteration 319, loss = 0.33508439\n",
      "Iteration 982, loss = 0.28240063\n",
      "Iteration 320, loss = 0.33496750\n",
      "Iteration 1757, loss = 0.21301212\n",
      "Iteration 321, loss = 0.33487183\n",
      "Iteration 23, loss = 0.60363830\n",
      "Iteration 322, loss = 0.33475457\n",
      "Iteration 323, loss = 0.33464069\n",
      "Iteration 1758, loss = 0.21291579\n",
      "Iteration 324, loss = 0.33456110\n",
      "Iteration 325, loss = 0.33442691\n",
      "Iteration 727, loss = 0.31065251\n",
      "Iteration 24, loss = 0.59997492\n",
      "Iteration 326, loss = 0.33433180\n",
      "Iteration 327, loss = 0.33421933\n",
      "Iteration 1759, loss = 0.21293040\n",
      "Iteration 25, loss = 0.59638841\n",
      "Iteration 728, loss = 0.31045634\n",
      "Iteration 328, loss = 0.33412802\n",
      "Iteration 1760, loss = 0.21277139\n",
      "Iteration 26, loss = 0.59285000\n",
      "Iteration 329, loss = 0.33400897\n",
      "Iteration 983, loss = 0.28223956\n",
      "Iteration 729, loss = 0.31034454\n",
      "Iteration 27, loss = 0.58933934\n",
      "Iteration 1761, loss = 0.21265859\n",
      "Iteration 28, loss = 0.58592117\n",
      "Iteration 183, loss = 0.46289611\n",
      "Iteration 330, loss = 0.33392412\n",
      "Iteration 2144, loss = 0.17320239\n",
      "Iteration 860, loss = 0.31885614\n",
      "Iteration 29, loss = 0.58236667\n",
      "Iteration 331, loss = 0.33381291\n",
      "Iteration 30, loss = 0.57902488\n",
      "Iteration 730, loss = 0.31017883\n",
      "Iteration 1762, loss = 0.21252545\n",
      "Iteration 31, loss = 0.57563686\n",
      "Iteration 332, loss = 0.33372229\n",
      "Iteration 32, loss = 0.57232746\n",
      "Iteration 861, loss = 0.31875814\n",
      "Iteration 33, loss = 0.56908028\n",
      "Iteration 333, loss = 0.33360947\n",
      "Iteration 984, loss = 0.28193672\n",
      "Iteration 1763, loss = 0.21244336\n",
      "Iteration 334, loss = 0.33350626\n",
      "Iteration 335, loss = 0.33340296\n",
      "Iteration 731, loss = 0.30998634\n",
      "Iteration 34, loss = 0.56580177\n",
      "Iteration 862, loss = 0.31868899\n",
      "Iteration 2145, loss = 0.17314442\n",
      "Iteration 336, loss = 0.33330853\n",
      "Iteration 732, loss = 0.30985186\n",
      "Iteration 1764, loss = 0.21228860\n",
      "Iteration 184, loss = 0.46229910\n",
      "Iteration 35, loss = 0.56261371\n",
      "Iteration 337, loss = 0.33320443\n",
      "Iteration 338, loss = 0.33311415\n",
      "Iteration 1765, loss = 0.21226408\n",
      "Iteration 36, loss = 0.55940000\n",
      "Iteration 339, loss = 0.33300024\n",
      "Iteration 733, loss = 0.30970291\n",
      "Iteration 340, loss = 0.33294344\n",
      "Iteration 1766, loss = 0.21213732\n",
      "Iteration 341, loss = 0.33282585\n",
      "Iteration 37, loss = 0.55623990\n",
      "Iteration 342, loss = 0.33272362\n",
      "Iteration 1767, loss = 0.21201529\n",
      "Iteration 343, loss = 0.33260628\n",
      "Iteration 38, loss = 0.55318228\n",
      "Iteration 863, loss = 0.31853600\n",
      "Iteration 344, loss = 0.33253017\n",
      "Iteration 345, loss = 0.33244139\n",
      "Iteration 346, loss = 0.33232783\n",
      "Iteration 1768, loss = 0.21187665\n",
      "Iteration 2146, loss = 0.17309210\n",
      "Iteration 347, loss = 0.33224090\n",
      "Iteration 985, loss = 0.28181334\n",
      "Iteration 1769, loss = 0.21190933\n",
      "Iteration 864, loss = 0.31840806\n",
      "Iteration 39, loss = 0.55013930\n",
      "Iteration 185, loss = 0.46170631\n",
      "Iteration 1770, loss = 0.21171918\n",
      "Iteration 734, loss = 0.30955280\n",
      "Iteration 348, loss = 0.33213248\n",
      "Iteration 2147, loss = 0.17296004\n",
      "Iteration 1771, loss = 0.21163104\n",
      "Iteration 40, loss = 0.54694304\n",
      "Iteration 349, loss = 0.33205028\n",
      "Iteration 1772, loss = 0.21153033\n",
      "Iteration 350, loss = 0.33194421\n",
      "Iteration 735, loss = 0.30940119\n",
      "Iteration 986, loss = 0.28159356\n",
      "Iteration 41, loss = 0.54400068\n",
      "Iteration 865, loss = 0.31831422\n",
      "Iteration 42, loss = 0.54103289\n",
      "Iteration 1773, loss = 0.21147511\n",
      "Iteration 186, loss = 0.46109318\n",
      "Iteration 43, loss = 0.53799868\n",
      "Iteration 2148, loss = 0.17287155\n",
      "Iteration 351, loss = 0.33185033\n",
      "Iteration 44, loss = 0.53510306\n",
      "Iteration 736, loss = 0.30922552\n",
      "Iteration 352, loss = 0.33176496\n",
      "Iteration 866, loss = 0.31821484\n",
      "Iteration 1774, loss = 0.21127888\n",
      "Iteration 187, loss = 0.46053732\n",
      "Iteration 45, loss = 0.53224554\n",
      "Iteration 353, loss = 0.33167293\n",
      "Iteration 987, loss = 0.28131959\n",
      "Iteration 46, loss = 0.52940392\n",
      "Iteration 354, loss = 0.33158682\n",
      "Iteration 737, loss = 0.30906067\n",
      "Iteration 47, loss = 0.52663625\n",
      "Iteration 2149, loss = 0.17277698\n",
      "Iteration 1775, loss = 0.21118963\n",
      "Iteration 355, loss = 0.33148619\n",
      "Iteration 48, loss = 0.52387372\n",
      "Iteration 356, loss = 0.33139181\n",
      "Iteration 867, loss = 0.31810826Iteration 49, loss = 0.52108175\n",
      "\n",
      "Iteration 738, loss = 0.30892892\n",
      "Iteration 357, loss = 0.33128835\n",
      "Iteration 50, loss = 0.51841512\n",
      "Iteration 1776, loss = 0.21118462\n",
      "Iteration 988, loss = 0.28115729\n",
      "Iteration 2150, loss = 0.17280269\n",
      "Iteration 739, loss = 0.30876433\n",
      "Iteration 51, loss = 0.51566391\n",
      "Iteration 868, loss = 0.31797467\n",
      "Iteration 1777, loss = 0.21103092\n",
      "Iteration 52, loss = 0.51312604\n",
      "Iteration 740, loss = 0.30868604\n",
      "Iteration 188, loss = 0.45999379\n",
      "Iteration 1778, loss = 0.21089028\n",
      "Iteration 2151, loss = 0.17273600\n",
      "Iteration 53, loss = 0.51048768\n",
      "Iteration 1779, loss = 0.21082777\n",
      "Iteration 54, loss = 0.50800631\n",
      "Iteration 869, loss = 0.31788407\n",
      "Iteration 55, loss = 0.50541118\n",
      "Iteration 1780, loss = 0.21078970\n",
      "Iteration 741, loss = 0.30849332\n",
      "Iteration 56, loss = 0.50300338\n",
      "Iteration 989, loss = 0.28088483\n",
      "Iteration 358, loss = 0.33120529\n",
      "Iteration 57, loss = 0.50056761\n",
      "Iteration 742, loss = 0.30844115\n",
      "Iteration 359, loss = 0.33110890\n",
      "Iteration 870, loss = 0.31778445\n",
      "Iteration 743, loss = 0.30816836\n",
      "Iteration 58, loss = 0.49805433\n",
      "Iteration 360, loss = 0.33101905\n",
      "Iteration 990, loss = 0.28070191\n",
      "Iteration 2152, loss = 0.17257877\n",
      "Iteration 361, loss = 0.33093630\n",
      "Iteration 189, loss = 0.45940726\n",
      "Iteration 1781, loss = 0.21067007\n",
      "Iteration 59, loss = 0.49578536\n",
      "Iteration 362, loss = 0.33084561\n",
      "Iteration 871, loss = 0.31768288\n",
      "Iteration 1782, loss = 0.21054294\n",
      "Iteration 363, loss = 0.33075177\n",
      "Iteration 364, loss = 0.33067138\n",
      "Iteration 60, loss = 0.49332973\n",
      "Iteration 365, loss = 0.33058037\n",
      "Iteration 991, loss = 0.28067257\n",
      "Iteration 366, loss = 0.33049014\n",
      "Iteration 61, loss = 0.49109691\n",
      "Iteration 367, loss = 0.33039958\n",
      "Iteration 744, loss = 0.30813127\n",
      "Iteration 2153, loss = 0.17245175\n",
      "Iteration 1783, loss = 0.21032273\n",
      "Iteration 62, loss = 0.48884651\n",
      "Iteration 368, loss = 0.33035419\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1784, loss = 0.21030453\n",
      "Iteration 63, loss = 0.48663437\n",
      "Iteration 190, loss = 0.45888151\n",
      "Iteration 1, loss = 0.74917021\n",
      "Iteration 1785, loss = 0.21013621\n",
      "Iteration 872, loss = 0.31758461\n",
      "Iteration 745, loss = 0.30789773\n",
      "Iteration 64, loss = 0.48442601\n",
      "Iteration 2, loss = 0.74570872\n",
      "Iteration 2154, loss = 0.17240913\n",
      "Iteration 1786, loss = 0.21006002\n",
      "Iteration 65, loss = 0.48233018\n",
      "Iteration 1787, loss = 0.20996034\n",
      "Iteration 992, loss = 0.28027208\n",
      "Iteration 3, loss = 0.74033422\n",
      "Iteration 66, loss = 0.48014893\n",
      "Iteration 1788, loss = 0.20982691\n",
      "Iteration 873, loss = 0.31747547\n",
      "Iteration 2155, loss = 0.17226534\n",
      "Iteration 746, loss = 0.30772261\n",
      "Iteration 1789, loss = 0.20985616\n",
      "Iteration 4, loss = 0.73393966\n",
      "Iteration 874, loss = 0.31735348\n",
      "Iteration 67, loss = 0.47811797\n",
      "Iteration 2156, loss = 0.17218162\n",
      "Iteration 747, loss = 0.30759388\n",
      "Iteration 191, loss = 0.45835020\n",
      "Iteration 68, loss = 0.47605945\n",
      "Iteration 993, loss = 0.28003207\n",
      "Iteration 5, loss = 0.72660453\n",
      "Iteration 875, loss = 0.31723792\n",
      "Iteration 1790, loss = 0.20964538\n",
      "Iteration 6, loss = 0.71910902\n",
      "Iteration 7, loss = 0.71141148\n",
      "Iteration 748, loss = 0.30740560\n",
      "Iteration 1791, loss = 0.20954797\n",
      "Iteration 8, loss = 0.70357425\n",
      "Iteration 69, loss = 0.47400130\n",
      "Iteration 2157, loss = 0.17212733\n",
      "Iteration 9, loss = 0.69568514\n",
      "Iteration 994, loss = 0.27989202\n",
      "Iteration 749, loss = 0.30724793\n",
      "Iteration 876, loss = 0.31716595\n",
      "Iteration 70, loss = 0.47207657Iteration 1792, loss = 0.20944764\n",
      "\n",
      "Iteration 10, loss = 0.68822681\n",
      "Iteration 71, loss = 0.47011465\n",
      "Iteration 750, loss = 0.30719632\n",
      "Iteration 192, loss = 0.45786912\n",
      "Iteration 72, loss = 0.46816598\n",
      "Iteration 73, loss = 0.46629520\n",
      "Iteration 877, loss = 0.31702475\n",
      "Iteration 751, loss = 0.30705650\n",
      "Iteration 74, loss = 0.46452436\n",
      "Iteration 75, loss = 0.46267778\n",
      "Iteration 11, loss = 0.68051588\n",
      "Iteration 76, loss = 0.46093507\n",
      "Iteration 752, loss = 0.30683042\n",
      "Iteration 77, loss = 0.45920877\n",
      "Iteration 12, loss = 0.67342524\n",
      "Iteration 753, loss = 0.30668384\n",
      "Iteration 2158, loss = 0.17211612\n",
      "Iteration 78, loss = 0.45743127\n",
      "Iteration 13, loss = 0.66643079\n",
      "Iteration 1793, loss = 0.20930451\n",
      "Iteration 754, loss = 0.30652736\n",
      "Iteration 79, loss = 0.45580642\n",
      "Iteration 2159, loss = 0.17186430\n",
      "Iteration 995, loss = 0.27962830\n",
      "Iteration 14, loss = 0.65960458\n",
      "Iteration 15, loss = 0.65289484\n",
      "Iteration 80, loss = 0.45416141\n",
      "Iteration 1794, loss = 0.20927955\n",
      "Iteration 2160, loss = 0.17179331\n",
      "Iteration 16, loss = 0.64660698\n",
      "Iteration 878, loss = 0.31691755\n",
      "Iteration 193, loss = 0.45725554\n",
      "Iteration 81, loss = 0.45251645\n",
      "Iteration 17, loss = 0.64021764\n",
      "Iteration 755, loss = 0.30652276\n",
      "Iteration 1795, loss = 0.20907400\n",
      "Iteration 18, loss = 0.63401633\n",
      "Iteration 82, loss = 0.45089282\n",
      "Iteration 996, loss = 0.27937479\n",
      "Iteration 879, loss = 0.31679853\n",
      "Iteration 19, loss = 0.62838199\n",
      "Iteration 1796, loss = 0.20911708\n",
      "Iteration 83, loss = 0.44936430\n",
      "Iteration 20, loss = 0.62263455\n",
      "Iteration 194, loss = 0.45686807\n",
      "Iteration 84, loss = 0.44785189\n",
      "Iteration 85, loss = 0.44632741\n",
      "Iteration 1797, loss = 0.20892048\n",
      "Iteration 21, loss = 0.61697177\n",
      "Iteration 86, loss = 0.44484429\n",
      "Iteration 997, loss = 0.27927809\n",
      "Iteration 880, loss = 0.31674895\n",
      "Iteration 2161, loss = 0.17173028\n",
      "Iteration 1798, loss = 0.20879974\n",
      "Iteration 87, loss = 0.44345828\n",
      "Iteration 756, loss = 0.30619299\n",
      "Iteration 195, loss = 0.45622536\n",
      "Iteration 88, loss = 0.44199765\n",
      "Iteration 89, loss = 0.44055587\n",
      "Iteration 22, loss = 0.61159332\n",
      "Iteration 90, loss = 0.43918018\n",
      "Iteration 91, loss = 0.43779895\n",
      "Iteration 1799, loss = 0.20869939\n",
      "Iteration 23, loss = 0.60621252\n",
      "Iteration 196, loss = 0.45570584\n",
      "Iteration 2162, loss = 0.17174116\n",
      "Iteration 24, loss = 0.60094454\n",
      "Iteration 757, loss = 0.30607341\n",
      "Iteration 92, loss = 0.43649730\n",
      "Iteration 881, loss = 0.31663823\n",
      "Iteration 1800, loss = 0.20861048\n",
      "Iteration 758, loss = 0.30591598\n",
      "Iteration 93, loss = 0.43520743\n",
      "Iteration 25, loss = 0.59585713\n",
      "Iteration 1801, loss = 0.20857521\n",
      "Iteration 998, loss = 0.27895652\n",
      "Iteration 26, loss = 0.59082719\n",
      "Iteration 94, loss = 0.43387207\n",
      "Iteration 882, loss = 0.31649177\n",
      "Iteration 27, loss = 0.58585368\n",
      "Iteration 95, loss = 0.43265282\n",
      "Iteration 28, loss = 0.58115054\n",
      "Iteration 96, loss = 0.43145241\n",
      "Iteration 1802, loss = 0.20838068\n",
      "Iteration 759, loss = 0.30583727\n",
      "Iteration 97, loss = 0.43021352\n",
      "Iteration 197, loss = 0.45519487\n",
      "Iteration 2163, loss = 0.17157421\n",
      "Iteration 883, loss = 0.31636752\n",
      "Iteration 98, loss = 0.42895418\n",
      "Iteration 99, loss = 0.42781766\n",
      "Iteration 1803, loss = 0.20825153\n",
      "Iteration 29, loss = 0.57637681\n",
      "Iteration 760, loss = 0.30562113\n",
      "Iteration 884, loss = 0.31628085\n",
      "Iteration 100, loss = 0.42664507\n",
      "Iteration 999, loss = 0.27873078\n",
      "Iteration 101, loss = 0.42551371\n",
      "Iteration 198, loss = 0.45469443\n",
      "Iteration 761, loss = 0.30549099\n",
      "Iteration 102, loss = 0.42439785\n",
      "Iteration 103, loss = 0.42332259\n",
      "Iteration 30, loss = 0.57181499\n",
      "Iteration 2164, loss = 0.17160713\n",
      "Iteration 104, loss = 0.42224370\n",
      "Iteration 1804, loss = 0.20824633\n",
      "Iteration 762, loss = 0.30529961\n",
      "Iteration 1000, loss = 0.27863344\n",
      "Iteration 105, loss = 0.42113930\n",
      "Iteration 31, loss = 0.56725087\n",
      "Iteration 1805, loss = 0.20813545\n",
      "Iteration 106, loss = 0.42011239\n",
      "Iteration 763, loss = 0.30518309\n",
      "Iteration 2165, loss = 0.17141706\n",
      "Iteration 107, loss = 0.41905486\n",
      "Iteration 885, loss = 0.31618073\n",
      "Iteration 32, loss = 0.56285434\n",
      "Iteration 1806, loss = 0.20802853\n",
      "Iteration 199, loss = 0.45416832\n",
      "Iteration 108, loss = 0.41805317\n",
      "Iteration 33, loss = 0.55841869\n",
      "Iteration 1807, loss = 0.20793614\n",
      "Iteration 764, loss = 0.30501013\n",
      "Iteration 109, loss = 0.41709361\n",
      "Iteration 886, loss = 0.31608818\n",
      "Iteration 2166, loss = 0.17137712\n",
      "Iteration 34, loss = 0.55414069\n",
      "Iteration 1808, loss = 0.20783395\n",
      "Iteration 35, loss = 0.54993493\n",
      "Iteration 110, loss = 0.41609860\n",
      "Iteration 1809, loss = 0.20773748\n",
      "Iteration 1001, loss = 0.27856198\n",
      "Iteration 111, loss = 0.41515714\n",
      "Iteration 1810, loss = 0.20755181\n",
      "Iteration 36, loss = 0.54586079\n",
      "Iteration 112, loss = 0.41419842\n",
      "Iteration 765, loss = 0.30483714\n",
      "Iteration 37, loss = 0.54194717\n",
      "Iteration 200, loss = 0.45372950\n",
      "Iteration 113, loss = 0.41325687\n",
      "Iteration 38, loss = 0.53785737\n",
      "Iteration 114, loss = 0.41235441\n",
      "Iteration 2167, loss = 0.17124158\n",
      "Iteration 115, loss = 0.41144353\n",
      "Iteration 39, loss = 0.53394705\n",
      "Iteration 1811, loss = 0.20748719\n",
      "Iteration 116, loss = 0.41054167\n",
      "Iteration 887, loss = 0.31595922\n",
      "Iteration 766, loss = 0.30475711\n",
      "Iteration 40, loss = 0.53025716\n",
      "Iteration 1002, loss = 0.27813564\n",
      "Iteration 117, loss = 0.40964665\n",
      "Iteration 201, loss = 0.45317899\n",
      "Iteration 1812, loss = 0.20736068\n",
      "Iteration 1813, loss = 0.20726792\n",
      "Iteration 2168, loss = 0.17118243\n",
      "Iteration 118, loss = 0.40882433\n",
      "Iteration 767, loss = 0.30449855\n",
      "Iteration 888, loss = 0.31584682\n",
      "Iteration 1814, loss = 0.20718176\n",
      "Iteration 41, loss = 0.52651252\n",
      "Iteration 202, loss = 0.45273313\n",
      "Iteration 119, loss = 0.40798164\n",
      "Iteration 1815, loss = 0.20711914\n",
      "Iteration 2169, loss = 0.17104443\n",
      "Iteration 42, loss = 0.52284382\n",
      "Iteration 889, loss = 0.31576758\n",
      "Iteration 120, loss = 0.40717866\n",
      "Iteration 1003, loss = 0.27847759\n",
      "Iteration 768, loss = 0.30442026\n",
      "Iteration 43, loss = 0.51931208\n",
      "Iteration 203, loss = 0.45224271\n",
      "Iteration 121, loss = 0.40633485\n",
      "Iteration 2170, loss = 0.17103693\n",
      "Iteration 122, loss = 0.40549503\n",
      "Iteration 1816, loss = 0.20700351\n",
      "Iteration 44, loss = 0.51574502\n",
      "Iteration 890, loss = 0.31561806\n",
      "Iteration 2171, loss = 0.17086765\n",
      "Iteration 769, loss = 0.30421712Iteration 123, loss = 0.40470513\n",
      "\n",
      "Iteration 124, loss = 0.40393299\n",
      "Iteration 45, loss = 0.51236878\n",
      "Iteration 1817, loss = 0.20684782\n",
      "Iteration 125, loss = 0.40320227\n",
      "Iteration 770, loss = 0.30403057\n",
      "Iteration 1818, loss = 0.20676370\n",
      "Iteration 46, loss = 0.50895385\n",
      "Iteration 126, loss = 0.40239217\n",
      "Iteration 127, loss = 0.40166264\n",
      "Iteration 47, loss = 0.50578943\n",
      "Iteration 204, loss = 0.45176494\n",
      "Iteration 128, loss = 0.40090066\n",
      "Iteration 1819, loss = 0.20670743\n",
      "Iteration 48, loss = 0.50254287\n",
      "Iteration 129, loss = 0.40015734\n",
      "Iteration 2172, loss = 0.17083681\n",
      "Iteration 49, loss = 0.49940665\n",
      "Iteration 1820, loss = 0.20657274\n",
      "Iteration 130, loss = 0.39952409\n",
      "Iteration 1821, loss = 0.20649790\n",
      "Iteration 131, loss = 0.39874151\n",
      "Iteration 891, loss = 0.31562896\n",
      "Iteration 205, loss = 0.45130434\n",
      "Iteration 1004, loss = 0.27774968\n",
      "Iteration 132, loss = 0.39805811\n",
      "Iteration 1822, loss = 0.20635699\n",
      "Iteration 771, loss = 0.30390253\n",
      "Iteration 892, loss = 0.31539945\n",
      "Iteration 1823, loss = 0.20649884\n",
      "Iteration 2173, loss = 0.17068568\n",
      "Iteration 133, loss = 0.39736951\n",
      "Iteration 50, loss = 0.49636014\n",
      "Iteration 1824, loss = 0.20621867\n",
      "Iteration 772, loss = 0.30377660\n",
      "Iteration 134, loss = 0.39667723\n",
      "Iteration 893, loss = 0.31529808\n",
      "Iteration 1005, loss = 0.27755469\n",
      "Iteration 51, loss = 0.49326559\n",
      "Iteration 773, loss = 0.30361198\n",
      "Iteration 135, loss = 0.39599270\n",
      "Iteration 1825, loss = 0.20620842\n",
      "Iteration 136, loss = 0.39535152\n",
      "Iteration 894, loss = 0.31521111\n",
      "Iteration 206, loss = 0.45079558\n",
      "Iteration 52, loss = 0.49026085\n",
      "Iteration 137, loss = 0.39467619\n",
      "Iteration 774, loss = 0.30344575\n",
      "Iteration 1826, loss = 0.20598268\n",
      "Iteration 138, loss = 0.39409276\n",
      "Iteration 53, loss = 0.48742803\n",
      "Iteration 139, loss = 0.39346616\n",
      "Iteration 775, loss = 0.30326528\n",
      "Iteration 1827, loss = 0.20587680\n",
      "Iteration 895, loss = 0.31510358\n",
      "Iteration 140, loss = 0.39280152\n",
      "Iteration 54, loss = 0.48459995\n",
      "Iteration 2174, loss = 0.17086587\n",
      "Iteration 1828, loss = 0.20577258\n",
      "Iteration 207, loss = 0.45033522\n",
      "Iteration 776, loss = 0.30311307\n",
      "Iteration 141, loss = 0.39219039\n",
      "Iteration 1829, loss = 0.20568064\n",
      "Iteration 55, loss = 0.48189494\n",
      "Iteration 1830, loss = 0.20557951\n",
      "Iteration 2175, loss = 0.17070304\n",
      "Iteration 1006, loss = 0.27764128\n",
      "Iteration 896, loss = 0.31500344\n",
      "Iteration 56, loss = 0.47912722\n",
      "Iteration 142, loss = 0.39157705\n",
      "Iteration 1831, loss = 0.20543316\n",
      "Iteration 777, loss = 0.30304140\n",
      "Iteration 208, loss = 0.44984886\n",
      "Iteration 2176, loss = 0.17049266\n",
      "Iteration 143, loss = 0.39100501\n",
      "Iteration 1832, loss = 0.20532485\n",
      "Iteration 1007, loss = 0.27709573\n",
      "Iteration 778, loss = 0.30283359\n",
      "Iteration 897, loss = 0.31490371\n",
      "Iteration 144, loss = 0.39040434\n",
      "Iteration 1833, loss = 0.20524489\n",
      "Iteration 145, loss = 0.38988306\n",
      "Iteration 57, loss = 0.47654366\n",
      "Iteration 209, loss = 0.44945729\n",
      "Iteration 779, loss = 0.30267515\n",
      "Iteration 146, loss = 0.38925523\n",
      "Iteration 1834, loss = 0.20511563\n",
      "Iteration 58, loss = 0.47389309\n",
      "Iteration 898, loss = 0.31475347\n",
      "Iteration 147, loss = 0.38868917\n",
      "Iteration 1008, loss = 0.27699872\n",
      "Iteration 148, loss = 0.38811507\n",
      "Iteration 780, loss = 0.30248560\n",
      "Iteration 149, loss = 0.38758491\n",
      "Iteration 1835, loss = 0.20505250\n",
      "Iteration 150, loss = 0.38704200\n",
      "Iteration 59, loss = 0.47141293\n",
      "Iteration 151, loss = 0.38654445\n",
      "Iteration 899, loss = 0.31475122\n",
      "Iteration 781, loss = 0.30234528\n",
      "Iteration 1836, loss = 0.20494528\n",
      "Iteration 2177, loss = 0.17034827\n",
      "Iteration 60, loss = 0.46888098\n",
      "Iteration 152, loss = 0.38602194\n",
      "Iteration 61, loss = 0.46650754\n",
      "Iteration 62, loss = 0.46422623\n",
      "Iteration 210, loss = 0.44899328\n",
      "Iteration 1009, loss = 0.27678944\n",
      "Iteration 63, loss = 0.46188444\n",
      "Iteration 2178, loss = 0.17027616\n",
      "Iteration 900, loss = 0.31456232\n",
      "Iteration 1837, loss = 0.20479633\n",
      "Iteration 64, loss = 0.45972661\n",
      "Iteration 153, loss = 0.38543089\n",
      "Iteration 782, loss = 0.30218695\n",
      "Iteration 2179, loss = 0.17024215\n",
      "Iteration 65, loss = 0.45744470\n",
      "Iteration 154, loss = 0.38497304\n",
      "Iteration 1838, loss = 0.20471457\n",
      "Iteration 155, loss = 0.38446236\n",
      "Iteration 783, loss = 0.30208361\n",
      "Iteration 1839, loss = 0.20466700\n",
      "Iteration 901, loss = 0.31444876\n",
      "Iteration 66, loss = 0.45535590\n",
      "Iteration 1840, loss = 0.20463533\n",
      "Iteration 156, loss = 0.38392757\n",
      "Iteration 784, loss = 0.30194084\n",
      "Iteration 2180, loss = 0.17013409\n",
      "Iteration 1841, loss = 0.20445262\n",
      "Iteration 67, loss = 0.45322918\n",
      "Iteration 211, loss = 0.44851848\n",
      "Iteration 157, loss = 0.38343059\n",
      "Iteration 158, loss = 0.38293321\n",
      "Iteration 1842, loss = 0.20436903\n",
      "Iteration 1010, loss = 0.27643513\n",
      "Iteration 1843, loss = 0.20422950\n",
      "Iteration 159, loss = 0.38245239\n",
      "Iteration 1844, loss = 0.20418216\n",
      "Iteration 785, loss = 0.30178968\n",
      "Iteration 2181, loss = 0.17007999\n",
      "Iteration 1845, loss = 0.20405005\n",
      "Iteration 160, loss = 0.38196699\n",
      "Iteration 902, loss = 0.31439234\n",
      "Iteration 68, loss = 0.45123145\n",
      "Iteration 212, loss = 0.44813315\n",
      "Iteration 786, loss = 0.30160010\n",
      "Iteration 161, loss = 0.38150952\n",
      "Iteration 1846, loss = 0.20406001\n",
      "Iteration 2182, loss = 0.17007467\n",
      "Iteration 162, loss = 0.38100577\n",
      "Iteration 69, loss = 0.44936457\n",
      "Iteration 1011, loss = 0.27631055\n",
      "Iteration 163, loss = 0.38055754\n",
      "Iteration 903, loss = 0.31423352\n",
      "Iteration 70, loss = 0.44735297\n",
      "Iteration 787, loss = 0.30157681\n",
      "Iteration 1847, loss = 0.20383838\n",
      "Iteration 213, loss = 0.44763828\n",
      "Iteration 2183, loss = 0.16992682\n",
      "Iteration 164, loss = 0.38010330\n",
      "Iteration 165, loss = 0.37963773\n",
      "Iteration 71, loss = 0.44541727\n",
      "Iteration 1012, loss = 0.27603107\n",
      "Iteration 904, loss = 0.31413496\n",
      "Iteration 788, loss = 0.30131719\n",
      "Iteration 1848, loss = 0.20372437\n",
      "Iteration 72, loss = 0.44364851\n",
      "Iteration 73, loss = 0.44186264\n",
      "Iteration 166, loss = 0.37918554\n",
      "Iteration 1849, loss = 0.20363548\n",
      "Iteration 2184, loss = 0.16975522\n",
      "Iteration 167, loss = 0.37874350\n",
      "Iteration 789, loss = 0.30114676\n",
      "Iteration 168, loss = 0.37831705\n",
      "Iteration 74, loss = 0.44014976\n",
      "Iteration 214, loss = 0.44721241\n",
      "Iteration 790, loss = 0.30101442\n",
      "Iteration 1850, loss = 0.20371482\n",
      "Iteration 169, loss = 0.37788111\n",
      "Iteration 1013, loss = 0.27591659\n",
      "Iteration 75, loss = 0.43835241\n",
      "Iteration 905, loss = 0.31402286\n",
      "Iteration 215, loss = 0.44676849\n",
      "Iteration 170, loss = 0.37743282\n",
      "Iteration 791, loss = 0.30086594\n",
      "Iteration 171, loss = 0.37703417\n",
      "Iteration 2185, loss = 0.16974177\n",
      "Iteration 1014, loss = 0.27577199\n",
      "Iteration 1851, loss = 0.20347150\n",
      "Iteration 172, loss = 0.37660204\n",
      "Iteration 906, loss = 0.31389664\n",
      "Iteration 76, loss = 0.43666355\n",
      "Iteration 1852, loss = 0.20332166\n",
      "Iteration 173, loss = 0.37616923\n",
      "Iteration 174, loss = 0.37579138\n",
      "Iteration 77, loss = 0.43511570\n",
      "Iteration 175, loss = 0.37536912\n",
      "Iteration 792, loss = 0.30068967\n",
      "Iteration 1015, loss = 0.27538090\n",
      "Iteration 2186, loss = 0.16968163\n",
      "Iteration 78, loss = 0.43354370\n",
      "Iteration 1853, loss = 0.20321970\n",
      "Iteration 79, loss = 0.43190277\n",
      "Iteration 793, loss = 0.30053942\n",
      "Iteration 907, loss = 0.31384992\n",
      "Iteration 1016, loss = 0.27517827\n",
      "Iteration 794, loss = 0.30039049\n",
      "Iteration 176, loss = 0.37497471\n",
      "Iteration 80, loss = 0.43042708\n",
      "Iteration 1854, loss = 0.20310365\n",
      "Iteration 216, loss = 0.44633399\n",
      "Iteration 2187, loss = 0.16956123\n",
      "Iteration 1855, loss = 0.20299602\n",
      "Iteration 81, loss = 0.42899341\n",
      "Iteration 177, loss = 0.37455858\n",
      "Iteration 795, loss = 0.30023706\n",
      "Iteration 178, loss = 0.37420195\n",
      "Iteration 82, loss = 0.42750765\n",
      "Iteration 908, loss = 0.31369916\n",
      "Iteration 179, loss = 0.37374219\n",
      "Iteration 1856, loss = 0.20288069\n",
      "Iteration 83, loss = 0.42612642\n",
      "Iteration 796, loss = 0.30016958\n",
      "Iteration 180, loss = 0.37338723\n",
      "Iteration 1017, loss = 0.27498688\n",
      "Iteration 84, loss = 0.42478946\n",
      "Iteration 1857, loss = 0.20278659\n",
      "Iteration 909, loss = 0.31361125\n",
      "Iteration 85, loss = 0.42332930\n",
      "Iteration 2188, loss = 0.16939780\n",
      "Iteration 217, loss = 0.44599403\n",
      "Iteration 86, loss = 0.42214795\n",
      "Iteration 910, loss = 0.31348263\n",
      "Iteration 1858, loss = 0.20277021\n",
      "Iteration 797, loss = 0.29997824\n",
      "Iteration 181, loss = 0.37300457\n",
      "Iteration 2189, loss = 0.16931706\n",
      "Iteration 87, loss = 0.42076642\n",
      "Iteration 911, loss = 0.31341345\n",
      "Iteration 182, loss = 0.37265969\n",
      "Iteration 2190, loss = 0.16927609\n",
      "Iteration 1859, loss = 0.20272707\n",
      "Iteration 1018, loss = 0.27476905\n",
      "Iteration 183, loss = 0.37227931\n",
      "Iteration 798, loss = 0.29979525\n",
      "Iteration 88, loss = 0.41948677\n",
      "Iteration 218, loss = 0.44551249\n",
      "Iteration 912, loss = 0.31327183\n",
      "Iteration 2191, loss = 0.16930251\n",
      "Iteration 1860, loss = 0.20251454Iteration 184, loss = 0.37189611\n",
      "Iteration 185, loss = 0.37152500Iteration 799, loss = 0.29962499\n",
      "Iteration 89, loss = 0.41832453\n",
      "\n",
      "\n",
      "Iteration 186, loss = 0.37115950\n",
      "Iteration 2192, loss = 0.16910152\n",
      "Iteration 913, loss = 0.31318546\n",
      "Iteration 1019, loss = 0.27456206\n",
      "Iteration 187, loss = 0.37079572\n",
      "Iteration 90, loss = 0.41710801\n",
      "Iteration 800, loss = 0.29952393\n",
      "Iteration 1861, loss = 0.20245180\n",
      "Iteration 188, loss = 0.37045529\n",
      "Iteration 91, loss = 0.41594994\n",
      "Iteration 189, loss = 0.37013178\n",
      "Iteration 190, loss = 0.36973787\n",
      "Iteration 92, loss = 0.41476624\n",
      "Iteration 191, loss = 0.36940719\n",
      "Iteration 93, loss = 0.41369214\n",
      "Iteration 219, loss = 0.44510417\n",
      "Iteration 2193, loss = 0.16907451\n",
      "Iteration 801, loss = 0.29939831\n",
      "Iteration 94, loss = 0.41263504\n",
      "Iteration 192, loss = 0.36907238\n",
      "Iteration 95, loss = 0.41150483\n",
      "Iteration 1862, loss = 0.20243054\n",
      "Iteration 1863, loss = 0.20223929\n",
      "Iteration 914, loss = 0.31305305\n",
      "Iteration 1020, loss = 0.27447233\n",
      "Iteration 802, loss = 0.29916745\n",
      "Iteration 193, loss = 0.36872535\n",
      "Iteration 803, loss = 0.29901636\n",
      "Iteration 2194, loss = 0.16899652\n",
      "Iteration 194, loss = 0.36839064\n",
      "Iteration 1864, loss = 0.20217624\n",
      "Iteration 220, loss = 0.44471875\n",
      "Iteration 915, loss = 0.31296643\n",
      "Iteration 195, loss = 0.36806006\n",
      "Iteration 1865, loss = 0.20200734\n",
      "Iteration 196, loss = 0.36773242\n",
      "Iteration 1866, loss = 0.20192090\n",
      "Iteration 197, loss = 0.36740298\n",
      "Iteration 2195, loss = 0.16883652\n",
      "Iteration 1021, loss = 0.27419155\n",
      "Iteration 1867, loss = 0.20180343\n",
      "Iteration 198, loss = 0.36706420\n",
      "Iteration 96, loss = 0.41047942\n",
      "Iteration 199, loss = 0.36675524\n",
      "Iteration 804, loss = 0.29891166\n",
      "Iteration 916, loss = 0.31294806\n",
      "Iteration 1868, loss = 0.20173096\n",
      "Iteration 200, loss = 0.36646808\n",
      "Iteration 97, loss = 0.40947758\n",
      "Iteration 221, loss = 0.44436267\n",
      "Iteration 201, loss = 0.36612440\n",
      "Iteration 98, loss = 0.40846804\n",
      "Iteration 202, loss = 0.36582832\n",
      "Iteration 2196, loss = 0.16875643\n",
      "Iteration 1022, loss = 0.27403623\n",
      "Iteration 1869, loss = 0.20175261\n",
      "Iteration 805, loss = 0.29871533\n",
      "Iteration 99, loss = 0.40745605\n",
      "Iteration 203, loss = 0.36550967\n",
      "Iteration 204, loss = 0.36519400\n",
      "Iteration 100, loss = 0.40650122\n",
      "Iteration 917, loss = 0.31271451\n",
      "Iteration 205, loss = 0.36488038\n",
      "Iteration 1870, loss = 0.20155505\n",
      "Iteration 222, loss = 0.44390954\n",
      "Iteration 101, loss = 0.40552622\n",
      "Iteration 806, loss = 0.29856857\n",
      "Iteration 206, loss = 0.36461192\n",
      "Iteration 207, loss = 0.36432420\n",
      "Iteration 918, loss = 0.31263624\n",
      "Iteration 102, loss = 0.40469935\n",
      "Iteration 208, loss = 0.36400105\n",
      "Iteration 2197, loss = 0.16870735\n",
      "Iteration 1871, loss = 0.20137550\n",
      "Iteration 209, loss = 0.36375574\n",
      "Iteration 103, loss = 0.40374524\n",
      "Iteration 1023, loss = 0.27384090\n",
      "Iteration 210, loss = 0.36342024\n",
      "Iteration 104, loss = 0.40286205\n",
      "Iteration 211, loss = 0.36314783\n",
      "Iteration 1872, loss = 0.20131429\n",
      "Iteration 212, loss = 0.36288399\n",
      "Iteration 213, loss = 0.36258986\n",
      "Iteration 1873, loss = 0.20120016\n",
      "Iteration 105, loss = 0.40201297\n",
      "Iteration 214, loss = 0.36232316\n",
      "Iteration 1874, loss = 0.20109908\n",
      "Iteration 215, loss = 0.36206672\n",
      "Iteration 106, loss = 0.40117686\n",
      "Iteration 1024, loss = 0.27355699\n",
      "Iteration 223, loss = 0.44354094\n",
      "Iteration 807, loss = 0.29841056\n",
      "Iteration 216, loss = 0.36179710\n",
      "Iteration 107, loss = 0.40036333\n",
      "Iteration 1875, loss = 0.20104981\n",
      "Iteration 2198, loss = 0.16855774\n",
      "Iteration 919, loss = 0.31252389\n",
      "Iteration 217, loss = 0.36150499\n",
      "Iteration 108, loss = 0.39950292\n",
      "Iteration 808, loss = 0.29827033\n",
      "Iteration 1876, loss = 0.20088717\n",
      "Iteration 218, loss = 0.36124864\n",
      "Iteration 224, loss = 0.44313955\n",
      "Iteration 219, loss = 0.36098876\n",
      "Iteration 109, loss = 0.39874803\n",
      "Iteration 220, loss = 0.36071864\n",
      "Iteration 1025, loss = 0.27368206\n",
      "Iteration 920, loss = 0.31241750\n",
      "Iteration 221, loss = 0.36046776\n",
      "Iteration 222, loss = 0.36020888\n",
      "Iteration 1877, loss = 0.20079612\n",
      "Iteration 110, loss = 0.39797233\n",
      "Iteration 223, loss = 0.35997339\n",
      "Iteration 225, loss = 0.44268719\n",
      "Iteration 921, loss = 0.31237149\n",
      "Iteration 111, loss = 0.39720824\n",
      "Iteration 1878, loss = 0.20071822\n",
      "Iteration 224, loss = 0.35973761\n",
      "Iteration 225, loss = 0.35946683\n",
      "Iteration 226, loss = 0.35921156\n",
      "Iteration 2199, loss = 0.16847297\n",
      "Iteration 112, loss = 0.39646536\n",
      "Iteration 1026, loss = 0.27323310\n",
      "Iteration 809, loss = 0.29812008\n",
      "Iteration 1879, loss = 0.20064145\n",
      "Iteration 227, loss = 0.35897274\n",
      "Iteration 228, loss = 0.35874125\n",
      "Iteration 1880, loss = 0.20059229\n",
      "Iteration 229, loss = 0.35849718\n",
      "Iteration 230, loss = 0.35827830\n",
      "Iteration 1881, loss = 0.20040036\n",
      "Iteration 113, loss = 0.39574527\n",
      "Iteration 922, loss = 0.31222443\n",
      "Iteration 810, loss = 0.29805353\n",
      "Iteration 226, loss = 0.44235081\n",
      "Iteration 1882, loss = 0.20031848\n",
      "Iteration 114, loss = 0.39498701\n",
      "Iteration 1027, loss = 0.27321928\n",
      "Iteration 2200, loss = 0.16840198\n",
      "Iteration 115, loss = 0.39428219\n",
      "Iteration 116, loss = 0.39361345\n",
      "Iteration 231, loss = 0.35802244\n",
      "Iteration 232, loss = 0.35778822\n",
      "Iteration 1883, loss = 0.20024817\n",
      "Iteration 227, loss = 0.44191960\n",
      "Iteration 117, loss = 0.39291476\n",
      "Iteration 811, loss = 0.29780923\n",
      "Iteration 1028, loss = 0.27272520\n",
      "Iteration 118, loss = 0.39225411\n",
      "Iteration 2201, loss = 0.16832215\n",
      "Iteration 923, loss = 0.31210488\n",
      "Iteration 1884, loss = 0.20010043\n",
      "Iteration 233, loss = 0.35755987\n",
      "Iteration 119, loss = 0.39158528\n",
      "Iteration 228, loss = 0.44161289\n",
      "Iteration 1885, loss = 0.20001482\n",
      "Iteration 812, loss = 0.29779159\n",
      "Iteration 234, loss = 0.35733027\n",
      "Iteration 2202, loss = 0.16826193\n",
      "Iteration 924, loss = 0.31199229\n",
      "Iteration 1886, loss = 0.19986836\n",
      "Iteration 235, loss = 0.35711662\n",
      "Iteration 120, loss = 0.39097069\n",
      "Iteration 1029, loss = 0.27256186\n",
      "Iteration 813, loss = 0.29750768\n",
      "Iteration 236, loss = 0.35688295\n",
      "Iteration 2203, loss = 0.16811636\n",
      "Iteration 121, loss = 0.39031450\n",
      "Iteration 237, loss = 0.35666007\n",
      "Iteration 925, loss = 0.31185558\n",
      "Iteration 238, loss = 0.35641489\n",
      "Iteration 814, loss = 0.29744474\n",
      "Iteration 122, loss = 0.38975212\n",
      "Iteration 229, loss = 0.44129250\n",
      "Iteration 1030, loss = 0.27248466\n",
      "Iteration 239, loss = 0.35618843\n",
      "Iteration 815, loss = 0.29726623\n",
      "Iteration 123, loss = 0.38909491\n",
      "Iteration 1887, loss = 0.19979104\n",
      "Iteration 124, loss = 0.38851559\n",
      "Iteration 240, loss = 0.35597548\n",
      "Iteration 125, loss = 0.38794562\n",
      "Iteration 2204, loss = 0.16807180\n",
      "Iteration 1031, loss = 0.27210957\n",
      "Iteration 1888, loss = 0.19971473\n",
      "Iteration 926, loss = 0.31174622\n",
      "Iteration 1889, loss = 0.19976174\n",
      "Iteration 241, loss = 0.35576705\n",
      "Iteration 126, loss = 0.38735286\n",
      "Iteration 242, loss = 0.35558005\n",
      "Iteration 816, loss = 0.29714011\n",
      "Iteration 2205, loss = 0.16809499\n",
      "Iteration 1890, loss = 0.19956164\n",
      "Iteration 230, loss = 0.44079700\n",
      "Iteration 243, loss = 0.35533282\n",
      "Iteration 127, loss = 0.38680288\n",
      "Iteration 1891, loss = 0.19940303\n",
      "Iteration 244, loss = 0.35511235\n",
      "Iteration 1032, loss = 0.27208766\n",
      "Iteration 927, loss = 0.31171015\n",
      "Iteration 128, loss = 0.38624030\n",
      "Iteration 2206, loss = 0.16792867\n",
      "Iteration 245, loss = 0.35494429\n",
      "Iteration 1892, loss = 0.19930416\n",
      "Iteration 246, loss = 0.35468272\n",
      "Iteration 817, loss = 0.29689534\n",
      "Iteration 129, loss = 0.38567742\n",
      "Iteration 247, loss = 0.35449026\n",
      "Iteration 248, loss = 0.35428940\n",
      "Iteration 130, loss = 0.38513755\n",
      "Iteration 818, loss = 0.29677673\n",
      "Iteration 231, loss = 0.44038608Iteration 928, loss = 0.31155631\n",
      "Iteration 2207, loss = 0.16780942\n",
      "Iteration 1893, loss = 0.19919480\n",
      "Iteration 249, loss = 0.35407406\n",
      "\n",
      "Iteration 250, loss = 0.35388133\n",
      "Iteration 1894, loss = 0.19915282\n",
      "Iteration 251, loss = 0.35367413\n",
      "Iteration 131, loss = 0.38462914\n",
      "Iteration 252, loss = 0.35346907\n",
      "Iteration 1895, loss = 0.19901004\n",
      "Iteration 132, loss = 0.38408434\n",
      "Iteration 819, loss = 0.29668850\n",
      "Iteration 2208, loss = 0.16775400\n",
      "Iteration 1033, loss = 0.27172425\n",
      "Iteration 232, loss = 0.44004464\n",
      "Iteration 253, loss = 0.35331395\n",
      "Iteration 133, loss = 0.38358975\n",
      "Iteration 929, loss = 0.31146533\n",
      "Iteration 820, loss = 0.29646051\n",
      "Iteration 254, loss = 0.35308434\n",
      "Iteration 1896, loss = 0.19889513\n",
      "Iteration 134, loss = 0.38307993\n",
      "Iteration 135, loss = 0.38258551\n",
      "Iteration 255, loss = 0.35288858\n",
      "Iteration 821, loss = 0.29638584\n",
      "Iteration 1897, loss = 0.19880766\n",
      "Iteration 256, loss = 0.35269678\n",
      "Iteration 136, loss = 0.38211660\n",
      "Iteration 257, loss = 0.35249571\n",
      "Iteration 137, loss = 0.38164746\n",
      "Iteration 258, loss = 0.35231162\n",
      "Iteration 233, loss = 0.43966901\n",
      "Iteration 138, loss = 0.38116019\n",
      "Iteration 930, loss = 0.31132764\n",
      "Iteration 259, loss = 0.35212232\n",
      "Iteration 822, loss = 0.29617967\n",
      "Iteration 2209, loss = 0.16774156\n",
      "Iteration 1034, loss = 0.27162447\n",
      "Iteration 260, loss = 0.35195037\n",
      "Iteration 139, loss = 0.38066846\n",
      "Iteration 1898, loss = 0.19867037\n",
      "Iteration 823, loss = 0.29604504\n",
      "Iteration 261, loss = 0.35175083\n",
      "Iteration 2210, loss = 0.16758461\n",
      "Iteration 262, loss = 0.35156607\n",
      "Iteration 1899, loss = 0.19862755\n",
      "Iteration 234, loss = 0.43929824\n",
      "Iteration 140, loss = 0.38023756\n",
      "Iteration 824, loss = 0.29587154\n",
      "Iteration 1035, loss = 0.27137824\n",
      "Iteration 1900, loss = 0.19849850\n",
      "Iteration 931, loss = 0.31126494\n",
      "Iteration 1901, loss = 0.19845674\n",
      "Iteration 2211, loss = 0.16746946\n",
      "Iteration 141, loss = 0.37979631\n",
      "Iteration 1036, loss = 0.27115594\n",
      "Iteration 1902, loss = 0.19833549\n",
      "Iteration 263, loss = 0.35138362\n",
      "Iteration 932, loss = 0.31110854\n",
      "Iteration 235, loss = 0.43897835\n",
      "Iteration 1903, loss = 0.19823387\n",
      "Iteration 264, loss = 0.35122358\n",
      "Iteration 265, loss = 0.35103251\n",
      "Iteration 825, loss = 0.29570067\n",
      "Iteration 1904, loss = 0.19818461\n",
      "Iteration 142, loss = 0.37933081\n",
      "Iteration 1037, loss = 0.27087489\n",
      "Iteration 933, loss = 0.31099278\n",
      "Iteration 266, loss = 0.35084823\n",
      "Iteration 2212, loss = 0.16741434\n",
      "Iteration 267, loss = 0.35067131\n",
      "Iteration 1905, loss = 0.19815787\n",
      "Iteration 1906, loss = 0.19789608\n",
      "Iteration 143, loss = 0.37890249\n",
      "Iteration 236, loss = 0.43858204\n",
      "Iteration 268, loss = 0.35048380\n",
      "Iteration 1038, loss = 0.27069094\n",
      "Iteration 144, loss = 0.37848651\n",
      "Iteration 1907, loss = 0.19779147\n",
      "Iteration 269, loss = 0.35030978\n",
      "Iteration 934, loss = 0.31087611\n",
      "Iteration 145, loss = 0.37803242\n",
      "Iteration 270, loss = 0.35013865\n",
      "Iteration 826, loss = 0.29558228\n",
      "Iteration 271, loss = 0.34996387\n",
      "Iteration 272, loss = 0.34979460\n",
      "Iteration 2213, loss = 0.16730780\n",
      "Iteration 827, loss = 0.29550650\n",
      "Iteration 146, loss = 0.37764947\n",
      "Iteration 273, loss = 0.34962037\n",
      "Iteration 147, loss = 0.37720282\n",
      "Iteration 274, loss = 0.34947601\n",
      "Iteration 935, loss = 0.31080264\n",
      "Iteration 828, loss = 0.29521803\n",
      "Iteration 1908, loss = 0.19769840\n",
      "Iteration 275, loss = 0.34929023\n",
      "Iteration 148, loss = 0.37681264\n",
      "Iteration 829, loss = 0.29522020\n",
      "Iteration 2214, loss = 0.16722674\n",
      "Iteration 1039, loss = 0.27078729\n",
      "Iteration 276, loss = 0.34912887\n",
      "Iteration 1909, loss = 0.19769750\n",
      "Iteration 237, loss = 0.43817226\n",
      "Iteration 830, loss = 0.29507086\n",
      "Iteration 1910, loss = 0.19751938\n",
      "Iteration 149, loss = 0.37640367\n",
      "Iteration 277, loss = 0.34894276\n",
      "Iteration 936, loss = 0.31069420\n",
      "Iteration 831, loss = 0.29479385\n",
      "Iteration 150, loss = 0.37602609\n",
      "Iteration 2215, loss = 0.16720360\n",
      "Iteration 151, loss = 0.37559573\n",
      "Iteration 278, loss = 0.34879363\n",
      "Iteration 832, loss = 0.29462978\n",
      "Iteration 152, loss = 0.37523119\n",
      "Iteration 1040, loss = 0.27031680\n",
      "Iteration 279, loss = 0.34861737\n",
      "Iteration 153, loss = 0.37486049\n",
      "Iteration 280, loss = 0.34845781\n",
      "Iteration 833, loss = 0.29449328\n",
      "Iteration 238, loss = 0.43787434\n",
      "Iteration 154, loss = 0.37444747\n",
      "Iteration 937, loss = 0.31056988\n",
      "Iteration 281, loss = 0.34831052\n",
      "Iteration 1911, loss = 0.19743268\n",
      "Iteration 834, loss = 0.29433168\n",
      "Iteration 155, loss = 0.37409716\n",
      "Iteration 1912, loss = 0.19741842\n",
      "Iteration 938, loss = 0.31047613\n",
      "Iteration 156, loss = 0.37373764\n",
      "Iteration 282, loss = 0.34813243\n",
      "Iteration 239, loss = 0.43752368\n",
      "Iteration 1913, loss = 0.19723942\n",
      "Iteration 2216, loss = 0.16712946\n",
      "Iteration 835, loss = 0.29417178Iteration 283, loss = 0.34797578\n",
      "Iteration 1041, loss = 0.27010742\n",
      "Iteration 157, loss = 0.37337836\n",
      "Iteration 1914, loss = 0.19710553\n",
      "Iteration 158, loss = 0.37303853\n",
      "Iteration 284, loss = 0.34785089\n",
      "Iteration 159, loss = 0.37270087\n",
      "Iteration 2217, loss = 0.16705938\n",
      "\n",
      "Iteration 1915, loss = 0.19703773\n",
      "Iteration 1042, loss = 0.27000065\n",
      "Iteration 285, loss = 0.34764950\n",
      "Iteration 160, loss = 0.37232860\n",
      "Iteration 286, loss = 0.34751715\n",
      "Iteration 161, loss = 0.37197407\n",
      "Iteration 162, loss = 0.37166559\n",
      "Iteration 836, loss = 0.29402946\n",
      "Iteration 1916, loss = 0.19695545\n",
      "Iteration 287, loss = 0.34735326\n",
      "Iteration 2218, loss = 0.16689157\n",
      "Iteration 240, loss = 0.43715334\n",
      "Iteration 163, loss = 0.37129976\n",
      "Iteration 164, loss = 0.37099359\n",
      "Iteration 288, loss = 0.34718379\n",
      "Iteration 1917, loss = 0.19682546\n",
      "Iteration 1043, loss = 0.26971970\n",
      "Iteration 837, loss = 0.29389477\n",
      "Iteration 289, loss = 0.34708846\n",
      "Iteration 1918, loss = 0.19676322\n",
      "Iteration 165, loss = 0.37066840\n",
      "Iteration 290, loss = 0.34687990\n",
      "Iteration 838, loss = 0.29371748\n",
      "Iteration 939, loss = 0.31040261\n",
      "Iteration 241, loss = 0.43686682\n",
      "Iteration 2219, loss = 0.16683754\n",
      "Iteration 1919, loss = 0.19670141\n",
      "Iteration 291, loss = 0.34673813\n",
      "Iteration 166, loss = 0.37034951\n",
      "Iteration 292, loss = 0.34659251\n",
      "Iteration 1920, loss = 0.19655666\n",
      "Iteration 293, loss = 0.34642347\n",
      "Iteration 1921, loss = 0.19644518\n",
      "Iteration 167, loss = 0.37001874\n",
      "Iteration 2220, loss = 0.16678419\n",
      "Iteration 1044, loss = 0.26941794\n",
      "Iteration 168, loss = 0.36973155\n",
      "Iteration 294, loss = 0.34629580\n",
      "Iteration 940, loss = 0.31023563\n",
      "Iteration 839, loss = 0.29358412\n",
      "Iteration 295, loss = 0.34613209\n",
      "Iteration 2221, loss = 0.16671077\n",
      "Iteration 169, loss = 0.36941660\n",
      "Iteration 296, loss = 0.34599855\n",
      "Iteration 1922, loss = 0.19628703\n",
      "Iteration 297, loss = 0.34585893\n",
      "Iteration 2222, loss = 0.16664145\n",
      "Iteration 170, loss = 0.36910730\n",
      "Iteration 1923, loss = 0.19621710\n",
      "Iteration 298, loss = 0.34570300\n",
      "Iteration 171, loss = 0.36882291\n",
      "Iteration 242, loss = 0.43646739\n",
      "Iteration 299, loss = 0.34557546\n",
      "Iteration 172, loss = 0.36851259\n",
      "Iteration 941, loss = 0.31013971Iteration 840, loss = 0.29339790\n",
      "\n",
      "Iteration 300, loss = 0.34541123\n",
      "Iteration 1045, loss = 0.26919793\n",
      "Iteration 301, loss = 0.34525420\n",
      "Iteration 841, loss = 0.29325805\n",
      "Iteration 173, loss = 0.36820091\n",
      "Iteration 1924, loss = 0.19612980Iteration 942, loss = 0.31002749\n",
      "\n",
      "Iteration 2223, loss = 0.16652405\n",
      "Iteration 174, loss = 0.36791919\n",
      "Iteration 842, loss = 0.29308444\n",
      "Iteration 243, loss = 0.43612160\n",
      "Iteration 302, loss = 0.34513050\n",
      "Iteration 943, loss = 0.30998638\n",
      "Iteration 1925, loss = 0.19601748\n",
      "Iteration 1046, loss = 0.26897310\n",
      "Iteration 303, loss = 0.34498673\n",
      "Iteration 843, loss = 0.29298302\n",
      "Iteration 944, loss = 0.30982207\n",
      "Iteration 2224, loss = 0.16649231\n",
      "Iteration 1926, loss = 0.19595641\n",
      "Iteration 175, loss = 0.36762030\n",
      "Iteration 176, loss = 0.36736405\n",
      "Iteration 244, loss = 0.43575444\n",
      "Iteration 2225, loss = 0.16635172\n",
      "Iteration 945, loss = 0.30973339\n",
      "Iteration 1927, loss = 0.19583816\n",
      "Iteration 177, loss = 0.36706550\n",
      "Iteration 844, loss = 0.29278854\n",
      "Iteration 2226, loss = 0.16627163\n",
      "Iteration 845, loss = 0.29263492\n",
      "Iteration 1928, loss = 0.19570220\n",
      "Iteration 304, loss = 0.34483599\n",
      "Iteration 1047, loss = 0.26889130\n",
      "Iteration 305, loss = 0.34470276\n",
      "Iteration 1929, loss = 0.19564632\n",
      "Iteration 245, loss = 0.43542225\n",
      "Iteration 846, loss = 0.29251342\n",
      "Iteration 178, loss = 0.36678478\n",
      "Iteration 306, loss = 0.34456191\n",
      "Iteration 2227, loss = 0.16623125\n",
      "Iteration 179, loss = 0.36650381\n",
      "Iteration 1930, loss = 0.19563029\n",
      "Iteration 307, loss = 0.34441399\n",
      "Iteration 2228, loss = 0.16608389\n",
      "Iteration 180, loss = 0.36624195\n",
      "Iteration 308, loss = 0.34428890\n",
      "Iteration 181, loss = 0.36595469\n",
      "Iteration 309, loss = 0.34415601\n",
      "Iteration 246, loss = 0.43513373\n",
      "Iteration 847, loss = 0.29244042\n",
      "Iteration 310, loss = 0.34402636\n",
      "Iteration 1048, loss = 0.26864545\n",
      "Iteration 1931, loss = 0.19546238\n",
      "Iteration 182, loss = 0.36568800\n",
      "Iteration 183, loss = 0.36543906\n",
      "Iteration 311, loss = 0.34389308\n",
      "Iteration 946, loss = 0.30967467\n",
      "Iteration 312, loss = 0.34374345\n",
      "Iteration 184, loss = 0.36517533\n",
      "Iteration 2229, loss = 0.16605234\n",
      "Iteration 313, loss = 0.34361569\n",
      "Iteration 314, loss = 0.34349819\n",
      "Iteration 315, loss = 0.34336599\n",
      "Iteration 1932, loss = 0.19535657\n",
      "Iteration 1049, loss = 0.26837452\n",
      "Iteration 247, loss = 0.43476595\n",
      "Iteration 848, loss = 0.29220464\n",
      "Iteration 185, loss = 0.36490173\n",
      "Iteration 2230, loss = 0.16592445\n",
      "Iteration 947, loss = 0.30951966\n",
      "Iteration 1933, loss = 0.19523011\n",
      "Iteration 316, loss = 0.34320277\n",
      "Iteration 1934, loss = 0.19515764\n",
      "Iteration 186, loss = 0.36463933\n",
      "Iteration 2231, loss = 0.16583526\n",
      "Iteration 1935, loss = 0.19503779\n",
      "Iteration 317, loss = 0.34311070\n",
      "Iteration 1050, loss = 0.26822866\n",
      "Iteration 187, loss = 0.36438588\n",
      "Iteration 318, loss = 0.34295278\n",
      "Iteration 948, loss = 0.30946239\n",
      "Iteration 319, loss = 0.34281650\n",
      "Iteration 320, loss = 0.34268800\n",
      "Iteration 321, loss = 0.34256359\n",
      "Iteration 949, loss = 0.30945434\n",
      "Iteration 1936, loss = 0.19497627\n",
      "Iteration 849, loss = 0.29203116\n",
      "Iteration 188, loss = 0.36417030\n",
      "Iteration 1051, loss = 0.26794903\n",
      "Iteration 322, loss = 0.34242178\n",
      "Iteration 189, loss = 0.36388469\n",
      "Iteration 248, loss = 0.43460181\n",
      "Iteration 323, loss = 0.34229912\n",
      "Iteration 324, loss = 0.34218292\n",
      "Iteration 2232, loss = 0.16577135\n",
      "Iteration 850, loss = 0.29194937\n",
      "Iteration 1937, loss = 0.19485529\n",
      "Iteration 190, loss = 0.36362639\n",
      "Iteration 325, loss = 0.34203940\n",
      "Iteration 326, loss = 0.34191912Iteration 950, loss = 0.30921273\n",
      "\n",
      "Iteration 851, loss = 0.29172920\n",
      "Iteration 191, loss = 0.36339286\n",
      "Iteration 327, loss = 0.34177441\n",
      "Iteration 1938, loss = 0.19481568\n",
      "Iteration 852, loss = 0.29168754\n",
      "Iteration 328, loss = 0.34165750\n",
      "Iteration 192, loss = 0.36313915\n",
      "Iteration 1939, loss = 0.19472873\n",
      "Iteration 951, loss = 0.30908341\n",
      "Iteration 329, loss = 0.34153183\n",
      "Iteration 853, loss = 0.29145118\n",
      "Iteration 330, loss = 0.34139539\n",
      "Iteration 1052, loss = 0.26781358\n",
      "Iteration 331, loss = 0.34128607\n",
      "Iteration 249, loss = 0.43408234\n",
      "Iteration 2233, loss = 0.16565209\n",
      "Iteration 332, loss = 0.34118428\n",
      "Iteration 193, loss = 0.36291719\n",
      "Iteration 333, loss = 0.34103190\n",
      "Iteration 334, loss = 0.34091969\n",
      "Iteration 854, loss = 0.29127812\n",
      "Iteration 194, loss = 0.36267035\n",
      "Iteration 952, loss = 0.30907970\n",
      "Iteration 335, loss = 0.34080150\n",
      "Iteration 336, loss = 0.34067360\n",
      "Iteration 855, loss = 0.29112644\n",
      "Iteration 1940, loss = 0.19456049\n",
      "Iteration 337, loss = 0.34054953\n",
      "Iteration 195, loss = 0.36244943\n",
      "Iteration 338, loss = 0.34043777\n",
      "Iteration 1941, loss = 0.19445885\n",
      "Iteration 339, loss = 0.34030318\n",
      "Iteration 2234, loss = 0.16558118\n",
      "Iteration 856, loss = 0.29105955\n",
      "Iteration 953, loss = 0.30891967\n",
      "Iteration 196, loss = 0.36220412\n",
      "Iteration 197, loss = 0.36196379\n",
      "Iteration 1053, loss = 0.26762066\n",
      "Iteration 340, loss = 0.34019040\n",
      "Iteration 857, loss = 0.29091945\n",
      "Iteration 198, loss = 0.36173796\n",
      "Iteration 1942, loss = 0.19439677\n",
      "Iteration 341, loss = 0.34007096\n",
      "Iteration 342, loss = 0.33994124\n",
      "Iteration 954, loss = 0.30877826\n",
      "Iteration 858, loss = 0.29080166\n",
      "Iteration 250, loss = 0.43376862\n",
      "Iteration 343, loss = 0.33985433\n",
      "Iteration 199, loss = 0.36152530\n",
      "Iteration 2235, loss = 0.16567426\n",
      "Iteration 859, loss = 0.29057373\n",
      "Iteration 200, loss = 0.36130743\n",
      "Iteration 955, loss = 0.30869444\n",
      "Iteration 344, loss = 0.33972045\n",
      "Iteration 201, loss = 0.36106940\n",
      "Iteration 1943, loss = 0.19439620\n",
      "Iteration 860, loss = 0.29038683\n",
      "Iteration 345, loss = 0.33957831\n",
      "Iteration 202, loss = 0.36085459\n",
      "Iteration 1944, loss = 0.19427090\n",
      "Iteration 203, loss = 0.36063912\n",
      "Iteration 956, loss = 0.30857735\n",
      "Iteration 346, loss = 0.33948403\n",
      "Iteration 204, loss = 0.36042601\n",
      "Iteration 1945, loss = 0.19405104\n",
      "Iteration 205, loss = 0.36021689\n",
      "Iteration 861, loss = 0.29022039\n",
      "Iteration 251, loss = 0.43342776\n",
      "Iteration 1054, loss = 0.26738861\n",
      "Iteration 1946, loss = 0.19400396\n",
      "Iteration 347, loss = 0.33936130\n",
      "Iteration 957, loss = 0.30845949\n",
      "Iteration 206, loss = 0.36001290\n",
      "Iteration 2236, loss = 0.16539620\n",
      "Iteration 862, loss = 0.29017607\n",
      "Iteration 348, loss = 0.33924620\n",
      "Iteration 207, loss = 0.35979713\n",
      "Iteration 349, loss = 0.33912139\n",
      "Iteration 1947, loss = 0.19395772\n",
      "Iteration 2237, loss = 0.16534378\n",
      "Iteration 863, loss = 0.28994005\n",
      "Iteration 958, loss = 0.30837873\n",
      "Iteration 350, loss = 0.33900417\n",
      "Iteration 351, loss = 0.33889434\n",
      "Iteration 208, loss = 0.35957737\n",
      "Iteration 352, loss = 0.33877818\n",
      "Iteration 209, loss = 0.35936808\n",
      "Iteration 864, loss = 0.28982203\n",
      "Iteration 1055, loss = 0.26731243\n",
      "Iteration 1948, loss = 0.19377410\n",
      "Iteration 252, loss = 0.43308851\n",
      "Iteration 2238, loss = 0.16527624\n",
      "Iteration 210, loss = 0.35917948\n",
      "Iteration 353, loss = 0.33866324\n",
      "Iteration 354, loss = 0.33856027\n",
      "Iteration 211, loss = 0.35896741\n",
      "Iteration 1949, loss = 0.19372786\n",
      "Iteration 355, loss = 0.33843213\n",
      "Iteration 959, loss = 0.30827410\n",
      "Iteration 865, loss = 0.28965740\n",
      "Iteration 356, loss = 0.33832799\n",
      "Iteration 212, loss = 0.35878544\n",
      "Iteration 1056, loss = 0.26698926\n",
      "Iteration 357, loss = 0.33821788\n",
      "Iteration 866, loss = 0.28949493\n",
      "Iteration 253, loss = 0.43276591\n",
      "Iteration 960, loss = 0.30818460\n",
      "Iteration 1950, loss = 0.19358229\n",
      "Iteration 358, loss = 0.33811490\n",
      "Iteration 213, loss = 0.35857749\n",
      "Iteration 1951, loss = 0.19354888\n",
      "Iteration 359, loss = 0.33801355\n",
      "Iteration 867, loss = 0.28943459\n",
      "Iteration 360, loss = 0.33788096\n",
      "Iteration 214, loss = 0.35837277\n",
      "Iteration 961, loss = 0.30801774\n",
      "Iteration 361, loss = 0.33779415\n",
      "Iteration 2239, loss = 0.16515636\n",
      "Iteration 362, loss = 0.33768669\n",
      "Iteration 1952, loss = 0.19340759\n",
      "Iteration 215, loss = 0.35816263\n",
      "Iteration 363, loss = 0.33755484\n",
      "Iteration 216, loss = 0.35797132\n",
      "Iteration 1953, loss = 0.19336831\n",
      "Iteration 364, loss = 0.33746338\n",
      "Iteration 868, loss = 0.28941033\n",
      "Iteration 254, loss = 0.43245877\n",
      "Iteration 1057, loss = 0.26683465\n",
      "Iteration 1954, loss = 0.19321924\n",
      "Iteration 869, loss = 0.28905479\n",
      "Iteration 365, loss = 0.33733670Iteration 2240, loss = 0.16511982\n",
      "Iteration 962, loss = 0.30794651\n",
      "Iteration 1955, loss = 0.19307026\n",
      "\n",
      "Iteration 217, loss = 0.35778150\n",
      "Iteration 366, loss = 0.33722822\n",
      "Iteration 218, loss = 0.35758520\n",
      "Iteration 870, loss = 0.28887804\n",
      "Iteration 367, loss = 0.33714084\n",
      "Iteration 1956, loss = 0.19304980\n",
      "Iteration 2241, loss = 0.16501964\n",
      "Iteration 219, loss = 0.35739418\n",
      "Iteration 1058, loss = 0.26652189\n",
      "Iteration 255, loss = 0.43216705\n",
      "Iteration 1957, loss = 0.19289021\n",
      "Iteration 220, loss = 0.35721470\n",
      "Iteration 871, loss = 0.28872936\n",
      "Iteration 1958, loss = 0.19282505\n",
      "Iteration 221, loss = 0.35703830\n",
      "Iteration 2242, loss = 0.16493226\n",
      "Iteration 963, loss = 0.30783356\n",
      "Iteration 368, loss = 0.33701796\n",
      "Iteration 222, loss = 0.35685054\n",
      "Iteration 872, loss = 0.28863456\n",
      "Iteration 1959, loss = 0.19276356\n",
      "Iteration 369, loss = 0.33691414\n",
      "Iteration 2243, loss = 0.16490986\n",
      "Iteration 223, loss = 0.35667014\n",
      "Iteration 370, loss = 0.33681658\n",
      "Iteration 256, loss = 0.43183414\n",
      "Iteration 1960, loss = 0.19264111\n",
      "Iteration 964, loss = 0.30768248\n",
      "Iteration 371, loss = 0.33670422\n",
      "Iteration 873, loss = 0.28841654\n",
      "Iteration 372, loss = 0.33663020\n",
      "Iteration 224, loss = 0.35647193\n",
      "Iteration 1961, loss = 0.19252888\n",
      "Iteration 373, loss = 0.33651391\n",
      "Iteration 2244, loss = 0.16480242\n",
      "Iteration 374, loss = 0.33637396\n",
      "Iteration 225, loss = 0.35629944\n",
      "Iteration 375, loss = 0.33629449\n",
      "Iteration 1962, loss = 0.19242644\n",
      "Iteration 376, loss = 0.33617148\n",
      "Iteration 1059, loss = 0.26637910\n",
      "Iteration 874, loss = 0.28839336\n",
      "Iteration 965, loss = 0.30758859\n",
      "Iteration 226, loss = 0.35616042\n",
      "Iteration 377, loss = 0.33607794\n",
      "Iteration 257, loss = 0.43150021\n",
      "Iteration 227, loss = 0.35594703\n",
      "Iteration 2245, loss = 0.16469905\n",
      "Iteration 1963, loss = 0.19231794\n",
      "Iteration 378, loss = 0.33597639\n",
      "Iteration 966, loss = 0.30747022\n",
      "Iteration 228, loss = 0.35577897\n",
      "Iteration 1964, loss = 0.19223942\n",
      "Iteration 875, loss = 0.28828206\n",
      "Iteration 1060, loss = 0.26618280\n",
      "Iteration 229, loss = 0.35559913\n",
      "Iteration 379, loss = 0.33586496\n",
      "Iteration 2246, loss = 0.16462972\n",
      "Iteration 230, loss = 0.35543475\n",
      "Iteration 380, loss = 0.33575322\n",
      "Iteration 1965, loss = 0.19210836\n",
      "Iteration 231, loss = 0.35524892\n",
      "Iteration 876, loss = 0.28799754\n",
      "Iteration 258, loss = 0.43116905\n",
      "Iteration 381, loss = 0.33570864\n",
      "Iteration 232, loss = 0.35509668\n",
      "Iteration 382, loss = 0.33555651\n",
      "Iteration 383, loss = 0.33545861\n",
      "Iteration 967, loss = 0.30741651\n",
      "Iteration 233, loss = 0.35490979\n",
      "Iteration 1966, loss = 0.19204703\n",
      "Iteration 384, loss = 0.33536452\n",
      "Iteration 234, loss = 0.35475623\n",
      "Iteration 1061, loss = 0.26592743\n",
      "Iteration 385, loss = 0.33526132\n",
      "Iteration 877, loss = 0.28791262\n",
      "Iteration 235, loss = 0.35455948\n",
      "Iteration 968, loss = 0.30736285\n",
      "Iteration 1967, loss = 0.19199285\n",
      "Iteration 236, loss = 0.35442777\n",
      "Iteration 386, loss = 0.33515562\n",
      "Iteration 237, loss = 0.35424832\n",
      "Iteration 1968, loss = 0.19183950\n",
      "Iteration 1062, loss = 0.26573159\n",
      "Iteration 2247, loss = 0.16461760\n",
      "Iteration 387, loss = 0.33505397\n",
      "Iteration 1969, loss = 0.19180665\n",
      "Iteration 969, loss = 0.30728406\n",
      "Iteration 238, loss = 0.35408472\n",
      "Iteration 259, loss = 0.43087305\n",
      "Iteration 388, loss = 0.33495658\n",
      "Iteration 1970, loss = 0.19163297\n",
      "Iteration 239, loss = 0.35395113\n",
      "Iteration 1971, loss = 0.19153735\n",
      "Iteration 1063, loss = 0.26562833\n",
      "Iteration 389, loss = 0.33486093\n",
      "Iteration 1972, loss = 0.19159415\n",
      "Iteration 878, loss = 0.28768495\n",
      "Iteration 970, loss = 0.30704497\n",
      "Iteration 240, loss = 0.35375866\n",
      "Iteration 390, loss = 0.33476977\n",
      "Iteration 1973, loss = 0.19145750\n",
      "Iteration 2248, loss = 0.16464270\n",
      "Iteration 241, loss = 0.35362040\n",
      "Iteration 391, loss = 0.33465736\n",
      "Iteration 1974, loss = 0.19125468\n",
      "Iteration 879, loss = 0.28751441\n",
      "Iteration 260, loss = 0.43054080\n",
      "Iteration 242, loss = 0.35344091\n",
      "Iteration 971, loss = 0.30696920\n",
      "Iteration 392, loss = 0.33455629\n",
      "Iteration 880, loss = 0.28747652\n",
      "Iteration 1064, loss = 0.26526193\n",
      "Iteration 1975, loss = 0.19113806\n",
      "Iteration 243, loss = 0.35328557\n",
      "Iteration 393, loss = 0.33446846\n",
      "Iteration 244, loss = 0.35311875\n",
      "Iteration 1976, loss = 0.19106543\n",
      "Iteration 972, loss = 0.30686509\n",
      "Iteration 1977, loss = 0.19106693\n",
      "Iteration 394, loss = 0.33438042\n",
      "Iteration 261, loss = 0.43023115\n",
      "Iteration 2249, loss = 0.16436222\n",
      "Iteration 245, loss = 0.35297617\n",
      "Iteration 1978, loss = 0.19097810\n",
      "Iteration 881, loss = 0.28722952\n",
      "Iteration 395, loss = 0.33426916\n",
      "Iteration 973, loss = 0.30676948Iteration 1979, loss = 0.19083351\n",
      "\n",
      "Iteration 882, loss = 0.28708753\n",
      "Iteration 396, loss = 0.33416939\n",
      "Iteration 246, loss = 0.35282233\n",
      "Iteration 397, loss = 0.33408067\n",
      "Iteration 2250, loss = 0.16429600\n",
      "Iteration 1980, loss = 0.19073012\n",
      "Iteration 883, loss = 0.28708424\n",
      "Iteration 398, loss = 0.33398516\n",
      "Iteration 1065, loss = 0.26509534\n",
      "Iteration 1981, loss = 0.19059703\n",
      "Iteration 247, loss = 0.35267862\n",
      "Iteration 262, loss = 0.42993814\n",
      "Iteration 884, loss = 0.28681921\n",
      "Iteration 2251, loss = 0.16422922\n",
      "Iteration 399, loss = 0.33391363\n",
      "Iteration 248, loss = 0.35253645\n",
      "Iteration 400, loss = 0.33378810\n",
      "Iteration 1982, loss = 0.19048579\n",
      "Iteration 401, loss = 0.33369937\n",
      "Iteration 263, loss = 0.42962360\n",
      "Iteration 402, loss = 0.33360374\n",
      "Iteration 974, loss = 0.30665100\n",
      "Iteration 1066, loss = 0.26503494\n",
      "Iteration 885, loss = 0.28664201\n",
      "Iteration 403, loss = 0.33354682\n",
      "Iteration 249, loss = 0.35237478\n",
      "Iteration 1983, loss = 0.19040707\n",
      "Iteration 2252, loss = 0.16415788\n",
      "Iteration 264, loss = 0.42937605\n",
      "Iteration 404, loss = 0.33342077\n",
      "Iteration 250, loss = 0.35221298Iteration 975, loss = 0.30652413\n",
      "\n",
      "Iteration 886, loss = 0.28649135\n",
      "Iteration 405, loss = 0.33331953\n",
      "Iteration 1984, loss = 0.19038054\n",
      "Iteration 1067, loss = 0.26464022\n",
      "Iteration 406, loss = 0.33322856\n",
      "Iteration 251, loss = 0.35207234\n",
      "Iteration 407, loss = 0.33314888\n",
      "Iteration 2253, loss = 0.16403710\n",
      "Iteration 408, loss = 0.33307001\n",
      "Iteration 1985, loss = 0.19025461\n",
      "Iteration 976, loss = 0.30642575\n",
      "Iteration 887, loss = 0.28639229\n",
      "Iteration 265, loss = 0.42905318\n",
      "Iteration 409, loss = 0.33296198\n",
      "Iteration 252, loss = 0.35191599\n",
      "Iteration 2254, loss = 0.16409540\n",
      "Iteration 1068, loss = 0.26446700\n",
      "Iteration 253, loss = 0.35179899\n",
      "Iteration 888, loss = 0.28632803\n",
      "Iteration 410, loss = 0.33287113\n",
      "Iteration 266, loss = 0.42869407\n",
      "Iteration 2255, loss = 0.16392594\n",
      "Iteration 411, loss = 0.33278588\n",
      "Iteration 254, loss = 0.35164102\n",
      "Iteration 412, loss = 0.33269585\n",
      "Iteration 413, loss = 0.33261563\n",
      "Iteration 2256, loss = 0.16382025\n",
      "Iteration 414, loss = 0.33252095\n",
      "Iteration 977, loss = 0.30635722\n",
      "Iteration 255, loss = 0.35149523\n",
      "Iteration 415, loss = 0.33243853\n",
      "Iteration 889, loss = 0.28605653\n",
      "Iteration 416, loss = 0.33234519\n",
      "Iteration 1069, loss = 0.26441477\n",
      "Iteration 256, loss = 0.35134412\n",
      "Iteration 417, loss = 0.33226453\n",
      "Iteration 1986, loss = 0.19015522\n",
      "Iteration 267, loss = 0.42842148\n",
      "Iteration 418, loss = 0.33217442\n",
      "Iteration 2257, loss = 0.16374622\n",
      "Iteration 1987, loss = 0.19004149\n",
      "Iteration 419, loss = 0.33208083\n",
      "Iteration 890, loss = 0.28605634\n",
      "Iteration 978, loss = 0.30622107\n",
      "Iteration 1988, loss = 0.19001387\n",
      "Iteration 1070, loss = 0.26400518\n",
      "Iteration 420, loss = 0.33199054\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1989, loss = 0.18982191\n",
      "Iteration 257, loss = 0.35119506\n",
      "Iteration 268, loss = 0.42808776\n",
      "Iteration 2258, loss = 0.16367642Iteration 891, loss = 0.28574595\n",
      "\n",
      "Iteration 1990, loss = 0.18975242\n",
      "Iteration 979, loss = 0.30612954\n",
      "Iteration 258, loss = 0.35108251\n",
      "Iteration 892, loss = 0.28556966\n",
      "Iteration 1991, loss = 0.18966901\n",
      "Iteration 1, loss = 0.75784620\n",
      "Iteration 259, loss = 0.35091472\n",
      "Iteration 2, loss = 0.75529343\n",
      "Iteration 1071, loss = 0.26384842\n",
      "Iteration 980, loss = 0.30605242\n",
      "Iteration 3, loss = 0.75164965\n",
      "Iteration 1992, loss = 0.18951316\n",
      "Iteration 4, loss = 0.74704484\n",
      "Iteration 260, loss = 0.35078446\n",
      "Iteration 893, loss = 0.28550598\n",
      "Iteration 5, loss = 0.74212275\n",
      "Iteration 6, loss = 0.73706836\n",
      "Iteration 269, loss = 0.42778385\n",
      "Iteration 261, loss = 0.35063238\n",
      "Iteration 1993, loss = 0.18944361\n",
      "Iteration 7, loss = 0.73191734\n",
      "Iteration 2259, loss = 0.16369021\n",
      "Iteration 981, loss = 0.30597802\n",
      "Iteration 262, loss = 0.35051968\n",
      "Iteration 894, loss = 0.28554337\n",
      "Iteration 8, loss = 0.72671618\n",
      "Iteration 1994, loss = 0.18931175\n",
      "Iteration 9, loss = 0.72174742\n",
      "Iteration 10, loss = 0.71685382\n",
      "Iteration 263, loss = 0.35034862\n",
      "Iteration 11, loss = 0.71216356\n",
      "Iteration 12, loss = 0.70766513\n",
      "Iteration 1995, loss = 0.18926700\n",
      "Iteration 1072, loss = 0.26359151\n",
      "Iteration 13, loss = 0.70320731\n",
      "Iteration 14, loss = 0.69887841\n",
      "Iteration 895, loss = 0.28525946\n",
      "Iteration 1996, loss = 0.18922062\n",
      "Iteration 15, loss = 0.69497457\n",
      "Iteration 264, loss = 0.35023402\n",
      "Iteration 16, loss = 0.69115838\n",
      "Iteration 270, loss = 0.42749167\n",
      "Iteration 17, loss = 0.68713591\n",
      "Iteration 18, loss = 0.68345338\n",
      "Iteration 896, loss = 0.28494864\n",
      "Iteration 1073, loss = 0.26358061\n",
      "Iteration 982, loss = 0.30577614\n",
      "Iteration 1997, loss = 0.18900325\n",
      "Iteration 19, loss = 0.67972548\n",
      "Iteration 20, loss = 0.67612249\n",
      "Iteration 897, loss = 0.28486990\n",
      "Iteration 983, loss = 0.30567642\n",
      "Iteration 271, loss = 0.42729012\n",
      "Iteration 265, loss = 0.35007346\n",
      "Iteration 21, loss = 0.67262134\n",
      "Iteration 22, loss = 0.66916485\n",
      "Iteration 2260, loss = 0.16352129\n",
      "Iteration 23, loss = 0.66563999\n",
      "Iteration 1998, loss = 0.18919299\n",
      "Iteration 24, loss = 0.66223080\n",
      "Iteration 266, loss = 0.34994895\n",
      "Iteration 1999, loss = 0.18887155\n",
      "Iteration 984, loss = 0.30558798\n",
      "Iteration 898, loss = 0.28471428\n",
      "Iteration 2261, loss = 0.16343078\n",
      "Iteration 267, loss = 0.34980268\n",
      "Iteration 25, loss = 0.65894261\n",
      "Iteration 1074, loss = 0.26324342\n",
      "Iteration 268, loss = 0.34967771\n",
      "Iteration 26, loss = 0.65562525\n",
      "Iteration 985, loss = 0.30548017\n",
      "Iteration 2000, loss = 0.18874395\n",
      "Iteration 2262, loss = 0.16334560\n",
      "Iteration 27, loss = 0.65221648\n",
      "Iteration 272, loss = 0.42694528\n",
      "Iteration 269, loss = 0.34956134\n",
      "Iteration 28, loss = 0.64894535\n",
      "Iteration 899, loss = 0.28458003\n",
      "Iteration 2001, loss = 0.18863187\n",
      "Iteration 29, loss = 0.64565100\n",
      "Iteration 270, loss = 0.34941064\n",
      "Iteration 30, loss = 0.64239498\n",
      "Iteration 1075, loss = 0.26296288\n",
      "Iteration 2263, loss = 0.16328085\n",
      "Iteration 31, loss = 0.63899862\n",
      "Iteration 986, loss = 0.30534849\n",
      "Iteration 900, loss = 0.28437651\n",
      "Iteration 32, loss = 0.63570575\n",
      "Iteration 33, loss = 0.63246847\n",
      "Iteration 2002, loss = 0.18856955\n",
      "Iteration 34, loss = 0.62914924\n",
      "Iteration 271, loss = 0.34930377\n",
      "Iteration 273, loss = 0.42666093\n",
      "Iteration 35, loss = 0.62586731\n",
      "Iteration 1076, loss = 0.26297035\n",
      "Iteration 2003, loss = 0.18846002\n",
      "Iteration 2264, loss = 0.16319214\n",
      "Iteration 272, loss = 0.34913576\n",
      "Iteration 987, loss = 0.30524721\n",
      "Iteration 36, loss = 0.62254009\n",
      "Iteration 2004, loss = 0.18837289\n",
      "Iteration 273, loss = 0.34900429\n",
      "Iteration 2005, loss = 0.18837231\n",
      "Iteration 901, loss = 0.28423900\n",
      "Iteration 37, loss = 0.61922286\n",
      "Iteration 274, loss = 0.34887815\n",
      "Iteration 2006, loss = 0.18820821\n",
      "Iteration 2265, loss = 0.16307437\n",
      "Iteration 902, loss = 0.28406861\n",
      "Iteration 274, loss = 0.42631228\n",
      "Iteration 275, loss = 0.34875647\n",
      "Iteration 988, loss = 0.30515926\n",
      "Iteration 38, loss = 0.61593374\n",
      "Iteration 1077, loss = 0.26268254\n",
      "Iteration 2007, loss = 0.18809121\n",
      "Iteration 276, loss = 0.34863917\n",
      "Iteration 2266, loss = 0.16318671\n",
      "Iteration 275, loss = 0.42607317\n",
      "Iteration 903, loss = 0.28391379\n",
      "Iteration 277, loss = 0.34850357\n",
      "Iteration 2008, loss = 0.18800192\n",
      "Iteration 39, loss = 0.61252880\n",
      "Iteration 278, loss = 0.34836540\n",
      "Iteration 40, loss = 0.60917760\n",
      "Iteration 989, loss = 0.30507001\n",
      "Iteration 2009, loss = 0.18789563\n",
      "Iteration 41, loss = 0.60584376\n",
      "Iteration 2267, loss = 0.16304197\n",
      "Iteration 904, loss = 0.28380080\n",
      "Iteration 42, loss = 0.60250690\n",
      "Iteration 2010, loss = 0.18784915\n",
      "Iteration 990, loss = 0.30494088\n",
      "Iteration 43, loss = 0.59909948\n",
      "Iteration 2268, loss = 0.16290400\n",
      "Iteration 2011, loss = 0.18784659\n",
      "Iteration 44, loss = 0.59573168\n",
      "Iteration 276, loss = 0.42578060\n",
      "Iteration 905, loss = 0.28364644\n",
      "Iteration 45, loss = 0.59241357\n",
      "Iteration 279, loss = 0.34825491\n",
      "Iteration 2012, loss = 0.18761561\n",
      "Iteration 46, loss = 0.58893819\n",
      "Iteration 991, loss = 0.30488318\n",
      "Iteration 280, loss = 0.34811841\n",
      "Iteration 2013, loss = 0.18750749\n",
      "Iteration 47, loss = 0.58557647\n",
      "Iteration 281, loss = 0.34799544\n",
      "Iteration 992, loss = 0.30476320\n",
      "Iteration 282, loss = 0.34786404\n",
      "Iteration 48, loss = 0.58224169\n",
      "Iteration 2269, loss = 0.16278737\n",
      "Iteration 283, loss = 0.34774272\n",
      "Iteration 993, loss = 0.30462250\n",
      "Iteration 49, loss = 0.57880534\n",
      "Iteration 284, loss = 0.34760720\n",
      "Iteration 50, loss = 0.57546213\n",
      "Iteration 285, loss = 0.34748319\n",
      "Iteration 2014, loss = 0.18740494\n",
      "Iteration 2270, loss = 0.16267320\n",
      "Iteration 1078, loss = 0.26235962\n",
      "Iteration 51, loss = 0.57210895\n",
      "Iteration 994, loss = 0.30452161\n",
      "Iteration 2015, loss = 0.18734686\n",
      "Iteration 906, loss = 0.28351798\n",
      "Iteration 52, loss = 0.56873503\n",
      "Iteration 2016, loss = 0.18730600\n",
      "Iteration 277, loss = 0.42547738\n",
      "Iteration 53, loss = 0.56531141\n",
      "Iteration 286, loss = 0.34736308\n",
      "Iteration 995, loss = 0.30441415\n",
      "Iteration 907, loss = 0.28337512\n",
      "Iteration 54, loss = 0.56198373\n",
      "Iteration 2017, loss = 0.18719076\n",
      "Iteration 1079, loss = 0.26219662\n",
      "Iteration 287, loss = 0.34726550\n",
      "Iteration 55, loss = 0.55857056\n",
      "Iteration 908, loss = 0.28315606\n",
      "Iteration 2271, loss = 0.16261186\n",
      "Iteration 56, loss = 0.55528189\n",
      "Iteration 278, loss = 0.42523983\n",
      "Iteration 57, loss = 0.55184734\n",
      "Iteration 2018, loss = 0.18701154\n",
      "Iteration 288, loss = 0.34712263\n",
      "Iteration 909, loss = 0.28303430\n",
      "Iteration 2019, loss = 0.18702391\n",
      "Iteration 289, loss = 0.34702405\n",
      "Iteration 2272, loss = 0.16254211\n",
      "Iteration 2020, loss = 0.18678401\n",
      "Iteration 58, loss = 0.54853330\n",
      "Iteration 1080, loss = 0.26196882\n",
      "Iteration 59, loss = 0.54523810\n",
      "Iteration 60, loss = 0.54184189\n",
      "Iteration 290, loss = 0.34688475\n",
      "Iteration 910, loss = 0.28293024\n",
      "Iteration 61, loss = 0.53852202\n",
      "Iteration 2021, loss = 0.18684414\n",
      "Iteration 62, loss = 0.53529436\n",
      "Iteration 996, loss = 0.30429663\n",
      "Iteration 63, loss = 0.53201785\n",
      "Iteration 279, loss = 0.42490140\n",
      "Iteration 64, loss = 0.52875094\n",
      "Iteration 911, loss = 0.28270512\n",
      "Iteration 65, loss = 0.52557141\n",
      "Iteration 66, loss = 0.52227469\n",
      "Iteration 67, loss = 0.51928270\n",
      "Iteration 68, loss = 0.51602311\n",
      "Iteration 912, loss = 0.28260376\n",
      "Iteration 69, loss = 0.51306456\n",
      "Iteration 2022, loss = 0.18664705\n",
      "Iteration 2273, loss = 0.16251742\n",
      "Iteration 291, loss = 0.34678193\n",
      "Iteration 1081, loss = 0.26173521\n",
      "Iteration 997, loss = 0.30425006\n",
      "Iteration 70, loss = 0.50998115\n",
      "Iteration 292, loss = 0.34663335\n",
      "Iteration 293, loss = 0.34652743\n",
      "Iteration 71, loss = 0.50695961\n",
      "Iteration 913, loss = 0.28239142\n",
      "Iteration 998, loss = 0.30409170\n",
      "Iteration 2023, loss = 0.18666258\n",
      "Iteration 72, loss = 0.50399934\n",
      "Iteration 294, loss = 0.34642876\n",
      "Iteration 73, loss = 0.50099856\n",
      "Iteration 295, loss = 0.34629466\n",
      "Iteration 2024, loss = 0.18649095\n",
      "Iteration 2274, loss = 0.16237959\n",
      "Iteration 1082, loss = 0.26166095\n",
      "Iteration 296, loss = 0.34616104\n",
      "Iteration 914, loss = 0.28231401\n",
      "Iteration 2025, loss = 0.18633106\n",
      "Iteration 297, loss = 0.34605029\n",
      "Iteration 74, loss = 0.49811220\n",
      "Iteration 280, loss = 0.42461869\n",
      "Iteration 2275, loss = 0.16238195\n",
      "Iteration 298, loss = 0.34594261\n",
      "Iteration 2026, loss = 0.18623544\n",
      "Iteration 75, loss = 0.49531731\n",
      "Iteration 999, loss = 0.30399648\n",
      "Iteration 299, loss = 0.34582045\n",
      "Iteration 76, loss = 0.49251733\n",
      "Iteration 300, loss = 0.34573535\n",
      "Iteration 915, loss = 0.28211615\n",
      "Iteration 77, loss = 0.48976485\n",
      "Iteration 1083, loss = 0.26139100\n",
      "Iteration 301, loss = 0.34561000\n",
      "Iteration 78, loss = 0.48709958\n",
      "Iteration 2276, loss = 0.16227120\n",
      "Iteration 79, loss = 0.48442990\n",
      "Iteration 1000, loss = 0.30388518\n",
      "Iteration 302, loss = 0.34548128\n",
      "Iteration 916, loss = 0.28197128\n",
      "Iteration 2027, loss = 0.18625630\n",
      "Iteration 80, loss = 0.48187758\n",
      "Iteration 281, loss = 0.42435082\n",
      "Iteration 303, loss = 0.34536690\n",
      "Iteration 81, loss = 0.47935218\n",
      "Iteration 2028, loss = 0.18607001\n",
      "Iteration 1001, loss = 0.30381217\n",
      "Iteration 82, loss = 0.47677392\n",
      "Iteration 2029, loss = 0.18603255\n",
      "Iteration 83, loss = 0.47437339\n",
      "Iteration 2277, loss = 0.16214581\n",
      "Iteration 1002, loss = 0.30369189\n",
      "Iteration 282, loss = 0.42408868\n",
      "Iteration 2030, loss = 0.18590223\n",
      "Iteration 917, loss = 0.28182511\n",
      "Iteration 304, loss = 0.34526038\n",
      "Iteration 84, loss = 0.47196429\n",
      "Iteration 2031, loss = 0.18575990\n",
      "Iteration 1084, loss = 0.26112835\n",
      "Iteration 1003, loss = 0.30357493\n",
      "Iteration 2278, loss = 0.16217841Iteration 85, loss = 0.46957784\n",
      "\n",
      "Iteration 918, loss = 0.28167138\n",
      "Iteration 86, loss = 0.46727243\n",
      "Iteration 87, loss = 0.46499372\n",
      "Iteration 305, loss = 0.34513858\n",
      "Iteration 1004, loss = 0.30346148\n",
      "Iteration 88, loss = 0.46283461\n",
      "Iteration 2032, loss = 0.18578323\n",
      "Iteration 89, loss = 0.46058594\n",
      "Iteration 283, loss = 0.42379097\n",
      "Iteration 90, loss = 0.45850447\n",
      "Iteration 306, loss = 0.34502777\n",
      "Iteration 919, loss = 0.28155923\n",
      "Iteration 91, loss = 0.45644284\n",
      "Iteration 92, loss = 0.45442515\n",
      "Iteration 307, loss = 0.34492382\n",
      "Iteration 2033, loss = 0.18560915\n",
      "Iteration 920, loss = 0.28143877\n",
      "Iteration 1005, loss = 0.30337301\n",
      "Iteration 2279, loss = 0.16205069\n",
      "Iteration 93, loss = 0.45244217\n",
      "Iteration 2034, loss = 0.18559413\n",
      "Iteration 308, loss = 0.34482133\n",
      "Iteration 1085, loss = 0.26092995\n",
      "Iteration 94, loss = 0.45040286\n",
      "Iteration 2035, loss = 0.18541455\n",
      "Iteration 1006, loss = 0.30325841\n",
      "Iteration 921, loss = 0.28125777\n",
      "Iteration 2036, loss = 0.18528377\n",
      "Iteration 95, loss = 0.44858159\n",
      "Iteration 96, loss = 0.44672658\n",
      "Iteration 284, loss = 0.42362091\n",
      "Iteration 2280, loss = 0.16196611\n",
      "Iteration 97, loss = 0.44490850\n",
      "Iteration 309, loss = 0.34468765\n",
      "Iteration 1007, loss = 0.30314645\n",
      "Iteration 310, loss = 0.34457976\n",
      "Iteration 2037, loss = 0.18521190\n",
      "Iteration 311, loss = 0.34447187\n",
      "Iteration 2038, loss = 0.18514642\n",
      "Iteration 98, loss = 0.44324399\n",
      "Iteration 312, loss = 0.34438118\n",
      "Iteration 99, loss = 0.44139007\n",
      "Iteration 1086, loss = 0.26073904\n",
      "Iteration 100, loss = 0.43978913\n",
      "Iteration 922, loss = 0.28105987\n",
      "Iteration 101, loss = 0.43808422\n",
      "Iteration 313, loss = 0.34425132\n",
      "Iteration 285, loss = 0.42325187\n",
      "Iteration 102, loss = 0.43643103\n",
      "Iteration 2039, loss = 0.18499226\n",
      "Iteration 103, loss = 0.43489013\n",
      "Iteration 314, loss = 0.34415179\n",
      "Iteration 104, loss = 0.43333470\n",
      "Iteration 923, loss = 0.28092892\n",
      "Iteration 105, loss = 0.43187143\n",
      "Iteration 315, loss = 0.34407478\n",
      "Iteration 1008, loss = 0.30306444\n",
      "Iteration 106, loss = 0.43044248\n",
      "Iteration 2281, loss = 0.16186418\n",
      "Iteration 107, loss = 0.42896952\n",
      "Iteration 108, loss = 0.42762366\n",
      "Iteration 316, loss = 0.34393419\n",
      "Iteration 2040, loss = 0.18505087\n",
      "Iteration 109, loss = 0.42628613\n",
      "Iteration 110, loss = 0.42495241\n",
      "Iteration 111, loss = 0.42364934\n",
      "Iteration 2041, loss = 0.18498447\n",
      "Iteration 924, loss = 0.28072686\n",
      "Iteration 1087, loss = 0.26051461\n",
      "Iteration 317, loss = 0.34382900\n",
      "Iteration 112, loss = 0.42236157\n",
      "Iteration 2282, loss = 0.16180319\n",
      "Iteration 2042, loss = 0.18469119\n",
      "Iteration 286, loss = 0.42297208\n",
      "Iteration 925, loss = 0.28061795\n",
      "Iteration 113, loss = 0.42111526\n",
      "Iteration 1009, loss = 0.30294220\n",
      "Iteration 114, loss = 0.42003203\n",
      "Iteration 318, loss = 0.34371551\n",
      "Iteration 2283, loss = 0.16183206\n",
      "Iteration 2043, loss = 0.18460037\n",
      "Iteration 115, loss = 0.41880910\n",
      "Iteration 116, loss = 0.41766567\n",
      "Iteration 926, loss = 0.28042644\n",
      "Iteration 117, loss = 0.41657558\n",
      "Iteration 287, loss = 0.42273985Iteration 319, loss = 0.34360051\n",
      "\n",
      "Iteration 2044, loss = 0.18454722\n",
      "Iteration 320, loss = 0.34351090\n",
      "Iteration 118, loss = 0.41546502\n",
      "Iteration 2045, loss = 0.18444069\n",
      "Iteration 321, loss = 0.34338671\n",
      "Iteration 1010, loss = 0.30281745\n",
      "Iteration 119, loss = 0.41445937\n",
      "Iteration 2284, loss = 0.16168679\n",
      "Iteration 927, loss = 0.28027475\n",
      "Iteration 120, loss = 0.41341914\n",
      "Iteration 1088, loss = 0.26044154\n",
      "Iteration 2046, loss = 0.18430741\n",
      "Iteration 121, loss = 0.41236644\n",
      "Iteration 322, loss = 0.34328519\n",
      "Iteration 928, loss = 0.28009897\n",
      "Iteration 288, loss = 0.42244721\n",
      "Iteration 2047, loss = 0.18429899\n",
      "Iteration 122, loss = 0.41139332\n",
      "Iteration 929, loss = 0.28003996\n",
      "Iteration 123, loss = 0.41047157\n",
      "Iteration 2285, loss = 0.16158117\n",
      "Iteration 2048, loss = 0.18421614\n",
      "Iteration 124, loss = 0.40951637\n",
      "Iteration 1011, loss = 0.30282500\n",
      "Iteration 323, loss = 0.34319476\n",
      "Iteration 2049, loss = 0.18404477\n",
      "Iteration 1089, loss = 0.26020582\n",
      "Iteration 930, loss = 0.27986754\n",
      "Iteration 2050, loss = 0.18395785\n",
      "Iteration 125, loss = 0.40856115\n",
      "Iteration 2286, loss = 0.16141465\n",
      "Iteration 289, loss = 0.42219137\n",
      "Iteration 126, loss = 0.40768387\n",
      "Iteration 1012, loss = 0.30259910\n",
      "Iteration 324, loss = 0.34308664\n",
      "Iteration 127, loss = 0.40677826\n",
      "Iteration 931, loss = 0.27970780\n",
      "Iteration 2287, loss = 0.16143262\n",
      "Iteration 128, loss = 0.40594294\n",
      "Iteration 129, loss = 0.40509250\n",
      "Iteration 1013, loss = 0.30249435\n",
      "Iteration 1090, loss = 0.25994565\n",
      "Iteration 325, loss = 0.34297935\n",
      "Iteration 290, loss = 0.42188414\n",
      "Iteration 2051, loss = 0.18384804\n",
      "Iteration 130, loss = 0.40429009\n",
      "Iteration 932, loss = 0.27950407\n",
      "Iteration 131, loss = 0.40344405\n",
      "Iteration 2052, loss = 0.18374503\n",
      "Iteration 132, loss = 0.40259825\n",
      "Iteration 2288, loss = 0.16134990\n",
      "Iteration 326, loss = 0.34287959\n",
      "Iteration 2053, loss = 0.18365713\n",
      "Iteration 1091, loss = 0.25973825\n",
      "Iteration 327, loss = 0.34276644\n",
      "Iteration 2054, loss = 0.18358744\n",
      "Iteration 133, loss = 0.40186318\n",
      "Iteration 933, loss = 0.27944361\n",
      "Iteration 1014, loss = 0.30237378\n",
      "Iteration 134, loss = 0.40109131\n",
      "Iteration 2055, loss = 0.18346139\n",
      "Iteration 328, loss = 0.34266868\n",
      "Iteration 291, loss = 0.42161487\n",
      "Iteration 2289, loss = 0.16124549\n",
      "Iteration 135, loss = 0.40032210\n",
      "Iteration 2056, loss = 0.18333369\n",
      "Iteration 136, loss = 0.39961537\n",
      "Iteration 329, loss = 0.34256768\n",
      "Iteration 137, loss = 0.39889518\n",
      "Iteration 1015, loss = 0.30229025\n",
      "Iteration 1092, loss = 0.25958774\n",
      "Iteration 138, loss = 0.39816423\n",
      "Iteration 330, loss = 0.34245985\n",
      "Iteration 934, loss = 0.27928646\n",
      "Iteration 2057, loss = 0.18329781\n",
      "Iteration 331, loss = 0.34235987\n",
      "Iteration 139, loss = 0.39746988\n",
      "Iteration 1016, loss = 0.30221572\n",
      "Iteration 140, loss = 0.39680911\n",
      "Iteration 141, loss = 0.39615275\n",
      "Iteration 332, loss = 0.34225907\n",
      "Iteration 2290, loss = 0.16115507\n",
      "Iteration 142, loss = 0.39543694\n",
      "Iteration 2058, loss = 0.18316801\n",
      "Iteration 935, loss = 0.27902770\n",
      "Iteration 292, loss = 0.42134932\n",
      "Iteration 2059, loss = 0.18305333\n",
      "Iteration 143, loss = 0.39486237\n",
      "Iteration 1093, loss = 0.25932048\n",
      "Iteration 936, loss = 0.27893096\n",
      "Iteration 333, loss = 0.34215584\n",
      "Iteration 144, loss = 0.39417195\n",
      "Iteration 145, loss = 0.39358758\n",
      "Iteration 1017, loss = 0.30221380\n",
      "Iteration 334, loss = 0.34204505\n",
      "Iteration 146, loss = 0.39299988\n",
      "Iteration 2291, loss = 0.16111284\n",
      "Iteration 147, loss = 0.39235384\n",
      "Iteration 148, loss = 0.39175811\n",
      "Iteration 2060, loss = 0.18295034\n",
      "Iteration 149, loss = 0.39125082\n",
      "Iteration 293, loss = 0.42108894\n",
      "Iteration 2061, loss = 0.18285714\n",
      "Iteration 335, loss = 0.34195425\n",
      "Iteration 937, loss = 0.27873316\n",
      "Iteration 2062, loss = 0.18275618\n",
      "Iteration 150, loss = 0.39061788\n",
      "Iteration 2292, loss = 0.16097232\n",
      "Iteration 336, loss = 0.34185906\n",
      "Iteration 2063, loss = 0.18274971\n",
      "Iteration 151, loss = 0.39011449\n",
      "Iteration 1018, loss = 0.30198151\n",
      "Iteration 152, loss = 0.38952392\n",
      "Iteration 337, loss = 0.34176335\n",
      "Iteration 294, loss = 0.42089070\n",
      "Iteration 153, loss = 0.38899312\n",
      "Iteration 938, loss = 0.27860994\n",
      "Iteration 338, loss = 0.34165091\n",
      "Iteration 2064, loss = 0.18253920\n",
      "Iteration 1094, loss = 0.25906791\n",
      "Iteration 154, loss = 0.38844018\n",
      "Iteration 2293, loss = 0.16087578\n",
      "Iteration 155, loss = 0.38789898\n",
      "Iteration 1019, loss = 0.30185201\n",
      "Iteration 2065, loss = 0.18255058\n",
      "Iteration 156, loss = 0.38741694\n",
      "Iteration 339, loss = 0.34154938\n",
      "Iteration 939, loss = 0.27852343\n",
      "Iteration 157, loss = 0.38687973\n",
      "Iteration 2066, loss = 0.18245514\n",
      "Iteration 158, loss = 0.38641039\n",
      "Iteration 159, loss = 0.38590348\n",
      "Iteration 940, loss = 0.27828376\n",
      "Iteration 160, loss = 0.38537953\n",
      "Iteration 2067, loss = 0.18230929\n",
      "Iteration 161, loss = 0.38491627\n",
      "Iteration 340, loss = 0.34144959\n",
      "Iteration 295, loss = 0.42060758\n",
      "Iteration 162, loss = 0.38441418\n",
      "Iteration 2294, loss = 0.16084672\n",
      "Iteration 941, loss = 0.27809740\n",
      "Iteration 163, loss = 0.38392734\n",
      "Iteration 2068, loss = 0.18224937\n",
      "Iteration 164, loss = 0.38351551\n",
      "Iteration 341, loss = 0.34136649\n",
      "Iteration 1095, loss = 0.25909355\n",
      "Iteration 1020, loss = 0.30182773\n",
      "Iteration 165, loss = 0.38302513\n",
      "Iteration 2295, loss = 0.16073754\n",
      "Iteration 166, loss = 0.38259216\n",
      "Iteration 342, loss = 0.34124932\n",
      "Iteration 942, loss = 0.27811376\n",
      "Iteration 2069, loss = 0.18207918\n",
      "Iteration 167, loss = 0.38209364\n",
      "Iteration 168, loss = 0.38165528\n",
      "Iteration 169, loss = 0.38122230\n",
      "Iteration 343, loss = 0.34115552\n",
      "Iteration 1021, loss = 0.30166027\n",
      "Iteration 170, loss = 0.38081589\n",
      "Iteration 171, loss = 0.38034853\n",
      "Iteration 2070, loss = 0.18196139\n",
      "Iteration 172, loss = 0.37990315\n",
      "Iteration 2296, loss = 0.16071514\n",
      "Iteration 1096, loss = 0.25882026\n",
      "Iteration 296, loss = 0.42031021\n",
      "Iteration 344, loss = 0.34108025\n",
      "Iteration 943, loss = 0.27786770\n",
      "Iteration 173, loss = 0.37952522\n",
      "Iteration 2071, loss = 0.18189179\n",
      "Iteration 174, loss = 0.37905813\n",
      "Iteration 345, loss = 0.34095372\n",
      "Iteration 1022, loss = 0.30153129\n",
      "Iteration 175, loss = 0.37864359\n",
      "Iteration 2297, loss = 0.16062444\n",
      "Iteration 176, loss = 0.37825540\n",
      "Iteration 2072, loss = 0.18180878\n",
      "Iteration 944, loss = 0.27764159\n",
      "Iteration 346, loss = 0.34088946\n",
      "Iteration 177, loss = 0.37780893\n",
      "Iteration 297, loss = 0.42009051\n",
      "Iteration 178, loss = 0.37744317\n",
      "Iteration 2298, loss = 0.16066813\n",
      "Iteration 179, loss = 0.37705303\n",
      "Iteration 1023, loss = 0.30144206\n",
      "Iteration 180, loss = 0.37659942\n",
      "Iteration 1097, loss = 0.25859915\n",
      "Iteration 181, loss = 0.37626077\n",
      "Iteration 2073, loss = 0.18167784\n",
      "Iteration 945, loss = 0.27756132\n",
      "Iteration 182, loss = 0.37583049\n",
      "Iteration 2299, loss = 0.16056092\n",
      "Iteration 347, loss = 0.34076297\n",
      "Iteration 183, loss = 0.37554159\n",
      "Iteration 348, loss = 0.34067099\n",
      "Iteration 2074, loss = 0.18162246\n",
      "Iteration 184, loss = 0.37509199\n",
      "Iteration 298, loss = 0.41983055Iteration 946, loss = 0.27737165\n",
      "\n",
      "Iteration 185, loss = 0.37468381\n",
      "Iteration 186, loss = 0.37433698\n",
      "Iteration 2075, loss = 0.18154174\n",
      "Iteration 1024, loss = 0.30130499\n",
      "Iteration 1098, loss = 0.25839134\n",
      "Iteration 947, loss = 0.27714182\n",
      "Iteration 2076, loss = 0.18143665\n",
      "Iteration 187, loss = 0.37395462\n",
      "Iteration 349, loss = 0.34056627\n",
      "Iteration 2077, loss = 0.18132169\n",
      "Iteration 188, loss = 0.37359333\n",
      "Iteration 189, loss = 0.37322333\n",
      "Iteration 948, loss = 0.27718162\n",
      "Iteration 190, loss = 0.37289702\n",
      "Iteration 1099, loss = 0.25820687\n",
      "Iteration 191, loss = 0.37253123\n",
      "Iteration 2078, loss = 0.18120269\n",
      "Iteration 350, loss = 0.34046930\n",
      "Iteration 2300, loss = 0.16034527\n",
      "Iteration 192, loss = 0.37213179\n",
      "Iteration 2079, loss = 0.18115468\n",
      "Iteration 1025, loss = 0.30126866\n",
      "Iteration 299, loss = 0.41961838\n",
      "Iteration 2080, loss = 0.18100919\n",
      "Iteration 351, loss = 0.34038781\n",
      "Iteration 193, loss = 0.37182566\n",
      "Iteration 949, loss = 0.27685772\n",
      "Iteration 194, loss = 0.37144673\n",
      "Iteration 2081, loss = 0.18090720\n",
      "Iteration 2301, loss = 0.16039151\n",
      "Iteration 195, loss = 0.37114229\n",
      "Iteration 1100, loss = 0.25795058\n",
      "Iteration 352, loss = 0.34028360\n",
      "Iteration 196, loss = 0.37077213\n",
      "Iteration 197, loss = 0.37043036\n",
      "Iteration 2082, loss = 0.18088688\n",
      "Iteration 198, loss = 0.37009851\n",
      "Iteration 1026, loss = 0.30112674\n",
      "Iteration 2083, loss = 0.18070551\n",
      "Iteration 300, loss = 0.41926449\n",
      "Iteration 353, loss = 0.34019150\n",
      "Iteration 199, loss = 0.36979652\n",
      "Iteration 354, loss = 0.34010706\n",
      "Iteration 2084, loss = 0.18060352\n",
      "Iteration 950, loss = 0.27671623\n",
      "Iteration 355, loss = 0.33999724\n",
      "Iteration 1101, loss = 0.25765455\n",
      "Iteration 200, loss = 0.36947792\n",
      "Iteration 356, loss = 0.33990184\n",
      "Iteration 2085, loss = 0.18053172\n",
      "Iteration 2302, loss = 0.16027884\n",
      "Iteration 201, loss = 0.36912104\n",
      "Iteration 951, loss = 0.27658397\n",
      "Iteration 202, loss = 0.36883557\n",
      "Iteration 1027, loss = 0.30100791\n",
      "Iteration 357, loss = 0.33981094\n",
      "Iteration 2086, loss = 0.18042656\n",
      "Iteration 203, loss = 0.36849251\n",
      "Iteration 301, loss = 0.41910447\n",
      "Iteration 204, loss = 0.36819542\n",
      "Iteration 358, loss = 0.33972089\n",
      "Iteration 205, loss = 0.36790981\n",
      "Iteration 2087, loss = 0.18033197\n",
      "Iteration 952, loss = 0.27641762\n",
      "Iteration 1102, loss = 0.25743506\n",
      "Iteration 206, loss = 0.36758794\n",
      "Iteration 359, loss = 0.33961159\n",
      "Iteration 207, loss = 0.36728006\n",
      "Iteration 953, loss = 0.27625649\n",
      "Iteration 2303, loss = 0.16012086\n",
      "Iteration 208, loss = 0.36698999\n",
      "Iteration 360, loss = 0.33952369\n",
      "Iteration 2088, loss = 0.18023762\n",
      "Iteration 954, loss = 0.27611933\n",
      "Iteration 209, loss = 0.36668470\n",
      "Iteration 210, loss = 0.36642803\n",
      "Iteration 1028, loss = 0.30092608\n",
      "Iteration 211, loss = 0.36611665\n",
      "Iteration 1103, loss = 0.25723946\n",
      "Iteration 302, loss = 0.41885830Iteration 2089, loss = 0.18011233\n",
      "\n",
      "Iteration 212, loss = 0.36582690\n",
      "Iteration 213, loss = 0.36556535\n",
      "Iteration 2304, loss = 0.16011473\n",
      "Iteration 361, loss = 0.33942667\n",
      "Iteration 214, loss = 0.36524090\n",
      "Iteration 955, loss = 0.27599813\n",
      "Iteration 215, loss = 0.36499250\n",
      "Iteration 362, loss = 0.33933306\n",
      "Iteration 1104, loss = 0.25708429\n",
      "Iteration 216, loss = 0.36471619\n",
      "Iteration 2090, loss = 0.18008640\n",
      "Iteration 363, loss = 0.33924497\n",
      "Iteration 1029, loss = 0.30084195\n",
      "Iteration 956, loss = 0.27585176\n",
      "Iteration 217, loss = 0.36442261\n",
      "Iteration 364, loss = 0.33915573\n",
      "Iteration 218, loss = 0.36416758\n",
      "Iteration 2091, loss = 0.17996285\n",
      "Iteration 957, loss = 0.27576307\n",
      "Iteration 219, loss = 0.36390651\n",
      "Iteration 2305, loss = 0.15998730\n",
      "Iteration 365, loss = 0.33906693\n",
      "Iteration 220, loss = 0.36361116\n",
      "Iteration 1030, loss = 0.30066863\n",
      "Iteration 221, loss = 0.36335572\n",
      "Iteration 303, loss = 0.41852913\n",
      "Iteration 1105, loss = 0.25682679\n",
      "Iteration 2092, loss = 0.17984159\n",
      "Iteration 366, loss = 0.33900102\n",
      "Iteration 958, loss = 0.27554657\n",
      "Iteration 222, loss = 0.36308595\n",
      "Iteration 223, loss = 0.36282957\n",
      "Iteration 224, loss = 0.36256227\n",
      "Iteration 225, loss = 0.36230140\n",
      "Iteration 2093, loss = 0.17972925\n",
      "Iteration 226, loss = 0.36203490\n",
      "Iteration 367, loss = 0.33888635\n",
      "Iteration 2306, loss = 0.16006346\n",
      "Iteration 227, loss = 0.36179044\n",
      "Iteration 959, loss = 0.27531322\n",
      "Iteration 1106, loss = 0.25662510\n",
      "Iteration 228, loss = 0.36151693\n",
      "Iteration 304, loss = 0.41828560\n",
      "Iteration 229, loss = 0.36125926\n",
      "Iteration 1031, loss = 0.30060259\n",
      "Iteration 368, loss = 0.33879463\n",
      "Iteration 2094, loss = 0.17969725\n",
      "Iteration 2307, loss = 0.15987356\n",
      "Iteration 2095, loss = 0.17961567\n",
      "Iteration 230, loss = 0.36102371\n",
      "Iteration 369, loss = 0.33870079\n",
      "Iteration 1107, loss = 0.25659645\n",
      "Iteration 960, loss = 0.27515807\n",
      "Iteration 370, loss = 0.33860704\n",
      "Iteration 2096, loss = 0.17953752\n",
      "Iteration 231, loss = 0.36079020\n",
      "Iteration 371, loss = 0.33851649\n",
      "Iteration 1032, loss = 0.30048223\n",
      "Iteration 232, loss = 0.36056279\n",
      "Iteration 233, loss = 0.36027476\n",
      "Iteration 2097, loss = 0.17938817\n",
      "Iteration 372, loss = 0.33842480\n",
      "Iteration 2308, loss = 0.15974613\n",
      "Iteration 234, loss = 0.36002800\n",
      "Iteration 305, loss = 0.41802682\n",
      "Iteration 235, loss = 0.35978330\n",
      "Iteration 373, loss = 0.33833500\n",
      "Iteration 236, loss = 0.35957560\n",
      "Iteration 237, loss = 0.35928825\n",
      "Iteration 374, loss = 0.33824545\n",
      "Iteration 961, loss = 0.27506117\n",
      "Iteration 2098, loss = 0.17926101\n",
      "Iteration 375, loss = 0.33817019\n",
      "Iteration 2309, loss = 0.15965884\n",
      "Iteration 1108, loss = 0.25629354\n",
      "Iteration 1033, loss = 0.30033446\n",
      "Iteration 376, loss = 0.33808735\n",
      "Iteration 2099, loss = 0.17916630\n",
      "Iteration 377, loss = 0.33797117\n",
      "Iteration 2100, loss = 0.17904799\n",
      "Iteration 2310, loss = 0.15967794\n",
      "Iteration 238, loss = 0.35905180\n",
      "Iteration 378, loss = 0.33788556\n",
      "Iteration 1034, loss = 0.30024500\n",
      "Iteration 379, loss = 0.33779223\n",
      "Iteration 962, loss = 0.27497619\n",
      "Iteration 2311, loss = 0.15959044\n",
      "Iteration 380, loss = 0.33771796\n",
      "Iteration 239, loss = 0.35884633Iteration 2101, loss = 0.17902149\n",
      "Iteration 381, loss = 0.33762709\n",
      "\n",
      "Iteration 306, loss = 0.41776352\n",
      "Iteration 240, loss = 0.35862044\n",
      "Iteration 2102, loss = 0.17885043\n",
      "Iteration 382, loss = 0.33752990\n",
      "Iteration 241, loss = 0.35835896\n",
      "Iteration 1035, loss = 0.30015657\n",
      "Iteration 2312, loss = 0.15949716\n",
      "Iteration 1109, loss = 0.25614381\n",
      "Iteration 963, loss = 0.27478723\n",
      "Iteration 242, loss = 0.35813930\n",
      "Iteration 383, loss = 0.33743262\n",
      "Iteration 2103, loss = 0.17876932\n",
      "Iteration 243, loss = 0.35792108\n",
      "Iteration 244, loss = 0.35768471\n",
      "Iteration 384, loss = 0.33735127\n",
      "Iteration 1036, loss = 0.30003867\n",
      "Iteration 964, loss = 0.27457404\n",
      "Iteration 245, loss = 0.35747408\n",
      "Iteration 2104, loss = 0.17864552\n",
      "Iteration 2313, loss = 0.15946103\n",
      "Iteration 307, loss = 0.41754803\n",
      "Iteration 385, loss = 0.33725861\n",
      "Iteration 1110, loss = 0.25607889\n",
      "Iteration 246, loss = 0.35722442\n",
      "Iteration 2105, loss = 0.17857805\n",
      "Iteration 386, loss = 0.33717299\n",
      "Iteration 2314, loss = 0.15937692\n",
      "Iteration 965, loss = 0.27456817\n",
      "Iteration 387, loss = 0.33708514\n",
      "Iteration 247, loss = 0.35702365\n",
      "Iteration 248, loss = 0.35681078\n",
      "Iteration 2315, loss = 0.15919115\n",
      "Iteration 1111, loss = 0.25576595\n",
      "Iteration 2106, loss = 0.17845730\n",
      "Iteration 249, loss = 0.35660728\n",
      "Iteration 1037, loss = 0.29996697\n",
      "Iteration 388, loss = 0.33700000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 250, loss = 0.35633819Iteration 308, loss = 0.41725307\n",
      "Iteration 966, loss = 0.27425866\n",
      "\n",
      "Iteration 2107, loss = 0.17844041\n",
      "Iteration 251, loss = 0.35621671\n",
      "Iteration 1038, loss = 0.29983538\n",
      "Iteration 252, loss = 0.35592146\n",
      "Iteration 2108, loss = 0.17837979\n",
      "Iteration 253, loss = 0.35569215\n",
      "Iteration 967, loss = 0.27421299\n",
      "Iteration 2316, loss = 0.15915871\n",
      "Iteration 254, loss = 0.35545871\n",
      "Iteration 1112, loss = 0.25544510\n",
      "Iteration 255, loss = 0.35525400\n",
      "Iteration 968, loss = 0.27397063\n",
      "Iteration 256, loss = 0.35503649\n",
      "Iteration 309, loss = 0.41705636\n",
      "Iteration 2109, loss = 0.17814147\n",
      "Iteration 257, loss = 0.35482525\n",
      "Iteration 2317, loss = 0.15918354\n",
      "Iteration 1039, loss = 0.29972093\n",
      "Iteration 258, loss = 0.35464666\n",
      "Iteration 2110, loss = 0.17809425\n",
      "Iteration 969, loss = 0.27379691\n",
      "Iteration 259, loss = 0.35442829\n",
      "Iteration 1, loss = 0.71622740\n",
      "Iteration 260, loss = 0.35421803\n",
      "Iteration 2318, loss = 0.15900926\n",
      "Iteration 2, loss = 0.71472989\n",
      "Iteration 310, loss = 0.41677644\n",
      "Iteration 1113, loss = 0.25537124\n",
      "Iteration 970, loss = 0.27380126\n",
      "Iteration 2111, loss = 0.17809739\n",
      "Iteration 261, loss = 0.35401301\n",
      "Iteration 1040, loss = 0.29963412\n",
      "Iteration 262, loss = 0.35382074\n",
      "Iteration 2319, loss = 0.15889630\n",
      "Iteration 3, loss = 0.71237171\n",
      "Iteration 2112, loss = 0.17793754\n",
      "Iteration 263, loss = 0.35358704\n",
      "Iteration 2320, loss = 0.15893941\n",
      "Iteration 4, loss = 0.70959548\n",
      "Iteration 2113, loss = 0.17776646\n",
      "Iteration 971, loss = 0.27350539\n",
      "Iteration 1114, loss = 0.25510663\n",
      "Iteration 264, loss = 0.35338785\n",
      "Iteration 1041, loss = 0.29949702\n",
      "Iteration 5, loss = 0.70622683\n",
      "Iteration 311, loss = 0.41656459\n",
      "Iteration 265, loss = 0.35319179\n",
      "Iteration 2114, loss = 0.17780816\n",
      "Iteration 266, loss = 0.35296993\n",
      "Iteration 6, loss = 0.70265509\n",
      "Iteration 1042, loss = 0.29939340\n",
      "Iteration 267, loss = 0.35278156\n",
      "Iteration 2321, loss = 0.15878633\n",
      "Iteration 7, loss = 0.69894118\n",
      "Iteration 972, loss = 0.27338907\n",
      "Iteration 2115, loss = 0.17762671\n",
      "Iteration 268, loss = 0.35258151\n",
      "Iteration 8, loss = 0.69498061\n",
      "Iteration 269, loss = 0.35237122\n",
      "Iteration 973, loss = 0.27322228\n",
      "Iteration 9, loss = 0.69104875\n",
      "Iteration 1115, loss = 0.25483346\n",
      "Iteration 312, loss = 0.41627782\n",
      "Iteration 2322, loss = 0.15875233\n",
      "Iteration 270, loss = 0.35217055\n",
      "Iteration 1043, loss = 0.29927056\n",
      "Iteration 10, loss = 0.68711084\n",
      "Iteration 2116, loss = 0.17745762\n",
      "Iteration 271, loss = 0.35202328\n",
      "Iteration 974, loss = 0.27303314\n",
      "Iteration 272, loss = 0.35177634\n",
      "Iteration 2117, loss = 0.17742337\n",
      "Iteration 1044, loss = 0.29920719\n",
      "Iteration 273, loss = 0.35159879\n",
      "Iteration 274, loss = 0.35141463\n",
      "Iteration 313, loss = 0.41605700\n",
      "Iteration 2323, loss = 0.15861017\n",
      "Iteration 275, loss = 0.35120974\n",
      "Iteration 1116, loss = 0.25458372\n",
      "Iteration 276, loss = 0.35099194\n",
      "Iteration 2118, loss = 0.17725148\n",
      "Iteration 277, loss = 0.35079867\n",
      "Iteration 975, loss = 0.27287328\n",
      "Iteration 2119, loss = 0.17720449\n",
      "Iteration 11, loss = 0.68315070\n",
      "Iteration 278, loss = 0.35061295\n",
      "Iteration 314, loss = 0.41580553\n",
      "Iteration 1117, loss = 0.25465580\n",
      "Iteration 1045, loss = 0.29906109\n",
      "Iteration 2324, loss = 0.15861064\n",
      "Iteration 279, loss = 0.35041489\n",
      "Iteration 280, loss = 0.35022501\n",
      "Iteration 2120, loss = 0.17711554\n",
      "Iteration 281, loss = 0.35007085\n",
      "Iteration 282, loss = 0.34986627\n",
      "Iteration 283, loss = 0.34974124\n",
      "Iteration 2121, loss = 0.17699578\n",
      "Iteration 284, loss = 0.34948488\n",
      "Iteration 12, loss = 0.67911593\n",
      "Iteration 976, loss = 0.27276211\n",
      "Iteration 285, loss = 0.34930273\n",
      "Iteration 286, loss = 0.34915205\n",
      "Iteration 2122, loss = 0.17690102\n",
      "Iteration 1118, loss = 0.25418955\n",
      "Iteration 13, loss = 0.67519163\n",
      "Iteration 977, loss = 0.27259279\n",
      "Iteration 287, loss = 0.34894948\n",
      "Iteration 1046, loss = 0.29896295\n",
      "Iteration 315, loss = 0.41557712\n",
      "Iteration 288, loss = 0.34877621\n",
      "Iteration 2325, loss = 0.15852242\n",
      "Iteration 289, loss = 0.34860169\n",
      "Iteration 978, loss = 0.27248503\n",
      "Iteration 290, loss = 0.34844966\n",
      "Iteration 2123, loss = 0.17686589\n",
      "Iteration 291, loss = 0.34826817\n",
      "Iteration 292, loss = 0.34810194\n",
      "Iteration 2124, loss = 0.17685416\n",
      "Iteration 14, loss = 0.67105114\n",
      "Iteration 293, loss = 0.34792681\n",
      "Iteration 979, loss = 0.27224698\n",
      "Iteration 1047, loss = 0.29885887\n",
      "Iteration 15, loss = 0.66720655\n",
      "Iteration 1119, loss = 0.25394455\n",
      "Iteration 16, loss = 0.66311559\n",
      "Iteration 294, loss = 0.34776068\n",
      "Iteration 316, loss = 0.41531855\n",
      "Iteration 2125, loss = 0.17663350\n",
      "Iteration 17, loss = 0.65917964\n",
      "Iteration 2326, loss = 0.15840816\n",
      "Iteration 1048, loss = 0.29873630\n",
      "Iteration 18, loss = 0.65514031\n",
      "Iteration 2126, loss = 0.17670181\n",
      "Iteration 980, loss = 0.27210116\n",
      "Iteration 295, loss = 0.34759799\n",
      "Iteration 981, loss = 0.27197466\n",
      "Iteration 296, loss = 0.34746473\n",
      "Iteration 2127, loss = 0.17649348\n",
      "Iteration 2327, loss = 0.15833253\n",
      "Iteration 19, loss = 0.65125444\n",
      "Iteration 317, loss = 0.41509778\n",
      "Iteration 1049, loss = 0.29863975\n",
      "Iteration 297, loss = 0.34728178\n",
      "Iteration 982, loss = 0.27181337\n",
      "Iteration 298, loss = 0.34710421\n",
      "Iteration 1120, loss = 0.25376544\n",
      "Iteration 20, loss = 0.64735876\n",
      "Iteration 2328, loss = 0.15822301\n",
      "Iteration 299, loss = 0.34696423\n",
      "Iteration 1050, loss = 0.29859999\n",
      "Iteration 983, loss = 0.27178166\n",
      "Iteration 318, loss = 0.41483415\n",
      "Iteration 300, loss = 0.34681195\n",
      "Iteration 21, loss = 0.64338599\n",
      "Iteration 2128, loss = 0.17631712\n",
      "Iteration 301, loss = 0.34662604\n",
      "Iteration 22, loss = 0.63952871\n",
      "Iteration 2129, loss = 0.17629378\n",
      "Iteration 302, loss = 0.34648396\n",
      "Iteration 984, loss = 0.27150434\n",
      "Iteration 23, loss = 0.63562689\n",
      "Iteration 303, loss = 0.34633343\n",
      "Iteration 24, loss = 0.63173615\n",
      "Iteration 985, loss = 0.27135458\n",
      "Iteration 2329, loss = 0.15815485\n",
      "Iteration 2130, loss = 0.17627093\n",
      "Iteration 1051, loss = 0.29844310\n",
      "Iteration 2131, loss = 0.17602469\n",
      "Iteration 986, loss = 0.27135096\n",
      "Iteration 25, loss = 0.62796280\n",
      "Iteration 2132, loss = 0.17596344\n",
      "Iteration 304, loss = 0.34617991\n",
      "Iteration 1121, loss = 0.25372112\n",
      "Iteration 2133, loss = 0.17583374\n",
      "Iteration 305, loss = 0.34602848\n",
      "Iteration 987, loss = 0.27103985\n",
      "Iteration 306, loss = 0.34586634\n",
      "Iteration 26, loss = 0.62410600\n",
      "Iteration 2330, loss = 0.15813559\n",
      "Iteration 319, loss = 0.41463584\n",
      "Iteration 2134, loss = 0.17571926\n",
      "Iteration 307, loss = 0.34573331\n",
      "Iteration 2135, loss = 0.17590668\n",
      "Iteration 27, loss = 0.62038300\n",
      "Iteration 1052, loss = 0.29832306\n",
      "Iteration 2136, loss = 0.17553828\n",
      "Iteration 308, loss = 0.34556029\n",
      "Iteration 2137, loss = 0.17543455\n",
      "Iteration 28, loss = 0.61659709\n",
      "Iteration 309, loss = 0.34542182\n",
      "Iteration 1053, loss = 0.29821871\n",
      "Iteration 310, loss = 0.34525987\n",
      "Iteration 2138, loss = 0.17546968\n",
      "Iteration 2331, loss = 0.15802766\n",
      "Iteration 1122, loss = 0.25336815\n",
      "Iteration 988, loss = 0.27084922\n",
      "Iteration 320, loss = 0.41435079\n",
      "Iteration 311, loss = 0.34510339\n",
      "Iteration 29, loss = 0.61289165\n",
      "Iteration 1054, loss = 0.29812916\n",
      "Iteration 312, loss = 0.34496962\n",
      "Iteration 2139, loss = 0.17525156\n",
      "Iteration 989, loss = 0.27068270\n",
      "Iteration 313, loss = 0.34480790\n",
      "Iteration 314, loss = 0.34468914\n",
      "Iteration 2332, loss = 0.15793234\n",
      "Iteration 315, loss = 0.34452450\n",
      "Iteration 990, loss = 0.27056878\n",
      "Iteration 2140, loss = 0.17523791\n",
      "Iteration 1123, loss = 0.25321397\n",
      "Iteration 30, loss = 0.60911525\n",
      "Iteration 991, loss = 0.27038430\n",
      "Iteration 2333, loss = 0.15788857\n",
      "Iteration 1055, loss = 0.29800618\n",
      "Iteration 316, loss = 0.34440134\n",
      "Iteration 31, loss = 0.60536385\n",
      "Iteration 317, loss = 0.34424695\n",
      "Iteration 32, loss = 0.60159531\n",
      "Iteration 992, loss = 0.27022397\n",
      "Iteration 318, loss = 0.34411976\n",
      "Iteration 1056, loss = 0.29795687\n",
      "Iteration 2141, loss = 0.17507935\n",
      "Iteration 1124, loss = 0.25305746\n",
      "Iteration 33, loss = 0.59801722\n",
      "Iteration 319, loss = 0.34395068\n",
      "Iteration 34, loss = 0.59434664\n",
      "Iteration 321, loss = 0.41411144\n",
      "Iteration 320, loss = 0.34381908\n",
      "Iteration 2142, loss = 0.17507547\n",
      "Iteration 321, loss = 0.34367400\n",
      "Iteration 1057, loss = 0.29778473\n",
      "Iteration 322, loss = 0.34353432\n",
      "Iteration 2143, loss = 0.17486038\n",
      "Iteration 35, loss = 0.59069801\n",
      "Iteration 993, loss = 0.27032495\n",
      "Iteration 323, loss = 0.34340004\n",
      "Iteration 2144, loss = 0.17488397\n",
      "Iteration 36, loss = 0.58715812\n",
      "Iteration 2334, loss = 0.15784864\n",
      "Iteration 1058, loss = 0.29766528\n",
      "Iteration 1125, loss = 0.25285976\n",
      "Iteration 2145, loss = 0.17469288\n",
      "Iteration 324, loss = 0.34326388\n",
      "Iteration 322, loss = 0.41388797\n",
      "Iteration 37, loss = 0.58354202\n",
      "Iteration 994, loss = 0.26995359\n",
      "Iteration 2146, loss = 0.17460781\n",
      "Iteration 38, loss = 0.58003349\n",
      "Iteration 325, loss = 0.34312348\n",
      "Iteration 995, loss = 0.26979962\n",
      "Iteration 39, loss = 0.57650704\n",
      "Iteration 326, loss = 0.34298023\n",
      "Iteration 2335, loss = 0.15769603\n",
      "Iteration 327, loss = 0.34285230\n",
      "Iteration 1126, loss = 0.25269321\n",
      "Iteration 996, loss = 0.26964084\n",
      "Iteration 328, loss = 0.34272468\n",
      "Iteration 40, loss = 0.57305537\n",
      "Iteration 2147, loss = 0.17461252\n",
      "Iteration 1059, loss = 0.29758937\n",
      "Iteration 2336, loss = 0.15768080\n",
      "Iteration 329, loss = 0.34255612\n",
      "Iteration 323, loss = 0.41367556\n",
      "Iteration 2148, loss = 0.17438274\n",
      "Iteration 41, loss = 0.56953849\n",
      "Iteration 1060, loss = 0.29745859\n",
      "Iteration 2149, loss = 0.17432204\n",
      "Iteration 997, loss = 0.26955264\n",
      "Iteration 330, loss = 0.34246577\n",
      "Iteration 2337, loss = 0.15758242\n",
      "Iteration 331, loss = 0.34231183\n",
      "Iteration 1127, loss = 0.25242205\n",
      "Iteration 1061, loss = 0.29744225\n",
      "Iteration 42, loss = 0.56612103\n",
      "Iteration 2150, loss = 0.17423430\n",
      "Iteration 324, loss = 0.41343983\n",
      "Iteration 998, loss = 0.26934791\n",
      "Iteration 332, loss = 0.34217701\n",
      "Iteration 2338, loss = 0.15750741\n",
      "Iteration 1062, loss = 0.29725821\n",
      "Iteration 999, loss = 0.26918943\n",
      "Iteration 43, loss = 0.56275479\n",
      "Iteration 333, loss = 0.34203319\n",
      "Iteration 2151, loss = 0.17427211\n",
      "Iteration 1128, loss = 0.25216297\n",
      "Iteration 2339, loss = 0.15741861\n",
      "Iteration 334, loss = 0.34189346\n",
      "Iteration 1000, loss = 0.26897270\n",
      "Iteration 2152, loss = 0.17397842\n",
      "Iteration 335, loss = 0.34175685\n",
      "Iteration 44, loss = 0.55939182\n",
      "Iteration 2153, loss = 0.17390735\n",
      "Iteration 336, loss = 0.34160729Iteration 2154, loss = 0.17399471\n",
      "Iteration 1063, loss = 0.29719378\n",
      "\n",
      "Iteration 2340, loss = 0.15736761\n",
      "Iteration 325, loss = 0.41319464\n",
      "Iteration 337, loss = 0.34146882\n",
      "Iteration 1129, loss = 0.25198238\n",
      "Iteration 338, loss = 0.34134910\n",
      "Iteration 339, loss = 0.34122090\n",
      "Iteration 340, loss = 0.34107764\n",
      "Iteration 2341, loss = 0.15735655\n",
      "Iteration 341, loss = 0.34096483\n",
      "Iteration 1001, loss = 0.26884906\n",
      "Iteration 45, loss = 0.55605125\n",
      "Iteration 342, loss = 0.34082270\n",
      "Iteration 1130, loss = 0.25183248\n",
      "Iteration 343, loss = 0.34069125\n",
      "Iteration 344, loss = 0.34054210\n",
      "Iteration 345, loss = 0.34043503\n",
      "Iteration 46, loss = 0.55272103\n",
      "Iteration 346, loss = 0.34030283\n",
      "Iteration 347, loss = 0.34016283\n",
      "Iteration 2155, loss = 0.17382217\n",
      "Iteration 348, loss = 0.34002997\n",
      "Iteration 2342, loss = 0.15719245\n",
      "Iteration 2156, loss = 0.17370234\n",
      "Iteration 349, loss = 0.33991301\n",
      "Iteration 47, loss = 0.54952366\n",
      "Iteration 350, loss = 0.33978775\n",
      "Iteration 2157, loss = 0.17354495\n",
      "Iteration 351, loss = 0.33966409Iteration 1002, loss = 0.26866853\n",
      "Iteration 326, loss = 0.41293445\n",
      "Iteration 2343, loss = 0.15714923\n",
      "Iteration 2158, loss = 0.17344832\n",
      "Iteration 1064, loss = 0.29710105\n",
      "Iteration 1131, loss = 0.25162373\n",
      "\n",
      "Iteration 48, loss = 0.54626284\n",
      "Iteration 352, loss = 0.33952221\n",
      "Iteration 2159, loss = 0.17338494\n",
      "Iteration 353, loss = 0.33941736\n",
      "Iteration 49, loss = 0.54298845\n",
      "Iteration 1003, loss = 0.26860858\n",
      "Iteration 354, loss = 0.33928994\n",
      "Iteration 1132, loss = 0.25137768\n",
      "Iteration 2160, loss = 0.17324124\n",
      "Iteration 355, loss = 0.33919041\n",
      "Iteration 1065, loss = 0.29697532\n",
      "Iteration 356, loss = 0.33904973\n",
      "Iteration 2161, loss = 0.17318987Iteration 2344, loss = 0.15707943\n",
      "\n",
      "Iteration 357, loss = 0.33890372\n",
      "Iteration 50, loss = 0.53988101\n",
      "Iteration 327, loss = 0.41276748\n",
      "Iteration 358, loss = 0.33879696\n",
      "Iteration 2162, loss = 0.17305018\n",
      "Iteration 359, loss = 0.33866777\n",
      "Iteration 360, loss = 0.33855367\n",
      "Iteration 1066, loss = 0.29684021\n",
      "Iteration 1004, loss = 0.26842685\n",
      "Iteration 1133, loss = 0.25122719\n",
      "Iteration 361, loss = 0.33845816\n",
      "Iteration 51, loss = 0.53675130\n",
      "Iteration 2163, loss = 0.17300478\n",
      "Iteration 362, loss = 0.33831315\n",
      "Iteration 363, loss = 0.33818882\n",
      "Iteration 364, loss = 0.33806852\n",
      "Iteration 52, loss = 0.53357901\n",
      "Iteration 2164, loss = 0.17295171\n",
      "Iteration 2345, loss = 0.15698446\n",
      "Iteration 53, loss = 0.53060258\n",
      "Iteration 2165, loss = 0.17275623\n",
      "Iteration 328, loss = 0.41255404\n",
      "Iteration 1005, loss = 0.26830704\n",
      "Iteration 2346, loss = 0.15695746\n",
      "Iteration 365, loss = 0.33795337\n",
      "Iteration 1067, loss = 0.29671622\n",
      "Iteration 54, loss = 0.52745007\n",
      "Iteration 366, loss = 0.33786726\n",
      "Iteration 55, loss = 0.52449655\n",
      "Iteration 2347, loss = 0.15681853\n",
      "Iteration 1134, loss = 0.25100911\n",
      "Iteration 367, loss = 0.33780629\n",
      "Iteration 56, loss = 0.52148718\n",
      "Iteration 1068, loss = 0.29660583\n",
      "Iteration 368, loss = 0.33761094\n",
      "Iteration 1006, loss = 0.26810068\n",
      "Iteration 369, loss = 0.33748580\n",
      "Iteration 2166, loss = 0.17269047\n",
      "Iteration 2348, loss = 0.15676237\n",
      "Iteration 2167, loss = 0.17256147\n",
      "Iteration 1069, loss = 0.29652535\n",
      "Iteration 1007, loss = 0.26786535\n",
      "Iteration 2349, loss = 0.15670647\n",
      "Iteration 2168, loss = 0.17246171\n",
      "Iteration 1070, loss = 0.29643324\n",
      "Iteration 2169, loss = 0.17238865\n",
      "Iteration 2350, loss = 0.15656472\n",
      "Iteration 1008, loss = 0.26772762\n",
      "Iteration 370, loss = 0.33739168\n",
      "Iteration 57, loss = 0.51856357\n",
      "Iteration 329, loss = 0.41230712\n",
      "Iteration 2170, loss = 0.17227239\n",
      "Iteration 1009, loss = 0.26762249\n",
      "Iteration 1135, loss = 0.25086173\n",
      "Iteration 371, loss = 0.33725207\n",
      "Iteration 372, loss = 0.33713218\n",
      "Iteration 58, loss = 0.51571642\n",
      "Iteration 373, loss = 0.33703327\n",
      "Iteration 1071, loss = 0.29631655\n",
      "Iteration 1136, loss = 0.25085984\n",
      "Iteration 374, loss = 0.33693552\n",
      "Iteration 59, loss = 0.51296701\n",
      "Iteration 1010, loss = 0.26746700\n",
      "Iteration 330, loss = 0.41212353\n",
      "Iteration 60, loss = 0.51007927\n",
      "Iteration 61, loss = 0.50730281\n",
      "Iteration 375, loss = 0.33679695\n",
      "Iteration 2171, loss = 0.17217965\n",
      "Iteration 1011, loss = 0.26731139\n",
      "Iteration 376, loss = 0.33667209\n",
      "Iteration 62, loss = 0.50460926\n",
      "Iteration 2172, loss = 0.17206870\n",
      "Iteration 331, loss = 0.41184785\n",
      "Iteration 377, loss = 0.33656218\n",
      "Iteration 63, loss = 0.50192111\n",
      "Iteration 378, loss = 0.33642924\n",
      "Iteration 64, loss = 0.49934088\n",
      "Iteration 379, loss = 0.33633924\n",
      "Iteration 1137, loss = 0.25049260\n",
      "Iteration 65, loss = 0.49661764\n",
      "Iteration 380, loss = 0.33622229\n",
      "Iteration 1012, loss = 0.26719873Iteration 2351, loss = 0.15650051\n",
      "\n",
      "Iteration 381, loss = 0.33610214\n",
      "Iteration 382, loss = 0.33600817\n",
      "Iteration 1072, loss = 0.29619814\n",
      "Iteration 332, loss = 0.41154885\n",
      "Iteration 383, loss = 0.33586684\n",
      "Iteration 384, loss = 0.33577434\n",
      "Iteration 385, loss = 0.33564932\n",
      "Iteration 1073, loss = 0.29613897\n",
      "Iteration 1138, loss = 0.25020527\n",
      "Iteration 386, loss = 0.33553874\n",
      "Iteration 387, loss = 0.33544539\n",
      "Iteration 1074, loss = 0.29599564\n",
      "Iteration 388, loss = 0.33531508\n",
      "Iteration 389, loss = 0.33525483\n",
      "Iteration 390, loss = 0.33513839\n",
      "Iteration 333, loss = 0.41132252\n",
      "Iteration 391, loss = 0.33498682\n",
      "Iteration 2173, loss = 0.17220163\n",
      "Iteration 1139, loss = 0.25024650\n",
      "Iteration 66, loss = 0.49406393\n",
      "Iteration 1013, loss = 0.26704927\n",
      "Iteration 2352, loss = 0.15648172\n",
      "Iteration 2174, loss = 0.17192727\n",
      "Iteration 334, loss = 0.41109119\n",
      "Iteration 2353, loss = 0.15636305\n",
      "Iteration 1014, loss = 0.26686530\n",
      "Iteration 2175, loss = 0.17187172\n",
      "Iteration 2176, loss = 0.17172378\n",
      "Iteration 1015, loss = 0.26676238\n",
      "Iteration 67, loss = 0.49164341\n",
      "Iteration 2177, loss = 0.17161160\n",
      "Iteration 335, loss = 0.41092691\n",
      "Iteration 2178, loss = 0.17173427\n",
      "Iteration 1075, loss = 0.29589460\n",
      "Iteration 68, loss = 0.48916142\n",
      "Iteration 2354, loss = 0.15635330\n",
      "Iteration 69, loss = 0.48670599\n",
      "Iteration 392, loss = 0.33488968\n",
      "Iteration 1076, loss = 0.29581159\n",
      "Iteration 2179, loss = 0.17141232\n",
      "Iteration 393, loss = 0.33476991\n",
      "Iteration 70, loss = 0.48430007\n",
      "Iteration 394, loss = 0.33466164\n",
      "Iteration 1016, loss = 0.26646571\n",
      "Iteration 395, loss = 0.33455049\n",
      "Iteration 396, loss = 0.33446552\n",
      "Iteration 1077, loss = 0.29569085\n",
      "Iteration 397, loss = 0.33432460\n",
      "Iteration 2180, loss = 0.17132265\n",
      "Iteration 1017, loss = 0.26632905\n",
      "Iteration 398, loss = 0.33423445\n",
      "Iteration 399, loss = 0.33411325\n",
      "Iteration 2181, loss = 0.17133602\n",
      "Iteration 400, loss = 0.33400401\n",
      "Iteration 401, loss = 0.33390831\n",
      "Iteration 1018, loss = 0.26643136\n",
      "Iteration 71, loss = 0.48194622\n",
      "Iteration 402, loss = 0.33379003Iteration 1140, loss = 0.24980114\n",
      "\n",
      "Iteration 1078, loss = 0.29554553\n",
      "Iteration 2355, loss = 0.15621962\n",
      "Iteration 1019, loss = 0.26605442\n",
      "Iteration 2182, loss = 0.17115742\n",
      "Iteration 72, loss = 0.47977894\n",
      "Iteration 403, loss = 0.33369685\n",
      "Iteration 404, loss = 0.33358743\n",
      "Iteration 405, loss = 0.33347740\n",
      "Iteration 336, loss = 0.41065769\n",
      "Iteration 406, loss = 0.33335812\n",
      "Iteration 1141, loss = 0.24986570\n",
      "Iteration 1079, loss = 0.29564329\n",
      "Iteration 73, loss = 0.47741570\n",
      "Iteration 1020, loss = 0.26585200\n",
      "Iteration 2356, loss = 0.15627868\n",
      "Iteration 407, loss = 0.33327364\n",
      "Iteration 1080, loss = 0.29539548\n",
      "Iteration 408, loss = 0.33315057\n",
      "Iteration 74, loss = 0.47520084\n",
      "Iteration 337, loss = 0.41050217\n",
      "Iteration 409, loss = 0.33308726\n",
      "Iteration 1142, loss = 0.24945835\n",
      "Iteration 410, loss = 0.33294198\n",
      "Iteration 411, loss = 0.33282919\n",
      "Iteration 75, loss = 0.47304807\n",
      "Iteration 412, loss = 0.33273452\n",
      "Iteration 2357, loss = 0.15619301\n",
      "Iteration 413, loss = 0.33261106\n",
      "Iteration 1081, loss = 0.29522620\n",
      "Iteration 1021, loss = 0.26570783\n",
      "Iteration 338, loss = 0.41019274\n",
      "Iteration 414, loss = 0.33253588\n",
      "Iteration 415, loss = 0.33242320\n",
      "Iteration 416, loss = 0.33232326\n",
      "Iteration 76, loss = 0.47091345\n",
      "Iteration 417, loss = 0.33222756\n",
      "Iteration 2358, loss = 0.15605023\n",
      "Iteration 1143, loss = 0.24925823\n",
      "Iteration 418, loss = 0.33209750\n",
      "Iteration 1022, loss = 0.26551602\n",
      "Iteration 419, loss = 0.33200687\n",
      "Iteration 1082, loss = 0.29522186\n",
      "Iteration 339, loss = 0.40997744\n",
      "Iteration 420, loss = 0.33188949\n",
      "Iteration 77, loss = 0.46892091\n",
      "Iteration 421, loss = 0.33179590\n",
      "Iteration 78, loss = 0.46676285\n",
      "Iteration 422, loss = 0.33173628\n",
      "Iteration 1083, loss = 0.29502391\n",
      "Iteration 1023, loss = 0.26545898\n",
      "Iteration 1144, loss = 0.24900992\n",
      "Iteration 79, loss = 0.46489477\n",
      "Iteration 2359, loss = 0.15597523\n",
      "Iteration 423, loss = 0.33159606\n",
      "Iteration 340, loss = 0.40976113\n",
      "Iteration 80, loss = 0.46296642\n",
      "Iteration 424, loss = 0.33149321\n",
      "Iteration 81, loss = 0.46100241\n",
      "Iteration 1084, loss = 0.29492607\n",
      "Iteration 1024, loss = 0.26524289\n",
      "Iteration 1145, loss = 0.24884501\n",
      "Iteration 2360, loss = 0.15584894\n",
      "Iteration 82, loss = 0.45905940\n",
      "Iteration 425, loss = 0.33139293\n",
      "Iteration 426, loss = 0.33128766\n",
      "Iteration 83, loss = 0.45727004\n",
      "Iteration 427, loss = 0.33119932\n",
      "Iteration 428, loss = 0.33109032\n",
      "Iteration 84, loss = 0.45549834\n",
      "Iteration 429, loss = 0.33098418Iteration 2361, loss = 0.15582446\n",
      "\n",
      "Iteration 1025, loss = 0.26505724\n",
      "Iteration 341, loss = 0.40954361\n",
      "Iteration 430, loss = 0.33088465\n",
      "Iteration 1146, loss = 0.24864265\n",
      "Iteration 431, loss = 0.33080564\n",
      "Iteration 1026, loss = 0.26489280\n",
      "Iteration 85, loss = 0.45377421\n",
      "Iteration 2362, loss = 0.15571498\n",
      "Iteration 432, loss = 0.33069548\n",
      "Iteration 1085, loss = 0.29482513\n",
      "Iteration 1027, loss = 0.26475439\n",
      "Iteration 433, loss = 0.33062616\n",
      "Iteration 86, loss = 0.45213882\n",
      "Iteration 342, loss = 0.40935117\n",
      "Iteration 434, loss = 0.33051494\n",
      "Iteration 2363, loss = 0.15566229\n",
      "Iteration 87, loss = 0.45045579\n",
      "Iteration 435, loss = 0.33040978\n",
      "Iteration 1086, loss = 0.29471901\n",
      "Iteration 436, loss = 0.33032869\n",
      "Iteration 1028, loss = 0.26459261\n",
      "Iteration 437, loss = 0.33021892\n",
      "Iteration 1147, loss = 0.24849860\n",
      "Iteration 438, loss = 0.33012472\n",
      "Iteration 439, loss = 0.33005204\n",
      "Iteration 88, loss = 0.44880898\n",
      "Iteration 1029, loss = 0.26443031\n",
      "Iteration 440, loss = 0.32993434\n",
      "Iteration 89, loss = 0.44725430\n",
      "Iteration 441, loss = 0.32984578\n",
      "Iteration 2364, loss = 0.15568878\n",
      "Iteration 2183, loss = 0.17111772\n",
      "Iteration 1030, loss = 0.26430450\n",
      "Iteration 90, loss = 0.44568534\n",
      "Iteration 442, loss = 0.32975199\n",
      "Iteration 343, loss = 0.40916247\n",
      "Iteration 2184, loss = 0.17093898\n",
      "Iteration 91, loss = 0.44416226\n",
      "Iteration 1087, loss = 0.29471036\n",
      "Iteration 1031, loss = 0.26421209\n",
      "Iteration 443, loss = 0.32965270\n",
      "Iteration 1148, loss = 0.24820546\n",
      "Iteration 92, loss = 0.44271623\n",
      "Iteration 444, loss = 0.32956142\n",
      "Iteration 2185, loss = 0.17083820\n",
      "Iteration 2365, loss = 0.15571810\n",
      "Iteration 445, loss = 0.32947962\n",
      "Iteration 93, loss = 0.44126087\n",
      "Iteration 1032, loss = 0.26391174\n",
      "Iteration 344, loss = 0.40887397\n",
      "Iteration 446, loss = 0.32938147\n",
      "Iteration 1088, loss = 0.29449114\n",
      "Iteration 447, loss = 0.32929090\n",
      "Iteration 94, loss = 0.43988745\n",
      "Iteration 448, loss = 0.32918499\n",
      "Iteration 1149, loss = 0.24803792\n",
      "Iteration 1033, loss = 0.26391008\n",
      "Iteration 449, loss = 0.32909283\n",
      "Iteration 450, loss = 0.32899995\n",
      "Iteration 95, loss = 0.43848356\n",
      "Iteration 451, loss = 0.32890401\n",
      "Iteration 2366, loss = 0.15548832\n",
      "Iteration 1089, loss = 0.29443228\n",
      "Iteration 452, loss = 0.32881478\n",
      "Iteration 96, loss = 0.43711358\n",
      "Iteration 453, loss = 0.32872078\n",
      "Iteration 454, loss = 0.32869791\n",
      "Iteration 1034, loss = 0.26357992\n",
      "Iteration 1150, loss = 0.24782566\n",
      "Iteration 345, loss = 0.40866754\n",
      "Iteration 2186, loss = 0.17081331\n",
      "Iteration 97, loss = 0.43580389\n",
      "Iteration 2367, loss = 0.15533315\n",
      "Iteration 1090, loss = 0.29435051\n",
      "Iteration 455, loss = 0.32855310\n",
      "Iteration 2187, loss = 0.17065185\n",
      "Iteration 98, loss = 0.43449187\n",
      "Iteration 456, loss = 0.32845575\n",
      "Iteration 1035, loss = 0.26369188\n",
      "Iteration 1151, loss = 0.24758893\n",
      "Iteration 457, loss = 0.32835676\n",
      "Iteration 99, loss = 0.43325677\n",
      "Iteration 1036, loss = 0.26333065\n",
      "Iteration 458, loss = 0.32828042\n",
      "Iteration 459, loss = 0.32817243\n",
      "Iteration 2188, loss = 0.17057350\n",
      "Iteration 460, loss = 0.32808989\n",
      "Iteration 1037, loss = 0.26314404\n",
      "Iteration 461, loss = 0.32799976\n",
      "Iteration 462, loss = 0.32790833\n",
      "Iteration 2368, loss = 0.15531780\n",
      "Iteration 463, loss = 0.32783393\n",
      "Iteration 1091, loss = 0.29417716\n",
      "Iteration 100, loss = 0.43197737\n",
      "Iteration 464, loss = 0.32772315\n",
      "Iteration 346, loss = 0.40843487\n",
      "Iteration 2189, loss = 0.17050943\n",
      "Iteration 465, loss = 0.32763437\n",
      "Iteration 101, loss = 0.43079271\n",
      "Iteration 1152, loss = 0.24750583\n",
      "Iteration 466, loss = 0.32755042\n",
      "Iteration 467, loss = 0.32747613\n",
      "Iteration 468, loss = 0.32737066\n",
      "Iteration 1092, loss = 0.29408988\n",
      "Iteration 469, loss = 0.32728712\n",
      "Iteration 102, loss = 0.42960950\n",
      "Iteration 1038, loss = 0.26303807\n",
      "Iteration 470, loss = 0.32718865\n",
      "Iteration 2190, loss = 0.17036178\n",
      "Iteration 103, loss = 0.42847772\n",
      "Iteration 1039, loss = 0.26280841\n",
      "Iteration 1093, loss = 0.29396730\n",
      "Iteration 471, loss = 0.32709996\n",
      "Iteration 347, loss = 0.40821485\n",
      "Iteration 104, loss = 0.42733207\n",
      "Iteration 1153, loss = 0.24755812\n",
      "Iteration 472, loss = 0.32701842\n",
      "Iteration 1040, loss = 0.26280199\n",
      "Iteration 2369, loss = 0.15521903\n",
      "Iteration 2191, loss = 0.17033695\n",
      "Iteration 473, loss = 0.32691330\n",
      "Iteration 1041, loss = 0.26251808\n",
      "Iteration 2192, loss = 0.17022164\n",
      "Iteration 105, loss = 0.42630009\n",
      "Iteration 1154, loss = 0.24708575\n",
      "Iteration 474, loss = 0.32687365\n",
      "Iteration 1042, loss = 0.26240718\n",
      "Iteration 475, loss = 0.32674281\n",
      "Iteration 1094, loss = 0.29386493\n",
      "Iteration 2193, loss = 0.17020995\n",
      "Iteration 106, loss = 0.42520422\n",
      "Iteration 476, loss = 0.32665203\n",
      "Iteration 477, loss = 0.32657901\n",
      "Iteration 2370, loss = 0.15512787\n",
      "Iteration 1043, loss = 0.26218326\n",
      "Iteration 478, loss = 0.32654073\n",
      "Iteration 479, loss = 0.32640191\n",
      "Iteration 107, loss = 0.42415034\n",
      "Iteration 348, loss = 0.40798760\n",
      "Iteration 480, loss = 0.32634695\n",
      "Iteration 1044, loss = 0.26204122\n",
      "Iteration 481, loss = 0.32627445\n",
      "Iteration 108, loss = 0.42307880\n",
      "Iteration 1095, loss = 0.29372921\n",
      "Iteration 2371, loss = 0.15505242\n",
      "Iteration 482, loss = 0.32614935\n",
      "Iteration 2194, loss = 0.17012762\n",
      "Iteration 1155, loss = 0.24681618\n",
      "Iteration 483, loss = 0.32607296\n",
      "Iteration 109, loss = 0.42211513\n",
      "Iteration 2372, loss = 0.15498566\n",
      "Iteration 1045, loss = 0.26188199\n",
      "Iteration 484, loss = 0.32598307\n",
      "Iteration 349, loss = 0.40779505\n",
      "Iteration 485, loss = 0.32590984\n",
      "Iteration 2195, loss = 0.17003913\n",
      "Iteration 1096, loss = 0.29365824\n",
      "Iteration 486, loss = 0.32581500\n",
      "Iteration 487, loss = 0.32573586\n",
      "Iteration 110, loss = 0.42109799\n",
      "Iteration 488, loss = 0.32564318\n",
      "Iteration 1046, loss = 0.26168995\n",
      "Iteration 489, loss = 0.32556645\n",
      "Iteration 1156, loss = 0.24674456\n",
      "Iteration 490, loss = 0.32546729\n",
      "Iteration 2373, loss = 0.15491975\n",
      "Iteration 2196, loss = 0.16985790\n",
      "Iteration 491, loss = 0.32543118\n",
      "Iteration 492, loss = 0.32529922\n",
      "Iteration 111, loss = 0.42017313\n",
      "Iteration 493, loss = 0.32520851\n",
      "Iteration 1047, loss = 0.26156267\n",
      "Iteration 2374, loss = 0.15487685\n",
      "Iteration 2197, loss = 0.16977710\n",
      "Iteration 112, loss = 0.41923103\n",
      "Iteration 494, loss = 0.32515488\n",
      "Iteration 1097, loss = 0.29355738\n",
      "Iteration 495, loss = 0.32504522\n",
      "Iteration 1048, loss = 0.26146505\n",
      "Iteration 2375, loss = 0.15481735\n",
      "Iteration 2198, loss = 0.16964977\n",
      "Iteration 1157, loss = 0.24645383\n",
      "Iteration 2199, loss = 0.16958895\n",
      "Iteration 1049, loss = 0.26119692\n",
      "Iteration 350, loss = 0.40762503\n",
      "Iteration 2200, loss = 0.16941727\n",
      "Iteration 113, loss = 0.41833811\n",
      "Iteration 2376, loss = 0.15474670\n",
      "Iteration 1050, loss = 0.26105355\n",
      "Iteration 114, loss = 0.41738707\n",
      "Iteration 496, loss = 0.32495728\n",
      "Iteration 2201, loss = 0.16930898\n",
      "Iteration 351, loss = 0.40734376\n",
      "Iteration 115, loss = 0.41651589\n",
      "Iteration 1051, loss = 0.26090237\n",
      "Iteration 497, loss = 0.32488348\n",
      "Iteration 498, loss = 0.32480215\n",
      "Iteration 116, loss = 0.41567570\n",
      "Iteration 1098, loss = 0.29356579\n",
      "Iteration 2377, loss = 0.15467503\n",
      "Iteration 499, loss = 0.32471744\n",
      "Iteration 2202, loss = 0.16927559\n",
      "Iteration 1158, loss = 0.24627518\n",
      "Iteration 500, loss = 0.32463187\n",
      "Iteration 117, loss = 0.41479660Iteration 2203, loss = 0.16931426\n",
      "\n",
      "Iteration 2204, loss = 0.16913209\n",
      "Iteration 2205, loss = 0.16897464\n",
      "Iteration 2378, loss = 0.15463005\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 118, loss = 0.41399986\n",
      "Iteration 501, loss = 0.32456036\n",
      "Iteration 119, loss = 0.41314525\n",
      "Iteration 352, loss = 0.40715673\n",
      "Iteration 120, loss = 0.41236561\n",
      "Iteration 2206, loss = 0.16887400\n",
      "Iteration 1052, loss = 0.26085137\n",
      "Iteration 502, loss = 0.32448416\n",
      "Iteration 1099, loss = 0.29339631\n",
      "Iteration 121, loss = 0.41156602Iteration 503, loss = 0.32439751\n",
      "Iteration 1053, loss = 0.26059548\n",
      "\n",
      "Iteration 504, loss = 0.32431082\n",
      "Iteration 1, loss = 0.78913838\n",
      "Iteration 505, loss = 0.32423125\n",
      "Iteration 506, loss = 0.32415015\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1100, loss = 0.29322191\n",
      "Iteration 2, loss = 0.78727122\n",
      "Iteration 1054, loss = 0.26048838\n",
      "Iteration 353, loss = 0.40691742\n",
      "Iteration 3, loss = 0.78463216\n",
      "Iteration 1101, loss = 0.29313131\n",
      "Iteration 1, loss = 0.77194603\n",
      "Iteration 2207, loss = 0.16877352\n",
      "Iteration 1159, loss = 0.24603539\n",
      "Iteration 1055, loss = 0.26025663\n",
      "Iteration 122, loss = 0.41077934\n",
      "Iteration 354, loss = 0.40671874\n",
      "Iteration 2, loss = 0.77055979\n",
      "Iteration 4, loss = 0.78117952\n",
      "Iteration 123, loss = 0.41001874\n",
      "Iteration 3, loss = 0.76848293\n",
      "Iteration 2208, loss = 0.16875048\n",
      "Iteration 1102, loss = 0.29304800\n",
      "Iteration 4, loss = 0.76575710\n",
      "Iteration 5, loss = 0.76309215\n",
      "Iteration 1056, loss = 0.26022923\n",
      "Iteration 124, loss = 0.40930762\n",
      "Iteration 1160, loss = 0.24591024\n",
      "Iteration 6, loss = 0.76023478\n",
      "Iteration 7, loss = 0.75728248\n",
      "Iteration 2209, loss = 0.16859923\n",
      "Iteration 8, loss = 0.75427569\n",
      "Iteration 1103, loss = 0.29294935\n",
      "Iteration 125, loss = 0.40852797\n",
      "Iteration 9, loss = 0.75141852\n",
      "Iteration 355, loss = 0.40653185\n",
      "Iteration 10, loss = 0.74853341\n",
      "Iteration 1057, loss = 0.26002751\n",
      "Iteration 2210, loss = 0.16853295\n",
      "Iteration 126, loss = 0.40786424\n",
      "Iteration 11, loss = 0.74594775\n",
      "Iteration 2211, loss = 0.16839483\n",
      "Iteration 1161, loss = 0.24565631\n",
      "Iteration 1104, loss = 0.29282146\n",
      "Iteration 5, loss = 0.77720364\n",
      "Iteration 12, loss = 0.74348379\n",
      "Iteration 127, loss = 0.40712510\n",
      "Iteration 356, loss = 0.40629693\n",
      "Iteration 13, loss = 0.74092801\n",
      "Iteration 1058, loss = 0.25976526\n",
      "Iteration 2212, loss = 0.16831913\n",
      "Iteration 14, loss = 0.73857380\n",
      "Iteration 128, loss = 0.40647082\n",
      "Iteration 15, loss = 0.73631320\n",
      "Iteration 6, loss = 0.77318337\n",
      "Iteration 2213, loss = 0.16820747\n",
      "Iteration 1059, loss = 0.25970279\n",
      "Iteration 1162, loss = 0.24547208\n",
      "Iteration 1105, loss = 0.29273164\n",
      "Iteration 16, loss = 0.73414085\n",
      "Iteration 129, loss = 0.40580067\n",
      "Iteration 1060, loss = 0.25949363\n",
      "Iteration 17, loss = 0.73223403\n",
      "Iteration 2214, loss = 0.16813184\n",
      "Iteration 18, loss = 0.73017302\n",
      "Iteration 1163, loss = 0.24528256\n",
      "Iteration 19, loss = 0.72830029\n",
      "Iteration 1061, loss = 0.25933510Iteration 1106, loss = 0.29260309\n",
      "Iteration 20, loss = 0.72637038\n",
      "\n",
      "Iteration 21, loss = 0.72465649\n",
      "Iteration 7, loss = 0.76916357\n",
      "Iteration 2215, loss = 0.16805043\n",
      "Iteration 22, loss = 0.72283019\n",
      "Iteration 357, loss = 0.40625547\n",
      "Iteration 23, loss = 0.72122315\n",
      "Iteration 2216, loss = 0.16805793\n",
      "Iteration 1164, loss = 0.24521005\n",
      "Iteration 24, loss = 0.71959274\n",
      "Iteration 2217, loss = 0.16788167\n",
      "Iteration 25, loss = 0.71791443\n",
      "Iteration 1107, loss = 0.29248608\n",
      "Iteration 26, loss = 0.71638281\n",
      "Iteration 2218, loss = 0.16777811\n",
      "Iteration 27, loss = 0.71471635\n",
      "Iteration 28, loss = 0.71318955\n",
      "Iteration 2219, loss = 0.16784580\n",
      "Iteration 29, loss = 0.71163859\n",
      "Iteration 1062, loss = 0.25920576\n",
      "Iteration 1165, loss = 0.24488015\n",
      "Iteration 2220, loss = 0.16758386\n",
      "Iteration 8, loss = 0.76510277\n",
      "Iteration 30, loss = 0.71019697\n",
      "Iteration 1108, loss = 0.29235436\n",
      "Iteration 2221, loss = 0.16762994\n",
      "Iteration 358, loss = 0.40586091\n",
      "Iteration 31, loss = 0.70865333\n",
      "Iteration 32, loss = 0.70720002\n",
      "Iteration 1063, loss = 0.25899413\n",
      "Iteration 2222, loss = 0.16744455\n",
      "Iteration 33, loss = 0.70578315\n",
      "Iteration 9, loss = 0.76083811\n",
      "Iteration 34, loss = 0.70430769\n",
      "Iteration 2223, loss = 0.16733717\n",
      "Iteration 35, loss = 0.70284515\n",
      "Iteration 2224, loss = 0.16724789\n",
      "Iteration 1064, loss = 0.25896379\n",
      "Iteration 1109, loss = 0.29227598\n",
      "Iteration 36, loss = 0.70138176\n",
      "Iteration 2225, loss = 0.16715279\n",
      "Iteration 10, loss = 0.75672555\n",
      "Iteration 359, loss = 0.40561984\n",
      "Iteration 37, loss = 0.69995940\n",
      "Iteration 1166, loss = 0.24467370\n",
      "Iteration 2226, loss = 0.16700311\n",
      "Iteration 1065, loss = 0.25883442\n",
      "Iteration 38, loss = 0.69846256\n",
      "Iteration 2227, loss = 0.16691131\n",
      "Iteration 1110, loss = 0.29215508\n",
      "Iteration 39, loss = 0.69704231\n",
      "Iteration 11, loss = 0.75277066\n",
      "Iteration 40, loss = 0.69558996\n",
      "Iteration 2228, loss = 0.16681670\n",
      "Iteration 41, loss = 0.69415731\n",
      "Iteration 1066, loss = 0.25848790\n",
      "Iteration 42, loss = 0.69264751\n",
      "Iteration 43, loss = 0.69114719\n",
      "Iteration 44, loss = 0.68968745\n",
      "Iteration 130, loss = 0.40517007\n",
      "Iteration 2229, loss = 0.16671718\n",
      "Iteration 1067, loss = 0.25834492\n",
      "Iteration 1167, loss = 0.24449593\n",
      "Iteration 45, loss = 0.68815166\n",
      "Iteration 12, loss = 0.74869954\n",
      "Iteration 131, loss = 0.40447997\n",
      "Iteration 1111, loss = 0.29204197\n",
      "Iteration 1068, loss = 0.25818656\n",
      "Iteration 360, loss = 0.40550155\n",
      "Iteration 2230, loss = 0.16662488\n",
      "Iteration 2231, loss = 0.16658249\n",
      "Iteration 1069, loss = 0.25802502\n",
      "Iteration 13, loss = 0.74482021\n",
      "Iteration 46, loss = 0.68664990\n",
      "Iteration 1112, loss = 0.29198164\n",
      "Iteration 132, loss = 0.40386962\n",
      "Iteration 1168, loss = 0.24459955\n",
      "Iteration 1070, loss = 0.25782144\n",
      "Iteration 2232, loss = 0.16646486\n",
      "Iteration 133, loss = 0.40324314\n",
      "Iteration 1071, loss = 0.25766850\n",
      "Iteration 14, loss = 0.74094493\n",
      "Iteration 2233, loss = 0.16640832\n",
      "Iteration 1169, loss = 0.24441230\n",
      "Iteration 2234, loss = 0.16634161\n",
      "Iteration 1072, loss = 0.25776263\n",
      "Iteration 361, loss = 0.40522190\n",
      "Iteration 47, loss = 0.68513995\n",
      "Iteration 2235, loss = 0.16616865\n",
      "Iteration 1113, loss = 0.29187727\n",
      "Iteration 15, loss = 0.73722630\n",
      "Iteration 134, loss = 0.40266596\n",
      "Iteration 48, loss = 0.68355672\n",
      "Iteration 135, loss = 0.40203122\n",
      "Iteration 49, loss = 0.68199684\n",
      "Iteration 1073, loss = 0.25737191\n",
      "Iteration 16, loss = 0.73357381\n",
      "Iteration 50, loss = 0.68044244\n",
      "Iteration 2236, loss = 0.16613438\n",
      "Iteration 136, loss = 0.40147375\n",
      "Iteration 1170, loss = 0.24391242\n",
      "Iteration 51, loss = 0.67885356\n",
      "Iteration 52, loss = 0.67718813\n",
      "Iteration 137, loss = 0.40087383\n",
      "Iteration 53, loss = 0.67557393\n",
      "Iteration 1114, loss = 0.29173799\n",
      "Iteration 17, loss = 0.73000083\n",
      "Iteration 1074, loss = 0.25719601\n",
      "Iteration 54, loss = 0.67389499\n",
      "Iteration 2237, loss = 0.16599045\n",
      "Iteration 138, loss = 0.40034466\n",
      "Iteration 55, loss = 0.67221272\n",
      "Iteration 2238, loss = 0.16588160\n",
      "Iteration 18, loss = 0.72631249\n",
      "Iteration 56, loss = 0.67047535\n",
      "Iteration 139, loss = 0.39973301\n",
      "Iteration 1171, loss = 0.24378715\n",
      "Iteration 1075, loss = 0.25722746\n",
      "Iteration 1115, loss = 0.29162497\n",
      "Iteration 362, loss = 0.40507196\n",
      "Iteration 57, loss = 0.66871454\n",
      "Iteration 140, loss = 0.39917852\n",
      "Iteration 58, loss = 0.66698275\n",
      "Iteration 141, loss = 0.39868607\n",
      "Iteration 2239, loss = 0.16583741\n",
      "Iteration 59, loss = 0.66518607\n",
      "Iteration 19, loss = 0.72278267\n",
      "Iteration 142, loss = 0.39812978\n",
      "Iteration 60, loss = 0.66339538\n",
      "Iteration 2240, loss = 0.16569910\n",
      "Iteration 1076, loss = 0.25687031\n",
      "Iteration 1172, loss = 0.24376038\n",
      "Iteration 1116, loss = 0.29153614\n",
      "Iteration 143, loss = 0.39762219\n",
      "Iteration 363, loss = 0.40481269Iteration 1077, loss = 0.25669392\n",
      "\n",
      "Iteration 144, loss = 0.39712810\n",
      "Iteration 20, loss = 0.71922648\n",
      "Iteration 2241, loss = 0.16562665\n",
      "Iteration 1078, loss = 0.25649999\n",
      "Iteration 61, loss = 0.66151446\n",
      "Iteration 2242, loss = 0.16554180\n",
      "Iteration 1117, loss = 0.29144046\n",
      "Iteration 145, loss = 0.39656741\n",
      "Iteration 1079, loss = 0.25639058\n",
      "Iteration 2243, loss = 0.16549583\n",
      "Iteration 62, loss = 0.65963328\n",
      "Iteration 63, loss = 0.65771064\n",
      "Iteration 1080, loss = 0.25628598\n",
      "Iteration 1173, loss = 0.24338593\n",
      "Iteration 1118, loss = 0.29131493\n",
      "Iteration 2244, loss = 0.16543138\n",
      "Iteration 146, loss = 0.39609182\n",
      "Iteration 21, loss = 0.71574863\n",
      "Iteration 2245, loss = 0.16534869\n",
      "Iteration 64, loss = 0.65581303\n",
      "Iteration 364, loss = 0.40467318\n",
      "Iteration 147, loss = 0.39560940\n",
      "Iteration 2246, loss = 0.16514880\n",
      "Iteration 65, loss = 0.65383657\n",
      "Iteration 1119, loss = 0.29120636\n",
      "Iteration 2247, loss = 0.16510150\n",
      "Iteration 1081, loss = 0.25613915\n",
      "Iteration 66, loss = 0.65187073\n",
      "Iteration 365, loss = 0.40443259\n",
      "Iteration 2248, loss = 0.16499072\n",
      "Iteration 148, loss = 0.39522242\n",
      "Iteration 67, loss = 0.64980290\n",
      "Iteration 2249, loss = 0.16487698\n",
      "Iteration 1174, loss = 0.24313053\n",
      "Iteration 149, loss = 0.39461797\n",
      "Iteration 68, loss = 0.64779357\n",
      "Iteration 69, loss = 0.64569988\n",
      "Iteration 150, loss = 0.39414630\n",
      "Iteration 70, loss = 0.64356490\n",
      "Iteration 366, loss = 0.40418977\n",
      "Iteration 1120, loss = 0.29110758\n",
      "Iteration 22, loss = 0.71240856\n",
      "Iteration 71, loss = 0.64137986\n",
      "Iteration 151, loss = 0.39369827\n",
      "Iteration 1082, loss = 0.25585268\n",
      "Iteration 72, loss = 0.63925316\n",
      "Iteration 2250, loss = 0.16478279\n",
      "Iteration 152, loss = 0.39325438\n",
      "Iteration 23, loss = 0.70902552\n",
      "Iteration 153, loss = 0.39276792\n",
      "Iteration 1121, loss = 0.29101962\n",
      "Iteration 2251, loss = 0.16471953\n",
      "Iteration 73, loss = 0.63698136\n",
      "Iteration 1175, loss = 0.24324470\n",
      "Iteration 74, loss = 0.63469232\n",
      "Iteration 2252, loss = 0.16465093\n",
      "Iteration 75, loss = 0.63248708\n",
      "Iteration 367, loss = 0.40396727\n",
      "Iteration 154, loss = 0.39237333\n",
      "Iteration 1083, loss = 0.25591052\n",
      "Iteration 2253, loss = 0.16451502\n",
      "Iteration 76, loss = 0.63012550\n",
      "Iteration 155, loss = 0.39192504\n",
      "Iteration 1176, loss = 0.24270979\n",
      "Iteration 77, loss = 0.62775635\n",
      "Iteration 24, loss = 0.70556516\n",
      "Iteration 78, loss = 0.62535226\n",
      "Iteration 1084, loss = 0.25560235\n",
      "Iteration 79, loss = 0.62288170\n",
      "Iteration 80, loss = 0.62050902\n",
      "Iteration 156, loss = 0.39151618\n",
      "Iteration 2254, loss = 0.16447489\n",
      "Iteration 81, loss = 0.61802127\n",
      "Iteration 82, loss = 0.61549729\n",
      "Iteration 25, loss = 0.70216784\n",
      "Iteration 1085, loss = 0.25541813\n",
      "Iteration 157, loss = 0.39108997\n",
      "Iteration 1177, loss = 0.24279516\n",
      "Iteration 2255, loss = 0.16434114\n",
      "Iteration 83, loss = 0.61297360\n",
      "Iteration 26, loss = 0.69879968\n",
      "Iteration 158, loss = 0.39067288\n",
      "Iteration 1122, loss = 0.29089590\n",
      "Iteration 1086, loss = 0.25529312\n",
      "Iteration 159, loss = 0.39026989\n",
      "Iteration 84, loss = 0.61037113\n",
      "Iteration 1178, loss = 0.24243895\n",
      "Iteration 368, loss = 0.40374622\n",
      "Iteration 27, loss = 0.69546878\n",
      "Iteration 85, loss = 0.60781279\n",
      "Iteration 1087, loss = 0.25506464\n",
      "Iteration 160, loss = 0.38985487\n",
      "Iteration 86, loss = 0.60519861\n",
      "Iteration 1123, loss = 0.29080198\n",
      "Iteration 2256, loss = 0.16423768\n",
      "Iteration 87, loss = 0.60247112\n",
      "Iteration 1179, loss = 0.24217673\n",
      "Iteration 88, loss = 0.59988583\n",
      "Iteration 161, loss = 0.38945185\n",
      "Iteration 1124, loss = 0.29068445\n",
      "Iteration 1088, loss = 0.25491707\n",
      "Iteration 89, loss = 0.59716151\n",
      "Iteration 2257, loss = 0.16412350\n",
      "Iteration 90, loss = 0.59442145\n",
      "Iteration 162, loss = 0.38907712\n",
      "Iteration 91, loss = 0.59171395\n",
      "Iteration 2258, loss = 0.16413695\n",
      "Iteration 28, loss = 0.69206973\n",
      "Iteration 92, loss = 0.58890020\n",
      "Iteration 1125, loss = 0.29055234\n",
      "Iteration 1089, loss = 0.25481659\n",
      "Iteration 1180, loss = 0.24201691\n",
      "Iteration 2259, loss = 0.16396408\n",
      "Iteration 163, loss = 0.38862657\n",
      "Iteration 93, loss = 0.58621911\n",
      "Iteration 1090, loss = 0.25451975\n",
      "Iteration 2260, loss = 0.16386087\n",
      "Iteration 29, loss = 0.68866932\n",
      "Iteration 164, loss = 0.38823325\n",
      "Iteration 94, loss = 0.58340838\n",
      "Iteration 1126, loss = 0.29050590\n",
      "Iteration 95, loss = 0.58063455\n",
      "Iteration 165, loss = 0.38790875\n",
      "Iteration 2261, loss = 0.16383074\n",
      "Iteration 1181, loss = 0.24178193\n",
      "Iteration 166, loss = 0.38750007\n",
      "Iteration 30, loss = 0.68533994\n",
      "Iteration 96, loss = 0.57779129\n",
      "Iteration 1091, loss = 0.25439344\n",
      "Iteration 1127, loss = 0.29035784\n",
      "Iteration 2262, loss = 0.16375344\n",
      "Iteration 97, loss = 0.57503330\n",
      "Iteration 2263, loss = 0.16359456\n",
      "Iteration 167, loss = 0.38711045\n",
      "Iteration 98, loss = 0.57215078\n",
      "Iteration 31, loss = 0.68203141\n",
      "Iteration 1182, loss = 0.24161510\n",
      "Iteration 99, loss = 0.56928841\n",
      "Iteration 1128, loss = 0.29025166\n",
      "Iteration 168, loss = 0.38674816\n",
      "Iteration 2264, loss = 0.16352865\n",
      "Iteration 1092, loss = 0.25419089\n",
      "Iteration 100, loss = 0.56649064\n",
      "Iteration 101, loss = 0.56367995\n",
      "Iteration 2265, loss = 0.16341937\n",
      "Iteration 1093, loss = 0.25404134\n",
      "Iteration 1129, loss = 0.29011484\n",
      "Iteration 32, loss = 0.67861882\n",
      "Iteration 169, loss = 0.38637664\n",
      "Iteration 102, loss = 0.56083863\n",
      "Iteration 2266, loss = 0.16334620\n",
      "Iteration 103, loss = 0.55803279\n",
      "Iteration 170, loss = 0.38601492\n",
      "Iteration 1183, loss = 0.24149219\n",
      "Iteration 1130, loss = 0.29003942\n",
      "Iteration 171, loss = 0.38570328\n",
      "Iteration 2267, loss = 0.16325035\n",
      "Iteration 104, loss = 0.55511918\n",
      "Iteration 1094, loss = 0.25397656\n",
      "Iteration 33, loss = 0.67519938\n",
      "Iteration 172, loss = 0.38535653\n",
      "Iteration 105, loss = 0.55228152\n",
      "Iteration 2268, loss = 0.16314777\n",
      "Iteration 173, loss = 0.38496881\n",
      "Iteration 1131, loss = 0.29017351\n",
      "Iteration 1095, loss = 0.25386406\n",
      "Iteration 1184, loss = 0.24123109\n",
      "Iteration 106, loss = 0.54948225\n",
      "Iteration 2269, loss = 0.16308249\n",
      "Iteration 34, loss = 0.67186565\n",
      "Iteration 174, loss = 0.38460525\n",
      "Iteration 107, loss = 0.54661868\n",
      "Iteration 2270, loss = 0.16296492\n",
      "Iteration 1096, loss = 0.25353864\n",
      "Iteration 1132, loss = 0.28990963\n",
      "Iteration 108, loss = 0.54380910\n",
      "Iteration 175, loss = 0.38426220\n",
      "Iteration 35, loss = 0.66855974\n",
      "Iteration 109, loss = 0.54103849\n",
      "Iteration 2271, loss = 0.16286837\n",
      "Iteration 1185, loss = 0.24103569\n",
      "Iteration 1097, loss = 0.25344908\n",
      "Iteration 176, loss = 0.38395725\n",
      "Iteration 2272, loss = 0.16281160\n",
      "Iteration 110, loss = 0.53828923\n",
      "Iteration 111, loss = 0.53546540\n",
      "Iteration 36, loss = 0.66510946\n",
      "Iteration 177, loss = 0.38366025\n",
      "Iteration 1098, loss = 0.25329014\n",
      "Iteration 112, loss = 0.53277905\n",
      "Iteration 178, loss = 0.38331530\n",
      "Iteration 2273, loss = 0.16269579\n",
      "Iteration 113, loss = 0.52997361\n",
      "Iteration 1099, loss = 0.25308534\n",
      "Iteration 114, loss = 0.52719363\n",
      "Iteration 1133, loss = 0.28973681\n",
      "Iteration 1186, loss = 0.24087136\n",
      "Iteration 37, loss = 0.66170119\n",
      "Iteration 2274, loss = 0.16261653\n",
      "Iteration 1100, loss = 0.25283923\n",
      "Iteration 115, loss = 0.52456237\n",
      "Iteration 2275, loss = 0.16251277\n",
      "Iteration 1101, loss = 0.25269695\n",
      "Iteration 38, loss = 0.65831164\n",
      "Iteration 116, loss = 0.52194203\n",
      "Iteration 2276, loss = 0.16243255\n",
      "Iteration 179, loss = 0.38294476\n",
      "Iteration 1187, loss = 0.24063141\n",
      "Iteration 369, loss = 0.40360247\n",
      "Iteration 2277, loss = 0.16236781\n",
      "Iteration 1102, loss = 0.25255991\n",
      "Iteration 117, loss = 0.51925732\n",
      "Iteration 118, loss = 0.51663084\n",
      "Iteration 180, loss = 0.38264005\n",
      "Iteration 181, loss = 0.38230913\n",
      "Iteration 1188, loss = 0.24042759\n",
      "Iteration 370, loss = 0.40332196\n",
      "Iteration 2278, loss = 0.16223124\n",
      "Iteration 119, loss = 0.51399972\n",
      "Iteration 1134, loss = 0.28961544\n",
      "Iteration 39, loss = 0.65488508\n",
      "Iteration 120, loss = 0.51132488\n",
      "Iteration 1103, loss = 0.25240438\n",
      "Iteration 121, loss = 0.50892462\n",
      "Iteration 182, loss = 0.38201043\n",
      "Iteration 1135, loss = 0.28948246\n",
      "Iteration 1104, loss = 0.25231709\n",
      "Iteration 1189, loss = 0.24044510\n",
      "Iteration 122, loss = 0.50630047\n",
      "Iteration 183, loss = 0.38174668\n",
      "Iteration 1105, loss = 0.25203569\n",
      "Iteration 184, loss = 0.38141348\n",
      "Iteration 2279, loss = 0.16215206\n",
      "Iteration 1136, loss = 0.28940893\n",
      "Iteration 123, loss = 0.50389928\n",
      "Iteration 371, loss = 0.40314820\n",
      "Iteration 124, loss = 0.50139478\n",
      "Iteration 1190, loss = 0.24018222\n",
      "Iteration 125, loss = 0.49900327\n",
      "Iteration 2280, loss = 0.16207673\n",
      "Iteration 1137, loss = 0.28926850\n",
      "Iteration 2281, loss = 0.16194324\n",
      "Iteration 185, loss = 0.38112057\n",
      "Iteration 126, loss = 0.49657255\n",
      "Iteration 1106, loss = 0.25185217\n",
      "Iteration 2282, loss = 0.16189319\n",
      "Iteration 40, loss = 0.65149048\n",
      "Iteration 186, loss = 0.38079623\n",
      "Iteration 127, loss = 0.49418704\n",
      "Iteration 1107, loss = 0.25168650\n",
      "Iteration 128, loss = 0.49190469\n",
      "Iteration 1191, loss = 0.23991841\n",
      "Iteration 187, loss = 0.38052225\n",
      "Iteration 41, loss = 0.64806575\n",
      "Iteration 129, loss = 0.48955874\n",
      "Iteration 1138, loss = 0.28930266\n",
      "Iteration 2283, loss = 0.16179050\n",
      "Iteration 130, loss = 0.48729470\n",
      "Iteration 372, loss = 0.40294465\n",
      "Iteration 188, loss = 0.38026051\n",
      "Iteration 131, loss = 0.48503561\n",
      "Iteration 1108, loss = 0.25154659\n",
      "Iteration 132, loss = 0.48285243\n",
      "Iteration 189, loss = 0.37994434\n",
      "Iteration 133, loss = 0.48068316\n",
      "Iteration 1109, loss = 0.25141009\n",
      "Iteration 1192, loss = 0.23988729\n",
      "Iteration 2284, loss = 0.16169945\n",
      "Iteration 134, loss = 0.47852235\n",
      "Iteration 190, loss = 0.37970819\n",
      "Iteration 135, loss = 0.47648612\n",
      "Iteration 1110, loss = 0.25118829\n",
      "Iteration 191, loss = 0.37940605\n",
      "Iteration 136, loss = 0.47446526\n",
      "Iteration 373, loss = 0.40278322\n",
      "Iteration 2285, loss = 0.16162694\n",
      "Iteration 192, loss = 0.37913185\n",
      "Iteration 137, loss = 0.47236465\n",
      "Iteration 42, loss = 0.64462316\n",
      "Iteration 2286, loss = 0.16151944\n",
      "Iteration 1139, loss = 0.28922922\n",
      "Iteration 193, loss = 0.37890896\n",
      "Iteration 138, loss = 0.47043903\n",
      "Iteration 1111, loss = 0.25116472\n",
      "Iteration 139, loss = 0.46846252\n",
      "Iteration 43, loss = 0.64118220\n",
      "Iteration 2287, loss = 0.16144050\n",
      "Iteration 194, loss = 0.37859353\n",
      "Iteration 1140, loss = 0.28900117\n",
      "Iteration 140, loss = 0.46665436\n",
      "Iteration 1193, loss = 0.23951996\n",
      "Iteration 141, loss = 0.46474361\n",
      "Iteration 2288, loss = 0.16135093\n",
      "Iteration 195, loss = 0.37833709\n",
      "Iteration 374, loss = 0.40253869\n",
      "Iteration 2289, loss = 0.16136523\n",
      "Iteration 1112, loss = 0.25095215\n",
      "Iteration 142, loss = 0.46286666\n",
      "Iteration 44, loss = 0.63767199\n",
      "Iteration 2290, loss = 0.16116839\n",
      "Iteration 1141, loss = 0.28884254\n",
      "Iteration 143, loss = 0.46113227\n",
      "Iteration 45, loss = 0.63428315\n",
      "Iteration 2291, loss = 0.16107181\n",
      "Iteration 196, loss = 0.37811654\n",
      "Iteration 1194, loss = 0.23944906\n",
      "Iteration 144, loss = 0.45945797\n",
      "Iteration 1113, loss = 0.25076697\n",
      "Iteration 375, loss = 0.40235747\n",
      "Iteration 145, loss = 0.45776025\n",
      "Iteration 1142, loss = 0.28875397\n",
      "Iteration 46, loss = 0.63082898\n",
      "Iteration 2292, loss = 0.16103533\n",
      "Iteration 146, loss = 0.45602688\n",
      "Iteration 2293, loss = 0.16093234\n",
      "Iteration 147, loss = 0.45443497\n",
      "Iteration 197, loss = 0.37782187\n",
      "Iteration 47, loss = 0.62742619\n",
      "Iteration 1114, loss = 0.25051951\n",
      "Iteration 148, loss = 0.45282100\n",
      "Iteration 1115, loss = 0.25040346\n",
      "Iteration 198, loss = 0.37752821\n",
      "Iteration 1143, loss = 0.28866068\n",
      "Iteration 149, loss = 0.45134253\n",
      "Iteration 376, loss = 0.40213848\n",
      "Iteration 2294, loss = 0.16077354\n",
      "Iteration 1195, loss = 0.23907470\n",
      "Iteration 1116, loss = 0.25020398\n",
      "Iteration 48, loss = 0.62386569\n",
      "Iteration 2295, loss = 0.16077363\n",
      "Iteration 199, loss = 0.37728314\n",
      "Iteration 2296, loss = 0.16066004\n",
      "Iteration 200, loss = 0.37705747\n",
      "Iteration 2297, loss = 0.16051688\n",
      "Iteration 49, loss = 0.62049691\n",
      "Iteration 201, loss = 0.37679284\n",
      "Iteration 2298, loss = 0.16043338\n",
      "Iteration 202, loss = 0.37654752\n",
      "Iteration 1144, loss = 0.28853912\n",
      "Iteration 50, loss = 0.61695196\n",
      "Iteration 2299, loss = 0.16031494\n",
      "Iteration 1196, loss = 0.23894641\n",
      "Iteration 203, loss = 0.37640425\n",
      "Iteration 150, loss = 0.44980695\n",
      "Iteration 2300, loss = 0.16041992\n",
      "Iteration 1145, loss = 0.28843599\n",
      "Iteration 51, loss = 0.61350023\n",
      "Iteration 2301, loss = 0.16018988\n",
      "Iteration 151, loss = 0.44826697\n",
      "Iteration 204, loss = 0.37613198\n",
      "Iteration 152, loss = 0.44684577\n",
      "Iteration 1117, loss = 0.25004403\n",
      "Iteration 153, loss = 0.44537429\n",
      "Iteration 2302, loss = 0.16006629\n",
      "Iteration 205, loss = 0.37584243\n",
      "Iteration 154, loss = 0.44398251\n",
      "Iteration 377, loss = 0.40194976\n",
      "Iteration 155, loss = 0.44262580\n",
      "Iteration 156, loss = 0.44135792\n",
      "Iteration 1118, loss = 0.24989661\n",
      "Iteration 2303, loss = 0.15997196\n",
      "Iteration 1146, loss = 0.28830574\n",
      "Iteration 206, loss = 0.37564883\n",
      "Iteration 52, loss = 0.60999829\n",
      "Iteration 157, loss = 0.44004772\n",
      "Iteration 207, loss = 0.37536536\n",
      "Iteration 1197, loss = 0.23879968\n",
      "Iteration 2304, loss = 0.15986066\n",
      "Iteration 1119, loss = 0.24971956\n",
      "Iteration 158, loss = 0.43875242\n",
      "Iteration 378, loss = 0.40177894\n",
      "Iteration 159, loss = 0.43753114\n",
      "Iteration 208, loss = 0.37510543\n",
      "Iteration 1147, loss = 0.28821460\n",
      "Iteration 2305, loss = 0.15990393\n",
      "Iteration 160, loss = 0.43627370\n",
      "Iteration 161, loss = 0.43513502\n",
      "Iteration 2306, loss = 0.15974431\n",
      "Iteration 53, loss = 0.60663932\n",
      "Iteration 1120, loss = 0.24960285\n",
      "Iteration 162, loss = 0.43393628\n",
      "Iteration 2307, loss = 0.15974357\n",
      "Iteration 379, loss = 0.40156071\n",
      "Iteration 163, loss = 0.43284616\n",
      "Iteration 164, loss = 0.43172603\n",
      "Iteration 209, loss = 0.37494299\n",
      "Iteration 1148, loss = 0.28808162\n",
      "Iteration 1121, loss = 0.24943909\n",
      "Iteration 210, loss = 0.37465617\n",
      "Iteration 54, loss = 0.60310875\n",
      "Iteration 1122, loss = 0.24954009\n",
      "Iteration 380, loss = 0.40144956\n",
      "Iteration 211, loss = 0.37441793\n",
      "Iteration 1149, loss = 0.28805054\n",
      "Iteration 165, loss = 0.43063112\n",
      "Iteration 212, loss = 0.37419566\n",
      "Iteration 1198, loss = 0.23862179\n",
      "Iteration 166, loss = 0.42951316Iteration 1123, loss = 0.24901662\n",
      "Iteration 213, loss = 0.37399638\n",
      "Iteration 1150, loss = 0.28789372\n",
      "\n",
      "Iteration 381, loss = 0.40118590\n",
      "Iteration 55, loss = 0.59972829\n",
      "Iteration 167, loss = 0.42855929\n",
      "Iteration 168, loss = 0.42748929\n",
      "Iteration 2308, loss = 0.15953349\n",
      "Iteration 169, loss = 0.42645934\n",
      "Iteration 170, loss = 0.42554446\n",
      "Iteration 2309, loss = 0.15950462\n",
      "Iteration 1151, loss = 0.28785049\n",
      "Iteration 1124, loss = 0.24891902\n",
      "Iteration 171, loss = 0.42446949\n",
      "Iteration 382, loss = 0.40097901\n",
      "Iteration 172, loss = 0.42360073\n",
      "Iteration 173, loss = 0.42265636\n",
      "Iteration 56, loss = 0.59629527\n",
      "Iteration 2310, loss = 0.15932775\n",
      "Iteration 1199, loss = 0.23843103\n",
      "Iteration 174, loss = 0.42170618\n",
      "Iteration 1125, loss = 0.24877708\n",
      "Iteration 175, loss = 0.42090584\n",
      "Iteration 2311, loss = 0.15940463\n",
      "Iteration 176, loss = 0.42005096\n",
      "Iteration 214, loss = 0.37376889\n",
      "Iteration 2312, loss = 0.15916231\n",
      "Iteration 383, loss = 0.40077282\n",
      "Iteration 2313, loss = 0.15910784\n",
      "Iteration 1152, loss = 0.28769064\n",
      "Iteration 177, loss = 0.41912102\n",
      "Iteration 215, loss = 0.37353760\n",
      "Iteration 178, loss = 0.41822214\n",
      "Iteration 2314, loss = 0.15898680\n",
      "Iteration 57, loss = 0.59285794\n",
      "Iteration 179, loss = 0.41742284\n",
      "Iteration 1126, loss = 0.24856246\n",
      "Iteration 180, loss = 0.41657414\n",
      "Iteration 2315, loss = 0.15886934\n",
      "Iteration 181, loss = 0.41575168\n",
      "Iteration 2316, loss = 0.15875349\n",
      "Iteration 1127, loss = 0.24839874\n",
      "Iteration 384, loss = 0.40064796\n",
      "Iteration 182, loss = 0.41495600\n",
      "Iteration 216, loss = 0.37337004\n",
      "Iteration 58, loss = 0.58941605\n",
      "Iteration 1200, loss = 0.23818894\n",
      "Iteration 217, loss = 0.37312418\n",
      "Iteration 183, loss = 0.41421141\n",
      "Iteration 218, loss = 0.37290935\n",
      "Iteration 2317, loss = 0.15873981\n",
      "Iteration 59, loss = 0.58600962\n",
      "Iteration 1128, loss = 0.24823686\n",
      "Iteration 1153, loss = 0.28769285\n",
      "Iteration 219, loss = 0.37266524\n",
      "Iteration 2318, loss = 0.15858812\n",
      "Iteration 1201, loss = 0.23815641\n",
      "Iteration 184, loss = 0.41344716\n",
      "Iteration 1129, loss = 0.24814291\n",
      "Iteration 185, loss = 0.41266109\n",
      "Iteration 186, loss = 0.41193519\n",
      "Iteration 60, loss = 0.58270675\n",
      "Iteration 220, loss = 0.37248891\n",
      "Iteration 187, loss = 0.41117106\n",
      "Iteration 385, loss = 0.40037625\n",
      "Iteration 188, loss = 0.41044818\n",
      "Iteration 1202, loss = 0.23775103\n",
      "Iteration 189, loss = 0.40973000\n",
      "Iteration 1130, loss = 0.24788407\n",
      "Iteration 2319, loss = 0.15849486\n",
      "Iteration 61, loss = 0.57923055\n",
      "Iteration 221, loss = 0.37228872\n",
      "Iteration 2320, loss = 0.15852160\n",
      "Iteration 1154, loss = 0.28750305\n",
      "Iteration 1131, loss = 0.24769419\n",
      "Iteration 190, loss = 0.40905325\n",
      "Iteration 222, loss = 0.37208039\n",
      "Iteration 2321, loss = 0.15835810\n",
      "Iteration 191, loss = 0.40836007\n",
      "Iteration 62, loss = 0.57596592\n",
      "Iteration 192, loss = 0.40765866\n",
      "Iteration 223, loss = 0.37187049\n",
      "Iteration 193, loss = 0.40697593\n",
      "Iteration 2322, loss = 0.15836431\n",
      "Iteration 224, loss = 0.37167461\n",
      "Iteration 1132, loss = 0.24764446\n",
      "Iteration 194, loss = 0.40634243\n",
      "Iteration 1203, loss = 0.23756776\n",
      "Iteration 195, loss = 0.40569335\n",
      "Iteration 1155, loss = 0.28735340\n",
      "Iteration 1133, loss = 0.24751638\n",
      "Iteration 386, loss = 0.40027343\n",
      "Iteration 225, loss = 0.37150016\n",
      "Iteration 2323, loss = 0.15819467\n",
      "Iteration 196, loss = 0.40502756\n",
      "Iteration 63, loss = 0.57262791\n",
      "Iteration 226, loss = 0.37126716\n",
      "Iteration 1156, loss = 0.28724868\n",
      "Iteration 1134, loss = 0.24725313\n",
      "Iteration 227, loss = 0.37116826\n",
      "Iteration 197, loss = 0.40438649\n",
      "Iteration 2324, loss = 0.15812298\n",
      "Iteration 1135, loss = 0.24712868\n",
      "Iteration 387, loss = 0.39999625\n",
      "Iteration 198, loss = 0.40375090\n",
      "Iteration 199, loss = 0.40313109\n",
      "Iteration 228, loss = 0.37091345\n",
      "Iteration 64, loss = 0.56935544\n",
      "Iteration 1136, loss = 0.24707129\n",
      "Iteration 200, loss = 0.40254933\n",
      "Iteration 201, loss = 0.40192737\n",
      "Iteration 2325, loss = 0.15801007\n",
      "Iteration 1137, loss = 0.24682072\n",
      "Iteration 65, loss = 0.56613954\n",
      "Iteration 229, loss = 0.37068552\n",
      "Iteration 1157, loss = 0.28714001\n",
      "Iteration 202, loss = 0.40133933\n",
      "Iteration 2326, loss = 0.15794628\n",
      "Iteration 1204, loss = 0.23751791\n",
      "Iteration 203, loss = 0.40073761\n",
      "Iteration 204, loss = 0.40023877\n",
      "Iteration 66, loss = 0.56280627\n",
      "Iteration 230, loss = 0.37053995\n",
      "Iteration 205, loss = 0.39959851\n",
      "Iteration 2327, loss = 0.15778492\n",
      "Iteration 206, loss = 0.39903782\n",
      "Iteration 388, loss = 0.39981650\n",
      "Iteration 1138, loss = 0.24653246\n",
      "Iteration 2328, loss = 0.15772009\n",
      "Iteration 207, loss = 0.39849181\n",
      "Iteration 1205, loss = 0.23716563\n",
      "Iteration 1158, loss = 0.28702459\n",
      "Iteration 231, loss = 0.37032364\n",
      "Iteration 1139, loss = 0.24639989\n",
      "Iteration 208, loss = 0.39792198\n",
      "Iteration 2329, loss = 0.15765519\n",
      "Iteration 67, loss = 0.55964948\n",
      "Iteration 232, loss = 0.37015082\n",
      "Iteration 1140, loss = 0.24630020\n",
      "Iteration 209, loss = 0.39739762\n",
      "Iteration 233, loss = 0.36994888\n",
      "Iteration 2330, loss = 0.15758181\n",
      "Iteration 1159, loss = 0.28691805\n",
      "Iteration 210, loss = 0.39687628\n",
      "Iteration 68, loss = 0.55646407\n",
      "Iteration 389, loss = 0.39964579\n",
      "Iteration 211, loss = 0.39633127\n",
      "Iteration 234, loss = 0.36977051\n",
      "Iteration 1141, loss = 0.24607582\n",
      "Iteration 212, loss = 0.39585627\n",
      "Iteration 213, loss = 0.39527179\n",
      "Iteration 2331, loss = 0.15749429\n",
      "Iteration 235, loss = 0.36963649\n",
      "Iteration 214, loss = 0.39473601\n",
      "Iteration 1142, loss = 0.24593429\n",
      "Iteration 236, loss = 0.36940134\n",
      "Iteration 2332, loss = 0.15736450\n",
      "Iteration 215, loss = 0.39422941\n",
      "Iteration 1160, loss = 0.28681504\n",
      "Iteration 2333, loss = 0.15730140\n",
      "Iteration 1143, loss = 0.24582143\n",
      "Iteration 1206, loss = 0.23716069\n",
      "Iteration 390, loss = 0.39946235\n",
      "Iteration 216, loss = 0.39373379\n",
      "Iteration 237, loss = 0.36926453\n",
      "Iteration 217, loss = 0.39324343\n",
      "Iteration 218, loss = 0.39269458\n",
      "Iteration 1144, loss = 0.24557850\n",
      "Iteration 69, loss = 0.55342490\n",
      "Iteration 2334, loss = 0.15723438\n",
      "Iteration 219, loss = 0.39220970\n",
      "Iteration 220, loss = 0.39173077\n",
      "Iteration 2335, loss = 0.15713395\n",
      "Iteration 238, loss = 0.36904859\n",
      "Iteration 221, loss = 0.39123594\n",
      "Iteration 1207, loss = 0.23690612\n",
      "Iteration 2336, loss = 0.15700533\n",
      "Iteration 222, loss = 0.39076717\n",
      "Iteration 239, loss = 0.36887218\n",
      "Iteration 223, loss = 0.39027461\n",
      "Iteration 1145, loss = 0.24543599\n",
      "Iteration 224, loss = 0.38981185\n",
      "Iteration 391, loss = 0.39928726\n",
      "Iteration 2337, loss = 0.15694224\n",
      "Iteration 225, loss = 0.38937601\n",
      "Iteration 240, loss = 0.36871244\n",
      "Iteration 226, loss = 0.38887270\n",
      "Iteration 70, loss = 0.55024484\n",
      "Iteration 1146, loss = 0.24537808\n",
      "Iteration 241, loss = 0.36857601\n",
      "Iteration 227, loss = 0.38841407\n",
      "Iteration 71, loss = 0.54717375\n",
      "Iteration 242, loss = 0.36835552\n",
      "Iteration 392, loss = 0.39909513\n",
      "Iteration 1208, loss = 0.23659717\n",
      "Iteration 228, loss = 0.38798159\n",
      "Iteration 243, loss = 0.36819222\n",
      "Iteration 72, loss = 0.54415920\n",
      "Iteration 1161, loss = 0.28674293\n",
      "Iteration 244, loss = 0.36805381\n",
      "Iteration 229, loss = 0.38747683\n",
      "Iteration 1209, loss = 0.23645086\n",
      "Iteration 230, loss = 0.38705895\n",
      "Iteration 2338, loss = 0.15689384\n",
      "Iteration 73, loss = 0.54122832\n",
      "Iteration 231, loss = 0.38655697\n",
      "Iteration 245, loss = 0.36786107\n",
      "Iteration 2339, loss = 0.15681171\n",
      "Iteration 393, loss = 0.39886609\n",
      "Iteration 1147, loss = 0.24512172\n",
      "Iteration 74, loss = 0.53825326\n",
      "Iteration 232, loss = 0.38616214\n",
      "Iteration 1162, loss = 0.28659531\n",
      "Iteration 1148, loss = 0.24499814\n",
      "Iteration 2340, loss = 0.15669400\n",
      "Iteration 233, loss = 0.38569894\n",
      "Iteration 1210, loss = 0.23625180\n",
      "Iteration 2341, loss = 0.15657391\n",
      "Iteration 1149, loss = 0.24478829\n",
      "Iteration 246, loss = 0.36772934\n",
      "Iteration 234, loss = 0.38527296\n",
      "Iteration 1163, loss = 0.28658147\n",
      "Iteration 394, loss = 0.39867115\n",
      "Iteration 235, loss = 0.38482594\n",
      "Iteration 2342, loss = 0.15649226\n",
      "Iteration 236, loss = 0.38440189\n",
      "Iteration 1150, loss = 0.24461934\n",
      "Iteration 247, loss = 0.36754010\n",
      "Iteration 2343, loss = 0.15637669\n",
      "Iteration 75, loss = 0.53529663\n",
      "Iteration 237, loss = 0.38394257\n",
      "Iteration 1164, loss = 0.28636369\n",
      "Iteration 248, loss = 0.36738794\n",
      "Iteration 1211, loss = 0.23613551\n",
      "Iteration 1151, loss = 0.24441535\n",
      "Iteration 2344, loss = 0.15631692\n",
      "Iteration 238, loss = 0.38362708\n",
      "Iteration 249, loss = 0.36718811\n",
      "Iteration 1165, loss = 0.28627177\n",
      "Iteration 239, loss = 0.38314016\n",
      "Iteration 2345, loss = 0.15620824\n",
      "Iteration 395, loss = 0.39860460\n",
      "Iteration 1152, loss = 0.24425848\n",
      "Iteration 250, loss = 0.36712575\n",
      "Iteration 76, loss = 0.53248365\n",
      "Iteration 2346, loss = 0.15616667\n",
      "Iteration 240, loss = 0.38274480\n",
      "Iteration 2347, loss = 0.15608114\n",
      "Iteration 1153, loss = 0.24417082\n",
      "Iteration 251, loss = 0.36685054\n",
      "Iteration 1212, loss = 0.23602745\n",
      "Iteration 77, loss = 0.52964094\n",
      "Iteration 241, loss = 0.38229741\n",
      "Iteration 2348, loss = 0.15607036\n",
      "Iteration 1154, loss = 0.24390463\n",
      "Iteration 242, loss = 0.38188704\n",
      "Iteration 2349, loss = 0.15588825\n",
      "Iteration 78, loss = 0.52693886\n",
      "Iteration 252, loss = 0.36671945\n",
      "Iteration 1166, loss = 0.28615475\n",
      "Iteration 2350, loss = 0.15577839\n",
      "Iteration 1155, loss = 0.24375727\n",
      "Iteration 243, loss = 0.38147675\n",
      "Iteration 1213, loss = 0.23570244\n",
      "Iteration 2351, loss = 0.15573210\n",
      "Iteration 1167, loss = 0.28604430\n",
      "Iteration 396, loss = 0.39836282\n",
      "Iteration 253, loss = 0.36659538\n",
      "Iteration 244, loss = 0.38107085\n",
      "Iteration 2352, loss = 0.15565301\n",
      "Iteration 1156, loss = 0.24362892\n",
      "Iteration 245, loss = 0.38067232\n",
      "Iteration 79, loss = 0.52414129\n",
      "Iteration 246, loss = 0.38031410\n",
      "Iteration 2353, loss = 0.15578224\n",
      "Iteration 1168, loss = 0.28594659\n",
      "Iteration 247, loss = 0.37991484\n",
      "Iteration 254, loss = 0.36637711\n",
      "Iteration 2354, loss = 0.15550438\n",
      "Iteration 1157, loss = 0.24348167\n",
      "Iteration 248, loss = 0.37953891\n",
      "Iteration 255, loss = 0.36621665\n",
      "Iteration 1214, loss = 0.23558726\n",
      "Iteration 2355, loss = 0.15537946\n",
      "Iteration 1158, loss = 0.24324992\n",
      "Iteration 80, loss = 0.52146128\n",
      "Iteration 256, loss = 0.36605684\n",
      "Iteration 397, loss = 0.39809748\n",
      "Iteration 2356, loss = 0.15529154\n",
      "Iteration 249, loss = 0.37916264\n",
      "Iteration 1169, loss = 0.28586494\n",
      "Iteration 2357, loss = 0.15519458\n",
      "Iteration 257, loss = 0.36588914\n",
      "Iteration 1159, loss = 0.24321323\n",
      "Iteration 250, loss = 0.37879117\n",
      "Iteration 258, loss = 0.36575305\n",
      "Iteration 251, loss = 0.37839330\n",
      "Iteration 81, loss = 0.51882158\n",
      "Iteration 259, loss = 0.36559216\n",
      "Iteration 2358, loss = 0.15508796\n",
      "Iteration 1215, loss = 0.23541439\n",
      "Iteration 398, loss = 0.39790994\n",
      "Iteration 1160, loss = 0.24290048\n",
      "Iteration 1170, loss = 0.28574357\n",
      "Iteration 252, loss = 0.37804652\n",
      "Iteration 260, loss = 0.36543700\n",
      "Iteration 82, loss = 0.51626418\n",
      "Iteration 1161, loss = 0.24276137\n",
      "Iteration 253, loss = 0.37765179\n",
      "Iteration 261, loss = 0.36527108\n",
      "Iteration 2359, loss = 0.15503438\n",
      "Iteration 1216, loss = 0.23528766\n",
      "Iteration 1162, loss = 0.24267954\n",
      "Iteration 254, loss = 0.37728796\n",
      "Iteration 255, loss = 0.37693961\n",
      "Iteration 262, loss = 0.36511976\n",
      "Iteration 83, loss = 0.51369015\n",
      "Iteration 256, loss = 0.37657183\n",
      "Iteration 399, loss = 0.39774485\n",
      "Iteration 1171, loss = 0.28567495\n",
      "Iteration 1217, loss = 0.23493665\n",
      "Iteration 263, loss = 0.36497910\n",
      "Iteration 1163, loss = 0.24253701\n",
      "Iteration 257, loss = 0.37624966\n",
      "Iteration 264, loss = 0.36481324\n",
      "Iteration 258, loss = 0.37588637\n",
      "Iteration 259, loss = 0.37554542\n",
      "Iteration 265, loss = 0.36466019\n",
      "Iteration 1172, loss = 0.28554413\n",
      "Iteration 84, loss = 0.51114776\n",
      "Iteration 260, loss = 0.37517720\n",
      "Iteration 1218, loss = 0.23484856\n",
      "Iteration 1164, loss = 0.24241924\n",
      "Iteration 261, loss = 0.37488069\n",
      "Iteration 400, loss = 0.39758974\n",
      "Iteration 266, loss = 0.36452509\n",
      "Iteration 262, loss = 0.37454601\n",
      "Iteration 1165, loss = 0.24209472\n",
      "Iteration 85, loss = 0.50872138\n",
      "Iteration 267, loss = 0.36434195\n",
      "Iteration 263, loss = 0.37414850\n",
      "Iteration 1166, loss = 0.24189136\n",
      "Iteration 1173, loss = 0.28542317\n",
      "Iteration 268, loss = 0.36422725\n",
      "Iteration 401, loss = 0.39741727\n",
      "Iteration 86, loss = 0.50623181\n",
      "Iteration 264, loss = 0.37383725\n",
      "Iteration 1219, loss = 0.23455250\n",
      "Iteration 1167, loss = 0.24174516\n",
      "Iteration 265, loss = 0.37349385\n",
      "Iteration 269, loss = 0.36408883\n",
      "Iteration 1174, loss = 0.28532357\n",
      "Iteration 266, loss = 0.37316386\n",
      "Iteration 1168, loss = 0.24160330\n",
      "Iteration 87, loss = 0.50392049\n",
      "Iteration 402, loss = 0.39716074\n",
      "Iteration 267, loss = 0.37285507\n",
      "Iteration 270, loss = 0.36390151\n",
      "Iteration 1175, loss = 0.28519584\n",
      "Iteration 1220, loss = 0.23437759\n",
      "Iteration 268, loss = 0.37255777\n",
      "Iteration 1169, loss = 0.24160751\n",
      "Iteration 271, loss = 0.36376750\n",
      "Iteration 88, loss = 0.50154014\n",
      "Iteration 272, loss = 0.36361543\n",
      "Iteration 269, loss = 0.37221850\n",
      "Iteration 1176, loss = 0.28507800\n",
      "Iteration 270, loss = 0.37187210\n",
      "Iteration 1170, loss = 0.24121386\n",
      "Iteration 271, loss = 0.37157120\n",
      "Iteration 273, loss = 0.36345273\n",
      "Iteration 1221, loss = 0.23419757\n",
      "Iteration 272, loss = 0.37123389\n",
      "Iteration 273, loss = 0.37093237\n",
      "Iteration 403, loss = 0.39695880\n",
      "Iteration 274, loss = 0.36332709\n",
      "Iteration 89, loss = 0.49926536\n",
      "Iteration 274, loss = 0.37061336\n",
      "Iteration 275, loss = 0.36317292\n",
      "Iteration 275, loss = 0.37032773\n",
      "Iteration 1177, loss = 0.28495478\n",
      "Iteration 1222, loss = 0.23393979\n",
      "Iteration 1171, loss = 0.24106402\n",
      "Iteration 276, loss = 0.36999479\n",
      "Iteration 276, loss = 0.36302902\n",
      "Iteration 90, loss = 0.49707237\n",
      "Iteration 277, loss = 0.36290515\n",
      "Iteration 277, loss = 0.36969684\n",
      "Iteration 1178, loss = 0.28488002\n",
      "Iteration 1172, loss = 0.24092679\n",
      "Iteration 404, loss = 0.39677212\n",
      "Iteration 278, loss = 0.36939185\n",
      "Iteration 278, loss = 0.36273613\n",
      "Iteration 279, loss = 0.36909743\n",
      "Iteration 91, loss = 0.49484927\n",
      "Iteration 1173, loss = 0.24071618\n",
      "Iteration 280, loss = 0.36876625\n",
      "Iteration 279, loss = 0.36261220\n",
      "Iteration 281, loss = 0.36848906\n",
      "Iteration 282, loss = 0.36819809\n",
      "Iteration 1223, loss = 0.23422619\n",
      "Iteration 92, loss = 0.49270396\n",
      "Iteration 1174, loss = 0.24056506\n",
      "Iteration 283, loss = 0.36793197\n",
      "Iteration 1179, loss = 0.28476615\n",
      "Iteration 405, loss = 0.39658800\n",
      "Iteration 280, loss = 0.36243984\n",
      "Iteration 1175, loss = 0.24059457\n",
      "Iteration 284, loss = 0.36760106\n",
      "Iteration 281, loss = 0.36231882\n",
      "Iteration 93, loss = 0.49060500\n",
      "Iteration 1176, loss = 0.24038904\n",
      "Iteration 285, loss = 0.36732254\n",
      "Iteration 282, loss = 0.36217522\n",
      "Iteration 1180, loss = 0.28465125\n",
      "Iteration 1224, loss = 0.23366078\n",
      "Iteration 1177, loss = 0.24011230\n",
      "Iteration 286, loss = 0.36704443\n",
      "Iteration 94, loss = 0.48850729\n",
      "Iteration 406, loss = 0.39643764\n",
      "Iteration 283, loss = 0.36201265\n",
      "Iteration 287, loss = 0.36681278\n",
      "Iteration 288, loss = 0.36650623\n",
      "Iteration 1178, loss = 0.23999889\n",
      "Iteration 289, loss = 0.36619044\n",
      "Iteration 284, loss = 0.36188848\n",
      "Iteration 290, loss = 0.36595790\n",
      "Iteration 1181, loss = 0.28454121\n",
      "Iteration 1225, loss = 0.23346306\n",
      "Iteration 285, loss = 0.36175278\n",
      "Iteration 1179, loss = 0.23969971\n",
      "Iteration 291, loss = 0.36569338\n",
      "Iteration 292, loss = 0.36539522\n",
      "Iteration 2360, loss = 0.15493698\n",
      "Iteration 293, loss = 0.36511945\n",
      "Iteration 1180, loss = 0.23957058\n",
      "Iteration 294, loss = 0.36489831\n",
      "Iteration 1182, loss = 0.28446403\n",
      "Iteration 95, loss = 0.48651981\n",
      "Iteration 407, loss = 0.39623352\n",
      "Iteration 286, loss = 0.36163376\n",
      "Iteration 295, loss = 0.36460172\n",
      "Iteration 296, loss = 0.36434985\n",
      "Iteration 297, loss = 0.36408962\n",
      "Iteration 1226, loss = 0.23323218\n",
      "Iteration 1181, loss = 0.23942268\n",
      "Iteration 2361, loss = 0.15487738\n",
      "Iteration 298, loss = 0.36382897\n",
      "Iteration 287, loss = 0.36146162\n",
      "Iteration 408, loss = 0.39603947\n",
      "Iteration 1183, loss = 0.28434722\n",
      "Iteration 288, loss = 0.36134327\n",
      "Iteration 1182, loss = 0.23920685\n",
      "Iteration 299, loss = 0.36356539\n",
      "Iteration 2362, loss = 0.15473238\n",
      "Iteration 1227, loss = 0.23319371\n",
      "Iteration 1183, loss = 0.23902234\n",
      "Iteration 409, loss = 0.39584727\n",
      "Iteration 2363, loss = 0.15465669\n",
      "Iteration 300, loss = 0.36329336\n",
      "Iteration 96, loss = 0.48458295\n",
      "Iteration 1184, loss = 0.23884881\n",
      "Iteration 289, loss = 0.36119028\n",
      "Iteration 2364, loss = 0.15458576\n",
      "Iteration 301, loss = 0.36307579\n",
      "Iteration 1185, loss = 0.23867687\n",
      "Iteration 290, loss = 0.36108050\n",
      "Iteration 2365, loss = 0.15461215\n",
      "Iteration 302, loss = 0.36283199\n",
      "Iteration 291, loss = 0.36090798\n",
      "Iteration 303, loss = 0.36255073\n",
      "Iteration 410, loss = 0.39572348\n",
      "Iteration 304, loss = 0.36228412\n",
      "Iteration 1228, loss = 0.23281383\n",
      "Iteration 292, loss = 0.36079905\n",
      "Iteration 1184, loss = 0.28423412\n",
      "Iteration 2366, loss = 0.15456123\n",
      "Iteration 1186, loss = 0.23862895\n",
      "Iteration 97, loss = 0.48259283\n",
      "Iteration 293, loss = 0.36065923\n",
      "Iteration 305, loss = 0.36209923\n",
      "Iteration 2367, loss = 0.15443341\n",
      "Iteration 411, loss = 0.39549962\n",
      "Iteration 306, loss = 0.36182776\n",
      "Iteration 1185, loss = 0.28411821\n",
      "Iteration 2368, loss = 0.15420163\n",
      "Iteration 307, loss = 0.36159522\n",
      "Iteration 1187, loss = 0.23837911\n",
      "Iteration 2369, loss = 0.15415365\n",
      "Iteration 294, loss = 0.36051493\n",
      "Iteration 1229, loss = 0.23310021\n",
      "Iteration 1188, loss = 0.23817444\n",
      "Iteration 308, loss = 0.36131300\n",
      "Iteration 98, loss = 0.48073953\n",
      "Iteration 2370, loss = 0.15416523\n",
      "Iteration 2371, loss = 0.15411984\n",
      "Iteration 309, loss = 0.36107132\n",
      "Iteration 1186, loss = 0.28406141\n",
      "Iteration 295, loss = 0.36040126\n",
      "Iteration 412, loss = 0.39526990\n",
      "Iteration 1230, loss = 0.23253511\n",
      "Iteration 1189, loss = 0.23819384\n",
      "Iteration 310, loss = 0.36083697\n",
      "Iteration 296, loss = 0.36024828\n",
      "Iteration 2372, loss = 0.15393966\n",
      "Iteration 1190, loss = 0.23792419\n",
      "Iteration 1187, loss = 0.28389501\n",
      "Iteration 99, loss = 0.47893772\n",
      "Iteration 311, loss = 0.36059909\n",
      "Iteration 297, loss = 0.36015337\n",
      "Iteration 298, loss = 0.36002005\n",
      "Iteration 312, loss = 0.36035403\n",
      "Iteration 2373, loss = 0.15380368\n",
      "Iteration 1191, loss = 0.23770158\n",
      "Iteration 313, loss = 0.36013026\n",
      "Iteration 1188, loss = 0.28379464\n",
      "Iteration 299, loss = 0.35983765\n",
      "Iteration 413, loss = 0.39509695\n",
      "Iteration 100, loss = 0.47708710\n",
      "Iteration 2374, loss = 0.15385396\n",
      "Iteration 1231, loss = 0.23234907\n",
      "Iteration 1192, loss = 0.23749535\n",
      "Iteration 314, loss = 0.35989624\n",
      "Iteration 300, loss = 0.35971787\n",
      "Iteration 101, loss = 0.47535094\n",
      "Iteration 315, loss = 0.35966858\n",
      "Iteration 414, loss = 0.39491345\n",
      "Iteration 1193, loss = 0.23735654\n",
      "Iteration 301, loss = 0.35957399\n",
      "Iteration 2375, loss = 0.15368143\n",
      "Iteration 1189, loss = 0.28368812\n",
      "Iteration 316, loss = 0.35948256\n",
      "Iteration 1194, loss = 0.23715270\n",
      "Iteration 302, loss = 0.35944153\n",
      "Iteration 1232, loss = 0.23210363\n",
      "Iteration 102, loss = 0.47362655\n",
      "Iteration 2376, loss = 0.15359908\n",
      "Iteration 317, loss = 0.35920103\n",
      "Iteration 2377, loss = 0.15366469\n",
      "Iteration 303, loss = 0.35935816\n",
      "Iteration 318, loss = 0.35901959\n",
      "Iteration 319, loss = 0.35872944\n",
      "Iteration 415, loss = 0.39477152Iteration 1190, loss = 0.28358554\n",
      "\n",
      "Iteration 1195, loss = 0.23699967\n",
      "Iteration 103, loss = 0.47195789\n",
      "Iteration 304, loss = 0.35919151\n",
      "Iteration 320, loss = 0.35851120\n",
      "Iteration 2378, loss = 0.15349872\n",
      "Iteration 321, loss = 0.35835058\n",
      "Iteration 1233, loss = 0.23206341\n",
      "Iteration 305, loss = 0.35904774\n",
      "Iteration 322, loss = 0.35807412\n",
      "Iteration 323, loss = 0.35782804\n",
      "Iteration 306, loss = 0.35894605\n",
      "Iteration 1196, loss = 0.23688480\n",
      "Iteration 2379, loss = 0.15333131\n",
      "Iteration 307, loss = 0.35883078\n",
      "Iteration 104, loss = 0.47038070\n",
      "Iteration 324, loss = 0.35762231\n",
      "Iteration 2380, loss = 0.15327996\n",
      "Iteration 308, loss = 0.35867441\n",
      "Iteration 1197, loss = 0.23674120\n",
      "Iteration 325, loss = 0.35740137\n",
      "Iteration 2381, loss = 0.15313786\n",
      "Iteration 309, loss = 0.35853780\n",
      "Iteration 326, loss = 0.35722789\n",
      "Iteration 1198, loss = 0.23651258\n",
      "Iteration 416, loss = 0.39453400\n",
      "Iteration 2382, loss = 0.15312340\n",
      "Iteration 310, loss = 0.35841773\n",
      "Iteration 1191, loss = 0.28348880\n",
      "Iteration 327, loss = 0.35697071\n",
      "Iteration 1234, loss = 0.23176991\n",
      "Iteration 311, loss = 0.35831611\n",
      "Iteration 328, loss = 0.35676157\n",
      "Iteration 312, loss = 0.35816346\n",
      "Iteration 417, loss = 0.39449950\n",
      "Iteration 2383, loss = 0.15300205\n",
      "Iteration 1192, loss = 0.28337026\n",
      "Iteration 329, loss = 0.35654300\n",
      "Iteration 105, loss = 0.46877748\n",
      "Iteration 1235, loss = 0.23163724\n",
      "Iteration 330, loss = 0.35631498\n",
      "Iteration 1199, loss = 0.23633836\n",
      "Iteration 331, loss = 0.35609193\n",
      "Iteration 313, loss = 0.35803142\n",
      "Iteration 332, loss = 0.35587253\n",
      "Iteration 2384, loss = 0.15289893\n",
      "Iteration 333, loss = 0.35568062\n",
      "Iteration 1200, loss = 0.23656514\n",
      "Iteration 1193, loss = 0.28334110\n",
      "Iteration 314, loss = 0.35791474\n",
      "Iteration 334, loss = 0.35546263\n",
      "Iteration 335, loss = 0.35533730\n",
      "Iteration 1201, loss = 0.23601192\n",
      "Iteration 315, loss = 0.35778549\n",
      "Iteration 418, loss = 0.39420054\n",
      "Iteration 336, loss = 0.35503499\n",
      "Iteration 337, loss = 0.35482278\n",
      "Iteration 316, loss = 0.35764487\n",
      "Iteration 2385, loss = 0.15278846\n",
      "Iteration 106, loss = 0.46723347\n",
      "Iteration 1202, loss = 0.23586225\n",
      "Iteration 338, loss = 0.35462528\n",
      "Iteration 317, loss = 0.35752553\n",
      "Iteration 2386, loss = 0.15272225\n",
      "Iteration 318, loss = 0.35741410\n",
      "Iteration 339, loss = 0.35441236\n",
      "Iteration 107, loss = 0.46566199\n",
      "Iteration 2387, loss = 0.15267541\n",
      "Iteration 1194, loss = 0.28314742\n",
      "Iteration 2388, loss = 0.15254399\n",
      "Iteration 1203, loss = 0.23565416\n",
      "Iteration 319, loss = 0.35729464\n",
      "Iteration 340, loss = 0.35420798\n",
      "Iteration 341, loss = 0.35403397\n",
      "Iteration 1195, loss = 0.28305117\n",
      "Iteration 320, loss = 0.35718913\n",
      "Iteration 419, loss = 0.39407228\n",
      "Iteration 342, loss = 0.35381622\n",
      "Iteration 1204, loss = 0.23550721\n",
      "Iteration 108, loss = 0.46421588\n",
      "Iteration 2389, loss = 0.15245395\n",
      "Iteration 1236, loss = 0.23138533\n",
      "Iteration 343, loss = 0.35360760\n",
      "Iteration 321, loss = 0.35703894\n",
      "Iteration 1205, loss = 0.23526427\n",
      "Iteration 344, loss = 0.35349829\n",
      "Iteration 345, loss = 0.35321255\n",
      "Iteration 1196, loss = 0.28293671\n",
      "Iteration 2390, loss = 0.15236463\n",
      "Iteration 109, loss = 0.46277760\n",
      "Iteration 346, loss = 0.35302406\n",
      "Iteration 322, loss = 0.35691780\n",
      "Iteration 2391, loss = 0.15241213\n",
      "Iteration 347, loss = 0.35281126\n",
      "Iteration 348, loss = 0.35261075\n",
      "Iteration 1197, loss = 0.28292585\n",
      "Iteration 2392, loss = 0.15225543\n",
      "Iteration 323, loss = 0.35678971\n",
      "Iteration 110, loss = 0.46142411\n",
      "Iteration 420, loss = 0.39386681\n",
      "Iteration 1206, loss = 0.23522019\n",
      "Iteration 324, loss = 0.35668248\n",
      "Iteration 349, loss = 0.35249929\n",
      "Iteration 2393, loss = 0.15211221\n",
      "Iteration 325, loss = 0.35655125\n",
      "Iteration 1237, loss = 0.23123193\n",
      "Iteration 2394, loss = 0.15199905\n",
      "Iteration 1207, loss = 0.23498809\n",
      "Iteration 350, loss = 0.35223077\n",
      "Iteration 2395, loss = 0.15195074\n",
      "Iteration 326, loss = 0.35649699\n",
      "Iteration 351, loss = 0.35203409\n",
      "Iteration 1198, loss = 0.28275514\n",
      "Iteration 1208, loss = 0.23488562\n",
      "Iteration 111, loss = 0.45996651\n",
      "Iteration 352, loss = 0.35184429\n",
      "Iteration 327, loss = 0.35630181\n",
      "Iteration 2396, loss = 0.15186655Iteration 1238, loss = 0.23101162\n",
      "\n",
      "Iteration 421, loss = 0.39374127\n",
      "Iteration 1199, loss = 0.28262295\n",
      "Iteration 353, loss = 0.35164014\n",
      "Iteration 112, loss = 0.45870633\n",
      "Iteration 354, loss = 0.35146071\n",
      "Iteration 2397, loss = 0.15178406\n",
      "Iteration 355, loss = 0.35127712\n",
      "Iteration 1200, loss = 0.28251970\n",
      "Iteration 2398, loss = 0.15169421\n",
      "Iteration 356, loss = 0.35109527\n",
      "Iteration 1239, loss = 0.23092809\n",
      "Iteration 328, loss = 0.35621861\n",
      "Iteration 1209, loss = 0.23457767\n",
      "Iteration 113, loss = 0.45738634\n",
      "Iteration 2399, loss = 0.15165134\n",
      "Iteration 1210, loss = 0.23446995\n",
      "Iteration 357, loss = 0.35092250\n",
      "Iteration 329, loss = 0.35607049\n",
      "Iteration 1201, loss = 0.28240919\n",
      "Iteration 330, loss = 0.35604046\n",
      "Iteration 1211, loss = 0.23434664\n",
      "Iteration 1240, loss = 0.23060154\n",
      "Iteration 2400, loss = 0.15152455\n",
      "Iteration 358, loss = 0.35068303\n",
      "Iteration 114, loss = 0.45608103\n",
      "Iteration 1212, loss = 0.23414388\n",
      "Iteration 359, loss = 0.35052355\n",
      "Iteration 2401, loss = 0.15147204\n",
      "Iteration 1213, loss = 0.23397865\n",
      "Iteration 422, loss = 0.39348007\n",
      "Iteration 331, loss = 0.35585181\n",
      "Iteration 115, loss = 0.45487253\n",
      "Iteration 360, loss = 0.35035284\n",
      "Iteration 1241, loss = 0.23044465\n",
      "Iteration 332, loss = 0.35572774\n",
      "Iteration 1214, loss = 0.23381062\n",
      "Iteration 116, loss = 0.45362528\n",
      "Iteration 333, loss = 0.35562577\n",
      "Iteration 1215, loss = 0.23358695\n",
      "Iteration 334, loss = 0.35549461\n",
      "Iteration 1242, loss = 0.23024584\n",
      "Iteration 117, loss = 0.45248847\n",
      "Iteration 1216, loss = 0.23336601\n",
      "Iteration 335, loss = 0.35539447\n",
      "Iteration 361, loss = 0.35015823\n",
      "Iteration 1202, loss = 0.28232361\n",
      "Iteration 362, loss = 0.34998311\n",
      "Iteration 2402, loss = 0.15137024\n",
      "Iteration 1217, loss = 0.23352672\n",
      "Iteration 363, loss = 0.34976449\n",
      "Iteration 336, loss = 0.35526580\n",
      "Iteration 118, loss = 0.45122629\n",
      "Iteration 364, loss = 0.34959852\n",
      "Iteration 2403, loss = 0.15129316\n",
      "Iteration 1243, loss = 0.23005247\n",
      "Iteration 1203, loss = 0.28222708\n",
      "Iteration 1218, loss = 0.23313024\n",
      "Iteration 337, loss = 0.35516254\n",
      "Iteration 423, loss = 0.39327330\n",
      "Iteration 365, loss = 0.34942058\n",
      "Iteration 366, loss = 0.34922039\n",
      "Iteration 119, loss = 0.45006588\n",
      "Iteration 1244, loss = 0.22988236\n",
      "Iteration 338, loss = 0.35508819\n",
      "Iteration 367, loss = 0.34904686\n",
      "Iteration 2404, loss = 0.15124003\n",
      "Iteration 1219, loss = 0.23286524\n",
      "Iteration 120, loss = 0.44903535\n",
      "Iteration 1204, loss = 0.28212494\n",
      "Iteration 368, loss = 0.34884452\n",
      "Iteration 2405, loss = 0.15110439\n",
      "Iteration 424, loss = 0.39336834\n",
      "Iteration 339, loss = 0.35493329\n",
      "Iteration 121, loss = 0.44788465\n",
      "Iteration 1205, loss = 0.28198260\n",
      "Iteration 2406, loss = 0.15102931\n",
      "Iteration 369, loss = 0.34868651\n",
      "Iteration 1220, loss = 0.23270478\n",
      "Iteration 370, loss = 0.34851756\n",
      "Iteration 2407, loss = 0.15113157\n",
      "Iteration 340, loss = 0.35483349\n",
      "Iteration 371, loss = 0.34830753\n",
      "Iteration 1245, loss = 0.22967994\n",
      "Iteration 122, loss = 0.44682593\n",
      "Iteration 372, loss = 0.34814278\n",
      "Iteration 2408, loss = 0.15102850\n",
      "Iteration 373, loss = 0.34797933\n",
      "Iteration 341, loss = 0.35470972\n",
      "Iteration 374, loss = 0.34778459\n",
      "Iteration 2409, loss = 0.15081237\n",
      "Iteration 1206, loss = 0.28197755\n",
      "Iteration 375, loss = 0.34760633\n",
      "Iteration 342, loss = 0.35462359\n",
      "Iteration 376, loss = 0.34742531\n",
      "Iteration 1221, loss = 0.23253967\n",
      "Iteration 425, loss = 0.39288389\n",
      "Iteration 123, loss = 0.44576102\n",
      "Iteration 377, loss = 0.34724872\n",
      "Iteration 343, loss = 0.35449213\n",
      "Iteration 2410, loss = 0.15073079\n",
      "Iteration 1246, loss = 0.22960348\n",
      "Iteration 1222, loss = 0.23237759\n",
      "Iteration 378, loss = 0.34710846\n",
      "Iteration 379, loss = 0.34690676\n",
      "Iteration 344, loss = 0.35439486\n",
      "Iteration 2411, loss = 0.15060487\n",
      "Iteration 1223, loss = 0.23221268\n",
      "Iteration 380, loss = 0.34672381\n",
      "Iteration 1207, loss = 0.28184714\n",
      "Iteration 381, loss = 0.34654338\n",
      "Iteration 426, loss = 0.39270659\n",
      "Iteration 345, loss = 0.35426977\n",
      "Iteration 346, loss = 0.35416174\n",
      "Iteration 382, loss = 0.34639761\n",
      "Iteration 124, loss = 0.44478250\n",
      "Iteration 1247, loss = 0.22929110\n",
      "Iteration 2412, loss = 0.15066449\n",
      "Iteration 383, loss = 0.34622764\n",
      "Iteration 347, loss = 0.35404599\n",
      "Iteration 384, loss = 0.34606279\n",
      "Iteration 1208, loss = 0.28166708\n",
      "Iteration 385, loss = 0.34590275\n",
      "Iteration 386, loss = 0.34579386\n",
      "Iteration 125, loss = 0.44377958\n",
      "Iteration 1224, loss = 0.23217414\n",
      "Iteration 387, loss = 0.34550826\n",
      "Iteration 2413, loss = 0.15043699\n",
      "Iteration 388, loss = 0.34535665\n",
      "Iteration 389, loss = 0.34523215\n",
      "Iteration 348, loss = 0.35396495\n",
      "Iteration 427, loss = 0.39258276\n",
      "Iteration 1248, loss = 0.22929010\n",
      "Iteration 1225, loss = 0.23183464\n",
      "Iteration 390, loss = 0.34503077\n",
      "Iteration 2414, loss = 0.15037788\n",
      "Iteration 1226, loss = 0.23168243\n",
      "Iteration 1209, loss = 0.28158002\n",
      "Iteration 2415, loss = 0.15029990\n",
      "Iteration 349, loss = 0.35386516\n",
      "Iteration 391, loss = 0.34486624\n",
      "Iteration 126, loss = 0.44276687\n",
      "Iteration 2416, loss = 0.15018079\n",
      "Iteration 1210, loss = 0.28152255\n",
      "Iteration 392, loss = 0.34468304\n",
      "Iteration 1249, loss = 0.22897788\n",
      "Iteration 350, loss = 0.35372240\n",
      "Iteration 393, loss = 0.34451523\n",
      "Iteration 1227, loss = 0.23166589\n",
      "Iteration 394, loss = 0.34437504\n",
      "Iteration 127, loss = 0.44178808\n",
      "Iteration 2417, loss = 0.15023610\n",
      "Iteration 395, loss = 0.34419169\n",
      "Iteration 428, loss = 0.39235005\n",
      "Iteration 351, loss = 0.35364322\n",
      "Iteration 396, loss = 0.34402603\n",
      "Iteration 397, loss = 0.34387726\n",
      "Iteration 1211, loss = 0.28136061\n",
      "Iteration 352, loss = 0.35349914\n",
      "Iteration 1250, loss = 0.22879088\n",
      "Iteration 1228, loss = 0.23133810\n",
      "Iteration 2418, loss = 0.15000963\n",
      "Iteration 398, loss = 0.34369823\n",
      "Iteration 353, loss = 0.35339899\n",
      "Iteration 399, loss = 0.34352680\n",
      "Iteration 1212, loss = 0.28128117\n",
      "Iteration 354, loss = 0.35327477\n",
      "Iteration 128, loss = 0.44090240\n",
      "Iteration 400, loss = 0.34338390\n",
      "Iteration 355, loss = 0.35318516\n",
      "Iteration 2419, loss = 0.15005587\n",
      "Iteration 1229, loss = 0.23120097\n",
      "Iteration 1213, loss = 0.28118435\n",
      "Iteration 401, loss = 0.34318642\n",
      "Iteration 429, loss = 0.39223098\n",
      "Iteration 356, loss = 0.35309120\n",
      "Iteration 402, loss = 0.34304614\n",
      "Iteration 2420, loss = 0.14993597\n",
      "Iteration 403, loss = 0.34288597\n",
      "Iteration 129, loss = 0.43995514\n",
      "Iteration 404, loss = 0.34271338\n",
      "Iteration 2421, loss = 0.14979533\n",
      "Iteration 1214, loss = 0.28102564\n",
      "Iteration 405, loss = 0.34262224\n",
      "Iteration 406, loss = 0.34239313\n",
      "Iteration 2422, loss = 0.14971238\n",
      "Iteration 1251, loss = 0.22881689\n",
      "Iteration 407, loss = 0.34222894\n",
      "Iteration 408, loss = 0.34205445\n",
      "Iteration 1215, loss = 0.28093411\n",
      "Iteration 2423, loss = 0.14963158\n",
      "Iteration 409, loss = 0.34192332\n",
      "Iteration 430, loss = 0.39197719\n",
      "Iteration 1230, loss = 0.23109066\n",
      "Iteration 410, loss = 0.34188301\n",
      "Iteration 2424, loss = 0.14958327\n",
      "Iteration 130, loss = 0.43910846\n",
      "Iteration 411, loss = 0.34163558\n",
      "Iteration 1231, loss = 0.23102267\n",
      "Iteration 1252, loss = 0.22839970\n",
      "Iteration 357, loss = 0.35297674\n",
      "Iteration 412, loss = 0.34143920\n",
      "Iteration 2425, loss = 0.14962283\n",
      "Iteration 131, loss = 0.43819040\n",
      "Iteration 413, loss = 0.34130202\n",
      "Iteration 358, loss = 0.35287159\n",
      "Iteration 1232, loss = 0.23061645\n",
      "Iteration 1216, loss = 0.28095531\n",
      "Iteration 2426, loss = 0.14942632\n",
      "Iteration 359, loss = 0.35275525\n",
      "Iteration 1233, loss = 0.23043075\n",
      "Iteration 2427, loss = 0.14934268\n",
      "Iteration 414, loss = 0.34122314\n",
      "Iteration 431, loss = 0.39178259\n",
      "Iteration 415, loss = 0.34102838\n",
      "Iteration 2428, loss = 0.14927509\n",
      "Iteration 360, loss = 0.35265894\n",
      "Iteration 416, loss = 0.34082883\n",
      "Iteration 1217, loss = 0.28078434\n",
      "Iteration 132, loss = 0.43732282\n",
      "Iteration 417, loss = 0.34066314\n",
      "Iteration 2429, loss = 0.14916446\n",
      "Iteration 1234, loss = 0.23032073\n",
      "Iteration 1253, loss = 0.22847753\n",
      "Iteration 418, loss = 0.34051272\n",
      "Iteration 133, loss = 0.43650145\n",
      "Iteration 2430, loss = 0.14917654\n",
      "Iteration 1218, loss = 0.28062192\n",
      "Iteration 361, loss = 0.35256125\n",
      "Iteration 419, loss = 0.34032865\n",
      "Iteration 1235, loss = 0.23013358\n",
      "Iteration 2431, loss = 0.14896863\n",
      "Iteration 134, loss = 0.43565989\n",
      "Iteration 420, loss = 0.34020441\n",
      "Iteration 2432, loss = 0.14886918\n",
      "Iteration 1236, loss = 0.22999151\n",
      "Iteration 432, loss = 0.39166295\n",
      "Iteration 1254, loss = 0.22800226\n",
      "Iteration 362, loss = 0.35245161\n",
      "Iteration 1219, loss = 0.28047310\n",
      "Iteration 421, loss = 0.34001829\n",
      "Iteration 422, loss = 0.33984113\n",
      "Iteration 1237, loss = 0.22982973\n",
      "Iteration 135, loss = 0.43487512\n",
      "Iteration 2433, loss = 0.14881045\n",
      "Iteration 423, loss = 0.33969067\n",
      "Iteration 1255, loss = 0.22786016\n",
      "Iteration 363, loss = 0.35238088\n",
      "Iteration 1238, loss = 0.22972652\n",
      "Iteration 2434, loss = 0.14868877\n",
      "Iteration 424, loss = 0.33952651\n",
      "Iteration 136, loss = 0.43412353\n",
      "Iteration 364, loss = 0.35231320\n",
      "Iteration 2435, loss = 0.14865911\n",
      "Iteration 1239, loss = 0.22941440\n",
      "Iteration 433, loss = 0.39141763\n",
      "Iteration 365, loss = 0.35215516\n",
      "Iteration 425, loss = 0.33937807\n",
      "Iteration 1240, loss = 0.22930609\n",
      "Iteration 426, loss = 0.33922793\n",
      "Iteration 1256, loss = 0.22787642\n",
      "Iteration 427, loss = 0.33910280\n",
      "Iteration 2436, loss = 0.14861089\n",
      "Iteration 434, loss = 0.39126078\n",
      "Iteration 1241, loss = 0.22925103\n",
      "Iteration 1220, loss = 0.28040270\n",
      "Iteration 2437, loss = 0.14848685\n",
      "Iteration 366, loss = 0.35204704\n",
      "Iteration 428, loss = 0.33891699\n",
      "Iteration 1257, loss = 0.22779658\n",
      "Iteration 429, loss = 0.33874460\n",
      "Iteration 367, loss = 0.35193770\n",
      "Iteration 1242, loss = 0.22891714\n",
      "Iteration 137, loss = 0.43332783\n",
      "Iteration 2438, loss = 0.14846485\n",
      "Iteration 430, loss = 0.33856969\n",
      "Iteration 435, loss = 0.39106229\n",
      "Iteration 1243, loss = 0.22880946\n",
      "Iteration 431, loss = 0.33844321\n",
      "Iteration 1221, loss = 0.28031780\n",
      "Iteration 368, loss = 0.35182973\n",
      "Iteration 1258, loss = 0.22742043\n",
      "Iteration 138, loss = 0.43256206\n",
      "Iteration 432, loss = 0.33832087\n",
      "Iteration 2439, loss = 0.14837103\n",
      "Iteration 433, loss = 0.33813931\n",
      "Iteration 434, loss = 0.33798878\n",
      "Iteration 1244, loss = 0.22870988\n",
      "Iteration 436, loss = 0.39091842\n",
      "Iteration 435, loss = 0.33781409\n",
      "Iteration 436, loss = 0.33765967\n",
      "Iteration 139, loss = 0.43179198\n",
      "Iteration 2440, loss = 0.14836853\n",
      "Iteration 1222, loss = 0.28016546\n",
      "Iteration 369, loss = 0.35172120\n",
      "Iteration 437, loss = 0.33751129\n",
      "Iteration 2441, loss = 0.14815900\n",
      "Iteration 438, loss = 0.33734512\n",
      "Iteration 370, loss = 0.35163679\n",
      "Iteration 1245, loss = 0.22836343\n",
      "Iteration 371, loss = 0.35154062\n",
      "Iteration 1259, loss = 0.22711078\n",
      "Iteration 140, loss = 0.43109348\n",
      "Iteration 2442, loss = 0.14807572\n",
      "Iteration 439, loss = 0.33719288\n",
      "Iteration 437, loss = 0.39070944\n",
      "Iteration 1223, loss = 0.28018347\n",
      "Iteration 1246, loss = 0.22823422\n",
      "Iteration 372, loss = 0.35142425\n",
      "Iteration 440, loss = 0.33703209\n",
      "Iteration 2443, loss = 0.14797174\n",
      "Iteration 373, loss = 0.35132122\n",
      "Iteration 1247, loss = 0.22814081\n",
      "Iteration 2444, loss = 0.14797811\n",
      "Iteration 441, loss = 0.33693307\n",
      "Iteration 374, loss = 0.35121499\n",
      "Iteration 141, loss = 0.43036544\n",
      "Iteration 442, loss = 0.33672729\n",
      "Iteration 1224, loss = 0.27995344\n",
      "Iteration 2445, loss = 0.14781749\n",
      "Iteration 443, loss = 0.33660106\n",
      "Iteration 1260, loss = 0.22710851\n",
      "Iteration 375, loss = 0.35114919\n",
      "Iteration 444, loss = 0.33644563\n",
      "Iteration 445, loss = 0.33627957\n",
      "Iteration 142, loss = 0.42960485\n",
      "Iteration 438, loss = 0.39059192\n",
      "Iteration 1248, loss = 0.22793470\n",
      "Iteration 2446, loss = 0.14776723\n",
      "Iteration 376, loss = 0.35102183\n",
      "Iteration 446, loss = 0.33614648\n",
      "Iteration 447, loss = 0.33597421\n",
      "Iteration 1225, loss = 0.27983145\n",
      "Iteration 448, loss = 0.33583144\n",
      "Iteration 377, loss = 0.35094927\n",
      "Iteration 143, loss = 0.42895911\n",
      "Iteration 2447, loss = 0.14766482\n",
      "Iteration 1261, loss = 0.22691133\n",
      "Iteration 1249, loss = 0.22767804\n",
      "Iteration 449, loss = 0.33568327\n",
      "Iteration 378, loss = 0.35085616\n",
      "Iteration 2448, loss = 0.14757517\n",
      "Iteration 450, loss = 0.33553504\n",
      "Iteration 1226, loss = 0.27981248\n",
      "Iteration 1250, loss = 0.22752708\n",
      "Iteration 2449, loss = 0.14765378\n",
      "Iteration 144, loss = 0.42830599\n",
      "Iteration 379, loss = 0.35071754\n",
      "Iteration 1262, loss = 0.22685062\n",
      "Iteration 451, loss = 0.33541509\n",
      "Iteration 439, loss = 0.39033608\n",
      "Iteration 380, loss = 0.35060926\n",
      "Iteration 452, loss = 0.33527639\n",
      "Iteration 1251, loss = 0.22732572\n",
      "Iteration 453, loss = 0.33514659\n",
      "Iteration 2450, loss = 0.14758898\n",
      "Iteration 454, loss = 0.33495315\n",
      "Iteration 1227, loss = 0.27993607\n",
      "Iteration 1263, loss = 0.22650969\n",
      "Iteration 2451, loss = 0.14753393\n",
      "Iteration 455, loss = 0.33481142\n",
      "Iteration 2452, loss = 0.14734977\n",
      "Iteration 456, loss = 0.33466205\n",
      "Iteration 1228, loss = 0.27953379\n",
      "Iteration 145, loss = 0.42760213\n",
      "Iteration 2453, loss = 0.14716192\n",
      "Iteration 1264, loss = 0.22623078\n",
      "Iteration 457, loss = 0.33451972\n",
      "Iteration 381, loss = 0.35051267\n",
      "Iteration 2454, loss = 0.14714833\n",
      "Iteration 1252, loss = 0.22725127\n",
      "Iteration 2455, loss = 0.14702399\n",
      "Iteration 458, loss = 0.33442131\n",
      "Iteration 382, loss = 0.35040785\n",
      "Iteration 2456, loss = 0.14690659\n",
      "Iteration 440, loss = 0.39015699\n",
      "Iteration 459, loss = 0.33424475\n",
      "Iteration 146, loss = 0.42697532\n",
      "Iteration 1253, loss = 0.22697840\n",
      "Iteration 460, loss = 0.33410268\n",
      "Iteration 461, loss = 0.33400047\n",
      "Iteration 2457, loss = 0.14687405\n",
      "Iteration 462, loss = 0.33380009\n",
      "Iteration 1229, loss = 0.27942731\n",
      "Iteration 463, loss = 0.33364068\n",
      "Iteration 1265, loss = 0.22608066\n",
      "Iteration 147, loss = 0.42634320\n",
      "Iteration 383, loss = 0.35030747\n",
      "Iteration 464, loss = 0.33353071\n",
      "Iteration 465, loss = 0.33335477\n",
      "Iteration 1254, loss = 0.22679617\n",
      "Iteration 466, loss = 0.33319534\n",
      "Iteration 2458, loss = 0.14677037\n",
      "Iteration 384, loss = 0.35022545\n",
      "Iteration 441, loss = 0.39003451\n",
      "Iteration 467, loss = 0.33306529\n",
      "Iteration 148, loss = 0.42564683\n",
      "Iteration 2459, loss = 0.14667866\n",
      "Iteration 1255, loss = 0.22664865\n",
      "Iteration 1230, loss = 0.27933636\n",
      "Iteration 468, loss = 0.33288790\n",
      "Iteration 149, loss = 0.42506064\n",
      "Iteration 2460, loss = 0.14664261\n",
      "Iteration 385, loss = 0.35009392\n",
      "Iteration 469, loss = 0.33277914\n",
      "Iteration 1266, loss = 0.22587885\n",
      "Iteration 1256, loss = 0.22651438\n",
      "Iteration 470, loss = 0.33260987\n",
      "Iteration 386, loss = 0.35001998\n",
      "Iteration 471, loss = 0.33247028\n",
      "Iteration 387, loss = 0.34991139\n",
      "Iteration 150, loss = 0.42443370\n",
      "Iteration 1231, loss = 0.27928014\n",
      "Iteration 442, loss = 0.38979363\n",
      "Iteration 2461, loss = 0.14656006\n",
      "Iteration 472, loss = 0.33238724\n",
      "Iteration 1257, loss = 0.22633449\n",
      "Iteration 473, loss = 0.33218957\n",
      "Iteration 388, loss = 0.34981238\n",
      "Iteration 151, loss = 0.42386127\n",
      "Iteration 474, loss = 0.33206422\n",
      "Iteration 1267, loss = 0.22563832\n",
      "Iteration 475, loss = 0.33193947\n",
      "Iteration 443, loss = 0.38960059\n",
      "Iteration 1258, loss = 0.22609470\n",
      "Iteration 389, loss = 0.34970274\n",
      "Iteration 1232, loss = 0.27915597\n",
      "Iteration 476, loss = 0.33172148\n",
      "Iteration 2462, loss = 0.14661502\n",
      "Iteration 477, loss = 0.33157505\n",
      "Iteration 390, loss = 0.34967821\n",
      "Iteration 1259, loss = 0.22592467\n",
      "Iteration 444, loss = 0.38946517\n",
      "Iteration 1233, loss = 0.27911957\n",
      "Iteration 2463, loss = 0.14636766Iteration 391, loss = 0.34950756\n",
      "\n",
      "Iteration 152, loss = 0.42328465\n",
      "Iteration 392, loss = 0.34941568\n",
      "Iteration 1260, loss = 0.22585653\n",
      "Iteration 478, loss = 0.33143981\n",
      "Iteration 1268, loss = 0.22560194\n",
      "Iteration 2464, loss = 0.14639940\n",
      "Iteration 153, loss = 0.42268060\n",
      "Iteration 1234, loss = 0.27891427\n",
      "Iteration 393, loss = 0.34933308\n",
      "Iteration 479, loss = 0.33130446\n",
      "Iteration 480, loss = 0.33119348\n",
      "Iteration 445, loss = 0.38929063\n",
      "Iteration 154, loss = 0.42210239\n",
      "Iteration 394, loss = 0.34921736\n",
      "Iteration 2465, loss = 0.14623841Iteration 481, loss = 0.33101519\n",
      "\n",
      "Iteration 1261, loss = 0.22562176\n",
      "Iteration 482, loss = 0.33088582\n",
      "Iteration 1235, loss = 0.27882269\n",
      "Iteration 483, loss = 0.33075336\n",
      "Iteration 155, loss = 0.42152892\n",
      "Iteration 484, loss = 0.33055638\n",
      "Iteration 1262, loss = 0.22535386\n",
      "Iteration 485, loss = 0.33043059\n",
      "Iteration 2466, loss = 0.14621097\n",
      "Iteration 395, loss = 0.34914224\n",
      "Iteration 1236, loss = 0.27887435\n",
      "Iteration 486, loss = 0.33029909\n",
      "Iteration 156, loss = 0.42101111\n",
      "Iteration 1269, loss = 0.22530959\n",
      "Iteration 1263, loss = 0.22519979\n",
      "Iteration 396, loss = 0.34902277\n",
      "Iteration 487, loss = 0.33012947\n",
      "Iteration 2467, loss = 0.14603235\n",
      "Iteration 488, loss = 0.33002527\n",
      "Iteration 446, loss = 0.38907897\n",
      "Iteration 397, loss = 0.34895535\n",
      "Iteration 2468, loss = 0.14597462\n",
      "Iteration 398, loss = 0.34884903\n",
      "Iteration 157, loss = 0.42047624\n",
      "Iteration 1237, loss = 0.27856621\n",
      "Iteration 489, loss = 0.32985704\n",
      "Iteration 1264, loss = 0.22506439\n",
      "Iteration 1270, loss = 0.22513141\n",
      "Iteration 399, loss = 0.34872876\n",
      "Iteration 490, loss = 0.32969585\n",
      "Iteration 2469, loss = 0.14590763\n",
      "Iteration 1265, loss = 0.22494154\n",
      "Iteration 400, loss = 0.34865251\n",
      "Iteration 491, loss = 0.32956843\n",
      "Iteration 401, loss = 0.34859149\n",
      "Iteration 492, loss = 0.32940399\n",
      "Iteration 447, loss = 0.38900630\n",
      "Iteration 402, loss = 0.34848649\n",
      "Iteration 1238, loss = 0.27853911\n",
      "Iteration 1271, loss = 0.22507509\n",
      "Iteration 403, loss = 0.34838943\n",
      "Iteration 404, loss = 0.34826683\n",
      "Iteration 158, loss = 0.41993739\n",
      "Iteration 1239, loss = 0.27847283\n",
      "Iteration 448, loss = 0.38868218\n",
      "Iteration 493, loss = 0.32927367\n",
      "Iteration 405, loss = 0.34816415\n",
      "Iteration 1272, loss = 0.22479570\n",
      "Iteration 494, loss = 0.32915157\n",
      "Iteration 2470, loss = 0.14586882\n",
      "Iteration 1240, loss = 0.27834076\n",
      "Iteration 495, loss = 0.32898993\n",
      "Iteration 2471, loss = 0.14568440\n",
      "Iteration 406, loss = 0.34806317\n",
      "Iteration 1266, loss = 0.22472066\n",
      "Iteration 496, loss = 0.32888419\n",
      "Iteration 497, loss = 0.32873002\n",
      "Iteration 449, loss = 0.38855507\n",
      "Iteration 2472, loss = 0.14562709\n",
      "Iteration 159, loss = 0.41944204\n",
      "Iteration 498, loss = 0.32857455\n",
      "Iteration 1241, loss = 0.27814191\n",
      "Iteration 407, loss = 0.34797057\n",
      "Iteration 499, loss = 0.32840460\n",
      "Iteration 1267, loss = 0.22454941\n",
      "Iteration 500, loss = 0.32828160\n",
      "Iteration 1273, loss = 0.22462999\n",
      "Iteration 1242, loss = 0.27805521\n",
      "Iteration 501, loss = 0.32814117\n",
      "Iteration 2473, loss = 0.14554360\n",
      "Iteration 502, loss = 0.32801384\n",
      "Iteration 408, loss = 0.34787465\n",
      "Iteration 160, loss = 0.41888377\n",
      "Iteration 1268, loss = 0.22441607\n",
      "Iteration 1243, loss = 0.27798980\n",
      "Iteration 503, loss = 0.32792797\n",
      "Iteration 450, loss = 0.38841665\n",
      "Iteration 409, loss = 0.34776571\n",
      "Iteration 1274, loss = 0.22450359\n",
      "Iteration 1269, loss = 0.22420311\n",
      "Iteration 504, loss = 0.32770256\n",
      "Iteration 505, loss = 0.32757738\n",
      "Iteration 2474, loss = 0.14559047\n",
      "Iteration 1270, loss = 0.22400703\n",
      "Iteration 506, loss = 0.32744897\n",
      "Iteration 507, loss = 0.32728917\n",
      "Iteration 1244, loss = 0.27779964\n",
      "Iteration 508, loss = 0.32714976\n",
      "Iteration 410, loss = 0.34766603\n",
      "Iteration 509, loss = 0.32700313\n",
      "Iteration 161, loss = 0.41839326\n",
      "Iteration 510, loss = 0.32685568\n",
      "Iteration 2475, loss = 0.14543677\n",
      "Iteration 1275, loss = 0.22428667\n",
      "Iteration 451, loss = 0.38816282\n",
      "Iteration 1271, loss = 0.22381918\n",
      "Iteration 511, loss = 0.32672221\n",
      "Iteration 2476, loss = 0.14536545\n",
      "Iteration 411, loss = 0.34756883\n",
      "Iteration 512, loss = 0.32656437\n",
      "Iteration 162, loss = 0.41787392\n",
      "Iteration 513, loss = 0.32643247\n",
      "Iteration 1245, loss = 0.27775620\n",
      "Iteration 2477, loss = 0.14529264\n",
      "Iteration 514, loss = 0.32627925\n",
      "Iteration 412, loss = 0.34747038\n",
      "Iteration 1272, loss = 0.22369725\n",
      "Iteration 515, loss = 0.32614002\n",
      "Iteration 2478, loss = 0.14517787\n",
      "Iteration 1246, loss = 0.27769126\n",
      "Iteration 516, loss = 0.32598695\n",
      "Iteration 517, loss = 0.32590502\n",
      "Iteration 413, loss = 0.34740604\n",
      "Iteration 1273, loss = 0.22362066\n",
      "Iteration 518, loss = 0.32571272\n",
      "Iteration 163, loss = 0.41739538\n",
      "Iteration 2479, loss = 0.14506255\n",
      "Iteration 1247, loss = 0.27750431\n",
      "Iteration 414, loss = 0.34728399\n",
      "Iteration 1274, loss = 0.22326957\n",
      "Iteration 1276, loss = 0.22408358\n",
      "Iteration 452, loss = 0.38796202\n",
      "Iteration 519, loss = 0.32557240\n",
      "Iteration 2480, loss = 0.14497648\n",
      "Iteration 415, loss = 0.34720022\n",
      "Iteration 164, loss = 0.41689039\n",
      "Iteration 520, loss = 0.32541537\n",
      "Iteration 2481, loss = 0.14488154\n",
      "Iteration 521, loss = 0.32526581\n",
      "Iteration 1275, loss = 0.22316228\n",
      "Iteration 1248, loss = 0.27739803\n",
      "Iteration 416, loss = 0.34709864\n",
      "Iteration 522, loss = 0.32514048\n",
      "Iteration 2482, loss = 0.14485964\n",
      "Iteration 523, loss = 0.32497902\n",
      "Iteration 1277, loss = 0.22392242\n",
      "Iteration 524, loss = 0.32487235\n",
      "Iteration 2483, loss = 0.14482914\n",
      "Iteration 453, loss = 0.38782048\n",
      "Iteration 1276, loss = 0.22302041\n",
      "Iteration 2484, loss = 0.14466825\n",
      "Iteration 2485, loss = 0.14465048\n",
      "Iteration 1249, loss = 0.27730942\n",
      "Iteration 525, loss = 0.32471962\n",
      "Iteration 1278, loss = 0.22369369\n",
      "Iteration 165, loss = 0.41642293\n",
      "Iteration 417, loss = 0.34700322\n",
      "Iteration 454, loss = 0.38769758\n",
      "Iteration 1277, loss = 0.22272411\n",
      "Iteration 526, loss = 0.32456114\n",
      "Iteration 418, loss = 0.34691521\n",
      "Iteration 166, loss = 0.41593246\n",
      "Iteration 527, loss = 0.32440489\n",
      "Iteration 1278, loss = 0.22255285\n",
      "Iteration 528, loss = 0.32426499\n",
      "Iteration 2486, loss = 0.14452697\n",
      "Iteration 529, loss = 0.32412890\n",
      "Iteration 530, loss = 0.32398564\n",
      "Iteration 2487, loss = 0.14452096\n",
      "Iteration 455, loss = 0.38744076\n",
      "Iteration 419, loss = 0.34685057\n",
      "Iteration 531, loss = 0.32384657\n",
      "Iteration 1279, loss = 0.22359231\n",
      "Iteration 2488, loss = 0.14446415\n",
      "Iteration 1250, loss = 0.27719096\n",
      "Iteration 532, loss = 0.32380588\n",
      "Iteration 420, loss = 0.34671628\n",
      "Iteration 533, loss = 0.32353837\n",
      "Iteration 2489, loss = 0.14426137\n",
      "Iteration 167, loss = 0.41547729\n",
      "Iteration 1251, loss = 0.27709409\n",
      "Iteration 534, loss = 0.32345257\n",
      "Iteration 421, loss = 0.34661266\n",
      "Iteration 2490, loss = 0.14421759\n",
      "Iteration 535, loss = 0.32325113\n",
      "Iteration 1280, loss = 0.22340125\n",
      "Iteration 422, loss = 0.34652747\n",
      "Iteration 168, loss = 0.41505052\n",
      "Iteration 536, loss = 0.32310643\n",
      "Iteration 2491, loss = 0.14407342\n",
      "Iteration 537, loss = 0.32296028\n",
      "Iteration 456, loss = 0.38731550\n",
      "Iteration 1252, loss = 0.27701061\n",
      "Iteration 538, loss = 0.32281973\n",
      "Iteration 423, loss = 0.34643472\n",
      "Iteration 539, loss = 0.32271841\n",
      "Iteration 2492, loss = 0.14400562\n",
      "Iteration 540, loss = 0.32254778\n",
      "Iteration 1279, loss = 0.22240404\n",
      "Iteration 424, loss = 0.34631962\n",
      "Iteration 2493, loss = 0.14409651\n",
      "Iteration 1253, loss = 0.27692441\n",
      "Iteration 425, loss = 0.34623503\n",
      "Iteration 2494, loss = 0.14399236\n",
      "Iteration 1281, loss = 0.22322548\n",
      "Iteration 426, loss = 0.34617450\n",
      "Iteration 541, loss = 0.32243392\n",
      "Iteration 169, loss = 0.41457844\n",
      "Iteration 457, loss = 0.38703954\n",
      "Iteration 542, loss = 0.32225889\n",
      "Iteration 1254, loss = 0.27679963\n",
      "Iteration 543, loss = 0.32210659\n",
      "Iteration 1280, loss = 0.22219214\n",
      "Iteration 544, loss = 0.32196094\n",
      "Iteration 427, loss = 0.34604209\n",
      "Iteration 2495, loss = 0.14399186Iteration 458, loss = 0.38687038\n",
      "\n",
      "Iteration 170, loss = 0.41414623\n",
      "Iteration 428, loss = 0.34597209\n",
      "Iteration 545, loss = 0.32182277\n",
      "Iteration 1281, loss = 0.22210213\n",
      "Iteration 1282, loss = 0.22293112\n",
      "Iteration 546, loss = 0.32169662\n",
      "Iteration 1255, loss = 0.27665815\n",
      "Iteration 429, loss = 0.34584939\n",
      "Iteration 547, loss = 0.32153131\n",
      "Iteration 2496, loss = 0.14371770\n",
      "Iteration 1282, loss = 0.22188148\n",
      "Iteration 2497, loss = 0.14375942\n",
      "Iteration 548, loss = 0.32139798\n",
      "Iteration 459, loss = 0.38671993\n",
      "Iteration 1256, loss = 0.27653455\n",
      "Iteration 430, loss = 0.34575746\n",
      "Iteration 549, loss = 0.32126113\n",
      "Iteration 171, loss = 0.41374293\n",
      "Iteration 550, loss = 0.32111847\n",
      "Iteration 2498, loss = 0.14355907\n",
      "Iteration 1283, loss = 0.22173432\n",
      "Iteration 431, loss = 0.34565542\n",
      "Iteration 1283, loss = 0.22279532\n",
      "Iteration 551, loss = 0.32097970\n",
      "Iteration 2499, loss = 0.14349379\n",
      "Iteration 432, loss = 0.34560852\n",
      "Iteration 552, loss = 0.32084408\n",
      "Iteration 460, loss = 0.38648758\n",
      "Iteration 1257, loss = 0.27658565\n",
      "Iteration 172, loss = 0.41331065\n",
      "Iteration 1284, loss = 0.22271447\n",
      "Iteration 2500, loss = 0.14337587\n",
      "Iteration 553, loss = 0.32073593\n",
      "Iteration 1284, loss = 0.22168151\n",
      "Iteration 554, loss = 0.32056961\n",
      "Iteration 1258, loss = 0.27637953\n",
      "Iteration 555, loss = 0.32045800\n",
      "Iteration 556, loss = 0.32029764\n",
      "Iteration 2501, loss = 0.14341602\n",
      "Iteration 433, loss = 0.34545804\n",
      "Iteration 1285, loss = 0.22142034\n",
      "Iteration 1285, loss = 0.22244186\n",
      "Iteration 2502, loss = 0.14325904\n",
      "Iteration 173, loss = 0.41286478\n",
      "Iteration 557, loss = 0.32015961\n",
      "Iteration 1259, loss = 0.27626445\n",
      "Iteration 558, loss = 0.32006168\n",
      "Iteration 461, loss = 0.38636853\n",
      "Iteration 2503, loss = 0.14318471\n",
      "Iteration 434, loss = 0.34536269\n",
      "Iteration 1286, loss = 0.22118678\n",
      "Iteration 559, loss = 0.31988486\n",
      "Iteration 2504, loss = 0.14311123\n",
      "Iteration 1260, loss = 0.27618375\n",
      "Iteration 435, loss = 0.34528273\n",
      "Iteration 1287, loss = 0.22113882\n",
      "Iteration 2505, loss = 0.14310259\n",
      "Iteration 560, loss = 0.31980497\n",
      "Iteration 174, loss = 0.41245804\n",
      "Iteration 436, loss = 0.34518339\n",
      "Iteration 2506, loss = 0.14298252\n",
      "Iteration 561, loss = 0.31962141\n",
      "Iteration 1286, loss = 0.22232004\n",
      "Iteration 1288, loss = 0.22078912\n",
      "Iteration 562, loss = 0.31947399\n",
      "Iteration 2507, loss = 0.14290177\n",
      "Iteration 563, loss = 0.31935204\n",
      "Iteration 564, loss = 0.31925332\n",
      "Iteration 2508, loss = 0.14279515\n",
      "Iteration 1289, loss = 0.22063288\n",
      "Iteration 565, loss = 0.31914703\n",
      "Iteration 437, loss = 0.34506694\n",
      "Iteration 1261, loss = 0.27602713\n",
      "Iteration 175, loss = 0.41206458\n",
      "Iteration 566, loss = 0.31895258\n",
      "Iteration 438, loss = 0.34497389Iteration 462, loss = 0.38612950\n",
      "Iteration 1287, loss = 0.22206765\n",
      "\n",
      "Iteration 567, loss = 0.31884087\n",
      "Iteration 2509, loss = 0.14274820\n",
      "Iteration 568, loss = 0.31867426\n",
      "Iteration 1290, loss = 0.22040570\n",
      "Iteration 1262, loss = 0.27598317\n",
      "Iteration 569, loss = 0.31854982\n",
      "Iteration 439, loss = 0.34490665\n",
      "Iteration 463, loss = 0.38596907\n",
      "Iteration 176, loss = 0.41161255\n",
      "Iteration 570, loss = 0.31841758\n",
      "Iteration 2510, loss = 0.14270797\n",
      "Iteration 1263, loss = 0.27580940\n",
      "Iteration 1291, loss = 0.22024339\n",
      "Iteration 440, loss = 0.34478725\n",
      "Iteration 2511, loss = 0.14265840\n",
      "Iteration 2512, loss = 0.14254455\n",
      "Iteration 1292, loss = 0.22025063\n",
      "Iteration 464, loss = 0.38580990\n",
      "Iteration 1264, loss = 0.27567768\n",
      "Iteration 571, loss = 0.31828428\n",
      "Iteration 441, loss = 0.34469819\n",
      "Iteration 572, loss = 0.31816053\n",
      "Iteration 1288, loss = 0.22189804\n",
      "Iteration 573, loss = 0.31803351\n",
      "Iteration 2513, loss = 0.14239940\n",
      "Iteration 177, loss = 0.41121500\n",
      "Iteration 574, loss = 0.31789742\n",
      "Iteration 1265, loss = 0.27558805\n",
      "Iteration 1293, loss = 0.21993314\n",
      "Iteration 575, loss = 0.31778824\n",
      "Iteration 2514, loss = 0.14236858\n",
      "Iteration 442, loss = 0.34460628\n",
      "Iteration 1294, loss = 0.21975176\n",
      "Iteration 465, loss = 0.38561136\n",
      "Iteration 576, loss = 0.31763833\n",
      "Iteration 443, loss = 0.34455713\n",
      "Iteration 2515, loss = 0.14233080\n",
      "Iteration 577, loss = 0.31749282\n",
      "Iteration 1289, loss = 0.22190560\n",
      "Iteration 578, loss = 0.31734251\n",
      "Iteration 1295, loss = 0.21960495\n",
      "Iteration 444, loss = 0.34447306\n",
      "Iteration 178, loss = 0.41084415\n",
      "Iteration 445, loss = 0.34430761\n",
      "Iteration 1266, loss = 0.27548629\n",
      "Iteration 579, loss = 0.31722663\n",
      "Iteration 1296, loss = 0.21937575\n",
      "Iteration 2516, loss = 0.14218879\n",
      "Iteration 446, loss = 0.34423361\n",
      "Iteration 580, loss = 0.31711905\n",
      "Iteration 2517, loss = 0.14207532\n",
      "Iteration 466, loss = 0.38549362\n",
      "Iteration 1297, loss = 0.21924734\n",
      "Iteration 1290, loss = 0.22153103\n",
      "Iteration 179, loss = 0.41045132\n",
      "Iteration 2518, loss = 0.14200634\n",
      "Iteration 581, loss = 0.31697518\n",
      "Iteration 1267, loss = 0.27538564\n",
      "Iteration 1298, loss = 0.21903746\n",
      "Iteration 447, loss = 0.34411147\n",
      "Iteration 582, loss = 0.31686049\n",
      "Iteration 1299, loss = 0.21896205\n",
      "Iteration 180, loss = 0.41007785\n",
      "Iteration 2519, loss = 0.14195861\n",
      "Iteration 1268, loss = 0.27526450\n",
      "Iteration 448, loss = 0.34402912\n",
      "Iteration 1300, loss = 0.21870130\n",
      "Iteration 467, loss = 0.38522443\n",
      "Iteration 181, loss = 0.40972699\n",
      "Iteration 583, loss = 0.31670410\n",
      "Iteration 584, loss = 0.31657344\n",
      "Iteration 1291, loss = 0.22148046\n",
      "Iteration 2520, loss = 0.14197051\n",
      "Iteration 585, loss = 0.31648970\n",
      "Iteration 1269, loss = 0.27516457\n",
      "Iteration 449, loss = 0.34394181\n",
      "Iteration 586, loss = 0.31633525\n",
      "Iteration 450, loss = 0.34381694\n",
      "Iteration 2521, loss = 0.14185018\n",
      "Iteration 1301, loss = 0.21853438\n",
      "Iteration 587, loss = 0.31616628\n",
      "Iteration 468, loss = 0.38506561\n",
      "Iteration 182, loss = 0.40933554\n",
      "Iteration 2522, loss = 0.14171142\n",
      "Iteration 1302, loss = 0.21836811\n",
      "Iteration 588, loss = 0.31606834\n",
      "Iteration 2523, loss = 0.14173807\n",
      "Iteration 589, loss = 0.31591777\n",
      "Iteration 1270, loss = 0.27524692\n",
      "Iteration 451, loss = 0.34372991\n",
      "Iteration 590, loss = 0.31578399\n",
      "Iteration 1292, loss = 0.22121710\n",
      "Iteration 591, loss = 0.31570444\n",
      "Iteration 183, loss = 0.40896192\n",
      "Iteration 2524, loss = 0.14159513\n",
      "Iteration 592, loss = 0.31552480\n",
      "Iteration 469, loss = 0.38496959\n",
      "Iteration 593, loss = 0.31540274\n",
      "Iteration 2525, loss = 0.14156269\n",
      "Iteration 452, loss = 0.34363679\n",
      "Iteration 1303, loss = 0.21816420\n",
      "Iteration 594, loss = 0.31528070\n",
      "Iteration 453, loss = 0.34355245\n",
      "Iteration 1271, loss = 0.27503452\n",
      "Iteration 2526, loss = 0.14144126\n",
      "Iteration 184, loss = 0.40862554\n",
      "Iteration 595, loss = 0.31513946\n",
      "Iteration 1304, loss = 0.21798325\n",
      "Iteration 454, loss = 0.34344885\n",
      "Iteration 470, loss = 0.38470565\n",
      "Iteration 596, loss = 0.31502121\n",
      "Iteration 1293, loss = 0.22108080\n",
      "Iteration 2527, loss = 0.14134458\n",
      "Iteration 185, loss = 0.40824306\n",
      "Iteration 597, loss = 0.31490766\n",
      "Iteration 1305, loss = 0.21802819\n",
      "Iteration 1272, loss = 0.27485932\n",
      "Iteration 455, loss = 0.34333597\n",
      "Iteration 598, loss = 0.31477028\n",
      "Iteration 2528, loss = 0.14125907\n",
      "Iteration 1306, loss = 0.21765523\n",
      "Iteration 599, loss = 0.31472022\n",
      "Iteration 186, loss = 0.40789025\n",
      "Iteration 456, loss = 0.34327274\n",
      "Iteration 600, loss = 0.31456735\n",
      "Iteration 471, loss = 0.38449360\n",
      "Iteration 457, loss = 0.34314151\n",
      "Iteration 601, loss = 0.31440393\n",
      "Iteration 458, loss = 0.34307569\n",
      "Iteration 1273, loss = 0.27478031\n",
      "Iteration 2529, loss = 0.14118960\n",
      "Iteration 187, loss = 0.40754157\n",
      "Iteration 602, loss = 0.31425841\n",
      "Iteration 1307, loss = 0.21751326\n",
      "Iteration 2530, loss = 0.14124485\n",
      "Iteration 472, loss = 0.38434628\n",
      "Iteration 1294, loss = 0.22080865\n",
      "Iteration 1274, loss = 0.27477634\n",
      "Iteration 603, loss = 0.31415094\n",
      "Iteration 459, loss = 0.34297014\n",
      "Iteration 2531, loss = 0.14099077\n",
      "Iteration 460, loss = 0.34288845\n",
      "Iteration 1308, loss = 0.21735882\n",
      "Iteration 604, loss = 0.31402337\n",
      "Iteration 2532, loss = 0.14102394\n",
      "Iteration 605, loss = 0.31393349\n",
      "Iteration 606, loss = 0.31374932\n",
      "Iteration 188, loss = 0.40718267\n",
      "Iteration 1309, loss = 0.21719047\n",
      "Iteration 607, loss = 0.31362827\n",
      "Iteration 2533, loss = 0.14087489\n",
      "Iteration 461, loss = 0.34276554\n",
      "Iteration 608, loss = 0.31351838\n",
      "Iteration 473, loss = 0.38415956\n",
      "Iteration 609, loss = 0.31338999\n",
      "Iteration 610, loss = 0.31326909\n",
      "Iteration 2534, loss = 0.14087841\n",
      "Iteration 462, loss = 0.34266542\n",
      "Iteration 189, loss = 0.40686040\n",
      "Iteration 1295, loss = 0.22064668\n",
      "Iteration 1275, loss = 0.27457041\n",
      "Iteration 2535, loss = 0.14075843\n",
      "Iteration 611, loss = 0.31313167\n",
      "Iteration 1310, loss = 0.21700888\n",
      "Iteration 463, loss = 0.34259951\n",
      "Iteration 612, loss = 0.31301872\n",
      "Iteration 464, loss = 0.34249087\n",
      "Iteration 613, loss = 0.31290953\n",
      "Iteration 474, loss = 0.38397904\n",
      "Iteration 2536, loss = 0.14069516\n",
      "Iteration 465, loss = 0.34238896\n",
      "Iteration 1311, loss = 0.21730888\n",
      "Iteration 1276, loss = 0.27443145\n",
      "Iteration 614, loss = 0.31279614\n",
      "Iteration 190, loss = 0.40652161\n",
      "Iteration 2537, loss = 0.14061752\n",
      "Iteration 615, loss = 0.31276505\n",
      "Iteration 616, loss = 0.31252080\n",
      "Iteration 1296, loss = 0.22074954\n",
      "Iteration 1277, loss = 0.27430142\n",
      "Iteration 475, loss = 0.38374623\n",
      "Iteration 617, loss = 0.31239424\n",
      "Iteration 466, loss = 0.34229674\n",
      "Iteration 191, loss = 0.40623774\n",
      "Iteration 1312, loss = 0.21667017\n",
      "Iteration 618, loss = 0.31226135\n",
      "Iteration 2538, loss = 0.14048071\n",
      "Iteration 467, loss = 0.34220610\n",
      "Iteration 2539, loss = 0.14042948\n",
      "Iteration 1297, loss = 0.22040316\n",
      "Iteration 1313, loss = 0.21647231\n",
      "Iteration 619, loss = 0.31216040\n",
      "Iteration 468, loss = 0.34211880\n",
      "Iteration 1278, loss = 0.27430188\n",
      "Iteration 476, loss = 0.38360711\n",
      "Iteration 469, loss = 0.34205021\n",
      "Iteration 1314, loss = 0.21631927\n",
      "Iteration 620, loss = 0.31204220\n",
      "Iteration 192, loss = 0.40587984\n",
      "Iteration 621, loss = 0.31190201\n",
      "Iteration 2540, loss = 0.14035283\n",
      "Iteration 1279, loss = 0.27418081\n",
      "Iteration 622, loss = 0.31177357\n",
      "Iteration 470, loss = 0.34192574\n",
      "Iteration 623, loss = 0.31173299\n",
      "Iteration 2541, loss = 0.14023214\n",
      "Iteration 1298, loss = 0.22016787\n",
      "Iteration 624, loss = 0.31153865\n",
      "Iteration 1315, loss = 0.21615856\n",
      "Iteration 625, loss = 0.31142361\n",
      "Iteration 2542, loss = 0.14031559\n",
      "Iteration 626, loss = 0.31130588\n",
      "Iteration 193, loss = 0.40555310\n",
      "Iteration 627, loss = 0.31120946\n",
      "Iteration 471, loss = 0.34184732\n",
      "Iteration 2543, loss = 0.14012691\n",
      "Iteration 477, loss = 0.38340860\n",
      "Iteration 628, loss = 0.31115101\n",
      "Iteration 1316, loss = 0.21592188\n",
      "Iteration 1280, loss = 0.27400584\n",
      "Iteration 194, loss = 0.40532368\n",
      "Iteration 472, loss = 0.34174251\n",
      "Iteration 629, loss = 0.31095242\n",
      "Iteration 2544, loss = 0.14004984\n",
      "Iteration 473, loss = 0.34163254\n",
      "Iteration 630, loss = 0.31083211\n",
      "Iteration 478, loss = 0.38324968\n",
      "Iteration 1299, loss = 0.21996150\n",
      "Iteration 1317, loss = 0.21588805\n",
      "Iteration 195, loss = 0.40492939\n",
      "Iteration 631, loss = 0.31069111\n",
      "Iteration 474, loss = 0.34158367\n",
      "Iteration 2545, loss = 0.13995299\n",
      "Iteration 632, loss = 0.31060183\n",
      "Iteration 1318, loss = 0.21571210\n",
      "Iteration 633, loss = 0.31044526\n",
      "Iteration 475, loss = 0.34150310\n",
      "Iteration 634, loss = 0.31032807\n",
      "Iteration 476, loss = 0.34134651\n",
      "Iteration 635, loss = 0.31021901\n",
      "Iteration 1281, loss = 0.27390630\n",
      "Iteration 636, loss = 0.31005916\n",
      "Iteration 196, loss = 0.40458769\n",
      "Iteration 1300, loss = 0.21976512\n",
      "Iteration 2546, loss = 0.13988614\n",
      "Iteration 1319, loss = 0.21547763\n",
      "Iteration 479, loss = 0.38303871\n",
      "Iteration 477, loss = 0.34131388\n",
      "Iteration 637, loss = 0.30993889\n",
      "Iteration 2547, loss = 0.13981973\n",
      "Iteration 197, loss = 0.40431275\n",
      "Iteration 638, loss = 0.30985652\n",
      "Iteration 1282, loss = 0.27380129\n",
      "Iteration 1320, loss = 0.21528196\n",
      "Iteration 478, loss = 0.34117333\n",
      "Iteration 2548, loss = 0.13973892\n",
      "Iteration 480, loss = 0.38285129\n",
      "Iteration 479, loss = 0.34113443\n",
      "Iteration 1301, loss = 0.21965734\n",
      "Iteration 639, loss = 0.30969333\n",
      "Iteration 1321, loss = 0.21517839\n",
      "Iteration 640, loss = 0.30964543\n",
      "Iteration 198, loss = 0.40401973\n",
      "Iteration 480, loss = 0.34101575\n",
      "Iteration 2549, loss = 0.13962616\n",
      "Iteration 641, loss = 0.30946328\n",
      "Iteration 1322, loss = 0.21508071\n",
      "Iteration 642, loss = 0.30935336\n",
      "Iteration 481, loss = 0.34089236\n",
      "Iteration 1283, loss = 0.27368388\n",
      "Iteration 643, loss = 0.30921940\n",
      "Iteration 644, loss = 0.30910804\n",
      "Iteration 482, loss = 0.34079682\n",
      "Iteration 1323, loss = 0.21522041\n",
      "Iteration 2550, loss = 0.13970105\n",
      "Iteration 483, loss = 0.34071489\n",
      "Iteration 1284, loss = 0.27360407\n",
      "Iteration 1324, loss = 0.21459550\n",
      "Iteration 645, loss = 0.30898099\n",
      "Iteration 484, loss = 0.34065223\n",
      "Iteration 1302, loss = 0.21957098\n",
      "Iteration 1325, loss = 0.21441573\n",
      "Iteration 485, loss = 0.34052656\n",
      "Iteration 481, loss = 0.38272504\n",
      "Iteration 199, loss = 0.40368459\n",
      "Iteration 646, loss = 0.30888145\n",
      "Iteration 2551, loss = 0.13951198\n",
      "Iteration 1326, loss = 0.21426368\n",
      "Iteration 486, loss = 0.34042769\n",
      "Iteration 1327, loss = 0.21404821\n",
      "Iteration 1303, loss = 0.21929279\n",
      "Iteration 200, loss = 0.40340942\n",
      "Iteration 647, loss = 0.30875230\n",
      "Iteration 1285, loss = 0.27347276\n",
      "Iteration 2552, loss = 0.13947544\n",
      "Iteration 482, loss = 0.38262994\n",
      "Iteration 487, loss = 0.34035989\n",
      "Iteration 648, loss = 0.30860632\n",
      "Iteration 488, loss = 0.34024293\n",
      "Iteration 2553, loss = 0.13939786\n",
      "Iteration 489, loss = 0.34016272\n",
      "Iteration 1304, loss = 0.21903338\n",
      "Iteration 1328, loss = 0.21399257\n",
      "Iteration 201, loss = 0.40311060\n",
      "Iteration 490, loss = 0.34004762\n",
      "Iteration 1286, loss = 0.27334520\n",
      "Iteration 649, loss = 0.30847187\n",
      "Iteration 1329, loss = 0.21375013\n",
      "Iteration 491, loss = 0.33998671\n",
      "Iteration 650, loss = 0.30835591\n",
      "Iteration 2554, loss = 0.13935610\n",
      "Iteration 651, loss = 0.30838233\n",
      "Iteration 492, loss = 0.33988720\n",
      "Iteration 2555, loss = 0.13945889\n",
      "Iteration 483, loss = 0.38229908\n",
      "Iteration 652, loss = 0.30812902\n",
      "Iteration 653, loss = 0.30797949\n",
      "Iteration 1287, loss = 0.27325101\n",
      "Iteration 2556, loss = 0.13912133\n",
      "Iteration 654, loss = 0.30789240\n",
      "Iteration 493, loss = 0.33980899\n",
      "Iteration 1305, loss = 0.21895300\n",
      "Iteration 202, loss = 0.40281678\n",
      "Iteration 1330, loss = 0.21372825\n",
      "Iteration 655, loss = 0.30772910\n",
      "Iteration 2557, loss = 0.13916799\n",
      "Iteration 494, loss = 0.33970386\n",
      "Iteration 1288, loss = 0.27314311\n",
      "Iteration 1331, loss = 0.21343014\n",
      "Iteration 656, loss = 0.30764513\n",
      "Iteration 495, loss = 0.33961895\n",
      "Iteration 1332, loss = 0.21331092\n",
      "Iteration 496, loss = 0.33953614\n",
      "Iteration 1289, loss = 0.27314709\n",
      "Iteration 2558, loss = 0.13903025\n",
      "Iteration 657, loss = 0.30760825\n",
      "Iteration 484, loss = 0.38213134\n",
      "Iteration 203, loss = 0.40253812\n",
      "Iteration 497, loss = 0.33941942\n",
      "Iteration 658, loss = 0.30738255\n",
      "Iteration 2559, loss = 0.13893725\n",
      "Iteration 1333, loss = 0.21324172\n",
      "Iteration 659, loss = 0.30725257\n",
      "Iteration 1306, loss = 0.21883852\n",
      "Iteration 1290, loss = 0.27293412\n",
      "Iteration 2560, loss = 0.13892297\n",
      "Iteration 204, loss = 0.40224603\n",
      "Iteration 1334, loss = 0.21301905\n",
      "Iteration 660, loss = 0.30713349\n",
      "Iteration 498, loss = 0.33939220\n",
      "Iteration 1335, loss = 0.21287623\n",
      "Iteration 2561, loss = 0.13890390\n",
      "Iteration 485, loss = 0.38191859\n",
      "Iteration 661, loss = 0.30712180\n",
      "Iteration 1291, loss = 0.27282438\n",
      "Iteration 1307, loss = 0.21861413\n",
      "Iteration 662, loss = 0.30688141\n",
      "Iteration 2562, loss = 0.13866573\n",
      "Iteration 205, loss = 0.40194976\n",
      "Iteration 663, loss = 0.30676389\n",
      "Iteration 499, loss = 0.33923290\n",
      "Iteration 1336, loss = 0.21269181\n",
      "Iteration 1292, loss = 0.27271399\n",
      "Iteration 486, loss = 0.38176534\n",
      "Iteration 206, loss = 0.40173416\n",
      "Iteration 2563, loss = 0.13867112\n",
      "Iteration 664, loss = 0.30668932\n",
      "Iteration 1308, loss = 0.21839714\n",
      "Iteration 1337, loss = 0.21246540\n",
      "Iteration 500, loss = 0.33914780\n",
      "Iteration 2564, loss = 0.13856253\n",
      "Iteration 1293, loss = 0.27264945\n",
      "Iteration 665, loss = 0.30660091\n",
      "Iteration 666, loss = 0.30640666\n",
      "Iteration 501, loss = 0.33905806\n",
      "Iteration 207, loss = 0.40142725\n",
      "Iteration 667, loss = 0.30627067\n",
      "Iteration 502, loss = 0.33896602\n",
      "Iteration 668, loss = 0.30614009\n",
      "Iteration 2565, loss = 0.13851223\n",
      "Iteration 503, loss = 0.33886274\n",
      "Iteration 1338, loss = 0.21228085\n",
      "Iteration 504, loss = 0.33878003\n",
      "Iteration 2566, loss = 0.13848064\n",
      "Iteration 2567, loss = 0.13832388\n",
      "Iteration 505, loss = 0.33870789\n",
      "Iteration 2568, loss = 0.13829328\n",
      "Iteration 1309, loss = 0.21817550\n",
      "Iteration 1339, loss = 0.21217713\n",
      "Iteration 506, loss = 0.33859583\n",
      "Iteration 487, loss = 0.38160576\n",
      "Iteration 2569, loss = 0.13816200\n",
      "Iteration 1340, loss = 0.21204235\n",
      "Iteration 507, loss = 0.33850834\n",
      "Iteration 1341, loss = 0.21175834\n",
      "Iteration 488, loss = 0.38137782\n",
      "Iteration 1310, loss = 0.21808034\n",
      "Iteration 508, loss = 0.33843583\n",
      "Iteration 208, loss = 0.40115298\n",
      "Iteration 2570, loss = 0.13813859\n",
      "Iteration 669, loss = 0.30603515\n",
      "Iteration 1294, loss = 0.27253843\n",
      "Iteration 670, loss = 0.30589744\n",
      "Iteration 671, loss = 0.30592250\n",
      "Iteration 1342, loss = 0.21158349\n",
      "Iteration 672, loss = 0.30572274\n",
      "Iteration 1295, loss = 0.27244254\n",
      "Iteration 209, loss = 0.40088533\n",
      "Iteration 673, loss = 0.30554468\n",
      "Iteration 1343, loss = 0.21143817\n",
      "Iteration 489, loss = 0.38120305\n",
      "Iteration 674, loss = 0.30546269\n",
      "Iteration 509, loss = 0.33836676\n",
      "Iteration 1344, loss = 0.21126893\n",
      "Iteration 1296, loss = 0.27232155\n",
      "Iteration 675, loss = 0.30528404\n",
      "Iteration 2571, loss = 0.13808048\n",
      "Iteration 676, loss = 0.30517498\n",
      "Iteration 677, loss = 0.30508211\n",
      "Iteration 2572, loss = 0.13800506\n",
      "Iteration 210, loss = 0.40058408\n",
      "Iteration 510, loss = 0.33823057\n",
      "Iteration 1345, loss = 0.21109679\n",
      "Iteration 1311, loss = 0.21781680\n",
      "Iteration 490, loss = 0.38112620\n",
      "Iteration 678, loss = 0.30494405\n",
      "Iteration 1346, loss = 0.21103162\n",
      "Iteration 2573, loss = 0.13791185\n",
      "Iteration 679, loss = 0.30484429\n",
      "Iteration 680, loss = 0.30469328\n",
      "Iteration 1347, loss = 0.21088694\n",
      "Iteration 1297, loss = 0.27216052\n",
      "Iteration 211, loss = 0.40032997\n",
      "Iteration 681, loss = 0.30465360\n",
      "Iteration 2574, loss = 0.13781147\n",
      "Iteration 511, loss = 0.33815226\n",
      "Iteration 682, loss = 0.30448192\n",
      "Iteration 2575, loss = 0.13786936\n",
      "Iteration 1312, loss = 0.21765587\n",
      "Iteration 683, loss = 0.30439187\n",
      "Iteration 1348, loss = 0.21065949\n",
      "Iteration 684, loss = 0.30423257\n",
      "Iteration 512, loss = 0.33805532\n",
      "Iteration 491, loss = 0.38084433\n",
      "Iteration 2576, loss = 0.13768246\n",
      "Iteration 685, loss = 0.30410679\n",
      "Iteration 1298, loss = 0.27210257\n",
      "Iteration 212, loss = 0.40008001\n",
      "Iteration 686, loss = 0.30398636\n",
      "Iteration 513, loss = 0.33801902\n",
      "Iteration 687, loss = 0.30393180\n",
      "Iteration 2577, loss = 0.13776640\n",
      "Iteration 1349, loss = 0.21048415\n",
      "Iteration 1299, loss = 0.27194739\n",
      "Iteration 1313, loss = 0.21756559\n",
      "Iteration 514, loss = 0.33789449\n",
      "Iteration 688, loss = 0.30379546\n",
      "Iteration 689, loss = 0.30366941\n",
      "Iteration 690, loss = 0.30352196\n",
      "Iteration 1350, loss = 0.21025995\n",
      "Iteration 515, loss = 0.33778630\n",
      "Iteration 492, loss = 0.38064760\n",
      "Iteration 691, loss = 0.30341252\n",
      "Iteration 2578, loss = 0.13756470\n",
      "Iteration 692, loss = 0.30328533\n",
      "Iteration 213, loss = 0.39979750\n",
      "Iteration 1351, loss = 0.21015033\n",
      "Iteration 693, loss = 0.30315752\n",
      "Iteration 1300, loss = 0.27189852\n",
      "Iteration 2579, loss = 0.13748993\n",
      "Iteration 516, loss = 0.33772680\n",
      "Iteration 1352, loss = 0.20991667\n",
      "Iteration 2580, loss = 0.13739610\n",
      "Iteration 517, loss = 0.33764226\n",
      "Iteration 1314, loss = 0.21734456\n",
      "Iteration 1301, loss = 0.27180428\n",
      "Iteration 694, loss = 0.30302429\n",
      "Iteration 1353, loss = 0.20993632\n",
      "Iteration 214, loss = 0.39956145\n",
      "Iteration 695, loss = 0.30292806\n",
      "Iteration 2581, loss = 0.13729200\n",
      "Iteration 518, loss = 0.33751687\n",
      "Iteration 696, loss = 0.30284071\n",
      "Iteration 2582, loss = 0.13726566\n",
      "Iteration 519, loss = 0.33742884\n",
      "Iteration 1354, loss = 0.20965943\n",
      "Iteration 493, loss = 0.38045584\n",
      "Iteration 697, loss = 0.30271715\n",
      "Iteration 1315, loss = 0.21708361\n",
      "Iteration 1355, loss = 0.20947200\n",
      "Iteration 698, loss = 0.30253926\n",
      "Iteration 2583, loss = 0.13715394\n",
      "Iteration 520, loss = 0.33733805\n",
      "Iteration 1302, loss = 0.27163591\n",
      "Iteration 215, loss = 0.39930799\n",
      "Iteration 2584, loss = 0.13709746\n",
      "Iteration 699, loss = 0.30242204\n",
      "Iteration 1356, loss = 0.20924185\n",
      "Iteration 1316, loss = 0.21696911\n",
      "Iteration 216, loss = 0.39904466\n",
      "Iteration 700, loss = 0.30234337\n",
      "Iteration 1357, loss = 0.20918502\n",
      "Iteration 521, loss = 0.33728436\n",
      "Iteration 494, loss = 0.38032756\n",
      "Iteration 2585, loss = 0.13714447\n",
      "Iteration 701, loss = 0.30218924\n",
      "Iteration 702, loss = 0.30206852\n",
      "Iteration 522, loss = 0.33715051\n",
      "Iteration 2586, loss = 0.13704347\n",
      "Iteration 703, loss = 0.30195883\n",
      "Iteration 1358, loss = 0.20889252\n",
      "Iteration 704, loss = 0.30192612\n",
      "Iteration 523, loss = 0.33706452\n",
      "Iteration 495, loss = 0.38010398\n",
      "Iteration 217, loss = 0.39877983\n",
      "Iteration 2587, loss = 0.13688431\n",
      "Iteration 705, loss = 0.30176796\n",
      "Iteration 1317, loss = 0.21673563\n",
      "Iteration 524, loss = 0.33702240\n",
      "Iteration 706, loss = 0.30161376\n",
      "Iteration 1359, loss = 0.20882595\n",
      "Iteration 1303, loss = 0.27152829\n",
      "Iteration 218, loss = 0.39856440\n",
      "Iteration 707, loss = 0.30148209\n",
      "Iteration 2588, loss = 0.13689825\n",
      "Iteration 525, loss = 0.33695173\n",
      "Iteration 1360, loss = 0.20870440\n",
      "Iteration 2589, loss = 0.13672930\n",
      "Iteration 1304, loss = 0.27143305\n",
      "Iteration 1318, loss = 0.21672778\n",
      "Iteration 2590, loss = 0.13670771\n",
      "Iteration 1361, loss = 0.20853215\n",
      "Iteration 708, loss = 0.30135020\n",
      "Iteration 526, loss = 0.33679516\n",
      "Iteration 709, loss = 0.30123764\n",
      "Iteration 496, loss = 0.37992190\n",
      "Iteration 219, loss = 0.39833936\n",
      "Iteration 1305, loss = 0.27131732\n",
      "Iteration 2591, loss = 0.13658768\n",
      "Iteration 1362, loss = 0.20827111\n",
      "Iteration 527, loss = 0.33671024\n",
      "Iteration 2592, loss = 0.13650348\n",
      "Iteration 710, loss = 0.30108946\n",
      "Iteration 220, loss = 0.39807722\n",
      "Iteration 1363, loss = 0.20807104\n",
      "Iteration 2593, loss = 0.13655858\n",
      "Iteration 711, loss = 0.30099343\n",
      "Iteration 1319, loss = 0.21641737\n",
      "Iteration 528, loss = 0.33660995\n",
      "Iteration 497, loss = 0.37972664\n",
      "Iteration 1306, loss = 0.27125784\n",
      "Iteration 2594, loss = 0.13636414\n",
      "Iteration 1364, loss = 0.20803209\n",
      "Iteration 712, loss = 0.30087584\n",
      "Iteration 221, loss = 0.39783597\n",
      "Iteration 2595, loss = 0.13632424\n",
      "Iteration 713, loss = 0.30072280\n",
      "Iteration 2596, loss = 0.13630330\n",
      "Iteration 529, loss = 0.33659477\n",
      "Iteration 714, loss = 0.30062668\n",
      "Iteration 222, loss = 0.39758049\n",
      "Iteration 1365, loss = 0.20775237\n",
      "Iteration 2597, loss = 0.13623732\n",
      "Iteration 715, loss = 0.30049885\n",
      "Iteration 1320, loss = 0.21623807\n",
      "Iteration 1366, loss = 0.20763796\n",
      "Iteration 498, loss = 0.37967071\n",
      "Iteration 530, loss = 0.33643954\n",
      "Iteration 716, loss = 0.30036064\n",
      "Iteration 2598, loss = 0.13609392\n",
      "Iteration 1367, loss = 0.20745209\n",
      "Iteration 1307, loss = 0.27115790\n",
      "Iteration 223, loss = 0.39736986\n",
      "Iteration 717, loss = 0.30024993\n",
      "Iteration 1368, loss = 0.20727533\n",
      "Iteration 531, loss = 0.33635414\n",
      "Iteration 718, loss = 0.30011930\n",
      "Iteration 1321, loss = 0.21622168\n",
      "Iteration 532, loss = 0.33625462\n",
      "Iteration 2599, loss = 0.13600298\n",
      "Iteration 224, loss = 0.39711700\n",
      "Iteration 499, loss = 0.37937692\n",
      "Iteration 1308, loss = 0.27109707\n",
      "Iteration 1369, loss = 0.20719039\n",
      "Iteration 719, loss = 0.30001946\n",
      "Iteration 533, loss = 0.33622316\n",
      "Iteration 2600, loss = 0.13596526\n",
      "Iteration 500, loss = 0.37916960\n",
      "Iteration 1309, loss = 0.27089903\n",
      "Iteration 534, loss = 0.33608470\n",
      "Iteration 720, loss = 0.29989288\n",
      "Iteration 225, loss = 0.39688457\n",
      "Iteration 1322, loss = 0.21589927\n",
      "Iteration 721, loss = 0.29981440\n",
      "Iteration 535, loss = 0.33599996\n",
      "Iteration 1310, loss = 0.27081065\n",
      "Iteration 2601, loss = 0.13592865\n",
      "Iteration 1370, loss = 0.20695350\n",
      "Iteration 722, loss = 0.29963292\n",
      "Iteration 723, loss = 0.29953089\n",
      "Iteration 536, loss = 0.33589526\n",
      "Iteration 1371, loss = 0.20676464\n",
      "Iteration 2602, loss = 0.13584287\n",
      "Iteration 1311, loss = 0.27066950\n",
      "Iteration 537, loss = 0.33582832\n",
      "Iteration 724, loss = 0.29939850\n",
      "Iteration 226, loss = 0.39666761\n",
      "Iteration 501, loss = 0.37900331\n",
      "Iteration 725, loss = 0.29930005\n",
      "Iteration 726, loss = 0.29918123\n",
      "Iteration 727, loss = 0.29905983\n",
      "Iteration 1372, loss = 0.20675509\n",
      "Iteration 538, loss = 0.33575609\n",
      "Iteration 2603, loss = 0.13575103\n",
      "Iteration 728, loss = 0.29895806\n",
      "Iteration 729, loss = 0.29880210\n",
      "Iteration 227, loss = 0.39643749\n",
      "Iteration 1312, loss = 0.27054220\n",
      "Iteration 539, loss = 0.33564862\n",
      "Iteration 1373, loss = 0.20655512\n",
      "Iteration 1323, loss = 0.21572923\n",
      "Iteration 502, loss = 0.37886064\n",
      "Iteration 540, loss = 0.33563576\n",
      "Iteration 1374, loss = 0.20625928\n",
      "Iteration 1313, loss = 0.27046205\n",
      "Iteration 730, loss = 0.29870874\n",
      "Iteration 228, loss = 0.39621096\n",
      "Iteration 731, loss = 0.29865186\n",
      "Iteration 1375, loss = 0.20621537\n",
      "Iteration 1314, loss = 0.27033640\n",
      "Iteration 541, loss = 0.33546397\n",
      "Iteration 2604, loss = 0.13573380\n",
      "Iteration 1324, loss = 0.21562466\n",
      "Iteration 732, loss = 0.29843653\n",
      "Iteration 1376, loss = 0.20598036\n",
      "Iteration 503, loss = 0.37865661\n",
      "Iteration 2605, loss = 0.13559027\n",
      "Iteration 733, loss = 0.29831738\n",
      "Iteration 229, loss = 0.39599996\n",
      "Iteration 734, loss = 0.29819679\n",
      "Iteration 542, loss = 0.33537350\n",
      "Iteration 735, loss = 0.29809759\n",
      "Iteration 1377, loss = 0.20592843\n",
      "Iteration 230, loss = 0.39578592\n",
      "Iteration 736, loss = 0.29794705\n",
      "Iteration 1325, loss = 0.21579058\n",
      "Iteration 1315, loss = 0.27026130\n",
      "Iteration 2606, loss = 0.13553679\n",
      "Iteration 543, loss = 0.33530555\n",
      "Iteration 737, loss = 0.29783896\n",
      "Iteration 504, loss = 0.37851043\n",
      "Iteration 544, loss = 0.33521241\n",
      "Iteration 2607, loss = 0.13543170\n",
      "Iteration 738, loss = 0.29772637\n",
      "Iteration 1378, loss = 0.20568746\n",
      "Iteration 1326, loss = 0.21525261\n",
      "Iteration 1316, loss = 0.27017032\n",
      "Iteration 739, loss = 0.29760809\n",
      "Iteration 545, loss = 0.33515320\n",
      "Iteration 740, loss = 0.29747275\n",
      "Iteration 2608, loss = 0.13541092\n",
      "Iteration 1379, loss = 0.20550189\n",
      "Iteration 741, loss = 0.29736575\n",
      "Iteration 231, loss = 0.39554247\n",
      "Iteration 546, loss = 0.33509478\n",
      "Iteration 742, loss = 0.29723179\n",
      "Iteration 2609, loss = 0.13531839\n",
      "Iteration 743, loss = 0.29715959\n",
      "Iteration 547, loss = 0.33492148\n",
      "Iteration 1317, loss = 0.27012546\n",
      "Iteration 1380, loss = 0.20535121\n",
      "Iteration 1327, loss = 0.21510635\n",
      "Iteration 744, loss = 0.29708811\n",
      "Iteration 505, loss = 0.37830430\n",
      "Iteration 745, loss = 0.29691185\n",
      "Iteration 2610, loss = 0.13523924\n",
      "Iteration 548, loss = 0.33486095\n",
      "Iteration 746, loss = 0.29676128\n",
      "Iteration 1318, loss = 0.26991352\n",
      "Iteration 232, loss = 0.39534080\n",
      "Iteration 2611, loss = 0.13515827\n",
      "Iteration 549, loss = 0.33474727\n",
      "Iteration 747, loss = 0.29664640\n",
      "Iteration 2612, loss = 0.13516350\n",
      "Iteration 1319, loss = 0.26985342\n",
      "Iteration 550, loss = 0.33467742\n",
      "Iteration 748, loss = 0.29651502\n",
      "Iteration 1381, loss = 0.20514163\n",
      "Iteration 2613, loss = 0.13503116\n",
      "Iteration 233, loss = 0.39511282\n",
      "Iteration 749, loss = 0.29639254\n",
      "Iteration 1328, loss = 0.21533492\n",
      "Iteration 750, loss = 0.29627250\n",
      "Iteration 1382, loss = 0.20506382\n",
      "Iteration 506, loss = 0.37811054\n",
      "Iteration 551, loss = 0.33455953\n",
      "Iteration 2614, loss = 0.13507833Iteration 234, loss = 0.39491242\n",
      "\n",
      "Iteration 1383, loss = 0.20480124\n",
      "Iteration 751, loss = 0.29615313\n",
      "Iteration 1320, loss = 0.26972417\n",
      "Iteration 552, loss = 0.33446441\n",
      "Iteration 752, loss = 0.29603617\n",
      "Iteration 753, loss = 0.29599744\n",
      "Iteration 235, loss = 0.39473882\n",
      "Iteration 2615, loss = 0.13499498\n",
      "Iteration 553, loss = 0.33437438\n",
      "Iteration 754, loss = 0.29580925\n",
      "Iteration 1329, loss = 0.21481263\n",
      "Iteration 1384, loss = 0.20469242\n",
      "Iteration 2616, loss = 0.13483702\n",
      "Iteration 755, loss = 0.29567820\n",
      "Iteration 507, loss = 0.37793604\n",
      "Iteration 554, loss = 0.33430281\n",
      "Iteration 756, loss = 0.29556568\n",
      "Iteration 2617, loss = 0.13479146\n",
      "Iteration 236, loss = 0.39449077\n",
      "Iteration 1385, loss = 0.20451532\n",
      "Iteration 757, loss = 0.29543772\n",
      "Iteration 555, loss = 0.33421978\n",
      "Iteration 2618, loss = 0.13466849\n",
      "Iteration 758, loss = 0.29536568\n",
      "Iteration 508, loss = 0.37774068\n",
      "Iteration 1321, loss = 0.26958745\n",
      "Iteration 1330, loss = 0.21468243\n",
      "Iteration 759, loss = 0.29523110\n",
      "Iteration 237, loss = 0.39429042\n",
      "Iteration 1386, loss = 0.20436285\n",
      "Iteration 556, loss = 0.33413105\n",
      "Iteration 760, loss = 0.29506436\n",
      "Iteration 761, loss = 0.29496318\n",
      "Iteration 2619, loss = 0.13461504\n",
      "Iteration 1387, loss = 0.20412796\n",
      "Iteration 762, loss = 0.29485674\n",
      "Iteration 763, loss = 0.29473547\n",
      "Iteration 557, loss = 0.33403763\n",
      "Iteration 2620, loss = 0.13452993\n",
      "Iteration 764, loss = 0.29461725\n",
      "Iteration 1331, loss = 0.21450107\n",
      "Iteration 1388, loss = 0.20396543\n",
      "Iteration 765, loss = 0.29451305\n",
      "Iteration 1322, loss = 0.26952612\n",
      "Iteration 558, loss = 0.33393259\n",
      "Iteration 766, loss = 0.29436816\n",
      "Iteration 767, loss = 0.29425089\n",
      "Iteration 559, loss = 0.33386366\n",
      "Iteration 1389, loss = 0.20382845\n",
      "Iteration 509, loss = 0.37760780\n",
      "Iteration 2621, loss = 0.13455358\n",
      "Iteration 238, loss = 0.39409702\n",
      "Iteration 1332, loss = 0.21445072\n",
      "Iteration 560, loss = 0.33378897\n",
      "Iteration 1323, loss = 0.26941256\n",
      "Iteration 768, loss = 0.29413167\n",
      "Iteration 1390, loss = 0.20401345\n",
      "Iteration 769, loss = 0.29400653\n",
      "Iteration 2622, loss = 0.13445220\n",
      "Iteration 561, loss = 0.33367909\n",
      "Iteration 770, loss = 0.29395414\n",
      "Iteration 1391, loss = 0.20357365\n",
      "Iteration 1324, loss = 0.26931573\n",
      "Iteration 2623, loss = 0.13436326\n",
      "Iteration 562, loss = 0.33357700\n",
      "Iteration 1333, loss = 0.21409505\n",
      "Iteration 771, loss = 0.29379942\n",
      "Iteration 2624, loss = 0.13433890\n",
      "Iteration 772, loss = 0.29367690\n",
      "Iteration 239, loss = 0.39390875\n",
      "Iteration 1392, loss = 0.20332613\n",
      "Iteration 773, loss = 0.29353128\n",
      "Iteration 2625, loss = 0.13420004\n",
      "Iteration 510, loss = 0.37736923\n",
      "Iteration 774, loss = 0.29341432\n",
      "Iteration 1325, loss = 0.26916907\n",
      "Iteration 1334, loss = 0.21390286\n",
      "Iteration 563, loss = 0.33349549\n",
      "Iteration 775, loss = 0.29333889\n",
      "Iteration 564, loss = 0.33339044\n",
      "Iteration 2626, loss = 0.13412745\n",
      "Iteration 776, loss = 0.29316664\n",
      "Iteration 240, loss = 0.39367231\n",
      "Iteration 1393, loss = 0.20335283\n",
      "Iteration 777, loss = 0.29303594\n",
      "Iteration 565, loss = 0.33335754\n",
      "Iteration 778, loss = 0.29291967\n",
      "Iteration 2627, loss = 0.13413009\n",
      "Iteration 779, loss = 0.29280075\n",
      "Iteration 566, loss = 0.33321406\n",
      "Iteration 1326, loss = 0.26907752\n",
      "Iteration 511, loss = 0.37722488\n",
      "Iteration 241, loss = 0.39346907\n",
      "Iteration 1335, loss = 0.21370494\n",
      "Iteration 2628, loss = 0.13398723\n",
      "Iteration 567, loss = 0.33313614\n",
      "Iteration 1394, loss = 0.20310007\n",
      "Iteration 780, loss = 0.29271190\n",
      "Iteration 242, loss = 0.39328699\n",
      "Iteration 568, loss = 0.33304345\n",
      "Iteration 1327, loss = 0.26894329\n",
      "Iteration 2629, loss = 0.13391625\n",
      "Iteration 569, loss = 0.33294283\n",
      "Iteration 781, loss = 0.29256313\n",
      "Iteration 782, loss = 0.29242448\n",
      "Iteration 243, loss = 0.39308161\n",
      "Iteration 2630, loss = 0.13386805\n",
      "Iteration 783, loss = 0.29231108\n",
      "Iteration 570, loss = 0.33286823\n",
      "Iteration 1328, loss = 0.26887432\n",
      "Iteration 1395, loss = 0.20304172\n",
      "Iteration 784, loss = 0.29219242\n",
      "Iteration 512, loss = 0.37715467\n",
      "Iteration 1336, loss = 0.21357668\n",
      "Iteration 785, loss = 0.29212623\n",
      "Iteration 2631, loss = 0.13384479\n",
      "Iteration 571, loss = 0.33276734\n",
      "Iteration 1329, loss = 0.26882846\n",
      "Iteration 244, loss = 0.39287453\n",
      "Iteration 786, loss = 0.29204574\n",
      "Iteration 572, loss = 0.33269177\n",
      "Iteration 1396, loss = 0.20275399\n",
      "Iteration 2632, loss = 0.13374144\n",
      "Iteration 787, loss = 0.29185680\n",
      "Iteration 1397, loss = 0.20253958\n",
      "Iteration 2633, loss = 0.13362538\n",
      "Iteration 788, loss = 0.29171144\n",
      "Iteration 513, loss = 0.37689160\n",
      "Iteration 1398, loss = 0.20242303\n",
      "Iteration 2634, loss = 0.13361423\n",
      "Iteration 573, loss = 0.33259615\n",
      "Iteration 245, loss = 0.39269221\n",
      "Iteration 1337, loss = 0.21343068\n",
      "Iteration 1330, loss = 0.26870962\n",
      "Iteration 2635, loss = 0.13351370\n",
      "Iteration 574, loss = 0.33249684\n",
      "Iteration 246, loss = 0.39246631\n",
      "Iteration 2636, loss = 0.13343280\n",
      "Iteration 1338, loss = 0.21334704\n",
      "Iteration 1331, loss = 0.26849082\n",
      "Iteration 2637, loss = 0.13337049\n",
      "Iteration 1399, loss = 0.20220767\n",
      "Iteration 789, loss = 0.29174554\n",
      "Iteration 575, loss = 0.33242132\n",
      "Iteration 790, loss = 0.29149169\n",
      "Iteration 791, loss = 0.29133479\n",
      "Iteration 2638, loss = 0.13325700\n",
      "Iteration 247, loss = 0.39230235\n",
      "Iteration 792, loss = 0.29119879\n",
      "Iteration 514, loss = 0.37666825\n",
      "Iteration 793, loss = 0.29110511\n",
      "Iteration 1339, loss = 0.21308586\n",
      "Iteration 1400, loss = 0.20215501\n",
      "Iteration 2639, loss = 0.13319902\n",
      "Iteration 794, loss = 0.29094491\n",
      "Iteration 576, loss = 0.33231544\n",
      "Iteration 1332, loss = 0.26839907\n",
      "Iteration 2640, loss = 0.13312800\n",
      "Iteration 795, loss = 0.29085465\n",
      "Iteration 796, loss = 0.29070900\n",
      "Iteration 1401, loss = 0.20185311\n",
      "Iteration 2641, loss = 0.13310910\n",
      "Iteration 797, loss = 0.29062834\n",
      "Iteration 248, loss = 0.39210411\n",
      "Iteration 577, loss = 0.33225843\n",
      "Iteration 798, loss = 0.29047292\n",
      "Iteration 2642, loss = 0.13298246\n",
      "Iteration 799, loss = 0.29034290\n",
      "Iteration 1402, loss = 0.20171397\n",
      "Iteration 515, loss = 0.37652274\n",
      "Iteration 800, loss = 0.29020751\n",
      "Iteration 578, loss = 0.33220069\n",
      "Iteration 801, loss = 0.29012887\n",
      "Iteration 249, loss = 0.39190884\n",
      "Iteration 1340, loss = 0.21279926\n",
      "Iteration 802, loss = 0.28997022\n",
      "Iteration 579, loss = 0.33207079\n",
      "Iteration 2643, loss = 0.13294088\n",
      "Iteration 803, loss = 0.28989225\n",
      "Iteration 1333, loss = 0.26832130\n",
      "Iteration 804, loss = 0.28974462\n",
      "Iteration 2644, loss = 0.13285883\n",
      "Iteration 805, loss = 0.28957017\n",
      "Iteration 580, loss = 0.33196334\n",
      "Iteration 1403, loss = 0.20164757\n",
      "Iteration 581, loss = 0.33189423\n",
      "Iteration 806, loss = 0.28945478\n",
      "Iteration 2645, loss = 0.13280212\n",
      "Iteration 1404, loss = 0.20137203\n",
      "Iteration 250, loss = 0.39172067\n",
      "Iteration 582, loss = 0.33177769\n",
      "Iteration 807, loss = 0.28935112\n",
      "Iteration 1405, loss = 0.20127325\n",
      "Iteration 2646, loss = 0.13271514\n",
      "Iteration 808, loss = 0.28921138\n",
      "Iteration 516, loss = 0.37630016\n",
      "Iteration 583, loss = 0.33168296\n",
      "Iteration 2647, loss = 0.13272730\n",
      "Iteration 809, loss = 0.28909370\n",
      "Iteration 1406, loss = 0.20107838\n",
      "Iteration 584, loss = 0.33161763\n",
      "Iteration 810, loss = 0.28904560\n",
      "Iteration 2648, loss = 0.13257456\n",
      "Iteration 811, loss = 0.28890775\n",
      "Iteration 585, loss = 0.33152679\n",
      "Iteration 1407, loss = 0.20093180\n",
      "Iteration 517, loss = 0.37619516\n",
      "Iteration 2649, loss = 0.13251670\n",
      "Iteration 812, loss = 0.28869762\n",
      "Iteration 586, loss = 0.33143061\n",
      "Iteration 813, loss = 0.28863471\n",
      "Iteration 2650, loss = 0.13245894\n",
      "Iteration 814, loss = 0.28846473\n",
      "Iteration 1408, loss = 0.20080273\n",
      "Iteration 815, loss = 0.28830593\n",
      "Iteration 2651, loss = 0.13238177\n",
      "Iteration 1334, loss = 0.26817762\n",
      "Iteration 587, loss = 0.33134433\n",
      "Iteration 816, loss = 0.28823107\n",
      "Iteration 1341, loss = 0.21271109\n",
      "Iteration 1409, loss = 0.20068803\n",
      "Iteration 2652, loss = 0.13234389\n",
      "Iteration 1410, loss = 0.20042597\n",
      "Iteration 251, loss = 0.39153908\n",
      "Iteration 588, loss = 0.33129114\n",
      "Iteration 817, loss = 0.28812375\n",
      "Iteration 1335, loss = 0.26805923\n",
      "Iteration 589, loss = 0.33119565\n",
      "Iteration 1342, loss = 0.21251992\n",
      "Iteration 518, loss = 0.37604780\n",
      "Iteration 252, loss = 0.39138668\n",
      "Iteration 818, loss = 0.28795061\n",
      "Iteration 819, loss = 0.28779745\n",
      "Iteration 820, loss = 0.28769974\n",
      "Iteration 1336, loss = 0.26793481\n",
      "Iteration 821, loss = 0.28756306\n",
      "Iteration 1343, loss = 0.21238546\n",
      "Iteration 253, loss = 0.39116420\n",
      "Iteration 822, loss = 0.28741129\n",
      "Iteration 823, loss = 0.28737547\n",
      "Iteration 590, loss = 0.33106944\n",
      "Iteration 824, loss = 0.28720042\n",
      "Iteration 825, loss = 0.28705744\n",
      "Iteration 2653, loss = 0.13225290\n",
      "Iteration 1337, loss = 0.26787434\n",
      "Iteration 826, loss = 0.28691301\n",
      "Iteration 2654, loss = 0.13221386\n",
      "Iteration 1344, loss = 0.21227295\n",
      "Iteration 2655, loss = 0.13208161\n",
      "Iteration 1411, loss = 0.20024361\n",
      "Iteration 519, loss = 0.37577058\n",
      "Iteration 591, loss = 0.33098293\n",
      "Iteration 254, loss = 0.39099281\n",
      "Iteration 592, loss = 0.33087906\n",
      "Iteration 2656, loss = 0.13202331\n",
      "Iteration 1412, loss = 0.20019500\n",
      "Iteration 520, loss = 0.37562047\n",
      "Iteration 2657, loss = 0.13197584\n",
      "Iteration 593, loss = 0.33077436\n",
      "Iteration 1338, loss = 0.26777266\n",
      "Iteration 594, loss = 0.33072327\n",
      "Iteration 1339, loss = 0.26761310\n",
      "Iteration 1413, loss = 0.20011000\n",
      "Iteration 2658, loss = 0.13203607\n",
      "Iteration 1345, loss = 0.21200229\n",
      "Iteration 521, loss = 0.37540341\n",
      "Iteration 595, loss = 0.33060048\n",
      "Iteration 596, loss = 0.33053916\n",
      "Iteration 1414, loss = 0.19974897\n",
      "Iteration 827, loss = 0.28680195\n",
      "Iteration 255, loss = 0.39083272\n",
      "Iteration 597, loss = 0.33041942\n",
      "Iteration 1340, loss = 0.26758911\n",
      "Iteration 2659, loss = 0.13185418\n",
      "Iteration 828, loss = 0.28669805\n",
      "Iteration 829, loss = 0.28664392\n",
      "Iteration 1415, loss = 0.19976231\n",
      "Iteration 2660, loss = 0.13174937\n",
      "Iteration 830, loss = 0.28639973\n",
      "Iteration 831, loss = 0.28625688\n",
      "Iteration 598, loss = 0.33032725\n",
      "Iteration 2661, loss = 0.13168350\n",
      "Iteration 256, loss = 0.39061884\n",
      "Iteration 832, loss = 0.28619223\n",
      "Iteration 833, loss = 0.28600794\n",
      "Iteration 834, loss = 0.28594433\n",
      "Iteration 2662, loss = 0.13170921\n",
      "Iteration 835, loss = 0.28575839\n",
      "Iteration 1341, loss = 0.26741061\n",
      "Iteration 1346, loss = 0.21197723\n",
      "Iteration 1416, loss = 0.19945120\n",
      "Iteration 836, loss = 0.28568347\n",
      "Iteration 257, loss = 0.39048964\n",
      "Iteration 2663, loss = 0.13159328\n",
      "Iteration 1342, loss = 0.26734003\n",
      "Iteration 837, loss = 0.28549608\n",
      "Iteration 2664, loss = 0.13147633\n",
      "Iteration 599, loss = 0.33028434\n",
      "Iteration 838, loss = 0.28538560\n",
      "Iteration 1417, loss = 0.19929055\n",
      "Iteration 2665, loss = 0.13145004\n",
      "Iteration 839, loss = 0.28536175\n",
      "Iteration 522, loss = 0.37522662\n",
      "Iteration 1347, loss = 0.21179590\n",
      "Iteration 840, loss = 0.28514136\n",
      "Iteration 258, loss = 0.39024836\n",
      "Iteration 841, loss = 0.28502228\n",
      "Iteration 600, loss = 0.33013899\n",
      "Iteration 842, loss = 0.28484852\n",
      "Iteration 2666, loss = 0.13131196\n",
      "Iteration 843, loss = 0.28475668\n",
      "Iteration 1418, loss = 0.19913338\n",
      "Iteration 844, loss = 0.28460130\n",
      "Iteration 601, loss = 0.33005723\n",
      "Iteration 2667, loss = 0.13125614\n",
      "Iteration 845, loss = 0.28446999\n",
      "Iteration 1419, loss = 0.19903038\n",
      "Iteration 602, loss = 0.32995604\n",
      "Iteration 2668, loss = 0.13121770\n",
      "Iteration 259, loss = 0.39011894\n",
      "Iteration 846, loss = 0.28435514\n",
      "Iteration 603, loss = 0.32986049\n",
      "Iteration 847, loss = 0.28427832\n",
      "Iteration 523, loss = 0.37508567\n",
      "Iteration 1343, loss = 0.26728922\n",
      "Iteration 848, loss = 0.28415912\n",
      "Iteration 2669, loss = 0.13110463\n",
      "Iteration 1420, loss = 0.19882730\n",
      "Iteration 849, loss = 0.28399391\n",
      "Iteration 260, loss = 0.38990178\n",
      "Iteration 604, loss = 0.32977038\n",
      "Iteration 2670, loss = 0.13116570\n",
      "Iteration 605, loss = 0.32971131\n",
      "Iteration 2671, loss = 0.13098683\n",
      "Iteration 1421, loss = 0.19878096\n",
      "Iteration 1348, loss = 0.21167644\n",
      "Iteration 261, loss = 0.38973071\n",
      "Iteration 524, loss = 0.37488519\n",
      "Iteration 2672, loss = 0.13096331\n",
      "Iteration 850, loss = 0.28383419\n",
      "Iteration 1344, loss = 0.26711839\n",
      "Iteration 851, loss = 0.28385394\n",
      "Iteration 606, loss = 0.32973085\n",
      "Iteration 2673, loss = 0.13082440\n",
      "Iteration 1349, loss = 0.21156360\n",
      "Iteration 607, loss = 0.32949397\n",
      "Iteration 852, loss = 0.28362465\n",
      "Iteration 262, loss = 0.38955738\n",
      "Iteration 525, loss = 0.37471036\n",
      "Iteration 2674, loss = 0.13082879\n",
      "Iteration 608, loss = 0.32939943\n",
      "Iteration 853, loss = 0.28353451\n",
      "Iteration 1345, loss = 0.26698172\n",
      "Iteration 609, loss = 0.32932137\n",
      "Iteration 854, loss = 0.28332049\n",
      "Iteration 1422, loss = 0.19846633\n",
      "Iteration 855, loss = 0.28338311\n",
      "Iteration 610, loss = 0.32924502\n",
      "Iteration 856, loss = 0.28307460\n",
      "Iteration 2675, loss = 0.13072490\n",
      "Iteration 1350, loss = 0.21136988\n",
      "Iteration 263, loss = 0.38937687\n",
      "Iteration 857, loss = 0.28296951\n",
      "Iteration 526, loss = 0.37458043\n",
      "Iteration 611, loss = 0.32912301\n",
      "Iteration 858, loss = 0.28282991\n",
      "Iteration 2676, loss = 0.13066825\n",
      "Iteration 859, loss = 0.28271496\n",
      "Iteration 1423, loss = 0.19834615\n",
      "Iteration 612, loss = 0.32902518\n",
      "Iteration 1346, loss = 0.26699574\n",
      "Iteration 860, loss = 0.28256231\n",
      "Iteration 861, loss = 0.28247349\n",
      "Iteration 264, loss = 0.38922725\n",
      "Iteration 613, loss = 0.32893325\n",
      "Iteration 862, loss = 0.28240556\n",
      "Iteration 863, loss = 0.28220963\n",
      "Iteration 864, loss = 0.28208672\n",
      "Iteration 2677, loss = 0.13058374\n",
      "Iteration 1424, loss = 0.19830142\n",
      "Iteration 865, loss = 0.28205732\n",
      "Iteration 866, loss = 0.28180717\n",
      "Iteration 1351, loss = 0.21115992\n",
      "Iteration 614, loss = 0.32884931\n",
      "Iteration 867, loss = 0.28167781\n",
      "Iteration 1347, loss = 0.26682324\n",
      "Iteration 1425, loss = 0.19807581\n",
      "Iteration 868, loss = 0.28161720\n",
      "Iteration 2678, loss = 0.13052895\n",
      "Iteration 869, loss = 0.28145813\n",
      "Iteration 870, loss = 0.28135162\n",
      "Iteration 527, loss = 0.37439731\n",
      "Iteration 265, loss = 0.38907930\n",
      "Iteration 615, loss = 0.32875083\n",
      "Iteration 2679, loss = 0.13044335\n",
      "Iteration 1348, loss = 0.26668343\n",
      "Iteration 871, loss = 0.28118161\n",
      "Iteration 616, loss = 0.32865085\n",
      "Iteration 266, loss = 0.38888964\n",
      "Iteration 872, loss = 0.28105364\n",
      "Iteration 2680, loss = 0.13035870\n",
      "Iteration 1352, loss = 0.21094182\n",
      "Iteration 873, loss = 0.28091446\n",
      "Iteration 1349, loss = 0.26651013\n",
      "Iteration 617, loss = 0.32856390\n",
      "Iteration 874, loss = 0.28084674\n",
      "Iteration 528, loss = 0.37416981\n",
      "Iteration 2681, loss = 0.13029790\n",
      "Iteration 1426, loss = 0.19788383\n",
      "Iteration 267, loss = 0.38875765\n",
      "Iteration 875, loss = 0.28068565\n",
      "Iteration 618, loss = 0.32854038\n",
      "Iteration 1427, loss = 0.19776179\n",
      "Iteration 876, loss = 0.28056293\n",
      "Iteration 2682, loss = 0.13041026\n",
      "Iteration 877, loss = 0.28041698\n",
      "Iteration 1350, loss = 0.26642515\n",
      "Iteration 1353, loss = 0.21073944\n",
      "Iteration 619, loss = 0.32837503\n",
      "Iteration 529, loss = 0.37406380\n",
      "Iteration 268, loss = 0.38854232\n",
      "Iteration 1428, loss = 0.19803304\n",
      "Iteration 620, loss = 0.32829608\n",
      "Iteration 1429, loss = 0.19738890\n",
      "Iteration 1351, loss = 0.26639001\n",
      "Iteration 878, loss = 0.28025625\n",
      "Iteration 269, loss = 0.38836938\n",
      "Iteration 2683, loss = 0.13018873\n",
      "Iteration 621, loss = 0.32819465\n",
      "Iteration 879, loss = 0.28015802\n",
      "Iteration 530, loss = 0.37391037\n",
      "Iteration 1352, loss = 0.26621396\n",
      "Iteration 880, loss = 0.28003537\n",
      "Iteration 2684, loss = 0.13014427\n",
      "Iteration 881, loss = 0.27991248\n",
      "Iteration 882, loss = 0.27975242\n",
      "Iteration 1430, loss = 0.19719156\n",
      "Iteration 270, loss = 0.38822119\n",
      "Iteration 883, loss = 0.27961899\n",
      "Iteration 2685, loss = 0.13004539\n",
      "Iteration 1354, loss = 0.21055477\n",
      "Iteration 884, loss = 0.27949649\n",
      "Iteration 2686, loss = 0.12996073\n",
      "Iteration 885, loss = 0.27933323\n",
      "Iteration 622, loss = 0.32813030\n",
      "Iteration 1431, loss = 0.19709383\n",
      "Iteration 886, loss = 0.27921656\n",
      "Iteration 887, loss = 0.27909181\n",
      "Iteration 271, loss = 0.38805962\n",
      "Iteration 2687, loss = 0.13008703\n",
      "Iteration 888, loss = 0.27894822\n",
      "Iteration 1353, loss = 0.26617535\n",
      "Iteration 531, loss = 0.37365274\n",
      "Iteration 889, loss = 0.27894245\n",
      "Iteration 623, loss = 0.32802491\n",
      "Iteration 2688, loss = 0.12989303\n",
      "Iteration 272, loss = 0.38790076\n",
      "Iteration 1432, loss = 0.19696568\n",
      "Iteration 624, loss = 0.32793249\n",
      "Iteration 890, loss = 0.27871027\n",
      "Iteration 2689, loss = 0.12978389\n",
      "Iteration 1355, loss = 0.21036950\n",
      "Iteration 891, loss = 0.27857470\n",
      "Iteration 892, loss = 0.27843828\n",
      "Iteration 1354, loss = 0.26598656\n",
      "Iteration 1433, loss = 0.19683248\n",
      "Iteration 893, loss = 0.27830918\n",
      "Iteration 1434, loss = 0.19658867\n",
      "Iteration 625, loss = 0.32786257\n",
      "Iteration 273, loss = 0.38774616\n",
      "Iteration 894, loss = 0.27816189\n",
      "Iteration 895, loss = 0.27816130\n",
      "Iteration 1356, loss = 0.21016573\n",
      "Iteration 626, loss = 0.32774875\n",
      "Iteration 2690, loss = 0.12965980\n",
      "Iteration 532, loss = 0.37345584\n",
      "Iteration 1435, loss = 0.19642767\n",
      "Iteration 627, loss = 0.32771918\n",
      "Iteration 274, loss = 0.38758178\n",
      "Iteration 2691, loss = 0.12972063\n",
      "Iteration 628, loss = 0.32752648\n",
      "Iteration 1357, loss = 0.21019788\n",
      "Iteration 1436, loss = 0.19630715\n",
      "Iteration 896, loss = 0.27795764\n",
      "Iteration 275, loss = 0.38743522\n",
      "Iteration 2692, loss = 0.12961879\n",
      "Iteration 1355, loss = 0.26589291\n",
      "Iteration 897, loss = 0.27773715\n",
      "Iteration 2693, loss = 0.12945767\n",
      "Iteration 629, loss = 0.32746297\n",
      "Iteration 898, loss = 0.27763509\n",
      "Iteration 533, loss = 0.37327206\n",
      "Iteration 1437, loss = 0.19611878\n",
      "Iteration 2694, loss = 0.12951051\n",
      "Iteration 899, loss = 0.27755362\n",
      "Iteration 630, loss = 0.32734833\n",
      "Iteration 900, loss = 0.27739084\n",
      "Iteration 1356, loss = 0.26580871\n",
      "Iteration 901, loss = 0.27725396\n",
      "Iteration 2695, loss = 0.12934471\n",
      "Iteration 631, loss = 0.32726825\n",
      "Iteration 1358, loss = 0.20987357\n",
      "Iteration 902, loss = 0.27712061\n",
      "Iteration 2696, loss = 0.12929239\n",
      "Iteration 534, loss = 0.37308275\n",
      "Iteration 632, loss = 0.32721090\n",
      "Iteration 276, loss = 0.38726009\n",
      "Iteration 1357, loss = 0.26577658\n",
      "Iteration 2697, loss = 0.12920260\n",
      "Iteration 633, loss = 0.32712355\n",
      "Iteration 1359, loss = 0.20972504\n",
      "Iteration 903, loss = 0.27697541\n",
      "Iteration 277, loss = 0.38708075\n",
      "Iteration 2698, loss = 0.12915942\n",
      "Iteration 904, loss = 0.27693644\n",
      "Iteration 905, loss = 0.27675515\n",
      "Iteration 2699, loss = 0.12912568\n",
      "Iteration 1438, loss = 0.19596567\n",
      "Iteration 906, loss = 0.27656735\n",
      "Iteration 535, loss = 0.37292459\n",
      "Iteration 907, loss = 0.27641937\n",
      "Iteration 2700, loss = 0.12906261\n",
      "Iteration 1358, loss = 0.26557425\n",
      "Iteration 1439, loss = 0.19598149\n",
      "Iteration 634, loss = 0.32698538\n",
      "Iteration 2701, loss = 0.12894222\n",
      "Iteration 635, loss = 0.32688607\n",
      "Iteration 278, loss = 0.38694934\n",
      "Iteration 908, loss = 0.27630002\n",
      "Iteration 1440, loss = 0.19564886\n",
      "Iteration 636, loss = 0.32680060\n",
      "Iteration 1360, loss = 0.20958656\n",
      "Iteration 2702, loss = 0.12890472\n",
      "Iteration 909, loss = 0.27619363\n",
      "Iteration 1441, loss = 0.19565297\n",
      "Iteration 279, loss = 0.38680068\n",
      "Iteration 637, loss = 0.32669348\n",
      "Iteration 910, loss = 0.27604864\n",
      "Iteration 2703, loss = 0.12882036\n",
      "Iteration 536, loss = 0.37273266\n",
      "Iteration 1442, loss = 0.19535546\n",
      "Iteration 1359, loss = 0.26551943\n",
      "Iteration 638, loss = 0.32660832\n",
      "Iteration 1361, loss = 0.20946094\n",
      "Iteration 639, loss = 0.32654726\n",
      "Iteration 2704, loss = 0.12875763\n",
      "Iteration 640, loss = 0.32644081\n",
      "Iteration 911, loss = 0.27592908\n",
      "Iteration 2705, loss = 0.12868583\n",
      "Iteration 1360, loss = 0.26535866\n",
      "Iteration 912, loss = 0.27576040\n",
      "Iteration 641, loss = 0.32634621\n",
      "Iteration 280, loss = 0.38662911\n",
      "Iteration 1362, loss = 0.20920917\n",
      "Iteration 913, loss = 0.27572246\n",
      "Iteration 1361, loss = 0.26527850\n",
      "Iteration 1443, loss = 0.19530476\n",
      "Iteration 537, loss = 0.37262822\n",
      "Iteration 914, loss = 0.27560434\n",
      "Iteration 642, loss = 0.32624192\n",
      "Iteration 2706, loss = 0.12884295\n",
      "Iteration 1362, loss = 0.26520145\n",
      "Iteration 915, loss = 0.27540915\n",
      "Iteration 281, loss = 0.38647541\n",
      "Iteration 1444, loss = 0.19516209\n",
      "Iteration 916, loss = 0.27528289\n",
      "Iteration 2707, loss = 0.12864922\n",
      "Iteration 643, loss = 0.32615913\n",
      "Iteration 917, loss = 0.27510659\n",
      "Iteration 538, loss = 0.37238195\n",
      "Iteration 918, loss = 0.27498902\n",
      "Iteration 1363, loss = 0.26503341\n",
      "Iteration 1445, loss = 0.19502303\n",
      "Iteration 919, loss = 0.27495461\n",
      "Iteration 644, loss = 0.32607100\n",
      "Iteration 2708, loss = 0.12854292\n",
      "Iteration 920, loss = 0.27474310\n",
      "Iteration 282, loss = 0.38635526\n",
      "Iteration 645, loss = 0.32594685\n",
      "Iteration 1363, loss = 0.20904016\n",
      "Iteration 1446, loss = 0.19487227\n",
      "Iteration 2709, loss = 0.12871673\n",
      "Iteration 646, loss = 0.32587870\n",
      "Iteration 921, loss = 0.27465507\n",
      "Iteration 2710, loss = 0.12837371\n",
      "Iteration 1364, loss = 0.26492010\n",
      "Iteration 283, loss = 0.38619018\n",
      "Iteration 1447, loss = 0.19459309\n",
      "Iteration 922, loss = 0.27458179\n",
      "Iteration 923, loss = 0.27433082\n",
      "Iteration 539, loss = 0.37219436\n",
      "Iteration 2711, loss = 0.12838742\n",
      "Iteration 924, loss = 0.27419720\n",
      "Iteration 647, loss = 0.32576117\n",
      "Iteration 1364, loss = 0.20889669\n",
      "Iteration 1448, loss = 0.19442750\n",
      "Iteration 925, loss = 0.27409026\n",
      "Iteration 648, loss = 0.32566607\n",
      "Iteration 2712, loss = 0.12834832\n",
      "Iteration 284, loss = 0.38601781\n",
      "Iteration 926, loss = 0.27391702\n",
      "Iteration 1449, loss = 0.19428835\n",
      "Iteration 649, loss = 0.32556388\n",
      "Iteration 1365, loss = 0.26484318Iteration 927, loss = 0.27386460\n",
      "\n",
      "Iteration 2713, loss = 0.12814195\n",
      "Iteration 650, loss = 0.32547591\n",
      "Iteration 540, loss = 0.37203486\n",
      "Iteration 1450, loss = 0.19434973\n",
      "Iteration 928, loss = 0.27366919\n",
      "Iteration 651, loss = 0.32540510\n",
      "Iteration 2714, loss = 0.12808597\n",
      "Iteration 1366, loss = 0.26475513\n",
      "Iteration 929, loss = 0.27351894\n",
      "Iteration 1365, loss = 0.20915772\n",
      "Iteration 285, loss = 0.38588135\n",
      "Iteration 930, loss = 0.27342148\n",
      "Iteration 931, loss = 0.27336020\n",
      "Iteration 2715, loss = 0.12803669\n",
      "Iteration 1451, loss = 0.19391325\n",
      "Iteration 2716, loss = 0.12802745\n",
      "Iteration 932, loss = 0.27310186\n",
      "Iteration 541, loss = 0.37183174\n",
      "Iteration 286, loss = 0.38573123\n",
      "Iteration 1367, loss = 0.26460304\n",
      "Iteration 2717, loss = 0.12794254\n",
      "Iteration 933, loss = 0.27296675\n",
      "Iteration 934, loss = 0.27284068\n",
      "Iteration 2718, loss = 0.12785239\n",
      "Iteration 935, loss = 0.27279014\n",
      "Iteration 652, loss = 0.32530793\n",
      "Iteration 1452, loss = 0.19387264\n",
      "Iteration 936, loss = 0.27262992\n",
      "Iteration 937, loss = 0.27240862\n",
      "Iteration 542, loss = 0.37171585\n",
      "Iteration 653, loss = 0.32522309\n",
      "Iteration 938, loss = 0.27230636\n",
      "Iteration 1368, loss = 0.26457867\n",
      "Iteration 287, loss = 0.38557816\n",
      "Iteration 1366, loss = 0.20861033\n",
      "Iteration 939, loss = 0.27215359\n",
      "Iteration 1453, loss = 0.19369504\n",
      "Iteration 2719, loss = 0.12786999\n",
      "Iteration 940, loss = 0.27200643\n",
      "Iteration 654, loss = 0.32512666\n",
      "Iteration 2720, loss = 0.12767955\n",
      "Iteration 941, loss = 0.27194617\n",
      "Iteration 2721, loss = 0.12763555\n",
      "Iteration 942, loss = 0.27174090\n",
      "Iteration 655, loss = 0.32500355\n",
      "Iteration 1454, loss = 0.19351179\n",
      "Iteration 943, loss = 0.27159599\n",
      "Iteration 656, loss = 0.32493810\n",
      "Iteration 944, loss = 0.27150116\n",
      "Iteration 1369, loss = 0.26455114\n",
      "Iteration 288, loss = 0.38541777\n",
      "Iteration 2722, loss = 0.12757638\n",
      "Iteration 1367, loss = 0.20838207\n",
      "Iteration 945, loss = 0.27132749\n",
      "Iteration 1455, loss = 0.19343341\n",
      "Iteration 946, loss = 0.27122148\n",
      "Iteration 657, loss = 0.32493658\n",
      "Iteration 543, loss = 0.37158154\n",
      "Iteration 2723, loss = 0.12749677\n",
      "Iteration 947, loss = 0.27108048\n",
      "Iteration 948, loss = 0.27092944\n",
      "Iteration 949, loss = 0.27080213\n",
      "Iteration 289, loss = 0.38528184\n",
      "Iteration 1370, loss = 0.26426878\n",
      "Iteration 658, loss = 0.32472727\n",
      "Iteration 1368, loss = 0.20865360\n",
      "Iteration 2724, loss = 0.12749584\n",
      "Iteration 950, loss = 0.27063465\n",
      "Iteration 1456, loss = 0.19315884\n",
      "Iteration 544, loss = 0.37135696\n",
      "Iteration 2725, loss = 0.12741447\n",
      "Iteration 951, loss = 0.27047871\n",
      "Iteration 290, loss = 0.38514733\n",
      "Iteration 659, loss = 0.32464522\n",
      "Iteration 952, loss = 0.27036620\n",
      "Iteration 953, loss = 0.27038692\n",
      "Iteration 1457, loss = 0.19306522\n",
      "Iteration 2726, loss = 0.12742305\n",
      "Iteration 954, loss = 0.27017888\n",
      "Iteration 955, loss = 0.26995538\n",
      "Iteration 2727, loss = 0.12735305\n",
      "Iteration 660, loss = 0.32462556\n",
      "Iteration 545, loss = 0.37112963\n",
      "Iteration 291, loss = 0.38499374\n",
      "Iteration 956, loss = 0.26986123\n",
      "Iteration 1369, loss = 0.20808054\n",
      "Iteration 1371, loss = 0.26433014\n",
      "Iteration 1458, loss = 0.19302079\n",
      "Iteration 661, loss = 0.32448365\n",
      "Iteration 957, loss = 0.26967845\n",
      "Iteration 2728, loss = 0.12728251\n",
      "Iteration 292, loss = 0.38483140\n",
      "Iteration 662, loss = 0.32435934\n",
      "Iteration 958, loss = 0.26949353\n",
      "Iteration 2729, loss = 0.12711896\n",
      "Iteration 959, loss = 0.26936492\n",
      "Iteration 1459, loss = 0.19270069\n",
      "Iteration 960, loss = 0.26922875\n",
      "Iteration 2730, loss = 0.12721940\n",
      "Iteration 1372, loss = 0.26413947\n",
      "Iteration 961, loss = 0.26909205\n",
      "Iteration 663, loss = 0.32427093\n",
      "Iteration 962, loss = 0.26901473\n",
      "Iteration 2731, loss = 0.12711092\n",
      "Iteration 293, loss = 0.38473439\n",
      "Iteration 664, loss = 0.32422740\n",
      "Iteration 1370, loss = 0.20797377\n",
      "Iteration 2732, loss = 0.12704156\n",
      "Iteration 546, loss = 0.37097222\n",
      "Iteration 963, loss = 0.26883020\n",
      "Iteration 964, loss = 0.26867600\n",
      "Iteration 2733, loss = 0.12684875\n",
      "Iteration 965, loss = 0.26858397\n",
      "Iteration 665, loss = 0.32410741\n",
      "Iteration 966, loss = 0.26837011\n",
      "Iteration 1460, loss = 0.19251123\n",
      "Iteration 967, loss = 0.26824010\n",
      "Iteration 666, loss = 0.32398943\n",
      "Iteration 1373, loss = 0.26402983\n",
      "Iteration 294, loss = 0.38456165\n",
      "Iteration 968, loss = 0.26810918\n",
      "Iteration 2734, loss = 0.12685821\n",
      "Iteration 969, loss = 0.26794572\n",
      "Iteration 667, loss = 0.32388873\n",
      "Iteration 970, loss = 0.26782800\n",
      "Iteration 547, loss = 0.37078314\n",
      "Iteration 295, loss = 0.38439396\n",
      "Iteration 971, loss = 0.26777164\n",
      "Iteration 668, loss = 0.32381475\n",
      "Iteration 1371, loss = 0.20799982\n",
      "Iteration 972, loss = 0.26758278\n",
      "Iteration 2735, loss = 0.12675722\n",
      "Iteration 973, loss = 0.26741270\n",
      "Iteration 669, loss = 0.32371488\n",
      "Iteration 1461, loss = 0.19251323\n",
      "Iteration 974, loss = 0.26732184\n",
      "Iteration 296, loss = 0.38426498\n",
      "Iteration 670, loss = 0.32361667\n",
      "Iteration 2736, loss = 0.12669152\n",
      "Iteration 1374, loss = 0.26391824\n",
      "Iteration 975, loss = 0.26713163\n",
      "Iteration 2737, loss = 0.12665260\n",
      "Iteration 1462, loss = 0.19241676\n",
      "Iteration 976, loss = 0.26696046\n",
      "Iteration 548, loss = 0.37066173\n",
      "Iteration 2738, loss = 0.12653659\n",
      "Iteration 1372, loss = 0.20773372\n",
      "Iteration 297, loss = 0.38410659\n",
      "Iteration 977, loss = 0.26682721\n",
      "Iteration 2739, loss = 0.12650439\n",
      "Iteration 978, loss = 0.26668249\n",
      "Iteration 979, loss = 0.26656961\n",
      "Iteration 1375, loss = 0.26374655\n",
      "Iteration 1463, loss = 0.19239965\n",
      "Iteration 671, loss = 0.32352307\n",
      "Iteration 2740, loss = 0.12651082\n",
      "Iteration 980, loss = 0.26638461\n",
      "Iteration 549, loss = 0.37043304\n",
      "Iteration 1373, loss = 0.20748218\n",
      "Iteration 2741, loss = 0.12633541\n",
      "Iteration 981, loss = 0.26623693\n",
      "Iteration 1464, loss = 0.19200822\n",
      "Iteration 1376, loss = 0.26364728\n",
      "Iteration 672, loss = 0.32342133\n",
      "Iteration 982, loss = 0.26612186\n",
      "Iteration 1465, loss = 0.19179230\n",
      "Iteration 2742, loss = 0.12629281\n",
      "Iteration 673, loss = 0.32337593\n",
      "Iteration 983, loss = 0.26604515\n",
      "Iteration 298, loss = 0.38406838\n",
      "Iteration 674, loss = 0.32325197\n",
      "Iteration 1374, loss = 0.20756208\n",
      "Iteration 2743, loss = 0.12625159\n",
      "Iteration 984, loss = 0.26582214\n",
      "Iteration 550, loss = 0.37029869\n",
      "Iteration 299, loss = 0.38386585\n",
      "Iteration 1466, loss = 0.19159126\n",
      "Iteration 1377, loss = 0.26353495\n",
      "Iteration 985, loss = 0.26570863\n",
      "Iteration 675, loss = 0.32322584\n",
      "Iteration 986, loss = 0.26556520\n",
      "Iteration 1467, loss = 0.19193377\n",
      "Iteration 2744, loss = 0.12613691\n",
      "Iteration 676, loss = 0.32305759\n",
      "Iteration 987, loss = 0.26539977\n",
      "Iteration 300, loss = 0.38371451\n",
      "Iteration 1378, loss = 0.26349903\n",
      "Iteration 988, loss = 0.26540539\n",
      "Iteration 2745, loss = 0.12618471\n",
      "Iteration 677, loss = 0.32299397\n",
      "Iteration 989, loss = 0.26510716\n",
      "Iteration 990, loss = 0.26497125\n",
      "Iteration 1468, loss = 0.19143941\n",
      "Iteration 2746, loss = 0.12603952\n",
      "Iteration 991, loss = 0.26482571\n",
      "Iteration 1375, loss = 0.20719936\n",
      "Iteration 551, loss = 0.37014144\n",
      "Iteration 1379, loss = 0.26329215\n",
      "Iteration 992, loss = 0.26480159\n",
      "Iteration 678, loss = 0.32285987\n",
      "Iteration 1469, loss = 0.19107537\n",
      "Iteration 2747, loss = 0.12595570\n",
      "Iteration 679, loss = 0.32281623\n",
      "Iteration 993, loss = 0.26454359\n",
      "Iteration 301, loss = 0.38353665\n",
      "Iteration 2748, loss = 0.12593566\n",
      "Iteration 1470, loss = 0.19091266\n",
      "Iteration 1380, loss = 0.26321827\n",
      "Iteration 2749, loss = 0.12583280\n",
      "Iteration 680, loss = 0.32268719\n",
      "Iteration 1471, loss = 0.19076187\n",
      "Iteration 302, loss = 0.38341536\n",
      "Iteration 2750, loss = 0.12580853\n",
      "Iteration 1381, loss = 0.26312298\n",
      "Iteration 1472, loss = 0.19067375\n",
      "Iteration 2751, loss = 0.12573142\n",
      "Iteration 681, loss = 0.32258966\n",
      "Iteration 1376, loss = 0.20697107\n",
      "Iteration 994, loss = 0.26440750\n",
      "Iteration 552, loss = 0.36999281\n",
      "Iteration 2752, loss = 0.12564561\n",
      "Iteration 1473, loss = 0.19053210\n",
      "Iteration 682, loss = 0.32254463\n",
      "Iteration 303, loss = 0.38328505\n",
      "Iteration 1382, loss = 0.26303289\n",
      "Iteration 1377, loss = 0.20673281\n",
      "Iteration 1474, loss = 0.19031927\n",
      "Iteration 2753, loss = 0.12562989\n",
      "Iteration 683, loss = 0.32242287\n",
      "Iteration 684, loss = 0.32232759\n",
      "Iteration 304, loss = 0.38316285\n",
      "Iteration 1383, loss = 0.26286178\n",
      "Iteration 1475, loss = 0.19022849\n",
      "Iteration 2754, loss = 0.12555773\n",
      "Iteration 995, loss = 0.26427310\n",
      "Iteration 553, loss = 0.36972231\n",
      "Iteration 996, loss = 0.26412820\n",
      "Iteration 685, loss = 0.32226413\n",
      "Iteration 997, loss = 0.26417214\n",
      "Iteration 686, loss = 0.32220391\n",
      "Iteration 998, loss = 0.26383675\n",
      "Iteration 999, loss = 0.26371269\n",
      "Iteration 1476, loss = 0.18993274\n",
      "Iteration 1378, loss = 0.20664444\n",
      "Iteration 687, loss = 0.32206686\n",
      "Iteration 305, loss = 0.38302466\n",
      "Iteration 1477, loss = 0.18980078\n",
      "Iteration 1000, loss = 0.26357185\n",
      "Iteration 554, loss = 0.36956013\n",
      "Iteration 1384, loss = 0.26284709\n",
      "Iteration 2755, loss = 0.12551977\n",
      "Iteration 1001, loss = 0.26342935\n",
      "Iteration 1002, loss = 0.26334724\n",
      "Iteration 1478, loss = 0.18966609\n",
      "Iteration 1003, loss = 0.26313726\n",
      "Iteration 1385, loss = 0.26268965\n",
      "Iteration 1004, loss = 0.26300799\n",
      "Iteration 688, loss = 0.32198682\n",
      "Iteration 555, loss = 0.36945631\n",
      "Iteration 1005, loss = 0.26289709\n",
      "Iteration 1479, loss = 0.18961443\n",
      "Iteration 1379, loss = 0.20642681\n",
      "Iteration 1006, loss = 0.26289300\n",
      "Iteration 1007, loss = 0.26264000\n",
      "Iteration 306, loss = 0.38293584\n",
      "Iteration 1386, loss = 0.26258821\n",
      "Iteration 1008, loss = 0.26241503\n",
      "Iteration 689, loss = 0.32188313\n",
      "Iteration 1009, loss = 0.26230013\n",
      "Iteration 1010, loss = 0.26213566\n",
      "Iteration 690, loss = 0.32178889\n",
      "Iteration 2756, loss = 0.12540865\n",
      "Iteration 307, loss = 0.38275284\n",
      "Iteration 1380, loss = 0.20630154\n",
      "Iteration 691, loss = 0.32168916\n",
      "Iteration 1480, loss = 0.18936092\n",
      "Iteration 1011, loss = 0.26202842\n",
      "Iteration 556, loss = 0.36922063\n",
      "Iteration 1387, loss = 0.26250706\n",
      "Iteration 1012, loss = 0.26188800\n",
      "Iteration 1481, loss = 0.18929429\n",
      "Iteration 2757, loss = 0.12534700\n",
      "Iteration 692, loss = 0.32166289\n",
      "Iteration 308, loss = 0.38260710\n",
      "Iteration 1388, loss = 0.26235722\n",
      "Iteration 1482, loss = 0.18901536\n",
      "Iteration 2758, loss = 0.12540133\n",
      "Iteration 693, loss = 0.32161430\n",
      "Iteration 1381, loss = 0.20619191\n",
      "Iteration 309, loss = 0.38245899\n",
      "Iteration 1483, loss = 0.18886171\n",
      "Iteration 694, loss = 0.32141245\n",
      "Iteration 1013, loss = 0.26172018\n",
      "Iteration 557, loss = 0.36904565\n",
      "Iteration 2759, loss = 0.12518075\n",
      "Iteration 1484, loss = 0.18871484\n",
      "Iteration 1389, loss = 0.26223627\n",
      "Iteration 1014, loss = 0.26159826\n",
      "Iteration 1015, loss = 0.26143614\n",
      "Iteration 2760, loss = 0.12527618\n",
      "Iteration 310, loss = 0.38232034\n",
      "Iteration 1016, loss = 0.26132077\n",
      "Iteration 695, loss = 0.32131764\n",
      "Iteration 1390, loss = 0.26218284\n",
      "Iteration 558, loss = 0.36886404\n",
      "Iteration 1017, loss = 0.26113380\n",
      "Iteration 2761, loss = 0.12505593\n",
      "Iteration 1485, loss = 0.18850629\n",
      "Iteration 311, loss = 0.38218560\n",
      "Iteration 1018, loss = 0.26102202\n",
      "Iteration 1382, loss = 0.20604168\n",
      "Iteration 696, loss = 0.32127109\n",
      "Iteration 2762, loss = 0.12502757\n",
      "Iteration 559, loss = 0.36877808\n",
      "Iteration 697, loss = 0.32119446\n",
      "Iteration 1391, loss = 0.26203306\n",
      "Iteration 1019, loss = 0.26090857\n",
      "Iteration 1383, loss = 0.20593801\n",
      "Iteration 698, loss = 0.32107870\n",
      "Iteration 1020, loss = 0.26077101\n",
      "Iteration 312, loss = 0.38206431\n",
      "Iteration 1486, loss = 0.18836660\n",
      "Iteration 1021, loss = 0.26058278\n",
      "Iteration 699, loss = 0.32097107\n",
      "Iteration 1022, loss = 0.26048254\n",
      "Iteration 2763, loss = 0.12498837\n",
      "Iteration 1023, loss = 0.26030580\n",
      "Iteration 313, loss = 0.38195773\n",
      "Iteration 1487, loss = 0.18819747\n",
      "Iteration 1024, loss = 0.26016413\n",
      "Iteration 700, loss = 0.32087385\n",
      "Iteration 2764, loss = 0.12490865\n",
      "Iteration 1392, loss = 0.26191067\n",
      "Iteration 701, loss = 0.32079055\n",
      "Iteration 1025, loss = 0.26008561\n",
      "Iteration 2765, loss = 0.12486407\n",
      "Iteration 560, loss = 0.36851228\n",
      "Iteration 702, loss = 0.32073537\n",
      "Iteration 314, loss = 0.38179987\n",
      "Iteration 2766, loss = 0.12478223\n",
      "Iteration 1488, loss = 0.18814386\n",
      "Iteration 703, loss = 0.32062782\n",
      "Iteration 1026, loss = 0.25993662\n",
      "Iteration 2767, loss = 0.12481774\n",
      "Iteration 1384, loss = 0.20573947\n",
      "Iteration 2768, loss = 0.12467768\n",
      "Iteration 1027, loss = 0.25975985\n",
      "Iteration 704, loss = 0.32050863\n",
      "Iteration 1393, loss = 0.26179595\n",
      "Iteration 1489, loss = 0.18801469\n",
      "Iteration 1028, loss = 0.25962434\n",
      "Iteration 315, loss = 0.38167882\n",
      "Iteration 705, loss = 0.32044704\n",
      "Iteration 2769, loss = 0.12457757\n",
      "Iteration 1029, loss = 0.25943373\n",
      "Iteration 2770, loss = 0.12453153\n",
      "Iteration 561, loss = 0.36833788\n",
      "Iteration 1394, loss = 0.26180864\n",
      "Iteration 1030, loss = 0.25933742\n",
      "Iteration 1490, loss = 0.18775137\n",
      "Iteration 316, loss = 0.38151406\n",
      "Iteration 1385, loss = 0.20559742\n",
      "Iteration 1031, loss = 0.25919312\n",
      "Iteration 706, loss = 0.32036153\n",
      "Iteration 1032, loss = 0.25907094\n",
      "Iteration 1491, loss = 0.18766203\n",
      "Iteration 1395, loss = 0.26160614\n",
      "Iteration 1033, loss = 0.25893418\n",
      "Iteration 317, loss = 0.38137280\n",
      "Iteration 1034, loss = 0.25875390\n",
      "Iteration 1492, loss = 0.18763024\n",
      "Iteration 1035, loss = 0.25859721Iteration 2771, loss = 0.12453404\n",
      "Iteration 707, loss = 0.32025829\n",
      "\n",
      "Iteration 1396, loss = 0.26154095\n",
      "Iteration 562, loss = 0.36826407\n",
      "Iteration 2772, loss = 0.12445009\n",
      "Iteration 1493, loss = 0.18733686\n",
      "Iteration 318, loss = 0.38125267\n",
      "Iteration 1386, loss = 0.20548670\n",
      "Iteration 2773, loss = 0.12436930\n",
      "Iteration 708, loss = 0.32012848\n",
      "Iteration 1036, loss = 0.25843190\n",
      "Iteration 2774, loss = 0.12427434\n",
      "Iteration 709, loss = 0.32003935\n",
      "Iteration 1397, loss = 0.26137243\n",
      "Iteration 1037, loss = 0.25852829\n",
      "Iteration 563, loss = 0.36800694\n",
      "Iteration 2775, loss = 0.12420187\n",
      "Iteration 1494, loss = 0.18709665\n",
      "Iteration 1038, loss = 0.25819287\n",
      "Iteration 2776, loss = 0.12419360\n",
      "Iteration 710, loss = 0.31994160\n",
      "Iteration 1398, loss = 0.26131667\n",
      "Iteration 1039, loss = 0.25802380\n",
      "Iteration 1387, loss = 0.20514981\n",
      "Iteration 1040, loss = 0.25794029\n",
      "Iteration 319, loss = 0.38111155\n",
      "Iteration 1495, loss = 0.18695900\n",
      "Iteration 1041, loss = 0.25774136\n",
      "Iteration 2777, loss = 0.12412818\n",
      "Iteration 1042, loss = 0.25763484\n",
      "Iteration 1399, loss = 0.26122906\n",
      "Iteration 1496, loss = 0.18674157\n",
      "Iteration 1043, loss = 0.25752197\n",
      "Iteration 711, loss = 0.31987985\n",
      "Iteration 1388, loss = 0.20518922\n",
      "Iteration 1400, loss = 0.26105906\n",
      "Iteration 1044, loss = 0.25737222\n",
      "Iteration 1497, loss = 0.18654566\n",
      "Iteration 564, loss = 0.36784091\n",
      "Iteration 2778, loss = 0.12404564\n",
      "Iteration 1045, loss = 0.25721668\n",
      "Iteration 712, loss = 0.31978015\n",
      "Iteration 1046, loss = 0.25712039\n",
      "Iteration 1047, loss = 0.25690151\n",
      "Iteration 713, loss = 0.31968549\n",
      "Iteration 320, loss = 0.38100872\n",
      "Iteration 1048, loss = 0.25674381\n",
      "Iteration 2779, loss = 0.12394918\n",
      "Iteration 1389, loss = 0.20497300\n",
      "Iteration 1498, loss = 0.18647236\n",
      "Iteration 1049, loss = 0.25662442\n",
      "Iteration 1401, loss = 0.26095201\n",
      "Iteration 714, loss = 0.31960120\n",
      "Iteration 2780, loss = 0.12397080\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1050, loss = 0.25660623\n",
      "Iteration 715, loss = 0.31952007\n",
      "Iteration 565, loss = 0.36775692\n",
      "Iteration 1051, loss = 0.25632478\n",
      "Iteration 716, loss = 0.31943270\n",
      "Iteration 1052, loss = 0.25619265\n",
      "Iteration 321, loss = 0.38089091\n",
      "Iteration 1053, loss = 0.25605949\n",
      "Iteration 1499, loss = 0.18625021\n",
      "Iteration 717, loss = 0.31936393\n",
      "Iteration 1402, loss = 0.26095200\n",
      "Iteration 1390, loss = 0.20466298\n",
      "Iteration 1054, loss = 0.25593064\n",
      "Iteration 566, loss = 0.36749555\n",
      "Iteration 1500, loss = 0.18605697\n",
      "Iteration 718, loss = 0.31926529\n",
      "Iteration 1055, loss = 0.25580707\n",
      "Iteration 322, loss = 0.38073381\n",
      "Iteration 1391, loss = 0.20451100\n",
      "Iteration 719, loss = 0.31915932\n",
      "Iteration 1056, loss = 0.25572314\n",
      "Iteration 1501, loss = 0.18639287\n",
      "Iteration 1057, loss = 0.25557211\n",
      "Iteration 720, loss = 0.31907246\n",
      "Iteration 1058, loss = 0.25534933\n",
      "Iteration 1502, loss = 0.18579149\n",
      "Iteration 1059, loss = 0.25518603\n",
      "Iteration 721, loss = 0.31894536\n",
      "Iteration 1403, loss = 0.26076863\n",
      "Iteration 1060, loss = 0.25504045\n",
      "Iteration 567, loss = 0.36731662\n",
      "Iteration 1061, loss = 0.25503207\n",
      "Iteration 323, loss = 0.38060244\n",
      "Iteration 1062, loss = 0.25482619\n",
      "Iteration 1503, loss = 0.18577826\n",
      "Iteration 1392, loss = 0.20450837\n",
      "Iteration 1063, loss = 0.25459946\n",
      "Iteration 1404, loss = 0.26069850\n",
      "Iteration 1064, loss = 0.25484526\n",
      "Iteration 722, loss = 0.31889950\n",
      "Iteration 1065, loss = 0.25435064\n",
      "Iteration 1066, loss = 0.25427248\n",
      "Iteration 723, loss = 0.31877362\n",
      "Iteration 1393, loss = 0.20426750\n",
      "Iteration 1504, loss = 0.18552856\n",
      "Iteration 1067, loss = 0.25403026\n",
      "Iteration 324, loss = 0.38047329\n",
      "Iteration 568, loss = 0.36711352\n",
      "Iteration 1068, loss = 0.25395811\n",
      "Iteration 724, loss = 0.31873899\n",
      "Iteration 1069, loss = 0.25380375\n",
      "Iteration 1505, loss = 0.18540224\n",
      "Iteration 569, loss = 0.36703948\n",
      "Iteration 725, loss = 0.31859167\n",
      "Iteration 1394, loss = 0.20406274\n",
      "Iteration 1070, loss = 0.25371446\n",
      "Iteration 726, loss = 0.31857646\n",
      "Iteration 1405, loss = 0.26068203\n",
      "Iteration 1071, loss = 0.25354203\n",
      "Iteration 727, loss = 0.31844024\n",
      "Iteration 1072, loss = 0.25333316\n",
      "Iteration 1073, loss = 0.25323402\n",
      "Iteration 325, loss = 0.38034716\n",
      "Iteration 570, loss = 0.36681979\n",
      "Iteration 1506, loss = 0.18530764\n",
      "Iteration 1074, loss = 0.25302131\n",
      "Iteration 728, loss = 0.31832184\n",
      "Iteration 1406, loss = 0.26041219\n",
      "Iteration 1075, loss = 0.25290080\n",
      "Iteration 729, loss = 0.31824516\n",
      "Iteration 1395, loss = 0.20388856\n",
      "Iteration 1507, loss = 0.18501092\n",
      "Iteration 571, loss = 0.36660174\n",
      "Iteration 730, loss = 0.31817732\n",
      "Iteration 1076, loss = 0.25292874\n",
      "Iteration 326, loss = 0.38023256\n",
      "Iteration 1077, loss = 0.25264540\n",
      "Iteration 1508, loss = 0.18485763\n",
      "Iteration 1078, loss = 0.25249318\n",
      "Iteration 731, loss = 0.31805186\n",
      "Iteration 1079, loss = 0.25234265\n",
      "Iteration 1509, loss = 0.18501870\n",
      "Iteration 732, loss = 0.31795703\n",
      "Iteration 1080, loss = 0.25216303\n",
      "Iteration 1081, loss = 0.25206860\n",
      "Iteration 1396, loss = 0.20372516\n",
      "Iteration 733, loss = 0.31786325\n",
      "Iteration 1082, loss = 0.25187192\n",
      "Iteration 327, loss = 0.38008711\n",
      "Iteration 1407, loss = 0.26038903\n",
      "Iteration 572, loss = 0.36649059\n",
      "Iteration 1, loss = 0.78300323\n",
      "Iteration 1083, loss = 0.25174538\n",
      "Iteration 1084, loss = 0.25157628\n",
      "Iteration 1085, loss = 0.25149799\n",
      "Iteration 1510, loss = 0.18454020\n",
      "Iteration 734, loss = 0.31783459\n",
      "Iteration 2, loss = 0.78158151\n",
      "Iteration 328, loss = 0.37998100\n",
      "Iteration 573, loss = 0.36632077\n",
      "Iteration 735, loss = 0.31768425\n",
      "Iteration 1086, loss = 0.25130075\n",
      "Iteration 3, loss = 0.77936367\n",
      "Iteration 1408, loss = 0.26022412\n",
      "Iteration 1397, loss = 0.20363099\n",
      "Iteration 736, loss = 0.31763255\n",
      "Iteration 1511, loss = 0.18431603\n",
      "Iteration 737, loss = 0.31749733\n",
      "Iteration 4, loss = 0.77681653\n",
      "Iteration 738, loss = 0.31741611\n",
      "Iteration 1398, loss = 0.20346011\n",
      "Iteration 329, loss = 0.37986844\n",
      "Iteration 1087, loss = 0.25114029\n",
      "Iteration 574, loss = 0.36611051\n",
      "Iteration 739, loss = 0.31732223\n",
      "Iteration 1409, loss = 0.26010526\n",
      "Iteration 5, loss = 0.77373766\n",
      "Iteration 1088, loss = 0.25099748\n",
      "Iteration 1512, loss = 0.18414291\n",
      "Iteration 6, loss = 0.77066730\n",
      "Iteration 1089, loss = 0.25095041\n",
      "Iteration 1399, loss = 0.20340466\n",
      "Iteration 1410, loss = 0.25996876Iteration 1090, loss = 0.25078146\n",
      "\n",
      "Iteration 7, loss = 0.76757678\n",
      "Iteration 575, loss = 0.36596119\n",
      "Iteration 1091, loss = 0.25056866\n",
      "Iteration 740, loss = 0.31728241\n",
      "Iteration 1092, loss = 0.25049515\n",
      "Iteration 8, loss = 0.76422014\n",
      "Iteration 1093, loss = 0.25029634\n",
      "Iteration 1411, loss = 0.25993167\n",
      "Iteration 741, loss = 0.31714161\n",
      "Iteration 1094, loss = 0.25013070\n",
      "Iteration 1095, loss = 0.25001428\n",
      "Iteration 330, loss = 0.37971134\n",
      "Iteration 742, loss = 0.31706777\n",
      "Iteration 1513, loss = 0.18414632\n",
      "Iteration 1096, loss = 0.24983793\n",
      "Iteration 1412, loss = 0.25979312\n",
      "Iteration 1514, loss = 0.18388994\n",
      "Iteration 1400, loss = 0.20310593\n",
      "Iteration 1097, loss = 0.24971668\n",
      "Iteration 331, loss = 0.37963041\n",
      "Iteration 743, loss = 0.31693916\n",
      "Iteration 576, loss = 0.36576847\n",
      "Iteration 1515, loss = 0.18363952\n",
      "Iteration 332, loss = 0.37945446\n",
      "Iteration 1098, loss = 0.24955631\n",
      "Iteration 1516, loss = 0.18348596\n",
      "Iteration 744, loss = 0.31687337\n",
      "Iteration 9, loss = 0.76103345\n",
      "Iteration 333, loss = 0.37939517\n",
      "Iteration 1401, loss = 0.20308165\n",
      "Iteration 1099, loss = 0.24950036\n",
      "Iteration 1517, loss = 0.18338084\n",
      "Iteration 1100, loss = 0.24925764\n",
      "Iteration 745, loss = 0.31681266\n",
      "Iteration 1101, loss = 0.24925600\n",
      "Iteration 746, loss = 0.31670567\n",
      "Iteration 10, loss = 0.75774898\n",
      "Iteration 1518, loss = 0.18357109\n",
      "Iteration 1102, loss = 0.24900562\n",
      "Iteration 747, loss = 0.31657973\n",
      "Iteration 1103, loss = 0.24893077\n",
      "Iteration 1104, loss = 0.24870052\n",
      "Iteration 1519, loss = 0.18301802\n",
      "Iteration 1105, loss = 0.24863039\n",
      "Iteration 1413, loss = 0.25972224\n",
      "Iteration 748, loss = 0.31650522\n",
      "Iteration 1106, loss = 0.24837965\n",
      "Iteration 334, loss = 0.37921444\n",
      "Iteration 577, loss = 0.36557483\n",
      "Iteration 1107, loss = 0.24827160\n",
      "Iteration 11, loss = 0.75466481\n",
      "Iteration 1402, loss = 0.20321890\n",
      "Iteration 1108, loss = 0.24817704\n",
      "Iteration 749, loss = 0.31641514\n",
      "Iteration 1109, loss = 0.24801515\n",
      "Iteration 335, loss = 0.37907228\n",
      "Iteration 1110, loss = 0.24782576\n",
      "Iteration 1414, loss = 0.25962782\n",
      "Iteration 1111, loss = 0.24768298\n",
      "Iteration 1112, loss = 0.24756614\n",
      "Iteration 578, loss = 0.36542147\n",
      "Iteration 12, loss = 0.75151279\n",
      "Iteration 750, loss = 0.31632399\n",
      "Iteration 1113, loss = 0.24738315\n",
      "Iteration 336, loss = 0.37895932\n",
      "Iteration 1114, loss = 0.24724046\n",
      "Iteration 13, loss = 0.74845716\n",
      "Iteration 1115, loss = 0.24710376\n",
      "Iteration 751, loss = 0.31620805\n",
      "Iteration 1116, loss = 0.24701086\n",
      "Iteration 1117, loss = 0.24685416\n",
      "Iteration 1415, loss = 0.25950964\n",
      "Iteration 579, loss = 0.36529324\n",
      "Iteration 752, loss = 0.31615196\n",
      "Iteration 1118, loss = 0.24668802\n",
      "Iteration 14, loss = 0.74552937\n",
      "Iteration 1119, loss = 0.24653338\n",
      "Iteration 1120, loss = 0.24638149\n",
      "Iteration 1416, loss = 0.25944573\n",
      "Iteration 337, loss = 0.37882523\n",
      "Iteration 753, loss = 0.31602428\n",
      "Iteration 754, loss = 0.31599091\n",
      "Iteration 1121, loss = 0.24625387\n",
      "Iteration 1417, loss = 0.25927075\n",
      "Iteration 1122, loss = 0.24617222\n",
      "Iteration 15, loss = 0.74255923\n",
      "Iteration 338, loss = 0.37872138\n",
      "Iteration 1123, loss = 0.24593552\n",
      "Iteration 755, loss = 0.31585697\n",
      "Iteration 1124, loss = 0.24579562\n",
      "Iteration 1125, loss = 0.24566878\n",
      "Iteration 1418, loss = 0.25916160\n",
      "Iteration 1126, loss = 0.24550028\n",
      "Iteration 339, loss = 0.37859435\n",
      "Iteration 580, loss = 0.36512389\n",
      "Iteration 1127, loss = 0.24540352\n",
      "Iteration 756, loss = 0.31577084\n",
      "Iteration 16, loss = 0.73969347\n",
      "Iteration 1520, loss = 0.18292406\n",
      "Iteration 757, loss = 0.31576747\n",
      "Iteration 1419, loss = 0.25905430\n",
      "Iteration 17, loss = 0.73683208\n",
      "Iteration 1521, loss = 0.18267183\n",
      "Iteration 758, loss = 0.31566644\n",
      "Iteration 1128, loss = 0.24522909\n",
      "Iteration 759, loss = 0.31550561\n",
      "Iteration 1420, loss = 0.25896941\n",
      "Iteration 1129, loss = 0.24505427\n",
      "Iteration 340, loss = 0.37845036\n",
      "Iteration 760, loss = 0.31540048\n",
      "Iteration 18, loss = 0.73405952\n",
      "Iteration 581, loss = 0.36488549\n",
      "Iteration 1130, loss = 0.24488512\n",
      "Iteration 1131, loss = 0.24476891\n",
      "Iteration 1132, loss = 0.24470626\n",
      "Iteration 19, loss = 0.73131553\n",
      "Iteration 761, loss = 0.31530576\n",
      "Iteration 1403, loss = 0.20267571\n",
      "Iteration 1133, loss = 0.24441573\n",
      "Iteration 1134, loss = 0.24435202\n",
      "Iteration 1135, loss = 0.24411980\n",
      "Iteration 20, loss = 0.72859753\n",
      "Iteration 762, loss = 0.31520588\n",
      "Iteration 341, loss = 0.37832281\n",
      "Iteration 1136, loss = 0.24395376\n",
      "Iteration 1421, loss = 0.25882331\n",
      "Iteration 763, loss = 0.31511060\n",
      "Iteration 1137, loss = 0.24380473\n",
      "Iteration 21, loss = 0.72578129\n",
      "Iteration 582, loss = 0.36470690\n",
      "Iteration 1422, loss = 0.25871479\n",
      "Iteration 764, loss = 0.31502861\n",
      "Iteration 342, loss = 0.37820400\n",
      "Iteration 22, loss = 0.72312211\n",
      "Iteration 1522, loss = 0.18264290\n",
      "Iteration 1138, loss = 0.24375580\n",
      "Iteration 765, loss = 0.31492830\n",
      "Iteration 23, loss = 0.72049381\n",
      "Iteration 1139, loss = 0.24352382\n",
      "Iteration 1423, loss = 0.25862263\n",
      "Iteration 766, loss = 0.31482144\n",
      "Iteration 343, loss = 0.37809098\n",
      "Iteration 1140, loss = 0.24364308\n",
      "Iteration 583, loss = 0.36450886\n",
      "Iteration 1523, loss = 0.18234664\n",
      "Iteration 24, loss = 0.71787518\n",
      "Iteration 767, loss = 0.31475888\n",
      "Iteration 1141, loss = 0.24330811\n",
      "Iteration 768, loss = 0.31469419\n",
      "Iteration 1524, loss = 0.18217646\n",
      "Iteration 344, loss = 0.37794770\n",
      "Iteration 25, loss = 0.71516680\n",
      "Iteration 1142, loss = 0.24313854\n",
      "Iteration 1143, loss = 0.24312061\n",
      "Iteration 1144, loss = 0.24284324\n",
      "Iteration 1525, loss = 0.18203576\n",
      "Iteration 1145, loss = 0.24257244\n",
      "Iteration 26, loss = 0.71267645\n",
      "Iteration 769, loss = 0.31457908\n",
      "Iteration 584, loss = 0.36437334\n",
      "Iteration 1146, loss = 0.24243804\n",
      "Iteration 1424, loss = 0.25851096\n",
      "Iteration 27, loss = 0.70993550\n",
      "Iteration 345, loss = 0.37785114\n",
      "Iteration 1526, loss = 0.18185110\n",
      "Iteration 1147, loss = 0.24237358\n",
      "Iteration 770, loss = 0.31447626\n",
      "Iteration 1148, loss = 0.24218820\n",
      "Iteration 28, loss = 0.70729767\n",
      "Iteration 1527, loss = 0.18171275\n",
      "Iteration 585, loss = 0.36425896\n",
      "Iteration 346, loss = 0.37779231\n",
      "Iteration 29, loss = 0.70468293\n",
      "Iteration 1425, loss = 0.25839642\n",
      "Iteration 347, loss = 0.37761295\n",
      "Iteration 1149, loss = 0.24202891\n",
      "Iteration 30, loss = 0.70203769\n",
      "Iteration 1528, loss = 0.18155970\n",
      "Iteration 1150, loss = 0.24187109\n",
      "Iteration 348, loss = 0.37751502\n",
      "Iteration 771, loss = 0.31439583\n",
      "Iteration 1426, loss = 0.25831153\n",
      "Iteration 1151, loss = 0.24170219\n",
      "Iteration 1404, loss = 0.20249708\n",
      "Iteration 1529, loss = 0.18151769\n",
      "Iteration 349, loss = 0.37737239\n",
      "Iteration 586, loss = 0.36398925\n",
      "Iteration 772, loss = 0.31431087\n",
      "Iteration 31, loss = 0.69936723\n",
      "Iteration 1152, loss = 0.24163313\n",
      "Iteration 1530, loss = 0.18119351\n",
      "Iteration 1153, loss = 0.24137878\n",
      "Iteration 1154, loss = 0.24120068\n",
      "Iteration 1427, loss = 0.25820845\n",
      "Iteration 773, loss = 0.31421006\n",
      "Iteration 587, loss = 0.36383527\n",
      "Iteration 774, loss = 0.31412797\n",
      "Iteration 1428, loss = 0.25807803\n",
      "Iteration 350, loss = 0.37730068\n",
      "Iteration 775, loss = 0.31402609\n",
      "Iteration 1155, loss = 0.24104422\n",
      "Iteration 1531, loss = 0.18104350\n",
      "Iteration 32, loss = 0.69669128\n",
      "Iteration 1156, loss = 0.24092221\n",
      "Iteration 588, loss = 0.36370526\n",
      "Iteration 1157, loss = 0.24088349\n",
      "Iteration 1532, loss = 0.18093151\n",
      "Iteration 1405, loss = 0.20228614\n",
      "Iteration 776, loss = 0.31392133\n",
      "Iteration 1158, loss = 0.24062458\n",
      "Iteration 33, loss = 0.69402130\n",
      "Iteration 351, loss = 0.37711381\n",
      "Iteration 1159, loss = 0.24043894\n",
      "Iteration 777, loss = 0.31383527\n",
      "Iteration 1406, loss = 0.20225756\n",
      "Iteration 778, loss = 0.31373855\n",
      "Iteration 1160, loss = 0.24038588\n",
      "Iteration 1533, loss = 0.18077556\n",
      "Iteration 1429, loss = 0.25802450\n",
      "Iteration 1161, loss = 0.24014772\n",
      "Iteration 1162, loss = 0.23997607\n",
      "Iteration 34, loss = 0.69136300\n",
      "Iteration 1163, loss = 0.23979709\n",
      "Iteration 1534, loss = 0.18079768\n",
      "Iteration 1164, loss = 0.23965024\n",
      "Iteration 35, loss = 0.68856451\n",
      "Iteration 1535, loss = 0.18060447\n",
      "Iteration 1407, loss = 0.20223702\n",
      "Iteration 36, loss = 0.68591055\n",
      "Iteration 1536, loss = 0.18045926\n",
      "Iteration 1430, loss = 0.25794345\n",
      "Iteration 37, loss = 0.68318488\n",
      "Iteration 1408, loss = 0.20233067\n",
      "Iteration 1537, loss = 0.18011425\n",
      "Iteration 779, loss = 0.31363793\n",
      "Iteration 780, loss = 0.31355768\n",
      "Iteration 1538, loss = 0.17998433\n",
      "Iteration 1165, loss = 0.23954255\n",
      "Iteration 38, loss = 0.68048731\n",
      "Iteration 1431, loss = 0.25776629\n",
      "Iteration 1166, loss = 0.23933770\n",
      "Iteration 352, loss = 0.37701693\n",
      "Iteration 781, loss = 0.31348543\n",
      "Iteration 39, loss = 0.67774465\n",
      "Iteration 1167, loss = 0.23917912\n",
      "Iteration 1168, loss = 0.23907032\n",
      "Iteration 1539, loss = 0.17994107\n",
      "Iteration 782, loss = 0.31337899\n",
      "Iteration 353, loss = 0.37693227\n",
      "Iteration 1540, loss = 0.17964859\n",
      "Iteration 783, loss = 0.31329585\n",
      "Iteration 1169, loss = 0.23886696\n",
      "Iteration 1409, loss = 0.20178424\n",
      "Iteration 1541, loss = 0.17946840\n",
      "Iteration 589, loss = 0.36349168\n",
      "Iteration 1170, loss = 0.23878535\n",
      "Iteration 40, loss = 0.67488510\n",
      "Iteration 784, loss = 0.31319380\n",
      "Iteration 1171, loss = 0.23854673\n",
      "Iteration 1410, loss = 0.20149996\n",
      "Iteration 1542, loss = 0.17934737\n",
      "Iteration 41, loss = 0.67217921\n",
      "Iteration 1172, loss = 0.23839574\n",
      "Iteration 1432, loss = 0.25768683\n",
      "Iteration 1173, loss = 0.23824900\n",
      "Iteration 785, loss = 0.31313452\n",
      "Iteration 1543, loss = 0.17913578\n",
      "Iteration 354, loss = 0.37678225\n",
      "Iteration 1174, loss = 0.23818562\n",
      "Iteration 1175, loss = 0.23792731\n",
      "Iteration 786, loss = 0.31303745\n",
      "Iteration 1544, loss = 0.17903157\n",
      "Iteration 590, loss = 0.36333288\n",
      "Iteration 1176, loss = 0.23777696\n",
      "Iteration 787, loss = 0.31299083\n",
      "Iteration 1411, loss = 0.20138567\n",
      "Iteration 1545, loss = 0.17890450\n",
      "Iteration 788, loss = 0.31285933\n",
      "Iteration 42, loss = 0.66933859\n",
      "Iteration 1177, loss = 0.23765170\n",
      "Iteration 1433, loss = 0.25756883\n",
      "Iteration 355, loss = 0.37666510\n",
      "Iteration 1546, loss = 0.17865521\n",
      "Iteration 1178, loss = 0.23746739\n",
      "Iteration 789, loss = 0.31274495\n",
      "Iteration 1179, loss = 0.23736109\n",
      "Iteration 1180, loss = 0.23716937\n",
      "Iteration 591, loss = 0.36314413\n",
      "Iteration 1181, loss = 0.23698458\n",
      "Iteration 356, loss = 0.37656101\n",
      "Iteration 790, loss = 0.31275534\n",
      "Iteration 1412, loss = 0.20123487\n",
      "Iteration 1182, loss = 0.23682167\n",
      "Iteration 43, loss = 0.66658218\n",
      "Iteration 1183, loss = 0.23667237\n",
      "Iteration 1434, loss = 0.25744449\n",
      "Iteration 1547, loss = 0.17860464\n",
      "Iteration 1184, loss = 0.23656963\n",
      "Iteration 592, loss = 0.36306258\n",
      "Iteration 1185, loss = 0.23633303\n",
      "Iteration 1435, loss = 0.25747739\n",
      "Iteration 1413, loss = 0.20113525\n",
      "Iteration 791, loss = 0.31256052\n",
      "Iteration 1186, loss = 0.23622544\n",
      "Iteration 1548, loss = 0.17834757\n",
      "Iteration 44, loss = 0.66375078\n",
      "Iteration 357, loss = 0.37640595\n",
      "Iteration 1187, loss = 0.23604210\n",
      "Iteration 1188, loss = 0.23602600\n",
      "Iteration 593, loss = 0.36283572\n",
      "Iteration 1549, loss = 0.17824218\n",
      "Iteration 1189, loss = 0.23582839\n",
      "Iteration 792, loss = 0.31247025\n",
      "Iteration 45, loss = 0.66090387\n",
      "Iteration 1190, loss = 0.23554418\n",
      "Iteration 1436, loss = 0.25727933\n",
      "Iteration 358, loss = 0.37632547\n",
      "Iteration 1191, loss = 0.23539197\n",
      "Iteration 1550, loss = 0.17835759\n",
      "Iteration 1414, loss = 0.20095794\n",
      "Iteration 1551, loss = 0.17801236\n",
      "Iteration 793, loss = 0.31241647\n",
      "Iteration 1192, loss = 0.23524215\n",
      "Iteration 359, loss = 0.37619174\n",
      "Iteration 594, loss = 0.36268136\n",
      "Iteration 46, loss = 0.65805285\n",
      "Iteration 1193, loss = 0.23514183\n",
      "Iteration 1437, loss = 0.25712066\n",
      "Iteration 1194, loss = 0.23499828\n",
      "Iteration 1195, loss = 0.23474210\n",
      "Iteration 1196, loss = 0.23461377\n",
      "Iteration 1197, loss = 0.23445510\n",
      "Iteration 1438, loss = 0.25702991\n",
      "Iteration 794, loss = 0.31229904\n",
      "Iteration 1198, loss = 0.23427852\n",
      "Iteration 1552, loss = 0.17779252\n",
      "Iteration 360, loss = 0.37607081\n",
      "Iteration 47, loss = 0.65518398\n",
      "Iteration 795, loss = 0.31217954\n",
      "Iteration 796, loss = 0.31211199\n",
      "Iteration 1199, loss = 0.23420187\n",
      "Iteration 595, loss = 0.36246509\n",
      "Iteration 1553, loss = 0.17773107\n",
      "Iteration 1439, loss = 0.25691712\n",
      "Iteration 48, loss = 0.65230853\n",
      "Iteration 1415, loss = 0.20076656\n",
      "Iteration 361, loss = 0.37596064\n",
      "Iteration 1200, loss = 0.23397398\n",
      "Iteration 1201, loss = 0.23387941\n",
      "Iteration 1202, loss = 0.23377080\n",
      "Iteration 596, loss = 0.36232632\n",
      "Iteration 1203, loss = 0.23353750\n",
      "Iteration 1416, loss = 0.20071531\n",
      "Iteration 1204, loss = 0.23334326\n",
      "Iteration 49, loss = 0.64936593\n",
      "Iteration 1205, loss = 0.23318578\n",
      "Iteration 797, loss = 0.31203774\n",
      "Iteration 362, loss = 0.37585869\n",
      "Iteration 50, loss = 0.64649667\n",
      "Iteration 1554, loss = 0.17742298\n",
      "Iteration 1417, loss = 0.20049829\n",
      "Iteration 798, loss = 0.31191440\n",
      "Iteration 51, loss = 0.64357283\n",
      "Iteration 1440, loss = 0.25683700\n",
      "Iteration 1555, loss = 0.17734123\n",
      "Iteration 52, loss = 0.64061263\n",
      "Iteration 1206, loss = 0.23302759\n",
      "Iteration 1556, loss = 0.17710810\n",
      "Iteration 799, loss = 0.31183082\n",
      "Iteration 597, loss = 0.36219675\n",
      "Iteration 1207, loss = 0.23283799\n",
      "Iteration 1441, loss = 0.25676416\n",
      "Iteration 1208, loss = 0.23265808\n",
      "Iteration 800, loss = 0.31173646\n",
      "Iteration 1209, loss = 0.23251762\n",
      "Iteration 1557, loss = 0.17699361\n",
      "Iteration 1210, loss = 0.23239476\n",
      "Iteration 801, loss = 0.31166393\n",
      "Iteration 1211, loss = 0.23222106\n",
      "Iteration 363, loss = 0.37573471\n",
      "Iteration 1212, loss = 0.23204398\n",
      "Iteration 802, loss = 0.31156867\n",
      "Iteration 1442, loss = 0.25667188\n",
      "Iteration 1213, loss = 0.23189828\n",
      "Iteration 803, loss = 0.31147802\n",
      "Iteration 1214, loss = 0.23174882\n",
      "Iteration 1558, loss = 0.17696330\n",
      "Iteration 1215, loss = 0.23181490\n",
      "Iteration 598, loss = 0.36195350\n",
      "Iteration 1559, loss = 0.17666904\n",
      "Iteration 53, loss = 0.63770086\n",
      "Iteration 1216, loss = 0.23141762\n",
      "Iteration 1217, loss = 0.23124797\n",
      "Iteration 1560, loss = 0.17654738\n",
      "Iteration 804, loss = 0.31138591\n",
      "Iteration 364, loss = 0.37564999\n",
      "Iteration 1218, loss = 0.23108618\n",
      "Iteration 1443, loss = 0.25665771\n",
      "Iteration 1219, loss = 0.23094850\n",
      "Iteration 54, loss = 0.63472894\n",
      "Iteration 1561, loss = 0.17641400\n",
      "Iteration 1418, loss = 0.20055896\n",
      "Iteration 805, loss = 0.31132908\n",
      "Iteration 806, loss = 0.31121357\n",
      "Iteration 1220, loss = 0.23076782\n",
      "Iteration 55, loss = 0.63178108\n",
      "Iteration 365, loss = 0.37554107\n",
      "Iteration 807, loss = 0.31109832\n",
      "Iteration 1444, loss = 0.25637901\n",
      "Iteration 1419, loss = 0.20013493\n",
      "Iteration 599, loss = 0.36184750\n",
      "Iteration 1221, loss = 0.23065372\n",
      "Iteration 808, loss = 0.31100185\n",
      "Iteration 1445, loss = 0.25630178\n",
      "Iteration 56, loss = 0.62878700\n",
      "Iteration 366, loss = 0.37548425\n",
      "Iteration 1222, loss = 0.23046087\n",
      "Iteration 809, loss = 0.31090995\n",
      "Iteration 1223, loss = 0.23026770\n",
      "Iteration 1446, loss = 0.25618998\n",
      "Iteration 810, loss = 0.31084267\n",
      "Iteration 1224, loss = 0.23009839\n",
      "Iteration 1420, loss = 0.20004386\n",
      "Iteration 600, loss = 0.36163831\n",
      "Iteration 57, loss = 0.62571423\n",
      "Iteration 1225, loss = 0.22996314\n",
      "Iteration 367, loss = 0.37529604\n",
      "Iteration 1226, loss = 0.22980799\n",
      "Iteration 1562, loss = 0.17631625\n",
      "Iteration 1227, loss = 0.22961594\n",
      "Iteration 1447, loss = 0.25604993\n",
      "Iteration 1228, loss = 0.22948797\n",
      "Iteration 1421, loss = 0.20017238\n",
      "Iteration 1229, loss = 0.22935691\n",
      "Iteration 368, loss = 0.37517041\n",
      "Iteration 1230, loss = 0.22914875\n",
      "Iteration 1231, loss = 0.22907983\n",
      "Iteration 1232, loss = 0.22883227\n",
      "Iteration 1448, loss = 0.25603265\n",
      "Iteration 811, loss = 0.31073799\n",
      "Iteration 1233, loss = 0.22867129\n",
      "Iteration 1234, loss = 0.22852423\n",
      "Iteration 1563, loss = 0.17608752\n",
      "Iteration 812, loss = 0.31063178\n",
      "Iteration 369, loss = 0.37509026\n",
      "Iteration 1235, loss = 0.22830102\n",
      "Iteration 1422, loss = 0.19994289\n",
      "Iteration 1449, loss = 0.25587393\n",
      "Iteration 1236, loss = 0.22813778\n",
      "Iteration 813, loss = 0.31054759\n",
      "Iteration 58, loss = 0.62276734\n",
      "Iteration 1564, loss = 0.17588586\n",
      "Iteration 370, loss = 0.37493740\n",
      "Iteration 601, loss = 0.36147983\n",
      "Iteration 1237, loss = 0.22803475\n",
      "Iteration 1565, loss = 0.17570780\n",
      "Iteration 814, loss = 0.31043122\n",
      "Iteration 59, loss = 0.61971681\n",
      "Iteration 1450, loss = 0.25580355\n",
      "Iteration 1238, loss = 0.22780889\n",
      "Iteration 1239, loss = 0.22763807\n",
      "Iteration 815, loss = 0.31037176\n",
      "Iteration 60, loss = 0.61668911\n",
      "Iteration 371, loss = 0.37484384\n",
      "Iteration 1240, loss = 0.22747436\n",
      "Iteration 816, loss = 0.31025044\n",
      "Iteration 1451, loss = 0.25570749\n",
      "Iteration 1423, loss = 0.19958649\n",
      "Iteration 1241, loss = 0.22733736\n",
      "Iteration 817, loss = 0.31017683\n",
      "Iteration 818, loss = 0.31006873\n",
      "Iteration 1242, loss = 0.22712954\n",
      "Iteration 61, loss = 0.61365294\n",
      "Iteration 1243, loss = 0.22693991\n",
      "Iteration 819, loss = 0.31001393\n",
      "Iteration 1244, loss = 0.22689568\n",
      "Iteration 1452, loss = 0.25558474\n",
      "Iteration 1566, loss = 0.17558512\n",
      "Iteration 62, loss = 0.61066100\n",
      "Iteration 820, loss = 0.30989038\n",
      "Iteration 1245, loss = 0.22669048\n",
      "Iteration 1567, loss = 0.17556408\n",
      "Iteration 1424, loss = 0.19940911\n",
      "Iteration 372, loss = 0.37472235\n",
      "Iteration 63, loss = 0.60749758\n",
      "Iteration 1246, loss = 0.22647985\n",
      "Iteration 602, loss = 0.36133589\n",
      "Iteration 821, loss = 0.30981254\n",
      "Iteration 1247, loss = 0.22628709\n",
      "Iteration 1248, loss = 0.22616589\n",
      "Iteration 822, loss = 0.30970515\n",
      "Iteration 1568, loss = 0.17523165\n",
      "Iteration 1453, loss = 0.25544260\n",
      "Iteration 1249, loss = 0.22596592\n",
      "Iteration 1250, loss = 0.22585233\n",
      "Iteration 1425, loss = 0.19924469\n",
      "Iteration 823, loss = 0.30964790\n",
      "Iteration 1569, loss = 0.17512022\n",
      "Iteration 64, loss = 0.60456381\n",
      "Iteration 1251, loss = 0.22565738\n",
      "Iteration 824, loss = 0.30951207\n",
      "Iteration 603, loss = 0.36114250\n",
      "Iteration 1570, loss = 0.17504997\n",
      "Iteration 1252, loss = 0.22554341\n",
      "Iteration 373, loss = 0.37460765\n",
      "Iteration 1253, loss = 0.22546168\n",
      "Iteration 65, loss = 0.60147529\n",
      "Iteration 825, loss = 0.30946753\n",
      "Iteration 1454, loss = 0.25531408\n",
      "Iteration 1571, loss = 0.17489149\n",
      "Iteration 826, loss = 0.30934289\n",
      "Iteration 1572, loss = 0.17468001\n",
      "Iteration 604, loss = 0.36097252\n",
      "Iteration 827, loss = 0.30927168\n",
      "Iteration 1254, loss = 0.22521383\n",
      "Iteration 1426, loss = 0.19903253\n",
      "Iteration 66, loss = 0.59844848\n",
      "Iteration 1573, loss = 0.17445804\n",
      "Iteration 1455, loss = 0.25527849\n",
      "Iteration 1255, loss = 0.22495520\n",
      "Iteration 67, loss = 0.59537759\n",
      "Iteration 1574, loss = 0.17435073\n",
      "Iteration 374, loss = 0.37450381\n",
      "Iteration 828, loss = 0.30916468\n",
      "Iteration 1256, loss = 0.22481507\n",
      "Iteration 68, loss = 0.59236215\n",
      "Iteration 1456, loss = 0.25513341\n",
      "Iteration 829, loss = 0.30907891\n",
      "Iteration 605, loss = 0.36077639\n",
      "Iteration 1575, loss = 0.17427524\n",
      "Iteration 1257, loss = 0.22473046\n",
      "Iteration 1457, loss = 0.25505350\n",
      "Iteration 375, loss = 0.37441120\n",
      "Iteration 69, loss = 0.58936125\n",
      "Iteration 1427, loss = 0.19895779\n",
      "Iteration 830, loss = 0.30898296\n",
      "Iteration 1258, loss = 0.22445601\n",
      "Iteration 1576, loss = 0.17400774\n",
      "Iteration 606, loss = 0.36065763\n",
      "Iteration 70, loss = 0.58636949\n",
      "Iteration 831, loss = 0.30889585\n",
      "Iteration 376, loss = 0.37430751\n",
      "Iteration 1577, loss = 0.17394391\n",
      "Iteration 1259, loss = 0.22434315\n",
      "Iteration 1428, loss = 0.19880126\n",
      "Iteration 1458, loss = 0.25492305\n",
      "Iteration 1578, loss = 0.17413485\n",
      "Iteration 832, loss = 0.30876561\n",
      "Iteration 71, loss = 0.58329554\n",
      "Iteration 1579, loss = 0.17375120\n",
      "Iteration 607, loss = 0.36055499\n",
      "Iteration 833, loss = 0.30870662\n",
      "Iteration 1459, loss = 0.25478669\n",
      "Iteration 1260, loss = 0.22412331\n",
      "Iteration 72, loss = 0.58033675\n",
      "Iteration 1261, loss = 0.22399514\n",
      "Iteration 834, loss = 0.30859564\n",
      "Iteration 377, loss = 0.37419502\n",
      "Iteration 1262, loss = 0.22378766\n",
      "Iteration 835, loss = 0.30849410\n",
      "Iteration 1429, loss = 0.19865088\n",
      "Iteration 1580, loss = 0.17346769\n",
      "Iteration 1263, loss = 0.22365027\n",
      "Iteration 1460, loss = 0.25471046\n",
      "Iteration 608, loss = 0.36042432\n",
      "Iteration 1264, loss = 0.22354931\n",
      "Iteration 1581, loss = 0.17328917\n",
      "Iteration 1265, loss = 0.22326091\n",
      "Iteration 378, loss = 0.37407129\n",
      "Iteration 836, loss = 0.30839722\n",
      "Iteration 73, loss = 0.57738344\n",
      "Iteration 1266, loss = 0.22310675Iteration 837, loss = 0.30830755\n",
      "\n",
      "Iteration 1461, loss = 0.25461267\n",
      "Iteration 838, loss = 0.30824501\n",
      "Iteration 1267, loss = 0.22293392\n",
      "Iteration 74, loss = 0.57433808\n",
      "Iteration 839, loss = 0.30817090\n",
      "Iteration 1462, loss = 0.25461893\n",
      "Iteration 609, loss = 0.36015166\n",
      "Iteration 1430, loss = 0.19848390\n",
      "Iteration 1268, loss = 0.22280819\n",
      "Iteration 379, loss = 0.37403163\n",
      "Iteration 840, loss = 0.30804440\n",
      "Iteration 1582, loss = 0.17323574\n",
      "Iteration 1269, loss = 0.22260520\n",
      "Iteration 1463, loss = 0.25438512\n",
      "Iteration 841, loss = 0.30794072\n",
      "Iteration 1270, loss = 0.22242447\n",
      "Iteration 1431, loss = 0.19828975\n",
      "Iteration 1271, loss = 0.22237200\n",
      "Iteration 75, loss = 0.57138931\n",
      "Iteration 380, loss = 0.37383248\n",
      "Iteration 1583, loss = 0.17298029\n",
      "Iteration 1272, loss = 0.22218047\n",
      "Iteration 610, loss = 0.35993384\n",
      "Iteration 1273, loss = 0.22209847\n",
      "Iteration 1274, loss = 0.22180495\n",
      "Iteration 1432, loss = 0.19833849\n",
      "Iteration 1584, loss = 0.17286712\n",
      "Iteration 1275, loss = 0.22176369\n",
      "Iteration 1464, loss = 0.25434342\n",
      "Iteration 1276, loss = 0.22147967\n",
      "Iteration 381, loss = 0.37377718\n",
      "Iteration 1277, loss = 0.22124849\n",
      "Iteration 1585, loss = 0.17289343\n",
      "Iteration 1278, loss = 0.22114296\n",
      "Iteration 1279, loss = 0.22100794\n",
      "Iteration 76, loss = 0.56844234\n",
      "Iteration 1433, loss = 0.19810704\n",
      "Iteration 1280, loss = 0.22086668\n",
      "Iteration 842, loss = 0.30786312\n",
      "Iteration 1586, loss = 0.17260111\n",
      "Iteration 1281, loss = 0.22061650\n",
      "Iteration 77, loss = 0.56544299\n",
      "Iteration 1282, loss = 0.22060127\n",
      "Iteration 843, loss = 0.30775514\n",
      "Iteration 1465, loss = 0.25417944\n",
      "Iteration 1587, loss = 0.17266048\n",
      "Iteration 611, loss = 0.35985816\n",
      "Iteration 1283, loss = 0.22027373\n",
      "Iteration 382, loss = 0.37368498\n",
      "Iteration 844, loss = 0.30768947\n",
      "Iteration 1588, loss = 0.17234246\n",
      "Iteration 78, loss = 0.56255091\n",
      "Iteration 845, loss = 0.30755236\n",
      "Iteration 1284, loss = 0.22006457\n",
      "Iteration 1434, loss = 0.19784404\n",
      "Iteration 846, loss = 0.30747556\n",
      "Iteration 79, loss = 0.55966071\n",
      "Iteration 1589, loss = 0.17215259\n",
      "Iteration 383, loss = 0.37351214\n",
      "Iteration 80, loss = 0.55675613\n",
      "Iteration 1590, loss = 0.17206127\n",
      "Iteration 847, loss = 0.30736840\n",
      "Iteration 384, loss = 0.37341452\n",
      "Iteration 81, loss = 0.55401052\n",
      "Iteration 1591, loss = 0.17181041\n",
      "Iteration 1466, loss = 0.25414791\n",
      "Iteration 1285, loss = 0.21989873\n",
      "Iteration 1435, loss = 0.19790868\n",
      "Iteration 848, loss = 0.30725946\n",
      "Iteration 1286, loss = 0.21970998\n",
      "Iteration 1287, loss = 0.21960188\n",
      "Iteration 849, loss = 0.30719923\n",
      "Iteration 1288, loss = 0.21939419\n",
      "Iteration 1592, loss = 0.17166430\n",
      "Iteration 1289, loss = 0.21922199\n",
      "Iteration 82, loss = 0.55110489\n",
      "Iteration 850, loss = 0.30713175\n",
      "Iteration 385, loss = 0.37328486\n",
      "Iteration 1593, loss = 0.17154854\n",
      "Iteration 612, loss = 0.35962768\n",
      "Iteration 1467, loss = 0.25397196\n",
      "Iteration 83, loss = 0.54841977\n",
      "Iteration 1436, loss = 0.19785265\n",
      "Iteration 1290, loss = 0.21904267\n",
      "Iteration 851, loss = 0.30713459\n",
      "Iteration 84, loss = 0.54562695\n",
      "Iteration 852, loss = 0.30694304\n",
      "Iteration 1468, loss = 0.25387596\n",
      "Iteration 1291, loss = 0.21888957\n",
      "Iteration 1594, loss = 0.17140835\n",
      "Iteration 386, loss = 0.37319971\n",
      "Iteration 853, loss = 0.30681942\n",
      "Iteration 1292, loss = 0.21874771\n",
      "Iteration 854, loss = 0.30671437\n",
      "Iteration 613, loss = 0.35946360\n",
      "Iteration 1293, loss = 0.21854209\n",
      "Iteration 1437, loss = 0.19741577\n",
      "Iteration 1469, loss = 0.25379607\n",
      "Iteration 1595, loss = 0.17131031\n",
      "Iteration 85, loss = 0.54286497\n",
      "Iteration 1294, loss = 0.21839269\n",
      "Iteration 387, loss = 0.37306947\n",
      "Iteration 855, loss = 0.30662592\n",
      "Iteration 1438, loss = 0.19747868\n",
      "Iteration 1295, loss = 0.21832964\n",
      "Iteration 614, loss = 0.35926193\n",
      "Iteration 86, loss = 0.54009810Iteration 1596, loss = 0.17104733\n",
      "Iteration 1296, loss = 0.21807726\n",
      "Iteration 856, loss = 0.30653251\n",
      "\n",
      "Iteration 1297, loss = 0.21792166\n",
      "Iteration 388, loss = 0.37296861\n",
      "Iteration 1470, loss = 0.25365979\n",
      "Iteration 1298, loss = 0.21773951\n",
      "Iteration 1597, loss = 0.17092518\n",
      "Iteration 1439, loss = 0.19709015\n",
      "Iteration 857, loss = 0.30646720\n",
      "Iteration 87, loss = 0.53744248\n",
      "Iteration 1299, loss = 0.21754459\n",
      "Iteration 1471, loss = 0.25369033\n",
      "Iteration 1300, loss = 0.21743628\n",
      "Iteration 615, loss = 0.35921101\n",
      "Iteration 389, loss = 0.37286403\n",
      "Iteration 1301, loss = 0.21723928\n",
      "Iteration 858, loss = 0.30633528\n",
      "Iteration 1302, loss = 0.21714748\n",
      "Iteration 88, loss = 0.53478315\n",
      "Iteration 1303, loss = 0.21692201\n",
      "Iteration 859, loss = 0.30624767\n",
      "Iteration 1598, loss = 0.17075968\n",
      "Iteration 1472, loss = 0.25343039\n",
      "Iteration 1304, loss = 0.21673498\n",
      "Iteration 860, loss = 0.30615254\n",
      "Iteration 1599, loss = 0.17060259\n",
      "Iteration 1440, loss = 0.19694896\n",
      "Iteration 616, loss = 0.35893841\n",
      "Iteration 1305, loss = 0.21654392\n",
      "Iteration 861, loss = 0.30604921\n",
      "Iteration 89, loss = 0.53215717\n",
      "Iteration 390, loss = 0.37276374\n",
      "Iteration 1600, loss = 0.17061264\n",
      "Iteration 1306, loss = 0.21637130\n",
      "Iteration 862, loss = 0.30600638\n",
      "Iteration 1307, loss = 0.21622792\n",
      "Iteration 90, loss = 0.52956850\n",
      "Iteration 863, loss = 0.30589885\n",
      "Iteration 617, loss = 0.35879842\n",
      "Iteration 1601, loss = 0.17037835\n",
      "Iteration 1308, loss = 0.21615517\n",
      "Iteration 864, loss = 0.30578876\n",
      "Iteration 91, loss = 0.52704658\n",
      "Iteration 1473, loss = 0.25331863\n",
      "Iteration 1602, loss = 0.17026569\n",
      "Iteration 865, loss = 0.30570949\n",
      "Iteration 1309, loss = 0.21587515\n",
      "Iteration 1441, loss = 0.19679473\n",
      "Iteration 618, loss = 0.35861034\n",
      "Iteration 866, loss = 0.30568762\n",
      "Iteration 1310, loss = 0.21576443\n",
      "Iteration 391, loss = 0.37264908\n",
      "Iteration 1603, loss = 0.16999274\n",
      "Iteration 1311, loss = 0.21555674\n",
      "Iteration 867, loss = 0.30550071\n",
      "Iteration 92, loss = 0.52464987\n",
      "Iteration 1312, loss = 0.21541940\n",
      "Iteration 619, loss = 0.35851540\n",
      "Iteration 1313, loss = 0.21525971\n",
      "Iteration 1604, loss = 0.16985606\n",
      "Iteration 1474, loss = 0.25325879\n",
      "Iteration 1314, loss = 0.21517113Iteration 868, loss = 0.30540161\n",
      "Iteration 93, loss = 0.52212345\n",
      "Iteration 1442, loss = 0.19662237\n",
      "\n",
      "Iteration 392, loss = 0.37255027\n",
      "Iteration 1315, loss = 0.21487972\n",
      "Iteration 620, loss = 0.35825119\n",
      "Iteration 94, loss = 0.51971784\n",
      "Iteration 1605, loss = 0.16981248\n",
      "Iteration 1475, loss = 0.25312737\n",
      "Iteration 1316, loss = 0.21471994\n",
      "Iteration 869, loss = 0.30532009\n",
      "Iteration 1317, loss = 0.21459766\n",
      "Iteration 393, loss = 0.37243612\n",
      "Iteration 95, loss = 0.51731103\n",
      "Iteration 1318, loss = 0.21443639\n",
      "Iteration 1319, loss = 0.21426692\n",
      "Iteration 870, loss = 0.30526006\n",
      "Iteration 1606, loss = 0.16952636\n",
      "Iteration 1320, loss = 0.21412472\n",
      "Iteration 1321, loss = 0.21400523\n",
      "Iteration 1443, loss = 0.19648155\n",
      "Iteration 394, loss = 0.37235752\n",
      "Iteration 96, loss = 0.51494063\n",
      "Iteration 1476, loss = 0.25310384\n",
      "Iteration 621, loss = 0.35812738\n",
      "Iteration 1322, loss = 0.21381979\n",
      "Iteration 1607, loss = 0.16935866\n",
      "Iteration 871, loss = 0.30513722\n",
      "Iteration 1323, loss = 0.21360918\n",
      "Iteration 1324, loss = 0.21366332\n",
      "Iteration 97, loss = 0.51267990\n",
      "Iteration 1325, loss = 0.21331947\n",
      "Iteration 622, loss = 0.35794634\n",
      "Iteration 1477, loss = 0.25296310\n",
      "Iteration 1326, loss = 0.21314154\n",
      "Iteration 1444, loss = 0.19637297\n",
      "Iteration 872, loss = 0.30508301\n",
      "Iteration 1608, loss = 0.16925846\n",
      "Iteration 395, loss = 0.37221490\n",
      "Iteration 98, loss = 0.51046085\n",
      "Iteration 1327, loss = 0.21302386\n",
      "Iteration 1478, loss = 0.25282741\n",
      "Iteration 873, loss = 0.30495607\n",
      "Iteration 1328, loss = 0.21289636\n",
      "Iteration 623, loss = 0.35775955\n",
      "Iteration 1329, loss = 0.21263318\n",
      "Iteration 874, loss = 0.30485372\n",
      "Iteration 99, loss = 0.50816758\n",
      "Iteration 396, loss = 0.37210086\n",
      "Iteration 1330, loss = 0.21249722\n",
      "Iteration 1479, loss = 0.25269889\n",
      "Iteration 624, loss = 0.35761773\n",
      "Iteration 875, loss = 0.30478167\n",
      "Iteration 1445, loss = 0.19621858\n",
      "Iteration 1331, loss = 0.21228358\n",
      "Iteration 1609, loss = 0.16920316\n",
      "Iteration 100, loss = 0.50610008\n",
      "Iteration 1332, loss = 0.21220784\n",
      "Iteration 1480, loss = 0.25258829\n",
      "Iteration 876, loss = 0.30466620\n",
      "Iteration 1333, loss = 0.21198003\n",
      "Iteration 397, loss = 0.37199432\n",
      "Iteration 1334, loss = 0.21192932\n",
      "Iteration 877, loss = 0.30457787\n",
      "Iteration 1610, loss = 0.16904801\n",
      "Iteration 1335, loss = 0.21160562\n",
      "Iteration 625, loss = 0.35750408\n",
      "Iteration 1481, loss = 0.25251370\n",
      "Iteration 1336, loss = 0.21147447\n",
      "Iteration 101, loss = 0.50387337\n",
      "Iteration 1446, loss = 0.19604951\n",
      "Iteration 398, loss = 0.37190380\n",
      "Iteration 1337, loss = 0.21129161\n",
      "Iteration 1338, loss = 0.21121024\n",
      "Iteration 1611, loss = 0.16888811\n",
      "Iteration 878, loss = 0.30453570\n",
      "Iteration 1482, loss = 0.25241767\n",
      "Iteration 1339, loss = 0.21094595\n",
      "Iteration 879, loss = 0.30438811\n",
      "Iteration 1340, loss = 0.21087067\n",
      "Iteration 880, loss = 0.30434204\n",
      "Iteration 1447, loss = 0.19609072\n",
      "Iteration 626, loss = 0.35727121\n",
      "Iteration 102, loss = 0.50182562\n",
      "Iteration 1341, loss = 0.21067444\n",
      "Iteration 1342, loss = 0.21058857\n",
      "Iteration 1612, loss = 0.16868386\n",
      "Iteration 1343, loss = 0.21034857\n",
      "Iteration 103, loss = 0.49972763\n",
      "Iteration 1483, loss = 0.25238775\n",
      "Iteration 1344, loss = 0.21013553\n",
      "Iteration 1345, loss = 0.20998400\n",
      "Iteration 399, loss = 0.37178744\n",
      "Iteration 104, loss = 0.49773656\n",
      "Iteration 1346, loss = 0.20981635\n",
      "Iteration 1347, loss = 0.20964790\n",
      "Iteration 1348, loss = 0.20965218\n",
      "Iteration 881, loss = 0.30428268\n",
      "Iteration 105, loss = 0.49566839Iteration 1349, loss = 0.20936500\n",
      "\n",
      "Iteration 627, loss = 0.35708419\n",
      "Iteration 1613, loss = 0.16863975\n",
      "Iteration 1350, loss = 0.20921786\n",
      "Iteration 882, loss = 0.30414397\n",
      "Iteration 1351, loss = 0.20913011\n",
      "Iteration 1484, loss = 0.25217607\n",
      "Iteration 1448, loss = 0.19597313Iteration 883, loss = 0.30404593\n",
      "\n",
      "Iteration 400, loss = 0.37168313\n",
      "Iteration 106, loss = 0.49379833\n",
      "Iteration 1352, loss = 0.20890056\n",
      "Iteration 1614, loss = 0.16842832\n",
      "Iteration 1485, loss = 0.25218928\n",
      "Iteration 1353, loss = 0.20872092\n",
      "Iteration 884, loss = 0.30401098\n",
      "Iteration 1615, loss = 0.16828913\n",
      "Iteration 628, loss = 0.35694641\n",
      "Iteration 1354, loss = 0.20845522\n",
      "Iteration 885, loss = 0.30384043\n",
      "Iteration 107, loss = 0.49192474\n",
      "Iteration 1486, loss = 0.25195220\n",
      "Iteration 1355, loss = 0.20831770\n",
      "Iteration 1616, loss = 0.16801552\n",
      "Iteration 401, loss = 0.37157017\n",
      "Iteration 1356, loss = 0.20818049\n",
      "Iteration 886, loss = 0.30373843\n",
      "Iteration 1449, loss = 0.19570053\n",
      "Iteration 1357, loss = 0.20802930\n",
      "Iteration 629, loss = 0.35677251\n",
      "Iteration 402, loss = 0.37148501\n",
      "Iteration 1358, loss = 0.20780068\n",
      "Iteration 108, loss = 0.48997260\n",
      "Iteration 1487, loss = 0.25185262\n",
      "Iteration 1617, loss = 0.16787707\n",
      "Iteration 1359, loss = 0.20769371\n",
      "Iteration 887, loss = 0.30365803\n",
      "Iteration 1360, loss = 0.20752164\n",
      "Iteration 1361, loss = 0.20747738\n",
      "Iteration 1488, loss = 0.25181998\n",
      "Iteration 109, loss = 0.48813993\n",
      "Iteration 630, loss = 0.35667784\n",
      "Iteration 1362, loss = 0.20720011\n",
      "Iteration 403, loss = 0.37136118\n",
      "Iteration 1618, loss = 0.16786070\n",
      "Iteration 1363, loss = 0.20702735\n",
      "Iteration 1489, loss = 0.25171561\n",
      "Iteration 110, loss = 0.48637199\n",
      "Iteration 888, loss = 0.30356183\n",
      "Iteration 1450, loss = 0.19544725\n",
      "Iteration 1364, loss = 0.20699631\n",
      "Iteration 631, loss = 0.35643607\n",
      "Iteration 111, loss = 0.48455896\n",
      "Iteration 1365, loss = 0.20665136\n",
      "Iteration 1619, loss = 0.16762151\n",
      "Iteration 404, loss = 0.37127576\n",
      "Iteration 1366, loss = 0.20675236\n",
      "Iteration 889, loss = 0.30346959\n",
      "Iteration 1367, loss = 0.20634964\n",
      "Iteration 1490, loss = 0.25153199\n",
      "Iteration 112, loss = 0.48278146\n",
      "Iteration 1368, loss = 0.20617755\n",
      "Iteration 890, loss = 0.30338984\n",
      "Iteration 1369, loss = 0.20600845\n",
      "Iteration 1620, loss = 0.16749547\n",
      "Iteration 1370, loss = 0.20584969\n",
      "Iteration 891, loss = 0.30330433\n",
      "Iteration 405, loss = 0.37116155\n",
      "Iteration 1371, loss = 0.20579415\n",
      "Iteration 1621, loss = 0.16734573\n",
      "Iteration 1372, loss = 0.20556907\n",
      "Iteration 892, loss = 0.30319583\n",
      "Iteration 1373, loss = 0.20532277\n",
      "Iteration 406, loss = 0.37104086\n",
      "Iteration 1622, loss = 0.16715713\n",
      "Iteration 113, loss = 0.48110535\n",
      "Iteration 1491, loss = 0.25144304\n",
      "Iteration 1374, loss = 0.20514385\n",
      "Iteration 893, loss = 0.30311180\n",
      "Iteration 1451, loss = 0.19535106\n",
      "Iteration 894, loss = 0.30305072\n",
      "Iteration 407, loss = 0.37093319\n",
      "Iteration 1375, loss = 0.20511362\n",
      "Iteration 1376, loss = 0.20497852\n",
      "Iteration 1492, loss = 0.25129612\n",
      "Iteration 632, loss = 0.35626809\n",
      "Iteration 1623, loss = 0.16701761\n",
      "Iteration 114, loss = 0.47944016\n",
      "Iteration 1377, loss = 0.20473205\n",
      "Iteration 1378, loss = 0.20453763\n",
      "Iteration 1379, loss = 0.20442901\n",
      "Iteration 408, loss = 0.37085286\n",
      "Iteration 1452, loss = 0.19517138\n",
      "Iteration 1380, loss = 0.20419831\n",
      "Iteration 895, loss = 0.30291791\n",
      "Iteration 1381, loss = 0.20403311\n",
      "Iteration 633, loss = 0.35606386\n",
      "Iteration 1382, loss = 0.20385832\n",
      "Iteration 1624, loss = 0.16689564\n",
      "Iteration 896, loss = 0.30283448\n",
      "Iteration 1383, loss = 0.20378071\n",
      "Iteration 1493, loss = 0.25122426\n",
      "Iteration 897, loss = 0.30270700\n",
      "Iteration 634, loss = 0.35590424\n",
      "Iteration 115, loss = 0.47782427\n",
      "Iteration 409, loss = 0.37073992\n",
      "Iteration 1625, loss = 0.16688251\n",
      "Iteration 1384, loss = 0.20353177\n",
      "Iteration 1453, loss = 0.19510592\n",
      "Iteration 898, loss = 0.30267597\n",
      "Iteration 1385, loss = 0.20337638\n",
      "Iteration 1494, loss = 0.25110643\n",
      "Iteration 1626, loss = 0.16661157\n",
      "Iteration 1386, loss = 0.20322565\n",
      "Iteration 1387, loss = 0.20301097\n",
      "Iteration 116, loss = 0.47618719\n",
      "Iteration 1388, loss = 0.20288464\n",
      "Iteration 1627, loss = 0.16652734\n",
      "Iteration 410, loss = 0.37062984\n",
      "Iteration 1389, loss = 0.20277042\n",
      "Iteration 899, loss = 0.30256156\n",
      "Iteration 117, loss = 0.47457906\n",
      "Iteration 1390, loss = 0.20258540\n",
      "Iteration 1495, loss = 0.25105201\n",
      "Iteration 411, loss = 0.37052316\n",
      "Iteration 1391, loss = 0.20236974\n",
      "Iteration 1392, loss = 0.20227424\n",
      "Iteration 118, loss = 0.47298283\n",
      "Iteration 1628, loss = 0.16633790\n",
      "Iteration 1393, loss = 0.20216636\n",
      "Iteration 900, loss = 0.30249530\n",
      "Iteration 1394, loss = 0.20193254\n",
      "Iteration 1454, loss = 0.19484008\n",
      "Iteration 412, loss = 0.37043155\n",
      "Iteration 635, loss = 0.35574468\n",
      "Iteration 901, loss = 0.30237961\n",
      "Iteration 1496, loss = 0.25094058\n",
      "Iteration 1395, loss = 0.20188037\n",
      "Iteration 1396, loss = 0.20176086\n",
      "Iteration 119, loss = 0.47144303\n",
      "Iteration 1629, loss = 0.16621323\n",
      "Iteration 1397, loss = 0.20141005\n",
      "Iteration 902, loss = 0.30225343\n",
      "Iteration 413, loss = 0.37030633\n",
      "Iteration 1497, loss = 0.25078383\n",
      "Iteration 1398, loss = 0.20123062\n",
      "Iteration 1630, loss = 0.16604735\n",
      "Iteration 903, loss = 0.30214723\n",
      "Iteration 1399, loss = 0.20110938\n",
      "Iteration 1400, loss = 0.20092555\n",
      "Iteration 904, loss = 0.30209257\n",
      "Iteration 1631, loss = 0.16589674\n",
      "Iteration 1498, loss = 0.25072434\n",
      "Iteration 120, loss = 0.47001008\n",
      "Iteration 1401, loss = 0.20081710\n",
      "Iteration 905, loss = 0.30198172\n",
      "Iteration 1402, loss = 0.20055361\n",
      "Iteration 1632, loss = 0.16584721\n",
      "Iteration 1455, loss = 0.19488873\n",
      "Iteration 1403, loss = 0.20050055\n",
      "Iteration 414, loss = 0.37024171\n",
      "Iteration 1404, loss = 0.20026345\n",
      "Iteration 1633, loss = 0.16562200\n",
      "Iteration 1405, loss = 0.20012917\n",
      "Iteration 636, loss = 0.35559362\n",
      "Iteration 906, loss = 0.30186671\n",
      "Iteration 1406, loss = 0.19992367\n",
      "Iteration 121, loss = 0.46854463\n",
      "Iteration 1407, loss = 0.19983973\n",
      "Iteration 415, loss = 0.37009169\n",
      "Iteration 1499, loss = 0.25061793\n",
      "Iteration 1408, loss = 0.19958354\n",
      "Iteration 122, loss = 0.46708602\n",
      "Iteration 1634, loss = 0.16542790\n",
      "Iteration 123, loss = 0.46563550\n",
      "Iteration 1635, loss = 0.16535138\n",
      "Iteration 1409, loss = 0.19950426\n",
      "Iteration 1456, loss = 0.19460681\n",
      "Iteration 1500, loss = 0.25051781\n",
      "Iteration 907, loss = 0.30179465\n",
      "Iteration 637, loss = 0.35540776\n",
      "Iteration 416, loss = 0.37003848\n",
      "Iteration 124, loss = 0.46434343\n",
      "Iteration 1636, loss = 0.16518470\n",
      "Iteration 1410, loss = 0.19935164\n",
      "Iteration 908, loss = 0.30172765\n",
      "Iteration 1411, loss = 0.19914317\n",
      "Iteration 909, loss = 0.30160906\n",
      "Iteration 638, loss = 0.35522457\n",
      "Iteration 1637, loss = 0.16507546\n",
      "Iteration 1412, loss = 0.19892373\n",
      "Iteration 910, loss = 0.30151534\n",
      "Iteration 417, loss = 0.36993025\n",
      "Iteration 1501, loss = 0.25043902\n",
      "Iteration 1457, loss = 0.19451827\n",
      "Iteration 125, loss = 0.46291669\n",
      "Iteration 1413, loss = 0.19877119\n",
      "Iteration 1414, loss = 0.19861275\n",
      "Iteration 639, loss = 0.35507339\n",
      "Iteration 911, loss = 0.30141365\n",
      "Iteration 1415, loss = 0.19840174\n",
      "Iteration 1502, loss = 0.25038175\n",
      "Iteration 1638, loss = 0.16485249\n",
      "Iteration 1416, loss = 0.19824556\n",
      "Iteration 912, loss = 0.30130801\n",
      "Iteration 1639, loss = 0.16469182\n",
      "Iteration 418, loss = 0.36978565\n",
      "Iteration 126, loss = 0.46159873\n",
      "Iteration 1417, loss = 0.19807026\n",
      "Iteration 640, loss = 0.35489276\n",
      "Iteration 1458, loss = 0.19437399\n",
      "Iteration 913, loss = 0.30123728\n",
      "Iteration 1503, loss = 0.25024346\n",
      "Iteration 1640, loss = 0.16482150\n",
      "Iteration 1418, loss = 0.19791055\n",
      "Iteration 914, loss = 0.30110564\n",
      "Iteration 127, loss = 0.46033107\n",
      "Iteration 1504, loss = 0.25020240\n",
      "Iteration 915, loss = 0.30103275\n",
      "Iteration 1419, loss = 0.19771057\n",
      "Iteration 1420, loss = 0.19757377\n",
      "Iteration 128, loss = 0.45899382\n",
      "Iteration 916, loss = 0.30094423\n",
      "Iteration 419, loss = 0.36971339\n",
      "Iteration 1641, loss = 0.16444359\n",
      "Iteration 1421, loss = 0.19742542\n",
      "Iteration 917, loss = 0.30084618\n",
      "Iteration 1459, loss = 0.19406669\n",
      "Iteration 129, loss = 0.45784838\n",
      "Iteration 918, loss = 0.30074146\n",
      "Iteration 1505, loss = 0.25000268\n",
      "Iteration 641, loss = 0.35490066\n",
      "Iteration 1642, loss = 0.16436309\n",
      "Iteration 420, loss = 0.36957753\n",
      "Iteration 1422, loss = 0.19730938\n",
      "Iteration 1423, loss = 0.19732067\n",
      "Iteration 919, loss = 0.30068164\n",
      "Iteration 421, loss = 0.36947879\n",
      "Iteration 130, loss = 0.45650487\n",
      "Iteration 1506, loss = 0.24987105\n",
      "Iteration 1643, loss = 0.16407967\n",
      "Iteration 1424, loss = 0.19692039\n",
      "Iteration 1425, loss = 0.19685135\n",
      "Iteration 920, loss = 0.30058115\n",
      "Iteration 1426, loss = 0.19662085\n",
      "Iteration 642, loss = 0.35455697\n",
      "Iteration 1644, loss = 0.16398612\n",
      "Iteration 921, loss = 0.30044846\n",
      "Iteration 422, loss = 0.36940097\n",
      "Iteration 1427, loss = 0.19642000\n",
      "Iteration 1460, loss = 0.19394251\n",
      "Iteration 1428, loss = 0.19636313\n",
      "Iteration 131, loss = 0.45537050\n",
      "Iteration 922, loss = 0.30034004\n",
      "Iteration 1645, loss = 0.16376466\n",
      "Iteration 1429, loss = 0.19616563\n",
      "Iteration 1507, loss = 0.24978334\n",
      "Iteration 923, loss = 0.30024602\n",
      "Iteration 423, loss = 0.36937762Iteration 132, loss = 0.45415146\n",
      "\n",
      "Iteration 1646, loss = 0.16363720\n",
      "Iteration 1430, loss = 0.19596021\n",
      "Iteration 924, loss = 0.30017237\n",
      "Iteration 1508, loss = 0.24979492\n",
      "Iteration 1431, loss = 0.19579277\n",
      "Iteration 643, loss = 0.35444925\n",
      "Iteration 1647, loss = 0.16346932\n",
      "Iteration 133, loss = 0.45294310\n",
      "Iteration 925, loss = 0.30006183\n",
      "Iteration 1432, loss = 0.19560318\n",
      "Iteration 1461, loss = 0.19386752\n",
      "Iteration 134, loss = 0.45185365\n",
      "Iteration 1509, loss = 0.24961076\n",
      "Iteration 1433, loss = 0.19546204\n",
      "Iteration 926, loss = 0.29997347\n",
      "Iteration 1434, loss = 0.19539964\n",
      "Iteration 135, loss = 0.45075776\n",
      "Iteration 1648, loss = 0.16345595\n",
      "Iteration 424, loss = 0.36915899\n",
      "Iteration 927, loss = 0.29986843\n",
      "Iteration 928, loss = 0.29981344\n",
      "Iteration 1510, loss = 0.24950123\n",
      "Iteration 1435, loss = 0.19515943\n",
      "Iteration 136, loss = 0.44964954\n",
      "Iteration 929, loss = 0.29969176\n",
      "Iteration 644, loss = 0.35427432\n",
      "Iteration 1436, loss = 0.19506963\n",
      "Iteration 1462, loss = 0.19363305\n",
      "Iteration 1649, loss = 0.16311150\n",
      "Iteration 1437, loss = 0.19490077\n",
      "Iteration 930, loss = 0.29958006\n",
      "Iteration 1438, loss = 0.19470694\n",
      "Iteration 1439, loss = 0.19445242\n",
      "Iteration 1650, loss = 0.16297486\n",
      "Iteration 1511, loss = 0.24933603\n",
      "Iteration 1440, loss = 0.19430227\n",
      "Iteration 425, loss = 0.36905894\n",
      "Iteration 137, loss = 0.44855161\n",
      "Iteration 1441, loss = 0.19413910\n",
      "Iteration 1651, loss = 0.16289907\n",
      "Iteration 1442, loss = 0.19411176\n",
      "Iteration 1443, loss = 0.19386450\n",
      "Iteration 138, loss = 0.44749419\n",
      "Iteration 931, loss = 0.29947124\n",
      "Iteration 1444, loss = 0.19363799\n",
      "Iteration 645, loss = 0.35405316\n",
      "Iteration 1512, loss = 0.24929325\n",
      "Iteration 1652, loss = 0.16285189\n",
      "Iteration 426, loss = 0.36894764\n",
      "Iteration 1445, loss = 0.19350114\n",
      "Iteration 1446, loss = 0.19339541\n",
      "Iteration 1463, loss = 0.19353469\n",
      "Iteration 932, loss = 0.29937228\n",
      "Iteration 1447, loss = 0.19314653\n",
      "Iteration 1653, loss = 0.16264937\n",
      "Iteration 1448, loss = 0.19313417\n",
      "Iteration 427, loss = 0.36885541\n",
      "Iteration 1449, loss = 0.19289367\n",
      "Iteration 1654, loss = 0.16250851\n",
      "Iteration 646, loss = 0.35388577\n",
      "Iteration 139, loss = 0.44647118\n",
      "Iteration 1450, loss = 0.19266289\n",
      "Iteration 428, loss = 0.36873932\n",
      "Iteration 1451, loss = 0.19256447\n",
      "Iteration 1513, loss = 0.24919833\n",
      "Iteration 1655, loss = 0.16229786\n",
      "Iteration 1452, loss = 0.19250389\n",
      "Iteration 1453, loss = 0.19221091\n",
      "Iteration 1454, loss = 0.19215784\n",
      "Iteration 647, loss = 0.35371217\n",
      "Iteration 933, loss = 0.29929032\n",
      "Iteration 429, loss = 0.36869110\n",
      "Iteration 140, loss = 0.44541104\n",
      "Iteration 934, loss = 0.29922539\n",
      "Iteration 1455, loss = 0.19213452\n",
      "Iteration 1456, loss = 0.19170418\n",
      "Iteration 141, loss = 0.44446487\n",
      "Iteration 1514, loss = 0.24902051\n",
      "Iteration 1457, loss = 0.19163201\n",
      "Iteration 935, loss = 0.29911158\n",
      "Iteration 1464, loss = 0.19336878\n",
      "Iteration 142, loss = 0.44346036\n",
      "Iteration 1656, loss = 0.16224292\n",
      "Iteration 430, loss = 0.36855236\n",
      "Iteration 1458, loss = 0.19139330\n",
      "Iteration 936, loss = 0.29904360\n",
      "Iteration 648, loss = 0.35356123\n",
      "Iteration 143, loss = 0.44254695\n",
      "Iteration 1459, loss = 0.19129060\n",
      "Iteration 1460, loss = 0.19110383\n",
      "Iteration 1461, loss = 0.19094693\n",
      "Iteration 144, loss = 0.44155403\n",
      "Iteration 1657, loss = 0.16195351\n",
      "Iteration 1515, loss = 0.24893675\n",
      "Iteration 937, loss = 0.29891193\n",
      "Iteration 1462, loss = 0.19078856\n",
      "Iteration 649, loss = 0.35338606\n",
      "Iteration 1465, loss = 0.19346727\n",
      "Iteration 1658, loss = 0.16192502\n",
      "Iteration 1463, loss = 0.19060398\n",
      "Iteration 145, loss = 0.44066555\n",
      "Iteration 938, loss = 0.29878750\n",
      "Iteration 1516, loss = 0.24883371\n",
      "Iteration 431, loss = 0.36842993\n",
      "Iteration 1464, loss = 0.19048498\n",
      "Iteration 650, loss = 0.35322459\n",
      "Iteration 939, loss = 0.29870712\n",
      "Iteration 1465, loss = 0.19035406\n",
      "Iteration 1466, loss = 0.19311253\n",
      "Iteration 432, loss = 0.36834158\n",
      "Iteration 940, loss = 0.29859833\n",
      "Iteration 1466, loss = 0.19018493\n",
      "Iteration 146, loss = 0.43970353\n",
      "Iteration 1659, loss = 0.16165866\n",
      "Iteration 941, loss = 0.29853029\n",
      "Iteration 433, loss = 0.36828053\n",
      "Iteration 1467, loss = 0.18999845\n",
      "Iteration 147, loss = 0.43886647\n",
      "Iteration 1660, loss = 0.16155551\n",
      "Iteration 942, loss = 0.29845547\n",
      "Iteration 148, loss = 0.43794683\n",
      "Iteration 1517, loss = 0.24872342\n",
      "Iteration 1661, loss = 0.16145406\n",
      "Iteration 434, loss = 0.36816779\n",
      "Iteration 1468, loss = 0.18984496\n",
      "Iteration 651, loss = 0.35318651\n",
      "Iteration 943, loss = 0.29832103\n",
      "Iteration 1467, loss = 0.19292396\n",
      "Iteration 1469, loss = 0.18972560\n",
      "Iteration 435, loss = 0.36804485Iteration 944, loss = 0.29824452\n",
      "\n",
      "Iteration 1662, loss = 0.16169544\n",
      "Iteration 945, loss = 0.29821136\n",
      "Iteration 1470, loss = 0.18946629\n",
      "Iteration 1663, loss = 0.16126448\n",
      "Iteration 1471, loss = 0.18935808\n",
      "Iteration 1518, loss = 0.24862641\n",
      "Iteration 149, loss = 0.43715982\n",
      "Iteration 1472, loss = 0.18915940\n",
      "Iteration 1473, loss = 0.18897418\n",
      "Iteration 1664, loss = 0.16097517\n",
      "Iteration 946, loss = 0.29804250\n",
      "Iteration 150, loss = 0.43629248\n",
      "Iteration 1474, loss = 0.18887480\n",
      "Iteration 1519, loss = 0.24853966\n",
      "Iteration 1475, loss = 0.18870820\n",
      "Iteration 151, loss = 0.43540511\n",
      "Iteration 1476, loss = 0.18851492\n",
      "Iteration 1665, loss = 0.16082579\n",
      "Iteration 436, loss = 0.36797918\n",
      "Iteration 1520, loss = 0.24840702\n",
      "Iteration 947, loss = 0.29791651\n",
      "Iteration 1477, loss = 0.18831051\n",
      "Iteration 1468, loss = 0.19276537\n",
      "Iteration 152, loss = 0.43456523\n",
      "Iteration 652, loss = 0.35285559\n",
      "Iteration 948, loss = 0.29784548\n",
      "Iteration 437, loss = 0.36782777\n",
      "Iteration 1478, loss = 0.18826513\n",
      "Iteration 1666, loss = 0.16071054\n",
      "Iteration 1479, loss = 0.18805639\n",
      "Iteration 949, loss = 0.29775192\n",
      "Iteration 1480, loss = 0.18813820\n",
      "Iteration 1481, loss = 0.18777737\n",
      "Iteration 950, loss = 0.29761772\n",
      "Iteration 1521, loss = 0.24833282\n",
      "Iteration 1667, loss = 0.16049077\n",
      "Iteration 1482, loss = 0.18775713\n",
      "Iteration 1469, loss = 0.19295535\n",
      "Iteration 438, loss = 0.36770319\n",
      "Iteration 1483, loss = 0.18733810\n",
      "Iteration 951, loss = 0.29755935\n",
      "Iteration 653, loss = 0.35268850\n",
      "Iteration 1484, loss = 0.18718291\n",
      "Iteration 1668, loss = 0.16037671\n",
      "Iteration 1522, loss = 0.24821039\n",
      "Iteration 153, loss = 0.43378714\n",
      "Iteration 439, loss = 0.36762536\n",
      "Iteration 1485, loss = 0.18715772\n",
      "Iteration 952, loss = 0.29743331\n",
      "Iteration 1669, loss = 0.16035239\n",
      "Iteration 1470, loss = 0.19265180\n",
      "Iteration 1486, loss = 0.18688352\n",
      "Iteration 953, loss = 0.29738977\n",
      "Iteration 654, loss = 0.35257777\n",
      "Iteration 1523, loss = 0.24819113\n",
      "Iteration 440, loss = 0.36751197\n",
      "Iteration 1670, loss = 0.16021856\n",
      "Iteration 1487, loss = 0.18682687\n",
      "Iteration 1488, loss = 0.18663008\n",
      "Iteration 1524, loss = 0.24801906\n",
      "Iteration 954, loss = 0.29727819\n",
      "Iteration 1671, loss = 0.16001979\n",
      "Iteration 1489, loss = 0.18647077\n",
      "Iteration 655, loss = 0.35234483\n",
      "Iteration 955, loss = 0.29715862\n",
      "Iteration 441, loss = 0.36741329\n",
      "Iteration 1490, loss = 0.18624912\n",
      "Iteration 956, loss = 0.29710913\n",
      "Iteration 1471, loss = 0.19232327\n",
      "Iteration 1491, loss = 0.18615536\n",
      "Iteration 957, loss = 0.29702687\n",
      "Iteration 442, loss = 0.36731662\n",
      "Iteration 1525, loss = 0.24789533\n",
      "Iteration 1492, loss = 0.18592224\n",
      "Iteration 1672, loss = 0.15981514\n",
      "Iteration 1493, loss = 0.18584720\n",
      "Iteration 1494, loss = 0.18571537\n",
      "Iteration 443, loss = 0.36724838\n",
      "Iteration 1526, loss = 0.24780665\n",
      "Iteration 1495, loss = 0.18543580\n",
      "Iteration 958, loss = 0.29688185\n",
      "Iteration 1496, loss = 0.18535611\n",
      "Iteration 1472, loss = 0.19222195\n",
      "Iteration 656, loss = 0.35223498\n",
      "Iteration 1497, loss = 0.18537369\n",
      "Iteration 959, loss = 0.29675812\n",
      "Iteration 444, loss = 0.36710481\n",
      "Iteration 1498, loss = 0.18505007\n",
      "Iteration 1673, loss = 0.15990881\n",
      "Iteration 1499, loss = 0.18493537\n",
      "Iteration 154, loss = 0.43298917\n",
      "Iteration 657, loss = 0.35209449\n",
      "Iteration 960, loss = 0.29668031\n",
      "Iteration 1500, loss = 0.18463163\n",
      "Iteration 1674, loss = 0.15950182\n",
      "Iteration 445, loss = 0.36699723\n",
      "Iteration 1501, loss = 0.18454724\n",
      "Iteration 1527, loss = 0.24780042\n",
      "Iteration 1473, loss = 0.19206269\n",
      "Iteration 1502, loss = 0.18429309\n",
      "Iteration 961, loss = 0.29658403\n",
      "Iteration 1503, loss = 0.18414132\n",
      "Iteration 1504, loss = 0.18396559\n",
      "Iteration 962, loss = 0.29652065\n",
      "Iteration 446, loss = 0.36691227\n",
      "Iteration 658, loss = 0.35183711\n",
      "Iteration 1675, loss = 0.15983351\n",
      "Iteration 1505, loss = 0.18394451\n",
      "Iteration 963, loss = 0.29637255\n",
      "Iteration 1506, loss = 0.18365868\n",
      "Iteration 1676, loss = 0.15920900\n",
      "Iteration 447, loss = 0.36680944\n",
      "Iteration 1507, loss = 0.18365737\n",
      "Iteration 1528, loss = 0.24757801\n",
      "Iteration 1508, loss = 0.18333427\n",
      "Iteration 659, loss = 0.35168991\n",
      "Iteration 1474, loss = 0.19191763\n",
      "Iteration 1509, loss = 0.18317814\n",
      "Iteration 964, loss = 0.29629478\n",
      "Iteration 1510, loss = 0.18301998\n",
      "Iteration 1677, loss = 0.15910965\n",
      "Iteration 1511, loss = 0.18314885\n",
      "Iteration 1529, loss = 0.24754071\n",
      "Iteration 1512, loss = 0.18275423\n",
      "Iteration 965, loss = 0.29620729\n",
      "Iteration 448, loss = 0.36670101\n",
      "Iteration 1513, loss = 0.18251748\n",
      "Iteration 1678, loss = 0.15892139\n",
      "Iteration 966, loss = 0.29608933\n",
      "Iteration 1514, loss = 0.18247057\n",
      "Iteration 155, loss = 0.43220471\n",
      "Iteration 1530, loss = 0.24738605\n",
      "Iteration 660, loss = 0.35149439\n",
      "Iteration 967, loss = 0.29600705\n",
      "Iteration 449, loss = 0.36661057\n",
      "Iteration 1515, loss = 0.18222863\n",
      "Iteration 1516, loss = 0.18205566\n",
      "Iteration 1475, loss = 0.19195610\n",
      "Iteration 1679, loss = 0.15874098\n",
      "Iteration 1517, loss = 0.18225798\n",
      "Iteration 968, loss = 0.29585346\n",
      "Iteration 450, loss = 0.36652972\n",
      "Iteration 1518, loss = 0.18171385\n",
      "Iteration 1531, loss = 0.24732275\n",
      "Iteration 1519, loss = 0.18172914\n",
      "Iteration 1680, loss = 0.15864232\n",
      "Iteration 1520, loss = 0.18140439\n",
      "Iteration 969, loss = 0.29582303\n",
      "Iteration 1476, loss = 0.19164407\n",
      "Iteration 1521, loss = 0.18135572\n",
      "Iteration 1681, loss = 0.15862219\n",
      "Iteration 661, loss = 0.35134997\n",
      "Iteration 1522, loss = 0.18118041\n",
      "Iteration 156, loss = 0.43142382\n",
      "Iteration 970, loss = 0.29569067\n",
      "Iteration 1532, loss = 0.24717844\n",
      "Iteration 451, loss = 0.36641324\n",
      "Iteration 1682, loss = 0.15851493\n",
      "Iteration 1523, loss = 0.18097919\n",
      "Iteration 1524, loss = 0.18074554\n",
      "Iteration 971, loss = 0.29558161\n",
      "Iteration 1533, loss = 0.24706946\n",
      "Iteration 452, loss = 0.36630508\n",
      "Iteration 1525, loss = 0.18066210\n",
      "Iteration 1683, loss = 0.15850147\n",
      "Iteration 157, loss = 0.43071350\n",
      "Iteration 662, loss = 0.35120550\n",
      "Iteration 1684, loss = 0.15814189\n",
      "Iteration 972, loss = 0.29551160\n",
      "Iteration 453, loss = 0.36620358\n",
      "Iteration 158, loss = 0.42995398\n",
      "Iteration 1534, loss = 0.24693677\n",
      "Iteration 973, loss = 0.29538145\n",
      "Iteration 1526, loss = 0.18059781\n",
      "Iteration 1477, loss = 0.19155402\n",
      "Iteration 1527, loss = 0.18034645\n",
      "Iteration 159, loss = 0.42924702\n",
      "Iteration 1528, loss = 0.18008773\n",
      "Iteration 1685, loss = 0.15788253\n",
      "Iteration 974, loss = 0.29529296\n",
      "Iteration 1529, loss = 0.17994413\n",
      "Iteration 1535, loss = 0.24692346\n",
      "Iteration 1530, loss = 0.17983680\n",
      "Iteration 454, loss = 0.36609914\n",
      "Iteration 1531, loss = 0.17973302\n",
      "Iteration 160, loss = 0.42855432\n",
      "Iteration 1532, loss = 0.17951457\n",
      "Iteration 1533, loss = 0.17931777\n",
      "Iteration 1536, loss = 0.24675057\n",
      "Iteration 161, loss = 0.42777872\n",
      "Iteration 455, loss = 0.36606780\n",
      "Iteration 1534, loss = 0.17935067\n",
      "Iteration 1535, loss = 0.17913500\n",
      "Iteration 1686, loss = 0.15779195\n",
      "Iteration 663, loss = 0.35097612\n",
      "Iteration 1536, loss = 0.17887719\n",
      "Iteration 162, loss = 0.42713326\n",
      "Iteration 1478, loss = 0.19137902\n",
      "Iteration 975, loss = 0.29516807\n",
      "Iteration 456, loss = 0.36595935\n",
      "Iteration 1687, loss = 0.15790133\n",
      "Iteration 1537, loss = 0.17875310\n",
      "Iteration 1537, loss = 0.24664673\n",
      "Iteration 664, loss = 0.35090858\n",
      "Iteration 976, loss = 0.29509392\n",
      "Iteration 1479, loss = 0.19127428\n",
      "Iteration 163, loss = 0.42649036\n",
      "Iteration 457, loss = 0.36581010\n",
      "Iteration 1538, loss = 0.17852155\n",
      "Iteration 1539, loss = 0.17845622\n",
      "Iteration 1538, loss = 0.24655518\n",
      "Iteration 164, loss = 0.42580295\n",
      "Iteration 1688, loss = 0.15765674\n",
      "Iteration 1540, loss = 0.17825935\n",
      "Iteration 977, loss = 0.29501034\n",
      "Iteration 1541, loss = 0.17816270\n",
      "Iteration 1689, loss = 0.15744211\n",
      "Iteration 1542, loss = 0.17797141\n",
      "Iteration 1480, loss = 0.19115660\n",
      "Iteration 978, loss = 0.29490825\n",
      "Iteration 458, loss = 0.36569491\n",
      "Iteration 1543, loss = 0.17783074\n",
      "Iteration 1539, loss = 0.24647454\n",
      "Iteration 1690, loss = 0.15739736\n",
      "Iteration 1544, loss = 0.17756895\n",
      "Iteration 979, loss = 0.29481321\n",
      "Iteration 1545, loss = 0.17746782\n",
      "Iteration 665, loss = 0.35065668\n",
      "Iteration 165, loss = 0.42511960\n",
      "Iteration 459, loss = 0.36563715\n",
      "Iteration 980, loss = 0.29469042\n",
      "Iteration 1546, loss = 0.17742857\n",
      "Iteration 1691, loss = 0.15768119\n",
      "Iteration 1547, loss = 0.17716296\n",
      "Iteration 1540, loss = 0.24638938\n",
      "Iteration 1548, loss = 0.17706570\n",
      "Iteration 1481, loss = 0.19087205\n",
      "Iteration 981, loss = 0.29459611\n",
      "Iteration 1549, loss = 0.17681408\n",
      "Iteration 166, loss = 0.42451495\n",
      "Iteration 982, loss = 0.29455044\n",
      "Iteration 1550, loss = 0.17663259\n",
      "Iteration 167, loss = 0.42390055\n",
      "Iteration 1551, loss = 0.17650923\n",
      "Iteration 983, loss = 0.29441836\n",
      "Iteration 1692, loss = 0.15700410\n",
      "Iteration 1552, loss = 0.17646984\n",
      "Iteration 1541, loss = 0.24629351\n",
      "Iteration 1482, loss = 0.19080614\n",
      "Iteration 1553, loss = 0.17616686\n",
      "Iteration 666, loss = 0.35048856\n",
      "Iteration 984, loss = 0.29434474\n",
      "Iteration 1554, loss = 0.17601523\n",
      "Iteration 1693, loss = 0.15682555\n",
      "Iteration 460, loss = 0.36552580\n",
      "Iteration 168, loss = 0.42325590\n",
      "Iteration 1555, loss = 0.17584081\n",
      "Iteration 1556, loss = 0.17569666\n",
      "Iteration 1542, loss = 0.24617012\n",
      "Iteration 985, loss = 0.29423419\n",
      "Iteration 1557, loss = 0.17558221\n",
      "Iteration 1558, loss = 0.17534463\n",
      "Iteration 986, loss = 0.29414388\n",
      "Iteration 169, loss = 0.42265334\n",
      "Iteration 1694, loss = 0.15667338\n",
      "Iteration 461, loss = 0.36540034\n",
      "Iteration 987, loss = 0.29403994\n",
      "Iteration 1543, loss = 0.24603670\n",
      "Iteration 1483, loss = 0.19059774\n",
      "Iteration 170, loss = 0.42199247\n",
      "Iteration 988, loss = 0.29392980\n",
      "Iteration 1559, loss = 0.17517190\n",
      "Iteration 462, loss = 0.36532659\n",
      "Iteration 667, loss = 0.35030851\n",
      "Iteration 989, loss = 0.29383143\n",
      "Iteration 1544, loss = 0.24593411\n",
      "Iteration 1560, loss = 0.17503675\n",
      "Iteration 990, loss = 0.29371873\n",
      "Iteration 1695, loss = 0.15665224\n",
      "Iteration 171, loss = 0.42142505\n",
      "Iteration 991, loss = 0.29365070\n",
      "Iteration 1696, loss = 0.15642394\n",
      "Iteration 172, loss = 0.42081632\n",
      "Iteration 1561, loss = 0.17490770\n",
      "Iteration 1545, loss = 0.24579904\n",
      "Iteration 1697, loss = 0.15626288\n",
      "Iteration 1562, loss = 0.17481166\n",
      "Iteration 173, loss = 0.42019249\n",
      "Iteration 992, loss = 0.29355057\n",
      "Iteration 463, loss = 0.36520211\n",
      "Iteration 1563, loss = 0.17467157\n",
      "Iteration 1484, loss = 0.19048138\n",
      "Iteration 1564, loss = 0.17451584\n",
      "Iteration 668, loss = 0.35014392\n",
      "Iteration 1565, loss = 0.17428587\n",
      "Iteration 464, loss = 0.36509741\n",
      "Iteration 1698, loss = 0.15613760\n",
      "Iteration 993, loss = 0.29352980\n",
      "Iteration 174, loss = 0.41966429\n",
      "Iteration 1566, loss = 0.17411595\n",
      "Iteration 1485, loss = 0.19033351\n",
      "Iteration 994, loss = 0.29338321\n",
      "Iteration 1546, loss = 0.24569573\n",
      "Iteration 1567, loss = 0.17402321\n",
      "Iteration 1699, loss = 0.15607627\n",
      "Iteration 995, loss = 0.29323582\n",
      "Iteration 1568, loss = 0.17398351\n",
      "Iteration 1700, loss = 0.15581507\n",
      "Iteration 465, loss = 0.36499603\n",
      "Iteration 175, loss = 0.41909597\n",
      "Iteration 996, loss = 0.29315427\n",
      "Iteration 1547, loss = 0.24562326\n",
      "Iteration 1569, loss = 0.17369782\n",
      "Iteration 669, loss = 0.35000394\n",
      "Iteration 1486, loss = 0.19049811\n",
      "Iteration 1701, loss = 0.15580799\n",
      "Iteration 176, loss = 0.41850788\n",
      "Iteration 997, loss = 0.29307467\n",
      "Iteration 1570, loss = 0.17358940\n",
      "Iteration 1548, loss = 0.24551703\n",
      "Iteration 466, loss = 0.36489792\n",
      "Iteration 1571, loss = 0.17349229\n",
      "Iteration 998, loss = 0.29292785\n",
      "Iteration 1572, loss = 0.17322028\n",
      "Iteration 670, loss = 0.34993444\n",
      "Iteration 1573, loss = 0.17301041\n",
      "Iteration 999, loss = 0.29289839\n",
      "Iteration 1574, loss = 0.17298994\n",
      "Iteration 1702, loss = 0.15573364\n",
      "Iteration 1000, loss = 0.29274748\n",
      "Iteration 1575, loss = 0.17278777\n",
      "Iteration 1576, loss = 0.17262043\n",
      "Iteration 467, loss = 0.36479604\n",
      "Iteration 1001, loss = 0.29263741\n",
      "Iteration 1577, loss = 0.17244407\n",
      "Iteration 671, loss = 0.34965604\n",
      "Iteration 1578, loss = 0.17225486\n",
      "Iteration 177, loss = 0.41793552\n",
      "Iteration 1579, loss = 0.17214668\n",
      "Iteration 1703, loss = 0.15548572\n",
      "Iteration 1580, loss = 0.17205584\n",
      "Iteration 1549, loss = 0.24549394\n",
      "Iteration 1581, loss = 0.17180439\n",
      "Iteration 178, loss = 0.41750978\n",
      "Iteration 1582, loss = 0.17164236\n",
      "Iteration 468, loss = 0.36470138\n",
      "Iteration 1487, loss = 0.19012923\n",
      "Iteration 1704, loss = 0.15530969\n",
      "Iteration 1583, loss = 0.17151572\n",
      "Iteration 1584, loss = 0.17183877\n",
      "Iteration 1002, loss = 0.29256496\n",
      "Iteration 1585, loss = 0.17124893\n",
      "Iteration 179, loss = 0.41683962\n",
      "Iteration 1705, loss = 0.15519866\n",
      "Iteration 1550, loss = 0.24531646\n",
      "Iteration 1586, loss = 0.17101068\n",
      "Iteration 1003, loss = 0.29243545\n",
      "Iteration 1488, loss = 0.19006531\n",
      "Iteration 180, loss = 0.41633566\n",
      "Iteration 1587, loss = 0.17092581\n",
      "Iteration 1706, loss = 0.15515117\n",
      "Iteration 1551, loss = 0.24518998\n",
      "Iteration 469, loss = 0.36460658\n",
      "Iteration 1588, loss = 0.17075322\n",
      "Iteration 181, loss = 0.41578071\n",
      "Iteration 1589, loss = 0.17054193\n",
      "Iteration 672, loss = 0.34959441\n",
      "Iteration 1489, loss = 0.18976588\n",
      "Iteration 1004, loss = 0.29240909\n",
      "Iteration 1707, loss = 0.15495671\n",
      "Iteration 1552, loss = 0.24510087\n",
      "Iteration 1590, loss = 0.17043931\n",
      "Iteration 470, loss = 0.36451020\n",
      "Iteration 1591, loss = 0.17023902\n",
      "Iteration 1592, loss = 0.17014043\n",
      "Iteration 182, loss = 0.41527118\n",
      "Iteration 1005, loss = 0.29226583\n",
      "Iteration 1708, loss = 0.15484319\n",
      "Iteration 471, loss = 0.36441020\n",
      "Iteration 1490, loss = 0.18967310\n",
      "Iteration 1593, loss = 0.16993286\n",
      "Iteration 1006, loss = 0.29219323\n",
      "Iteration 1594, loss = 0.16988255\n",
      "Iteration 673, loss = 0.34927348\n",
      "Iteration 1553, loss = 0.24497188\n",
      "Iteration 1595, loss = 0.16965725\n",
      "Iteration 1709, loss = 0.15479943Iteration 183, loss = 0.41480278\n",
      "\n",
      "Iteration 1007, loss = 0.29207571\n",
      "Iteration 1596, loss = 0.16944389\n",
      "Iteration 1008, loss = 0.29195016\n",
      "Iteration 1491, loss = 0.18962648\n",
      "Iteration 1597, loss = 0.16933094\n",
      "Iteration 1710, loss = 0.15459982\n",
      "Iteration 1009, loss = 0.29185697\n",
      "Iteration 472, loss = 0.36434528\n",
      "Iteration 1554, loss = 0.24494784\n",
      "Iteration 1598, loss = 0.16943428\n",
      "Iteration 1010, loss = 0.29173138\n",
      "Iteration 674, loss = 0.34909419\n",
      "Iteration 184, loss = 0.41426964\n",
      "Iteration 1711, loss = 0.15447643\n",
      "Iteration 1599, loss = 0.16902876\n",
      "Iteration 1011, loss = 0.29166803\n",
      "Iteration 1555, loss = 0.24478511\n",
      "Iteration 185, loss = 0.41382973\n",
      "Iteration 1600, loss = 0.16886493\n",
      "Iteration 473, loss = 0.36421919\n",
      "Iteration 675, loss = 0.34893944\n",
      "Iteration 1712, loss = 0.15435121\n",
      "Iteration 1601, loss = 0.16870105\n",
      "Iteration 186, loss = 0.41331648\n",
      "Iteration 1012, loss = 0.29155377\n",
      "Iteration 1492, loss = 0.18939753\n",
      "Iteration 1602, loss = 0.16858179\n",
      "Iteration 1556, loss = 0.24466261\n",
      "Iteration 1013, loss = 0.29145982\n",
      "Iteration 474, loss = 0.36409924\n",
      "Iteration 1713, loss = 0.15413244\n",
      "Iteration 1014, loss = 0.29139970\n",
      "Iteration 1557, loss = 0.24459449\n",
      "Iteration 1603, loss = 0.16848742\n",
      "Iteration 1015, loss = 0.29123549\n",
      "Iteration 187, loss = 0.41282159\n",
      "Iteration 676, loss = 0.34880784\n",
      "Iteration 1604, loss = 0.16835772\n",
      "Iteration 1714, loss = 0.15401862\n",
      "Iteration 1016, loss = 0.29112353\n",
      "Iteration 475, loss = 0.36400650\n",
      "Iteration 188, loss = 0.41237462\n",
      "Iteration 1605, loss = 0.16812448\n",
      "Iteration 1558, loss = 0.24457468\n",
      "Iteration 1017, loss = 0.29104985\n",
      "Iteration 1606, loss = 0.16798676\n",
      "Iteration 1493, loss = 0.18916365\n",
      "Iteration 1715, loss = 0.15377811\n",
      "Iteration 677, loss = 0.34859056\n",
      "Iteration 189, loss = 0.41194887\n",
      "Iteration 1716, loss = 0.15373887\n",
      "Iteration 1559, loss = 0.24437950\n",
      "Iteration 190, loss = 0.41144998\n",
      "Iteration 1717, loss = 0.15358341\n",
      "Iteration 1018, loss = 0.29097384\n",
      "Iteration 1494, loss = 0.18907052\n",
      "Iteration 191, loss = 0.41102903\n",
      "Iteration 476, loss = 0.36391309Iteration 1560, loss = 0.24430075\n",
      "\n",
      "Iteration 1718, loss = 0.15349878\n",
      "Iteration 1019, loss = 0.29084044\n",
      "Iteration 1607, loss = 0.16784729\n",
      "Iteration 192, loss = 0.41057928\n",
      "Iteration 1608, loss = 0.16765548\n",
      "Iteration 1719, loss = 0.15327703\n",
      "Iteration 1020, loss = 0.29077312\n",
      "Iteration 678, loss = 0.34843431\n",
      "Iteration 1609, loss = 0.16753850\n",
      "Iteration 1561, loss = 0.24414229\n",
      "Iteration 477, loss = 0.36381429\n",
      "Iteration 1610, loss = 0.16739548\n",
      "Iteration 1021, loss = 0.29069125\n",
      "Iteration 1495, loss = 0.18894857\n",
      "Iteration 1611, loss = 0.16719857\n",
      "Iteration 193, loss = 0.41015363\n",
      "Iteration 1720, loss = 0.15316719\n",
      "Iteration 1612, loss = 0.16726484\n",
      "Iteration 1562, loss = 0.24406605\n",
      "Iteration 1022, loss = 0.29057526\n",
      "Iteration 1613, loss = 0.16698481\n",
      "Iteration 1721, loss = 0.15303290\n",
      "Iteration 1614, loss = 0.16679114\n",
      "Iteration 1615, loss = 0.16657824\n",
      "Iteration 1563, loss = 0.24393629\n",
      "Iteration 1616, loss = 0.16648794\n",
      "Iteration 194, loss = 0.40972084\n",
      "Iteration 478, loss = 0.36372659\n",
      "Iteration 679, loss = 0.34825991\n",
      "Iteration 1023, loss = 0.29043693\n",
      "Iteration 1496, loss = 0.18877775\n",
      "Iteration 1617, loss = 0.16628316\n",
      "Iteration 1722, loss = 0.15297510\n",
      "Iteration 1618, loss = 0.16627669\n",
      "Iteration 1564, loss = 0.24386174\n",
      "Iteration 1024, loss = 0.29037743\n",
      "Iteration 1619, loss = 0.16600805\n",
      "Iteration 1620, loss = 0.16586410\n",
      "Iteration 195, loss = 0.40928617\n",
      "Iteration 1025, loss = 0.29028465\n",
      "Iteration 1621, loss = 0.16573982\n",
      "Iteration 1026, loss = 0.29013320\n",
      "Iteration 1723, loss = 0.15284148\n",
      "Iteration 1622, loss = 0.16555123\n",
      "Iteration 680, loss = 0.34822183Iteration 196, loss = 0.40891325\n",
      "\n",
      "Iteration 1027, loss = 0.29006253\n",
      "Iteration 479, loss = 0.36361773\n",
      "Iteration 1623, loss = 0.16541160\n",
      "Iteration 1565, loss = 0.24372457\n",
      "Iteration 1028, loss = 0.28992847\n",
      "Iteration 1624, loss = 0.16523245\n",
      "Iteration 1497, loss = 0.18866685\n",
      "Iteration 1625, loss = 0.16509767\n",
      "Iteration 1029, loss = 0.28996068\n",
      "Iteration 1724, loss = 0.15270692\n",
      "Iteration 197, loss = 0.40844100\n",
      "Iteration 1030, loss = 0.28977445\n",
      "Iteration 1626, loss = 0.16496193\n",
      "Iteration 1031, loss = 0.28972733\n",
      "Iteration 198, loss = 0.40808245\n",
      "Iteration 1627, loss = 0.16475219\n",
      "Iteration 1032, loss = 0.28953989\n",
      "Iteration 1566, loss = 0.24361065\n",
      "Iteration 1725, loss = 0.15266990\n",
      "Iteration 480, loss = 0.36351215\n",
      "Iteration 681, loss = 0.34795076\n",
      "Iteration 199, loss = 0.40765949\n",
      "Iteration 1628, loss = 0.16465799\n",
      "Iteration 1033, loss = 0.28944092\n",
      "Iteration 1629, loss = 0.16455703\n",
      "Iteration 1630, loss = 0.16458404\n",
      "Iteration 1498, loss = 0.18858301\n",
      "Iteration 1631, loss = 0.16424582\n",
      "Iteration 200, loss = 0.40724915\n",
      "Iteration 1034, loss = 0.28932429\n",
      "Iteration 1726, loss = 0.15237269\n",
      "Iteration 481, loss = 0.36341709\n",
      "Iteration 1632, loss = 0.16400744\n",
      "Iteration 1633, loss = 0.16399296\n",
      "Iteration 1035, loss = 0.28923700\n",
      "Iteration 1567, loss = 0.24357824\n",
      "Iteration 1634, loss = 0.16376987\n",
      "Iteration 201, loss = 0.40688487\n",
      "Iteration 482, loss = 0.36332559\n",
      "Iteration 202, loss = 0.40654420\n",
      "Iteration 682, loss = 0.34781151\n",
      "Iteration 1635, loss = 0.16355615\n",
      "Iteration 1036, loss = 0.28914336\n",
      "Iteration 1727, loss = 0.15233630\n",
      "Iteration 1568, loss = 0.24343617\n",
      "Iteration 1636, loss = 0.16352536\n",
      "Iteration 1499, loss = 0.18835641\n",
      "Iteration 1637, loss = 0.16330446\n",
      "Iteration 1728, loss = 0.15232349\n",
      "Iteration 683, loss = 0.34765332\n",
      "Iteration 1638, loss = 0.16313479\n",
      "Iteration 1037, loss = 0.28905620\n",
      "Iteration 483, loss = 0.36326486\n",
      "Iteration 1639, loss = 0.16299279\n",
      "Iteration 203, loss = 0.40609389\n",
      "Iteration 1729, loss = 0.15203848\n",
      "Iteration 1640, loss = 0.16281119\n",
      "Iteration 1500, loss = 0.18829294\n",
      "Iteration 1038, loss = 0.28897251\n",
      "Iteration 204, loss = 0.40576676\n",
      "Iteration 1730, loss = 0.15188279\n",
      "Iteration 1641, loss = 0.16287210\n",
      "Iteration 205, loss = 0.40548112\n",
      "Iteration 1731, loss = 0.15175853\n",
      "Iteration 1039, loss = 0.28884552\n",
      "Iteration 484, loss = 0.36314682\n",
      "Iteration 1642, loss = 0.16251042\n",
      "Iteration 1643, loss = 0.16264294\n",
      "Iteration 1040, loss = 0.28872284\n",
      "Iteration 1644, loss = 0.16224918\n",
      "Iteration 684, loss = 0.34742902\n",
      "Iteration 1645, loss = 0.16215691\n",
      "Iteration 1501, loss = 0.18813286\n",
      "Iteration 485, loss = 0.36311091\n",
      "Iteration 1041, loss = 0.28862679\n",
      "Iteration 206, loss = 0.40502653\n",
      "Iteration 1732, loss = 0.15178915\n",
      "Iteration 1646, loss = 0.16200245\n",
      "Iteration 1647, loss = 0.16175696\n",
      "Iteration 207, loss = 0.40467513\n",
      "Iteration 1733, loss = 0.15150772\n",
      "Iteration 1042, loss = 0.28853452\n",
      "Iteration 1648, loss = 0.16173030\n",
      "Iteration 685, loss = 0.34723333\n",
      "Iteration 208, loss = 0.40427416\n",
      "Iteration 1649, loss = 0.16142724\n",
      "Iteration 1502, loss = 0.18801223\n",
      "Iteration 1043, loss = 0.28844203\n",
      "Iteration 486, loss = 0.36296480\n",
      "Iteration 1734, loss = 0.15153356\n",
      "Iteration 209, loss = 0.40393147\n",
      "Iteration 1650, loss = 0.16149132\n",
      "Iteration 1044, loss = 0.28841299\n",
      "Iteration 1651, loss = 0.16125322\n",
      "Iteration 686, loss = 0.34705318\n",
      "Iteration 1652, loss = 0.16101803\n",
      "Iteration 1653, loss = 0.16083904\n",
      "Iteration 1045, loss = 0.28823843\n",
      "Iteration 210, loss = 0.40357849\n",
      "Iteration 487, loss = 0.36284250\n",
      "Iteration 1654, loss = 0.16071450\n",
      "Iteration 1735, loss = 0.15145737\n",
      "Iteration 1503, loss = 0.18782212Iteration 211, loss = 0.40324502\n",
      "\n",
      "Iteration 1655, loss = 0.16074660\n",
      "Iteration 1046, loss = 0.28814131\n",
      "Iteration 1656, loss = 0.16045676\n",
      "Iteration 1736, loss = 0.15105417\n",
      "Iteration 1047, loss = 0.28802575\n",
      "Iteration 488, loss = 0.36275303\n",
      "Iteration 1657, loss = 0.16021544\n",
      "Iteration 1048, loss = 0.28793464\n",
      "Iteration 212, loss = 0.40289580\n",
      "Iteration 1658, loss = 0.16022523\n",
      "Iteration 1049, loss = 0.28783597\n",
      "Iteration 687, loss = 0.34688947\n",
      "Iteration 1659, loss = 0.15990330\n",
      "Iteration 1737, loss = 0.15105206\n",
      "Iteration 1504, loss = 0.18773359\n",
      "Iteration 1660, loss = 0.15988617\n",
      "Iteration 1050, loss = 0.28774812\n",
      "Iteration 213, loss = 0.40254650\n",
      "Iteration 489, loss = 0.36267666\n",
      "Iteration 1051, loss = 0.28771457\n",
      "Iteration 1661, loss = 0.15965034\n",
      "Iteration 1738, loss = 0.15090514\n",
      "Iteration 1662, loss = 0.15959924\n",
      "Iteration 1052, loss = 0.28753912\n",
      "Iteration 1505, loss = 0.18760121\n",
      "Iteration 1663, loss = 0.15943731\n",
      "Iteration 214, loss = 0.40222870\n",
      "Iteration 1664, loss = 0.15917441\n",
      "Iteration 1053, loss = 0.28744663\n",
      "Iteration 1665, loss = 0.15909675\n",
      "Iteration 1739, loss = 0.15072633\n",
      "Iteration 490, loss = 0.36255936\n",
      "Iteration 1054, loss = 0.28734413\n",
      "Iteration 688, loss = 0.34685607\n",
      "Iteration 1666, loss = 0.15898164\n",
      "Iteration 215, loss = 0.40186819\n",
      "Iteration 1569, loss = 0.24332580\n",
      "Iteration 1667, loss = 0.15871363\n",
      "Iteration 1055, loss = 0.28726378\n",
      "Iteration 1668, loss = 0.15875044\n",
      "Iteration 1740, loss = 0.15062085\n",
      "Iteration 1669, loss = 0.15836718\n",
      "Iteration 1506, loss = 0.18741814\n",
      "Iteration 1056, loss = 0.28712924\n",
      "Iteration 1670, loss = 0.15834188\n",
      "Iteration 1741, loss = 0.15042526\n",
      "Iteration 491, loss = 0.36245129\n",
      "Iteration 216, loss = 0.40152663\n",
      "Iteration 1671, loss = 0.15826888\n",
      "Iteration 1057, loss = 0.28712336\n",
      "Iteration 689, loss = 0.34650693\n",
      "Iteration 1058, loss = 0.28697500\n",
      "Iteration 1742, loss = 0.15027065\n",
      "Iteration 1672, loss = 0.15794296\n",
      "Iteration 217, loss = 0.40122211\n",
      "Iteration 1673, loss = 0.15775737\n",
      "Iteration 1507, loss = 0.18727309\n",
      "Iteration 492, loss = 0.36235201\n",
      "Iteration 1674, loss = 0.15771597\n",
      "Iteration 218, loss = 0.40090646\n",
      "Iteration 1675, loss = 0.15808391\n",
      "Iteration 1059, loss = 0.28687189\n",
      "Iteration 1676, loss = 0.15748120\n",
      "Iteration 493, loss = 0.36225313\n",
      "Iteration 1743, loss = 0.15015039\n",
      "Iteration 219, loss = 0.40061229\n",
      "Iteration 1677, loss = 0.15716559\n",
      "Iteration 690, loss = 0.34635712\n",
      "Iteration 1678, loss = 0.15706852\n",
      "Iteration 1508, loss = 0.18712001\n",
      "Iteration 1060, loss = 0.28672629\n",
      "Iteration 220, loss = 0.40029182\n",
      "Iteration 1679, loss = 0.15689186\n",
      "Iteration 1744, loss = 0.15018495\n",
      "Iteration 1680, loss = 0.15675786\n",
      "Iteration 494, loss = 0.36214969\n",
      "Iteration 221, loss = 0.39993783\n",
      "Iteration 1681, loss = 0.15672766\n",
      "Iteration 1061, loss = 0.28665084\n",
      "Iteration 1682, loss = 0.15645069\n",
      "Iteration 1745, loss = 0.15010019\n",
      "Iteration 1683, loss = 0.15629344\n",
      "Iteration 1509, loss = 0.18709478\n",
      "Iteration 1062, loss = 0.28652430\n",
      "Iteration 495, loss = 0.36206900\n",
      "Iteration 1746, loss = 0.14996768\n",
      "Iteration 1063, loss = 0.28652147\n",
      "Iteration 1684, loss = 0.15623869\n",
      "Iteration 691, loss = 0.34634163\n",
      "Iteration 222, loss = 0.39967533\n",
      "Iteration 1064, loss = 0.28631227\n",
      "Iteration 1685, loss = 0.15607077\n",
      "Iteration 1510, loss = 0.18683918\n",
      "Iteration 1065, loss = 0.28622654\n",
      "Iteration 223, loss = 0.39936491\n",
      "Iteration 1747, loss = 0.14967269\n",
      "Iteration 1686, loss = 0.15585575\n",
      "Iteration 1066, loss = 0.28615381\n",
      "Iteration 496, loss = 0.36196174\n",
      "Iteration 224, loss = 0.39908904\n",
      "Iteration 692, loss = 0.34602233\n",
      "Iteration 1687, loss = 0.15568448\n",
      "Iteration 1067, loss = 0.28607194\n",
      "Iteration 225, loss = 0.39881497\n",
      "Iteration 1688, loss = 0.15565252\n",
      "Iteration 497, loss = 0.36185309\n",
      "Iteration 1511, loss = 0.18675562\n",
      "Iteration 1748, loss = 0.14962861\n",
      "Iteration 1570, loss = 0.24323103\n",
      "Iteration 1689, loss = 0.15556901\n",
      "Iteration 226, loss = 0.39847606\n",
      "Iteration 1690, loss = 0.15521126\n",
      "Iteration 1068, loss = 0.28593427\n",
      "Iteration 693, loss = 0.34592796\n",
      "Iteration 1691, loss = 0.15508412\n",
      "Iteration 1692, loss = 0.15496876\n",
      "Iteration 227, loss = 0.39830294\n",
      "Iteration 1571, loss = 0.24309369\n",
      "Iteration 1069, loss = 0.28580348\n",
      "Iteration 1512, loss = 0.18671550\n",
      "Iteration 1749, loss = 0.14942099\n",
      "Iteration 1070, loss = 0.28574868\n",
      "Iteration 1750, loss = 0.14938896\n",
      "Iteration 1572, loss = 0.24300451\n",
      "Iteration 498, loss = 0.36177565\n",
      "Iteration 1071, loss = 0.28561198\n",
      "Iteration 694, loss = 0.34571514\n",
      "Iteration 1072, loss = 0.28553422\n",
      "Iteration 1513, loss = 0.18657651\n",
      "Iteration 1751, loss = 0.14918895\n",
      "Iteration 1073, loss = 0.28540888\n",
      "Iteration 695, loss = 0.34551776\n",
      "Iteration 1752, loss = 0.14904432\n",
      "Iteration 1514, loss = 0.18631120\n",
      "Iteration 499, loss = 0.36167565\n",
      "Iteration 228, loss = 0.39792530\n",
      "Iteration 1693, loss = 0.15479883\n",
      "Iteration 1694, loss = 0.15481850\n",
      "Iteration 500, loss = 0.36155700\n",
      "Iteration 1695, loss = 0.15445666\n",
      "Iteration 696, loss = 0.34536000\n",
      "Iteration 1074, loss = 0.28530857\n",
      "Iteration 1696, loss = 0.15430239\n",
      "Iteration 1753, loss = 0.14914940\n",
      "Iteration 1697, loss = 0.15419705\n",
      "Iteration 229, loss = 0.39765505\n",
      "Iteration 1573, loss = 0.24290932\n",
      "Iteration 501, loss = 0.36145964\n",
      "Iteration 1698, loss = 0.15404358\n",
      "Iteration 1754, loss = 0.14893779\n",
      "Iteration 230, loss = 0.39736621\n",
      "Iteration 1075, loss = 0.28520811\n",
      "Iteration 1574, loss = 0.24277801\n",
      "Iteration 1699, loss = 0.15396795\n",
      "Iteration 1515, loss = 0.18629005\n",
      "Iteration 1076, loss = 0.28515978\n",
      "Iteration 1755, loss = 0.14870646\n",
      "Iteration 1700, loss = 0.15374087\n",
      "Iteration 1077, loss = 0.28500221\n",
      "Iteration 502, loss = 0.36137177\n",
      "Iteration 1701, loss = 0.15355093\n",
      "Iteration 1078, loss = 0.28494129\n",
      "Iteration 1702, loss = 0.15340861\n",
      "Iteration 1575, loss = 0.24268221\n",
      "Iteration 231, loss = 0.39711591\n",
      "Iteration 1703, loss = 0.15355235\n",
      "Iteration 697, loss = 0.34529112\n",
      "Iteration 1756, loss = 0.14862290\n",
      "Iteration 232, loss = 0.39688244\n",
      "Iteration 1704, loss = 0.15323530\n",
      "Iteration 503, loss = 0.36126969\n",
      "Iteration 1079, loss = 0.28481491\n",
      "Iteration 1705, loss = 0.15300190\n",
      "Iteration 233, loss = 0.39653598\n",
      "Iteration 1576, loss = 0.24257238\n",
      "Iteration 1706, loss = 0.15287928\n",
      "Iteration 1757, loss = 0.14843673\n",
      "Iteration 504, loss = 0.36119970\n",
      "Iteration 1516, loss = 0.18603564\n",
      "Iteration 1080, loss = 0.28472687\n",
      "Iteration 1707, loss = 0.15263926\n",
      "Iteration 234, loss = 0.39635009\n",
      "Iteration 1708, loss = 0.15263756\n",
      "Iteration 1081, loss = 0.28462983\n",
      "Iteration 505, loss = 0.36108849\n",
      "Iteration 698, loss = 0.34500973\n",
      "Iteration 1709, loss = 0.15248209\n",
      "Iteration 1082, loss = 0.28453680\n",
      "Iteration 1577, loss = 0.24248051\n",
      "Iteration 506, loss = 0.36101147\n",
      "Iteration 235, loss = 0.39606794\n",
      "Iteration 1578, loss = 0.24239426\n",
      "Iteration 236, loss = 0.39577679\n",
      "Iteration 507, loss = 0.36088509\n",
      "Iteration 699, loss = 0.34482897\n",
      "Iteration 1083, loss = 0.28441341\n",
      "Iteration 1517, loss = 0.18615169\n",
      "Iteration 237, loss = 0.39549815\n",
      "Iteration 1579, loss = 0.24227921\n",
      "Iteration 1710, loss = 0.15224182\n",
      "Iteration 508, loss = 0.36079387\n",
      "Iteration 1084, loss = 0.28431074\n",
      "Iteration 1711, loss = 0.15210854\n",
      "Iteration 1758, loss = 0.14830174\n",
      "Iteration 1712, loss = 0.15207610\n",
      "Iteration 1580, loss = 0.24221051\n",
      "Iteration 238, loss = 0.39524561\n",
      "Iteration 509, loss = 0.36067169\n",
      "Iteration 700, loss = 0.34463824\n",
      "Iteration 1713, loss = 0.15176367\n",
      "Iteration 1085, loss = 0.28422825\n",
      "Iteration 1759, loss = 0.14825186\n",
      "Iteration 1518, loss = 0.18594116\n",
      "Iteration 1581, loss = 0.24209722\n",
      "Iteration 239, loss = 0.39499965\n",
      "Iteration 510, loss = 0.36058260\n",
      "Iteration 1086, loss = 0.28410453\n",
      "Iteration 1760, loss = 0.14826420\n",
      "Iteration 1582, loss = 0.24195473\n",
      "Iteration 1714, loss = 0.15156013\n",
      "Iteration 1715, loss = 0.15147594\n",
      "Iteration 1087, loss = 0.28398643\n",
      "Iteration 240, loss = 0.39476169\n",
      "Iteration 511, loss = 0.36050727\n",
      "Iteration 1761, loss = 0.14803940\n",
      "Iteration 1716, loss = 0.15136753\n",
      "Iteration 701, loss = 0.34479217\n",
      "Iteration 1088, loss = 0.28393698\n",
      "Iteration 1583, loss = 0.24187675\n",
      "Iteration 1717, loss = 0.15113317\n",
      "Iteration 241, loss = 0.39451770\n",
      "Iteration 1519, loss = 0.18579395\n",
      "Iteration 1584, loss = 0.24184446\n",
      "Iteration 1089, loss = 0.28380262\n",
      "Iteration 1718, loss = 0.15096552\n",
      "Iteration 1719, loss = 0.15084102\n",
      "Iteration 1090, loss = 0.28369883\n",
      "Iteration 1720, loss = 0.15082589\n",
      "Iteration 512, loss = 0.36044009\n",
      "Iteration 242, loss = 0.39424934\n",
      "Iteration 1762, loss = 0.14789974\n",
      "Iteration 1721, loss = 0.15062504\n",
      "Iteration 702, loss = 0.34429902\n",
      "Iteration 1091, loss = 0.28362605\n",
      "Iteration 1585, loss = 0.24171785\n",
      "Iteration 1722, loss = 0.15045907\n",
      "Iteration 513, loss = 0.36031032\n",
      "Iteration 1520, loss = 0.18554784\n",
      "Iteration 1092, loss = 0.28347777\n",
      "Iteration 1723, loss = 0.15027333\n",
      "Iteration 1763, loss = 0.14770473\n",
      "Iteration 243, loss = 0.39401750\n",
      "Iteration 1586, loss = 0.24152304\n",
      "Iteration 1724, loss = 0.15033428\n",
      "Iteration 1764, loss = 0.14748655\n",
      "Iteration 1093, loss = 0.28340068\n",
      "Iteration 1725, loss = 0.15000028\n",
      "Iteration 703, loss = 0.34413195\n",
      "Iteration 514, loss = 0.36018896\n",
      "Iteration 1726, loss = 0.14981660\n",
      "Iteration 1587, loss = 0.24146182\n",
      "Iteration 244, loss = 0.39378899\n",
      "Iteration 1765, loss = 0.14764646\n",
      "Iteration 1094, loss = 0.28328137\n",
      "Iteration 1727, loss = 0.14963053\n",
      "Iteration 1521, loss = 0.18537687\n",
      "Iteration 1766, loss = 0.14735815\n",
      "Iteration 515, loss = 0.36011743\n",
      "Iteration 1095, loss = 0.28320382\n",
      "Iteration 1728, loss = 0.14969377\n",
      "Iteration 1729, loss = 0.14939855\n",
      "Iteration 1767, loss = 0.14717769\n",
      "Iteration 704, loss = 0.34394933\n",
      "Iteration 245, loss = 0.39356504\n",
      "Iteration 1730, loss = 0.14923607\n",
      "Iteration 1588, loss = 0.24137187\n",
      "Iteration 1731, loss = 0.14908377\n",
      "Iteration 516, loss = 0.36003999\n",
      "Iteration 1732, loss = 0.14890044\n",
      "Iteration 1096, loss = 0.28309072\n",
      "Iteration 1768, loss = 0.14703019\n",
      "Iteration 246, loss = 0.39343235\n",
      "Iteration 1522, loss = 0.18527501\n",
      "Iteration 1733, loss = 0.14880491\n",
      "Iteration 1589, loss = 0.24132097\n",
      "Iteration 1097, loss = 0.28298454\n",
      "Iteration 705, loss = 0.34390642\n",
      "Iteration 1734, loss = 0.14860763\n",
      "Iteration 517, loss = 0.35990202\n",
      "Iteration 247, loss = 0.39307180\n",
      "Iteration 1098, loss = 0.28289825\n",
      "Iteration 1735, loss = 0.14852114\n",
      "Iteration 248, loss = 0.39285754\n",
      "Iteration 1769, loss = 0.14695046\n",
      "Iteration 518, loss = 0.35980573\n",
      "Iteration 1099, loss = 0.28286992\n",
      "Iteration 1523, loss = 0.18527281\n",
      "Iteration 249, loss = 0.39259579\n",
      "Iteration 1736, loss = 0.14853234\n",
      "Iteration 706, loss = 0.34361788\n",
      "Iteration 1770, loss = 0.14682993\n",
      "Iteration 1737, loss = 0.14858452\n",
      "Iteration 1100, loss = 0.28270623\n",
      "Iteration 1590, loss = 0.24111504\n",
      "Iteration 1738, loss = 0.14821721\n",
      "Iteration 1101, loss = 0.28260088\n",
      "Iteration 519, loss = 0.35975115\n",
      "Iteration 1102, loss = 0.28253481\n",
      "Iteration 1771, loss = 0.14667455\n",
      "Iteration 1103, loss = 0.28234841\n",
      "Iteration 520, loss = 0.35961127\n",
      "Iteration 1739, loss = 0.14793198\n",
      "Iteration 250, loss = 0.39235719\n",
      "Iteration 1740, loss = 0.14777466\n",
      "Iteration 1104, loss = 0.28226782\n",
      "Iteration 521, loss = 0.35951789\n",
      "Iteration 1741, loss = 0.14758014\n",
      "Iteration 1742, loss = 0.14762141\n",
      "Iteration 1743, loss = 0.14731267\n",
      "Iteration 251, loss = 0.39214599\n",
      "Iteration 1524, loss = 0.18503821\n",
      "Iteration 1772, loss = 0.14659646\n",
      "Iteration 1591, loss = 0.24110827\n",
      "Iteration 1744, loss = 0.14738594\n",
      "Iteration 252, loss = 0.39194333\n",
      "Iteration 1745, loss = 0.14703076\n",
      "Iteration 253, loss = 0.39168085\n",
      "Iteration 1773, loss = 0.14645763\n",
      "Iteration 1105, loss = 0.28216651\n",
      "Iteration 254, loss = 0.39145361\n",
      "Iteration 1525, loss = 0.18499175\n",
      "Iteration 1592, loss = 0.24088246\n",
      "Iteration 522, loss = 0.35945467Iteration 707, loss = 0.34346045\n",
      "Iteration 1106, loss = 0.28212290\n",
      "Iteration 1746, loss = 0.14685768\n",
      "Iteration 255, loss = 0.39124789\n",
      "Iteration 1747, loss = 0.14677096\n",
      "Iteration 1107, loss = 0.28196551\n",
      "\n",
      "Iteration 1774, loss = 0.14634435\n",
      "Iteration 1748, loss = 0.14679659\n",
      "Iteration 1108, loss = 0.28189131\n",
      "Iteration 256, loss = 0.39102248\n",
      "Iteration 708, loss = 0.34326177\n",
      "Iteration 1749, loss = 0.14675061Iteration 1109, loss = 0.28178491\n",
      "\n",
      "Iteration 1526, loss = 0.18478183\n",
      "Iteration 1750, loss = 0.14625081\n",
      "Iteration 1110, loss = 0.28166331\n",
      "Iteration 1751, loss = 0.14608654\n",
      "Iteration 1593, loss = 0.24080837\n",
      "Iteration 1775, loss = 0.14621161\n",
      "Iteration 257, loss = 0.39076757\n",
      "Iteration 709, loss = 0.34309521\n",
      "Iteration 258, loss = 0.39058839\n",
      "Iteration 1752, loss = 0.14600659\n",
      "Iteration 1594, loss = 0.24072159\n",
      "Iteration 1776, loss = 0.14636117\n",
      "Iteration 1527, loss = 0.18460455\n",
      "Iteration 259, loss = 0.39031688\n",
      "Iteration 1111, loss = 0.28157696\n",
      "Iteration 1753, loss = 0.14590461\n",
      "Iteration 523, loss = 0.35931859\n",
      "Iteration 710, loss = 0.34293929Iteration 1754, loss = 0.14589008\n",
      "\n",
      "Iteration 1112, loss = 0.28145739\n",
      "Iteration 1755, loss = 0.14551653\n",
      "Iteration 524, loss = 0.35922798\n",
      "Iteration 1528, loss = 0.18444493\n",
      "Iteration 1113, loss = 0.28135206\n",
      "Iteration 1777, loss = 0.14594436\n",
      "Iteration 1595, loss = 0.24061007\n",
      "Iteration 1756, loss = 0.14569156\n",
      "Iteration 260, loss = 0.39017726\n",
      "Iteration 525, loss = 0.35913000\n",
      "Iteration 1757, loss = 0.14552415\n",
      "Iteration 1529, loss = 0.18442824\n",
      "Iteration 261, loss = 0.38993812\n",
      "Iteration 1114, loss = 0.28127398\n",
      "Iteration 1778, loss = 0.14577727\n",
      "Iteration 711, loss = 0.34283973\n",
      "Iteration 1596, loss = 0.24052889\n",
      "Iteration 1758, loss = 0.14511016\n",
      "Iteration 526, loss = 0.35910944\n",
      "Iteration 1115, loss = 0.28119408\n",
      "Iteration 1597, loss = 0.24043138\n",
      "Iteration 1759, loss = 0.14494631\n",
      "Iteration 262, loss = 0.38965923\n",
      "Iteration 1779, loss = 0.14573687\n",
      "Iteration 527, loss = 0.35902270\n",
      "Iteration 1760, loss = 0.14483902\n",
      "Iteration 1780, loss = 0.14576732\n",
      "Iteration 712, loss = 0.34260256\n",
      "Iteration 1761, loss = 0.14463703\n",
      "Iteration 1762, loss = 0.14456543\n",
      "Iteration 1530, loss = 0.18430473\n",
      "Iteration 1763, loss = 0.14445963\n",
      "Iteration 1116, loss = 0.28103787\n",
      "Iteration 1764, loss = 0.14422278\n",
      "Iteration 263, loss = 0.38947787\n",
      "Iteration 1765, loss = 0.14418405\n",
      "Iteration 1766, loss = 0.14395503\n",
      "Iteration 1781, loss = 0.14544524\n",
      "Iteration 1598, loss = 0.24032073\n",
      "Iteration 528, loss = 0.35882554\n",
      "Iteration 1767, loss = 0.14377500\n",
      "Iteration 713, loss = 0.34247136\n",
      "Iteration 264, loss = 0.38926576\n",
      "Iteration 1768, loss = 0.14372250\n",
      "Iteration 1769, loss = 0.14351817\n",
      "Iteration 1117, loss = 0.28095816\n",
      "Iteration 1531, loss = 0.18445202\n",
      "Iteration 529, loss = 0.35879562\n",
      "Iteration 265, loss = 0.38917076\n",
      "Iteration 1782, loss = 0.14541895\n",
      "Iteration 1770, loss = 0.14351044\n",
      "Iteration 1599, loss = 0.24017677\n",
      "Iteration 1118, loss = 0.28089785\n",
      "Iteration 1771, loss = 0.14333170\n",
      "Iteration 266, loss = 0.38885858\n",
      "Iteration 1772, loss = 0.14328754\n",
      "Iteration 1783, loss = 0.14515517\n",
      "Iteration 714, loss = 0.34234525\n",
      "Iteration 1532, loss = 0.18398667\n",
      "Iteration 530, loss = 0.35863487\n",
      "Iteration 1773, loss = 0.14303160\n",
      "Iteration 1774, loss = 0.14288740\n",
      "Iteration 1119, loss = 0.28074378\n",
      "Iteration 267, loss = 0.38868595\n",
      "Iteration 1120, loss = 0.28062713\n",
      "Iteration 1784, loss = 0.14522165\n",
      "Iteration 268, loss = 0.38837981\n",
      "Iteration 1775, loss = 0.14265281\n",
      "Iteration 1121, loss = 0.28054158\n",
      "Iteration 1533, loss = 0.18381112\n",
      "Iteration 1600, loss = 0.24010613\n",
      "Iteration 1785, loss = 0.14499748\n",
      "Iteration 531, loss = 0.35857563\n",
      "Iteration 1776, loss = 0.14262052\n",
      "Iteration 1122, loss = 0.28042804\n",
      "Iteration 269, loss = 0.38824769\n",
      "Iteration 1777, loss = 0.14256471\n",
      "Iteration 1123, loss = 0.28033354\n",
      "Iteration 1786, loss = 0.14490159\n",
      "Iteration 1778, loss = 0.14237238\n",
      "Iteration 1601, loss = 0.23999911\n",
      "Iteration 270, loss = 0.38800078\n",
      "Iteration 1779, loss = 0.14204866\n",
      "Iteration 1780, loss = 0.14206538\n",
      "Iteration 1787, loss = 0.14475276\n",
      "Iteration 1534, loss = 0.18378918\n",
      "Iteration 1781, loss = 0.14177393\n",
      "Iteration 1124, loss = 0.28024706\n",
      "Iteration 271, loss = 0.38777346\n",
      "Iteration 715, loss = 0.34222040\n",
      "Iteration 532, loss = 0.35845599\n",
      "Iteration 272, loss = 0.38758430\n",
      "Iteration 1125, loss = 0.28012940\n",
      "Iteration 1788, loss = 0.14467492\n",
      "Iteration 1126, loss = 0.28001318\n",
      "Iteration 1782, loss = 0.14173622\n",
      "Iteration 1127, loss = 0.27992660\n",
      "Iteration 1602, loss = 0.23989457\n",
      "Iteration 1789, loss = 0.14445470\n",
      "Iteration 1128, loss = 0.27988012\n",
      "Iteration 533, loss = 0.35837140\n",
      "Iteration 273, loss = 0.38741520\n",
      "Iteration 1783, loss = 0.14163554\n",
      "Iteration 1603, loss = 0.23977436\n",
      "Iteration 1129, loss = 0.27971564\n",
      "Iteration 716, loss = 0.34190579\n",
      "Iteration 1535, loss = 0.18349973\n",
      "Iteration 1130, loss = 0.27961929\n",
      "Iteration 1790, loss = 0.14443788\n",
      "Iteration 1784, loss = 0.14164633\n",
      "Iteration 534, loss = 0.35829454\n",
      "Iteration 1785, loss = 0.14134896\n",
      "Iteration 1604, loss = 0.23974777\n",
      "Iteration 1786, loss = 0.14112258\n",
      "Iteration 1131, loss = 0.27951429\n",
      "Iteration 274, loss = 0.38717944\n",
      "Iteration 1787, loss = 0.14091161\n",
      "Iteration 1791, loss = 0.14434310\n",
      "Iteration 1536, loss = 0.18340003\n",
      "Iteration 1132, loss = 0.27943686\n",
      "Iteration 717, loss = 0.34172995\n",
      "Iteration 1792, loss = 0.14410858\n",
      "Iteration 1788, loss = 0.14090053\n",
      "Iteration 1605, loss = 0.23952808\n",
      "Iteration 1789, loss = 0.14062552\n",
      "Iteration 535, loss = 0.35820329\n",
      "Iteration 1133, loss = 0.27934544\n",
      "Iteration 275, loss = 0.38697006\n",
      "Iteration 1790, loss = 0.14049223\n",
      "Iteration 1793, loss = 0.14402251\n",
      "Iteration 536, loss = 0.35807489\n",
      "Iteration 1791, loss = 0.14037890\n",
      "Iteration 276, loss = 0.38676025\n",
      "Iteration 1606, loss = 0.23944874\n",
      "Iteration 1134, loss = 0.27927778\n",
      "Iteration 1794, loss = 0.14391084\n",
      "Iteration 1792, loss = 0.14050220\n",
      "Iteration 718, loss = 0.34154022\n",
      "Iteration 1537, loss = 0.18330110\n",
      "Iteration 1135, loss = 0.27916663\n",
      "Iteration 277, loss = 0.38664142\n",
      "Iteration 1795, loss = 0.14398967\n",
      "Iteration 1793, loss = 0.14017164\n",
      "Iteration 1796, loss = 0.14366755\n",
      "Iteration 278, loss = 0.38636483\n",
      "Iteration 1607, loss = 0.23940038\n",
      "Iteration 537, loss = 0.35795049\n",
      "Iteration 1794, loss = 0.13995644\n",
      "Iteration 1795, loss = 0.13991584\n",
      "Iteration 1797, loss = 0.14355591\n",
      "Iteration 1796, loss = 0.13969170\n",
      "Iteration 719, loss = 0.34141552\n",
      "Iteration 1797, loss = 0.13954546\n",
      "Iteration 1608, loss = 0.23921953\n",
      "Iteration 1798, loss = 0.14346359\n",
      "Iteration 279, loss = 0.38617949\n",
      "Iteration 1798, loss = 0.13948550\n",
      "Iteration 538, loss = 0.35790940\n",
      "Iteration 1136, loss = 0.27905471\n",
      "Iteration 1799, loss = 0.13929843\n",
      "Iteration 1538, loss = 0.18334719\n",
      "Iteration 1137, loss = 0.27889982\n",
      "Iteration 1609, loss = 0.23917021\n",
      "Iteration 1799, loss = 0.14339839\n",
      "Iteration 539, loss = 0.35779480\n",
      "Iteration 1610, loss = 0.23906152\n",
      "Iteration 1138, loss = 0.27882083\n",
      "Iteration 280, loss = 0.38597434\n",
      "Iteration 1800, loss = 0.14324011\n",
      "Iteration 1800, loss = 0.13919321\n",
      "Iteration 1139, loss = 0.27869623\n",
      "Iteration 720, loss = 0.34133975\n",
      "Iteration 1801, loss = 0.13896124Iteration 1801, loss = 0.14308212\n",
      "\n",
      "Iteration 1539, loss = 0.18306987\n",
      "Iteration 281, loss = 0.38580236\n",
      "Iteration 540, loss = 0.35769430\n",
      "Iteration 1140, loss = 0.27859076\n",
      "Iteration 1802, loss = 0.13886408\n",
      "Iteration 1802, loss = 0.14295934\n",
      "Iteration 1141, loss = 0.27852915\n",
      "Iteration 282, loss = 0.38559571\n",
      "Iteration 721, loss = 0.34105659\n",
      "Iteration 1540, loss = 0.18296060\n",
      "Iteration 1142, loss = 0.27838979\n",
      "Iteration 1803, loss = 0.14303125\n",
      "Iteration 541, loss = 0.35760531\n",
      "Iteration 1803, loss = 0.13875763\n",
      "Iteration 1143, loss = 0.27832124\n",
      "Iteration 1804, loss = 0.13849128\n",
      "Iteration 1541, loss = 0.18274270\n",
      "Iteration 1804, loss = 0.14281540\n",
      "Iteration 1611, loss = 0.23893570\n",
      "Iteration 283, loss = 0.38541828\n",
      "Iteration 1805, loss = 0.13849038\n",
      "Iteration 1144, loss = 0.27820502\n",
      "Iteration 542, loss = 0.35751317\n",
      "Iteration 1806, loss = 0.13840679\n",
      "Iteration 1807, loss = 0.13804861\n",
      "Iteration 1145, loss = 0.27811162\n",
      "Iteration 1805, loss = 0.14273211\n",
      "Iteration 722, loss = 0.34092122\n",
      "Iteration 1808, loss = 0.13794970\n",
      "Iteration 1146, loss = 0.27801224\n",
      "Iteration 1809, loss = 0.13791240\n",
      "Iteration 543, loss = 0.35740884\n",
      "Iteration 1810, loss = 0.13774465\n",
      "Iteration 1612, loss = 0.23886617\n",
      "Iteration 284, loss = 0.38522257\n",
      "Iteration 1806, loss = 0.14254997\n",
      "Iteration 1147, loss = 0.27791019\n",
      "Iteration 1542, loss = 0.18263690\n",
      "Iteration 1811, loss = 0.13750220\n",
      "Iteration 285, loss = 0.38506186\n",
      "Iteration 723, loss = 0.34070524\n",
      "Iteration 1812, loss = 0.13738138\n",
      "Iteration 1813, loss = 0.13729631\n",
      "Iteration 544, loss = 0.35729208\n",
      "Iteration 286, loss = 0.38486776\n",
      "Iteration 1148, loss = 0.27777389\n",
      "Iteration 1807, loss = 0.14242774\n",
      "Iteration 1613, loss = 0.23878718\n",
      "Iteration 1814, loss = 0.13711625\n",
      "Iteration 1815, loss = 0.13692385\n",
      "Iteration 1808, loss = 0.14236300\n",
      "Iteration 287, loss = 0.38469731\n",
      "Iteration 1543, loss = 0.18258871\n",
      "Iteration 724, loss = 0.34050367\n",
      "Iteration 1149, loss = 0.27777092\n",
      "Iteration 1816, loss = 0.13684102\n",
      "Iteration 545, loss = 0.35721846\n",
      "Iteration 1150, loss = 0.27760419\n",
      "Iteration 1817, loss = 0.13670097\n",
      "Iteration 1614, loss = 0.23865221\n",
      "Iteration 288, loss = 0.38448206\n",
      "Iteration 1818, loss = 0.13653269\n",
      "Iteration 1809, loss = 0.14220964\n",
      "Iteration 1819, loss = 0.13640562\n",
      "Iteration 289, loss = 0.38427192\n",
      "Iteration 546, loss = 0.35711659\n",
      "Iteration 725, loss = 0.34046361\n",
      "Iteration 1615, loss = 0.23859682\n",
      "Iteration 1151, loss = 0.27749546\n",
      "Iteration 1544, loss = 0.18250959\n",
      "Iteration 1820, loss = 0.13662527\n",
      "Iteration 1810, loss = 0.14232694\n",
      "Iteration 1152, loss = 0.27741845\n",
      "Iteration 1821, loss = 0.13633962\n",
      "Iteration 547, loss = 0.35699615\n",
      "Iteration 290, loss = 0.38408629\n",
      "Iteration 1822, loss = 0.13593795\n",
      "Iteration 1823, loss = 0.13598826\n",
      "Iteration 1153, loss = 0.27729090\n",
      "Iteration 291, loss = 0.38389430\n",
      "Iteration 1824, loss = 0.13571257\n",
      "Iteration 1811, loss = 0.14200782\n",
      "Iteration 1154, loss = 0.27721988\n",
      "Iteration 726, loss = 0.34021626\n",
      "Iteration 548, loss = 0.35695627\n",
      "Iteration 1616, loss = 0.23842329\n",
      "Iteration 1545, loss = 0.18220354\n",
      "Iteration 1825, loss = 0.13566834\n",
      "Iteration 292, loss = 0.38371526\n",
      "Iteration 1155, loss = 0.27710878\n",
      "Iteration 1826, loss = 0.13547810\n",
      "Iteration 1827, loss = 0.13527609\n",
      "Iteration 1812, loss = 0.14181525\n",
      "Iteration 1828, loss = 0.13510409\n",
      "Iteration 293, loss = 0.38360528\n",
      "Iteration 1156, loss = 0.27699918\n",
      "Iteration 1829, loss = 0.13505972\n",
      "Iteration 1830, loss = 0.13486390\n",
      "Iteration 1157, loss = 0.27687400\n",
      "Iteration 1617, loss = 0.23828907\n",
      "Iteration 549, loss = 0.35682830\n",
      "Iteration 1546, loss = 0.18225000\n",
      "Iteration 727, loss = 0.33998756\n",
      "Iteration 1831, loss = 0.13476396\n",
      "Iteration 1158, loss = 0.27675505\n",
      "Iteration 294, loss = 0.38335426\n",
      "Iteration 1618, loss = 0.23823178\n",
      "Iteration 1832, loss = 0.13462466\n",
      "Iteration 1813, loss = 0.14172694\n",
      "Iteration 1159, loss = 0.27667437\n",
      "Iteration 295, loss = 0.38323778\n",
      "Iteration 1833, loss = 0.13461752\n",
      "Iteration 1547, loss = 0.18200909\n",
      "Iteration 1834, loss = 0.13435354\n",
      "Iteration 550, loss = 0.35673175\n",
      "Iteration 1814, loss = 0.14159285\n",
      "Iteration 1160, loss = 0.27653692\n",
      "Iteration 1835, loss = 0.13416640\n",
      "Iteration 1619, loss = 0.23821186\n",
      "Iteration 728, loss = 0.33984737\n",
      "Iteration 1161, loss = 0.27645760\n",
      "Iteration 551, loss = 0.35665080\n",
      "Iteration 1836, loss = 0.13403059\n",
      "Iteration 296, loss = 0.38304906\n",
      "Iteration 1837, loss = 0.13402413\n",
      "Iteration 1815, loss = 0.14151709\n",
      "Iteration 1548, loss = 0.18185349\n",
      "Iteration 1838, loss = 0.13372929\n",
      "Iteration 1839, loss = 0.13372779\n",
      "Iteration 552, loss = 0.35653597\n",
      "Iteration 1816, loss = 0.14142522\n",
      "Iteration 1840, loss = 0.13351783\n",
      "Iteration 1162, loss = 0.27637578\n",
      "Iteration 1620, loss = 0.23800816\n",
      "Iteration 1841, loss = 0.13342354\n",
      "Iteration 1842, loss = 0.13338604\n",
      "Iteration 1817, loss = 0.14126166\n",
      "Iteration 297, loss = 0.38281607\n",
      "Iteration 1163, loss = 0.27628656\n",
      "Iteration 1818, loss = 0.14131466\n",
      "Iteration 1843, loss = 0.13323838\n",
      "Iteration 298, loss = 0.38262432\n",
      "Iteration 1549, loss = 0.18174012\n",
      "Iteration 1844, loss = 0.13306927\n",
      "Iteration 1164, loss = 0.27613707\n",
      "Iteration 553, loss = 0.35641885\n",
      "Iteration 1819, loss = 0.14106270\n",
      "Iteration 1845, loss = 0.13284705\n",
      "Iteration 729, loss = 0.33966453\n",
      "Iteration 1846, loss = 0.13272031\n",
      "Iteration 1165, loss = 0.27603131\n",
      "Iteration 1621, loss = 0.23787182\n",
      "Iteration 1820, loss = 0.14094582\n",
      "Iteration 1847, loss = 0.13253537\n",
      "Iteration 1848, loss = 0.13240939\n",
      "Iteration 299, loss = 0.38257410\n",
      "Iteration 1849, loss = 0.13231717\n",
      "Iteration 1850, loss = 0.13222873\n",
      "Iteration 1851, loss = 0.13204919\n",
      "Iteration 300, loss = 0.38230501\n",
      "Iteration 1166, loss = 0.27590961\n",
      "Iteration 1821, loss = 0.14084701\n",
      "Iteration 554, loss = 0.35632138\n",
      "Iteration 1852, loss = 0.13190491\n",
      "Iteration 1853, loss = 0.13182232\n",
      "Iteration 301, loss = 0.38211906\n",
      "Iteration 1167, loss = 0.27583271\n",
      "Iteration 555, loss = 0.35623304\n",
      "Iteration 1822, loss = 0.14082243Iteration 1854, loss = 0.13170126\n",
      "\n",
      "Iteration 1550, loss = 0.18163069\n",
      "Iteration 1168, loss = 0.27574456\n",
      "Iteration 730, loss = 0.33952961\n",
      "Iteration 1622, loss = 0.23777218\n",
      "Iteration 1855, loss = 0.13145498\n",
      "Iteration 1856, loss = 0.13152258\n",
      "Iteration 302, loss = 0.38193918\n",
      "Iteration 1169, loss = 0.27564802\n",
      "Iteration 1551, loss = 0.18162963\n",
      "Iteration 731, loss = 0.33931156\n",
      "Iteration 1857, loss = 0.13126153\n",
      "Iteration 1823, loss = 0.14065858\n",
      "Iteration 556, loss = 0.35615663\n",
      "Iteration 1824, loss = 0.14066575\n",
      "Iteration 1623, loss = 0.23769752\n",
      "Iteration 1858, loss = 0.13115873\n",
      "Iteration 1859, loss = 0.13104546\n",
      "Iteration 303, loss = 0.38176274\n",
      "Iteration 1825, loss = 0.14040950\n",
      "Iteration 1170, loss = 0.27555873\n",
      "Iteration 1860, loss = 0.13086163\n",
      "Iteration 1624, loss = 0.23757129\n",
      "Iteration 557, loss = 0.35603983\n",
      "Iteration 1171, loss = 0.27539391\n",
      "Iteration 304, loss = 0.38163240\n",
      "Iteration 1552, loss = 0.18140633\n",
      "Iteration 1172, loss = 0.27528182\n",
      "Iteration 1861, loss = 0.13072324\n",
      "Iteration 1173, loss = 0.27531089\n",
      "Iteration 1625, loss = 0.23754727\n",
      "Iteration 1174, loss = 0.27511356\n",
      "Iteration 1862, loss = 0.13067664\n",
      "Iteration 1626, loss = 0.23737088\n",
      "Iteration 1553, loss = 0.18124605\n",
      "Iteration 1175, loss = 0.27498325\n",
      "Iteration 305, loss = 0.38138389\n",
      "Iteration 1863, loss = 0.13038577\n",
      "Iteration 1176, loss = 0.27487852\n",
      "Iteration 1627, loss = 0.23727589\n",
      "Iteration 306, loss = 0.38130642\n",
      "Iteration 1864, loss = 0.13028245\n",
      "Iteration 558, loss = 0.35599833\n",
      "Iteration 732, loss = 0.33920697\n",
      "Iteration 1826, loss = 0.14029394\n",
      "Iteration 1865, loss = 0.13020002\n",
      "Iteration 1554, loss = 0.18123279\n",
      "Iteration 1866, loss = 0.13007431\n",
      "Iteration 1177, loss = 0.27482862\n",
      "Iteration 1867, loss = 0.12986939\n",
      "Iteration 1827, loss = 0.14017200\n",
      "Iteration 1628, loss = 0.23713167\n",
      "Iteration 1868, loss = 0.12982661\n",
      "Iteration 733, loss = 0.33919048\n",
      "Iteration 559, loss = 0.35584480\n",
      "Iteration 1828, loss = 0.14006759\n",
      "Iteration 1869, loss = 0.12955485\n",
      "Iteration 1178, loss = 0.27468743\n",
      "Iteration 1870, loss = 0.12971043\n",
      "Iteration 1871, loss = 0.12943722\n",
      "Iteration 1829, loss = 0.13993937\n",
      "Iteration 1555, loss = 0.18111001\n",
      "Iteration 1872, loss = 0.12921630\n",
      "Iteration 1179, loss = 0.27457996\n",
      "Iteration 734, loss = 0.33878033\n",
      "Iteration 560, loss = 0.35575288\n",
      "Iteration 1629, loss = 0.23716536\n",
      "Iteration 1830, loss = 0.13980937\n",
      "Iteration 1180, loss = 0.27448913\n",
      "Iteration 307, loss = 0.38102223\n",
      "Iteration 1873, loss = 0.12907417\n",
      "Iteration 1831, loss = 0.13970525\n",
      "Iteration 1874, loss = 0.12890767\n",
      "Iteration 1875, loss = 0.12878841\n",
      "Iteration 1181, loss = 0.27448712\n",
      "Iteration 1876, loss = 0.12879114\n",
      "Iteration 561, loss = 0.35566252\n",
      "Iteration 1877, loss = 0.12863381\n",
      "Iteration 1630, loss = 0.23700890\n",
      "Iteration 1878, loss = 0.12836942\n",
      "Iteration 1832, loss = 0.13959502\n",
      "Iteration 1879, loss = 0.12844298\n",
      "Iteration 735, loss = 0.33866183\n",
      "Iteration 1182, loss = 0.27431353\n",
      "Iteration 308, loss = 0.38089332\n",
      "Iteration 1556, loss = 0.18093792\n",
      "Iteration 1631, loss = 0.23684653\n",
      "Iteration 562, loss = 0.35555296\n",
      "Iteration 1183, loss = 0.27414628\n",
      "Iteration 1833, loss = 0.13952140\n",
      "Iteration 1880, loss = 0.12809998\n",
      "Iteration 309, loss = 0.38068724\n",
      "Iteration 1881, loss = 0.12800458\n",
      "Iteration 1184, loss = 0.27409471\n",
      "Iteration 1834, loss = 0.13942361\n",
      "Iteration 736, loss = 0.33848692\n",
      "Iteration 1882, loss = 0.12799587\n",
      "Iteration 1883, loss = 0.12768543\n",
      "Iteration 1185, loss = 0.27394777\n",
      "Iteration 310, loss = 0.38053596\n",
      "Iteration 1557, loss = 0.18075177\n",
      "Iteration 1884, loss = 0.12757863\n",
      "Iteration 1186, loss = 0.27399099\n",
      "Iteration 563, loss = 0.35553945\n",
      "Iteration 1835, loss = 0.13934859\n",
      "Iteration 1187, loss = 0.27375943\n",
      "Iteration 1632, loss = 0.23674830\n",
      "Iteration 1885, loss = 0.12752421\n",
      "Iteration 1836, loss = 0.13920465\n",
      "Iteration 1188, loss = 0.27364399\n",
      "Iteration 1886, loss = 0.12739173\n",
      "Iteration 311, loss = 0.38038899\n",
      "Iteration 737, loss = 0.33827305\n",
      "Iteration 1887, loss = 0.12710768\n",
      "Iteration 312, loss = 0.38017086\n",
      "Iteration 564, loss = 0.35536898\n",
      "Iteration 1837, loss = 0.13909253\n",
      "Iteration 1189, loss = 0.27352992\n",
      "Iteration 313, loss = 0.38002233\n",
      "Iteration 1838, loss = 0.13901596\n",
      "Iteration 1558, loss = 0.18060002\n",
      "Iteration 1633, loss = 0.23665546\n",
      "Iteration 1888, loss = 0.12692284\n",
      "Iteration 314, loss = 0.37988217\n",
      "Iteration 1839, loss = 0.13887340\n",
      "Iteration 1889, loss = 0.12683360\n",
      "Iteration 1190, loss = 0.27342146\n",
      "Iteration 315, loss = 0.37978764\n",
      "Iteration 1890, loss = 0.12672636\n",
      "Iteration 1891, loss = 0.12658091\n",
      "Iteration 1892, loss = 0.12638647\n",
      "Iteration 738, loss = 0.33815487\n",
      "Iteration 1191, loss = 0.27337950\n",
      "Iteration 1893, loss = 0.12623221\n",
      "Iteration 565, loss = 0.35528695\n",
      "Iteration 1894, loss = 0.12609561\n",
      "Iteration 1634, loss = 0.23654496\n",
      "Iteration 1895, loss = 0.12597900\n",
      "Iteration 1192, loss = 0.27322362\n",
      "Iteration 1840, loss = 0.13877520\n",
      "Iteration 1896, loss = 0.12590903\n",
      "Iteration 316, loss = 0.37960877\n",
      "Iteration 1897, loss = 0.12574028\n",
      "Iteration 1559, loss = 0.18050542\n",
      "Iteration 1898, loss = 0.12562692\n",
      "Iteration 739, loss = 0.33799199\n",
      "Iteration 1899, loss = 0.12541297\n",
      "Iteration 566, loss = 0.35517129\n",
      "Iteration 1193, loss = 0.27314047\n",
      "Iteration 1635, loss = 0.23641901\n",
      "Iteration 1900, loss = 0.12530755\n",
      "Iteration 1841, loss = 0.13885732\n",
      "Iteration 317, loss = 0.37931842\n",
      "Iteration 740, loss = 0.33774795\n",
      "Iteration 1901, loss = 0.12536763\n",
      "Iteration 1194, loss = 0.27301704\n",
      "Iteration 1636, loss = 0.23630377\n",
      "Iteration 1902, loss = 0.12497177\n",
      "Iteration 1842, loss = 0.13859734\n",
      "Iteration 1903, loss = 0.12483198\n",
      "Iteration 567, loss = 0.35507141\n",
      "Iteration 318, loss = 0.37916836\n",
      "Iteration 1843, loss = 0.13844702\n",
      "Iteration 1195, loss = 0.27289925\n",
      "Iteration 1560, loss = 0.18048472\n",
      "Iteration 1904, loss = 0.12474490\n",
      "Iteration 1905, loss = 0.12457909\n",
      "Iteration 1637, loss = 0.23628023\n",
      "Iteration 1906, loss = 0.12453286\n",
      "Iteration 1907, loss = 0.12441217\n",
      "Iteration 741, loss = 0.33761395\n",
      "Iteration 568, loss = 0.35496304\n",
      "Iteration 1908, loss = 0.12427364\n",
      "Iteration 1844, loss = 0.13837444\n",
      "Iteration 1561, loss = 0.18038510\n",
      "Iteration 319, loss = 0.37902180\n",
      "Iteration 1909, loss = 0.12410129\n",
      "Iteration 1638, loss = 0.23609435\n",
      "Iteration 1910, loss = 0.12406144\n",
      "Iteration 1845, loss = 0.13833425\n",
      "Iteration 1196, loss = 0.27285081\n",
      "Iteration 1911, loss = 0.12385055\n",
      "Iteration 569, loss = 0.35488034\n",
      "Iteration 1197, loss = 0.27267897\n",
      "Iteration 1912, loss = 0.12356833\n",
      "Iteration 742, loss = 0.33744310\n",
      "Iteration 1639, loss = 0.23601142\n",
      "Iteration 1913, loss = 0.12348311\n",
      "Iteration 320, loss = 0.37893212\n",
      "Iteration 1198, loss = 0.27259519\n",
      "Iteration 1914, loss = 0.12331261\n",
      "Iteration 1199, loss = 0.27246914\n",
      "Iteration 1846, loss = 0.13807894\n",
      "Iteration 1562, loss = 0.18037236\n",
      "Iteration 1200, loss = 0.27240611\n",
      "Iteration 321, loss = 0.37864874\n",
      "Iteration 1915, loss = 0.12324310\n",
      "Iteration 1847, loss = 0.13792430\n",
      "Iteration 570, loss = 0.35477370\n",
      "Iteration 1640, loss = 0.23589061\n",
      "Iteration 322, loss = 0.37862113\n",
      "Iteration 1201, loss = 0.27227408\n",
      "Iteration 1916, loss = 0.12311288\n",
      "Iteration 1848, loss = 0.13788647\n",
      "Iteration 743, loss = 0.33731868\n",
      "Iteration 1917, loss = 0.12288847\n",
      "Iteration 1563, loss = 0.18000263\n",
      "Iteration 1918, loss = 0.12295676\n",
      "Iteration 1202, loss = 0.27220887\n",
      "Iteration 323, loss = 0.37834201\n",
      "Iteration 1641, loss = 0.23580770\n",
      "Iteration 571, loss = 0.35468375\n",
      "Iteration 1919, loss = 0.12272083\n",
      "Iteration 1849, loss = 0.13776462\n",
      "Iteration 1203, loss = 0.27208644\n",
      "Iteration 1920, loss = 0.12254612\n",
      "Iteration 744, loss = 0.33712414\n",
      "Iteration 1642, loss = 0.23567336\n",
      "Iteration 1850, loss = 0.13770063\n",
      "Iteration 1204, loss = 0.27200222\n",
      "Iteration 1921, loss = 0.12272965\n",
      "Iteration 572, loss = 0.35462280\n",
      "Iteration 324, loss = 0.37822448\n",
      "Iteration 1922, loss = 0.12244536\n",
      "Iteration 1205, loss = 0.27188421\n",
      "Iteration 1643, loss = 0.23561620\n",
      "Iteration 1923, loss = 0.12223825\n",
      "Iteration 1564, loss = 0.17983820\n",
      "Iteration 1851, loss = 0.13753295\n",
      "Iteration 745, loss = 0.33689822\n",
      "Iteration 1924, loss = 0.12200629\n",
      "Iteration 1206, loss = 0.27176551\n",
      "Iteration 325, loss = 0.37801545\n",
      "Iteration 746, loss = 0.33678950\n",
      "Iteration 326, loss = 0.37790471\n",
      "Iteration 1925, loss = 0.12191212\n",
      "Iteration 1207, loss = 0.27163871\n",
      "Iteration 573, loss = 0.35454201\n",
      "Iteration 1926, loss = 0.12178062\n",
      "Iteration 1852, loss = 0.13741550\n",
      "Iteration 1927, loss = 0.12184981\n",
      "Iteration 1565, loss = 0.17976737\n",
      "Iteration 1644, loss = 0.23546805\n",
      "Iteration 747, loss = 0.33660651\n",
      "Iteration 1208, loss = 0.27153386\n",
      "Iteration 574, loss = 0.35444821\n",
      "Iteration 1928, loss = 0.12162381\n",
      "Iteration 327, loss = 0.37772457\n",
      "Iteration 1209, loss = 0.27141560\n",
      "Iteration 1929, loss = 0.12134188\n",
      "Iteration 1930, loss = 0.12131200\n",
      "Iteration 1853, loss = 0.13734921\n",
      "Iteration 1931, loss = 0.12110609\n",
      "Iteration 328, loss = 0.37766237\n",
      "Iteration 1566, loss = 0.17975398\n",
      "Iteration 575, loss = 0.35428342\n",
      "Iteration 1210, loss = 0.27148597\n",
      "Iteration 1932, loss = 0.12106375\n",
      "Iteration 1854, loss = 0.13725679\n",
      "Iteration 1645, loss = 0.23542061\n",
      "Iteration 1211, loss = 0.27121559\n",
      "Iteration 329, loss = 0.37739359\n",
      "Iteration 748, loss = 0.33638706\n",
      "Iteration 1212, loss = 0.27110681\n",
      "Iteration 1933, loss = 0.12087596\n",
      "Iteration 1934, loss = 0.12088224\n",
      "Iteration 1213, loss = 0.27099385\n",
      "Iteration 1935, loss = 0.12059042\n",
      "Iteration 330, loss = 0.37721972\n",
      "Iteration 1855, loss = 0.13715788\n",
      "Iteration 1936, loss = 0.12071918\n",
      "Iteration 1214, loss = 0.27090185\n",
      "Iteration 1937, loss = 0.12036086\n",
      "Iteration 576, loss = 0.35419959\n",
      "Iteration 1567, loss = 0.17954112\n",
      "Iteration 1215, loss = 0.27081806\n",
      "Iteration 1646, loss = 0.23529277\n",
      "Iteration 1938, loss = 0.12024837\n",
      "Iteration 331, loss = 0.37710489\n",
      "Iteration 749, loss = 0.33621792\n",
      "Iteration 332, loss = 0.37689379\n",
      "Iteration 1647, loss = 0.23528464\n",
      "Iteration 1216, loss = 0.27071108\n",
      "Iteration 1568, loss = 0.17974273\n",
      "Iteration 577, loss = 0.35412027\n",
      "Iteration 1939, loss = 0.12023092\n",
      "Iteration 1856, loss = 0.13716386\n",
      "Iteration 1217, loss = 0.27060626\n",
      "Iteration 1940, loss = 0.12010739\n",
      "Iteration 1941, loss = 0.11986101\n",
      "Iteration 1648, loss = 0.23507139\n",
      "Iteration 1942, loss = 0.11970788\n",
      "Iteration 578, loss = 0.35398886\n",
      "Iteration 333, loss = 0.37676126\n",
      "Iteration 1943, loss = 0.11968638\n",
      "Iteration 1218, loss = 0.27048229\n",
      "Iteration 750, loss = 0.33603566\n",
      "Iteration 1569, loss = 0.17953008\n",
      "Iteration 1857, loss = 0.13692542\n",
      "Iteration 1649, loss = 0.23495256\n",
      "Iteration 1944, loss = 0.11941293\n",
      "Iteration 334, loss = 0.37659003\n",
      "Iteration 1219, loss = 0.27039591\n",
      "Iteration 579, loss = 0.35392801\n",
      "Iteration 1945, loss = 0.11931502\n",
      "Iteration 1946, loss = 0.11930346\n",
      "Iteration 1220, loss = 0.27027976\n",
      "Iteration 1570, loss = 0.17917281\n",
      "Iteration 1947, loss = 0.11918788\n",
      "Iteration 1650, loss = 0.23494197\n",
      "Iteration 751, loss = 0.33593040\n",
      "Iteration 1948, loss = 0.11895137\n",
      "Iteration 335, loss = 0.37643497\n",
      "Iteration 1949, loss = 0.11890224\n",
      "Iteration 1221, loss = 0.27013870\n",
      "Iteration 1950, loss = 0.11883506\n",
      "Iteration 1651, loss = 0.23475053\n",
      "Iteration 1571, loss = 0.17907202\n",
      "Iteration 580, loss = 0.35379904\n",
      "Iteration 1951, loss = 0.11865744\n",
      "Iteration 336, loss = 0.37632924\n",
      "Iteration 1858, loss = 0.13682166\n",
      "Iteration 1652, loss = 0.23465649\n",
      "Iteration 1222, loss = 0.27002863\n",
      "Iteration 337, loss = 0.37611691\n",
      "Iteration 1952, loss = 0.11868740\n",
      "Iteration 1859, loss = 0.13663842\n",
      "Iteration 1953, loss = 0.11833326\n",
      "Iteration 1860, loss = 0.13660416\n",
      "Iteration 752, loss = 0.33581247\n",
      "Iteration 1954, loss = 0.11824302\n",
      "Iteration 1223, loss = 0.26994322\n",
      "Iteration 581, loss = 0.35370239\n",
      "Iteration 1653, loss = 0.23452970\n",
      "Iteration 338, loss = 0.37599328Iteration 1955, loss = 0.11818340\n",
      "\n",
      "Iteration 1861, loss = 0.13650512\n",
      "Iteration 1956, loss = 0.11798054\n",
      "Iteration 1224, loss = 0.26998695\n",
      "Iteration 1957, loss = 0.11782528\n",
      "Iteration 1958, loss = 0.11775988\n",
      "Iteration 1862, loss = 0.13644034\n",
      "Iteration 1225, loss = 0.26972659\n",
      "Iteration 1959, loss = 0.11818101\n",
      "Iteration 582, loss = 0.35359718\n",
      "Iteration 1960, loss = 0.11752733\n",
      "Iteration 1961, loss = 0.11737819\n",
      "Iteration 1962, loss = 0.11733536\n",
      "Iteration 1572, loss = 0.17895432\n",
      "Iteration 1654, loss = 0.23442102\n",
      "Iteration 1863, loss = 0.13625254\n",
      "Iteration 1963, loss = 0.11730262\n",
      "Iteration 339, loss = 0.37584781\n",
      "Iteration 753, loss = 0.33561760\n",
      "Iteration 583, loss = 0.35352137\n",
      "Iteration 1226, loss = 0.26967124\n",
      "Iteration 1864, loss = 0.13626232\n",
      "Iteration 340, loss = 0.37567990\n",
      "Iteration 1964, loss = 0.11709059\n",
      "Iteration 1573, loss = 0.17876641\n",
      "Iteration 1865, loss = 0.13609277\n",
      "Iteration 341, loss = 0.37550012\n",
      "Iteration 1965, loss = 0.11705028\n",
      "Iteration 1655, loss = 0.23436755\n",
      "Iteration 1966, loss = 0.11684010\n",
      "Iteration 342, loss = 0.37533435\n",
      "Iteration 1866, loss = 0.13610333\n",
      "Iteration 1967, loss = 0.11667119\n",
      "Iteration 1574, loss = 0.17882268\n",
      "Iteration 1227, loss = 0.26959906\n",
      "Iteration 754, loss = 0.33543765\n",
      "Iteration 584, loss = 0.35344605\n",
      "Iteration 343, loss = 0.37527680\n",
      "Iteration 1968, loss = 0.11651403\n",
      "Iteration 1228, loss = 0.26940052\n",
      "Iteration 1867, loss = 0.13597651\n",
      "Iteration 344, loss = 0.37502300\n",
      "Iteration 1229, loss = 0.26927853\n",
      "Iteration 1969, loss = 0.11647774\n",
      "Iteration 1868, loss = 0.13585424\n",
      "Iteration 1656, loss = 0.23450106\n",
      "Iteration 1230, loss = 0.26920309\n",
      "Iteration 585, loss = 0.35331600\n",
      "Iteration 1970, loss = 0.11639508\n",
      "Iteration 755, loss = 0.33520481\n",
      "Iteration 345, loss = 0.37488709\n",
      "Iteration 1231, loss = 0.26910153\n",
      "Iteration 1575, loss = 0.17858714\n",
      "Iteration 346, loss = 0.37471794\n",
      "Iteration 1657, loss = 0.23418827\n",
      "Iteration 1971, loss = 0.11619854\n",
      "Iteration 586, loss = 0.35325324\n",
      "Iteration 756, loss = 0.33504698\n",
      "Iteration 1869, loss = 0.13562156\n",
      "Iteration 1232, loss = 0.26898093\n",
      "Iteration 1972, loss = 0.11623868\n",
      "Iteration 1658, loss = 0.23415804\n",
      "Iteration 587, loss = 0.35310457\n",
      "Iteration 1973, loss = 0.11600585\n",
      "Iteration 1870, loss = 0.13556303\n",
      "Iteration 1974, loss = 0.11571577\n",
      "Iteration 347, loss = 0.37454791\n",
      "Iteration 1871, loss = 0.13544890\n",
      "Iteration 1975, loss = 0.11571657\n",
      "Iteration 757, loss = 0.33481821\n",
      "Iteration 1976, loss = 0.11553672\n",
      "Iteration 348, loss = 0.37439548\n",
      "Iteration 1977, loss = 0.11547116\n",
      "Iteration 1576, loss = 0.17862536\n",
      "Iteration 1233, loss = 0.26888195\n",
      "Iteration 1659, loss = 0.23396983\n",
      "Iteration 1978, loss = 0.11525699\n",
      "Iteration 1979, loss = 0.11526323\n",
      "Iteration 588, loss = 0.35302004\n",
      "Iteration 1980, loss = 0.11511230\n",
      "Iteration 1872, loss = 0.13535714\n",
      "Iteration 349, loss = 0.37427375\n",
      "Iteration 1981, loss = 0.11489245\n",
      "Iteration 758, loss = 0.33472897\n",
      "Iteration 1234, loss = 0.26889127\n",
      "Iteration 1982, loss = 0.11481483\n",
      "Iteration 589, loss = 0.35291391\n",
      "Iteration 350, loss = 0.37413077\n",
      "Iteration 1983, loss = 0.11470918\n",
      "Iteration 1235, loss = 0.26863314\n",
      "Iteration 1577, loss = 0.17849880\n",
      "Iteration 1984, loss = 0.11453463\n",
      "Iteration 1236, loss = 0.26858054\n",
      "Iteration 1873, loss = 0.13522914\n",
      "Iteration 1660, loss = 0.23388871\n",
      "Iteration 1237, loss = 0.26853158\n",
      "Iteration 1874, loss = 0.13518477\n",
      "Iteration 1238, loss = 0.26833152\n",
      "Iteration 351, loss = 0.37393173\n",
      "Iteration 1661, loss = 0.23375329\n",
      "Iteration 1985, loss = 0.11455919\n",
      "Iteration 1239, loss = 0.26818844\n",
      "Iteration 1986, loss = 0.11427496\n",
      "Iteration 1875, loss = 0.13503013\n",
      "Iteration 590, loss = 0.35285675\n",
      "Iteration 1987, loss = 0.11426431\n",
      "Iteration 1988, loss = 0.11421352\n",
      "Iteration 759, loss = 0.33447346\n",
      "Iteration 352, loss = 0.37379737\n",
      "Iteration 1989, loss = 0.11407354\n",
      "Iteration 1662, loss = 0.23378548\n",
      "Iteration 353, loss = 0.37366586\n",
      "Iteration 1240, loss = 0.26820003\n",
      "Iteration 1990, loss = 0.11385525\n",
      "Iteration 1663, loss = 0.23350719\n",
      "Iteration 1578, loss = 0.17827807\n",
      "Iteration 1991, loss = 0.11376779\n",
      "Iteration 1992, loss = 0.11364793\n",
      "Iteration 1993, loss = 0.11363315\n",
      "Iteration 760, loss = 0.33436549\n",
      "Iteration 591, loss = 0.35274524\n",
      "Iteration 1664, loss = 0.23339502\n",
      "Iteration 1994, loss = 0.11344739\n",
      "Iteration 354, loss = 0.37346562\n",
      "Iteration 1241, loss = 0.26799136\n",
      "Iteration 1242, loss = 0.26787930\n",
      "Iteration 1876, loss = 0.13495435\n",
      "Iteration 355, loss = 0.37334948\n",
      "Iteration 1665, loss = 0.23330607\n",
      "Iteration 1995, loss = 0.11348474\n",
      "Iteration 1243, loss = 0.26782561\n",
      "Iteration 592, loss = 0.35265849\n",
      "Iteration 1579, loss = 0.17819941\n",
      "Iteration 1996, loss = 0.11313683\n",
      "Iteration 1244, loss = 0.26767081\n",
      "Iteration 1997, loss = 0.11304577\n",
      "Iteration 1245, loss = 0.26765463\n",
      "Iteration 593, loss = 0.35252580\n",
      "Iteration 1877, loss = 0.13489622\n",
      "Iteration 761, loss = 0.33414147\n",
      "Iteration 356, loss = 0.37326067\n",
      "Iteration 1998, loss = 0.11300871\n",
      "Iteration 1999, loss = 0.11287331\n",
      "Iteration 1878, loss = 0.13469707\n",
      "Iteration 2000, loss = 0.11273845\n",
      "Iteration 594, loss = 0.35243555\n",
      "Iteration 2001, loss = 0.11265228\n",
      "Iteration 1580, loss = 0.17794850\n",
      "Iteration 2002, loss = 0.11268262\n",
      "Iteration 1666, loss = 0.23319750\n",
      "Iteration 762, loss = 0.33412418\n",
      "Iteration 1246, loss = 0.26746867\n",
      "Iteration 595, loss = 0.35231962\n",
      "Iteration 1879, loss = 0.13474060\n",
      "Iteration 357, loss = 0.37306205\n",
      "Iteration 1247, loss = 0.26735683\n",
      "Iteration 763, loss = 0.33394883\n",
      "Iteration 1880, loss = 0.13452164\n",
      "Iteration 1667, loss = 0.23306584\n",
      "Iteration 1248, loss = 0.26726049\n",
      "Iteration 1881, loss = 0.13443841\n",
      "Iteration 358, loss = 0.37287513\n",
      "Iteration 596, loss = 0.35225240\n",
      "Iteration 1249, loss = 0.26714196\n",
      "Iteration 1882, loss = 0.13434847\n",
      "Iteration 764, loss = 0.33363283\n",
      "Iteration 1581, loss = 0.17804421\n",
      "Iteration 1668, loss = 0.23303222\n",
      "Iteration 597, loss = 0.35212407\n",
      "Iteration 2003, loss = 0.11249903\n",
      "Iteration 1250, loss = 0.26700587\n",
      "Iteration 2004, loss = 0.11234266\n",
      "Iteration 2005, loss = 0.11220671\n",
      "Iteration 359, loss = 0.37276653\n",
      "Iteration 1883, loss = 0.13428964\n",
      "Iteration 2006, loss = 0.11205040\n",
      "Iteration 2007, loss = 0.11215406\n",
      "Iteration 1251, loss = 0.26692985\n",
      "Iteration 1669, loss = 0.23292604\n",
      "Iteration 598, loss = 0.35203534\n",
      "Iteration 2008, loss = 0.11190607\n",
      "Iteration 360, loss = 0.37266320\n",
      "Iteration 1884, loss = 0.13414962\n",
      "Iteration 2009, loss = 0.11175956\n",
      "Iteration 1582, loss = 0.17780544\n",
      "Iteration 2010, loss = 0.11165792\n",
      "Iteration 361, loss = 0.37243936\n",
      "Iteration 2011, loss = 0.11150079\n",
      "Iteration 599, loss = 0.35194548\n",
      "Iteration 1885, loss = 0.13399136\n",
      "Iteration 1252, loss = 0.26682074\n",
      "Iteration 765, loss = 0.33342556\n",
      "Iteration 2012, loss = 0.11142191\n",
      "Iteration 1253, loss = 0.26672685\n",
      "Iteration 1670, loss = 0.23281968\n",
      "Iteration 1886, loss = 0.13402127\n",
      "Iteration 362, loss = 0.37229047\n",
      "Iteration 2013, loss = 0.11133532\n",
      "Iteration 1254, loss = 0.26657884\n",
      "Iteration 600, loss = 0.35183079\n",
      "Iteration 2014, loss = 0.11108583\n",
      "Iteration 1255, loss = 0.26647394\n",
      "Iteration 1887, loss = 0.13385621\n",
      "Iteration 2015, loss = 0.11111840\n",
      "Iteration 363, loss = 0.37217960\n",
      "Iteration 1671, loss = 0.23270507\n",
      "Iteration 2016, loss = 0.11094398\n",
      "Iteration 1256, loss = 0.26635941\n",
      "Iteration 1583, loss = 0.17770546\n",
      "Iteration 2017, loss = 0.11082016\n",
      "Iteration 1672, loss = 0.23257917\n",
      "Iteration 2018, loss = 0.11074991\n",
      "Iteration 1888, loss = 0.13401613\n",
      "Iteration 364, loss = 0.37202228\n",
      "Iteration 601, loss = 0.35176032\n",
      "Iteration 1257, loss = 0.26623179\n",
      "Iteration 2019, loss = 0.11089992\n",
      "Iteration 365, loss = 0.37188462\n",
      "Iteration 766, loss = 0.33326533\n",
      "Iteration 1889, loss = 0.13369538\n",
      "Iteration 1258, loss = 0.26617814\n",
      "Iteration 2020, loss = 0.11046140\n",
      "Iteration 1673, loss = 0.23249699\n",
      "Iteration 366, loss = 0.37172954\n",
      "Iteration 602, loss = 0.35165864\n",
      "Iteration 2021, loss = 0.11040816\n",
      "Iteration 1584, loss = 0.17744725\n",
      "Iteration 1890, loss = 0.13359373\n",
      "Iteration 2022, loss = 0.11026686\n",
      "Iteration 2023, loss = 0.11011614\n",
      "Iteration 767, loss = 0.33323857\n",
      "Iteration 1259, loss = 0.26603574\n",
      "Iteration 2024, loss = 0.10996931\n",
      "Iteration 367, loss = 0.37154027\n",
      "Iteration 1674, loss = 0.23236221\n",
      "Iteration 1585, loss = 0.17745005\n",
      "Iteration 603, loss = 0.35153768\n",
      "Iteration 1260, loss = 0.26593727\n",
      "Iteration 2025, loss = 0.10992954\n",
      "Iteration 2026, loss = 0.10978708\n",
      "Iteration 1261, loss = 0.26579483\n",
      "Iteration 1891, loss = 0.13345170\n",
      "Iteration 2027, loss = 0.10967102\n",
      "Iteration 1675, loss = 0.23227563\n",
      "Iteration 604, loss = 0.35146733\n",
      "Iteration 2028, loss = 0.10961141\n",
      "Iteration 768, loss = 0.33314582\n",
      "Iteration 368, loss = 0.37139464\n",
      "Iteration 1262, loss = 0.26573373\n",
      "Iteration 1892, loss = 0.13337521\n",
      "Iteration 605, loss = 0.35137550\n",
      "Iteration 369, loss = 0.37130308\n",
      "Iteration 1676, loss = 0.23216415\n",
      "Iteration 1893, loss = 0.13336097\n",
      "Iteration 1263, loss = 0.26563599\n",
      "Iteration 370, loss = 0.37116966\n",
      "Iteration 2029, loss = 0.10955982\n",
      "Iteration 769, loss = 0.33288533\n",
      "Iteration 1264, loss = 0.26549243\n",
      "Iteration 606, loss = 0.35125474\n",
      "Iteration 2030, loss = 0.10935784\n",
      "Iteration 1677, loss = 0.23204474\n",
      "Iteration 1586, loss = 0.17728028\n",
      "Iteration 1265, loss = 0.26536084\n",
      "Iteration 2031, loss = 0.10933028\n",
      "Iteration 2032, loss = 0.10913824\n",
      "Iteration 607, loss = 0.35117044\n",
      "Iteration 1678, loss = 0.23193379\n",
      "Iteration 2033, loss = 0.10907885\n",
      "Iteration 1266, loss = 0.26526820\n",
      "Iteration 1894, loss = 0.13310494\n",
      "Iteration 2034, loss = 0.10899266\n",
      "Iteration 371, loss = 0.37095276\n",
      "Iteration 2035, loss = 0.10887060\n",
      "Iteration 1267, loss = 0.26524055\n",
      "Iteration 1587, loss = 0.17718828\n",
      "Iteration 2036, loss = 0.10871942\n",
      "Iteration 1268, loss = 0.26504827\n",
      "Iteration 770, loss = 0.33263611\n",
      "Iteration 608, loss = 0.35104852\n",
      "Iteration 1895, loss = 0.13305471\n",
      "Iteration 372, loss = 0.37084535\n",
      "Iteration 1679, loss = 0.23192272\n",
      "Iteration 1269, loss = 0.26495524\n",
      "Iteration 1588, loss = 0.17698385\n",
      "Iteration 2037, loss = 0.10861114\n",
      "Iteration 1270, loss = 0.26481116\n",
      "Iteration 1896, loss = 0.13306505\n",
      "Iteration 1271, loss = 0.26473465\n",
      "Iteration 609, loss = 0.35095206\n",
      "Iteration 771, loss = 0.33244645\n",
      "Iteration 1272, loss = 0.26459780\n",
      "Iteration 1897, loss = 0.13290985\n",
      "Iteration 2038, loss = 0.10863327\n",
      "Iteration 1680, loss = 0.23177222\n",
      "Iteration 1898, loss = 0.13281551\n",
      "Iteration 373, loss = 0.37067652\n",
      "Iteration 2039, loss = 0.10835524\n",
      "Iteration 610, loss = 0.35084033\n",
      "Iteration 2040, loss = 0.10827641\n",
      "Iteration 1273, loss = 0.26453133\n",
      "Iteration 1899, loss = 0.13279793\n",
      "Iteration 2041, loss = 0.10819269\n",
      "Iteration 374, loss = 0.37051468\n",
      "Iteration 2042, loss = 0.10806062\n",
      "Iteration 2043, loss = 0.10799418\n",
      "Iteration 1900, loss = 0.13259607\n",
      "Iteration 1681, loss = 0.23167627\n",
      "Iteration 375, loss = 0.37038581\n",
      "Iteration 2044, loss = 0.10786668\n",
      "Iteration 1589, loss = 0.17689467Iteration 1274, loss = 0.26436383\n",
      "\n",
      "Iteration 772, loss = 0.33221309\n",
      "Iteration 1275, loss = 0.26426200\n",
      "Iteration 611, loss = 0.35075540\n",
      "Iteration 1682, loss = 0.23151429\n",
      "Iteration 2045, loss = 0.10778548\n",
      "Iteration 1276, loss = 0.26417929\n",
      "Iteration 1901, loss = 0.13257004\n",
      "Iteration 376, loss = 0.37030790\n",
      "Iteration 2046, loss = 0.10772266\n",
      "Iteration 612, loss = 0.35064926\n",
      "Iteration 2047, loss = 0.10751719\n",
      "Iteration 1277, loss = 0.26407930\n",
      "Iteration 1590, loss = 0.17681322\n",
      "Iteration 2048, loss = 0.10743658\n",
      "Iteration 1902, loss = 0.13240140\n",
      "Iteration 773, loss = 0.33220323\n",
      "Iteration 2049, loss = 0.10755509\n",
      "Iteration 1683, loss = 0.23142709\n",
      "Iteration 377, loss = 0.37009351\n",
      "Iteration 1278, loss = 0.26392434\n",
      "Iteration 2050, loss = 0.10718641\n",
      "Iteration 1279, loss = 0.26382015\n",
      "Iteration 613, loss = 0.35060308\n",
      "Iteration 378, loss = 0.37003755\n",
      "Iteration 1903, loss = 0.13229136\n",
      "Iteration 2051, loss = 0.10710726\n",
      "Iteration 774, loss = 0.33190173\n",
      "Iteration 1591, loss = 0.17667241\n",
      "Iteration 2052, loss = 0.10700801\n",
      "Iteration 1280, loss = 0.26371858\n",
      "Iteration 2053, loss = 0.10724277\n",
      "Iteration 1281, loss = 0.26365020\n",
      "Iteration 379, loss = 0.36979440\n",
      "Iteration 2054, loss = 0.10676195\n",
      "Iteration 1684, loss = 0.23138818\n",
      "Iteration 1282, loss = 0.26352548\n",
      "Iteration 1904, loss = 0.13217406\n",
      "Iteration 1592, loss = 0.17667244\n",
      "Iteration 2055, loss = 0.10664993\n",
      "Iteration 380, loss = 0.36969240\n",
      "Iteration 614, loss = 0.35047715\n",
      "Iteration 775, loss = 0.33188793\n",
      "Iteration 1283, loss = 0.26353648\n",
      "Iteration 2056, loss = 0.10661628\n",
      "Iteration 381, loss = 0.36957166\n",
      "Iteration 1685, loss = 0.23123446\n",
      "Iteration 1284, loss = 0.26328536\n",
      "Iteration 2057, loss = 0.10645777\n",
      "Iteration 1905, loss = 0.13234146\n",
      "Iteration 2058, loss = 0.10641896\n",
      "Iteration 615, loss = 0.35034138\n",
      "Iteration 1285, loss = 0.26318313\n",
      "Iteration 1593, loss = 0.17640081\n",
      "Iteration 382, loss = 0.36935048\n",
      "Iteration 2059, loss = 0.10633445\n",
      "Iteration 776, loss = 0.33159902Iteration 1286, loss = 0.26304395\n",
      "Iteration 1686, loss = 0.23112537\n",
      "\n",
      "Iteration 2060, loss = 0.10637934\n",
      "Iteration 383, loss = 0.36921009\n",
      "Iteration 1287, loss = 0.26302962\n",
      "Iteration 1906, loss = 0.13200517\n",
      "Iteration 616, loss = 0.35024558\n",
      "Iteration 2061, loss = 0.10603790\n",
      "Iteration 384, loss = 0.36918406\n",
      "Iteration 1687, loss = 0.23108747\n",
      "Iteration 2062, loss = 0.10601040\n",
      "Iteration 1288, loss = 0.26282729\n",
      "Iteration 1594, loss = 0.17631882\n",
      "Iteration 2063, loss = 0.10582600\n",
      "Iteration 1289, loss = 0.26269416\n",
      "Iteration 385, loss = 0.36893503\n",
      "Iteration 2064, loss = 0.10577466\n",
      "Iteration 2065, loss = 0.10564983\n",
      "Iteration 617, loss = 0.35019919\n",
      "Iteration 1907, loss = 0.13194307\n",
      "Iteration 777, loss = 0.33138874\n",
      "Iteration 1290, loss = 0.26263677\n",
      "Iteration 386, loss = 0.36879992\n",
      "Iteration 2066, loss = 0.10554298\n",
      "Iteration 1595, loss = 0.17626157\n",
      "Iteration 2067, loss = 0.10552121\n",
      "Iteration 1291, loss = 0.26249453\n",
      "Iteration 1688, loss = 0.23095967\n",
      "Iteration 1908, loss = 0.13182955\n",
      "Iteration 387, loss = 0.36867454\n",
      "Iteration 2068, loss = 0.10526057\n",
      "Iteration 618, loss = 0.35005432\n",
      "Iteration 2069, loss = 0.10526476\n",
      "Iteration 1292, loss = 0.26237549\n",
      "Iteration 2070, loss = 0.10516239\n",
      "Iteration 388, loss = 0.36853639\n",
      "Iteration 778, loss = 0.33119888\n",
      "Iteration 2071, loss = 0.10514244\n",
      "Iteration 1909, loss = 0.13172554\n",
      "Iteration 2072, loss = 0.10486331\n",
      "Iteration 1689, loss = 0.23079814\n",
      "Iteration 389, loss = 0.36851138\n",
      "Iteration 2073, loss = 0.10475192\n",
      "Iteration 1293, loss = 0.26235182\n",
      "Iteration 2074, loss = 0.10475172\n",
      "Iteration 1910, loss = 0.13162798\n",
      "Iteration 1294, loss = 0.26215141\n",
      "Iteration 619, loss = 0.34995198\n",
      "Iteration 2075, loss = 0.10452415\n",
      "Iteration 2076, loss = 0.10456216\n",
      "Iteration 1295, loss = 0.26203934\n",
      "Iteration 1596, loss = 0.17614339\n",
      "Iteration 2077, loss = 0.10439133\n",
      "Iteration 2078, loss = 0.10437021\n",
      "Iteration 1911, loss = 0.13158306\n",
      "Iteration 2079, loss = 0.10419079\n",
      "Iteration 1296, loss = 0.26195808\n",
      "Iteration 1690, loss = 0.23070708\n",
      "Iteration 620, loss = 0.34986470\n",
      "Iteration 2080, loss = 0.10410519\n",
      "Iteration 2081, loss = 0.10400067\n",
      "Iteration 1597, loss = 0.17598571\n",
      "Iteration 1297, loss = 0.26185132\n",
      "Iteration 779, loss = 0.33109978\n",
      "Iteration 2082, loss = 0.10405558\n",
      "Iteration 390, loss = 0.36821654\n",
      "Iteration 1912, loss = 0.13146461\n",
      "Iteration 2083, loss = 0.10385860\n",
      "Iteration 621, loss = 0.34978445\n",
      "Iteration 391, loss = 0.36808327\n",
      "Iteration 1298, loss = 0.26174565\n",
      "Iteration 1691, loss = 0.23059010\n",
      "Iteration 2084, loss = 0.10363517\n",
      "Iteration 1299, loss = 0.26164777\n",
      "Iteration 780, loss = 0.33096419\n",
      "Iteration 2085, loss = 0.10371078\n",
      "Iteration 392, loss = 0.36791444\n",
      "Iteration 1692, loss = 0.23050693\n",
      "Iteration 1913, loss = 0.13138536\n",
      "Iteration 1300, loss = 0.26153359\n",
      "Iteration 393, loss = 0.36779057\n",
      "Iteration 622, loss = 0.34965457\n",
      "Iteration 1598, loss = 0.17617110\n",
      "Iteration 2086, loss = 0.10364727\n",
      "Iteration 1693, loss = 0.23050485\n",
      "Iteration 1301, loss = 0.26138923\n",
      "Iteration 2087, loss = 0.10330868\n",
      "Iteration 394, loss = 0.36764892\n",
      "Iteration 781, loss = 0.33068350\n",
      "Iteration 1914, loss = 0.13125400\n",
      "Iteration 1302, loss = 0.26131767\n",
      "Iteration 2088, loss = 0.10346758\n",
      "Iteration 1694, loss = 0.23028153\n",
      "Iteration 1915, loss = 0.13113188\n",
      "Iteration 2089, loss = 0.10322264\n",
      "Iteration 395, loss = 0.36750042\n",
      "Iteration 1303, loss = 0.26120262\n",
      "Iteration 623, loss = 0.34955470\n",
      "Iteration 2090, loss = 0.10299477\n",
      "Iteration 1695, loss = 0.23027339\n",
      "Iteration 2091, loss = 0.10295688\n",
      "Iteration 1916, loss = 0.13117246\n",
      "Iteration 1599, loss = 0.17573450\n",
      "Iteration 1304, loss = 0.26107680\n",
      "Iteration 2092, loss = 0.10297181Iteration 624, loss = 0.34944528\n",
      "\n",
      "Iteration 1917, loss = 0.13099212\n",
      "Iteration 396, loss = 0.36736341\n",
      "Iteration 2093, loss = 0.10277503\n",
      "Iteration 782, loss = 0.33069470\n",
      "Iteration 1918, loss = 0.13090030\n",
      "Iteration 1696, loss = 0.23012333\n",
      "Iteration 2094, loss = 0.10265139\n",
      "Iteration 397, loss = 0.36721612\n",
      "Iteration 1305, loss = 0.26096001\n",
      "Iteration 1600, loss = 0.17564114\n",
      "Iteration 625, loss = 0.34936100\n",
      "Iteration 1919, loss = 0.13083498\n",
      "Iteration 1697, loss = 0.23010513\n",
      "Iteration 398, loss = 0.36716543\n",
      "Iteration 1306, loss = 0.26087342\n",
      "Iteration 783, loss = 0.33035155\n",
      "Iteration 2095, loss = 0.10254939\n",
      "Iteration 1601, loss = 0.17561361\n",
      "Iteration 626, loss = 0.34928553\n",
      "Iteration 1307, loss = 0.26082384\n",
      "Iteration 1920, loss = 0.13065760\n",
      "Iteration 2096, loss = 0.10247041\n",
      "Iteration 399, loss = 0.36694432\n",
      "Iteration 2097, loss = 0.10239634\n",
      "Iteration 2098, loss = 0.10224286\n",
      "Iteration 1602, loss = 0.17546368\n",
      "Iteration 2099, loss = 0.10215087\n",
      "Iteration 1308, loss = 0.26062685\n",
      "Iteration 2100, loss = 0.10203313\n",
      "Iteration 1698, loss = 0.22986605\n",
      "Iteration 400, loss = 0.36676916\n",
      "Iteration 2101, loss = 0.10197593\n",
      "Iteration 627, loss = 0.34918431\n",
      "Iteration 784, loss = 0.33018021\n",
      "Iteration 2102, loss = 0.10188419\n",
      "Iteration 1921, loss = 0.13059369\n",
      "Iteration 401, loss = 0.36668326\n",
      "Iteration 628, loss = 0.34907616\n",
      "Iteration 1309, loss = 0.26053155\n",
      "Iteration 1699, loss = 0.22982755\n",
      "Iteration 1922, loss = 0.13061388\n",
      "Iteration 2103, loss = 0.10167829\n",
      "Iteration 402, loss = 0.36655393\n",
      "Iteration 1603, loss = 0.17534003\n",
      "Iteration 629, loss = 0.34898125\n",
      "Iteration 1923, loss = 0.13045330\n",
      "Iteration 2104, loss = 0.10172487\n",
      "Iteration 1310, loss = 0.26048365\n",
      "Iteration 2105, loss = 0.10158820\n",
      "Iteration 2106, loss = 0.10146292\n",
      "Iteration 1924, loss = 0.13047178\n",
      "Iteration 2107, loss = 0.10134434\n",
      "Iteration 785, loss = 0.33013412\n",
      "Iteration 1604, loss = 0.17522968\n",
      "Iteration 630, loss = 0.34888108\n",
      "Iteration 1700, loss = 0.22969106\n",
      "Iteration 403, loss = 0.36637675\n",
      "Iteration 2108, loss = 0.10122732\n",
      "Iteration 2109, loss = 0.10133314\n",
      "Iteration 1925, loss = 0.13026789\n",
      "Iteration 1311, loss = 0.26031144\n",
      "Iteration 404, loss = 0.36620326\n",
      "Iteration 2110, loss = 0.10107321\n",
      "Iteration 1701, loss = 0.22953904\n",
      "Iteration 1312, loss = 0.26021785\n",
      "Iteration 786, loss = 0.32984466\n",
      "Iteration 1926, loss = 0.13025204\n",
      "Iteration 2111, loss = 0.10091373\n",
      "Iteration 1313, loss = 0.26006098\n",
      "Iteration 631, loss = 0.34875837\n",
      "Iteration 1314, loss = 0.26001848\n",
      "Iteration 2112, loss = 0.10084164\n",
      "Iteration 2113, loss = 0.10074459\n",
      "Iteration 1605, loss = 0.17510730\n",
      "Iteration 1927, loss = 0.13004350\n",
      "Iteration 2114, loss = 0.10074280\n",
      "Iteration 632, loss = 0.34867681\n",
      "Iteration 1702, loss = 0.22966825\n",
      "Iteration 405, loss = 0.36605679\n",
      "Iteration 1315, loss = 0.25985576\n",
      "Iteration 2115, loss = 0.10050761\n",
      "Iteration 787, loss = 0.32966416\n",
      "Iteration 406, loss = 0.36590712\n",
      "Iteration 633, loss = 0.34860659\n",
      "Iteration 1703, loss = 0.22944620\n",
      "Iteration 2116, loss = 0.10043904\n",
      "Iteration 1928, loss = 0.12998032\n",
      "Iteration 2117, loss = 0.10042281\n",
      "Iteration 1316, loss = 0.25976844\n",
      "Iteration 407, loss = 0.36586241\n",
      "Iteration 2118, loss = 0.10023109\n",
      "Iteration 2119, loss = 0.10009870\n",
      "Iteration 1704, loss = 0.22929130\n",
      "Iteration 408, loss = 0.36563841\n",
      "Iteration 1929, loss = 0.12989529\n",
      "Iteration 2120, loss = 0.10019397\n",
      "Iteration 788, loss = 0.32955527\n",
      "Iteration 1317, loss = 0.25965277\n",
      "Iteration 634, loss = 0.34849301\n",
      "Iteration 1930, loss = 0.12989113\n",
      "Iteration 2121, loss = 0.09994108\n",
      "Iteration 1705, loss = 0.22917670\n",
      "Iteration 1318, loss = 0.25956580\n",
      "Iteration 409, loss = 0.36548311\n",
      "Iteration 1319, loss = 0.25940409\n",
      "Iteration 1931, loss = 0.12968419\n",
      "Iteration 635, loss = 0.34840884\n",
      "Iteration 2122, loss = 0.10006145\n",
      "Iteration 1606, loss = 0.17492718\n",
      "Iteration 2123, loss = 0.10000123\n",
      "Iteration 2124, loss = 0.09968254\n",
      "Iteration 1320, loss = 0.25931876\n",
      "Iteration 1706, loss = 0.22906914\n",
      "Iteration 410, loss = 0.36536453\n",
      "Iteration 636, loss = 0.34826632\n",
      "Iteration 411, loss = 0.36519641\n",
      "Iteration 1932, loss = 0.12965669\n",
      "Iteration 2125, loss = 0.09963745\n",
      "Iteration 789, loss = 0.32937766\n",
      "Iteration 1707, loss = 0.22895747\n",
      "Iteration 1607, loss = 0.17486807\n",
      "Iteration 2126, loss = 0.09944878\n",
      "Iteration 2127, loss = 0.09935155\n",
      "Iteration 1321, loss = 0.25916731\n",
      "Iteration 637, loss = 0.34821302\n",
      "Iteration 2128, loss = 0.09942217\n",
      "Iteration 1933, loss = 0.12956065\n",
      "Iteration 1322, loss = 0.25907231\n",
      "Iteration 412, loss = 0.36510192\n",
      "Iteration 2129, loss = 0.09924697\n",
      "Iteration 1708, loss = 0.22889035\n",
      "Iteration 638, loss = 0.34810068\n",
      "Iteration 790, loss = 0.32912400\n",
      "Iteration 413, loss = 0.36502154\n",
      "Iteration 1709, loss = 0.22876564\n",
      "Iteration 639, loss = 0.34799706\n",
      "Iteration 1608, loss = 0.17473908\n",
      "Iteration 1934, loss = 0.12942076\n",
      "Iteration 2130, loss = 0.09913072\n",
      "Iteration 1935, loss = 0.12937659\n",
      "Iteration 791, loss = 0.32895617\n",
      "Iteration 1710, loss = 0.22874058\n",
      "Iteration 2131, loss = 0.09902353\n",
      "Iteration 1323, loss = 0.25895778\n",
      "Iteration 2132, loss = 0.09892868\n",
      "Iteration 1936, loss = 0.12932244\n",
      "Iteration 2133, loss = 0.09904180\n",
      "Iteration 1609, loss = 0.17462061\n",
      "Iteration 1324, loss = 0.25886890\n",
      "Iteration 1937, loss = 0.12918988\n",
      "Iteration 792, loss = 0.32885842\n",
      "Iteration 414, loss = 0.36481326\n",
      "Iteration 640, loss = 0.34791638\n",
      "Iteration 2134, loss = 0.09874397\n",
      "Iteration 2135, loss = 0.09861138\n",
      "Iteration 415, loss = 0.36467385\n",
      "Iteration 1711, loss = 0.22858341\n",
      "Iteration 1938, loss = 0.12907488\n",
      "Iteration 1325, loss = 0.25875274\n",
      "Iteration 641, loss = 0.34777785\n",
      "Iteration 2136, loss = 0.09848797\n",
      "Iteration 1939, loss = 0.12915290\n",
      "Iteration 1326, loss = 0.25863419\n",
      "Iteration 2137, loss = 0.09843140\n",
      "Iteration 1327, loss = 0.25856362\n",
      "Iteration 1712, loss = 0.22848566\n",
      "Iteration 1610, loss = 0.17459290\n",
      "Iteration 416, loss = 0.36449960\n",
      "Iteration 793, loss = 0.32868705\n",
      "Iteration 1328, loss = 0.25844140\n",
      "Iteration 2138, loss = 0.09841990\n",
      "Iteration 1940, loss = 0.12889389\n",
      "Iteration 2139, loss = 0.09825628\n",
      "Iteration 1329, loss = 0.25831931\n",
      "Iteration 2140, loss = 0.09818819\n",
      "Iteration 1330, loss = 0.25821861\n",
      "Iteration 2141, loss = 0.09827988\n",
      "Iteration 1941, loss = 0.12890188\n",
      "Iteration 417, loss = 0.36436767\n",
      "Iteration 642, loss = 0.34768452\n",
      "Iteration 1331, loss = 0.25809535\n",
      "Iteration 1713, loss = 0.22833990\n",
      "Iteration 2142, loss = 0.09795792\n",
      "Iteration 1942, loss = 0.12886906\n",
      "Iteration 1611, loss = 0.17455672\n",
      "Iteration 1332, loss = 0.25800526\n",
      "Iteration 2143, loss = 0.09782003\n",
      "Iteration 418, loss = 0.36421506\n",
      "Iteration 1943, loss = 0.12863327\n",
      "Iteration 1714, loss = 0.22829131\n",
      "Iteration 794, loss = 0.32850111\n",
      "Iteration 2144, loss = 0.09775493\n",
      "Iteration 643, loss = 0.34758612\n",
      "Iteration 1333, loss = 0.25791072\n",
      "Iteration 1944, loss = 0.12859714\n",
      "Iteration 1334, loss = 0.25775924\n",
      "Iteration 2145, loss = 0.09766836\n",
      "Iteration 1715, loss = 0.22814625\n",
      "Iteration 1335, loss = 0.25766504\n",
      "Iteration 419, loss = 0.36405692\n",
      "Iteration 644, loss = 0.34750371\n",
      "Iteration 1945, loss = 0.12849490\n",
      "Iteration 1336, loss = 0.25752370\n",
      "Iteration 1612, loss = 0.17425801\n",
      "Iteration 1716, loss = 0.22803370\n",
      "Iteration 420, loss = 0.36394215\n",
      "Iteration 1946, loss = 0.12842197\n",
      "Iteration 795, loss = 0.32829861\n",
      "Iteration 2146, loss = 0.09767164\n",
      "Iteration 1717, loss = 0.22795713\n",
      "Iteration 421, loss = 0.36381823\n",
      "Iteration 2147, loss = 0.09748068\n",
      "Iteration 2148, loss = 0.09750395\n",
      "Iteration 1613, loss = 0.17412123\n",
      "Iteration 2149, loss = 0.09740688\n",
      "Iteration 1337, loss = 0.25745187\n",
      "Iteration 422, loss = 0.36362510\n",
      "Iteration 1718, loss = 0.22783441\n",
      "Iteration 2150, loss = 0.09743871\n",
      "Iteration 2151, loss = 0.09718483\n",
      "Iteration 1338, loss = 0.25732270\n",
      "Iteration 1947, loss = 0.12830708\n",
      "Iteration 423, loss = 0.36347673\n",
      "Iteration 1339, loss = 0.25720066\n",
      "Iteration 2152, loss = 0.09704021\n",
      "Iteration 1719, loss = 0.22775599\n",
      "Iteration 2153, loss = 0.09709731\n",
      "Iteration 424, loss = 0.36336701\n",
      "Iteration 645, loss = 0.34741754\n",
      "Iteration 1340, loss = 0.25712450\n",
      "Iteration 2154, loss = 0.09696182\n",
      "Iteration 1948, loss = 0.12840349\n",
      "Iteration 425, loss = 0.36320858\n",
      "Iteration 1614, loss = 0.17407646\n",
      "Iteration 1341, loss = 0.25704006\n",
      "Iteration 796, loss = 0.32817758Iteration 1720, loss = 0.22761420\n",
      "Iteration 646, loss = 0.34729111\n",
      "\n",
      "Iteration 1342, loss = 0.25683528\n",
      "Iteration 2155, loss = 0.09686574\n",
      "Iteration 1949, loss = 0.12817172\n",
      "Iteration 1343, loss = 0.25676615\n",
      "Iteration 647, loss = 0.34723455\n",
      "Iteration 2156, loss = 0.09664507\n",
      "Iteration 1344, loss = 0.25664593\n",
      "Iteration 426, loss = 0.36303565\n",
      "Iteration 2157, loss = 0.09663841\n",
      "Iteration 1345, loss = 0.25652469\n",
      "Iteration 1950, loss = 0.12802510\n",
      "Iteration 427, loss = 0.36295171\n",
      "Iteration 1615, loss = 0.17406932\n",
      "Iteration 2158, loss = 0.09652518\n",
      "Iteration 648, loss = 0.34709846\n",
      "Iteration 1346, loss = 0.25639918\n",
      "Iteration 428, loss = 0.36278656\n",
      "Iteration 797, loss = 0.32793561\n",
      "Iteration 2159, loss = 0.09644182\n",
      "Iteration 429, loss = 0.36266666\n",
      "Iteration 1721, loss = 0.22752947\n",
      "Iteration 1347, loss = 0.25630479\n",
      "Iteration 2160, loss = 0.09653684\n",
      "Iteration 1951, loss = 0.12802435\n",
      "Iteration 649, loss = 0.34702813\n",
      "Iteration 1348, loss = 0.25618182\n",
      "Iteration 1722, loss = 0.22744748\n",
      "Iteration 1349, loss = 0.25610185\n",
      "Iteration 2161, loss = 0.09621638\n",
      "Iteration 430, loss = 0.36251086\n",
      "Iteration 1616, loss = 0.17382398\n",
      "Iteration 1350, loss = 0.25596356\n",
      "Iteration 650, loss = 0.34689598\n",
      "Iteration 1351, loss = 0.25585929\n",
      "Iteration 431, loss = 0.36235879\n",
      "Iteration 2162, loss = 0.09615842\n",
      "Iteration 651, loss = 0.34681503\n",
      "Iteration 2163, loss = 0.09606289\n",
      "Iteration 1952, loss = 0.12786630\n",
      "Iteration 2164, loss = 0.09612415\n",
      "Iteration 1352, loss = 0.25579229\n",
      "Iteration 798, loss = 0.32781724\n",
      "Iteration 1353, loss = 0.25569117\n",
      "Iteration 2165, loss = 0.09583085\n",
      "Iteration 432, loss = 0.36227550\n",
      "Iteration 1723, loss = 0.22734605\n",
      "Iteration 1617, loss = 0.17374892\n",
      "Iteration 652, loss = 0.34670430\n",
      "Iteration 1354, loss = 0.25555625\n",
      "Iteration 2166, loss = 0.09572246\n",
      "Iteration 1953, loss = 0.12780116\n",
      "Iteration 433, loss = 0.36207312\n",
      "Iteration 799, loss = 0.32761350\n",
      "Iteration 2167, loss = 0.09578696\n",
      "Iteration 1954, loss = 0.12780065\n",
      "Iteration 653, loss = 0.34662072\n",
      "Iteration 1355, loss = 0.25540918\n",
      "Iteration 2168, loss = 0.09560431\n",
      "Iteration 1955, loss = 0.12765770\n",
      "Iteration 2169, loss = 0.09560385\n",
      "Iteration 800, loss = 0.32746935\n",
      "Iteration 434, loss = 0.36193188\n",
      "Iteration 2170, loss = 0.09540429\n",
      "Iteration 1618, loss = 0.17361100\n",
      "Iteration 1724, loss = 0.22724987\n",
      "Iteration 654, loss = 0.34648750\n",
      "Iteration 1356, loss = 0.25531378\n",
      "Iteration 2171, loss = 0.09532867\n",
      "Iteration 1956, loss = 0.12753489\n",
      "Iteration 2172, loss = 0.09526757\n",
      "Iteration 1357, loss = 0.25526386\n",
      "Iteration 2173, loss = 0.09509936\n",
      "Iteration 435, loss = 0.36178350\n",
      "Iteration 655, loss = 0.34639529\n",
      "Iteration 1957, loss = 0.12754544\n",
      "Iteration 1358, loss = 0.25506580\n",
      "Iteration 1725, loss = 0.22720408\n",
      "Iteration 2174, loss = 0.09499203\n",
      "Iteration 1619, loss = 0.17351296\n",
      "Iteration 2175, loss = 0.09493383\n",
      "Iteration 801, loss = 0.32727621\n",
      "Iteration 1958, loss = 0.12756499Iteration 1726, loss = 0.22700757\n",
      "\n",
      "Iteration 436, loss = 0.36172772\n",
      "Iteration 1359, loss = 0.25494162\n",
      "Iteration 2176, loss = 0.09482553\n",
      "Iteration 2177, loss = 0.09474065\n",
      "Iteration 656, loss = 0.34629859\n",
      "Iteration 1620, loss = 0.17344400\n",
      "Iteration 1959, loss = 0.12745233\n",
      "Iteration 1360, loss = 0.25490864\n",
      "Iteration 802, loss = 0.32708189\n",
      "Iteration 437, loss = 0.36152239\n",
      "Iteration 2178, loss = 0.09466982\n",
      "Iteration 1960, loss = 0.12723385\n",
      "Iteration 1361, loss = 0.25484816\n",
      "Iteration 2179, loss = 0.09458985\n",
      "Iteration 2180, loss = 0.09447905\n",
      "Iteration 1362, loss = 0.25463389\n",
      "Iteration 803, loss = 0.32696711\n",
      "Iteration 2181, loss = 0.09438558\n",
      "Iteration 1727, loss = 0.22694678\n",
      "Iteration 438, loss = 0.36137502Iteration 1363, loss = 0.25450710\n",
      "\n",
      "Iteration 1961, loss = 0.12710401\n",
      "Iteration 2182, loss = 0.09430037\n",
      "Iteration 2183, loss = 0.09423129\n",
      "Iteration 1621, loss = 0.17331387\n",
      "Iteration 1364, loss = 0.25439297\n",
      "Iteration 439, loss = 0.36133164\n",
      "Iteration 2184, loss = 0.09410865\n",
      "Iteration 657, loss = 0.34621249\n",
      "Iteration 2185, loss = 0.09400321\n",
      "Iteration 2186, loss = 0.09391561\n",
      "Iteration 1365, loss = 0.25429647\n",
      "Iteration 804, loss = 0.32675136\n",
      "Iteration 2187, loss = 0.09413766\n",
      "Iteration 1728, loss = 0.22682161\n",
      "Iteration 2188, loss = 0.09379472\n",
      "Iteration 1366, loss = 0.25416138\n",
      "Iteration 658, loss = 0.34613294\n",
      "Iteration 2189, loss = 0.09376762\n",
      "Iteration 2190, loss = 0.09358853\n",
      "Iteration 1962, loss = 0.12708905\n",
      "Iteration 1367, loss = 0.25409461\n",
      "Iteration 659, loss = 0.34605063\n",
      "Iteration 440, loss = 0.36109785\n",
      "Iteration 2191, loss = 0.09363742\n",
      "Iteration 1368, loss = 0.25397016\n",
      "Iteration 2192, loss = 0.09342570\n",
      "Iteration 1729, loss = 0.22672779\n",
      "Iteration 2193, loss = 0.09335651\n",
      "Iteration 1369, loss = 0.25383488\n",
      "Iteration 2194, loss = 0.09320930\n",
      "Iteration 805, loss = 0.32665141\n",
      "Iteration 441, loss = 0.36094746\n",
      "Iteration 1370, loss = 0.25377003\n",
      "Iteration 660, loss = 0.34592180\n",
      "Iteration 1730, loss = 0.22660703\n",
      "Iteration 2195, loss = 0.09317421\n",
      "Iteration 1622, loss = 0.17324786\n",
      "Iteration 1963, loss = 0.12693091\n",
      "Iteration 2196, loss = 0.09311338\n",
      "Iteration 442, loss = 0.36080869\n",
      "Iteration 1731, loss = 0.22656792\n",
      "Iteration 2197, loss = 0.09300063\n",
      "Iteration 2198, loss = 0.09303362\n",
      "Iteration 1371, loss = 0.25368008\n",
      "Iteration 443, loss = 0.36068769\n",
      "Iteration 1964, loss = 0.12687082\n",
      "Iteration 661, loss = 0.34582163\n",
      "Iteration 1732, loss = 0.22637680\n",
      "Iteration 1372, loss = 0.25361307\n",
      "Iteration 806, loss = 0.32644748\n",
      "Iteration 444, loss = 0.36053964\n",
      "Iteration 2199, loss = 0.09294885\n",
      "Iteration 2200, loss = 0.09267831\n",
      "Iteration 1733, loss = 0.22635530\n",
      "Iteration 2201, loss = 0.09264796\n",
      "Iteration 662, loss = 0.34572262\n",
      "Iteration 1373, loss = 0.25347816\n",
      "Iteration 2202, loss = 0.09261409\n",
      "Iteration 1965, loss = 0.12682911\n",
      "Iteration 1623, loss = 0.17330811\n",
      "Iteration 445, loss = 0.36041993\n",
      "Iteration 807, loss = 0.32627038\n",
      "Iteration 1734, loss = 0.22619785\n",
      "Iteration 2203, loss = 0.09244019\n",
      "Iteration 1966, loss = 0.12669448\n",
      "Iteration 2204, loss = 0.09236118\n",
      "Iteration 446, loss = 0.36026304\n",
      "Iteration 1374, loss = 0.25338019\n",
      "Iteration 2205, loss = 0.09229075\n",
      "Iteration 1624, loss = 0.17296654\n",
      "Iteration 663, loss = 0.34561583\n",
      "Iteration 2206, loss = 0.09234351\n",
      "Iteration 1375, loss = 0.25314288\n",
      "Iteration 2207, loss = 0.09217184\n",
      "Iteration 2208, loss = 0.09201951\n",
      "Iteration 1967, loss = 0.12665322\n",
      "Iteration 1376, loss = 0.25304220\n",
      "Iteration 808, loss = 0.32611814\n",
      "Iteration 2209, loss = 0.09196998\n",
      "Iteration 664, loss = 0.34554591\n",
      "Iteration 2210, loss = 0.09186958\n",
      "Iteration 1735, loss = 0.22608529\n",
      "Iteration 1625, loss = 0.17284775\n",
      "Iteration 1377, loss = 0.25293922\n",
      "Iteration 447, loss = 0.36009198\n",
      "Iteration 2211, loss = 0.09180587Iteration 1968, loss = 0.12664735\n",
      "Iteration 665, loss = 0.34543671\n",
      "Iteration 1736, loss = 0.22600089\n",
      "\n",
      "Iteration 1378, loss = 0.25279881\n",
      "Iteration 809, loss = 0.32592357\n",
      "Iteration 1969, loss = 0.12646551\n",
      "Iteration 448, loss = 0.36001709\n",
      "Iteration 2212, loss = 0.09165399\n",
      "Iteration 666, loss = 0.34531023\n",
      "Iteration 1379, loss = 0.25270613\n",
      "Iteration 2213, loss = 0.09163115\n",
      "Iteration 1380, loss = 0.25257862\n",
      "Iteration 2214, loss = 0.09156017\n",
      "Iteration 449, loss = 0.35986703\n",
      "Iteration 2215, loss = 0.09143943\n",
      "Iteration 667, loss = 0.34522910\n",
      "Iteration 2216, loss = 0.09140448\n",
      "Iteration 1626, loss = 0.17280032\n",
      "Iteration 2217, loss = 0.09127220\n",
      "Iteration 450, loss = 0.35966918\n",
      "Iteration 2218, loss = 0.09122513\n",
      "Iteration 1381, loss = 0.25245421\n",
      "Iteration 1970, loss = 0.12650888\n",
      "Iteration 810, loss = 0.32576475\n",
      "Iteration 1737, loss = 0.22597126\n",
      "Iteration 668, loss = 0.34513412\n",
      "Iteration 2219, loss = 0.09111830\n",
      "Iteration 1382, loss = 0.25234333\n",
      "Iteration 2220, loss = 0.09108694\n",
      "Iteration 2221, loss = 0.09092889\n",
      "Iteration 1383, loss = 0.25229596\n",
      "Iteration 1627, loss = 0.17282344\n",
      "Iteration 2222, loss = 0.09089985\n",
      "Iteration 669, loss = 0.34508825\n",
      "Iteration 1971, loss = 0.12649692\n",
      "Iteration 451, loss = 0.35955966\n",
      "Iteration 2223, loss = 0.09075301\n",
      "Iteration 1384, loss = 0.25216084\n",
      "Iteration 2224, loss = 0.09080100\n",
      "Iteration 2225, loss = 0.09065043\n",
      "Iteration 670, loss = 0.34505857\n",
      "Iteration 811, loss = 0.32557954\n",
      "Iteration 2226, loss = 0.09057873\n",
      "Iteration 1972, loss = 0.12621071\n",
      "Iteration 1385, loss = 0.25205735\n",
      "Iteration 2227, loss = 0.09048233\n",
      "Iteration 1738, loss = 0.22584106\n",
      "Iteration 2228, loss = 0.09046279\n",
      "Iteration 1973, loss = 0.12628854\n",
      "Iteration 671, loss = 0.34484588\n",
      "Iteration 2229, loss = 0.09028458\n",
      "Iteration 1386, loss = 0.25189621\n",
      "Iteration 2230, loss = 0.09026765\n",
      "Iteration 1387, loss = 0.25183618\n",
      "Iteration 452, loss = 0.35942576\n",
      "Iteration 2231, loss = 0.09020070\n",
      "Iteration 1628, loss = 0.17265829\n",
      "Iteration 672, loss = 0.34472098\n",
      "Iteration 2232, loss = 0.09010686\n",
      "Iteration 1974, loss = 0.12623947\n",
      "Iteration 2233, loss = 0.09008277\n",
      "Iteration 1388, loss = 0.25169064\n",
      "Iteration 453, loss = 0.35925277\n",
      "Iteration 1739, loss = 0.22571824\n",
      "Iteration 2234, loss = 0.08990867\n",
      "Iteration 1389, loss = 0.25155586\n",
      "Iteration 673, loss = 0.34466340\n",
      "Iteration 812, loss = 0.32543930\n",
      "Iteration 2235, loss = 0.08986705\n",
      "Iteration 1975, loss = 0.12597710\n",
      "Iteration 1740, loss = 0.22559772\n",
      "Iteration 2236, loss = 0.08986351\n",
      "Iteration 2237, loss = 0.09004599\n",
      "Iteration 454, loss = 0.35913511\n",
      "Iteration 1390, loss = 0.25147540Iteration 1976, loss = 0.12635218\n",
      "\n",
      "Iteration 674, loss = 0.34457854\n",
      "Iteration 455, loss = 0.35896472\n",
      "Iteration 2238, loss = 0.08967989\n",
      "Iteration 1977, loss = 0.12584743\n",
      "Iteration 813, loss = 0.32534384\n",
      "Iteration 2239, loss = 0.08948017\n",
      "Iteration 675, loss = 0.34447465\n",
      "Iteration 1391, loss = 0.25135663\n",
      "Iteration 1741, loss = 0.22560245\n",
      "Iteration 456, loss = 0.35886709\n",
      "Iteration 1978, loss = 0.12579249\n",
      "Iteration 2240, loss = 0.08950632\n",
      "Iteration 457, loss = 0.35871104\n",
      "Iteration 1392, loss = 0.25129290\n",
      "Iteration 1979, loss = 0.12592419\n",
      "Iteration 1629, loss = 0.17244277\n",
      "Iteration 2241, loss = 0.08937388\n",
      "Iteration 1393, loss = 0.25111522\n",
      "Iteration 1742, loss = 0.22541261\n",
      "Iteration 1980, loss = 0.12580144\n",
      "Iteration 676, loss = 0.34433465\n",
      "Iteration 2242, loss = 0.08936598\n",
      "Iteration 1394, loss = 0.25097702\n",
      "Iteration 458, loss = 0.35861887\n",
      "Iteration 2243, loss = 0.08923518\n",
      "Iteration 1981, loss = 0.12548921\n",
      "Iteration 1395, loss = 0.25088745\n",
      "Iteration 677, loss = 0.34424569\n",
      "Iteration 2244, loss = 0.08911087\n",
      "Iteration 459, loss = 0.35843194\n",
      "Iteration 1743, loss = 0.22528329\n",
      "Iteration 814, loss = 0.32507068\n",
      "Iteration 678, loss = 0.34414815\n",
      "Iteration 460, loss = 0.35832094\n",
      "Iteration 2245, loss = 0.08914162\n",
      "Iteration 1396, loss = 0.25080332\n",
      "Iteration 1982, loss = 0.12551212\n",
      "Iteration 1397, loss = 0.25065147\n",
      "Iteration 1630, loss = 0.17236696\n",
      "Iteration 679, loss = 0.34402217\n",
      "Iteration 461, loss = 0.35814880\n",
      "Iteration 2246, loss = 0.08893610\n",
      "Iteration 1983, loss = 0.12536881\n",
      "Iteration 1398, loss = 0.25052146\n",
      "Iteration 1744, loss = 0.22519567\n",
      "Iteration 815, loss = 0.32488484\n",
      "Iteration 1631, loss = 0.17222862\n",
      "Iteration 462, loss = 0.35801177\n",
      "Iteration 2247, loss = 0.08886796\n",
      "Iteration 1399, loss = 0.25051434\n",
      "Iteration 680, loss = 0.34395816\n",
      "Iteration 2248, loss = 0.08911552\n",
      "Iteration 1984, loss = 0.12532750\n",
      "Iteration 1400, loss = 0.25029595\n",
      "Iteration 1745, loss = 0.22512033\n",
      "Iteration 463, loss = 0.35790765\n",
      "Iteration 2249, loss = 0.08875171\n",
      "Iteration 1985, loss = 0.12518882\n",
      "Iteration 816, loss = 0.32483170\n",
      "Iteration 464, loss = 0.35772363\n",
      "Iteration 1746, loss = 0.22499483\n",
      "Iteration 1632, loss = 0.17220319\n",
      "Iteration 2250, loss = 0.08878860\n",
      "Iteration 1401, loss = 0.25026348\n",
      "Iteration 681, loss = 0.34382742\n",
      "Iteration 2251, loss = 0.08856404\n",
      "Iteration 2252, loss = 0.08848214\n",
      "Iteration 1986, loss = 0.12517670\n",
      "Iteration 465, loss = 0.35759656\n",
      "Iteration 1402, loss = 0.25007400\n",
      "Iteration 2253, loss = 0.08839947\n",
      "Iteration 2254, loss = 0.08841961\n",
      "Iteration 1747, loss = 0.22489180\n",
      "Iteration 2255, loss = 0.08827696\n",
      "Iteration 817, loss = 0.32461853\n",
      "Iteration 466, loss = 0.35743837\n",
      "Iteration 1403, loss = 0.24991610\n",
      "Iteration 2256, loss = 0.08827367\n",
      "Iteration 2257, loss = 0.08818612\n",
      "Iteration 1987, loss = 0.12505248\n",
      "Iteration 467, loss = 0.35741981\n",
      "Iteration 1404, loss = 0.24983114\n",
      "Iteration 2258, loss = 0.08809876\n",
      "Iteration 468, loss = 0.35719799\n",
      "Iteration 1405, loss = 0.24972218\n",
      "Iteration 1988, loss = 0.12499408\n",
      "Iteration 1748, loss = 0.22477087\n",
      "Iteration 682, loss = 0.34372849\n",
      "Iteration 2259, loss = 0.08799388\n",
      "Iteration 1406, loss = 0.24959049\n",
      "Iteration 1633, loss = 0.17207416\n",
      "Iteration 2260, loss = 0.08792201\n",
      "Iteration 469, loss = 0.35705367\n",
      "Iteration 2261, loss = 0.08782568\n",
      "Iteration 1989, loss = 0.12494840\n",
      "Iteration 818, loss = 0.32438391\n",
      "Iteration 2262, loss = 0.08780541\n",
      "Iteration 1407, loss = 0.24945637\n",
      "Iteration 2263, loss = 0.08779376\n",
      "Iteration 2264, loss = 0.08760999\n",
      "Iteration 1634, loss = 0.17190022\n",
      "Iteration 2265, loss = 0.08758414\n",
      "Iteration 1408, loss = 0.24933531\n",
      "Iteration 2266, loss = 0.08750266\n",
      "Iteration 1749, loss = 0.22467896\n",
      "Iteration 2267, loss = 0.08741996\n",
      "Iteration 1409, loss = 0.24929377\n",
      "Iteration 2268, loss = 0.08760840\n",
      "Iteration 1635, loss = 0.17187000\n",
      "Iteration 2269, loss = 0.08729527\n",
      "Iteration 1410, loss = 0.24909150\n",
      "Iteration 470, loss = 0.35689336\n",
      "Iteration 2270, loss = 0.08745658\n",
      "Iteration 2271, loss = 0.08715669\n",
      "Iteration 683, loss = 0.34365171\n",
      "Iteration 1750, loss = 0.22456557\n",
      "Iteration 1990, loss = 0.12479272\n",
      "Iteration 2272, loss = 0.08705414\n",
      "Iteration 819, loss = 0.32426945\n",
      "Iteration 2273, loss = 0.08696989\n",
      "Iteration 471, loss = 0.35673724\n",
      "Iteration 1991, loss = 0.12477138\n",
      "Iteration 1636, loss = 0.17165805\n",
      "Iteration 472, loss = 0.35674506\n",
      "Iteration 684, loss = 0.34355658\n",
      "Iteration 1992, loss = 0.12474023\n",
      "Iteration 2274, loss = 0.08691678\n",
      "Iteration 1411, loss = 0.24896203\n",
      "Iteration 1751, loss = 0.22444695\n",
      "Iteration 1412, loss = 0.24895273\n",
      "Iteration 1993, loss = 0.12459766\n",
      "Iteration 2275, loss = 0.08687382\n",
      "Iteration 685, loss = 0.34341732\n",
      "Iteration 1413, loss = 0.24873431\n",
      "Iteration 820, loss = 0.32439529\n",
      "Iteration 473, loss = 0.35648376\n",
      "Iteration 1994, loss = 0.12448709\n",
      "Iteration 1414, loss = 0.24864691\n",
      "Iteration 686, loss = 0.34332050\n",
      "Iteration 1637, loss = 0.17156335\n",
      "Iteration 2276, loss = 0.08678740\n",
      "Iteration 1415, loss = 0.24850605\n",
      "Iteration 474, loss = 0.35639599\n",
      "Iteration 1752, loss = 0.22439785\n",
      "Iteration 1995, loss = 0.12440278\n",
      "Iteration 1416, loss = 0.24848510\n",
      "Iteration 687, loss = 0.34321977\n",
      "Iteration 2277, loss = 0.08664151\n",
      "Iteration 1753, loss = 0.22433122\n",
      "Iteration 1417, loss = 0.24828257\n",
      "Iteration 475, loss = 0.35619007\n",
      "Iteration 2278, loss = 0.08665115\n",
      "Iteration 1418, loss = 0.24815173\n",
      "Iteration 821, loss = 0.32389088\n",
      "Iteration 2279, loss = 0.08656178\n",
      "Iteration 688, loss = 0.34316702\n",
      "Iteration 1754, loss = 0.22416588\n",
      "Iteration 476, loss = 0.35604803\n",
      "Iteration 2280, loss = 0.08649446\n",
      "Iteration 1996, loss = 0.12441590\n",
      "Iteration 1419, loss = 0.24805991\n",
      "Iteration 2281, loss = 0.08639047\n",
      "Iteration 1997, loss = 0.12427568\n",
      "Iteration 822, loss = 0.32371590\n",
      "Iteration 1420, loss = 0.24791980\n",
      "Iteration 689, loss = 0.34302781\n",
      "Iteration 1638, loss = 0.17147547\n",
      "Iteration 2282, loss = 0.08635159\n",
      "Iteration 477, loss = 0.35590028\n",
      "Iteration 1755, loss = 0.22419344\n",
      "Iteration 1998, loss = 0.12419641\n",
      "Iteration 478, loss = 0.35578871\n",
      "Iteration 1639, loss = 0.17150763\n",
      "Iteration 1421, loss = 0.24780695\n",
      "Iteration 479, loss = 0.35563829\n",
      "Iteration 2283, loss = 0.08628243\n",
      "Iteration 690, loss = 0.34291093\n",
      "Iteration 1756, loss = 0.22399755\n",
      "Iteration 1422, loss = 0.24767204\n",
      "Iteration 2284, loss = 0.08640881\n",
      "Iteration 823, loss = 0.32356896\n",
      "Iteration 2285, loss = 0.08614631\n",
      "Iteration 1423, loss = 0.24753804\n",
      "Iteration 1999, loss = 0.12417215\n",
      "Iteration 2286, loss = 0.08607746\n",
      "Iteration 1424, loss = 0.24744695\n",
      "Iteration 691, loss = 0.34285983\n",
      "Iteration 1640, loss = 0.17133267\n",
      "Iteration 2287, loss = 0.08608436\n",
      "Iteration 1757, loss = 0.22390135\n",
      "Iteration 480, loss = 0.35551389\n",
      "Iteration 2288, loss = 0.08592047\n",
      "Iteration 1425, loss = 0.24736697\n",
      "Iteration 2000, loss = 0.12408309\n",
      "Iteration 2289, loss = 0.08595969\n",
      "Iteration 824, loss = 0.32336084\n",
      "Iteration 2290, loss = 0.08577931\n",
      "Iteration 2291, loss = 0.08577665\n",
      "Iteration 1641, loss = 0.17133130\n",
      "Iteration 1426, loss = 0.24722793\n",
      "Iteration 481, loss = 0.35534415\n",
      "Iteration 2292, loss = 0.08572386\n",
      "Iteration 1758, loss = 0.22390545\n",
      "Iteration 482, loss = 0.35523190\n",
      "Iteration 1427, loss = 0.24716015\n",
      "Iteration 1428, loss = 0.24704797\n",
      "Iteration 483, loss = 0.35507143\n",
      "Iteration 1642, loss = 0.17118924\n",
      "Iteration 1759, loss = 0.22369928\n",
      "Iteration 1429, loss = 0.24692582\n",
      "Iteration 2001, loss = 0.12403811\n",
      "Iteration 484, loss = 0.35494316\n",
      "Iteration 1430, loss = 0.24677068\n",
      "Iteration 1760, loss = 0.22358442\n",
      "Iteration 1431, loss = 0.24661742\n",
      "Iteration 485, loss = 0.35479291\n",
      "Iteration 825, loss = 0.32320930\n",
      "Iteration 1761, loss = 0.22348930\n",
      "Iteration 2002, loss = 0.12390969\n",
      "Iteration 1432, loss = 0.24655428\n",
      "Iteration 692, loss = 0.34271727\n",
      "Iteration 2293, loss = 0.08553746\n",
      "Iteration 1643, loss = 0.17137150\n",
      "Iteration 2294, loss = 0.08548321\n",
      "Iteration 2003, loss = 0.12383602\n",
      "Iteration 693, loss = 0.34260770\n",
      "Iteration 486, loss = 0.35467370Iteration 2295, loss = 0.08542012\n",
      "\n",
      "Iteration 1433, loss = 0.24643678\n",
      "Iteration 1644, loss = 0.17093733\n",
      "Iteration 2004, loss = 0.12381800\n",
      "Iteration 1434, loss = 0.24627736\n",
      "Iteration 2296, loss = 0.08549329\n",
      "Iteration 694, loss = 0.34251075\n",
      "Iteration 2297, loss = 0.08529331\n",
      "Iteration 826, loss = 0.32299808\n",
      "Iteration 487, loss = 0.35455668\n",
      "Iteration 2298, loss = 0.08522793\n",
      "Iteration 2299, loss = 0.08524174\n",
      "Iteration 1435, loss = 0.24619922\n",
      "Iteration 2300, loss = 0.08519935\n",
      "Iteration 1436, loss = 0.24608070\n",
      "Iteration 695, loss = 0.34240146\n",
      "Iteration 2005, loss = 0.12368816\n",
      "Iteration 1762, loss = 0.22346633\n",
      "Iteration 488, loss = 0.35442666\n",
      "Iteration 827, loss = 0.32286613\n",
      "Iteration 2301, loss = 0.08502221\n",
      "Iteration 1645, loss = 0.17082628\n",
      "Iteration 2302, loss = 0.08504055\n",
      "Iteration 489, loss = 0.35423392\n",
      "Iteration 696, loss = 0.34229855\n",
      "Iteration 2303, loss = 0.08490871\n",
      "Iteration 1437, loss = 0.24594432\n",
      "Iteration 2006, loss = 0.12376133\n",
      "Iteration 828, loss = 0.32268441\n",
      "Iteration 2304, loss = 0.08484940\n",
      "Iteration 2305, loss = 0.08486366\n",
      "Iteration 2306, loss = 0.08464331\n",
      "Iteration 1438, loss = 0.24581448\n",
      "Iteration 2007, loss = 0.12359324\n",
      "Iteration 490, loss = 0.35433755\n",
      "Iteration 2307, loss = 0.08469324\n",
      "Iteration 1763, loss = 0.22329500\n",
      "Iteration 1439, loss = 0.24569152\n",
      "Iteration 2308, loss = 0.08453875\n",
      "Iteration 1646, loss = 0.17065228\n",
      "Iteration 491, loss = 0.35399371\n",
      "Iteration 829, loss = 0.32259660\n",
      "Iteration 1440, loss = 0.24560854\n",
      "Iteration 697, loss = 0.34219990\n",
      "Iteration 2309, loss = 0.08448293\n",
      "Iteration 2008, loss = 0.12359946\n",
      "Iteration 492, loss = 0.35393536\n",
      "Iteration 2310, loss = 0.08441441\n",
      "Iteration 2311, loss = 0.08441151\n",
      "Iteration 1441, loss = 0.24551257\n",
      "Iteration 1764, loss = 0.22325292\n",
      "Iteration 2009, loss = 0.12339390\n",
      "Iteration 2312, loss = 0.08446314\n",
      "Iteration 1442, loss = 0.24539910\n",
      "Iteration 1443, loss = 0.24521870\n",
      "Iteration 2313, loss = 0.08426830\n",
      "Iteration 493, loss = 0.35368037\n",
      "Iteration 1647, loss = 0.17056396\n",
      "Iteration 830, loss = 0.32249098\n",
      "Iteration 698, loss = 0.34210307\n",
      "Iteration 2314, loss = 0.08422753\n",
      "Iteration 2010, loss = 0.12335005\n",
      "Iteration 1444, loss = 0.24508367\n",
      "Iteration 1765, loss = 0.22306457\n",
      "Iteration 2315, loss = 0.08409744Iteration 1445, loss = 0.24498974\n",
      "Iteration 2011, loss = 0.12348300\n",
      "\n",
      "Iteration 1446, loss = 0.24491137\n",
      "Iteration 494, loss = 0.35358548\n",
      "Iteration 1648, loss = 0.17048320\n",
      "Iteration 2316, loss = 0.08410069\n",
      "Iteration 2012, loss = 0.12326636\n",
      "Iteration 2317, loss = 0.08407535\n",
      "Iteration 1447, loss = 0.24476887\n",
      "Iteration 1766, loss = 0.22297943\n",
      "Iteration 2318, loss = 0.08402500\n",
      "Iteration 831, loss = 0.32220193\n",
      "Iteration 1767, loss = 0.22286094\n",
      "Iteration 699, loss = 0.34198417\n",
      "Iteration 2319, loss = 0.08388498\n",
      "Iteration 495, loss = 0.35344694\n",
      "Iteration 1649, loss = 0.17035215\n",
      "Iteration 2013, loss = 0.12327461\n",
      "Iteration 2320, loss = 0.08382248\n",
      "Iteration 1448, loss = 0.24462364\n",
      "Iteration 832, loss = 0.32201134\n",
      "Iteration 2321, loss = 0.08374380\n",
      "Iteration 2322, loss = 0.08383096\n",
      "Iteration 2014, loss = 0.12303238\n",
      "Iteration 2323, loss = 0.08365152\n",
      "Iteration 2324, loss = 0.08355801\n",
      "Iteration 1449, loss = 0.24452333\n",
      "Iteration 496, loss = 0.35330250\n",
      "Iteration 2325, loss = 0.08345863\n",
      "Iteration 2015, loss = 0.12300672\n",
      "Iteration 1650, loss = 0.17023862\n",
      "Iteration 2326, loss = 0.08356864\n",
      "Iteration 1450, loss = 0.24442158\n",
      "Iteration 700, loss = 0.34189392\n",
      "Iteration 1768, loss = 0.22277601\n",
      "Iteration 2327, loss = 0.08334745\n",
      "Iteration 2328, loss = 0.08327868\n",
      "Iteration 2329, loss = 0.08321967\n",
      "Iteration 1451, loss = 0.24427613\n",
      "Iteration 833, loss = 0.32182303\n",
      "Iteration 1769, loss = 0.22272119\n",
      "Iteration 2330, loss = 0.08316788\n",
      "Iteration 701, loss = 0.34179278\n",
      "Iteration 2016, loss = 0.12300395\n",
      "Iteration 2331, loss = 0.08319542\n",
      "Iteration 497, loss = 0.35314859\n",
      "Iteration 2332, loss = 0.08306252\n",
      "Iteration 1651, loss = 0.17010835\n",
      "Iteration 1452, loss = 0.24418580\n",
      "Iteration 2333, loss = 0.08300703\n",
      "Iteration 2334, loss = 0.08306536\n",
      "Iteration 702, loss = 0.34171053\n",
      "Iteration 2335, loss = 0.08285354\n",
      "Iteration 1770, loss = 0.22257967\n",
      "Iteration 498, loss = 0.35302200\n",
      "Iteration 2017, loss = 0.12282077\n",
      "Iteration 1453, loss = 0.24405668\n",
      "Iteration 834, loss = 0.32165777\n",
      "Iteration 2336, loss = 0.08285830\n",
      "Iteration 1454, loss = 0.24393411\n",
      "Iteration 499, loss = 0.35294707\n",
      "Iteration 2337, loss = 0.08274267\n",
      "Iteration 703, loss = 0.34158567\n",
      "Iteration 1455, loss = 0.24385318\n",
      "Iteration 2338, loss = 0.08271349\n",
      "Iteration 1771, loss = 0.22252775\n",
      "Iteration 2018, loss = 0.12277154\n",
      "Iteration 2339, loss = 0.08264161\n",
      "Iteration 500, loss = 0.35274286\n",
      "Iteration 2340, loss = 0.08261187\n",
      "Iteration 1652, loss = 0.16999711\n",
      "Iteration 704, loss = 0.34148441\n",
      "Iteration 2341, loss = 0.08251585\n",
      "Iteration 1456, loss = 0.24370380\n",
      "Iteration 2342, loss = 0.08245772\n",
      "Iteration 1457, loss = 0.24361148\n",
      "Iteration 501, loss = 0.35262328\n",
      "Iteration 2343, loss = 0.08238301\n",
      "Iteration 835, loss = 0.32170322\n",
      "Iteration 2344, loss = 0.08229873\n",
      "Iteration 1458, loss = 0.24356460\n",
      "Iteration 2345, loss = 0.08228997\n",
      "Iteration 1653, loss = 0.16992542\n",
      "Iteration 2019, loss = 0.12270718\n",
      "Iteration 2346, loss = 0.08215714\n",
      "Iteration 1459, loss = 0.24343468\n",
      "Iteration 705, loss = 0.34137317\n",
      "Iteration 2347, loss = 0.08211028\n",
      "Iteration 1772, loss = 0.22236774\n",
      "Iteration 502, loss = 0.35247141\n",
      "Iteration 2348, loss = 0.08207247\n",
      "Iteration 1654, loss = 0.16982021\n",
      "Iteration 2349, loss = 0.08197301\n",
      "Iteration 1460, loss = 0.24329050\n",
      "Iteration 503, loss = 0.35239403\n",
      "Iteration 1773, loss = 0.22226891\n",
      "Iteration 2020, loss = 0.12260244\n",
      "Iteration 2021, loss = 0.12252870\n",
      "Iteration 2350, loss = 0.08207071\n",
      "Iteration 1461, loss = 0.24311862\n",
      "Iteration 504, loss = 0.35218712\n",
      "Iteration 2351, loss = 0.08192912\n",
      "Iteration 836, loss = 0.32133046\n",
      "Iteration 1655, loss = 0.16980272\n",
      "Iteration 706, loss = 0.34134683\n",
      "Iteration 2352, loss = 0.08183409\n",
      "Iteration 1774, loss = 0.22216386\n",
      "Iteration 2353, loss = 0.08179607\n",
      "Iteration 2022, loss = 0.12248770\n",
      "Iteration 1462, loss = 0.24299483\n",
      "Iteration 2354, loss = 0.08170066\n",
      "Iteration 707, loss = 0.34119069\n",
      "Iteration 1656, loss = 0.16966431\n",
      "Iteration 1463, loss = 0.24290135\n",
      "Iteration 2023, loss = 0.12255608\n",
      "Iteration 2024, loss = 0.12237207\n",
      "Iteration 505, loss = 0.35215164\n",
      "Iteration 1657, loss = 0.16955093\n",
      "Iteration 1775, loss = 0.22211942\n",
      "Iteration 2355, loss = 0.08167167Iteration 708, loss = 0.34109020\n",
      "\n",
      "Iteration 506, loss = 0.35194225\n",
      "Iteration 2356, loss = 0.08170162\n",
      "Iteration 1776, loss = 0.22201913\n",
      "Iteration 507, loss = 0.35176225\n",
      "Iteration 2357, loss = 0.08154078\n",
      "Iteration 837, loss = 0.32113763\n",
      "Iteration 1464, loss = 0.24274736\n",
      "Iteration 2358, loss = 0.08149996\n",
      "Iteration 508, loss = 0.35165512\n",
      "Iteration 1465, loss = 0.24264024\n",
      "Iteration 1777, loss = 0.22189557\n",
      "Iteration 2025, loss = 0.12231457\n",
      "Iteration 709, loss = 0.34100255\n",
      "Iteration 2359, loss = 0.08152445\n",
      "Iteration 2360, loss = 0.08146675\n",
      "Iteration 1466, loss = 0.24254521\n",
      "Iteration 509, loss = 0.35159022\n",
      "Iteration 2361, loss = 0.08141015\n",
      "Iteration 838, loss = 0.32098970\n",
      "Iteration 2362, loss = 0.08130445\n",
      "Iteration 1467, loss = 0.24244218\n",
      "Iteration 510, loss = 0.35138923\n",
      "Iteration 2363, loss = 0.08122839\n",
      "Iteration 1778, loss = 0.22180222\n",
      "Iteration 1468, loss = 0.24236979\n",
      "Iteration 2364, loss = 0.08121525\n",
      "Iteration 2026, loss = 0.12219195\n",
      "Iteration 1469, loss = 0.24228406\n",
      "Iteration 839, loss = 0.32089954\n",
      "Iteration 511, loss = 0.35129977\n",
      "Iteration 2365, loss = 0.08108166\n",
      "Iteration 1470, loss = 0.24213347\n",
      "Iteration 2366, loss = 0.08114237\n",
      "Iteration 1471, loss = 0.24193244\n",
      "Iteration 710, loss = 0.34086884\n",
      "Iteration 2027, loss = 0.12209801\n",
      "Iteration 512, loss = 0.35117018\n",
      "Iteration 1472, loss = 0.24183308\n",
      "Iteration 2367, loss = 0.08105783\n",
      "Iteration 2368, loss = 0.08094601\n",
      "Iteration 1473, loss = 0.24180171\n",
      "Iteration 2369, loss = 0.08083913\n",
      "Iteration 1658, loss = 0.16951797\n",
      "Iteration 711, loss = 0.34077626\n",
      "Iteration 2370, loss = 0.08097984\n",
      "Iteration 1779, loss = 0.22170562\n",
      "Iteration 2028, loss = 0.12209287\n",
      "Iteration 2371, loss = 0.08069723\n",
      "Iteration 840, loss = 0.32070584\n",
      "Iteration 712, loss = 0.34068570\n",
      "Iteration 2372, loss = 0.08070748\n",
      "Iteration 1659, loss = 0.16938178\n",
      "Iteration 1474, loss = 0.24163794\n",
      "Iteration 2373, loss = 0.08060233\n",
      "Iteration 2029, loss = 0.12200797\n",
      "Iteration 513, loss = 0.35096201\n",
      "Iteration 2374, loss = 0.08061672\n",
      "Iteration 1780, loss = 0.22155435\n",
      "Iteration 2375, loss = 0.08059748\n",
      "Iteration 2376, loss = 0.08047572\n",
      "Iteration 514, loss = 0.35084807\n",
      "Iteration 841, loss = 0.32047046\n",
      "Iteration 2377, loss = 0.08039503Iteration 1475, loss = 0.24159450\n",
      "\n",
      "Iteration 1660, loss = 0.16920688\n",
      "Iteration 1476, loss = 0.24138871\n",
      "Iteration 2030, loss = 0.12191642\n",
      "Iteration 713, loss = 0.34057083\n",
      "Iteration 2378, loss = 0.08036374\n",
      "Iteration 515, loss = 0.35067901\n",
      "Iteration 2379, loss = 0.08028085\n",
      "Iteration 1781, loss = 0.22144875\n",
      "Iteration 2031, loss = 0.12186129\n",
      "Iteration 2380, loss = 0.08026599\n",
      "Iteration 1477, loss = 0.24133591\n",
      "Iteration 842, loss = 0.32038046\n",
      "Iteration 2381, loss = 0.08015695\n",
      "Iteration 714, loss = 0.34050499\n",
      "Iteration 2382, loss = 0.08023320\n",
      "Iteration 2383, loss = 0.08005236\n",
      "Iteration 1478, loss = 0.24115734\n",
      "Iteration 1782, loss = 0.22140453\n",
      "Iteration 2032, loss = 0.12187526\n",
      "Iteration 516, loss = 0.35056375\n",
      "Iteration 2384, loss = 0.08002901\n",
      "Iteration 715, loss = 0.34037161\n",
      "Iteration 2385, loss = 0.07997161\n",
      "Iteration 1479, loss = 0.24119645\n",
      "Iteration 1661, loss = 0.16943971\n",
      "Iteration 2033, loss = 0.12173883\n",
      "Iteration 843, loss = 0.32015824\n",
      "Iteration 1480, loss = 0.24091074\n",
      "Iteration 2386, loss = 0.08003348\n",
      "Iteration 1481, loss = 0.24081551\n",
      "Iteration 2387, loss = 0.07990606\n",
      "Iteration 2034, loss = 0.12163296\n",
      "Iteration 1783, loss = 0.22132684\n",
      "Iteration 716, loss = 0.34025581\n",
      "Iteration 2388, loss = 0.07986925\n",
      "Iteration 517, loss = 0.35043226\n",
      "Iteration 2389, loss = 0.07975492\n",
      "Iteration 2035, loss = 0.12156988\n",
      "Iteration 2390, loss = 0.07967501\n",
      "Iteration 1482, loss = 0.24074892\n",
      "Iteration 518, loss = 0.35032749\n",
      "Iteration 2391, loss = 0.07963841\n",
      "Iteration 1662, loss = 0.16901815\n",
      "Iteration 2392, loss = 0.07966844\n",
      "Iteration 844, loss = 0.32001855\n",
      "Iteration 2393, loss = 0.07953010\n",
      "Iteration 717, loss = 0.34015298\n",
      "Iteration 519, loss = 0.35014114\n",
      "Iteration 2394, loss = 0.07952269\n",
      "Iteration 1483, loss = 0.24059552\n",
      "Iteration 1784, loss = 0.22122945\n",
      "Iteration 2395, loss = 0.07948372\n",
      "Iteration 2036, loss = 0.12164914\n",
      "Iteration 2396, loss = 0.07953125\n",
      "Iteration 1484, loss = 0.24050077\n",
      "Iteration 1663, loss = 0.16894096\n",
      "Iteration 520, loss = 0.35001332\n",
      "Iteration 718, loss = 0.34010381\n",
      "Iteration 1785, loss = 0.22107324\n",
      "Iteration 1485, loss = 0.24036593\n",
      "Iteration 2397, loss = 0.07936612\n",
      "Iteration 2037, loss = 0.12147658\n",
      "Iteration 845, loss = 0.31980290\n",
      "Iteration 2398, loss = 0.07939072\n",
      "Iteration 2038, loss = 0.12149090\n",
      "Iteration 521, loss = 0.34987111\n",
      "Iteration 2399, loss = 0.07920100\n",
      "Iteration 1664, loss = 0.16881043\n",
      "Iteration 1786, loss = 0.22102531\n",
      "Iteration 1486, loss = 0.24024887\n",
      "Iteration 2400, loss = 0.07916592\n",
      "Iteration 719, loss = 0.33996120\n",
      "Iteration 2401, loss = 0.07916569\n",
      "Iteration 1487, loss = 0.24008256\n",
      "Iteration 522, loss = 0.34977716\n",
      "Iteration 2402, loss = 0.07907235\n",
      "Iteration 1488, loss = 0.23998791\n",
      "Iteration 2403, loss = 0.07901556\n",
      "Iteration 1787, loss = 0.22089100\n",
      "Iteration 720, loss = 0.33985760\n",
      "Iteration 2404, loss = 0.07895979\n",
      "Iteration 2405, loss = 0.07893569\n",
      "Iteration 2039, loss = 0.12130537\n",
      "Iteration 846, loss = 0.31962808\n",
      "Iteration 2406, loss = 0.07897728\n",
      "Iteration 2407, loss = 0.07880226\n",
      "Iteration 523, loss = 0.34958005\n",
      "Iteration 2408, loss = 0.07886334\n",
      "Iteration 2040, loss = 0.12125716\n",
      "Iteration 721, loss = 0.33979244\n",
      "Iteration 2409, loss = 0.07879885\n",
      "Iteration 1665, loss = 0.16873685\n",
      "Iteration 847, loss = 0.31943246\n",
      "Iteration 524, loss = 0.34946719\n",
      "Iteration 2410, loss = 0.07867812\n",
      "Iteration 2041, loss = 0.12119472\n",
      "Iteration 525, loss = 0.34933339Iteration 2411, loss = 0.07860488\n",
      "\n",
      "Iteration 1489, loss = 0.23991280\n",
      "Iteration 2412, loss = 0.07861314\n",
      "Iteration 1666, loss = 0.16861867\n",
      "Iteration 1788, loss = 0.22076141\n",
      "Iteration 2413, loss = 0.07856014\n",
      "Iteration 1490, loss = 0.23976784\n",
      "Iteration 2414, loss = 0.07846013\n",
      "Iteration 526, loss = 0.34915683\n",
      "Iteration 2415, loss = 0.07842229\n",
      "Iteration 1667, loss = 0.16853259\n",
      "Iteration 2416, loss = 0.07837843\n",
      "Iteration 1491, loss = 0.23969179\n",
      "Iteration 527, loss = 0.34901133\n",
      "Iteration 2417, loss = 0.07834569\n",
      "Iteration 1492, loss = 0.23952017\n",
      "Iteration 2042, loss = 0.12114281\n",
      "Iteration 1789, loss = 0.22070678\n",
      "Iteration 2418, loss = 0.07825847\n",
      "Iteration 722, loss = 0.33965768\n",
      "Iteration 848, loss = 0.31931215\n",
      "Iteration 2419, loss = 0.07824222\n",
      "Iteration 1790, loss = 0.22059924\n",
      "Iteration 1493, loss = 0.23937858\n",
      "Iteration 2043, loss = 0.12108050\n",
      "Iteration 723, loss = 0.33956576\n",
      "Iteration 1791, loss = 0.22043817\n",
      "Iteration 1668, loss = 0.16846991\n",
      "Iteration 2420, loss = 0.07822894\n",
      "Iteration 1494, loss = 0.23934806\n",
      "Iteration 1792, loss = 0.22039860\n",
      "Iteration 2044, loss = 0.12103850\n",
      "Iteration 849, loss = 0.31913139\n",
      "Iteration 1495, loss = 0.23920349\n",
      "Iteration 724, loss = 0.33946304\n",
      "Iteration 528, loss = 0.34890835\n",
      "Iteration 2421, loss = 0.07813164\n",
      "Iteration 1496, loss = 0.23904673\n",
      "Iteration 1669, loss = 0.16839644Iteration 2045, loss = 0.12094344\n",
      "Iteration 2422, loss = 0.07816042Iteration 725, loss = 0.33934384\n",
      "\n",
      "\n",
      "Iteration 2423, loss = 0.07802529\n",
      "Iteration 529, loss = 0.34874067\n",
      "Iteration 850, loss = 0.31893454\n",
      "Iteration 530, loss = 0.34859328\n",
      "Iteration 2046, loss = 0.12080702\n",
      "Iteration 2424, loss = 0.07797575\n",
      "Iteration 1497, loss = 0.23895816\n",
      "Iteration 2425, loss = 0.07814073\n",
      "Iteration 531, loss = 0.34852331\n",
      "Iteration 2426, loss = 0.07790910\n",
      "Iteration 1793, loss = 0.22028161\n",
      "Iteration 1498, loss = 0.23878862\n",
      "Iteration 726, loss = 0.33925345\n",
      "Iteration 2427, loss = 0.07805475\n",
      "Iteration 2428, loss = 0.07778561\n",
      "Iteration 2429, loss = 0.07772283\n",
      "Iteration 851, loss = 0.31896187\n",
      "Iteration 2430, loss = 0.07768696\n",
      "Iteration 2431, loss = 0.07761290\n",
      "Iteration 2047, loss = 0.12072580\n",
      "Iteration 2432, loss = 0.07760109\n",
      "Iteration 1499, loss = 0.23872115\n",
      "Iteration 1670, loss = 0.16826301\n",
      "Iteration 532, loss = 0.34832066\n",
      "Iteration 2433, loss = 0.07755667\n",
      "Iteration 727, loss = 0.33914469\n",
      "Iteration 2048, loss = 0.12068384\n",
      "Iteration 1794, loss = 0.22017181\n",
      "Iteration 1500, loss = 0.23863884\n",
      "Iteration 2434, loss = 0.07755124\n",
      "Iteration 1671, loss = 0.16816820\n",
      "Iteration 533, loss = 0.34817898\n",
      "Iteration 2435, loss = 0.07745495\n",
      "Iteration 852, loss = 0.31864766\n",
      "Iteration 728, loss = 0.33904697\n",
      "Iteration 1795, loss = 0.22001811\n",
      "Iteration 2436, loss = 0.07739419\n",
      "Iteration 2049, loss = 0.12064859\n",
      "Iteration 2437, loss = 0.07734762\n",
      "Iteration 2438, loss = 0.07732404\n",
      "Iteration 2050, loss = 0.12061658\n",
      "Iteration 1501, loss = 0.23853848\n",
      "Iteration 2439, loss = 0.07731835\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 534, loss = 0.34804664\n",
      "Iteration 1502, loss = 0.23841788\n",
      "Iteration 1796, loss = 0.21993530\n",
      "Iteration 2051, loss = 0.12060906\n",
      "Iteration 1672, loss = 0.16809571\n",
      "Iteration 535, loss = 0.34787965\n",
      "Iteration 1503, loss = 0.23821679\n",
      "Iteration 729, loss = 0.33895957\n",
      "Iteration 1797, loss = 0.21995051\n",
      "Iteration 2052, loss = 0.12044923\n",
      "Iteration 536, loss = 0.34777266\n",
      "Iteration 853, loss = 0.31864831\n",
      "Iteration 1504, loss = 0.23809228\n",
      "Iteration 730, loss = 0.33883511\n",
      "Iteration 1673, loss = 0.16795084\n",
      "Iteration 2053, loss = 0.12033289\n",
      "Iteration 537, loss = 0.34775048\n",
      "Iteration 1505, loss = 0.23798195\n",
      "Iteration 1506, loss = 0.23797027\n",
      "Iteration 731, loss = 0.33875841\n",
      "Iteration 2054, loss = 0.12032881\n",
      "Iteration 1798, loss = 0.21974313\n",
      "Iteration 854, loss = 0.31825534\n",
      "Iteration 1507, loss = 0.23773411\n",
      "Iteration 538, loss = 0.34746725\n",
      "Iteration 1508, loss = 0.23758006\n",
      "Iteration 1, loss = 0.80662216\n",
      "Iteration 539, loss = 0.34738577\n",
      "Iteration 2055, loss = 0.12023090\n",
      "Iteration 732, loss = 0.33871698\n",
      "Iteration 1674, loss = 0.16782917\n",
      "Iteration 1509, loss = 0.23754259\n",
      "Iteration 855, loss = 0.31817180\n",
      "Iteration 540, loss = 0.34721126\n",
      "Iteration 1799, loss = 0.21964989\n",
      "Iteration 733, loss = 0.33855788\n",
      "Iteration 1510, loss = 0.23736204\n",
      "Iteration 2056, loss = 0.12017681\n",
      "Iteration 541, loss = 0.34705302\n",
      "Iteration 856, loss = 0.31795249\n",
      "Iteration 734, loss = 0.33844548\n",
      "Iteration 1511, loss = 0.23721888\n",
      "Iteration 2, loss = 0.80593783\n",
      "Iteration 1800, loss = 0.21956745\n",
      "Iteration 2057, loss = 0.12019021\n",
      "Iteration 1675, loss = 0.16784144\n",
      "Iteration 2058, loss = 0.12012265\n",
      "Iteration 1512, loss = 0.23709561\n",
      "Iteration 542, loss = 0.34691120\n",
      "Iteration 857, loss = 0.31786476\n",
      "Iteration 1513, loss = 0.23699701\n",
      "Iteration 735, loss = 0.33837189\n",
      "Iteration 543, loss = 0.34678494\n",
      "Iteration 1801, loss = 0.21944246\n",
      "Iteration 2059, loss = 0.11998633\n",
      "Iteration 1514, loss = 0.23687724\n",
      "Iteration 1676, loss = 0.16774426\n",
      "Iteration 3, loss = 0.80486135\n",
      "Iteration 544, loss = 0.34663004\n",
      "Iteration 736, loss = 0.33824218\n",
      "Iteration 1515, loss = 0.23683112\n",
      "Iteration 858, loss = 0.31755769\n",
      "Iteration 545, loss = 0.34648164\n",
      "Iteration 2060, loss = 0.12006092\n",
      "Iteration 1516, loss = 0.23663105\n",
      "Iteration 737, loss = 0.33815733\n",
      "Iteration 1677, loss = 0.16764660\n",
      "Iteration 1517, loss = 0.23651447\n",
      "Iteration 2061, loss = 0.11987026\n",
      "Iteration 546, loss = 0.34634235\n",
      "Iteration 1802, loss = 0.21938189\n",
      "Iteration 4, loss = 0.80352195\n",
      "Iteration 547, loss = 0.34622829\n",
      "Iteration 2062, loss = 0.11980103\n",
      "Iteration 1518, loss = 0.23637571\n",
      "Iteration 548, loss = 0.34607275\n",
      "Iteration 2063, loss = 0.11967226\n",
      "Iteration 1803, loss = 0.21931446\n",
      "Iteration 1519, loss = 0.23646723\n",
      "Iteration 549, loss = 0.34594734\n",
      "Iteration 1520, loss = 0.23613904\n",
      "Iteration 2064, loss = 0.11972932\n",
      "Iteration 1678, loss = 0.16759673\n",
      "Iteration 2065, loss = 0.11956991\n",
      "Iteration 5, loss = 0.80198904\n",
      "Iteration 1521, loss = 0.23605621\n",
      "Iteration 2066, loss = 0.11958514\n",
      "Iteration 1804, loss = 0.21912607\n",
      "Iteration 738, loss = 0.33805088\n",
      "Iteration 859, loss = 0.31741874\n",
      "Iteration 1522, loss = 0.23593564\n",
      "Iteration 550, loss = 0.34586584\n",
      "Iteration 1523, loss = 0.23582673\n",
      "Iteration 1805, loss = 0.21905698\n",
      "Iteration 1679, loss = 0.16741456\n",
      "Iteration 6, loss = 0.80035861\n",
      "Iteration 2067, loss = 0.11948028\n",
      "Iteration 1524, loss = 0.23569310\n",
      "Iteration 739, loss = 0.33793395\n",
      "Iteration 1525, loss = 0.23551869\n",
      "Iteration 860, loss = 0.31722292\n",
      "Iteration 551, loss = 0.34565490\n",
      "Iteration 2068, loss = 0.11941173Iteration 1806, loss = 0.21893459\n",
      "\n",
      "Iteration 1680, loss = 0.16730393\n",
      "Iteration 740, loss = 0.33783540\n",
      "Iteration 552, loss = 0.34551183\n",
      "Iteration 1807, loss = 0.21886425\n",
      "Iteration 2069, loss = 0.11936089\n",
      "Iteration 1526, loss = 0.23544066\n",
      "Iteration 861, loss = 0.31717467\n",
      "Iteration 7, loss = 0.79860584\n",
      "Iteration 1808, loss = 0.21874609\n",
      "Iteration 1527, loss = 0.23533206\n",
      "Iteration 2070, loss = 0.11932919\n",
      "Iteration 1681, loss = 0.16722699\n",
      "Iteration 553, loss = 0.34538222\n",
      "Iteration 1528, loss = 0.23523999\n",
      "Iteration 2071, loss = 0.11923437\n",
      "Iteration 1529, loss = 0.23510155\n",
      "Iteration 554, loss = 0.34520560\n",
      "Iteration 1809, loss = 0.21864391\n",
      "Iteration 741, loss = 0.33773055\n",
      "Iteration 8, loss = 0.79685425\n",
      "Iteration 1530, loss = 0.23496193\n",
      "Iteration 555, loss = 0.34508729\n",
      "Iteration 2072, loss = 0.11913828\n",
      "Iteration 1531, loss = 0.23482207\n",
      "Iteration 862, loss = 0.31696203\n",
      "Iteration 1810, loss = 0.21851508\n",
      "Iteration 742, loss = 0.33762833\n",
      "Iteration 9, loss = 0.79501039\n",
      "Iteration 2073, loss = 0.11913683\n",
      "Iteration 556, loss = 0.34491663\n",
      "Iteration 1532, loss = 0.23469534\n",
      "Iteration 1533, loss = 0.23456581\n",
      "Iteration 1811, loss = 0.21844041\n",
      "Iteration 743, loss = 0.33758433\n",
      "Iteration 1534, loss = 0.23454376\n",
      "Iteration 863, loss = 0.31697398\n",
      "Iteration 1535, loss = 0.23452729\n",
      "Iteration 744, loss = 0.33744552\n",
      "Iteration 1812, loss = 0.21832498\n",
      "Iteration 1536, loss = 0.23423217\n",
      "Iteration 1682, loss = 0.16714397\n",
      "Iteration 745, loss = 0.33731650\n",
      "Iteration 10, loss = 0.79312596\n",
      "Iteration 1537, loss = 0.23412187\n",
      "Iteration 1813, loss = 0.21820639\n",
      "Iteration 2074, loss = 0.11905845\n",
      "Iteration 557, loss = 0.34479970\n",
      "Iteration 1683, loss = 0.16702422\n",
      "Iteration 864, loss = 0.31655782\n",
      "Iteration 11, loss = 0.79133530\n",
      "Iteration 558, loss = 0.34465180\n",
      "Iteration 1538, loss = 0.23395864\n",
      "Iteration 746, loss = 0.33722290\n",
      "Iteration 1814, loss = 0.21815742\n",
      "Iteration 2075, loss = 0.11895497\n",
      "Iteration 1539, loss = 0.23382632\n",
      "Iteration 559, loss = 0.34448335\n",
      "Iteration 1815, loss = 0.21818035\n",
      "Iteration 865, loss = 0.31646375\n",
      "Iteration 1684, loss = 0.16687873\n",
      "Iteration 1540, loss = 0.23377563\n",
      "Iteration 747, loss = 0.33713100\n",
      "Iteration 560, loss = 0.34436494\n",
      "Iteration 2076, loss = 0.11915629\n",
      "Iteration 12, loss = 0.78944827\n",
      "Iteration 1816, loss = 0.21795939\n",
      "Iteration 1541, loss = 0.23361874\n",
      "Iteration 2077, loss = 0.11904500\n",
      "Iteration 748, loss = 0.33702913\n",
      "Iteration 561, loss = 0.34423372\n",
      "Iteration 562, loss = 0.34403990\n",
      "Iteration 13, loss = 0.78752064\n",
      "Iteration 866, loss = 0.31626580\n",
      "Iteration 1542, loss = 0.23349056\n",
      "Iteration 2078, loss = 0.11879530\n",
      "Iteration 1817, loss = 0.21793127\n",
      "Iteration 1543, loss = 0.23341129\n",
      "Iteration 1685, loss = 0.16680130\n",
      "Iteration 749, loss = 0.33691584\n",
      "Iteration 563, loss = 0.34396665\n",
      "Iteration 2079, loss = 0.11872669\n",
      "Iteration 1544, loss = 0.23324382\n",
      "Iteration 2080, loss = 0.11868992\n",
      "Iteration 1818, loss = 0.21771781\n",
      "Iteration 750, loss = 0.33688822\n",
      "Iteration 14, loss = 0.78570848\n",
      "Iteration 564, loss = 0.34385815\n",
      "Iteration 2081, loss = 0.11863077\n",
      "Iteration 1545, loss = 0.23312931\n",
      "Iteration 751, loss = 0.33671425\n",
      "Iteration 1686, loss = 0.16691270\n",
      "Iteration 1546, loss = 0.23298482\n",
      "Iteration 867, loss = 0.31622247\n",
      "Iteration 2082, loss = 0.11869428\n",
      "Iteration 1819, loss = 0.21766694\n",
      "Iteration 565, loss = 0.34364035\n",
      "Iteration 1547, loss = 0.23290783\n",
      "Iteration 566, loss = 0.34347531\n",
      "Iteration 2083, loss = 0.11853944\n",
      "Iteration 1820, loss = 0.21757387\n",
      "Iteration 15, loss = 0.78394030\n",
      "Iteration 1548, loss = 0.23279886\n",
      "Iteration 752, loss = 0.33664295\n",
      "Iteration 1821, loss = 0.21742765\n",
      "Iteration 1549, loss = 0.23266812\n",
      "Iteration 1687, loss = 0.16664975\n",
      "Iteration 868, loss = 0.31585664\n",
      "Iteration 2084, loss = 0.11840618\n",
      "Iteration 1822, loss = 0.21729633\n",
      "Iteration 567, loss = 0.34334412\n",
      "Iteration 1550, loss = 0.23252242\n",
      "Iteration 753, loss = 0.33652045\n",
      "Iteration 16, loss = 0.78200560\n",
      "Iteration 1551, loss = 0.23239599\n",
      "Iteration 568, loss = 0.34325552\n",
      "Iteration 1552, loss = 0.23233065\n",
      "Iteration 754, loss = 0.33640959\n",
      "Iteration 869, loss = 0.31566897\n",
      "Iteration 569, loss = 0.34304302\n",
      "Iteration 2085, loss = 0.11845491\n",
      "Iteration 17, loss = 0.78015618\n",
      "Iteration 1688, loss = 0.16655723\n",
      "Iteration 1553, loss = 0.23215598\n",
      "Iteration 570, loss = 0.34290491\n",
      "Iteration 571, loss = 0.34278743\n",
      "Iteration 2086, loss = 0.11830748\n",
      "Iteration 1823, loss = 0.21728610\n",
      "Iteration 1554, loss = 0.23211611\n",
      "Iteration 755, loss = 0.33644392\n",
      "Iteration 870, loss = 0.31550470\n",
      "Iteration 1555, loss = 0.23193423\n",
      "Iteration 1689, loss = 0.16642542\n",
      "Iteration 2087, loss = 0.11826416\n",
      "Iteration 756, loss = 0.33619826\n",
      "Iteration 1824, loss = 0.21715342\n",
      "Iteration 572, loss = 0.34264305\n",
      "Iteration 871, loss = 0.31534236\n",
      "Iteration 2088, loss = 0.11834293\n",
      "Iteration 1690, loss = 0.16636457\n",
      "Iteration 18, loss = 0.77830379\n",
      "Iteration 573, loss = 0.34248047\n",
      "Iteration 2089, loss = 0.11813744\n",
      "Iteration 757, loss = 0.33609319\n",
      "Iteration 574, loss = 0.34233798\n",
      "Iteration 2090, loss = 0.11807649\n",
      "Iteration 1825, loss = 0.21706951\n",
      "Iteration 872, loss = 0.31518307\n",
      "Iteration 758, loss = 0.33599879\n",
      "Iteration 1556, loss = 0.23185391\n",
      "Iteration 575, loss = 0.34220221\n",
      "Iteration 1826, loss = 0.21695503\n",
      "Iteration 2091, loss = 0.11802355\n",
      "Iteration 576, loss = 0.34205866\n",
      "Iteration 1557, loss = 0.23170836\n",
      "Iteration 19, loss = 0.77651811\n",
      "Iteration 1827, loss = 0.21691332\n",
      "Iteration 1558, loss = 0.23156222\n",
      "Iteration 577, loss = 0.34198071\n",
      "Iteration 1691, loss = 0.16623831\n",
      "Iteration 2092, loss = 0.11798325\n",
      "Iteration 759, loss = 0.33590253\n",
      "Iteration 1559, loss = 0.23143028\n",
      "Iteration 1828, loss = 0.21682042\n",
      "Iteration 578, loss = 0.34181284\n",
      "Iteration 760, loss = 0.33583642\n",
      "Iteration 873, loss = 0.31503222\n",
      "Iteration 579, loss = 0.34164474\n",
      "Iteration 2093, loss = 0.11796636\n",
      "Iteration 1829, loss = 0.21681955\n",
      "Iteration 1560, loss = 0.23136212\n",
      "Iteration 1692, loss = 0.16614433\n",
      "Iteration 580, loss = 0.34146438\n",
      "Iteration 20, loss = 0.77467354\n",
      "Iteration 761, loss = 0.33570368\n",
      "Iteration 1561, loss = 0.23120904\n",
      "Iteration 874, loss = 0.31485752\n",
      "Iteration 2094, loss = 0.11781936\n",
      "Iteration 581, loss = 0.34139144\n",
      "Iteration 1562, loss = 0.23111484\n",
      "Iteration 582, loss = 0.34123942\n",
      "Iteration 1563, loss = 0.23094076\n",
      "Iteration 762, loss = 0.33561275\n",
      "Iteration 2095, loss = 0.11784741\n",
      "Iteration 583, loss = 0.34105794\n",
      "Iteration 1830, loss = 0.21657291\n",
      "Iteration 21, loss = 0.77279317\n",
      "Iteration 1564, loss = 0.23084197\n",
      "Iteration 1565, loss = 0.23076997\n",
      "Iteration 584, loss = 0.34090871\n",
      "Iteration 1693, loss = 0.16605009\n",
      "Iteration 1831, loss = 0.21653952\n",
      "Iteration 2096, loss = 0.11774061\n",
      "Iteration 22, loss = 0.77091886\n",
      "Iteration 763, loss = 0.33548704\n",
      "Iteration 585, loss = 0.34085913\n",
      "Iteration 1694, loss = 0.16598527\n",
      "Iteration 1566, loss = 0.23059257\n",
      "Iteration 1832, loss = 0.21647827\n",
      "Iteration 2097, loss = 0.11773003\n",
      "Iteration 875, loss = 0.31462002\n",
      "Iteration 1567, loss = 0.23061651\n",
      "Iteration 764, loss = 0.33538006Iteration 586, loss = 0.34063429\n",
      "\n",
      "Iteration 23, loss = 0.76906072\n",
      "Iteration 1568, loss = 0.23043258\n",
      "Iteration 1833, loss = 0.21623146\n",
      "Iteration 587, loss = 0.34058345\n",
      "Iteration 1695, loss = 0.16584824\n",
      "Iteration 1569, loss = 0.23021777\n",
      "Iteration 765, loss = 0.33528540\n",
      "Iteration 1570, loss = 0.23016927\n",
      "Iteration 876, loss = 0.31448641\n",
      "Iteration 2098, loss = 0.11767589\n",
      "Iteration 1834, loss = 0.21612427\n",
      "Iteration 766, loss = 0.33519391\n",
      "Iteration 1571, loss = 0.22997949\n",
      "Iteration 588, loss = 0.34031972\n",
      "Iteration 2099, loss = 0.11754454\n",
      "Iteration 1572, loss = 0.22986692\n",
      "Iteration 1696, loss = 0.16591555\n",
      "Iteration 1573, loss = 0.22979234\n",
      "Iteration 24, loss = 0.76717135\n",
      "Iteration 589, loss = 0.34017230\n",
      "Iteration 1835, loss = 0.21609640\n",
      "Iteration 2100, loss = 0.11761461\n",
      "Iteration 877, loss = 0.31427767\n",
      "Iteration 1574, loss = 0.22959578\n",
      "Iteration 2101, loss = 0.11746016\n",
      "Iteration 25, loss = 0.76527916\n",
      "Iteration 1575, loss = 0.22947756\n",
      "Iteration 1836, loss = 0.21595463\n",
      "Iteration 590, loss = 0.34002961\n",
      "Iteration 1697, loss = 0.16570200\n",
      "Iteration 1576, loss = 0.22939944\n",
      "Iteration 2102, loss = 0.11745674\n",
      "Iteration 591, loss = 0.33989617\n",
      "Iteration 1577, loss = 0.22929133\n",
      "Iteration 26, loss = 0.76339085\n",
      "Iteration 1837, loss = 0.21587758\n",
      "Iteration 592, loss = 0.33972903\n",
      "Iteration 2103, loss = 0.11743256\n",
      "Iteration 1698, loss = 0.16560835\n",
      "Iteration 1838, loss = 0.21574066\n",
      "Iteration 878, loss = 0.31412418\n",
      "Iteration 767, loss = 0.33506463\n",
      "Iteration 27, loss = 0.76142966\n",
      "Iteration 1578, loss = 0.22915309\n",
      "Iteration 593, loss = 0.33962913\n",
      "Iteration 2104, loss = 0.11724508\n",
      "Iteration 1699, loss = 0.16558776\n",
      "Iteration 1579, loss = 0.22903110\n",
      "Iteration 1580, loss = 0.22888301\n",
      "Iteration 594, loss = 0.33948808\n",
      "Iteration 768, loss = 0.33498338\n",
      "Iteration 1839, loss = 0.21571421\n",
      "Iteration 28, loss = 0.75948531\n",
      "Iteration 1581, loss = 0.22875696\n",
      "Iteration 1700, loss = 0.16541464\n",
      "Iteration 595, loss = 0.33931251\n",
      "Iteration 769, loss = 0.33489267\n",
      "Iteration 879, loss = 0.31393706\n",
      "Iteration 596, loss = 0.33921854\n",
      "Iteration 1582, loss = 0.22864515\n",
      "Iteration 1701, loss = 0.16533616\n",
      "Iteration 1583, loss = 0.22863734\n",
      "Iteration 2105, loss = 0.11723835\n",
      "Iteration 770, loss = 0.33481078\n",
      "Iteration 1840, loss = 0.21558806\n",
      "Iteration 880, loss = 0.31378510\n",
      "Iteration 1584, loss = 0.22839532\n",
      "Iteration 2106, loss = 0.11728846\n",
      "Iteration 771, loss = 0.33468438\n",
      "Iteration 597, loss = 0.33902076\n",
      "Iteration 1585, loss = 0.22828982\n",
      "Iteration 1841, loss = 0.21541343\n",
      "Iteration 881, loss = 0.31358496\n",
      "Iteration 1586, loss = 0.22817839\n",
      "Iteration 29, loss = 0.75750515\n",
      "Iteration 2107, loss = 0.11716682\n",
      "Iteration 1587, loss = 0.22815768\n",
      "Iteration 772, loss = 0.33459505\n",
      "Iteration 1588, loss = 0.22796769\n",
      "Iteration 1842, loss = 0.21544173\n",
      "Iteration 882, loss = 0.31361574\n",
      "Iteration 1589, loss = 0.22783831\n",
      "Iteration 2108, loss = 0.11711452\n",
      "Iteration 1702, loss = 0.16523553\n",
      "Iteration 773, loss = 0.33448409\n",
      "Iteration 1843, loss = 0.21524718\n",
      "Iteration 1590, loss = 0.22767021\n",
      "Iteration 30, loss = 0.75544654\n",
      "Iteration 2109, loss = 0.11708036\n",
      "Iteration 1703, loss = 0.16513244\n",
      "Iteration 774, loss = 0.33439869\n",
      "Iteration 883, loss = 0.31326797\n",
      "Iteration 1591, loss = 0.22766812\n",
      "Iteration 1844, loss = 0.21518932\n",
      "Iteration 1592, loss = 0.22745438\n",
      "Iteration 775, loss = 0.33425607\n",
      "Iteration 1845, loss = 0.21503559\n",
      "Iteration 2110, loss = 0.11692106\n",
      "Iteration 1593, loss = 0.22736849\n",
      "Iteration 1704, loss = 0.16510438\n",
      "Iteration 31, loss = 0.75340428\n",
      "Iteration 2111, loss = 0.11690987\n",
      "Iteration 1846, loss = 0.21495692\n",
      "Iteration 776, loss = 0.33417162\n",
      "Iteration 1594, loss = 0.22718950\n",
      "Iteration 884, loss = 0.31306760\n",
      "Iteration 2112, loss = 0.11680270\n",
      "Iteration 1595, loss = 0.22708342\n",
      "Iteration 32, loss = 0.75134445\n",
      "Iteration 1705, loss = 0.16530296\n",
      "Iteration 777, loss = 0.33409876\n",
      "Iteration 1596, loss = 0.22706823\n",
      "Iteration 1847, loss = 0.21488942\n",
      "Iteration 1597, loss = 0.22684093\n",
      "Iteration 2113, loss = 0.11693415\n",
      "Iteration 885, loss = 0.31288815\n",
      "Iteration 33, loss = 0.74925519\n",
      "Iteration 1598, loss = 0.22672313\n",
      "Iteration 2114, loss = 0.11680087\n",
      "Iteration 1848, loss = 0.21469080\n",
      "Iteration 778, loss = 0.33396732\n",
      "Iteration 1599, loss = 0.22660137\n",
      "Iteration 2115, loss = 0.11669608\n",
      "Iteration 886, loss = 0.31277366\n",
      "Iteration 1600, loss = 0.22646127\n",
      "Iteration 1706, loss = 0.16490972\n",
      "Iteration 2116, loss = 0.11670534\n",
      "Iteration 1601, loss = 0.22633900\n",
      "Iteration 1849, loss = 0.21465120\n",
      "Iteration 34, loss = 0.74717694\n",
      "Iteration 2117, loss = 0.11658768\n",
      "Iteration 598, loss = 0.33886665\n",
      "Iteration 1707, loss = 0.16491742\n",
      "Iteration 1602, loss = 0.22621228\n",
      "Iteration 887, loss = 0.31259885\n",
      "Iteration 779, loss = 0.33383714\n",
      "Iteration 1850, loss = 0.21459948\n",
      "Iteration 2118, loss = 0.11654481\n",
      "Iteration 1603, loss = 0.22614001\n",
      "Iteration 1708, loss = 0.16471610\n",
      "Iteration 780, loss = 0.33376964\n",
      "Iteration 888, loss = 0.31236353\n",
      "Iteration 35, loss = 0.74498289\n",
      "Iteration 1604, loss = 0.22594534\n",
      "Iteration 1851, loss = 0.21442055\n",
      "Iteration 2119, loss = 0.11645627\n",
      "Iteration 781, loss = 0.33365524\n",
      "Iteration 2120, loss = 0.11639605\n",
      "Iteration 1605, loss = 0.22594629\n",
      "Iteration 1852, loss = 0.21433561\n",
      "Iteration 889, loss = 0.31226284\n",
      "Iteration 2121, loss = 0.11631072\n",
      "Iteration 1709, loss = 0.16466886\n",
      "Iteration 1606, loss = 0.22574767\n",
      "Iteration 782, loss = 0.33354008\n",
      "Iteration 1853, loss = 0.21422785\n",
      "Iteration 2122, loss = 0.11627184\n",
      "Iteration 36, loss = 0.74277433\n",
      "Iteration 1607, loss = 0.22565418\n",
      "Iteration 1608, loss = 0.22548981Iteration 2123, loss = 0.11626731\n",
      "\n",
      "Iteration 2124, loss = 0.11616603\n",
      "Iteration 37, loss = 0.74055086\n",
      "Iteration 783, loss = 0.33347913\n",
      "Iteration 2125, loss = 0.11611299\n",
      "Iteration 599, loss = 0.33870561\n",
      "Iteration 890, loss = 0.31206793\n",
      "Iteration 1609, loss = 0.22537273\n",
      "Iteration 1854, loss = 0.21414439\n",
      "Iteration 600, loss = 0.33860354\n",
      "Iteration 1710, loss = 0.16456374\n",
      "Iteration 1610, loss = 0.22533362\n",
      "Iteration 784, loss = 0.33334325\n",
      "Iteration 601, loss = 0.33841267\n",
      "Iteration 891, loss = 0.31194772\n",
      "Iteration 1855, loss = 0.21410321\n",
      "Iteration 602, loss = 0.33827984\n",
      "Iteration 785, loss = 0.33323139\n",
      "Iteration 2126, loss = 0.11616060\n",
      "Iteration 786, loss = 0.33311679\n",
      "Iteration 892, loss = 0.31173677\n",
      "Iteration 2127, loss = 0.11614488\n",
      "Iteration 787, loss = 0.33300503\n",
      "Iteration 603, loss = 0.33811423\n",
      "Iteration 1611, loss = 0.22513725\n",
      "Iteration 1856, loss = 0.21391821\n",
      "Iteration 38, loss = 0.73831331\n",
      "Iteration 1612, loss = 0.22502653\n",
      "Iteration 1711, loss = 0.16445151\n",
      "Iteration 1613, loss = 0.22494190\n",
      "Iteration 1857, loss = 0.21381879\n",
      "Iteration 604, loss = 0.33801940\n",
      "Iteration 1614, loss = 0.22483342\n",
      "Iteration 2128, loss = 0.11596549\n",
      "Iteration 605, loss = 0.33782665\n",
      "Iteration 1712, loss = 0.16448554\n",
      "Iteration 1615, loss = 0.22464802\n",
      "Iteration 1858, loss = 0.21372175\n",
      "Iteration 2129, loss = 0.11599144\n",
      "Iteration 39, loss = 0.73607097\n",
      "Iteration 1616, loss = 0.22469904\n",
      "Iteration 2130, loss = 0.11585193\n",
      "Iteration 1713, loss = 0.16432341\n",
      "Iteration 893, loss = 0.31146339\n",
      "Iteration 1617, loss = 0.22454148\n",
      "Iteration 40, loss = 0.73377469\n",
      "Iteration 2131, loss = 0.11583242\n",
      "Iteration 1714, loss = 0.16419161\n",
      "Iteration 1859, loss = 0.21364660\n",
      "Iteration 606, loss = 0.33777993\n",
      "Iteration 788, loss = 0.33296827\n",
      "Iteration 1618, loss = 0.22429815\n",
      "Iteration 2132, loss = 0.11576107\n",
      "Iteration 607, loss = 0.33755270\n",
      "Iteration 1619, loss = 0.22424151\n",
      "Iteration 894, loss = 0.31143550\n",
      "Iteration 2133, loss = 0.11567304\n",
      "Iteration 1860, loss = 0.21365239\n",
      "Iteration 789, loss = 0.33281718\n",
      "Iteration 41, loss = 0.73141066\n",
      "Iteration 1861, loss = 0.21342324\n",
      "Iteration 608, loss = 0.33745251\n",
      "Iteration 1620, loss = 0.22402906\n",
      "Iteration 1862, loss = 0.21336826\n",
      "Iteration 42, loss = 0.72906353\n",
      "Iteration 2134, loss = 0.11563303\n",
      "Iteration 609, loss = 0.33723371\n",
      "Iteration 1621, loss = 0.22395135\n",
      "Iteration 610, loss = 0.33705274\n",
      "Iteration 2135, loss = 0.11560513\n",
      "Iteration 895, loss = 0.31116065\n",
      "Iteration 1715, loss = 0.16408012\n",
      "Iteration 1622, loss = 0.22378630\n",
      "Iteration 790, loss = 0.33269917\n",
      "Iteration 611, loss = 0.33707872\n",
      "Iteration 2136, loss = 0.11549733\n",
      "Iteration 1863, loss = 0.21321613\n",
      "Iteration 1623, loss = 0.22372929\n",
      "Iteration 1716, loss = 0.16397013\n",
      "Iteration 612, loss = 0.33683533\n",
      "Iteration 791, loss = 0.33261385\n",
      "Iteration 1624, loss = 0.22363427\n",
      "Iteration 613, loss = 0.33660580\n",
      "Iteration 614, loss = 0.33646303\n",
      "Iteration 1717, loss = 0.16396500\n",
      "Iteration 896, loss = 0.31109174\n",
      "Iteration 792, loss = 0.33250561\n",
      "Iteration 615, loss = 0.33639252\n",
      "Iteration 616, loss = 0.33619687\n",
      "Iteration 2137, loss = 0.11548657\n",
      "Iteration 43, loss = 0.72662901\n",
      "Iteration 1864, loss = 0.21312162\n",
      "Iteration 1625, loss = 0.22358947\n",
      "Iteration 1718, loss = 0.16385394\n",
      "Iteration 897, loss = 0.31083754\n",
      "Iteration 1865, loss = 0.21302294\n",
      "Iteration 44, loss = 0.72414505\n",
      "Iteration 2138, loss = 0.11566865\n",
      "Iteration 1719, loss = 0.16376514\n",
      "Iteration 1866, loss = 0.21296841\n",
      "Iteration 898, loss = 0.31074920\n",
      "Iteration 2139, loss = 0.11536459\n",
      "Iteration 1626, loss = 0.22334486\n",
      "Iteration 899, loss = 0.31050332\n",
      "Iteration 45, loss = 0.72164075\n",
      "Iteration 1627, loss = 0.22321523\n",
      "Iteration 793, loss = 0.33237856\n",
      "Iteration 617, loss = 0.33604285\n",
      "Iteration 1628, loss = 0.22314019\n",
      "Iteration 1720, loss = 0.16365920\n",
      "Iteration 2140, loss = 0.11539745\n",
      "Iteration 1629, loss = 0.22304167\n",
      "Iteration 618, loss = 0.33584859\n",
      "Iteration 1867, loss = 0.21282450\n",
      "Iteration 900, loss = 0.31043201\n",
      "Iteration 619, loss = 0.33570144\n",
      "Iteration 794, loss = 0.33230108\n",
      "Iteration 2141, loss = 0.11530784\n",
      "Iteration 1630, loss = 0.22286718\n",
      "Iteration 46, loss = 0.71912221\n",
      "Iteration 1631, loss = 0.22274541\n",
      "Iteration 1868, loss = 0.21274597\n",
      "Iteration 795, loss = 0.33217364\n",
      "Iteration 901, loss = 0.31022017\n",
      "Iteration 2142, loss = 0.11520152\n",
      "Iteration 1632, loss = 0.22260772\n",
      "Iteration 796, loss = 0.33210015\n",
      "Iteration 620, loss = 0.33555338\n",
      "Iteration 1721, loss = 0.16357503\n",
      "Iteration 2143, loss = 0.11515711\n",
      "Iteration 47, loss = 0.71653049\n",
      "Iteration 1633, loss = 0.22250840\n",
      "Iteration 621, loss = 0.33539010\n",
      "Iteration 797, loss = 0.33196290\n",
      "Iteration 1722, loss = 0.16351854\n",
      "Iteration 1869, loss = 0.21278696\n",
      "Iteration 622, loss = 0.33527003\n",
      "Iteration 2144, loss = 0.11519906\n",
      "Iteration 1870, loss = 0.21255152\n",
      "Iteration 1634, loss = 0.22237115\n",
      "Iteration 48, loss = 0.71391807\n",
      "Iteration 2145, loss = 0.11509180\n",
      "Iteration 902, loss = 0.30989659\n",
      "Iteration 623, loss = 0.33517172\n",
      "Iteration 798, loss = 0.33192998\n",
      "Iteration 1635, loss = 0.22225217\n",
      "Iteration 1723, loss = 0.16341505\n",
      "Iteration 624, loss = 0.33498404\n",
      "Iteration 1636, loss = 0.22212454\n",
      "Iteration 1637, loss = 0.22198790\n",
      "Iteration 625, loss = 0.33479224\n",
      "Iteration 2146, loss = 0.11501245\n",
      "Iteration 799, loss = 0.33175185\n",
      "Iteration 49, loss = 0.71120787\n",
      "Iteration 1724, loss = 0.16328705\n",
      "Iteration 903, loss = 0.30975672\n",
      "Iteration 1871, loss = 0.21244830\n",
      "Iteration 626, loss = 0.33465311\n",
      "Iteration 2147, loss = 0.11496689\n",
      "Iteration 1638, loss = 0.22204568\n",
      "Iteration 904, loss = 0.30957245\n",
      "Iteration 800, loss = 0.33166875Iteration 627, loss = 0.33450213\n",
      "\n",
      "Iteration 50, loss = 0.70848819\n",
      "Iteration 2148, loss = 0.11493574\n",
      "Iteration 628, loss = 0.33432959\n",
      "Iteration 1872, loss = 0.21235107\n",
      "Iteration 1725, loss = 0.16318820\n",
      "Iteration 1639, loss = 0.22176848\n",
      "Iteration 2149, loss = 0.11500043\n",
      "Iteration 629, loss = 0.33417769\n",
      "Iteration 801, loss = 0.33155951\n",
      "Iteration 1640, loss = 0.22165232\n",
      "Iteration 51, loss = 0.70580575\n",
      "Iteration 1726, loss = 0.16321696\n",
      "Iteration 802, loss = 0.33144428\n",
      "Iteration 2150, loss = 0.11487564\n",
      "Iteration 1641, loss = 0.22155782\n",
      "Iteration 630, loss = 0.33400963\n",
      "Iteration 1642, loss = 0.22147489\n",
      "Iteration 905, loss = 0.30965595\n",
      "Iteration 1873, loss = 0.21232262\n",
      "Iteration 631, loss = 0.33401476\n",
      "Iteration 1643, loss = 0.22135185\n",
      "Iteration 52, loss = 0.70293915\n",
      "Iteration 1874, loss = 0.21214638\n",
      "Iteration 1644, loss = 0.22116215\n",
      "Iteration 803, loss = 0.33133257\n",
      "Iteration 2151, loss = 0.11481183\n",
      "Iteration 632, loss = 0.33392225\n",
      "Iteration 1645, loss = 0.22115741\n",
      "Iteration 53, loss = 0.70016202\n",
      "Iteration 1646, loss = 0.22096828\n",
      "Iteration 633, loss = 0.33357406\n",
      "Iteration 1875, loss = 0.21200070\n",
      "Iteration 1647, loss = 0.22085257\n",
      "Iteration 2152, loss = 0.11477444\n",
      "Iteration 906, loss = 0.30921233\n",
      "Iteration 1727, loss = 0.16305758\n",
      "Iteration 804, loss = 0.33126281\n",
      "Iteration 1648, loss = 0.22070679\n",
      "Iteration 2153, loss = 0.11473560\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1649, loss = 0.22058930\n",
      "Iteration 634, loss = 0.33368627\n",
      "Iteration 1876, loss = 0.21197331\n",
      "Iteration 805, loss = 0.33116477\n",
      "Iteration 907, loss = 0.30911351\n",
      "Iteration 635, loss = 0.33328984\n",
      "Iteration 1728, loss = 0.16298880\n",
      "Iteration 54, loss = 0.69729541\n",
      "Iteration 636, loss = 0.33319503\n",
      "Iteration 1, loss = 0.76686576\n",
      "Iteration 2, loss = 0.76206108\n",
      "Iteration 3, loss = 0.75456338\n",
      "Iteration 4, loss = 0.74593318\n",
      "Iteration 5, loss = 0.73582835\n",
      "Iteration 6, loss = 0.72569481\n",
      "Iteration 7, loss = 0.71513198\n",
      "Iteration 8, loss = 0.70428859\n",
      "Iteration 9, loss = 0.69446977\n",
      "Iteration 1650, loss = 0.22053014\n",
      "Iteration 908, loss = 0.30894282\n",
      "Iteration 637, loss = 0.33295906Iteration 10, loss = 0.68403030\n",
      "\n",
      "Iteration 11, loss = 0.67466302\n",
      "Iteration 12, loss = 0.66526306\n",
      "Iteration 13, loss = 0.65627011\n",
      "Iteration 1651, loss = 0.22036658\n",
      "Iteration 14, loss = 0.64768338\n",
      "Iteration 15, loss = 0.63961684\n",
      "Iteration 16, loss = 0.63206730\n",
      "Iteration 17, loss = 0.62443443\n",
      "Iteration 806, loss = 0.33103133\n",
      "Iteration 18, loss = 0.61750011\n",
      "Iteration 19, loss = 0.61064335\n",
      "Iteration 1652, loss = 0.22022531\n",
      "Iteration 55, loss = 0.69434477\n",
      "Iteration 807, loss = 0.33091296\n",
      "Iteration 1653, loss = 0.22017006\n",
      "Iteration 909, loss = 0.30871497\n",
      "Iteration 1729, loss = 0.16293144\n",
      "Iteration 20, loss = 0.60423889\n",
      "Iteration 638, loss = 0.33280320\n",
      "Iteration 808, loss = 0.33086194\n",
      "Iteration 21, loss = 0.59796812\n",
      "Iteration 1877, loss = 0.21183373\n",
      "Iteration 22, loss = 0.59213886\n",
      "Iteration 1654, loss = 0.21995201\n",
      "Iteration 23, loss = 0.58633621\n",
      "Iteration 24, loss = 0.58086499\n",
      "Iteration 25, loss = 0.57552550\n",
      "Iteration 26, loss = 0.57048826\n",
      "Iteration 27, loss = 0.56560839\n",
      "Iteration 809, loss = 0.33071949\n",
      "Iteration 28, loss = 0.56080204\n",
      "Iteration 1730, loss = 0.16283907\n",
      "Iteration 29, loss = 0.55632467\n",
      "Iteration 1655, loss = 0.21993285\n",
      "Iteration 1878, loss = 0.21178014\n",
      "Iteration 1656, loss = 0.21976000\n",
      "Iteration 1657, loss = 0.21963555\n",
      "Iteration 1879, loss = 0.21163467\n",
      "Iteration 30, loss = 0.55188201\n",
      "Iteration 1731, loss = 0.16272570\n",
      "Iteration 31, loss = 0.54749330\n",
      "Iteration 32, loss = 0.54337305\n",
      "Iteration 1658, loss = 0.21946675\n",
      "Iteration 33, loss = 0.53940006\n",
      "Iteration 34, loss = 0.53547568\n",
      "Iteration 35, loss = 0.53176707\n",
      "Iteration 1659, loss = 0.21934114\n",
      "Iteration 36, loss = 0.52806348\n",
      "Iteration 639, loss = 0.33262530\n",
      "Iteration 37, loss = 0.52454299\n",
      "Iteration 38, loss = 0.52101601\n",
      "Iteration 810, loss = 0.33060649\n",
      "Iteration 56, loss = 0.69142585\n",
      "Iteration 1732, loss = 0.16263359\n",
      "Iteration 640, loss = 0.33246681\n",
      "Iteration 1660, loss = 0.21927686\n",
      "Iteration 39, loss = 0.51765997\n",
      "Iteration 40, loss = 0.51436184\n",
      "Iteration 1880, loss = 0.21156142\n",
      "Iteration 41, loss = 0.51113333\n",
      "Iteration 42, loss = 0.50819296\n",
      "Iteration 43, loss = 0.50504867\n",
      "Iteration 44, loss = 0.50215493\n",
      "Iteration 910, loss = 0.30854868\n",
      "Iteration 45, loss = 0.49929595\n",
      "Iteration 46, loss = 0.49650411\n",
      "Iteration 47, loss = 0.49378062\n",
      "Iteration 48, loss = 0.49105001\n",
      "Iteration 49, loss = 0.48855071\n",
      "Iteration 1661, loss = 0.21919311\n",
      "Iteration 641, loss = 0.33259745\n",
      "Iteration 50, loss = 0.48600287\n",
      "Iteration 51, loss = 0.48349647\n",
      "Iteration 811, loss = 0.33050293\n",
      "Iteration 52, loss = 0.48114031\n",
      "Iteration 53, loss = 0.47875565\n",
      "Iteration 54, loss = 0.47648363\n",
      "Iteration 55, loss = 0.47423792\n",
      "Iteration 56, loss = 0.47206551\n",
      "Iteration 1881, loss = 0.21139561\n",
      "Iteration 57, loss = 0.46992807\n",
      "Iteration 1662, loss = 0.21904412\n",
      "Iteration 58, loss = 0.46784281\n",
      "Iteration 59, loss = 0.46579863\n",
      "Iteration 60, loss = 0.46384669\n",
      "Iteration 61, loss = 0.46178210\n",
      "Iteration 62, loss = 0.45990719\n",
      "Iteration 63, loss = 0.45799334\n",
      "Iteration 64, loss = 0.45619300\n",
      "Iteration 1663, loss = 0.21890409\n",
      "Iteration 65, loss = 0.45437725\n",
      "Iteration 57, loss = 0.68843601\n",
      "Iteration 642, loss = 0.33219186\n",
      "Iteration 66, loss = 0.45257274\n",
      "Iteration 911, loss = 0.30841607\n",
      "Iteration 67, loss = 0.45081818\n",
      "Iteration 68, loss = 0.44922179\n",
      "Iteration 69, loss = 0.44752716\n",
      "Iteration 1882, loss = 0.21135825\n",
      "Iteration 1733, loss = 0.16257722\n",
      "Iteration 70, loss = 0.44585705\n",
      "Iteration 71, loss = 0.44431260\n",
      "Iteration 1664, loss = 0.21877459\n",
      "Iteration 72, loss = 0.44266718\n",
      "Iteration 73, loss = 0.44116888\n",
      "Iteration 74, loss = 0.43960633\n",
      "Iteration 75, loss = 0.43815006\n",
      "Iteration 1665, loss = 0.21867046\n",
      "Iteration 812, loss = 0.33039355\n",
      "Iteration 76, loss = 0.43668020\n",
      "Iteration 1666, loss = 0.21854759\n",
      "Iteration 77, loss = 0.43527405\n",
      "Iteration 78, loss = 0.43385240\n",
      "Iteration 912, loss = 0.30817227\n",
      "Iteration 79, loss = 0.43246944\n",
      "Iteration 643, loss = 0.33200796\n",
      "Iteration 1883, loss = 0.21124416\n",
      "Iteration 80, loss = 0.43112704\n",
      "Iteration 58, loss = 0.68541965\n",
      "Iteration 81, loss = 0.42980155\n",
      "Iteration 82, loss = 0.42847574\n",
      "Iteration 83, loss = 0.42722104\n",
      "Iteration 84, loss = 0.42593294\n",
      "Iteration 813, loss = 0.33029848\n",
      "Iteration 85, loss = 0.42472693\n",
      "Iteration 644, loss = 0.33185456\n",
      "Iteration 86, loss = 0.42348680\n",
      "Iteration 87, loss = 0.42228316\n",
      "Iteration 913, loss = 0.30806711\n",
      "Iteration 88, loss = 0.42111295Iteration 1734, loss = 0.16256583\n",
      "Iteration 814, loss = 0.33026271\n",
      "Iteration 1667, loss = 0.21841995\n",
      "\n",
      "Iteration 89, loss = 0.41997677\n",
      "Iteration 90, loss = 0.41881944\n",
      "Iteration 1884, loss = 0.21117158\n",
      "Iteration 91, loss = 0.41769184\n",
      "Iteration 645, loss = 0.33180902\n",
      "Iteration 92, loss = 0.41660447\n",
      "Iteration 93, loss = 0.41552976\n",
      "Iteration 94, loss = 0.41441660\n",
      "Iteration 1668, loss = 0.21833525\n",
      "Iteration 95, loss = 0.41340654\n",
      "Iteration 96, loss = 0.41237899\n",
      "Iteration 97, loss = 0.41132643\n",
      "Iteration 98, loss = 0.41035779\n",
      "Iteration 815, loss = 0.33006808\n",
      "Iteration 1885, loss = 0.21101918\n",
      "Iteration 99, loss = 0.40937575\n",
      "Iteration 100, loss = 0.40839714\n",
      "Iteration 59, loss = 0.68227804\n",
      "Iteration 101, loss = 0.40746064\n",
      "Iteration 102, loss = 0.40649051\n",
      "Iteration 103, loss = 0.40558165\n",
      "Iteration 104, loss = 0.40466300\n",
      "Iteration 105, loss = 0.40376194\n",
      "Iteration 106, loss = 0.40287826\n",
      "Iteration 107, loss = 0.40199689\n",
      "Iteration 108, loss = 0.40114735\n",
      "Iteration 109, loss = 0.40032464\n",
      "Iteration 816, loss = 0.32997736\n",
      "Iteration 110, loss = 0.39947713\n",
      "Iteration 914, loss = 0.30791182\n",
      "Iteration 111, loss = 0.39865758\n",
      "Iteration 1735, loss = 0.16237804\n",
      "Iteration 646, loss = 0.33156464\n",
      "Iteration 112, loss = 0.39783659\n",
      "Iteration 1669, loss = 0.21815850\n",
      "Iteration 113, loss = 0.39704755\n",
      "Iteration 114, loss = 0.39628368\n",
      "Iteration 115, loss = 0.39548649\n",
      "Iteration 116, loss = 0.39471036\n",
      "Iteration 117, loss = 0.39396501\n",
      "Iteration 118, loss = 0.39323744\n",
      "Iteration 1670, loss = 0.21810825\n",
      "Iteration 1886, loss = 0.21093317\n",
      "Iteration 119, loss = 0.39246518\n",
      "Iteration 120, loss = 0.39176823\n",
      "Iteration 121, loss = 0.39106869\n",
      "Iteration 122, loss = 0.39034131\n",
      "Iteration 123, loss = 0.38964832\n",
      "Iteration 124, loss = 0.38895322\n",
      "Iteration 125, loss = 0.38828794\n",
      "Iteration 647, loss = 0.33140012\n",
      "Iteration 1671, loss = 0.21795718\n",
      "Iteration 126, loss = 0.38759930\n",
      "Iteration 127, loss = 0.38695355\n",
      "Iteration 128, loss = 0.38631794\n",
      "Iteration 129, loss = 0.38565753\n",
      "Iteration 130, loss = 0.38500371\n",
      "Iteration 131, loss = 0.38440522\n",
      "Iteration 132, loss = 0.38376074\n",
      "Iteration 1887, loss = 0.21088858\n",
      "Iteration 1672, loss = 0.21781667\n",
      "Iteration 133, loss = 0.38315120\n",
      "Iteration 134, loss = 0.38252719\n",
      "Iteration 915, loss = 0.30761811\n",
      "Iteration 817, loss = 0.32989407\n",
      "Iteration 1736, loss = 0.16234701\n",
      "Iteration 648, loss = 0.33132349\n",
      "Iteration 135, loss = 0.38194774\n",
      "Iteration 136, loss = 0.38132727\n",
      "Iteration 1673, loss = 0.21769143\n",
      "Iteration 1888, loss = 0.21081726\n",
      "Iteration 60, loss = 0.67928984\n",
      "Iteration 649, loss = 0.33120052\n",
      "Iteration 1674, loss = 0.21756300\n",
      "Iteration 137, loss = 0.38074640\n",
      "Iteration 138, loss = 0.38016464\n",
      "Iteration 139, loss = 0.37958344\n",
      "Iteration 916, loss = 0.30745663\n",
      "Iteration 140, loss = 0.37905291\n",
      "Iteration 141, loss = 0.37844645\n",
      "Iteration 142, loss = 0.37792158Iteration 650, loss = 0.33095928\n",
      "\n",
      "Iteration 143, loss = 0.37736960\n",
      "Iteration 144, loss = 0.37680016\n",
      "Iteration 145, loss = 0.37629173\n",
      "Iteration 146, loss = 0.37573478\n",
      "Iteration 147, loss = 0.37522695\n",
      "Iteration 148, loss = 0.37469167\n",
      "Iteration 149, loss = 0.37418970\n",
      "Iteration 651, loss = 0.33075401\n",
      "Iteration 150, loss = 0.37369537\n",
      "Iteration 151, loss = 0.37314765\n",
      "Iteration 652, loss = 0.33058905\n",
      "Iteration 152, loss = 0.37268551\n",
      "Iteration 153, loss = 0.37217423\n",
      "Iteration 818, loss = 0.32981590\n",
      "Iteration 653, loss = 0.33044676\n",
      "Iteration 154, loss = 0.37170422\n",
      "Iteration 917, loss = 0.30734273\n",
      "Iteration 819, loss = 0.32965171\n",
      "Iteration 155, loss = 0.37120774\n",
      "Iteration 156, loss = 0.37074164\n",
      "Iteration 157, loss = 0.37026183\n",
      "Iteration 158, loss = 0.36978619\n",
      "Iteration 159, loss = 0.36930742\n",
      "Iteration 160, loss = 0.36884817\n",
      "Iteration 161, loss = 0.36839936\n",
      "Iteration 1737, loss = 0.16219848\n",
      "Iteration 162, loss = 0.36794236\n",
      "Iteration 163, loss = 0.36751362\n",
      "Iteration 654, loss = 0.33025392\n",
      "Iteration 164, loss = 0.36703858\n",
      "Iteration 165, loss = 0.36660514\n",
      "Iteration 820, loss = 0.32953937\n",
      "Iteration 166, loss = 0.36617971\n",
      "Iteration 167, loss = 0.36570913\n",
      "Iteration 168, loss = 0.36532145\n",
      "Iteration 169, loss = 0.36488125\n",
      "Iteration 918, loss = 0.30718506\n",
      "Iteration 1889, loss = 0.21070218\n",
      "Iteration 170, loss = 0.36446783\n",
      "Iteration 171, loss = 0.36401268\n",
      "Iteration 172, loss = 0.36361183\n",
      "Iteration 61, loss = 0.67606334\n",
      "Iteration 173, loss = 0.36320249\n",
      "Iteration 1675, loss = 0.21752333\n",
      "Iteration 174, loss = 0.36278822\n",
      "Iteration 175, loss = 0.36239090\n",
      "Iteration 821, loss = 0.32942606\n",
      "Iteration 176, loss = 0.36199555\n",
      "Iteration 177, loss = 0.36159355\n",
      "Iteration 178, loss = 0.36117796\n",
      "Iteration 1676, loss = 0.21732644\n",
      "Iteration 179, loss = 0.36076794\n",
      "Iteration 180, loss = 0.36039688\n",
      "Iteration 181, loss = 0.36001550\n",
      "Iteration 182, loss = 0.35961177\n",
      "Iteration 822, loss = 0.32936111\n",
      "Iteration 183, loss = 0.35923318\n",
      "Iteration 1677, loss = 0.21723506\n",
      "Iteration 1890, loss = 0.21054240\n",
      "Iteration 823, loss = 0.32929976\n",
      "Iteration 1678, loss = 0.21708545\n",
      "Iteration 655, loss = 0.33012698\n",
      "Iteration 1679, loss = 0.21698804\n",
      "Iteration 824, loss = 0.32913813\n",
      "Iteration 1680, loss = 0.21689779\n",
      "Iteration 184, loss = 0.35885345\n",
      "Iteration 656, loss = 0.32998149\n",
      "Iteration 1681, loss = 0.21677718\n",
      "Iteration 1891, loss = 0.21047858\n",
      "Iteration 185, loss = 0.35846624\n",
      "Iteration 186, loss = 0.35811189\n",
      "Iteration 187, loss = 0.35771840\n",
      "Iteration 188, loss = 0.35734601\n",
      "Iteration 1738, loss = 0.16216366\n",
      "Iteration 189, loss = 0.35697967\n",
      "Iteration 190, loss = 0.35661795\n",
      "Iteration 191, loss = 0.35625086\n",
      "Iteration 192, loss = 0.35589557\n",
      "Iteration 193, loss = 0.35553901\n",
      "Iteration 657, loss = 0.32987690\n",
      "Iteration 194, loss = 0.35519467\n",
      "Iteration 1682, loss = 0.21662330Iteration 195, loss = 0.35481835\n",
      "\n",
      "Iteration 196, loss = 0.35450482\n",
      "Iteration 919, loss = 0.30693330\n",
      "Iteration 62, loss = 0.67290047\n",
      "Iteration 197, loss = 0.35414912\n",
      "Iteration 198, loss = 0.35378248\n",
      "Iteration 199, loss = 0.35344776\n",
      "Iteration 658, loss = 0.32971047\n",
      "Iteration 200, loss = 0.35311231\n",
      "Iteration 201, loss = 0.35277144\n",
      "Iteration 202, loss = 0.35244489\n",
      "Iteration 203, loss = 0.35211054\n",
      "Iteration 204, loss = 0.35179202\n",
      "Iteration 205, loss = 0.35146185\n",
      "Iteration 206, loss = 0.35112629\n",
      "Iteration 1739, loss = 0.16212568\n",
      "Iteration 1892, loss = 0.21036006\n",
      "Iteration 63, loss = 0.66975125\n",
      "Iteration 207, loss = 0.35081166\n",
      "Iteration 208, loss = 0.35050280\n",
      "Iteration 209, loss = 0.35019600\n",
      "Iteration 825, loss = 0.32907371Iteration 210, loss = 0.34986640\n",
      "\n",
      "Iteration 211, loss = 0.34954323\n",
      "Iteration 212, loss = 0.34925269\n",
      "Iteration 920, loss = 0.30677842\n",
      "Iteration 213, loss = 0.34892945\n",
      "Iteration 214, loss = 0.34862563\n",
      "Iteration 215, loss = 0.34831751\n",
      "Iteration 1683, loss = 0.21658214\n",
      "Iteration 64, loss = 0.66644564\n",
      "Iteration 216, loss = 0.34801732\n",
      "Iteration 217, loss = 0.34772371\n",
      "Iteration 218, loss = 0.34742864\n",
      "Iteration 659, loss = 0.32947073\n",
      "Iteration 219, loss = 0.34711817\n",
      "Iteration 220, loss = 0.34682800\n",
      "Iteration 1893, loss = 0.21029273\n",
      "Iteration 1684, loss = 0.21638772\n",
      "Iteration 221, loss = 0.34653002\n",
      "Iteration 222, loss = 0.34624807\n",
      "Iteration 660, loss = 0.32945285\n",
      "Iteration 826, loss = 0.32895809\n",
      "Iteration 1685, loss = 0.21641167\n",
      "Iteration 1686, loss = 0.21620319\n",
      "Iteration 827, loss = 0.32880592\n",
      "Iteration 1894, loss = 0.21017367\n",
      "Iteration 223, loss = 0.34596211\n",
      "Iteration 224, loss = 0.34566143\n",
      "Iteration 225, loss = 0.34538071\n",
      "Iteration 226, loss = 0.34508771\n",
      "Iteration 661, loss = 0.32913489\n",
      "Iteration 1687, loss = 0.21606525\n",
      "Iteration 1740, loss = 0.16202400\n",
      "Iteration 227, loss = 0.34483349\n",
      "Iteration 228, loss = 0.34453491\n",
      "Iteration 1895, loss = 0.21004504\n",
      "Iteration 229, loss = 0.34425489\n",
      "Iteration 828, loss = 0.32868731\n",
      "Iteration 230, loss = 0.34399255\n",
      "Iteration 231, loss = 0.34370076\n",
      "Iteration 232, loss = 0.34342608\n",
      "Iteration 921, loss = 0.30664487\n",
      "Iteration 1688, loss = 0.21594161\n",
      "Iteration 233, loss = 0.34316730\n",
      "Iteration 662, loss = 0.32902361\n",
      "Iteration 234, loss = 0.34289676\n",
      "Iteration 235, loss = 0.34262297\n",
      "Iteration 236, loss = 0.34234615\n",
      "Iteration 237, loss = 0.34210773\n",
      "Iteration 1741, loss = 0.16192431\n",
      "Iteration 65, loss = 0.66335391\n",
      "Iteration 238, loss = 0.34182503\n",
      "Iteration 663, loss = 0.32887832\n",
      "Iteration 239, loss = 0.34155566\n",
      "Iteration 922, loss = 0.30640393\n",
      "Iteration 240, loss = 0.34129332\n",
      "Iteration 1689, loss = 0.21579836\n",
      "Iteration 829, loss = 0.32858411\n",
      "Iteration 1896, loss = 0.20992700\n",
      "Iteration 241, loss = 0.34105502\n",
      "Iteration 242, loss = 0.34078215\n",
      "Iteration 66, loss = 0.65994798\n",
      "Iteration 243, loss = 0.34053856\n",
      "Iteration 244, loss = 0.34027066\n",
      "Iteration 245, loss = 0.34001599\n",
      "Iteration 246, loss = 0.33977857\n",
      "Iteration 247, loss = 0.33951994\n",
      "Iteration 1690, loss = 0.21573577\n",
      "Iteration 248, loss = 0.33928818\n",
      "Iteration 249, loss = 0.33903287\n",
      "Iteration 250, loss = 0.33876122\n",
      "Iteration 1691, loss = 0.21555555\n",
      "Iteration 251, loss = 0.33854559\n",
      "Iteration 252, loss = 0.33828466\n",
      "Iteration 1742, loss = 0.16188765\n",
      "Iteration 253, loss = 0.33803604\n",
      "Iteration 254, loss = 0.33779893\n",
      "Iteration 255, loss = 0.33756630\n",
      "Iteration 256, loss = 0.33730915\n",
      "Iteration 1897, loss = 0.20996486\n",
      "Iteration 257, loss = 0.33708190\n",
      "Iteration 67, loss = 0.65667976\n",
      "Iteration 258, loss = 0.33683226\n",
      "Iteration 259, loss = 0.33658862\n",
      "Iteration 260, loss = 0.33636394\n",
      "Iteration 261, loss = 0.33613459\n",
      "Iteration 262, loss = 0.33588378\n",
      "Iteration 263, loss = 0.33565804\n",
      "Iteration 264, loss = 0.33542469\n",
      "Iteration 265, loss = 0.33518683\n",
      "Iteration 266, loss = 0.33495702\n",
      "Iteration 830, loss = 0.32852466\n",
      "Iteration 1692, loss = 0.21552524\n",
      "Iteration 267, loss = 0.33473060\n",
      "Iteration 268, loss = 0.33450349\n",
      "Iteration 269, loss = 0.33427853\n",
      "Iteration 1743, loss = 0.16183608\n",
      "Iteration 664, loss = 0.32887807\n",
      "Iteration 270, loss = 0.33404429\n",
      "Iteration 271, loss = 0.33382489\n",
      "Iteration 272, loss = 0.33358760\n",
      "Iteration 273, loss = 0.33337708\n",
      "Iteration 274, loss = 0.33315846\n",
      "Iteration 275, loss = 0.33293927\n",
      "Iteration 665, loss = 0.32863989\n",
      "Iteration 276, loss = 0.33270443\n",
      "Iteration 1693, loss = 0.21533994\n",
      "Iteration 277, loss = 0.33252225\n",
      "Iteration 278, loss = 0.33227752\n",
      "Iteration 279, loss = 0.33206493\n",
      "Iteration 1694, loss = 0.21518611\n",
      "Iteration 666, loss = 0.32837325\n",
      "Iteration 280, loss = 0.33186682\n",
      "Iteration 281, loss = 0.33162398\n",
      "Iteration 1744, loss = 0.16173530\n",
      "Iteration 282, loss = 0.33142979\n",
      "Iteration 283, loss = 0.33121924\n",
      "Iteration 284, loss = 0.33100604\n",
      "Iteration 285, loss = 0.33080666\n",
      "Iteration 286, loss = 0.33061235\n",
      "Iteration 1695, loss = 0.21511710\n",
      "Iteration 287, loss = 0.33040126\n",
      "Iteration 288, loss = 0.33019977\n",
      "Iteration 289, loss = 0.32998270\n",
      "Iteration 290, loss = 0.32978817\n",
      "Iteration 291, loss = 0.32958027\n",
      "Iteration 1898, loss = 0.20975633\n",
      "Iteration 292, loss = 0.32939240\n",
      "Iteration 293, loss = 0.32920303\n",
      "Iteration 667, loss = 0.32831638\n",
      "Iteration 1696, loss = 0.21513693\n",
      "Iteration 294, loss = 0.32899776\n",
      "Iteration 295, loss = 0.32880310\n",
      "Iteration 296, loss = 0.32860111\n",
      "Iteration 297, loss = 0.32840362\n",
      "Iteration 68, loss = 0.65347262\n",
      "Iteration 298, loss = 0.32822533\n",
      "Iteration 299, loss = 0.32801789\n",
      "Iteration 300, loss = 0.32783122\n",
      "Iteration 301, loss = 0.32763711\n",
      "Iteration 1697, loss = 0.21486390\n",
      "Iteration 302, loss = 0.32743660\n",
      "Iteration 303, loss = 0.32725426\n",
      "Iteration 304, loss = 0.32706997\n",
      "Iteration 668, loss = 0.32805790\n",
      "Iteration 305, loss = 0.32688662\n",
      "Iteration 831, loss = 0.32844200\n",
      "Iteration 1745, loss = 0.16173283\n",
      "Iteration 306, loss = 0.32668682\n",
      "Iteration 1698, loss = 0.21475049\n",
      "Iteration 307, loss = 0.32652097\n",
      "Iteration 308, loss = 0.32632666\n",
      "Iteration 669, loss = 0.32788771\n",
      "Iteration 309, loss = 0.32614357\n",
      "Iteration 310, loss = 0.32595512\n",
      "Iteration 1699, loss = 0.21463048\n",
      "Iteration 311, loss = 0.32579211\n",
      "Iteration 312, loss = 0.32560139\n",
      "Iteration 313, loss = 0.32540019\n",
      "Iteration 314, loss = 0.32523927\n",
      "Iteration 315, loss = 0.32505033\n",
      "Iteration 316, loss = 0.32487636\n",
      "Iteration 317, loss = 0.32469820\n",
      "Iteration 1700, loss = 0.21455926\n",
      "Iteration 1899, loss = 0.20966877\n",
      "Iteration 318, loss = 0.32452214\n",
      "Iteration 319, loss = 0.32434329\n",
      "Iteration 320, loss = 0.32417655\n",
      "Iteration 321, loss = 0.32401784\n",
      "Iteration 322, loss = 0.32382236\n",
      "Iteration 323, loss = 0.32365796\n",
      "Iteration 324, loss = 0.32348892\n",
      "Iteration 1701, loss = 0.21446359\n",
      "Iteration 923, loss = 0.30628130\n",
      "Iteration 670, loss = 0.32786087\n",
      "Iteration 325, loss = 0.32332115\n",
      "Iteration 69, loss = 0.64998006\n",
      "Iteration 326, loss = 0.32314640\n",
      "Iteration 1900, loss = 0.20958412\n",
      "Iteration 327, loss = 0.32297601\n",
      "Iteration 671, loss = 0.32755540\n",
      "Iteration 328, loss = 0.32280987Iteration 832, loss = 0.32827172\n",
      "Iteration 1702, loss = 0.21426551\n",
      "Iteration 924, loss = 0.30608085\n",
      "Iteration 672, loss = 0.32741681\n",
      "\n",
      "Iteration 329, loss = 0.32265658\n",
      "Iteration 833, loss = 0.32819241\n",
      "Iteration 1746, loss = 0.16154981\n",
      "Iteration 673, loss = 0.32722437\n",
      "Iteration 330, loss = 0.32248108\n",
      "Iteration 331, loss = 0.32230740\n",
      "Iteration 332, loss = 0.32215594\n",
      "Iteration 333, loss = 0.32198613\n",
      "Iteration 334, loss = 0.32182684\n",
      "Iteration 335, loss = 0.32166276\n",
      "Iteration 336, loss = 0.32149228\n",
      "Iteration 337, loss = 0.32136023\n",
      "Iteration 338, loss = 0.32117849\n",
      "Iteration 1703, loss = 0.21413004\n",
      "Iteration 339, loss = 0.32102060\n",
      "Iteration 340, loss = 0.32087238\n",
      "Iteration 341, loss = 0.32070392\n",
      "Iteration 1901, loss = 0.20950384\n",
      "Iteration 342, loss = 0.32054671\n",
      "Iteration 343, loss = 0.32039363\n",
      "Iteration 344, loss = 0.32023571\n",
      "Iteration 345, loss = 0.32008412\n",
      "Iteration 925, loss = 0.30591847\n",
      "Iteration 346, loss = 0.31993640\n",
      "Iteration 347, loss = 0.31978155\n",
      "Iteration 834, loss = 0.32803540\n",
      "Iteration 348, loss = 0.31962725\n",
      "Iteration 349, loss = 0.31946884\n",
      "Iteration 674, loss = 0.32709927\n",
      "Iteration 350, loss = 0.31932660\n",
      "Iteration 70, loss = 0.64671545\n",
      "Iteration 1704, loss = 0.21400976\n",
      "Iteration 351, loss = 0.31916578\n",
      "Iteration 1902, loss = 0.20941764\n",
      "Iteration 352, loss = 0.31902129\n",
      "Iteration 353, loss = 0.31887259\n",
      "Iteration 354, loss = 0.31873263\n",
      "Iteration 355, loss = 0.31858132\n",
      "Iteration 1747, loss = 0.16141612\n",
      "Iteration 356, loss = 0.31843232\n",
      "Iteration 357, loss = 0.31829027\n",
      "Iteration 926, loss = 0.30571200\n",
      "Iteration 358, loss = 0.31813949\n",
      "Iteration 835, loss = 0.32793947\n",
      "Iteration 359, loss = 0.31798621\n",
      "Iteration 1705, loss = 0.21389080Iteration 360, loss = 0.31785488\n",
      "Iteration 1903, loss = 0.20935811\n",
      "Iteration 361, loss = 0.31769800\n",
      "\n",
      "Iteration 362, loss = 0.31755972\n",
      "Iteration 363, loss = 0.31740325\n",
      "Iteration 364, loss = 0.31726995\n",
      "Iteration 365, loss = 0.31713075\n",
      "Iteration 675, loss = 0.32699808\n",
      "Iteration 366, loss = 0.31699080\n",
      "Iteration 367, loss = 0.31685238\n",
      "Iteration 368, loss = 0.31671125\n",
      "Iteration 1904, loss = 0.20912870\n",
      "Iteration 836, loss = 0.32783991\n",
      "Iteration 369, loss = 0.31659029\n",
      "Iteration 1706, loss = 0.21379416\n",
      "Iteration 370, loss = 0.31643023\n",
      "Iteration 927, loss = 0.30568823\n",
      "Iteration 371, loss = 0.31629356\n",
      "Iteration 676, loss = 0.32683062\n",
      "Iteration 71, loss = 0.64326980\n",
      "Iteration 837, loss = 0.32773914\n",
      "Iteration 1905, loss = 0.20905771\n",
      "Iteration 1707, loss = 0.21372392\n",
      "Iteration 1748, loss = 0.16139222\n",
      "Iteration 372, loss = 0.31616300\n",
      "Iteration 373, loss = 0.31602854\n",
      "Iteration 374, loss = 0.31588559\n",
      "Iteration 677, loss = 0.32656684\n",
      "Iteration 375, loss = 0.31576395\n",
      "Iteration 1906, loss = 0.20900375\n",
      "Iteration 376, loss = 0.31560917\n",
      "Iteration 377, loss = 0.31547937\n",
      "Iteration 72, loss = 0.63997007\n",
      "Iteration 838, loss = 0.32765018\n",
      "Iteration 378, loss = 0.31535388\n",
      "Iteration 928, loss = 0.30547381\n",
      "Iteration 379, loss = 0.31522889\n",
      "Iteration 380, loss = 0.31510348\n",
      "Iteration 1907, loss = 0.20901429\n",
      "Iteration 1708, loss = 0.21356353\n",
      "Iteration 381, loss = 0.31495394\n",
      "Iteration 382, loss = 0.31483028\n",
      "Iteration 839, loss = 0.32751000\n",
      "Iteration 678, loss = 0.32640961\n",
      "Iteration 383, loss = 0.31469834\n",
      "Iteration 384, loss = 0.31456047\n",
      "Iteration 385, loss = 0.31443620\n",
      "Iteration 386, loss = 0.31431782\n",
      "Iteration 387, loss = 0.31417252\n",
      "Iteration 388, loss = 0.31404320\n",
      "Iteration 1709, loss = 0.21345519\n",
      "Iteration 1908, loss = 0.20881745\n",
      "Iteration 389, loss = 0.31393054\n",
      "Iteration 1749, loss = 0.16126440\n",
      "Iteration 390, loss = 0.31380542\n",
      "Iteration 929, loss = 0.30521180\n",
      "Iteration 391, loss = 0.31367263\n",
      "Iteration 840, loss = 0.32740441\n",
      "Iteration 392, loss = 0.31353352\n",
      "Iteration 393, loss = 0.31342298\n",
      "Iteration 394, loss = 0.31330393\n",
      "Iteration 395, loss = 0.31317903\n",
      "Iteration 396, loss = 0.31306493\n",
      "Iteration 397, loss = 0.31292651\n",
      "Iteration 398, loss = 0.31280979\n",
      "Iteration 399, loss = 0.31268735\n",
      "Iteration 400, loss = 0.31256801Iteration 1710, loss = 0.21331371\n",
      "\n",
      "Iteration 679, loss = 0.32630141\n",
      "Iteration 401, loss = 0.31244367\n",
      "Iteration 1909, loss = 0.20871974\n",
      "Iteration 402, loss = 0.31231765\n",
      "Iteration 403, loss = 0.31220243\n",
      "Iteration 404, loss = 0.31207918\n",
      "Iteration 405, loss = 0.31197006\n",
      "Iteration 406, loss = 0.31185090\n",
      "Iteration 407, loss = 0.31173313\n",
      "Iteration 408, loss = 0.31162029\n",
      "Iteration 841, loss = 0.32730571\n",
      "Iteration 409, loss = 0.31149793\n",
      "Iteration 410, loss = 0.31138019\n",
      "Iteration 411, loss = 0.31128871\n",
      "Iteration 930, loss = 0.30503409\n",
      "Iteration 412, loss = 0.31114768\n",
      "Iteration 413, loss = 0.31103736\n",
      "Iteration 414, loss = 0.31093172\n",
      "Iteration 415, loss = 0.31082420\n",
      "Iteration 416, loss = 0.31069432\n",
      "Iteration 417, loss = 0.31058383\n",
      "Iteration 680, loss = 0.32610358\n",
      "Iteration 418, loss = 0.31047414\n",
      "Iteration 419, loss = 0.31036042\n",
      "Iteration 1711, loss = 0.21319677\n",
      "Iteration 420, loss = 0.31025493\n",
      "Iteration 421, loss = 0.31014913\n",
      "Iteration 422, loss = 0.31003853\n",
      "Iteration 842, loss = 0.32719782\n",
      "Iteration 423, loss = 0.30991413\n",
      "Iteration 424, loss = 0.30983152\n",
      "Iteration 425, loss = 0.30971740\n",
      "Iteration 73, loss = 0.63658731\n",
      "Iteration 1712, loss = 0.21309098\n",
      "Iteration 1910, loss = 0.20856805\n",
      "Iteration 426, loss = 0.30959424\n",
      "Iteration 427, loss = 0.30948387\n",
      "Iteration 428, loss = 0.30936958\n",
      "Iteration 681, loss = 0.32603728\n",
      "Iteration 1713, loss = 0.21307923\n",
      "Iteration 429, loss = 0.30928288\n",
      "Iteration 430, loss = 0.30916738\n",
      "Iteration 431, loss = 0.30905582\n",
      "Iteration 1714, loss = 0.21283423\n",
      "Iteration 1911, loss = 0.20853396\n",
      "Iteration 432, loss = 0.30894180\n",
      "Iteration 1750, loss = 0.16118673\n",
      "Iteration 682, loss = 0.32577646\n",
      "Iteration 433, loss = 0.30883883\n",
      "Iteration 434, loss = 0.30873991\n",
      "Iteration 435, loss = 0.30863558\n",
      "Iteration 436, loss = 0.30852387\n",
      "Iteration 437, loss = 0.30841480\n",
      "Iteration 438, loss = 0.30830224\n",
      "Iteration 439, loss = 0.30821045\n",
      "Iteration 74, loss = 0.63308701\n",
      "Iteration 843, loss = 0.32710187\n",
      "Iteration 440, loss = 0.30809642\n",
      "Iteration 441, loss = 0.30799484\n",
      "Iteration 442, loss = 0.30788461\n",
      "Iteration 931, loss = 0.30488281\n",
      "Iteration 443, loss = 0.30780296\n",
      "Iteration 1715, loss = 0.21275491\n",
      "Iteration 444, loss = 0.30768425\n",
      "Iteration 445, loss = 0.30759247\n",
      "Iteration 446, loss = 0.30748441\n",
      "Iteration 683, loss = 0.32563118\n",
      "Iteration 447, loss = 0.30739347\n",
      "Iteration 448, loss = 0.30728181\n",
      "Iteration 449, loss = 0.30718165\n",
      "Iteration 450, loss = 0.30708122\n",
      "Iteration 1751, loss = 0.16126115\n",
      "Iteration 451, loss = 0.30698017\n",
      "Iteration 452, loss = 0.30688930\n",
      "Iteration 1716, loss = 0.21267394Iteration 1912, loss = 0.20835933\n",
      "\n",
      "Iteration 453, loss = 0.30677708\n",
      "Iteration 75, loss = 0.62972007\n",
      "Iteration 1717, loss = 0.21250671\n",
      "Iteration 454, loss = 0.30668671\n",
      "Iteration 455, loss = 0.30659863\n",
      "Iteration 456, loss = 0.30649212\n",
      "Iteration 457, loss = 0.30639480Iteration 684, loss = 0.32559379\n",
      "\n",
      "Iteration 1718, loss = 0.21234175\n",
      "Iteration 458, loss = 0.30632471\n",
      "Iteration 1752, loss = 0.16101512\n",
      "Iteration 459, loss = 0.30620627\n",
      "Iteration 1913, loss = 0.20838286\n",
      "Iteration 460, loss = 0.30611071\n",
      "Iteration 1719, loss = 0.21227104\n",
      "Iteration 461, loss = 0.30601567\n",
      "Iteration 932, loss = 0.30478340\n",
      "Iteration 462, loss = 0.30592586\n",
      "Iteration 463, loss = 0.30582706\n",
      "Iteration 844, loss = 0.32700927\n",
      "Iteration 1720, loss = 0.21218220\n",
      "Iteration 685, loss = 0.32529829\n",
      "Iteration 464, loss = 0.30573140\n",
      "Iteration 465, loss = 0.30564008\n",
      "Iteration 466, loss = 0.30554187\n",
      "Iteration 467, loss = 0.30546031\n",
      "Iteration 468, loss = 0.30537127\n",
      "Iteration 686, loss = 0.32512288\n",
      "Iteration 1914, loss = 0.20816181\n",
      "Iteration 469, loss = 0.30526703\n",
      "Iteration 470, loss = 0.30518177\n",
      "Iteration 1753, loss = 0.16106436\n",
      "Iteration 471, loss = 0.30508939\n",
      "Iteration 472, loss = 0.30499134\n",
      "Iteration 1721, loss = 0.21213265\n",
      "Iteration 473, loss = 0.30490824\n",
      "Iteration 845, loss = 0.32688833\n",
      "Iteration 474, loss = 0.30480860\n",
      "Iteration 687, loss = 0.32501075\n",
      "Iteration 475, loss = 0.30472255\n",
      "Iteration 76, loss = 0.62636523\n",
      "Iteration 476, loss = 0.30463103\n",
      "Iteration 477, loss = 0.30454899\n",
      "Iteration 478, loss = 0.30445871\n",
      "Iteration 1915, loss = 0.20807150\n",
      "Iteration 479, loss = 0.30436431\n",
      "Iteration 933, loss = 0.30449459\n",
      "Iteration 480, loss = 0.30428265\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1722, loss = 0.21195309\n",
      "Iteration 1723, loss = 0.21179691\n",
      "Iteration 688, loss = 0.32482998\n",
      "Iteration 1916, loss = 0.20797759\n",
      "Iteration 1724, loss = 0.21173370\n",
      "Iteration 689, loss = 0.32473495\n",
      "Iteration 1725, loss = 0.21169143\n",
      "Iteration 690, loss = 0.32455782\n",
      "Iteration 1, loss = 0.68803453\n",
      "Iteration 2, loss = 0.68498130\n",
      "Iteration 1754, loss = 0.16084789\n",
      "Iteration 3, loss = 0.68035845\n",
      "Iteration 77, loss = 0.62300924\n",
      "Iteration 846, loss = 0.32676672\n",
      "Iteration 1726, loss = 0.21167202\n",
      "Iteration 4, loss = 0.67483659\n",
      "Iteration 1917, loss = 0.20785506\n",
      "Iteration 5, loss = 0.66843954\n",
      "Iteration 6, loss = 0.66151634\n",
      "Iteration 7, loss = 0.65495721\n",
      "Iteration 8, loss = 0.64793954\n",
      "Iteration 9, loss = 0.64111853\n",
      "Iteration 10, loss = 0.63400895\n",
      "Iteration 11, loss = 0.62752136\n",
      "Iteration 12, loss = 0.62120952\n",
      "Iteration 13, loss = 0.61459633\n",
      "Iteration 14, loss = 0.60874904\n",
      "Iteration 15, loss = 0.60274296\n",
      "Iteration 1727, loss = 0.21139236\n",
      "Iteration 16, loss = 0.59695639\n",
      "Iteration 691, loss = 0.32431207\n",
      "Iteration 17, loss = 0.59155326\n",
      "Iteration 1728, loss = 0.21119301\n",
      "Iteration 1755, loss = 0.16093025\n",
      "Iteration 18, loss = 0.58610601\n",
      "Iteration 19, loss = 0.58121995\n",
      "Iteration 1729, loss = 0.21106491\n",
      "Iteration 847, loss = 0.32666380\n",
      "Iteration 20, loss = 0.57600534\n",
      "Iteration 21, loss = 0.57106934\n",
      "Iteration 22, loss = 0.56638683\n",
      "Iteration 23, loss = 0.56185556\n",
      "Iteration 1730, loss = 0.21102906\n",
      "Iteration 1918, loss = 0.20781908\n",
      "Iteration 24, loss = 0.55747845\n",
      "Iteration 692, loss = 0.32414606\n",
      "Iteration 25, loss = 0.55309619\n",
      "Iteration 1731, loss = 0.21081618\n",
      "Iteration 26, loss = 0.54894628\n",
      "Iteration 27, loss = 0.54483349\n",
      "Iteration 28, loss = 0.54084258\n",
      "Iteration 29, loss = 0.53695632\n",
      "Iteration 934, loss = 0.30432915\n",
      "Iteration 30, loss = 0.53327525\n",
      "Iteration 31, loss = 0.52949630\n",
      "Iteration 32, loss = 0.52602382\n",
      "Iteration 33, loss = 0.52251678\n",
      "Iteration 34, loss = 0.51907870\n",
      "Iteration 693, loss = 0.32410603\n",
      "Iteration 35, loss = 0.51579995\n",
      "Iteration 78, loss = 0.61948902\n",
      "Iteration 36, loss = 0.51253324\n",
      "Iteration 37, loss = 0.50937485\n",
      "Iteration 38, loss = 0.50628403\n",
      "Iteration 39, loss = 0.50330002\n",
      "Iteration 848, loss = 0.32655433\n",
      "Iteration 1756, loss = 0.16075191\n",
      "Iteration 1919, loss = 0.20769902\n",
      "Iteration 1732, loss = 0.21074477\n",
      "Iteration 935, loss = 0.30424011\n",
      "Iteration 1920, loss = 0.20771249\n",
      "Iteration 849, loss = 0.32643631\n",
      "Iteration 79, loss = 0.61605861\n",
      "Iteration 40, loss = 0.50024405Iteration 1733, loss = 0.21060614\n",
      "\n",
      "Iteration 1757, loss = 0.16063570\n",
      "Iteration 41, loss = 0.49744596\n",
      "Iteration 42, loss = 0.49456236\n",
      "Iteration 694, loss = 0.32382329\n",
      "Iteration 43, loss = 0.49177988\n",
      "Iteration 936, loss = 0.30403921\n",
      "Iteration 850, loss = 0.32633151\n",
      "Iteration 44, loss = 0.48912803\n",
      "Iteration 45, loss = 0.48643109\n",
      "Iteration 46, loss = 0.48395455\n",
      "Iteration 1734, loss = 0.21060834\n",
      "Iteration 47, loss = 0.48133747\n",
      "Iteration 851, loss = 0.32625202\n",
      "Iteration 48, loss = 0.47888157\n",
      "Iteration 1921, loss = 0.20750397\n",
      "Iteration 937, loss = 0.30380636\n",
      "Iteration 49, loss = 0.47644410\n",
      "Iteration 80, loss = 0.61269040\n",
      "Iteration 695, loss = 0.32382416\n",
      "Iteration 1735, loss = 0.21035824\n",
      "Iteration 50, loss = 0.47407940\n",
      "Iteration 1758, loss = 0.16058426\n",
      "Iteration 696, loss = 0.32351633\n",
      "Iteration 1922, loss = 0.20740785\n",
      "Iteration 51, loss = 0.47172215\n",
      "Iteration 52, loss = 0.46943603\n",
      "Iteration 81, loss = 0.60925880\n",
      "Iteration 697, loss = 0.32343295\n",
      "Iteration 53, loss = 0.46722753\n",
      "Iteration 54, loss = 0.46503239\n",
      "Iteration 852, loss = 0.32617446\n",
      "Iteration 938, loss = 0.30366913\n",
      "Iteration 55, loss = 0.46290598\n",
      "Iteration 1736, loss = 0.21036943\n",
      "Iteration 56, loss = 0.46079769\n",
      "Iteration 57, loss = 0.45872065\n",
      "Iteration 1923, loss = 0.20731629\n",
      "Iteration 58, loss = 0.45665146\n",
      "Iteration 59, loss = 0.45471986\n",
      "Iteration 60, loss = 0.45274674\n",
      "Iteration 61, loss = 0.45082734\n",
      "Iteration 1737, loss = 0.21014361\n",
      "Iteration 853, loss = 0.32603232\n",
      "Iteration 1759, loss = 0.16050457\n",
      "Iteration 1924, loss = 0.20718119\n",
      "Iteration 62, loss = 0.44900298\n",
      "Iteration 63, loss = 0.44712231\n",
      "Iteration 939, loss = 0.30350946\n",
      "Iteration 64, loss = 0.44536217\n",
      "Iteration 65, loss = 0.44355888\n",
      "Iteration 66, loss = 0.44181195\n",
      "Iteration 698, loss = 0.32322158\n",
      "Iteration 67, loss = 0.44013650\n",
      "Iteration 82, loss = 0.60590746\n",
      "Iteration 68, loss = 0.43847580\n",
      "Iteration 69, loss = 0.43685118\n",
      "Iteration 70, loss = 0.43520328\n",
      "Iteration 854, loss = 0.32592960\n",
      "Iteration 71, loss = 0.43361232\n",
      "Iteration 72, loss = 0.43208466\n",
      "Iteration 73, loss = 0.43051557\n",
      "Iteration 74, loss = 0.42904667\n",
      "Iteration 1738, loss = 0.21018370\n",
      "Iteration 75, loss = 0.42756848\n",
      "Iteration 76, loss = 0.42609536\n",
      "Iteration 1925, loss = 0.20709880\n",
      "Iteration 699, loss = 0.32300352\n",
      "Iteration 77, loss = 0.42469814\n",
      "Iteration 700, loss = 0.32283226\n",
      "Iteration 1760, loss = 0.16054453\n",
      "Iteration 1926, loss = 0.20700128\n",
      "Iteration 855, loss = 0.32581541\n",
      "Iteration 1739, loss = 0.20992276\n",
      "Iteration 701, loss = 0.32269371\n",
      "Iteration 1927, loss = 0.20696134\n",
      "Iteration 940, loss = 0.30330570\n",
      "Iteration 78, loss = 0.42324120\n",
      "Iteration 856, loss = 0.32571552\n",
      "Iteration 1761, loss = 0.16051225\n",
      "Iteration 79, loss = 0.42191397\n",
      "Iteration 1740, loss = 0.20977320\n",
      "Iteration 80, loss = 0.42052915\n",
      "Iteration 81, loss = 0.41918736\n",
      "Iteration 83, loss = 0.60258547\n",
      "Iteration 82, loss = 0.41790724\n",
      "Iteration 702, loss = 0.32256247\n",
      "Iteration 83, loss = 0.41660784\n",
      "Iteration 1762, loss = 0.16033757\n",
      "Iteration 84, loss = 0.41538779\n",
      "Iteration 941, loss = 0.30316225\n",
      "Iteration 85, loss = 0.41408738\n",
      "Iteration 1741, loss = 0.20967998\n",
      "Iteration 857, loss = 0.32562194\n",
      "Iteration 86, loss = 0.41292985\n",
      "Iteration 87, loss = 0.41164143\n",
      "Iteration 703, loss = 0.32235118\n",
      "Iteration 88, loss = 0.41046878\n",
      "Iteration 1742, loss = 0.20963236\n",
      "Iteration 89, loss = 0.40932298\n",
      "Iteration 90, loss = 0.40818894\n",
      "Iteration 1743, loss = 0.20962201\n",
      "Iteration 91, loss = 0.40701828\n",
      "Iteration 858, loss = 0.32550571\n",
      "Iteration 84, loss = 0.59927224\n",
      "Iteration 1928, loss = 0.20681019\n",
      "Iteration 92, loss = 0.40592220Iteration 1744, loss = 0.20935093\n",
      "\n",
      "Iteration 942, loss = 0.30295236\n",
      "Iteration 93, loss = 0.40482242\n",
      "Iteration 704, loss = 0.32213695\n",
      "Iteration 94, loss = 0.40375855\n",
      "Iteration 95, loss = 0.40270135\n",
      "Iteration 85, loss = 0.59592793\n",
      "Iteration 859, loss = 0.32541281\n",
      "Iteration 96, loss = 0.40164061\n",
      "Iteration 97, loss = 0.40063428\n",
      "Iteration 98, loss = 0.39958143\n",
      "Iteration 99, loss = 0.39859726\n",
      "Iteration 100, loss = 0.39763773\n",
      "Iteration 705, loss = 0.32203804\n",
      "Iteration 101, loss = 0.39663557\n",
      "Iteration 102, loss = 0.39568300\n",
      "Iteration 103, loss = 0.39472947\n",
      "Iteration 1745, loss = 0.20922416\n",
      "Iteration 104, loss = 0.39383032\n",
      "Iteration 86, loss = 0.59262890\n",
      "Iteration 105, loss = 0.39288148\n",
      "Iteration 106, loss = 0.39200905\n",
      "Iteration 1763, loss = 0.16041214\n",
      "Iteration 107, loss = 0.39112879\n",
      "Iteration 108, loss = 0.39025523\n",
      "Iteration 706, loss = 0.32180066Iteration 943, loss = 0.30286641\n",
      "Iteration 1929, loss = 0.20677625\n",
      "\n",
      "Iteration 109, loss = 0.38939597\n",
      "Iteration 110, loss = 0.38850760\n",
      "Iteration 860, loss = 0.32528964\n",
      "Iteration 111, loss = 0.38770066\n",
      "Iteration 112, loss = 0.38687330\n",
      "Iteration 113, loss = 0.38604635\n",
      "Iteration 1746, loss = 0.20916833\n",
      "Iteration 114, loss = 0.38523891\n",
      "Iteration 115, loss = 0.38450761\n",
      "Iteration 116, loss = 0.38369093\n",
      "Iteration 1764, loss = 0.16011686\n",
      "Iteration 117, loss = 0.38293707\n",
      "Iteration 118, loss = 0.38216389\n",
      "Iteration 1747, loss = 0.20909349\n",
      "Iteration 119, loss = 0.38138369\n",
      "Iteration 120, loss = 0.38068195\n",
      "Iteration 121, loss = 0.37993913\n",
      "Iteration 1748, loss = 0.20888408\n",
      "Iteration 122, loss = 0.37923196\n",
      "Iteration 707, loss = 0.32179156\n",
      "Iteration 123, loss = 0.37848581\n",
      "Iteration 1930, loss = 0.20678683\n",
      "Iteration 1749, loss = 0.20874804\n",
      "Iteration 944, loss = 0.30270831\n",
      "Iteration 124, loss = 0.37785893\n",
      "Iteration 87, loss = 0.58925347\n",
      "Iteration 125, loss = 0.37711398\n",
      "Iteration 861, loss = 0.32515713\n",
      "Iteration 126, loss = 0.37644626\n",
      "Iteration 127, loss = 0.37578998\n",
      "Iteration 128, loss = 0.37509841\n",
      "Iteration 708, loss = 0.32146077\n",
      "Iteration 129, loss = 0.37446045\n",
      "Iteration 130, loss = 0.37382490\n",
      "Iteration 131, loss = 0.37316728\n",
      "Iteration 132, loss = 0.37253444\n",
      "Iteration 133, loss = 0.37192359\n",
      "Iteration 134, loss = 0.37126519\n",
      "Iteration 709, loss = 0.32127974\n",
      "Iteration 1931, loss = 0.20657580\n",
      "Iteration 135, loss = 0.37069006\n",
      "Iteration 1750, loss = 0.20861262\n",
      "Iteration 136, loss = 0.37009264\n",
      "Iteration 1765, loss = 0.16010046\n",
      "Iteration 137, loss = 0.36948977\n",
      "Iteration 138, loss = 0.36889654\n",
      "Iteration 139, loss = 0.36830879\n",
      "Iteration 140, loss = 0.36777508\n",
      "Iteration 141, loss = 0.36718684\n",
      "Iteration 142, loss = 0.36662210\n",
      "Iteration 945, loss = 0.30248669\n",
      "Iteration 143, loss = 0.36607908\n",
      "Iteration 862, loss = 0.32507697\n",
      "Iteration 144, loss = 0.36553376\n",
      "Iteration 145, loss = 0.36500995\n",
      "Iteration 88, loss = 0.58609143\n",
      "Iteration 146, loss = 0.36445210\n",
      "Iteration 710, loss = 0.32114437\n",
      "Iteration 1751, loss = 0.20847911\n",
      "Iteration 147, loss = 0.36392221\n",
      "Iteration 148, loss = 0.36342201\n",
      "Iteration 149, loss = 0.36291044\n",
      "Iteration 711, loss = 0.32102317\n",
      "Iteration 1752, loss = 0.20835744\n",
      "Iteration 1766, loss = 0.15994438\n",
      "Iteration 1932, loss = 0.20656834\n",
      "Iteration 863, loss = 0.32497603\n",
      "Iteration 150, loss = 0.36236723\n",
      "Iteration 946, loss = 0.30230866\n",
      "Iteration 151, loss = 0.36186892\n",
      "Iteration 152, loss = 0.36136988\n",
      "Iteration 89, loss = 0.58299823\n",
      "Iteration 153, loss = 0.36089896\n",
      "Iteration 1753, loss = 0.20821248\n",
      "Iteration 154, loss = 0.36039949\n",
      "Iteration 155, loss = 0.35993814\n",
      "Iteration 712, loss = 0.32081150\n",
      "Iteration 156, loss = 0.35940932\n",
      "Iteration 1767, loss = 0.15984955\n",
      "Iteration 1933, loss = 0.20633434\n",
      "Iteration 157, loss = 0.35897236\n",
      "Iteration 158, loss = 0.35850457\n",
      "Iteration 159, loss = 0.35801339\n",
      "Iteration 160, loss = 0.35756572\n",
      "Iteration 1754, loss = 0.20824693\n",
      "Iteration 161, loss = 0.35709237\n",
      "Iteration 864, loss = 0.32484683\n",
      "Iteration 162, loss = 0.35667027\n",
      "Iteration 947, loss = 0.30214603\n",
      "Iteration 163, loss = 0.35621658\n",
      "Iteration 164, loss = 0.35578097\n",
      "Iteration 713, loss = 0.32063047\n",
      "Iteration 165, loss = 0.35532732\n",
      "Iteration 166, loss = 0.35489799\n",
      "Iteration 90, loss = 0.57979115\n",
      "Iteration 167, loss = 0.35448250\n",
      "Iteration 168, loss = 0.35405537\n",
      "Iteration 714, loss = 0.32047419\n",
      "Iteration 169, loss = 0.35365049\n",
      "Iteration 170, loss = 0.35320823\n",
      "Iteration 171, loss = 0.35280829\n",
      "Iteration 1755, loss = 0.20803444\n",
      "Iteration 865, loss = 0.32473377\n",
      "Iteration 172, loss = 0.35239908\n",
      "Iteration 715, loss = 0.32033276\n",
      "Iteration 1756, loss = 0.20789683\n",
      "Iteration 173, loss = 0.35201933\n",
      "Iteration 174, loss = 0.35159215\n",
      "Iteration 175, loss = 0.35121516\n",
      "Iteration 948, loss = 0.30205762\n",
      "Iteration 716, loss = 0.32016146\n",
      "Iteration 866, loss = 0.32465088\n",
      "Iteration 176, loss = 0.35081400\n",
      "Iteration 1757, loss = 0.20778011\n",
      "Iteration 1934, loss = 0.20628086\n",
      "Iteration 177, loss = 0.35042868\n",
      "Iteration 178, loss = 0.35003245\n",
      "Iteration 1768, loss = 0.15987700\n",
      "Iteration 179, loss = 0.34965868\n",
      "Iteration 180, loss = 0.34931116\n",
      "Iteration 181, loss = 0.34892452\n",
      "Iteration 182, loss = 0.34857076\n",
      "Iteration 717, loss = 0.31997108\n",
      "Iteration 183, loss = 0.34818539\n",
      "Iteration 867, loss = 0.32453633\n",
      "Iteration 91, loss = 0.57686716\n",
      "Iteration 184, loss = 0.34782225\n",
      "Iteration 185, loss = 0.34747110\n",
      "Iteration 186, loss = 0.34711819\n",
      "Iteration 187, loss = 0.34679186\n",
      "Iteration 188, loss = 0.34640673\n",
      "Iteration 1758, loss = 0.20767002\n",
      "Iteration 189, loss = 0.34607994\n",
      "Iteration 718, loss = 0.31984081\n",
      "Iteration 190, loss = 0.34574213\n",
      "Iteration 191, loss = 0.34539310\n",
      "Iteration 192, loss = 0.34505919\n",
      "Iteration 1759, loss = 0.20758080\n",
      "Iteration 193, loss = 0.34472113\n",
      "Iteration 949, loss = 0.30179039\n",
      "Iteration 194, loss = 0.34439236\n",
      "Iteration 195, loss = 0.34408721\n",
      "Iteration 196, loss = 0.34376653\n",
      "Iteration 197, loss = 0.34342770\n",
      "Iteration 1760, loss = 0.20740269\n",
      "Iteration 198, loss = 0.34309858\n",
      "Iteration 1769, loss = 0.15972079\n",
      "Iteration 199, loss = 0.34278954\n",
      "Iteration 200, loss = 0.34248916\n",
      "Iteration 201, loss = 0.34216885\n",
      "Iteration 202, loss = 0.34185226\n",
      "Iteration 868, loss = 0.32442702\n",
      "Iteration 203, loss = 0.34155511\n",
      "Iteration 719, loss = 0.31962299\n",
      "Iteration 204, loss = 0.34124743\n",
      "Iteration 1935, loss = 0.20619916\n",
      "Iteration 205, loss = 0.34095660\n",
      "Iteration 1761, loss = 0.20730488\n",
      "Iteration 206, loss = 0.34065597\n",
      "Iteration 207, loss = 0.34034852\n",
      "Iteration 208, loss = 0.34006616\n",
      "Iteration 92, loss = 0.57358937\n",
      "Iteration 209, loss = 0.33978194\n",
      "Iteration 210, loss = 0.33947065\n",
      "Iteration 720, loss = 0.31950222\n",
      "Iteration 1770, loss = 0.15967165\n",
      "Iteration 211, loss = 0.33918673\n",
      "Iteration 1762, loss = 0.20729550\n",
      "Iteration 950, loss = 0.30170027\n",
      "Iteration 212, loss = 0.33890209\n",
      "Iteration 213, loss = 0.33861296\n",
      "Iteration 869, loss = 0.32432055\n",
      "Iteration 214, loss = 0.33833951\n",
      "Iteration 215, loss = 0.33805526\n",
      "Iteration 1763, loss = 0.20717669\n",
      "Iteration 216, loss = 0.33777988\n",
      "Iteration 217, loss = 0.33750575\n",
      "Iteration 721, loss = 0.31934428\n",
      "Iteration 218, loss = 0.33722956\n",
      "Iteration 219, loss = 0.33694936\n",
      "Iteration 93, loss = 0.57066546\n",
      "Iteration 1764, loss = 0.20694290\n",
      "Iteration 220, loss = 0.33671414\n",
      "Iteration 221, loss = 0.33642141\n",
      "Iteration 1765, loss = 0.20688442\n",
      "Iteration 222, loss = 0.33616684\n",
      "Iteration 1936, loss = 0.20599462\n",
      "Iteration 722, loss = 0.31914725\n",
      "Iteration 1771, loss = 0.15961473\n",
      "Iteration 223, loss = 0.33588913\n",
      "Iteration 1766, loss = 0.20680771\n",
      "Iteration 224, loss = 0.33563949\n",
      "Iteration 225, loss = 0.33539462\n",
      "Iteration 951, loss = 0.30141380\n",
      "Iteration 226, loss = 0.33511519\n",
      "Iteration 870, loss = 0.32423812\n",
      "Iteration 227, loss = 0.33486146\n",
      "Iteration 228, loss = 0.33460496\n",
      "Iteration 229, loss = 0.33437388\n",
      "Iteration 1772, loss = 0.15953144\n",
      "Iteration 1767, loss = 0.20668303\n",
      "Iteration 230, loss = 0.33410385\n",
      "Iteration 231, loss = 0.33385217\n",
      "Iteration 232, loss = 0.33361073\n",
      "Iteration 1937, loss = 0.20591492\n",
      "Iteration 233, loss = 0.33339671\n",
      "Iteration 94, loss = 0.56760789\n",
      "Iteration 234, loss = 0.33313748\n",
      "Iteration 235, loss = 0.33287391\n",
      "Iteration 723, loss = 0.31899413\n",
      "Iteration 236, loss = 0.33264043\n",
      "Iteration 237, loss = 0.33240078\n",
      "Iteration 238, loss = 0.33216834\n",
      "Iteration 1768, loss = 0.20648128\n",
      "Iteration 239, loss = 0.33194418\n",
      "Iteration 240, loss = 0.33170903\n",
      "Iteration 1938, loss = 0.20580650\n",
      "Iteration 871, loss = 0.32412677\n",
      "Iteration 1773, loss = 0.15943737\n",
      "Iteration 724, loss = 0.31878889\n",
      "Iteration 952, loss = 0.30123852\n",
      "Iteration 1769, loss = 0.20654995\n",
      "Iteration 1939, loss = 0.20573906\n",
      "Iteration 725, loss = 0.31868467\n",
      "Iteration 241, loss = 0.33147897\n",
      "Iteration 242, loss = 0.33125716\n",
      "Iteration 1770, loss = 0.20625819\n",
      "Iteration 243, loss = 0.33101900\n",
      "Iteration 872, loss = 0.32405111\n",
      "Iteration 244, loss = 0.33080213\n",
      "Iteration 245, loss = 0.33057260\n",
      "Iteration 1940, loss = 0.20562867\n",
      "Iteration 246, loss = 0.33034220\n",
      "Iteration 95, loss = 0.56473932\n",
      "Iteration 247, loss = 0.33013811\n",
      "Iteration 248, loss = 0.32991341\n",
      "Iteration 1774, loss = 0.15939580\n",
      "Iteration 726, loss = 0.31848297\n",
      "Iteration 249, loss = 0.32968446\n",
      "Iteration 953, loss = 0.30110153\n",
      "Iteration 250, loss = 0.32946010\n",
      "Iteration 873, loss = 0.32392593\n",
      "Iteration 1771, loss = 0.20623360\n",
      "Iteration 251, loss = 0.32926783\n",
      "Iteration 727, loss = 0.31840501\n",
      "Iteration 252, loss = 0.32904672\n",
      "Iteration 253, loss = 0.32883477\n",
      "Iteration 1772, loss = 0.20602078\n",
      "Iteration 1941, loss = 0.20559218\n",
      "Iteration 728, loss = 0.31815401\n",
      "Iteration 254, loss = 0.32864051\n",
      "Iteration 1773, loss = 0.20596138\n",
      "Iteration 255, loss = 0.32841026\n",
      "Iteration 954, loss = 0.30094472\n",
      "Iteration 256, loss = 0.32822219\n",
      "Iteration 729, loss = 0.31795964\n",
      "Iteration 1774, loss = 0.20586993\n",
      "Iteration 1775, loss = 0.15929219\n",
      "Iteration 1775, loss = 0.20572281\n",
      "Iteration 96, loss = 0.56178889\n",
      "Iteration 955, loss = 0.30069856\n",
      "Iteration 1776, loss = 0.20572441\n",
      "Iteration 257, loss = 0.32799547\n",
      "Iteration 258, loss = 0.32779726\n",
      "Iteration 874, loss = 0.32381291\n",
      "Iteration 259, loss = 0.32759673\n",
      "Iteration 730, loss = 0.31791265\n",
      "Iteration 260, loss = 0.32739554\n",
      "Iteration 1777, loss = 0.20547937\n",
      "Iteration 261, loss = 0.32718217\n",
      "Iteration 1942, loss = 0.20558718\n",
      "Iteration 875, loss = 0.32370635\n",
      "Iteration 262, loss = 0.32699942\n",
      "Iteration 263, loss = 0.32681480\n",
      "Iteration 1776, loss = 0.15924166\n",
      "Iteration 1778, loss = 0.20530220\n",
      "Iteration 264, loss = 0.32659366\n",
      "Iteration 265, loss = 0.32639412\n",
      "Iteration 266, loss = 0.32620796\n",
      "Iteration 267, loss = 0.32603523\n",
      "Iteration 268, loss = 0.32583413\n",
      "Iteration 731, loss = 0.31763988\n",
      "Iteration 269, loss = 0.32564401\n",
      "Iteration 876, loss = 0.32358392\n",
      "Iteration 1943, loss = 0.20552846\n",
      "Iteration 1779, loss = 0.20531407\n",
      "Iteration 270, loss = 0.32544058\n",
      "Iteration 271, loss = 0.32524834\n",
      "Iteration 272, loss = 0.32506124\n",
      "Iteration 273, loss = 0.32490019\n",
      "Iteration 274, loss = 0.32470601\n",
      "Iteration 97, loss = 0.55913713\n",
      "Iteration 275, loss = 0.32453222\n",
      "Iteration 732, loss = 0.31746408\n",
      "Iteration 276, loss = 0.32433786\n",
      "Iteration 956, loss = 0.30067974\n",
      "Iteration 277, loss = 0.32415480\n",
      "Iteration 278, loss = 0.32396976\n",
      "Iteration 1780, loss = 0.20518021\n",
      "Iteration 733, loss = 0.31729972\n",
      "Iteration 279, loss = 0.32379702\n",
      "Iteration 877, loss = 0.32348704\n",
      "Iteration 1944, loss = 0.20524224\n",
      "Iteration 280, loss = 0.32360813\n",
      "Iteration 1781, loss = 0.20507239\n",
      "Iteration 1777, loss = 0.15927637\n",
      "Iteration 281, loss = 0.32343728\n",
      "Iteration 282, loss = 0.32326699\n",
      "Iteration 283, loss = 0.32311832\n",
      "Iteration 734, loss = 0.31712137\n",
      "Iteration 284, loss = 0.32291483\n",
      "Iteration 285, loss = 0.32273798\n",
      "Iteration 98, loss = 0.55617393\n",
      "Iteration 1782, loss = 0.20484071Iteration 286, loss = 0.32256875\n",
      "\n",
      "Iteration 287, loss = 0.32239368\n",
      "Iteration 288, loss = 0.32223749\n",
      "Iteration 289, loss = 0.32208159\n",
      "Iteration 290, loss = 0.32189073\n",
      "Iteration 735, loss = 0.31698270\n",
      "Iteration 878, loss = 0.32336503\n",
      "Iteration 291, loss = 0.32174019\n",
      "Iteration 292, loss = 0.32157362\n",
      "Iteration 293, loss = 0.32139891\n",
      "Iteration 1945, loss = 0.20514485\n",
      "Iteration 294, loss = 0.32124657\n",
      "Iteration 1783, loss = 0.20473116\n",
      "Iteration 295, loss = 0.32107064\n",
      "Iteration 296, loss = 0.32092261\n",
      "Iteration 736, loss = 0.31679508\n",
      "Iteration 297, loss = 0.32076804\n",
      "Iteration 957, loss = 0.30040209\n",
      "Iteration 298, loss = 0.32060184\n",
      "Iteration 1784, loss = 0.20465688\n",
      "Iteration 737, loss = 0.31662414\n",
      "Iteration 99, loss = 0.55346586\n",
      "Iteration 299, loss = 0.32044224\n",
      "Iteration 300, loss = 0.32028478\n",
      "Iteration 1778, loss = 0.15902990\n",
      "Iteration 1946, loss = 0.20508395\n",
      "Iteration 1785, loss = 0.20450278\n",
      "Iteration 301, loss = 0.32013007\n",
      "Iteration 879, loss = 0.32328424\n",
      "Iteration 302, loss = 0.31997270\n",
      "Iteration 738, loss = 0.31650889\n",
      "Iteration 303, loss = 0.31982544\n",
      "Iteration 304, loss = 0.31967233\n",
      "Iteration 305, loss = 0.31951199\n",
      "Iteration 306, loss = 0.31937280\n",
      "Iteration 1786, loss = 0.20433814\n",
      "Iteration 307, loss = 0.31921729\n",
      "Iteration 308, loss = 0.31906124\n",
      "Iteration 309, loss = 0.31891788\n",
      "Iteration 1787, loss = 0.20431888\n",
      "Iteration 880, loss = 0.32319608\n",
      "Iteration 310, loss = 0.31877360\n",
      "Iteration 311, loss = 0.31860999\n",
      "Iteration 958, loss = 0.30018713\n",
      "Iteration 312, loss = 0.31847347Iteration 739, loss = 0.31631889\n",
      "\n",
      "Iteration 313, loss = 0.31832210\n",
      "Iteration 1788, loss = 0.20417483\n",
      "Iteration 1947, loss = 0.20501903\n",
      "Iteration 314, loss = 0.31818638\n",
      "Iteration 315, loss = 0.31803499\n",
      "Iteration 881, loss = 0.32307285\n",
      "Iteration 316, loss = 0.31789208\n",
      "Iteration 1789, loss = 0.20420753\n",
      "Iteration 100, loss = 0.55087416\n",
      "Iteration 317, loss = 0.31774108\n",
      "Iteration 318, loss = 0.31761436\n",
      "Iteration 740, loss = 0.31621602\n",
      "Iteration 319, loss = 0.31746928\n",
      "Iteration 1948, loss = 0.20489403\n",
      "Iteration 320, loss = 0.31733149\n",
      "Iteration 321, loss = 0.31719632\n",
      "Iteration 959, loss = 0.30003966\n",
      "Iteration 1779, loss = 0.15899068\n",
      "Iteration 1790, loss = 0.20396651\n",
      "Iteration 322, loss = 0.31704485\n",
      "Iteration 741, loss = 0.31600622\n",
      "Iteration 882, loss = 0.32296401\n",
      "Iteration 323, loss = 0.31692275\n",
      "Iteration 742, loss = 0.31587993\n",
      "Iteration 1791, loss = 0.20379560\n",
      "Iteration 324, loss = 0.31677156\n",
      "Iteration 1949, loss = 0.20476167\n",
      "Iteration 325, loss = 0.31665324\n",
      "Iteration 101, loss = 0.54814818\n",
      "Iteration 883, loss = 0.32289909\n",
      "Iteration 326, loss = 0.31651712\n",
      "Iteration 327, loss = 0.31636749\n",
      "Iteration 328, loss = 0.31625216\n",
      "Iteration 1950, loss = 0.20473860\n",
      "Iteration 329, loss = 0.31610610\n",
      "Iteration 743, loss = 0.31562807\n",
      "Iteration 1792, loss = 0.20368356Iteration 330, loss = 0.31597258\n",
      "\n",
      "Iteration 960, loss = 0.29983683\n",
      "Iteration 331, loss = 0.31584390\n",
      "Iteration 1951, loss = 0.20461916\n",
      "Iteration 332, loss = 0.31571132\n",
      "Iteration 1780, loss = 0.15894645\n",
      "Iteration 1793, loss = 0.20355847\n",
      "Iteration 102, loss = 0.54568778\n",
      "Iteration 744, loss = 0.31551317\n",
      "Iteration 333, loss = 0.31557439\n",
      "Iteration 884, loss = 0.32274943\n",
      "Iteration 334, loss = 0.31546014\n",
      "Iteration 335, loss = 0.31530758\n",
      "Iteration 336, loss = 0.31518395\n",
      "Iteration 337, loss = 0.31506268\n",
      "Iteration 1794, loss = 0.20351809\n",
      "Iteration 338, loss = 0.31492722\n",
      "Iteration 745, loss = 0.31533790\n",
      "Iteration 339, loss = 0.31480191\n",
      "Iteration 961, loss = 0.29990736\n",
      "Iteration 340, loss = 0.31468897\n",
      "Iteration 885, loss = 0.32265628\n",
      "Iteration 1952, loss = 0.20448368\n",
      "Iteration 341, loss = 0.31456392\n",
      "Iteration 342, loss = 0.31442344\n",
      "Iteration 343, loss = 0.31430680\n",
      "Iteration 1795, loss = 0.20337297\n",
      "Iteration 344, loss = 0.31418425\n",
      "Iteration 746, loss = 0.31515186\n",
      "Iteration 345, loss = 0.31405519\n",
      "Iteration 346, loss = 0.31393826\n",
      "Iteration 1781, loss = 0.15883486\n",
      "Iteration 347, loss = 0.31381796\n",
      "Iteration 348, loss = 0.31368658\n",
      "Iteration 886, loss = 0.32252499\n",
      "Iteration 349, loss = 0.31356640\n",
      "Iteration 350, loss = 0.31344870\n",
      "Iteration 962, loss = 0.29984188\n",
      "Iteration 1953, loss = 0.20443657\n",
      "Iteration 351, loss = 0.31335024\n",
      "Iteration 1796, loss = 0.20331789\n",
      "Iteration 103, loss = 0.54309627\n",
      "Iteration 887, loss = 0.32247064\n",
      "Iteration 352, loss = 0.31320492\n",
      "Iteration 1797, loss = 0.20306490\n",
      "Iteration 353, loss = 0.31308583\n",
      "Iteration 1954, loss = 0.20426557\n",
      "Iteration 354, loss = 0.31297134\n",
      "Iteration 355, loss = 0.31285356\n",
      "Iteration 1798, loss = 0.20302091\n",
      "Iteration 747, loss = 0.31496687\n",
      "Iteration 356, loss = 0.31273139\n",
      "Iteration 357, loss = 0.31263008\n",
      "Iteration 1799, loss = 0.20288577\n",
      "Iteration 358, loss = 0.31250594\n",
      "Iteration 963, loss = 0.29939806\n",
      "Iteration 359, loss = 0.31237512\n",
      "Iteration 360, loss = 0.31225939\n",
      "Iteration 888, loss = 0.32230278\n",
      "Iteration 361, loss = 0.31215126\n",
      "Iteration 748, loss = 0.31479067\n",
      "Iteration 362, loss = 0.31203998\n",
      "Iteration 1955, loss = 0.20429969\n",
      "Iteration 363, loss = 0.31191405\n",
      "Iteration 1782, loss = 0.15880167\n",
      "Iteration 364, loss = 0.31180510\n",
      "Iteration 365, loss = 0.31169220\n",
      "Iteration 104, loss = 0.54065362\n",
      "Iteration 366, loss = 0.31158263\n",
      "Iteration 367, loss = 0.31147334\n",
      "Iteration 1800, loss = 0.20282762\n",
      "Iteration 368, loss = 0.31136418\n",
      "Iteration 1956, loss = 0.20411813\n",
      "Iteration 749, loss = 0.31462114\n",
      "Iteration 964, loss = 0.29918852\n",
      "Iteration 369, loss = 0.31123889\n",
      "Iteration 370, loss = 0.31112427\n",
      "Iteration 371, loss = 0.31101953\n",
      "Iteration 889, loss = 0.32221552\n",
      "Iteration 372, loss = 0.31091820\n",
      "Iteration 373, loss = 0.31080045\n",
      "Iteration 750, loss = 0.31458165\n",
      "Iteration 374, loss = 0.31070268\n",
      "Iteration 375, loss = 0.31060880\n",
      "Iteration 376, loss = 0.31048126\n",
      "Iteration 1801, loss = 0.20275630\n",
      "Iteration 105, loss = 0.53826112\n",
      "Iteration 377, loss = 0.31037100\n",
      "Iteration 751, loss = 0.31427276\n",
      "Iteration 378, loss = 0.31025666\n",
      "Iteration 1802, loss = 0.20256432\n",
      "Iteration 379, loss = 0.31016124\n",
      "Iteration 380, loss = 0.31005296\n",
      "Iteration 890, loss = 0.32210468\n",
      "Iteration 752, loss = 0.31416478\n",
      "Iteration 965, loss = 0.29909586\n",
      "Iteration 381, loss = 0.30994483\n",
      "Iteration 1783, loss = 0.15876481\n",
      "Iteration 1957, loss = 0.20400886\n",
      "Iteration 382, loss = 0.30984543\n",
      "Iteration 1803, loss = 0.20239576\n",
      "Iteration 106, loss = 0.53588282\n",
      "Iteration 383, loss = 0.30974932\n",
      "Iteration 384, loss = 0.30964610\n",
      "Iteration 385, loss = 0.30956509\n",
      "Iteration 753, loss = 0.31393917\n",
      "Iteration 386, loss = 0.30943323\n",
      "Iteration 387, loss = 0.30932722\n",
      "Iteration 388, loss = 0.30922467\n",
      "Iteration 389, loss = 0.30912967\n",
      "Iteration 390, loss = 0.30902703\n",
      "Iteration 391, loss = 0.30892778\n",
      "Iteration 392, loss = 0.30881874\n",
      "Iteration 754, loss = 0.31377023\n",
      "Iteration 393, loss = 0.30873391\n",
      "Iteration 1804, loss = 0.20237867\n",
      "Iteration 394, loss = 0.30862040\n",
      "Iteration 1784, loss = 0.15862832\n",
      "Iteration 395, loss = 0.30854300\n",
      "Iteration 396, loss = 0.30843589\n",
      "Iteration 397, loss = 0.30833854\n",
      "Iteration 398, loss = 0.30824100\n",
      "Iteration 399, loss = 0.30814485\n",
      "Iteration 1958, loss = 0.20394028\n",
      "Iteration 400, loss = 0.30804809\n",
      "Iteration 401, loss = 0.30795396\n",
      "Iteration 402, loss = 0.30784591\n",
      "Iteration 1805, loss = 0.20217341\n",
      "Iteration 403, loss = 0.30775917\n",
      "Iteration 404, loss = 0.30767766\n",
      "Iteration 405, loss = 0.30758360\n",
      "Iteration 406, loss = 0.30747960\n",
      "Iteration 755, loss = 0.31376352\n",
      "Iteration 1959, loss = 0.20394589\n",
      "Iteration 407, loss = 0.30739181\n",
      "Iteration 408, loss = 0.30729526\n",
      "Iteration 409, loss = 0.30720272\n",
      "Iteration 410, loss = 0.30711858\n",
      "Iteration 411, loss = 0.30702614\n",
      "Iteration 412, loss = 0.30693396\n",
      "Iteration 413, loss = 0.30683231\n",
      "Iteration 1785, loss = 0.15854913\n",
      "Iteration 966, loss = 0.29880371\n",
      "Iteration 414, loss = 0.30674768\n",
      "Iteration 415, loss = 0.30665874\n",
      "Iteration 416, loss = 0.30656672\n",
      "Iteration 1960, loss = 0.20369630\n",
      "Iteration 756, loss = 0.31341858\n",
      "Iteration 417, loss = 0.30648692\n",
      "Iteration 418, loss = 0.30638891\n",
      "Iteration 891, loss = 0.32202027\n",
      "Iteration 419, loss = 0.30630436\n",
      "Iteration 420, loss = 0.30621634\n",
      "Iteration 421, loss = 0.30612275\n",
      "Iteration 422, loss = 0.30603815\n",
      "Iteration 423, loss = 0.30595269\n",
      "Iteration 424, loss = 0.30586069\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1786, loss = 0.15850421\n",
      "Iteration 967, loss = 0.29873794\n",
      "Iteration 892, loss = 0.32186319\n",
      "Iteration 1, loss = 0.73737791\n",
      "Iteration 757, loss = 0.31330383Iteration 1806, loss = 0.20208946\n",
      "\n",
      "Iteration 2, loss = 0.73207368\n",
      "Iteration 3, loss = 0.72407872\n",
      "Iteration 4, loss = 0.71463965\n",
      "Iteration 107, loss = 0.53356236\n",
      "Iteration 968, loss = 0.29852347\n",
      "Iteration 5, loss = 0.70414919\n",
      "Iteration 893, loss = 0.32180026\n",
      "Iteration 6, loss = 0.69334067\n",
      "Iteration 1961, loss = 0.20361168\n",
      "Iteration 7, loss = 0.68209380\n",
      "Iteration 758, loss = 0.31322494\n",
      "Iteration 1807, loss = 0.20193697\n",
      "Iteration 8, loss = 0.67160528\n",
      "Iteration 1787, loss = 0.15861341\n",
      "Iteration 9, loss = 0.66086714\n",
      "Iteration 894, loss = 0.32166586\n",
      "Iteration 10, loss = 0.65072575\n",
      "Iteration 1962, loss = 0.20354890\n",
      "Iteration 11, loss = 0.64097940\n",
      "Iteration 1808, loss = 0.20195240\n",
      "Iteration 12, loss = 0.63210745\n",
      "Iteration 13, loss = 0.62316609\n",
      "Iteration 895, loss = 0.32155458\n",
      "Iteration 969, loss = 0.29829308\n",
      "Iteration 108, loss = 0.53142567\n",
      "Iteration 1963, loss = 0.20339162\n",
      "Iteration 1809, loss = 0.20170290\n",
      "Iteration 759, loss = 0.31297564\n",
      "Iteration 14, loss = 0.61505856\n",
      "Iteration 896, loss = 0.32145924\n",
      "Iteration 1810, loss = 0.20157947\n",
      "Iteration 15, loss = 0.60724180\n",
      "Iteration 16, loss = 0.59979202\n",
      "Iteration 1964, loss = 0.20328944\n",
      "Iteration 1788, loss = 0.15830489\n",
      "Iteration 1811, loss = 0.20147583\n",
      "Iteration 17, loss = 0.59299028\n",
      "Iteration 18, loss = 0.58621319\n",
      "Iteration 897, loss = 0.32136798\n",
      "Iteration 1965, loss = 0.20320019\n",
      "Iteration 19, loss = 0.57987858\n",
      "Iteration 109, loss = 0.52924144\n",
      "Iteration 760, loss = 0.31281024\n",
      "Iteration 1789, loss = 0.15831620\n",
      "Iteration 20, loss = 0.57357186\n",
      "Iteration 898, loss = 0.32122713\n",
      "Iteration 1812, loss = 0.20136307\n",
      "Iteration 21, loss = 0.56775230\n",
      "Iteration 970, loss = 0.29810752\n",
      "Iteration 1813, loss = 0.20122612\n",
      "Iteration 22, loss = 0.56224360\n",
      "Iteration 899, loss = 0.32119930\n",
      "Iteration 23, loss = 0.55695333\n",
      "Iteration 24, loss = 0.55199745\n",
      "Iteration 25, loss = 0.54691799\n",
      "Iteration 26, loss = 0.54220223\n",
      "Iteration 110, loss = 0.52705553\n",
      "Iteration 761, loss = 0.31258438\n",
      "Iteration 1966, loss = 0.20310296\n",
      "Iteration 27, loss = 0.53768512\n",
      "Iteration 900, loss = 0.32102462\n",
      "Iteration 1790, loss = 0.15819444\n",
      "Iteration 28, loss = 0.53315664\n",
      "Iteration 1814, loss = 0.20112407\n",
      "Iteration 29, loss = 0.52894652\n",
      "Iteration 1815, loss = 0.20103006\n",
      "Iteration 111, loss = 0.52503273\n",
      "Iteration 762, loss = 0.31241560\n",
      "Iteration 30, loss = 0.52486056\n",
      "Iteration 31, loss = 0.52085671\n",
      "Iteration 32, loss = 0.51697727\n",
      "Iteration 971, loss = 0.29809738\n",
      "Iteration 1816, loss = 0.20099500\n",
      "Iteration 901, loss = 0.32091668\n",
      "Iteration 33, loss = 0.51318558\n",
      "Iteration 1967, loss = 0.20309969\n",
      "Iteration 34, loss = 0.50956963\n",
      "Iteration 35, loss = 0.50610560\n",
      "Iteration 36, loss = 0.50253672\n",
      "Iteration 1817, loss = 0.20076765\n",
      "Iteration 37, loss = 0.49918851\n",
      "Iteration 902, loss = 0.32082487\n",
      "Iteration 38, loss = 0.49598434\n",
      "Iteration 1818, loss = 0.20071962\n",
      "Iteration 39, loss = 0.49281696\n",
      "Iteration 40, loss = 0.48981758\n",
      "Iteration 763, loss = 0.31228863\n",
      "Iteration 41, loss = 0.48684595\n",
      "Iteration 1819, loss = 0.20060804\n",
      "Iteration 42, loss = 0.48384947\n",
      "Iteration 972, loss = 0.29782889\n",
      "Iteration 903, loss = 0.32071627\n",
      "Iteration 43, loss = 0.48108279\n",
      "Iteration 44, loss = 0.47829625\n",
      "Iteration 1791, loss = 0.15820743\n",
      "Iteration 1820, loss = 0.20044167\n",
      "Iteration 45, loss = 0.47555200\n",
      "Iteration 764, loss = 0.31218103\n",
      "Iteration 1968, loss = 0.20291084\n",
      "Iteration 973, loss = 0.29759186\n",
      "Iteration 112, loss = 0.52294871\n",
      "Iteration 46, loss = 0.47294533\n",
      "Iteration 47, loss = 0.47043380\n",
      "Iteration 48, loss = 0.46787483\n",
      "Iteration 1792, loss = 0.15804032\n",
      "Iteration 49, loss = 0.46539482\n",
      "Iteration 50, loss = 0.46303836\n",
      "Iteration 51, loss = 0.46068500\n",
      "Iteration 52, loss = 0.45843763\n",
      "Iteration 765, loss = 0.31190066\n",
      "Iteration 1969, loss = 0.20291474\n",
      "Iteration 1793, loss = 0.15798977\n",
      "Iteration 1821, loss = 0.20035342\n",
      "Iteration 53, loss = 0.45622306\n",
      "Iteration 904, loss = 0.32058476\n",
      "Iteration 766, loss = 0.31179169\n",
      "Iteration 1822, loss = 0.20019689\n",
      "Iteration 54, loss = 0.45398564\n",
      "Iteration 113, loss = 0.52095026\n",
      "Iteration 55, loss = 0.45185675\n",
      "Iteration 1970, loss = 0.20281413\n",
      "Iteration 56, loss = 0.44977823\n",
      "Iteration 767, loss = 0.31156666\n",
      "Iteration 1823, loss = 0.20017143\n",
      "Iteration 974, loss = 0.29759999\n",
      "Iteration 57, loss = 0.44780700\n",
      "Iteration 905, loss = 0.32046880\n",
      "Iteration 1824, loss = 0.20005206\n",
      "Iteration 58, loss = 0.44578693\n",
      "Iteration 1825, loss = 0.19993227\n",
      "Iteration 59, loss = 0.44383597\n",
      "Iteration 1794, loss = 0.15788777\n",
      "Iteration 1971, loss = 0.20264300\n",
      "Iteration 60, loss = 0.44192113\n",
      "Iteration 975, loss = 0.29736525\n",
      "Iteration 1826, loss = 0.19978591\n",
      "Iteration 61, loss = 0.44007394\n",
      "Iteration 62, loss = 0.43823051\n",
      "Iteration 768, loss = 0.31147365\n",
      "Iteration 114, loss = 0.51901944\n",
      "Iteration 63, loss = 0.43645624\n",
      "Iteration 64, loss = 0.43468551\n",
      "Iteration 1827, loss = 0.19972110\n",
      "Iteration 769, loss = 0.31128621\n",
      "Iteration 906, loss = 0.32038452\n",
      "Iteration 1972, loss = 0.20250954\n",
      "Iteration 1828, loss = 0.19958771\n",
      "Iteration 1795, loss = 0.15782990\n",
      "Iteration 65, loss = 0.43294770\n",
      "Iteration 66, loss = 0.43124944\n",
      "Iteration 976, loss = 0.29711404\n",
      "Iteration 1973, loss = 0.20243350\n",
      "Iteration 770, loss = 0.31109435\n",
      "Iteration 67, loss = 0.42962497\n",
      "Iteration 115, loss = 0.51714162\n",
      "Iteration 1829, loss = 0.19952361\n",
      "Iteration 68, loss = 0.42805103\n",
      "Iteration 69, loss = 0.42645784\n",
      "Iteration 70, loss = 0.42489558\n",
      "Iteration 907, loss = 0.32027335\n",
      "Iteration 71, loss = 0.42340134\n",
      "Iteration 771, loss = 0.31098546\n",
      "Iteration 72, loss = 0.42190758\n",
      "Iteration 73, loss = 0.42044952\n",
      "Iteration 1830, loss = 0.19934490\n",
      "Iteration 74, loss = 0.41901412\n",
      "Iteration 75, loss = 0.41761072\n",
      "Iteration 116, loss = 0.51528916\n",
      "Iteration 1974, loss = 0.20237534\n",
      "Iteration 772, loss = 0.31078839\n",
      "Iteration 76, loss = 0.41621999\n",
      "Iteration 77, loss = 0.41488408\n",
      "Iteration 78, loss = 0.41356661\n",
      "Iteration 79, loss = 0.41223964\n",
      "Iteration 80, loss = 0.41100454\n",
      "Iteration 977, loss = 0.29690685\n",
      "Iteration 908, loss = 0.32015103\n",
      "Iteration 81, loss = 0.40972376\n",
      "Iteration 1831, loss = 0.19919598\n",
      "Iteration 82, loss = 0.40850773\n",
      "Iteration 83, loss = 0.40730163\n",
      "Iteration 1796, loss = 0.15790183\n",
      "Iteration 84, loss = 0.40615317\n",
      "Iteration 773, loss = 0.31062530\n",
      "Iteration 85, loss = 0.40493903\n",
      "Iteration 86, loss = 0.40376635\n",
      "Iteration 1975, loss = 0.20226127\n",
      "Iteration 87, loss = 0.40266654\n",
      "Iteration 88, loss = 0.40160186\n",
      "Iteration 1832, loss = 0.19906117\n",
      "Iteration 774, loss = 0.31039672\n",
      "Iteration 909, loss = 0.32003812\n",
      "Iteration 775, loss = 0.31024966\n",
      "Iteration 89, loss = 0.40042908\n",
      "Iteration 978, loss = 0.29674228\n",
      "Iteration 90, loss = 0.39942103\n",
      "Iteration 91, loss = 0.39836854\n",
      "Iteration 1833, loss = 0.19897736\n",
      "Iteration 117, loss = 0.51358750\n",
      "Iteration 92, loss = 0.39735732\n",
      "Iteration 776, loss = 0.31004675\n",
      "Iteration 1797, loss = 0.15773933\n",
      "Iteration 910, loss = 0.31995695\n",
      "Iteration 93, loss = 0.39633262\n",
      "Iteration 94, loss = 0.39533999\n",
      "Iteration 979, loss = 0.29655656\n",
      "Iteration 1976, loss = 0.20212495\n",
      "Iteration 1834, loss = 0.19880285\n",
      "Iteration 95, loss = 0.39435290\n",
      "Iteration 911, loss = 0.31986227\n",
      "Iteration 96, loss = 0.39339535\n",
      "Iteration 777, loss = 0.30994781\n",
      "Iteration 97, loss = 0.39240499\n",
      "Iteration 1798, loss = 0.15768771\n",
      "Iteration 98, loss = 0.39147876\n",
      "Iteration 1835, loss = 0.19872781\n",
      "Iteration 99, loss = 0.39056710\n",
      "Iteration 1977, loss = 0.20205146\n",
      "Iteration 100, loss = 0.38969453\n",
      "Iteration 980, loss = 0.29642523\n",
      "Iteration 912, loss = 0.31971095\n",
      "Iteration 1836, loss = 0.19858239\n",
      "Iteration 101, loss = 0.38878250\n",
      "Iteration 778, loss = 0.30969888\n",
      "Iteration 118, loss = 0.51182105\n",
      "Iteration 102, loss = 0.38792706\n",
      "Iteration 103, loss = 0.38701849\n",
      "Iteration 104, loss = 0.38618924\n",
      "Iteration 1837, loss = 0.19849200\n",
      "Iteration 105, loss = 0.38536681\n",
      "Iteration 1838, loss = 0.19835094\n",
      "Iteration 106, loss = 0.38453148\n",
      "Iteration 1978, loss = 0.20206435\n",
      "Iteration 779, loss = 0.30950504\n",
      "Iteration 119, loss = 0.51004270\n",
      "Iteration 913, loss = 0.31963045\n",
      "Iteration 780, loss = 0.30934157\n",
      "Iteration 107, loss = 0.38375802\n",
      "Iteration 108, loss = 0.38290722\n",
      "Iteration 109, loss = 0.38215918\n",
      "Iteration 1799, loss = 0.15766238\n",
      "Iteration 110, loss = 0.38136183\n",
      "Iteration 111, loss = 0.38059728\n",
      "Iteration 781, loss = 0.30919284\n",
      "Iteration 112, loss = 0.37985319\n",
      "Iteration 113, loss = 0.37911148\n",
      "Iteration 120, loss = 0.50844352\n",
      "Iteration 114, loss = 0.37837001\n",
      "Iteration 981, loss = 0.29622718\n",
      "Iteration 782, loss = 0.30898760\n",
      "Iteration 115, loss = 0.37764322\n",
      "Iteration 914, loss = 0.31949592\n",
      "Iteration 1979, loss = 0.20195348\n",
      "Iteration 116, loss = 0.37697095\n",
      "Iteration 1839, loss = 0.19828542\n",
      "Iteration 117, loss = 0.37624504\n",
      "Iteration 1800, loss = 0.15748328Iteration 982, loss = 0.29608878\n",
      "\n",
      "Iteration 121, loss = 0.50685356\n",
      "Iteration 118, loss = 0.37556117\n",
      "Iteration 1840, loss = 0.19840061\n",
      "Iteration 119, loss = 0.37486365\n",
      "Iteration 1980, loss = 0.20176438\n",
      "Iteration 120, loss = 0.37421372\n",
      "Iteration 915, loss = 0.31939869\n",
      "Iteration 1841, loss = 0.19800410\n",
      "Iteration 121, loss = 0.37353509\n",
      "Iteration 122, loss = 0.37286624\n",
      "Iteration 783, loss = 0.30884253\n",
      "Iteration 123, loss = 0.37223843\n",
      "Iteration 124, loss = 0.37158538\n",
      "Iteration 122, loss = 0.50527658\n",
      "Iteration 125, loss = 0.37097825\n",
      "Iteration 784, loss = 0.30864520\n",
      "Iteration 1981, loss = 0.20167489\n",
      "Iteration 1842, loss = 0.19793726\n",
      "Iteration 916, loss = 0.31932004\n",
      "Iteration 785, loss = 0.30853444\n",
      "Iteration 126, loss = 0.37035095\n",
      "Iteration 983, loss = 0.29585804\n",
      "Iteration 127, loss = 0.36971305\n",
      "Iteration 1843, loss = 0.19778956\n",
      "Iteration 1982, loss = 0.20157036\n",
      "Iteration 123, loss = 0.50372220\n",
      "Iteration 128, loss = 0.36911447\n",
      "Iteration 1844, loss = 0.19767747\n",
      "Iteration 786, loss = 0.30829185\n",
      "Iteration 129, loss = 0.36854193\n",
      "Iteration 130, loss = 0.36791469\n",
      "Iteration 1845, loss = 0.19756844\n",
      "Iteration 131, loss = 0.36733602\n",
      "Iteration 917, loss = 0.31916958\n",
      "Iteration 132, loss = 0.36675248\n",
      "Iteration 787, loss = 0.30821718\n",
      "Iteration 984, loss = 0.29567762\n",
      "Iteration 1846, loss = 0.19771616\n",
      "Iteration 133, loss = 0.36619535\n",
      "Iteration 788, loss = 0.30801633\n",
      "Iteration 1983, loss = 0.20148347\n",
      "Iteration 134, loss = 0.36566645\n",
      "Iteration 135, loss = 0.36508428\n",
      "Iteration 124, loss = 0.50223442\n",
      "Iteration 136, loss = 0.36451993\n",
      "Iteration 137, loss = 0.36395963\n",
      "Iteration 918, loss = 0.31907010\n",
      "Iteration 138, loss = 0.36343649\n",
      "Iteration 1847, loss = 0.19730299\n",
      "Iteration 139, loss = 0.36290013\n",
      "Iteration 140, loss = 0.36237151\n",
      "Iteration 1984, loss = 0.20140061\n",
      "Iteration 141, loss = 0.36186321\n",
      "Iteration 142, loss = 0.36136077\n",
      "Iteration 1848, loss = 0.19721958\n",
      "Iteration 143, loss = 0.36086688\n",
      "Iteration 919, loss = 0.31897225\n",
      "Iteration 985, loss = 0.29554755\n",
      "Iteration 789, loss = 0.30779698\n",
      "Iteration 1849, loss = 0.19714491\n",
      "Iteration 144, loss = 0.36033036\n",
      "Iteration 1985, loss = 0.20129194\n",
      "Iteration 145, loss = 0.35983746\n",
      "Iteration 146, loss = 0.35936684\n",
      "Iteration 125, loss = 0.50072760\n",
      "Iteration 147, loss = 0.35886098\n",
      "Iteration 148, loss = 0.35839592\n",
      "Iteration 790, loss = 0.30767536\n",
      "Iteration 149, loss = 0.35793881\n",
      "Iteration 150, loss = 0.35747650\n",
      "Iteration 151, loss = 0.35699137\n",
      "Iteration 152, loss = 0.35652537\n",
      "Iteration 1850, loss = 0.19708145\n",
      "Iteration 153, loss = 0.35608933\n",
      "Iteration 791, loss = 0.30744577\n",
      "Iteration 154, loss = 0.35564569\n",
      "Iteration 920, loss = 0.31884038\n",
      "Iteration 155, loss = 0.35519178\n",
      "Iteration 156, loss = 0.35477459\n",
      "Iteration 1851, loss = 0.19685241\n",
      "Iteration 986, loss = 0.29546610\n",
      "Iteration 1986, loss = 0.20118208\n",
      "Iteration 157, loss = 0.35432388\n",
      "Iteration 158, loss = 0.35389563\n",
      "Iteration 159, loss = 0.35347375\n",
      "Iteration 126, loss = 0.49935945\n",
      "Iteration 792, loss = 0.30726027\n",
      "Iteration 160, loss = 0.35305358\n",
      "Iteration 921, loss = 0.31877624\n",
      "Iteration 161, loss = 0.35262885\n",
      "Iteration 162, loss = 0.35221390\n",
      "Iteration 1852, loss = 0.19676520\n",
      "Iteration 163, loss = 0.35182141\n",
      "Iteration 1987, loss = 0.20108589\n",
      "Iteration 164, loss = 0.35140703\n",
      "Iteration 165, loss = 0.35100713\n",
      "Iteration 166, loss = 0.35059242\n",
      "Iteration 987, loss = 0.29513852\n",
      "Iteration 1853, loss = 0.19662520\n",
      "Iteration 793, loss = 0.30711135\n",
      "Iteration 167, loss = 0.35022887\n",
      "Iteration 168, loss = 0.34982342\n",
      "Iteration 922, loss = 0.31867876\n",
      "Iteration 169, loss = 0.34945585\n",
      "Iteration 170, loss = 0.34907527\n",
      "Iteration 1988, loss = 0.20099056\n",
      "Iteration 1854, loss = 0.19652595\n",
      "Iteration 171, loss = 0.34868423\n",
      "Iteration 127, loss = 0.49799856\n",
      "Iteration 172, loss = 0.34831798\n",
      "Iteration 173, loss = 0.34795322\n",
      "Iteration 1855, loss = 0.19639665\n",
      "Iteration 794, loss = 0.30693116\n",
      "Iteration 923, loss = 0.31855542\n",
      "Iteration 174, loss = 0.34758222\n",
      "Iteration 1856, loss = 0.19624762\n",
      "Iteration 175, loss = 0.34723206\n",
      "Iteration 1801, loss = 0.15756551\n",
      "Iteration 176, loss = 0.34687930\n",
      "Iteration 924, loss = 0.31844214\n",
      "Iteration 1857, loss = 0.19611788\n",
      "Iteration 988, loss = 0.29506758\n",
      "Iteration 177, loss = 0.34650772\n",
      "Iteration 795, loss = 0.30675338\n",
      "Iteration 178, loss = 0.34613845\n",
      "Iteration 1858, loss = 0.19608612\n",
      "Iteration 1989, loss = 0.20097445\n",
      "Iteration 179, loss = 0.34580148\n",
      "Iteration 1859, loss = 0.19599785\n",
      "Iteration 796, loss = 0.30659410\n",
      "Iteration 180, loss = 0.34546780\n",
      "Iteration 181, loss = 0.34514285\n",
      "Iteration 128, loss = 0.49660118\n",
      "Iteration 182, loss = 0.34476937\n",
      "Iteration 797, loss = 0.30639917\n",
      "Iteration 925, loss = 0.31830972\n",
      "Iteration 1802, loss = 0.15742587\n",
      "Iteration 798, loss = 0.30623116\n",
      "Iteration 129, loss = 0.49528501\n",
      "Iteration 926, loss = 0.31821295\n",
      "Iteration 989, loss = 0.29482505\n",
      "Iteration 1860, loss = 0.19579317\n",
      "Iteration 183, loss = 0.34444409\n",
      "Iteration 184, loss = 0.34410189\n",
      "Iteration 1803, loss = 0.15729496\n",
      "Iteration 185, loss = 0.34379435\n",
      "Iteration 799, loss = 0.30603831\n",
      "Iteration 186, loss = 0.34343943\n",
      "Iteration 130, loss = 0.49392029\n",
      "Iteration 187, loss = 0.34313137\n",
      "Iteration 188, loss = 0.34279428\n",
      "Iteration 1861, loss = 0.19571056\n",
      "Iteration 189, loss = 0.34248341\n",
      "Iteration 190, loss = 0.34217391\n",
      "Iteration 191, loss = 0.34185548\n",
      "Iteration 1862, loss = 0.19565854\n",
      "Iteration 192, loss = 0.34155431\n",
      "Iteration 800, loss = 0.30590309\n",
      "Iteration 1990, loss = 0.20078471\n",
      "Iteration 193, loss = 0.34123929\n",
      "Iteration 927, loss = 0.31816084\n",
      "Iteration 131, loss = 0.49274401\n",
      "Iteration 194, loss = 0.34092439\n",
      "Iteration 1863, loss = 0.19547782\n",
      "Iteration 195, loss = 0.34061230\n",
      "Iteration 1804, loss = 0.15724041\n",
      "Iteration 196, loss = 0.34032042\n",
      "Iteration 197, loss = 0.34001863\n",
      "Iteration 198, loss = 0.33973427\n",
      "Iteration 1864, loss = 0.19535242\n",
      "Iteration 132, loss = 0.49152193\n",
      "Iteration 801, loss = 0.30567369\n",
      "Iteration 199, loss = 0.33941864\n",
      "Iteration 200, loss = 0.33914256\n",
      "Iteration 1991, loss = 0.20097486\n",
      "Iteration 990, loss = 0.29462892\n",
      "Iteration 1805, loss = 0.15713029\n",
      "Iteration 201, loss = 0.33887060\n",
      "Iteration 928, loss = 0.31800516\n",
      "Iteration 1865, loss = 0.19520737\n",
      "Iteration 202, loss = 0.33855925\n",
      "Iteration 203, loss = 0.33827504\n",
      "Iteration 133, loss = 0.49022553\n",
      "Iteration 204, loss = 0.33799499\n",
      "Iteration 205, loss = 0.33771267\n",
      "Iteration 206, loss = 0.33743515\n",
      "Iteration 802, loss = 0.30553183\n",
      "Iteration 207, loss = 0.33715004\n",
      "Iteration 1866, loss = 0.19508152\n",
      "Iteration 208, loss = 0.33690093\n",
      "Iteration 1867, loss = 0.19507762\n",
      "Iteration 209, loss = 0.33660817\n",
      "Iteration 1992, loss = 0.20061707\n",
      "Iteration 210, loss = 0.33635155\n",
      "Iteration 211, loss = 0.33608719\n",
      "Iteration 212, loss = 0.33580628\n",
      "Iteration 213, loss = 0.33555556\n",
      "Iteration 1806, loss = 0.15706176\n",
      "Iteration 214, loss = 0.33527906\n",
      "Iteration 803, loss = 0.30536314\n",
      "Iteration 215, loss = 0.33502665\n",
      "Iteration 1868, loss = 0.19489914\n",
      "Iteration 216, loss = 0.33475466\n",
      "Iteration 217, loss = 0.33452387\n",
      "Iteration 1993, loss = 0.20057670\n",
      "Iteration 218, loss = 0.33425926\n",
      "Iteration 219, loss = 0.33400099\n",
      "Iteration 220, loss = 0.33375097\n",
      "Iteration 804, loss = 0.30517555\n",
      "Iteration 991, loss = 0.29446946\n",
      "Iteration 929, loss = 0.31793312\n",
      "Iteration 221, loss = 0.33350741\n",
      "Iteration 1994, loss = 0.20062075\n",
      "Iteration 1869, loss = 0.19477628\n",
      "Iteration 134, loss = 0.48916323\n",
      "Iteration 222, loss = 0.33324764\n",
      "Iteration 223, loss = 0.33298437\n",
      "Iteration 805, loss = 0.30501407\n",
      "Iteration 224, loss = 0.33273670\n",
      "Iteration 1870, loss = 0.19463190\n",
      "Iteration 1807, loss = 0.15701535\n",
      "Iteration 806, loss = 0.30481057\n",
      "Iteration 1871, loss = 0.19452491\n",
      "Iteration 807, loss = 0.30468928\n",
      "Iteration 1808, loss = 0.15693971\n",
      "Iteration 992, loss = 0.29451718\n",
      "Iteration 135, loss = 0.48796225\n",
      "Iteration 808, loss = 0.30447932\n",
      "Iteration 1872, loss = 0.19443299\n",
      "Iteration 225, loss = 0.33252870\n",
      "Iteration 1995, loss = 0.20031641\n",
      "Iteration 809, loss = 0.30433788\n",
      "Iteration 226, loss = 0.33227864\n",
      "Iteration 993, loss = 0.29407592\n",
      "Iteration 1809, loss = 0.15688455\n",
      "Iteration 1873, loss = 0.19439578\n",
      "Iteration 227, loss = 0.33202857\n",
      "Iteration 930, loss = 0.31780013\n",
      "Iteration 228, loss = 0.33179282\n",
      "Iteration 1874, loss = 0.19416597\n",
      "Iteration 810, loss = 0.30433564\n",
      "Iteration 1996, loss = 0.20029950\n",
      "Iteration 229, loss = 0.33154651\n",
      "Iteration 230, loss = 0.33133047\n",
      "Iteration 231, loss = 0.33108121\n",
      "Iteration 811, loss = 0.30398843\n",
      "Iteration 232, loss = 0.33084923\n",
      "Iteration 1875, loss = 0.19412611\n",
      "Iteration 1810, loss = 0.15681188\n",
      "Iteration 233, loss = 0.33062473\n",
      "Iteration 931, loss = 0.31767936\n",
      "Iteration 234, loss = 0.33040657\n",
      "Iteration 235, loss = 0.33017312\n",
      "Iteration 236, loss = 0.32994719\n",
      "Iteration 994, loss = 0.29399622\n",
      "Iteration 237, loss = 0.32971489\n",
      "Iteration 812, loss = 0.30398413\n",
      "Iteration 1997, loss = 0.20017068\n",
      "Iteration 238, loss = 0.32951154\n",
      "Iteration 239, loss = 0.32926552\n",
      "Iteration 240, loss = 0.32906336\n",
      "Iteration 241, loss = 0.32883735\n",
      "Iteration 1876, loss = 0.19403420\n",
      "Iteration 136, loss = 0.48686482\n",
      "Iteration 242, loss = 0.32860982\n",
      "Iteration 813, loss = 0.30358762\n",
      "Iteration 1998, loss = 0.20007634\n",
      "Iteration 932, loss = 0.31755942\n",
      "Iteration 243, loss = 0.32839347\n",
      "Iteration 244, loss = 0.32817378\n",
      "Iteration 1877, loss = 0.19389527\n",
      "Iteration 245, loss = 0.32796670\n",
      "Iteration 1811, loss = 0.15677135\n",
      "Iteration 814, loss = 0.30344844\n",
      "Iteration 995, loss = 0.29376143\n",
      "Iteration 1878, loss = 0.19371437\n",
      "Iteration 246, loss = 0.32774221\n",
      "Iteration 247, loss = 0.32755814\n",
      "Iteration 248, loss = 0.32734349\n",
      "Iteration 249, loss = 0.32712470\n",
      "Iteration 137, loss = 0.48569842\n",
      "Iteration 250, loss = 0.32692661\n",
      "Iteration 933, loss = 0.31742679\n",
      "Iteration 251, loss = 0.32671377\n",
      "Iteration 252, loss = 0.32651995\n",
      "Iteration 1879, loss = 0.19367009\n",
      "Iteration 815, loss = 0.30331060\n",
      "Iteration 1999, loss = 0.19988085\n",
      "Iteration 1812, loss = 0.15669904\n",
      "Iteration 253, loss = 0.32631869\n",
      "Iteration 996, loss = 0.29358640\n",
      "Iteration 254, loss = 0.32610657\n",
      "Iteration 816, loss = 0.30303850\n",
      "Iteration 1880, loss = 0.19357713\n",
      "Iteration 1881, loss = 0.19337817\n",
      "Iteration 255, loss = 0.32590212\n",
      "Iteration 817, loss = 0.30290752\n",
      "Iteration 934, loss = 0.31735682\n",
      "Iteration 256, loss = 0.32571599\n",
      "Iteration 138, loss = 0.48458154\n",
      "Iteration 257, loss = 0.32554180\n",
      "Iteration 258, loss = 0.32531988\n",
      "Iteration 259, loss = 0.32512700\n",
      "Iteration 1882, loss = 0.19327317\n",
      "Iteration 260, loss = 0.32493120\n",
      "Iteration 2000, loss = 0.19981602\n",
      "Iteration 261, loss = 0.32474855\n",
      "Iteration 262, loss = 0.32454590\n",
      "Iteration 1883, loss = 0.19317617\n",
      "Iteration 997, loss = 0.29360532\n",
      "Iteration 263, loss = 0.32436991\n",
      "Iteration 264, loss = 0.32417022\n",
      "Iteration 265, loss = 0.32397977\n",
      "Iteration 818, loss = 0.30276191\n",
      "Iteration 266, loss = 0.32379515\n",
      "Iteration 267, loss = 0.32361268\n",
      "Iteration 1884, loss = 0.19310500\n",
      "Iteration 268, loss = 0.32342584\n",
      "Iteration 269, loss = 0.32324360\n",
      "Iteration 1813, loss = 0.15661921\n",
      "Iteration 935, loss = 0.31720807\n",
      "Iteration 270, loss = 0.32305536\n",
      "Iteration 819, loss = 0.30261729\n",
      "Iteration 271, loss = 0.32288490\n",
      "Iteration 272, loss = 0.32268753\n",
      "Iteration 1885, loss = 0.19295781\n",
      "Iteration 273, loss = 0.32250497\n",
      "Iteration 274, loss = 0.32233327\n",
      "Iteration 139, loss = 0.48356297\n",
      "Iteration 820, loss = 0.30236768\n",
      "Iteration 275, loss = 0.32216409\n",
      "Iteration 2001, loss = 0.19972474\n",
      "Iteration 276, loss = 0.32198410\n",
      "Iteration 277, loss = 0.32180683\n",
      "Iteration 278, loss = 0.32163944\n",
      "Iteration 279, loss = 0.32145000\n",
      "Iteration 280, loss = 0.32128382\n",
      "Iteration 821, loss = 0.30222859\n",
      "Iteration 281, loss = 0.32111703\n",
      "Iteration 1886, loss = 0.19278752\n",
      "Iteration 936, loss = 0.31711382\n",
      "Iteration 282, loss = 0.32095096\n",
      "Iteration 283, loss = 0.32078674\n",
      "Iteration 998, loss = 0.29324827\n",
      "Iteration 284, loss = 0.32060542\n",
      "Iteration 822, loss = 0.30198735\n",
      "Iteration 285, loss = 0.32043155\n",
      "Iteration 1887, loss = 0.19273656\n",
      "Iteration 823, loss = 0.30185940\n",
      "Iteration 286, loss = 0.32026617\n",
      "Iteration 140, loss = 0.48257426\n",
      "Iteration 1814, loss = 0.15655174\n",
      "Iteration 999, loss = 0.29303809\n",
      "Iteration 937, loss = 0.31700288\n",
      "Iteration 287, loss = 0.32011180\n",
      "Iteration 2002, loss = 0.19965654\n",
      "Iteration 1888, loss = 0.19259475\n",
      "Iteration 288, loss = 0.31994167\n",
      "Iteration 824, loss = 0.30173726\n",
      "Iteration 289, loss = 0.31976866\n",
      "Iteration 290, loss = 0.31961611\n",
      "Iteration 291, loss = 0.31944656\n",
      "Iteration 292, loss = 0.31928736\n",
      "Iteration 938, loss = 0.31689661\n",
      "Iteration 293, loss = 0.31912007\n",
      "Iteration 1889, loss = 0.19249009\n",
      "Iteration 2003, loss = 0.19954630\n",
      "Iteration 294, loss = 0.31894849\n",
      "Iteration 295, loss = 0.31879159\n",
      "Iteration 296, loss = 0.31864548\n",
      "Iteration 297, loss = 0.31848710\n",
      "Iteration 1815, loss = 0.15660961\n",
      "Iteration 939, loss = 0.31681474\n",
      "Iteration 298, loss = 0.31832878\n",
      "Iteration 1000, loss = 0.29297877\n",
      "Iteration 299, loss = 0.31817376\n",
      "Iteration 141, loss = 0.48146420\n",
      "Iteration 300, loss = 0.31800743\n",
      "Iteration 1890, loss = 0.19252436\n",
      "Iteration 825, loss = 0.30145645\n",
      "Iteration 940, loss = 0.31667716\n",
      "Iteration 301, loss = 0.31786114\n",
      "Iteration 302, loss = 0.31770753\n",
      "Iteration 1891, loss = 0.19228887\n",
      "Iteration 303, loss = 0.31754836\n",
      "Iteration 2004, loss = 0.19944629\n",
      "Iteration 142, loss = 0.48044867\n",
      "Iteration 1892, loss = 0.19209969\n",
      "Iteration 304, loss = 0.31740127\n",
      "Iteration 305, loss = 0.31722970\n",
      "Iteration 1001, loss = 0.29312222\n",
      "Iteration 306, loss = 0.31708449\n",
      "Iteration 826, loss = 0.30130174\n",
      "Iteration 307, loss = 0.31693783\n",
      "Iteration 1893, loss = 0.19198906\n",
      "Iteration 308, loss = 0.31678259\n",
      "Iteration 2005, loss = 0.19949064\n",
      "Iteration 1816, loss = 0.15640726\n",
      "Iteration 309, loss = 0.31663319\n",
      "Iteration 941, loss = 0.31656871\n",
      "Iteration 310, loss = 0.31647113\n",
      "Iteration 311, loss = 0.31633482\n",
      "Iteration 312, loss = 0.31618272\n",
      "Iteration 1894, loss = 0.19192719\n",
      "Iteration 143, loss = 0.47954822\n",
      "Iteration 2006, loss = 0.19924780\n",
      "Iteration 827, loss = 0.30114766\n",
      "Iteration 313, loss = 0.31602718\n",
      "Iteration 314, loss = 0.31588815\n",
      "Iteration 1895, loss = 0.19176291\n",
      "Iteration 315, loss = 0.31574544\n",
      "Iteration 316, loss = 0.31560056\n",
      "Iteration 317, loss = 0.31545920\n",
      "Iteration 318, loss = 0.31531191\n",
      "Iteration 319, loss = 0.31516449\n",
      "Iteration 2007, loss = 0.19911438\n",
      "Iteration 320, loss = 0.31503337\n",
      "Iteration 1002, loss = 0.29251266\n",
      "Iteration 144, loss = 0.47858917\n",
      "Iteration 942, loss = 0.31646484\n",
      "Iteration 1896, loss = 0.19167834\n",
      "Iteration 828, loss = 0.30096662\n",
      "Iteration 321, loss = 0.31487661\n",
      "Iteration 1817, loss = 0.15634385\n",
      "Iteration 322, loss = 0.31473847\n",
      "Iteration 323, loss = 0.31459743\n",
      "Iteration 943, loss = 0.31636681\n",
      "Iteration 324, loss = 0.31445972\n",
      "Iteration 325, loss = 0.31431981\n",
      "Iteration 1897, loss = 0.19154721\n",
      "Iteration 326, loss = 0.31418799\n",
      "Iteration 327, loss = 0.31404891\n",
      "Iteration 2008, loss = 0.19916792\n",
      "Iteration 328, loss = 0.31390973\n",
      "Iteration 1898, loss = 0.19141963\n",
      "Iteration 329, loss = 0.31377093\n",
      "Iteration 330, loss = 0.31364584\n",
      "Iteration 944, loss = 0.31625671\n",
      "Iteration 331, loss = 0.31350240\n",
      "Iteration 332, loss = 0.31337602\n",
      "Iteration 1899, loss = 0.19130152\n",
      "Iteration 333, loss = 0.31323228\n",
      "Iteration 334, loss = 0.31310248\n",
      "Iteration 1003, loss = 0.29247427\n",
      "Iteration 335, loss = 0.31297402\n",
      "Iteration 145, loss = 0.47751986\n",
      "Iteration 1818, loss = 0.15628521\n",
      "Iteration 336, loss = 0.31282741\n",
      "Iteration 337, loss = 0.31271225\n",
      "Iteration 2009, loss = 0.19893752\n",
      "Iteration 338, loss = 0.31257430\n",
      "Iteration 339, loss = 0.31243996\n",
      "Iteration 829, loss = 0.30091389\n",
      "Iteration 340, loss = 0.31232617\n",
      "Iteration 341, loss = 0.31218509\n",
      "Iteration 342, loss = 0.31206603\n",
      "Iteration 945, loss = 0.31612295\n",
      "Iteration 343, loss = 0.31194243\n",
      "Iteration 1004, loss = 0.29227391\n",
      "Iteration 344, loss = 0.31180147\n",
      "Iteration 830, loss = 0.30059402\n",
      "Iteration 345, loss = 0.31168303\n",
      "Iteration 346, loss = 0.31155735\n",
      "Iteration 2010, loss = 0.19897133\n",
      "Iteration 347, loss = 0.31142681\n",
      "Iteration 946, loss = 0.31603652\n",
      "Iteration 146, loss = 0.47664895\n",
      "Iteration 348, loss = 0.31130925\n",
      "Iteration 349, loss = 0.31117930\n",
      "Iteration 831, loss = 0.30037482\n",
      "Iteration 350, loss = 0.31105266\n",
      "Iteration 1819, loss = 0.15625199\n",
      "Iteration 351, loss = 0.31095030\n",
      "Iteration 1900, loss = 0.19116910\n",
      "Iteration 352, loss = 0.31081107\n",
      "Iteration 2011, loss = 0.19877101\n",
      "Iteration 1901, loss = 0.19108900\n",
      "Iteration 353, loss = 0.31068331\n",
      "Iteration 354, loss = 0.31056360\n",
      "Iteration 1902, loss = 0.19099505\n",
      "Iteration 832, loss = 0.30018912\n",
      "Iteration 1005, loss = 0.29205529\n",
      "Iteration 947, loss = 0.31591278\n",
      "Iteration 355, loss = 0.31044750\n",
      "Iteration 356, loss = 0.31033145\n",
      "Iteration 1820, loss = 0.15615227\n",
      "Iteration 357, loss = 0.31020320\n",
      "Iteration 2012, loss = 0.19867533\n",
      "Iteration 358, loss = 0.31008572\n",
      "Iteration 359, loss = 0.30998276\n",
      "Iteration 360, loss = 0.30984174\n",
      "Iteration 361, loss = 0.30972432\n",
      "Iteration 362, loss = 0.30961946\n",
      "Iteration 363, loss = 0.30949428\n",
      "Iteration 147, loss = 0.47573615\n",
      "Iteration 364, loss = 0.30937893\n",
      "Iteration 1903, loss = 0.19085642\n",
      "Iteration 948, loss = 0.31586791\n",
      "Iteration 833, loss = 0.30001906\n",
      "Iteration 365, loss = 0.30926769\n",
      "Iteration 1821, loss = 0.15609041\n",
      "Iteration 2013, loss = 0.19856684Iteration 366, loss = 0.30915047\n",
      "\n",
      "Iteration 367, loss = 0.30903322\n",
      "Iteration 1006, loss = 0.29193964\n",
      "Iteration 1904, loss = 0.19079737\n",
      "Iteration 949, loss = 0.31578181\n",
      "Iteration 834, loss = 0.29985958\n",
      "Iteration 1905, loss = 0.19076050\n",
      "Iteration 1822, loss = 0.15604355\n",
      "Iteration 368, loss = 0.30892088\n",
      "Iteration 369, loss = 0.30880117\n",
      "Iteration 370, loss = 0.30868359\n",
      "Iteration 148, loss = 0.47488904\n",
      "Iteration 371, loss = 0.30858359\n",
      "Iteration 2014, loss = 0.19850686\n",
      "Iteration 372, loss = 0.30847099\n",
      "Iteration 1906, loss = 0.19050844\n",
      "Iteration 373, loss = 0.30836376\n",
      "Iteration 374, loss = 0.30823454\n",
      "Iteration 835, loss = 0.29967309\n",
      "Iteration 950, loss = 0.31560064\n",
      "Iteration 1907, loss = 0.19045754\n",
      "Iteration 375, loss = 0.30812893\n",
      "Iteration 1908, loss = 0.19026800\n",
      "Iteration 1007, loss = 0.29172025\n",
      "Iteration 376, loss = 0.30801679\n",
      "Iteration 2015, loss = 0.19841976\n",
      "Iteration 836, loss = 0.29949376\n",
      "Iteration 377, loss = 0.30790799\n",
      "Iteration 1823, loss = 0.15593316\n",
      "Iteration 149, loss = 0.47403400\n",
      "Iteration 1909, loss = 0.19019873\n",
      "Iteration 378, loss = 0.30779558\n",
      "Iteration 379, loss = 0.30769226\n",
      "Iteration 951, loss = 0.31547572\n",
      "Iteration 380, loss = 0.30758558\n",
      "Iteration 381, loss = 0.30747141\n",
      "Iteration 382, loss = 0.30736626\n",
      "Iteration 383, loss = 0.30725935\n",
      "Iteration 1824, loss = 0.15586339\n",
      "Iteration 2016, loss = 0.19822516\n",
      "Iteration 384, loss = 0.30713909\n",
      "Iteration 1910, loss = 0.19007784\n",
      "Iteration 385, loss = 0.30704313\n",
      "Iteration 952, loss = 0.31543121\n",
      "Iteration 386, loss = 0.30692421\n",
      "Iteration 387, loss = 0.30682597\n",
      "Iteration 388, loss = 0.30670830\n",
      "Iteration 389, loss = 0.30661059\n",
      "Iteration 1008, loss = 0.29159508\n",
      "Iteration 390, loss = 0.30650708Iteration 1911, loss = 0.18992466\n",
      "\n",
      "Iteration 837, loss = 0.29931827\n",
      "Iteration 391, loss = 0.30640567\n",
      "Iteration 392, loss = 0.30628460\n",
      "Iteration 953, loss = 0.31531246\n",
      "Iteration 2017, loss = 0.19818572\n",
      "Iteration 393, loss = 0.30618340\n",
      "Iteration 150, loss = 0.47313850\n",
      "Iteration 838, loss = 0.29912963\n",
      "Iteration 1912, loss = 0.18987072\n",
      "Iteration 394, loss = 0.30607613\n",
      "Iteration 1825, loss = 0.15577502\n",
      "Iteration 395, loss = 0.30597820\n",
      "Iteration 954, loss = 0.31517458\n",
      "Iteration 1913, loss = 0.18976586\n",
      "Iteration 2018, loss = 0.19808989\n",
      "Iteration 396, loss = 0.30587542\n",
      "Iteration 839, loss = 0.29905774\n",
      "Iteration 1009, loss = 0.29136099\n",
      "Iteration 1914, loss = 0.18957582\n",
      "Iteration 397, loss = 0.30578220\n",
      "Iteration 398, loss = 0.30566376\n",
      "Iteration 840, loss = 0.29878263\n",
      "Iteration 399, loss = 0.30556935\n",
      "Iteration 400, loss = 0.30546530\n",
      "Iteration 1915, loss = 0.18952112\n",
      "Iteration 151, loss = 0.47225206\n",
      "Iteration 401, loss = 0.30536958\n",
      "Iteration 402, loss = 0.30525487\n",
      "Iteration 1010, loss = 0.29112568\n",
      "Iteration 403, loss = 0.30516738\n",
      "Iteration 1826, loss = 0.15573461\n",
      "Iteration 955, loss = 0.31504469\n",
      "Iteration 404, loss = 0.30505818\n",
      "Iteration 2019, loss = 0.19797137\n",
      "Iteration 405, loss = 0.30496317\n",
      "Iteration 1916, loss = 0.18945768\n",
      "Iteration 841, loss = 0.29854866\n",
      "Iteration 406, loss = 0.30486849\n",
      "Iteration 407, loss = 0.30476692\n",
      "Iteration 956, loss = 0.31495942\n",
      "Iteration 408, loss = 0.30466639\n",
      "Iteration 409, loss = 0.30456836\n",
      "Iteration 1917, loss = 0.18931194\n",
      "Iteration 410, loss = 0.30446962\n",
      "Iteration 1827, loss = 0.15569349\n",
      "Iteration 411, loss = 0.30438500\n",
      "Iteration 842, loss = 0.29841586\n",
      "Iteration 412, loss = 0.30428527\n",
      "Iteration 1918, loss = 0.18919293\n",
      "Iteration 413, loss = 0.30417543\n",
      "Iteration 2020, loss = 0.19787435\n",
      "Iteration 1919, loss = 0.18911745\n",
      "Iteration 152, loss = 0.47148118\n",
      "Iteration 414, loss = 0.30408652\n",
      "Iteration 957, loss = 0.31483208\n",
      "Iteration 843, loss = 0.29831540\n",
      "Iteration 1828, loss = 0.15562109\n",
      "Iteration 415, loss = 0.30398712\n",
      "Iteration 1920, loss = 0.18911110\n",
      "Iteration 1011, loss = 0.29098359\n",
      "Iteration 416, loss = 0.30388433\n",
      "Iteration 417, loss = 0.30380157\n",
      "Iteration 418, loss = 0.30369343\n",
      "Iteration 2021, loss = 0.19777678\n",
      "Iteration 419, loss = 0.30362082\n",
      "Iteration 153, loss = 0.47066067\n",
      "Iteration 420, loss = 0.30351610\n",
      "Iteration 421, loss = 0.30341459\n",
      "Iteration 422, loss = 0.30332536\n",
      "Iteration 423, loss = 0.30323177\n",
      "Iteration 424, loss = 0.30315104\n",
      "Iteration 844, loss = 0.29799199Iteration 1921, loss = 0.18881218\n",
      "\n",
      "Iteration 425, loss = 0.30304568\n",
      "Iteration 426, loss = 0.30295512\n",
      "Iteration 427, loss = 0.30287828\n",
      "Iteration 1012, loss = 0.29079749\n",
      "Iteration 428, loss = 0.30278391\n",
      "Iteration 2022, loss = 0.19776754\n",
      "Iteration 845, loss = 0.29782158\n",
      "Iteration 429, loss = 0.30267844\n",
      "Iteration 430, loss = 0.30258696\n",
      "Iteration 431, loss = 0.30249995\n",
      "Iteration 432, loss = 0.30241108\n",
      "Iteration 846, loss = 0.29764256\n",
      "Iteration 433, loss = 0.30232610\n",
      "Iteration 1013, loss = 0.29089387\n",
      "Iteration 434, loss = 0.30223230\n",
      "Iteration 435, loss = 0.30213774\n",
      "Iteration 436, loss = 0.30205209\n",
      "Iteration 958, loss = 0.31475531\n",
      "Iteration 2023, loss = 0.19757843\n",
      "Iteration 1922, loss = 0.18869956\n",
      "Iteration 437, loss = 0.30198171\n",
      "Iteration 847, loss = 0.29755633\n",
      "Iteration 438, loss = 0.30188690\n",
      "Iteration 439, loss = 0.30179464\n",
      "Iteration 154, loss = 0.46983361\n",
      "Iteration 440, loss = 0.30170567\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 848, loss = 0.29727659\n",
      "Iteration 1829, loss = 0.15555135\n",
      "Iteration 1014, loss = 0.29048914\n",
      "Iteration 1923, loss = 0.18867910\n",
      "Iteration 959, loss = 0.31470150\n",
      "Iteration 1, loss = 0.77999764\n",
      "Iteration 2, loss = 0.77576835\n",
      "Iteration 3, loss = 0.76927475\n",
      "Iteration 4, loss = 0.76161403\n",
      "Iteration 155, loss = 0.46904174\n",
      "Iteration 5, loss = 0.75327914\n",
      "Iteration 1924, loss = 0.18848051\n",
      "Iteration 2024, loss = 0.19752176\n",
      "Iteration 849, loss = 0.29708205\n",
      "Iteration 6, loss = 0.74426893\n",
      "Iteration 7, loss = 0.73521781\n",
      "Iteration 1830, loss = 0.15547976\n",
      "Iteration 850, loss = 0.29700494\n",
      "Iteration 960, loss = 0.31452429\n",
      "Iteration 8, loss = 0.72654780\n",
      "Iteration 9, loss = 0.71787566\n",
      "Iteration 1925, loss = 0.18832094\n",
      "Iteration 851, loss = 0.29670330\n",
      "Iteration 10, loss = 0.70957584\n",
      "Iteration 961, loss = 0.31437812\n",
      "Iteration 11, loss = 0.70131406\n",
      "Iteration 156, loss = 0.46836497\n",
      "Iteration 12, loss = 0.69340254\n",
      "Iteration 1831, loss = 0.15542146\n",
      "Iteration 13, loss = 0.68602710\n",
      "Iteration 1926, loss = 0.18831795\n",
      "Iteration 14, loss = 0.67852982\n",
      "Iteration 962, loss = 0.31430138\n",
      "Iteration 15, loss = 0.67193495\n",
      "Iteration 2025, loss = 0.19748223\n",
      "Iteration 852, loss = 0.29655925\n",
      "Iteration 16, loss = 0.66510926\n",
      "Iteration 17, loss = 0.65851003\n",
      "Iteration 157, loss = 0.46756077\n",
      "Iteration 18, loss = 0.65231744\n",
      "Iteration 19, loss = 0.64632964\n",
      "Iteration 20, loss = 0.64035117\n",
      "Iteration 963, loss = 0.31415414\n",
      "Iteration 1015, loss = 0.29022340\n",
      "Iteration 21, loss = 0.63469019\n",
      "Iteration 853, loss = 0.29637607\n",
      "Iteration 22, loss = 0.62921432\n",
      "Iteration 23, loss = 0.62375581\n",
      "Iteration 24, loss = 0.61847153\n",
      "Iteration 2026, loss = 0.19731066\n",
      "Iteration 25, loss = 0.61338857\n",
      "Iteration 854, loss = 0.29616930\n",
      "Iteration 964, loss = 0.31405986\n",
      "Iteration 1832, loss = 0.15543351\n",
      "Iteration 26, loss = 0.60839799\n",
      "Iteration 27, loss = 0.60339646\n",
      "Iteration 1927, loss = 0.18812717\n",
      "Iteration 855, loss = 0.29594279\n",
      "Iteration 28, loss = 0.59873385\n",
      "Iteration 2027, loss = 0.19726594\n",
      "Iteration 1016, loss = 0.29008120\n",
      "Iteration 29, loss = 0.59394684\n",
      "Iteration 856, loss = 0.29578150\n",
      "Iteration 30, loss = 0.58937820\n",
      "Iteration 31, loss = 0.58490649\n",
      "Iteration 158, loss = 0.46685021\n",
      "Iteration 965, loss = 0.31393016\n",
      "Iteration 32, loss = 0.58044229\n",
      "Iteration 1833, loss = 0.15533430\n",
      "Iteration 33, loss = 0.57612750\n",
      "Iteration 34, loss = 0.57182266\n",
      "Iteration 857, loss = 0.29575225\n",
      "Iteration 35, loss = 0.56763377\n",
      "Iteration 2028, loss = 0.19724581\n",
      "Iteration 1017, loss = 0.28990030\n",
      "Iteration 36, loss = 0.56354096\n",
      "Iteration 858, loss = 0.29552661\n",
      "Iteration 37, loss = 0.55948367\n",
      "Iteration 966, loss = 0.31385836\n",
      "Iteration 38, loss = 0.55548682\n",
      "Iteration 39, loss = 0.55154794\n",
      "Iteration 40, loss = 0.54761407\n",
      "Iteration 859, loss = 0.29522748\n",
      "Iteration 1018, loss = 0.28970412\n",
      "Iteration 41, loss = 0.54378231\n",
      "Iteration 1834, loss = 0.15522379\n",
      "Iteration 2029, loss = 0.19701499\n",
      "Iteration 42, loss = 0.53998985\n",
      "Iteration 967, loss = 0.31374141\n",
      "Iteration 860, loss = 0.29510103\n",
      "Iteration 43, loss = 0.53636429\n",
      "Iteration 44, loss = 0.53271114\n",
      "Iteration 159, loss = 0.46610496\n",
      "Iteration 45, loss = 0.52908067\n",
      "Iteration 46, loss = 0.52559374\n",
      "Iteration 47, loss = 0.52217723\n",
      "Iteration 861, loss = 0.29486749\n",
      "Iteration 48, loss = 0.51881169\n",
      "Iteration 49, loss = 0.51535959\n",
      "Iteration 50, loss = 0.51207119\n",
      "Iteration 51, loss = 0.50890294\n",
      "Iteration 1835, loss = 0.15515563\n",
      "Iteration 52, loss = 0.50571375\n",
      "Iteration 53, loss = 0.50255716\n",
      "Iteration 2030, loss = 0.19691418\n",
      "Iteration 54, loss = 0.49957882\n",
      "Iteration 160, loss = 0.46535854\n",
      "Iteration 862, loss = 0.29474695\n",
      "Iteration 55, loss = 0.49657760\n",
      "Iteration 968, loss = 0.31372490\n",
      "Iteration 56, loss = 0.49373894\n",
      "Iteration 1019, loss = 0.28952762\n",
      "Iteration 57, loss = 0.49082937\n",
      "Iteration 58, loss = 0.48794919\n",
      "Iteration 59, loss = 0.48518502\n",
      "Iteration 60, loss = 0.48232752\n",
      "Iteration 969, loss = 0.31355116\n",
      "Iteration 863, loss = 0.29450213\n",
      "Iteration 61, loss = 0.47988333\n",
      "Iteration 2031, loss = 0.19680315\n",
      "Iteration 1836, loss = 0.15521703\n",
      "Iteration 161, loss = 0.46468595\n",
      "Iteration 62, loss = 0.47712706\n",
      "Iteration 63, loss = 0.47454599\n",
      "Iteration 64, loss = 0.47192577\n",
      "Iteration 65, loss = 0.46952384\n",
      "Iteration 66, loss = 0.46712077\n",
      "Iteration 67, loss = 0.46468886\n",
      "Iteration 68, loss = 0.46225726\n",
      "Iteration 970, loss = 0.31338228\n",
      "Iteration 69, loss = 0.46005361\n",
      "Iteration 70, loss = 0.45776573\n",
      "Iteration 71, loss = 0.45558460\n",
      "Iteration 72, loss = 0.45350344\n",
      "Iteration 2032, loss = 0.19672783\n",
      "Iteration 73, loss = 0.45134145\n",
      "Iteration 74, loss = 0.44931643\n",
      "Iteration 1020, loss = 0.28940781\n",
      "Iteration 971, loss = 0.31327068\n",
      "Iteration 75, loss = 0.44720322\n",
      "Iteration 76, loss = 0.44519106\n",
      "Iteration 864, loss = 0.29432711\n",
      "Iteration 77, loss = 0.44337126\n",
      "Iteration 78, loss = 0.44141724\n",
      "Iteration 79, loss = 0.43959352\n",
      "Iteration 1837, loss = 0.15502176\n",
      "Iteration 2033, loss = 0.19661274\n",
      "Iteration 80, loss = 0.43776207\n",
      "Iteration 81, loss = 0.43596313\n",
      "Iteration 865, loss = 0.29415339\n",
      "Iteration 82, loss = 0.43424147\n",
      "Iteration 162, loss = 0.46398487\n",
      "Iteration 83, loss = 0.43262586\n",
      "Iteration 84, loss = 0.43082795\n",
      "Iteration 1021, loss = 0.28918884\n",
      "Iteration 972, loss = 0.31320685\n",
      "Iteration 85, loss = 0.42930533\n",
      "Iteration 866, loss = 0.29389096\n",
      "Iteration 86, loss = 0.42771092\n",
      "Iteration 87, loss = 0.42614525\n",
      "Iteration 88, loss = 0.42464527\n",
      "Iteration 2034, loss = 0.19651409\n",
      "Iteration 1838, loss = 0.15493472\n",
      "Iteration 89, loss = 0.42314850\n",
      "Iteration 163, loss = 0.46326684\n",
      "Iteration 90, loss = 0.42165081\n",
      "Iteration 91, loss = 0.42021505\n",
      "Iteration 2035, loss = 0.19643644\n",
      "Iteration 973, loss = 0.31311379\n",
      "Iteration 92, loss = 0.41883043\n",
      "Iteration 93, loss = 0.41743673\n",
      "Iteration 1022, loss = 0.28902627\n",
      "Iteration 94, loss = 0.41611206\n",
      "Iteration 95, loss = 0.41475549\n",
      "Iteration 867, loss = 0.29377878\n",
      "Iteration 2036, loss = 0.19633393\n",
      "Iteration 96, loss = 0.41346734\n",
      "Iteration 97, loss = 0.41222512\n",
      "Iteration 1839, loss = 0.15491563\n",
      "Iteration 98, loss = 0.41100524\n",
      "Iteration 99, loss = 0.40973125\n",
      "Iteration 100, loss = 0.40860692\n",
      "Iteration 868, loss = 0.29379636\n",
      "Iteration 101, loss = 0.40738862\n",
      "Iteration 2037, loss = 0.19623927\n",
      "Iteration 102, loss = 0.40627503\n",
      "Iteration 1023, loss = 0.28893563\n",
      "Iteration 103, loss = 0.40514547\n",
      "Iteration 974, loss = 0.31307578\n",
      "Iteration 104, loss = 0.40406776\n",
      "Iteration 164, loss = 0.46260157\n",
      "Iteration 105, loss = 0.40297976\n",
      "Iteration 106, loss = 0.40194365\n",
      "Iteration 107, loss = 0.40093968\n",
      "Iteration 2038, loss = 0.19617575\n",
      "Iteration 869, loss = 0.29345456\n",
      "Iteration 1840, loss = 0.15487856\n",
      "Iteration 108, loss = 0.39988310\n",
      "Iteration 109, loss = 0.39889347\n",
      "Iteration 110, loss = 0.39788694\n",
      "Iteration 111, loss = 0.39698757\n",
      "Iteration 1024, loss = 0.28867112\n",
      "Iteration 975, loss = 0.31284523\n",
      "Iteration 112, loss = 0.39604865\n",
      "Iteration 1928, loss = 0.18800809\n",
      "Iteration 113, loss = 0.39514084\n",
      "Iteration 2039, loss = 0.19624022\n",
      "Iteration 870, loss = 0.29314789\n",
      "Iteration 114, loss = 0.39424653\n",
      "Iteration 1929, loss = 0.18791514\n",
      "Iteration 115, loss = 0.39335587\n",
      "Iteration 116, loss = 0.39243948\n",
      "Iteration 117, loss = 0.39158664\n",
      "Iteration 976, loss = 0.31273930\n",
      "Iteration 118, loss = 0.39076709\n",
      "Iteration 165, loss = 0.46193440\n",
      "Iteration 1930, loss = 0.18785520\n",
      "Iteration 119, loss = 0.38993322\n",
      "Iteration 1841, loss = 0.15476308\n",
      "Iteration 2040, loss = 0.19597607\n",
      "Iteration 120, loss = 0.38916256\n",
      "Iteration 166, loss = 0.46134532\n",
      "Iteration 121, loss = 0.38834022\n",
      "Iteration 122, loss = 0.38753909\n",
      "Iteration 871, loss = 0.29298831\n",
      "Iteration 2041, loss = 0.19585643\n",
      "Iteration 977, loss = 0.31262605\n",
      "Iteration 1931, loss = 0.18773781\n",
      "Iteration 872, loss = 0.29286421\n",
      "Iteration 1842, loss = 0.15476223\n",
      "Iteration 1025, loss = 0.28881697\n",
      "Iteration 1932, loss = 0.18764532\n",
      "Iteration 123, loss = 0.38673975\n",
      "Iteration 124, loss = 0.38599796\n",
      "Iteration 2042, loss = 0.19575819\n",
      "Iteration 125, loss = 0.38522224\n",
      "Iteration 167, loss = 0.46069461\n",
      "Iteration 978, loss = 0.31251588\n",
      "Iteration 126, loss = 0.38451040\n",
      "Iteration 127, loss = 0.38378948\n",
      "Iteration 128, loss = 0.38302557\n",
      "Iteration 873, loss = 0.29260121\n",
      "Iteration 1933, loss = 0.18746705\n",
      "Iteration 129, loss = 0.38234820\n",
      "Iteration 130, loss = 0.38166142\n",
      "Iteration 1843, loss = 0.15465659\n",
      "Iteration 2043, loss = 0.19582038\n",
      "Iteration 131, loss = 0.38099365\n",
      "Iteration 132, loss = 0.38030373\n",
      "Iteration 1934, loss = 0.18734492\n",
      "Iteration 1026, loss = 0.28834113\n",
      "Iteration 133, loss = 0.37969157\n",
      "Iteration 979, loss = 0.31238017\n",
      "Iteration 134, loss = 0.37897342\n",
      "Iteration 874, loss = 0.29261491\n",
      "Iteration 135, loss = 0.37836684\n",
      "Iteration 168, loss = 0.46010183\n",
      "Iteration 136, loss = 0.37776461\n",
      "Iteration 137, loss = 0.37713067\n",
      "Iteration 138, loss = 0.37656067\n",
      "Iteration 139, loss = 0.37593108\n",
      "Iteration 1935, loss = 0.18725881\n",
      "Iteration 140, loss = 0.37535871\n",
      "Iteration 141, loss = 0.37474268\n",
      "Iteration 2044, loss = 0.19556425\n",
      "Iteration 142, loss = 0.37414647\n",
      "Iteration 143, loss = 0.37359981\n",
      "Iteration 1936, loss = 0.18714515\n",
      "Iteration 144, loss = 0.37305725\n",
      "Iteration 875, loss = 0.29235932\n",
      "Iteration 145, loss = 0.37248324\n",
      "Iteration 146, loss = 0.37192802\n",
      "Iteration 147, loss = 0.37140005\n",
      "Iteration 980, loss = 0.31227053\n",
      "Iteration 148, loss = 0.37084216\n",
      "Iteration 149, loss = 0.37031289\n",
      "Iteration 876, loss = 0.29199286\n",
      "Iteration 150, loss = 0.36980253\n",
      "Iteration 1844, loss = 0.15457721\n",
      "Iteration 151, loss = 0.36928245\n",
      "Iteration 152, loss = 0.36877108\n",
      "Iteration 1937, loss = 0.18706078\n",
      "Iteration 153, loss = 0.36827977\n",
      "Iteration 877, loss = 0.29180490\n",
      "Iteration 154, loss = 0.36777370\n",
      "Iteration 2045, loss = 0.19554713\n",
      "Iteration 155, loss = 0.36727043\n",
      "Iteration 1938, loss = 0.18694880\n",
      "Iteration 1027, loss = 0.28832019\n",
      "Iteration 981, loss = 0.31225584\n",
      "Iteration 156, loss = 0.36682056\n",
      "Iteration 157, loss = 0.36632893\n",
      "Iteration 158, loss = 0.36586590\n",
      "Iteration 159, loss = 0.36539047\n",
      "Iteration 160, loss = 0.36494355\n",
      "Iteration 161, loss = 0.36447193\n",
      "Iteration 982, loss = 0.31207136\n",
      "Iteration 162, loss = 0.36405156\n",
      "Iteration 1939, loss = 0.18683872\n",
      "Iteration 163, loss = 0.36358297\n",
      "Iteration 2046, loss = 0.19541921\n",
      "Iteration 164, loss = 0.36313930\n",
      "Iteration 169, loss = 0.45947276\n",
      "Iteration 165, loss = 0.36271028\n",
      "Iteration 1028, loss = 0.28793845\n",
      "Iteration 878, loss = 0.29162607\n",
      "Iteration 1845, loss = 0.15457591\n",
      "Iteration 166, loss = 0.36225248\n",
      "Iteration 1940, loss = 0.18672498\n",
      "Iteration 167, loss = 0.36186330\n",
      "Iteration 168, loss = 0.36142780\n",
      "Iteration 169, loss = 0.36100599\n",
      "Iteration 1941, loss = 0.18655423\n",
      "Iteration 983, loss = 0.31195172\n",
      "Iteration 2047, loss = 0.19535590\n",
      "Iteration 170, loss = 0.36064221\n",
      "Iteration 1846, loss = 0.15445006\n",
      "Iteration 1942, loss = 0.18646362\n",
      "Iteration 171, loss = 0.36018999\n",
      "Iteration 879, loss = 0.29142409\n",
      "Iteration 1029, loss = 0.28778121\n",
      "Iteration 172, loss = 0.35981461\n",
      "Iteration 173, loss = 0.35938852\n",
      "Iteration 2048, loss = 0.19519285\n",
      "Iteration 170, loss = 0.45883897\n",
      "Iteration 174, loss = 0.35899571\n",
      "Iteration 175, loss = 0.35862542\n",
      "Iteration 176, loss = 0.35820935\n",
      "Iteration 177, loss = 0.35783856\n",
      "Iteration 178, loss = 0.35746200\n",
      "Iteration 880, loss = 0.29129327\n",
      "Iteration 179, loss = 0.35706363\n",
      "Iteration 2049, loss = 0.19515673\n",
      "Iteration 984, loss = 0.31184115\n",
      "Iteration 180, loss = 0.35670546\n",
      "Iteration 1943, loss = 0.18643847\n",
      "Iteration 181, loss = 0.35630542\n",
      "Iteration 182, loss = 0.35595265\n",
      "Iteration 171, loss = 0.45828288\n",
      "Iteration 183, loss = 0.35558371\n",
      "Iteration 184, loss = 0.35521832\n",
      "Iteration 1944, loss = 0.18630007\n",
      "Iteration 185, loss = 0.35487313\n",
      "Iteration 186, loss = 0.35451441\n",
      "Iteration 1030, loss = 0.28762356\n",
      "Iteration 187, loss = 0.35416018\n",
      "Iteration 188, loss = 0.35380072\n",
      "Iteration 189, loss = 0.35343860\n",
      "Iteration 985, loss = 0.31173533\n",
      "Iteration 190, loss = 0.35308450\n",
      "Iteration 1847, loss = 0.15438573\n",
      "Iteration 191, loss = 0.35277423\n",
      "Iteration 881, loss = 0.29107474\n",
      "Iteration 192, loss = 0.35240109\n",
      "Iteration 1945, loss = 0.18612705\n",
      "Iteration 2050, loss = 0.19521823\n",
      "Iteration 193, loss = 0.35209626\n",
      "Iteration 1031, loss = 0.28753290\n",
      "Iteration 986, loss = 0.31161011\n",
      "Iteration 1946, loss = 0.18601628\n",
      "Iteration 882, loss = 0.29093003\n",
      "Iteration 194, loss = 0.35173852\n",
      "Iteration 172, loss = 0.45768822\n",
      "Iteration 1947, loss = 0.18609188\n",
      "Iteration 195, loss = 0.35139832\n",
      "Iteration 196, loss = 0.35107953\n",
      "Iteration 1948, loss = 0.18576025\n",
      "Iteration 2051, loss = 0.19493062\n",
      "Iteration 197, loss = 0.35075521\n",
      "Iteration 987, loss = 0.31149503\n",
      "Iteration 198, loss = 0.35042034\n",
      "Iteration 883, loss = 0.29073157\n",
      "Iteration 199, loss = 0.35010737Iteration 1848, loss = 0.15437760\n",
      "\n",
      "Iteration 1949, loss = 0.18568252\n",
      "Iteration 2052, loss = 0.19487459\n",
      "Iteration 884, loss = 0.29045662\n",
      "Iteration 200, loss = 0.34978492\n",
      "Iteration 1032, loss = 0.28733532\n",
      "Iteration 201, loss = 0.34946666\n",
      "Iteration 1950, loss = 0.18556130\n",
      "Iteration 202, loss = 0.34914763\n",
      "Iteration 173, loss = 0.45708645\n",
      "Iteration 988, loss = 0.31138442\n",
      "Iteration 885, loss = 0.29032223\n",
      "Iteration 203, loss = 0.34883545\n",
      "Iteration 204, loss = 0.34856337\n",
      "Iteration 205, loss = 0.34821811\n",
      "Iteration 206, loss = 0.34792316\n",
      "Iteration 207, loss = 0.34761607\n",
      "Iteration 886, loss = 0.29009036\n",
      "Iteration 208, loss = 0.34730977\n",
      "Iteration 989, loss = 0.31126500\n",
      "Iteration 209, loss = 0.34701134\n",
      "Iteration 210, loss = 0.34673284\n",
      "Iteration 211, loss = 0.34643492\n",
      "Iteration 212, loss = 0.34611997\n",
      "Iteration 887, loss = 0.28990140\n",
      "Iteration 213, loss = 0.34582432\n",
      "Iteration 174, loss = 0.45646773\n",
      "Iteration 214, loss = 0.34556353\n",
      "Iteration 2053, loss = 0.19485292\n",
      "Iteration 1951, loss = 0.18551557\n",
      "Iteration 215, loss = 0.34527358\n",
      "Iteration 990, loss = 0.31118963\n",
      "Iteration 216, loss = 0.34496531\n",
      "Iteration 2054, loss = 0.19465177\n",
      "Iteration 1952, loss = 0.18531401\n",
      "Iteration 888, loss = 0.28987671\n",
      "Iteration 1849, loss = 0.15429551\n",
      "Iteration 1033, loss = 0.28729800\n",
      "Iteration 217, loss = 0.34469848\n",
      "Iteration 1953, loss = 0.18521833\n",
      "Iteration 218, loss = 0.34443216\n",
      "Iteration 219, loss = 0.34412873\n",
      "Iteration 220, loss = 0.34387049\n",
      "Iteration 2055, loss = 0.19456166\n",
      "Iteration 991, loss = 0.31113098\n",
      "Iteration 221, loss = 0.34359960\n",
      "Iteration 222, loss = 0.34331042\n",
      "Iteration 889, loss = 0.28954045\n",
      "Iteration 223, loss = 0.34301415\n",
      "Iteration 1850, loss = 0.15431012\n",
      "Iteration 224, loss = 0.34276308\n",
      "Iteration 225, loss = 0.34249104\n",
      "Iteration 1954, loss = 0.18513114\n",
      "Iteration 1034, loss = 0.28695356\n",
      "Iteration 226, loss = 0.34222850\n",
      "Iteration 227, loss = 0.34195558\n",
      "Iteration 228, loss = 0.34170476\n",
      "Iteration 1955, loss = 0.18506039\n",
      "Iteration 229, loss = 0.34150040\n",
      "Iteration 230, loss = 0.34117180\n",
      "Iteration 175, loss = 0.45593458\n",
      "Iteration 1956, loss = 0.18491740\n",
      "Iteration 231, loss = 0.34092518\n",
      "Iteration 1851, loss = 0.15412119\n",
      "Iteration 232, loss = 0.34064591\n",
      "Iteration 992, loss = 0.31095662\n",
      "Iteration 890, loss = 0.28949592\n",
      "Iteration 2056, loss = 0.19447268\n",
      "Iteration 233, loss = 0.34043169\n",
      "Iteration 234, loss = 0.34011955\n",
      "Iteration 235, loss = 0.33987216\n",
      "Iteration 236, loss = 0.33962548\n",
      "Iteration 237, loss = 0.33937446\n",
      "Iteration 238, loss = 0.33912406\n",
      "Iteration 239, loss = 0.33885585\n",
      "Iteration 1035, loss = 0.28673980\n",
      "Iteration 891, loss = 0.28920154\n",
      "Iteration 1957, loss = 0.18473779\n",
      "Iteration 240, loss = 0.33861033\n",
      "Iteration 1958, loss = 0.18464712\n",
      "Iteration 241, loss = 0.33837514\n",
      "Iteration 1852, loss = 0.15411850\n",
      "Iteration 2057, loss = 0.19440736\n",
      "Iteration 176, loss = 0.45539632\n",
      "Iteration 993, loss = 0.31083443\n",
      "Iteration 242, loss = 0.33811993\n",
      "Iteration 1959, loss = 0.18461346\n",
      "Iteration 243, loss = 0.33789085\n",
      "Iteration 244, loss = 0.33763038\n",
      "Iteration 892, loss = 0.28890686\n",
      "Iteration 1960, loss = 0.18446888\n",
      "Iteration 245, loss = 0.33738469\n",
      "Iteration 246, loss = 0.33713934\n",
      "Iteration 247, loss = 0.33691376\n",
      "Iteration 2058, loss = 0.19427022\n",
      "Iteration 1961, loss = 0.18431571\n",
      "Iteration 893, loss = 0.28885244\n",
      "Iteration 248, loss = 0.33667669\n",
      "Iteration 1036, loss = 0.28651298\n",
      "Iteration 1962, loss = 0.18423136\n",
      "Iteration 249, loss = 0.33642110\n",
      "Iteration 1853, loss = 0.15403330\n",
      "Iteration 994, loss = 0.31072424\n",
      "Iteration 894, loss = 0.28868200\n",
      "Iteration 1963, loss = 0.18410816\n",
      "Iteration 177, loss = 0.45482866\n",
      "Iteration 1964, loss = 0.18398944\n",
      "Iteration 250, loss = 0.33618366\n",
      "Iteration 1965, loss = 0.18387540\n",
      "Iteration 178, loss = 0.45426697\n",
      "Iteration 2059, loss = 0.19418938\n",
      "Iteration 251, loss = 0.33595041\n",
      "Iteration 1037, loss = 0.28643308\n",
      "Iteration 252, loss = 0.33572368\n",
      "Iteration 895, loss = 0.28840808\n",
      "Iteration 253, loss = 0.33548318\n",
      "Iteration 1966, loss = 0.18379625\n",
      "Iteration 254, loss = 0.33524157\n",
      "Iteration 255, loss = 0.33502564\n",
      "Iteration 256, loss = 0.33481915\n",
      "Iteration 257, loss = 0.33456475\n",
      "Iteration 1854, loss = 0.15398788\n",
      "Iteration 995, loss = 0.31061324\n",
      "Iteration 896, loss = 0.28818554\n",
      "Iteration 258, loss = 0.33433726\n",
      "Iteration 2060, loss = 0.19405901\n",
      "Iteration 259, loss = 0.33413260\n",
      "Iteration 1967, loss = 0.18365341\n",
      "Iteration 260, loss = 0.33392124\n",
      "Iteration 1038, loss = 0.28623736\n",
      "Iteration 1968, loss = 0.18355325\n",
      "Iteration 261, loss = 0.33368409\n",
      "Iteration 1969, loss = 0.18346986\n",
      "Iteration 179, loss = 0.45376666\n",
      "Iteration 262, loss = 0.33346086\n",
      "Iteration 897, loss = 0.28798716\n",
      "Iteration 1970, loss = 0.18334013\n",
      "Iteration 996, loss = 0.31049477\n",
      "Iteration 2061, loss = 0.19405823\n",
      "Iteration 263, loss = 0.33325307\n",
      "Iteration 1855, loss = 0.15388747\n",
      "Iteration 264, loss = 0.33301418\n",
      "Iteration 265, loss = 0.33280537\n",
      "Iteration 266, loss = 0.33259118\n",
      "Iteration 267, loss = 0.33236848\n",
      "Iteration 997, loss = 0.31042551\n",
      "Iteration 1039, loss = 0.28612662\n",
      "Iteration 180, loss = 0.45322143\n",
      "Iteration 268, loss = 0.33215781\n",
      "Iteration 898, loss = 0.28780263\n",
      "Iteration 1971, loss = 0.18321198\n",
      "Iteration 269, loss = 0.33194470\n",
      "Iteration 270, loss = 0.33173643\n",
      "Iteration 899, loss = 0.28764004\n",
      "Iteration 271, loss = 0.33152935\n",
      "Iteration 1972, loss = 0.18323918\n",
      "Iteration 1040, loss = 0.28580928\n",
      "Iteration 2062, loss = 0.19401351\n",
      "Iteration 272, loss = 0.33131927\n",
      "Iteration 273, loss = 0.33111093\n",
      "Iteration 1856, loss = 0.15385852\n",
      "Iteration 998, loss = 0.31027494\n",
      "Iteration 274, loss = 0.33091717\n",
      "Iteration 275, loss = 0.33072031\n",
      "Iteration 276, loss = 0.33049330\n",
      "Iteration 1973, loss = 0.18303495\n",
      "Iteration 2063, loss = 0.19395362\n",
      "Iteration 277, loss = 0.33029529\n",
      "Iteration 181, loss = 0.45274798\n",
      "Iteration 278, loss = 0.33007573\n",
      "Iteration 900, loss = 0.28737481\n",
      "Iteration 1974, loss = 0.18287516\n",
      "Iteration 1857, loss = 0.15379872\n",
      "Iteration 279, loss = 0.32988206\n",
      "Iteration 1041, loss = 0.28577813\n",
      "Iteration 280, loss = 0.32967674\n",
      "Iteration 999, loss = 0.31020489\n",
      "Iteration 281, loss = 0.32947618\n",
      "Iteration 182, loss = 0.45220455\n",
      "Iteration 2064, loss = 0.19379097\n",
      "Iteration 1975, loss = 0.18275929\n",
      "Iteration 282, loss = 0.32927771\n",
      "Iteration 1858, loss = 0.15372566\n",
      "Iteration 1976, loss = 0.18266772\n",
      "Iteration 283, loss = 0.32908482\n",
      "Iteration 901, loss = 0.28719389\n",
      "Iteration 284, loss = 0.32888226\n",
      "Iteration 1977, loss = 0.18272789\n",
      "Iteration 183, loss = 0.45171782\n",
      "Iteration 285, loss = 0.32868949\n",
      "Iteration 1042, loss = 0.28559675\n",
      "Iteration 286, loss = 0.32850395\n",
      "Iteration 1000, loss = 0.31004524\n",
      "Iteration 1978, loss = 0.18249450\n",
      "Iteration 287, loss = 0.32828580\n",
      "Iteration 902, loss = 0.28699115\n",
      "Iteration 288, loss = 0.32811852\n",
      "Iteration 1979, loss = 0.18231102\n",
      "Iteration 289, loss = 0.32789363\n",
      "Iteration 290, loss = 0.32770670\n",
      "Iteration 291, loss = 0.32753316\n",
      "Iteration 2065, loss = 0.19360528\n",
      "Iteration 903, loss = 0.28681306\n",
      "Iteration 292, loss = 0.32733012\n",
      "Iteration 293, loss = 0.32713359\n",
      "Iteration 1043, loss = 0.28528899\n",
      "Iteration 1859, loss = 0.15364696\n",
      "Iteration 294, loss = 0.32696315\n",
      "Iteration 1980, loss = 0.18221893Iteration 295, loss = 0.32679526\n",
      "Iteration 904, loss = 0.28670817\n",
      "\n",
      "Iteration 1001, loss = 0.30996310\n",
      "Iteration 184, loss = 0.45126229\n",
      "Iteration 296, loss = 0.32660133\n",
      "Iteration 2066, loss = 0.19354170\n",
      "Iteration 297, loss = 0.32638900\n",
      "Iteration 298, loss = 0.32623670\n",
      "Iteration 905, loss = 0.28642461\n",
      "Iteration 299, loss = 0.32603166\n",
      "Iteration 300, loss = 0.32584444\n",
      "Iteration 1981, loss = 0.18211975\n",
      "Iteration 906, loss = 0.28626561\n",
      "Iteration 301, loss = 0.32567172\n",
      "Iteration 2067, loss = 0.19343285\n",
      "Iteration 1860, loss = 0.15358302\n",
      "Iteration 1044, loss = 0.28519568\n",
      "Iteration 302, loss = 0.32550956\n",
      "Iteration 1002, loss = 0.30990177\n",
      "Iteration 907, loss = 0.28603379\n",
      "Iteration 1982, loss = 0.18198075\n",
      "Iteration 303, loss = 0.32529931\n",
      "Iteration 304, loss = 0.32513156\n",
      "Iteration 305, loss = 0.32496811\n",
      "Iteration 306, loss = 0.32477255\n",
      "Iteration 1003, loss = 0.30973374\n",
      "Iteration 307, loss = 0.32459656\n",
      "Iteration 185, loss = 0.45077833\n",
      "Iteration 308, loss = 0.32442256\n",
      "Iteration 2068, loss = 0.19334880\n",
      "Iteration 1045, loss = 0.28492354\n",
      "Iteration 1983, loss = 0.18200225\n",
      "Iteration 908, loss = 0.28587686\n",
      "Iteration 1004, loss = 0.30964379\n",
      "Iteration 309, loss = 0.32425070\n",
      "Iteration 1861, loss = 0.15356519\n",
      "Iteration 310, loss = 0.32408838\n",
      "Iteration 1984, loss = 0.18183278\n",
      "Iteration 311, loss = 0.32390300\n",
      "Iteration 312, loss = 0.32373591\n",
      "Iteration 313, loss = 0.32357481\n",
      "Iteration 909, loss = 0.28568832\n",
      "Iteration 314, loss = 0.32339342\n",
      "Iteration 1005, loss = 0.30948158\n",
      "Iteration 1046, loss = 0.28473026\n",
      "Iteration 1985, loss = 0.18177350\n",
      "Iteration 315, loss = 0.32323054\n",
      "Iteration 2069, loss = 0.19332876\n",
      "Iteration 316, loss = 0.32306581\n",
      "Iteration 1986, loss = 0.18158466\n",
      "Iteration 317, loss = 0.32288875\n",
      "Iteration 910, loss = 0.28549420\n",
      "Iteration 318, loss = 0.32271381\n",
      "Iteration 2070, loss = 0.19322467\n",
      "Iteration 186, loss = 0.45024979\n",
      "Iteration 1006, loss = 0.30935502\n",
      "Iteration 1862, loss = 0.15344326\n",
      "Iteration 911, loss = 0.28522018\n",
      "Iteration 319, loss = 0.32256181\n",
      "Iteration 320, loss = 0.32238913\n",
      "Iteration 1047, loss = 0.28458006\n",
      "Iteration 321, loss = 0.32221868\n",
      "Iteration 1987, loss = 0.18157822\n",
      "Iteration 2071, loss = 0.19311284\n",
      "Iteration 322, loss = 0.32207172\n",
      "Iteration 323, loss = 0.32188252\n",
      "Iteration 324, loss = 0.32172983\n",
      "Iteration 1988, loss = 0.18149644\n",
      "Iteration 1007, loss = 0.30925934\n",
      "Iteration 325, loss = 0.32156994\n",
      "Iteration 326, loss = 0.32140383\n",
      "Iteration 327, loss = 0.32123201\n",
      "Iteration 187, loss = 0.44973936\n",
      "Iteration 912, loss = 0.28504451\n",
      "Iteration 2072, loss = 0.19297322\n",
      "Iteration 328, loss = 0.32108641\n",
      "Iteration 1989, loss = 0.18127462\n",
      "Iteration 1048, loss = 0.28438816\n",
      "Iteration 329, loss = 0.32093514\n",
      "Iteration 1008, loss = 0.30919243\n",
      "Iteration 330, loss = 0.32078987\n",
      "Iteration 1863, loss = 0.15347223\n",
      "Iteration 1990, loss = 0.18121777\n",
      "Iteration 331, loss = 0.32060626\n",
      "Iteration 332, loss = 0.32044260\n",
      "Iteration 913, loss = 0.28483325\n",
      "Iteration 333, loss = 0.32030875\n",
      "Iteration 2073, loss = 0.19294932\n",
      "Iteration 334, loss = 0.32013683\n",
      "Iteration 335, loss = 0.31998922\n",
      "Iteration 336, loss = 0.31983396\n",
      "Iteration 337, loss = 0.31968381\n",
      "Iteration 1991, loss = 0.18101524\n",
      "Iteration 338, loss = 0.31952446\n",
      "Iteration 339, loss = 0.31937996\n",
      "Iteration 340, loss = 0.31922217\n",
      "Iteration 1864, loss = 0.15351232\n",
      "Iteration 1009, loss = 0.30905115\n",
      "Iteration 341, loss = 0.31906477\n",
      "Iteration 1992, loss = 0.18085590\n",
      "Iteration 342, loss = 0.31893143\n",
      "Iteration 914, loss = 0.28469058\n",
      "Iteration 343, loss = 0.31878693\n",
      "Iteration 2074, loss = 0.19284226\n",
      "Iteration 188, loss = 0.44932581\n",
      "Iteration 344, loss = 0.31862665\n",
      "Iteration 1049, loss = 0.28422689\n",
      "Iteration 345, loss = 0.31847834\n",
      "Iteration 1010, loss = 0.30892270\n",
      "Iteration 915, loss = 0.28447418\n",
      "Iteration 346, loss = 0.31831733\n",
      "Iteration 1993, loss = 0.18085777\n",
      "Iteration 2075, loss = 0.19269197\n",
      "Iteration 347, loss = 0.31819035\n",
      "Iteration 348, loss = 0.31803655\n",
      "Iteration 1865, loss = 0.15343798\n",
      "Iteration 349, loss = 0.31789382\n",
      "Iteration 350, loss = 0.31775332\n",
      "Iteration 1994, loss = 0.18062814\n",
      "Iteration 351, loss = 0.31760878\n",
      "Iteration 916, loss = 0.28439899\n",
      "Iteration 189, loss = 0.44883005\n",
      "Iteration 352, loss = 0.31746984\n",
      "Iteration 1995, loss = 0.18055826\n",
      "Iteration 1050, loss = 0.28405775\n",
      "Iteration 1011, loss = 0.30882978\n",
      "Iteration 2076, loss = 0.19263212\n",
      "Iteration 353, loss = 0.31730576\n",
      "Iteration 354, loss = 0.31716066\n",
      "Iteration 355, loss = 0.31705266\n",
      "Iteration 917, loss = 0.28405131\n",
      "Iteration 356, loss = 0.31689695\n",
      "Iteration 1996, loss = 0.18047755\n",
      "Iteration 357, loss = 0.31676207\n",
      "Iteration 190, loss = 0.44836858\n",
      "Iteration 1866, loss = 0.15323612\n",
      "Iteration 918, loss = 0.28385738\n",
      "Iteration 1012, loss = 0.30867108\n",
      "Iteration 358, loss = 0.31662015\n",
      "Iteration 1051, loss = 0.28383579\n",
      "Iteration 2077, loss = 0.19255558\n",
      "Iteration 359, loss = 0.31647489\n",
      "Iteration 919, loss = 0.28375401\n",
      "Iteration 1997, loss = 0.18031666\n",
      "Iteration 920, loss = 0.28353748\n",
      "Iteration 2078, loss = 0.19246535\n",
      "Iteration 1998, loss = 0.18029147\n",
      "Iteration 360, loss = 0.31633669\n",
      "Iteration 191, loss = 0.44794799\n",
      "Iteration 361, loss = 0.31619691\n",
      "Iteration 1999, loss = 0.18009934\n",
      "Iteration 2079, loss = 0.19238277\n",
      "Iteration 921, loss = 0.28340566\n",
      "Iteration 362, loss = 0.31607673\n",
      "Iteration 1013, loss = 0.30858659\n",
      "Iteration 363, loss = 0.31594475\n",
      "Iteration 2000, loss = 0.18001798\n",
      "Iteration 364, loss = 0.31580579\n",
      "Iteration 365, loss = 0.31565904\n",
      "Iteration 922, loss = 0.28315180\n",
      "Iteration 2001, loss = 0.17990892\n",
      "Iteration 366, loss = 0.31552156\n",
      "Iteration 192, loss = 0.44756407\n",
      "Iteration 367, loss = 0.31539073\n",
      "Iteration 2002, loss = 0.17975566\n",
      "Iteration 1867, loss = 0.15320366\n",
      "Iteration 368, loss = 0.31525103\n",
      "Iteration 1052, loss = 0.28377374\n",
      "Iteration 369, loss = 0.31512002\n",
      "Iteration 370, loss = 0.31499508\n",
      "Iteration 371, loss = 0.31484810\n",
      "Iteration 372, loss = 0.31472711\n",
      "Iteration 373, loss = 0.31460271\n",
      "Iteration 374, loss = 0.31446292\n",
      "Iteration 375, loss = 0.31433549\n",
      "Iteration 923, loss = 0.28294004\n",
      "Iteration 376, loss = 0.31419817\n",
      "Iteration 377, loss = 0.31409629\n",
      "Iteration 1053, loss = 0.28361649\n",
      "Iteration 378, loss = 0.31393567\n",
      "Iteration 2003, loss = 0.17972018\n",
      "Iteration 1014, loss = 0.30843684\n",
      "Iteration 379, loss = 0.31381002\n",
      "Iteration 1868, loss = 0.15313191\n",
      "Iteration 193, loss = 0.44704719\n",
      "Iteration 2080, loss = 0.19221457\n",
      "Iteration 380, loss = 0.31369613\n",
      "Iteration 2004, loss = 0.17959741\n",
      "Iteration 381, loss = 0.31356592\n",
      "Iteration 382, loss = 0.31341916\n",
      "Iteration 383, loss = 0.31331018\n",
      "Iteration 384, loss = 0.31317376\n",
      "Iteration 385, loss = 0.31305156\n",
      "Iteration 386, loss = 0.31295048\n",
      "Iteration 924, loss = 0.28275115\n",
      "Iteration 387, loss = 0.31279711\n",
      "Iteration 194, loss = 0.44674266\n",
      "Iteration 388, loss = 0.31268138\n",
      "Iteration 389, loss = 0.31255597\n",
      "Iteration 2005, loss = 0.17953657\n",
      "Iteration 390, loss = 0.31242930\n",
      "Iteration 1054, loss = 0.28338386\n",
      "Iteration 925, loss = 0.28256703\n",
      "Iteration 391, loss = 0.31231127\n",
      "Iteration 2081, loss = 0.19213409\n",
      "Iteration 392, loss = 0.31219626\n",
      "Iteration 393, loss = 0.31208731\n",
      "Iteration 1869, loss = 0.15305528\n",
      "Iteration 1015, loss = 0.30836289\n",
      "Iteration 394, loss = 0.31194343\n",
      "Iteration 2006, loss = 0.17935574\n",
      "Iteration 926, loss = 0.28236075\n",
      "Iteration 395, loss = 0.31182301\n",
      "Iteration 396, loss = 0.31170640\n",
      "Iteration 2007, loss = 0.17924044\n",
      "Iteration 195, loss = 0.44626633\n",
      "Iteration 2082, loss = 0.19210378\n",
      "Iteration 397, loss = 0.31160315\n",
      "Iteration 398, loss = 0.31146718\n",
      "Iteration 399, loss = 0.31134618\n",
      "Iteration 400, loss = 0.31124619\n",
      "Iteration 401, loss = 0.31110186\n",
      "Iteration 927, loss = 0.28216603\n",
      "Iteration 1055, loss = 0.28319430\n",
      "Iteration 1016, loss = 0.30822921Iteration 402, loss = 0.31098169\n",
      "\n",
      "Iteration 2008, loss = 0.17913140\n",
      "Iteration 403, loss = 0.31085848\n",
      "Iteration 404, loss = 0.31074633\n",
      "Iteration 2083, loss = 0.19194559\n",
      "Iteration 405, loss = 0.31062245\n",
      "Iteration 1870, loss = 0.15305563\n",
      "Iteration 2009, loss = 0.17902926\n",
      "Iteration 406, loss = 0.31050889\n",
      "Iteration 928, loss = 0.28184983\n",
      "Iteration 1056, loss = 0.28304172\n",
      "Iteration 2010, loss = 0.17905179\n",
      "Iteration 2084, loss = 0.19191717\n",
      "Iteration 2011, loss = 0.17879544\n",
      "Iteration 196, loss = 0.44578734\n",
      "Iteration 929, loss = 0.28178130\n",
      "Iteration 2012, loss = 0.17866383\n",
      "Iteration 1057, loss = 0.28284344\n",
      "Iteration 1017, loss = 0.30813293\n",
      "Iteration 930, loss = 0.28168965\n",
      "Iteration 407, loss = 0.31038693\n",
      "Iteration 2013, loss = 0.17855820\n",
      "Iteration 408, loss = 0.31027362\n",
      "Iteration 409, loss = 0.31017485\n",
      "Iteration 2085, loss = 0.19199125\n",
      "Iteration 410, loss = 0.31005190\n",
      "Iteration 2014, loss = 0.17846367\n",
      "Iteration 1018, loss = 0.30798979Iteration 411, loss = 0.30993435\n",
      "\n",
      "Iteration 931, loss = 0.28125655\n",
      "Iteration 1871, loss = 0.15293930\n",
      "Iteration 2015, loss = 0.17845338\n",
      "Iteration 2086, loss = 0.19183317\n",
      "Iteration 197, loss = 0.44540720\n",
      "Iteration 412, loss = 0.30983571\n",
      "Iteration 413, loss = 0.30969975\n",
      "Iteration 1058, loss = 0.28262670\n",
      "Iteration 932, loss = 0.28113985\n",
      "Iteration 414, loss = 0.30958384\n",
      "Iteration 1019, loss = 0.30787365\n",
      "Iteration 415, loss = 0.30946090\n",
      "Iteration 416, loss = 0.30935478\n",
      "Iteration 1872, loss = 0.15290967\n",
      "Iteration 2016, loss = 0.17830488\n",
      "Iteration 417, loss = 0.30924701\n",
      "Iteration 418, loss = 0.30913812\n",
      "Iteration 419, loss = 0.30900750\n",
      "Iteration 1059, loss = 0.28243890\n",
      "Iteration 933, loss = 0.28088746\n",
      "Iteration 420, loss = 0.30890988\n",
      "Iteration 2017, loss = 0.17816918\n",
      "Iteration 2087, loss = 0.19158405\n",
      "Iteration 421, loss = 0.30879042\n",
      "Iteration 1020, loss = 0.30779957\n",
      "Iteration 422, loss = 0.30866864\n",
      "Iteration 2018, loss = 0.17803392\n",
      "Iteration 423, loss = 0.30855737\n",
      "Iteration 424, loss = 0.30844966\n",
      "Iteration 425, loss = 0.30833778\n",
      "Iteration 1873, loss = 0.15308155\n",
      "Iteration 426, loss = 0.30825438\n",
      "Iteration 198, loss = 0.44500816\n",
      "Iteration 427, loss = 0.30813261\n",
      "Iteration 428, loss = 0.30799765\n",
      "Iteration 934, loss = 0.28067466\n",
      "Iteration 429, loss = 0.30789725\n",
      "Iteration 430, loss = 0.30779438\n",
      "Iteration 2088, loss = 0.19156342\n",
      "Iteration 2019, loss = 0.17807645\n",
      "Iteration 431, loss = 0.30767234\n",
      "Iteration 432, loss = 0.30759025\n",
      "Iteration 433, loss = 0.30747730\n",
      "Iteration 1021, loss = 0.30766092\n",
      "Iteration 935, loss = 0.28048528\n",
      "Iteration 434, loss = 0.30734878\n",
      "Iteration 936, loss = 0.28035765\n",
      "Iteration 2020, loss = 0.17781302\n",
      "Iteration 199, loss = 0.44459523\n",
      "Iteration 1060, loss = 0.28230264\n",
      "Iteration 435, loss = 0.30725729\n",
      "Iteration 436, loss = 0.30715738\n",
      "Iteration 1874, loss = 0.15275983\n",
      "Iteration 437, loss = 0.30703644\n",
      "Iteration 438, loss = 0.30694620\n",
      "Iteration 2021, loss = 0.17773557\n",
      "Iteration 439, loss = 0.30682023\n",
      "Iteration 440, loss = 0.30671340\n",
      "Iteration 937, loss = 0.28008537\n",
      "Iteration 441, loss = 0.30660825\n",
      "Iteration 1022, loss = 0.30752850\n",
      "Iteration 442, loss = 0.30649676\n",
      "Iteration 938, loss = 0.28000110\n",
      "Iteration 1875, loss = 0.15270208\n",
      "Iteration 2022, loss = 0.17766914\n",
      "Iteration 2089, loss = 0.19141065\n",
      "Iteration 443, loss = 0.30639451\n",
      "Iteration 444, loss = 0.30629288\n",
      "Iteration 445, loss = 0.30618035\n",
      "Iteration 2023, loss = 0.17762342\n",
      "Iteration 446, loss = 0.30608402\n",
      "Iteration 1023, loss = 0.30740921\n",
      "Iteration 447, loss = 0.30598369\n",
      "Iteration 448, loss = 0.30587272\n",
      "Iteration 939, loss = 0.27984699\n",
      "Iteration 1061, loss = 0.28213509\n",
      "Iteration 449, loss = 0.30577629\n",
      "Iteration 450, loss = 0.30566221\n",
      "Iteration 200, loss = 0.44420191\n",
      "Iteration 451, loss = 0.30555542\n",
      "Iteration 940, loss = 0.27948235\n",
      "Iteration 452, loss = 0.30548174\n",
      "Iteration 1024, loss = 0.30731836\n",
      "Iteration 2090, loss = 0.19130901\n",
      "Iteration 453, loss = 0.30535622\n",
      "Iteration 1876, loss = 0.15268180\n",
      "Iteration 454, loss = 0.30524688\n",
      "Iteration 2024, loss = 0.17740737\n",
      "Iteration 941, loss = 0.27931452\n",
      "Iteration 2025, loss = 0.17726698\n",
      "Iteration 2091, loss = 0.19127545\n",
      "Iteration 455, loss = 0.30514463\n",
      "Iteration 1062, loss = 0.28185040\n",
      "Iteration 1025, loss = 0.30723852\n",
      "Iteration 2026, loss = 0.17718244\n",
      "Iteration 2027, loss = 0.17713609\n",
      "Iteration 1877, loss = 0.15261504\n",
      "Iteration 201, loss = 0.44378346\n",
      "Iteration 942, loss = 0.27920906\n",
      "Iteration 456, loss = 0.30507562\n",
      "Iteration 1026, loss = 0.30708062\n",
      "Iteration 2092, loss = 0.19121240\n",
      "Iteration 943, loss = 0.27895700\n",
      "Iteration 2028, loss = 0.17698295\n",
      "Iteration 1878, loss = 0.15261984\n",
      "Iteration 457, loss = 0.30494835\n",
      "Iteration 1063, loss = 0.28181237\n",
      "Iteration 1027, loss = 0.30698455\n",
      "Iteration 944, loss = 0.27876381\n",
      "Iteration 458, loss = 0.30490078\n",
      "Iteration 202, loss = 0.44344371\n",
      "Iteration 2029, loss = 0.17690944\n",
      "Iteration 459, loss = 0.30475845\n",
      "Iteration 945, loss = 0.27864912\n",
      "Iteration 2030, loss = 0.17669951\n",
      "Iteration 460, loss = 0.30463940\n",
      "Iteration 1064, loss = 0.28164571\n",
      "Iteration 946, loss = 0.27826574\n",
      "Iteration 461, loss = 0.30456881\n",
      "Iteration 2031, loss = 0.17660699\n",
      "Iteration 462, loss = 0.30443917\n",
      "Iteration 203, loss = 0.44305813\n",
      "Iteration 463, loss = 0.30435391\n",
      "Iteration 2093, loss = 0.19105423\n",
      "Iteration 464, loss = 0.30425485\n",
      "Iteration 1028, loss = 0.30686897\n",
      "Iteration 465, loss = 0.30415518\n",
      "Iteration 466, loss = 0.30406145\n",
      "Iteration 467, loss = 0.30395300\n",
      "Iteration 947, loss = 0.27805705\n",
      "Iteration 468, loss = 0.30385864\n",
      "Iteration 469, loss = 0.30375445\n",
      "Iteration 470, loss = 0.30366840\n",
      "Iteration 1879, loss = 0.15252833\n",
      "Iteration 471, loss = 0.30357091\n",
      "Iteration 1029, loss = 0.30676478\n",
      "Iteration 2032, loss = 0.17656693\n",
      "Iteration 472, loss = 0.30346363\n",
      "Iteration 2094, loss = 0.19097714\n",
      "Iteration 1065, loss = 0.28141727\n",
      "Iteration 473, loss = 0.30336875\n",
      "Iteration 474, loss = 0.30327355\n",
      "Iteration 475, loss = 0.30318019\n",
      "Iteration 204, loss = 0.44266178\n",
      "Iteration 2033, loss = 0.17641437\n",
      "Iteration 948, loss = 0.27806249\n",
      "Iteration 476, loss = 0.30307958\n",
      "Iteration 1030, loss = 0.30661858\n",
      "Iteration 477, loss = 0.30298310\n",
      "Iteration 478, loss = 0.30289592\n",
      "Iteration 479, loss = 0.30278061\n",
      "Iteration 2034, loss = 0.17635993\n",
      "Iteration 480, loss = 0.30269835\n",
      "Iteration 481, loss = 0.30261184\n",
      "Iteration 949, loss = 0.27764285\n",
      "Iteration 482, loss = 0.30249646\n",
      "Iteration 1066, loss = 0.28118869\n",
      "Iteration 1031, loss = 0.30653425\n",
      "Iteration 483, loss = 0.30240991\n",
      "Iteration 484, loss = 0.30231726\n",
      "Iteration 205, loss = 0.44233492\n",
      "Iteration 485, loss = 0.30222612\n",
      "Iteration 486, loss = 0.30212629\n",
      "Iteration 2035, loss = 0.17627252\n",
      "Iteration 487, loss = 0.30203358\n",
      "Iteration 488, loss = 0.30193098\n",
      "Iteration 2095, loss = 0.19105336\n",
      "Iteration 489, loss = 0.30184544\n",
      "Iteration 490, loss = 0.30174296\n",
      "Iteration 491, loss = 0.30164689\n",
      "Iteration 1880, loss = 0.15243102\n",
      "Iteration 492, loss = 0.30155844\n",
      "Iteration 2036, loss = 0.17611754\n",
      "Iteration 1067, loss = 0.28103113\n",
      "Iteration 493, loss = 0.30145262\n",
      "Iteration 950, loss = 0.27747712\n",
      "Iteration 2096, loss = 0.19079940\n",
      "Iteration 2037, loss = 0.17603851\n",
      "Iteration 494, loss = 0.30137422\n",
      "Iteration 1032, loss = 0.30636985\n",
      "Iteration 206, loss = 0.44193925\n",
      "Iteration 495, loss = 0.30127354\n",
      "Iteration 951, loss = 0.27744426\n",
      "Iteration 496, loss = 0.30118097\n",
      "Iteration 497, loss = 0.30109230\n",
      "Iteration 498, loss = 0.30099693\n",
      "Iteration 499, loss = 0.30090914\n",
      "Iteration 2038, loss = 0.17602497\n",
      "Iteration 952, loss = 0.27711584\n",
      "Iteration 500, loss = 0.30082067\n",
      "Iteration 1068, loss = 0.28098304\n",
      "Iteration 2039, loss = 0.17574840\n",
      "Iteration 501, loss = 0.30071995\n",
      "Iteration 1033, loss = 0.30628320\n",
      "Iteration 502, loss = 0.30062873\n",
      "Iteration 503, loss = 0.30053706\n",
      "Iteration 2040, loss = 0.17578602\n",
      "Iteration 2097, loss = 0.19072794\n",
      "Iteration 953, loss = 0.27682767\n",
      "Iteration 504, loss = 0.30043650\n",
      "Iteration 505, loss = 0.30034746\n",
      "Iteration 506, loss = 0.30027014\n",
      "Iteration 507, loss = 0.30018544\n",
      "Iteration 207, loss = 0.44157698\n",
      "Iteration 2041, loss = 0.17574244\n",
      "Iteration 508, loss = 0.30006404\n",
      "Iteration 2098, loss = 0.19058308\n",
      "Iteration 1881, loss = 0.15241555\n",
      "Iteration 509, loss = 0.29997630\n",
      "Iteration 1069, loss = 0.28080917\n",
      "Iteration 510, loss = 0.29988479\n",
      "Iteration 954, loss = 0.27667307\n",
      "Iteration 1034, loss = 0.30615006\n",
      "Iteration 511, loss = 0.29980613\n",
      "Iteration 512, loss = 0.29970506\n",
      "Iteration 2099, loss = 0.19056971\n",
      "Iteration 2042, loss = 0.17543622\n",
      "Iteration 955, loss = 0.27648647\n",
      "Iteration 513, loss = 0.29961859\n",
      "Iteration 2043, loss = 0.17536918\n",
      "Iteration 1882, loss = 0.15229252\n",
      "Iteration 514, loss = 0.29953246Iteration 1070, loss = 0.28049499\n",
      "Iteration 1035, loss = 0.30611787\n",
      "\n",
      "Iteration 2044, loss = 0.17527611\n",
      "Iteration 208, loss = 0.44119527\n",
      "Iteration 515, loss = 0.29945612\n",
      "Iteration 956, loss = 0.27642249\n",
      "Iteration 516, loss = 0.29936057\n",
      "Iteration 1036, loss = 0.30591840\n",
      "Iteration 2045, loss = 0.17510244\n",
      "Iteration 517, loss = 0.29927049\n",
      "Iteration 2046, loss = 0.17501419\n",
      "Iteration 518, loss = 0.29920041\n",
      "Iteration 1071, loss = 0.28027513\n",
      "Iteration 2100, loss = 0.19039247\n",
      "Iteration 1883, loss = 0.15237031\n",
      "Iteration 2047, loss = 0.17492816\n",
      "Iteration 519, loss = 0.29909257\n",
      "Iteration 520, loss = 0.29904078\n",
      "Iteration 957, loss = 0.27605564\n",
      "Iteration 521, loss = 0.29893348\n",
      "Iteration 522, loss = 0.29885833\n",
      "Iteration 523, loss = 0.29874095\n",
      "Iteration 524, loss = 0.29865743\n",
      "Iteration 209, loss = 0.44088288\n",
      "Iteration 525, loss = 0.29859206\n",
      "Iteration 958, loss = 0.27593100\n",
      "Iteration 526, loss = 0.29849471\n",
      "Iteration 2101, loss = 0.19041279\n",
      "Iteration 1037, loss = 0.30581012\n",
      "Iteration 527, loss = 0.29841027\n",
      "Iteration 2048, loss = 0.17481463\n",
      "Iteration 528, loss = 0.29831965\n",
      "Iteration 959, loss = 0.27566365\n",
      "Iteration 529, loss = 0.29823408\n",
      "Iteration 2049, loss = 0.17464890\n",
      "Iteration 1884, loss = 0.15226444\n",
      "Iteration 530, loss = 0.29816351\n",
      "Iteration 1038, loss = 0.30566897\n",
      "Iteration 531, loss = 0.29809013\n",
      "Iteration 2102, loss = 0.19021776\n",
      "Iteration 532, loss = 0.29799090\n",
      "Iteration 533, loss = 0.29790096\n",
      "Iteration 960, loss = 0.27541631\n",
      "Iteration 534, loss = 0.29782124\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1072, loss = 0.28011062\n",
      "Iteration 1039, loss = 0.30556717\n",
      "Iteration 961, loss = 0.27523314\n",
      "Iteration 2103, loss = 0.19016584\n",
      "Iteration 210, loss = 0.44056291\n",
      "Iteration 2050, loss = 0.17461347\n",
      "Iteration 962, loss = 0.27513122\n",
      "Iteration 963, loss = 0.27493226\n",
      "Iteration 1073, loss = 0.28012344\n",
      "Iteration 1040, loss = 0.30546772\n",
      "Iteration 2051, loss = 0.17446991\n",
      "Iteration 1885, loss = 0.15216411\n",
      "Iteration 2052, loss = 0.17452378\n",
      "Iteration 1041, loss = 0.30537759\n",
      "Iteration 211, loss = 0.44020194\n",
      "Iteration 1, loss = 0.70126928\n",
      "Iteration 2053, loss = 0.17430361\n",
      "Iteration 964, loss = 0.27462410Iteration 2, loss = 0.69950285\n",
      "\n",
      "Iteration 2054, loss = 0.17418528\n",
      "Iteration 1074, loss = 0.27980821\n",
      "Iteration 3, loss = 0.69671957\n",
      "Iteration 2104, loss = 0.19006133\n",
      "Iteration 1886, loss = 0.15210176\n",
      "Iteration 4, loss = 0.69324308\n",
      "Iteration 2055, loss = 0.17402486\n",
      "Iteration 965, loss = 0.27447119\n",
      "Iteration 5, loss = 0.68942440\n",
      "Iteration 1042, loss = 0.30523579\n",
      "Iteration 2056, loss = 0.17391652\n",
      "Iteration 6, loss = 0.68524952\n",
      "Iteration 7, loss = 0.68099545\n",
      "Iteration 8, loss = 0.67643989\n",
      "Iteration 2057, loss = 0.17383142\n",
      "Iteration 212, loss = 0.43988413\n",
      "Iteration 2105, loss = 0.19004973\n",
      "Iteration 9, loss = 0.67199422\n",
      "Iteration 1075, loss = 0.27958160\n",
      "Iteration 1043, loss = 0.30511311\n",
      "Iteration 966, loss = 0.27422185\n",
      "Iteration 1887, loss = 0.15202352\n",
      "Iteration 10, loss = 0.66746453\n",
      "Iteration 11, loss = 0.66294245\n",
      "Iteration 12, loss = 0.65835670\n",
      "Iteration 13, loss = 0.65395971\n",
      "Iteration 2058, loss = 0.17367230\n",
      "Iteration 213, loss = 0.43950283\n",
      "Iteration 2106, loss = 0.18984676\n",
      "Iteration 967, loss = 0.27401824\n",
      "Iteration 14, loss = 0.64956144\n",
      "Iteration 15, loss = 0.64510969\n",
      "Iteration 1044, loss = 0.30500845\n",
      "Iteration 2059, loss = 0.17360262\n",
      "Iteration 968, loss = 0.27378858\n",
      "Iteration 16, loss = 0.64085510\n",
      "Iteration 2060, loss = 0.17356759\n",
      "Iteration 17, loss = 0.63638354\n",
      "Iteration 2107, loss = 0.18977479\n",
      "Iteration 2061, loss = 0.17341384\n",
      "Iteration 18, loss = 0.63207822\n",
      "Iteration 1076, loss = 0.27978713\n",
      "Iteration 1888, loss = 0.15196690\n",
      "Iteration 2062, loss = 0.17328186\n",
      "Iteration 19, loss = 0.62800973\n",
      "Iteration 214, loss = 0.43917799\n",
      "Iteration 20, loss = 0.62386615\n",
      "Iteration 2108, loss = 0.18965294\n",
      "Iteration 21, loss = 0.61975883\n",
      "Iteration 22, loss = 0.61562970\n",
      "Iteration 1045, loss = 0.30493910\n",
      "Iteration 969, loss = 0.27358061\n",
      "Iteration 2063, loss = 0.17318512\n",
      "Iteration 23, loss = 0.61164606\n",
      "Iteration 24, loss = 0.60757658\n",
      "Iteration 2109, loss = 0.18963091\n",
      "Iteration 25, loss = 0.60358840\n",
      "Iteration 215, loss = 0.43884586\n",
      "Iteration 1046, loss = 0.30477597\n",
      "Iteration 26, loss = 0.59968387\n",
      "Iteration 970, loss = 0.27333070\n",
      "Iteration 2064, loss = 0.17308918\n",
      "Iteration 1077, loss = 0.27935924\n",
      "Iteration 27, loss = 0.59576750\n",
      "Iteration 1889, loss = 0.15195421\n",
      "Iteration 28, loss = 0.59184657\n",
      "Iteration 29, loss = 0.58804091\n",
      "Iteration 2110, loss = 0.18950516\n",
      "Iteration 30, loss = 0.58426300\n",
      "Iteration 31, loss = 0.58050596\n",
      "Iteration 971, loss = 0.27316433\n",
      "Iteration 1890, loss = 0.15189291\n",
      "Iteration 32, loss = 0.57668441\n",
      "Iteration 1047, loss = 0.30463911\n",
      "Iteration 33, loss = 0.57291773\n",
      "Iteration 34, loss = 0.56924269\n",
      "Iteration 1078, loss = 0.27904375\n",
      "Iteration 2065, loss = 0.17299250\n",
      "Iteration 35, loss = 0.56552843\n",
      "Iteration 972, loss = 0.27293941\n",
      "Iteration 216, loss = 0.43853141\n",
      "Iteration 36, loss = 0.56180709\n",
      "Iteration 1891, loss = 0.15183204\n",
      "Iteration 2066, loss = 0.17287863\n",
      "Iteration 2111, loss = 0.18945768\n",
      "Iteration 1048, loss = 0.30452886\n",
      "Iteration 37, loss = 0.55820897\n",
      "Iteration 973, loss = 0.27271637\n",
      "Iteration 1079, loss = 0.27890112\n",
      "Iteration 2067, loss = 0.17272066\n",
      "Iteration 38, loss = 0.55466960\n",
      "Iteration 217, loss = 0.43824993\n",
      "Iteration 2068, loss = 0.17270354\n",
      "Iteration 39, loss = 0.55101232\n",
      "Iteration 2069, loss = 0.17252126\n",
      "Iteration 1080, loss = 0.27872985\n",
      "Iteration 40, loss = 0.54752395\n",
      "Iteration 41, loss = 0.54399051\n",
      "Iteration 218, loss = 0.43787861\n",
      "Iteration 2070, loss = 0.17240335\n",
      "Iteration 2112, loss = 0.18958676\n",
      "Iteration 42, loss = 0.54043788\n",
      "Iteration 43, loss = 0.53699050\n",
      "Iteration 974, loss = 0.27267924\n",
      "Iteration 44, loss = 0.53361210\n",
      "Iteration 45, loss = 0.53013430\n",
      "Iteration 1892, loss = 0.15176385\n",
      "Iteration 2113, loss = 0.18927880\n",
      "Iteration 46, loss = 0.52677690\n",
      "Iteration 975, loss = 0.27233702\n",
      "Iteration 47, loss = 0.52349384\n",
      "Iteration 1049, loss = 0.30443010\n",
      "Iteration 48, loss = 0.52019899\n",
      "Iteration 2071, loss = 0.17232568\n",
      "Iteration 49, loss = 0.51689167\n",
      "Iteration 50, loss = 0.51378013\n",
      "Iteration 1081, loss = 0.27851320\n",
      "Iteration 51, loss = 0.51055312\n",
      "Iteration 1050, loss = 0.30431813\n",
      "Iteration 2072, loss = 0.17231263\n",
      "Iteration 52, loss = 0.50735105\n",
      "Iteration 1893, loss = 0.15176948\n",
      "Iteration 2073, loss = 0.17214127\n",
      "Iteration 976, loss = 0.27215968\n",
      "Iteration 53, loss = 0.50421680\n",
      "Iteration 2074, loss = 0.17210833\n",
      "Iteration 219, loss = 0.43757304\n",
      "Iteration 977, loss = 0.27194793\n",
      "Iteration 54, loss = 0.50114840\n",
      "Iteration 2114, loss = 0.18935834\n",
      "Iteration 55, loss = 0.49814873\n",
      "Iteration 1082, loss = 0.27834636\n",
      "Iteration 56, loss = 0.49512596\n",
      "Iteration 220, loss = 0.43727602\n",
      "Iteration 2075, loss = 0.17194585\n",
      "Iteration 2115, loss = 0.18918485\n",
      "Iteration 57, loss = 0.49213738\n",
      "Iteration 1894, loss = 0.15164602\n",
      "Iteration 1051, loss = 0.30421909\n",
      "Iteration 978, loss = 0.27172484\n",
      "Iteration 58, loss = 0.48918851\n",
      "Iteration 979, loss = 0.27149688\n",
      "Iteration 1083, loss = 0.27814646\n",
      "Iteration 59, loss = 0.48638698\n",
      "Iteration 1895, loss = 0.15166082\n",
      "Iteration 2116, loss = 0.18904006\n",
      "Iteration 60, loss = 0.48347874\n",
      "Iteration 61, loss = 0.48075766\n",
      "Iteration 2076, loss = 0.17182361\n",
      "Iteration 221, loss = 0.43698122\n",
      "Iteration 62, loss = 0.47792755\n",
      "Iteration 980, loss = 0.27127296\n",
      "Iteration 1052, loss = 0.30411093\n",
      "Iteration 2077, loss = 0.17170867\n",
      "Iteration 1896, loss = 0.15157313\n",
      "Iteration 981, loss = 0.27110345\n",
      "Iteration 2078, loss = 0.17158749\n",
      "Iteration 2117, loss = 0.18891742\n",
      "Iteration 982, loss = 0.27095438\n",
      "Iteration 63, loss = 0.47514730\n",
      "Iteration 64, loss = 0.47253262\n",
      "Iteration 65, loss = 0.46986967\n",
      "Iteration 1053, loss = 0.30395219\n",
      "Iteration 983, loss = 0.27084403\n",
      "Iteration 2079, loss = 0.17153893\n",
      "Iteration 1084, loss = 0.27795966\n",
      "Iteration 66, loss = 0.46730425\n",
      "Iteration 222, loss = 0.43668885\n",
      "Iteration 2118, loss = 0.18888820\n",
      "Iteration 67, loss = 0.46482320\n",
      "Iteration 2080, loss = 0.17141330\n",
      "Iteration 68, loss = 0.46228985\n",
      "Iteration 2081, loss = 0.17126900\n",
      "Iteration 69, loss = 0.45985526\n",
      "Iteration 1054, loss = 0.30387215\n",
      "Iteration 2119, loss = 0.18866697\n",
      "Iteration 2082, loss = 0.17145346\n",
      "Iteration 70, loss = 0.45738848\n",
      "Iteration 71, loss = 0.45506946Iteration 1085, loss = 0.27794560\n",
      "\n",
      "Iteration 72, loss = 0.45275011\n",
      "Iteration 73, loss = 0.45047421\n",
      "Iteration 223, loss = 0.43635788\n",
      "Iteration 2083, loss = 0.17113187\n",
      "Iteration 74, loss = 0.44819820\n",
      "Iteration 1897, loss = 0.15153309\n",
      "Iteration 984, loss = 0.27051195\n",
      "Iteration 2120, loss = 0.18858296\n",
      "Iteration 2084, loss = 0.17102204\n",
      "Iteration 1055, loss = 0.30374707\n",
      "Iteration 75, loss = 0.44606210\n",
      "Iteration 985, loss = 0.27023177\n",
      "Iteration 76, loss = 0.44387906\n",
      "Iteration 77, loss = 0.44174237\n",
      "Iteration 1898, loss = 0.15147162\n",
      "Iteration 2085, loss = 0.17089298\n",
      "Iteration 78, loss = 0.43964186\n",
      "Iteration 2121, loss = 0.18855766\n",
      "Iteration 986, loss = 0.27008427\n",
      "Iteration 224, loss = 0.43608788\n",
      "Iteration 1056, loss = 0.30364176\n",
      "Iteration 2086, loss = 0.17078571\n",
      "Iteration 1899, loss = 0.15145422\n",
      "Iteration 2087, loss = 0.17064716\n",
      "Iteration 987, loss = 0.26990811\n",
      "Iteration 225, loss = 0.43574596\n",
      "Iteration 1057, loss = 0.30352522\n",
      "Iteration 2088, loss = 0.17056635\n",
      "Iteration 988, loss = 0.26967515\n",
      "Iteration 79, loss = 0.43761496\n",
      "Iteration 80, loss = 0.43561524\n",
      "Iteration 1900, loss = 0.15141366\n",
      "Iteration 989, loss = 0.26939474\n",
      "Iteration 81, loss = 0.43367701\n",
      "Iteration 2089, loss = 0.17052157\n",
      "Iteration 1086, loss = 0.27763820\n",
      "Iteration 226, loss = 0.43546189\n",
      "Iteration 82, loss = 0.43173311\n",
      "Iteration 990, loss = 0.26916265\n",
      "Iteration 2090, loss = 0.17035053\n",
      "Iteration 2122, loss = 0.18848923\n",
      "Iteration 2091, loss = 0.17030610\n",
      "Iteration 83, loss = 0.42986634\n",
      "Iteration 1058, loss = 0.30339246\n",
      "Iteration 991, loss = 0.26900183\n",
      "Iteration 1087, loss = 0.27747389\n",
      "Iteration 84, loss = 0.42801672\n",
      "Iteration 2092, loss = 0.17011615\n",
      "Iteration 85, loss = 0.42617377\n",
      "Iteration 2123, loss = 0.18833065\n",
      "Iteration 2093, loss = 0.17002442\n",
      "Iteration 1059, loss = 0.30326672\n",
      "Iteration 86, loss = 0.42434458\n",
      "Iteration 992, loss = 0.26875117\n",
      "Iteration 87, loss = 0.42267747\n",
      "Iteration 2094, loss = 0.16999553\n",
      "Iteration 1901, loss = 0.15142936\n",
      "Iteration 88, loss = 0.42090324\n",
      "Iteration 89, loss = 0.41925152\n",
      "Iteration 2124, loss = 0.18823231\n",
      "Iteration 993, loss = 0.26893754\n",
      "Iteration 90, loss = 0.41762563\n",
      "Iteration 2095, loss = 0.16985498\n",
      "Iteration 1060, loss = 0.30319061\n",
      "Iteration 91, loss = 0.41600816\n",
      "Iteration 227, loss = 0.43515715\n",
      "Iteration 1902, loss = 0.15127115\n",
      "Iteration 1088, loss = 0.27735425\n",
      "Iteration 92, loss = 0.41441552\n",
      "Iteration 994, loss = 0.26835855\n",
      "Iteration 93, loss = 0.41289123\n",
      "Iteration 94, loss = 0.41132534\n",
      "Iteration 95, loss = 0.40991768\n",
      "Iteration 2096, loss = 0.16981605\n",
      "Iteration 2125, loss = 0.18814613\n",
      "Iteration 995, loss = 0.26813416\n",
      "Iteration 2097, loss = 0.16967054\n",
      "Iteration 1061, loss = 0.30308657\n",
      "Iteration 96, loss = 0.40843454\n",
      "Iteration 97, loss = 0.40696699\n",
      "Iteration 2098, loss = 0.16953722\n",
      "Iteration 98, loss = 0.40558215\n",
      "Iteration 2126, loss = 0.18802958\n",
      "Iteration 1903, loss = 0.15115719\n",
      "Iteration 2099, loss = 0.16938028\n",
      "Iteration 99, loss = 0.40424838\n",
      "Iteration 1062, loss = 0.30294719\n",
      "Iteration 996, loss = 0.26800768\n",
      "Iteration 1089, loss = 0.27716052\n",
      "Iteration 100, loss = 0.40285212\n",
      "Iteration 2100, loss = 0.16931710\n",
      "Iteration 101, loss = 0.40153787\n",
      "Iteration 2127, loss = 0.18801689\n",
      "Iteration 102, loss = 0.40031248\n",
      "Iteration 1063, loss = 0.30280922\n",
      "Iteration 2101, loss = 0.16935746\n",
      "Iteration 103, loss = 0.39900338\n",
      "Iteration 2102, loss = 0.16907552\n",
      "Iteration 104, loss = 0.39776397\n",
      "Iteration 997, loss = 0.26773932\n",
      "Iteration 1904, loss = 0.15114206\n",
      "Iteration 1090, loss = 0.27689673\n",
      "Iteration 2103, loss = 0.16904220\n",
      "Iteration 105, loss = 0.39654227\n",
      "Iteration 998, loss = 0.26758968\n",
      "Iteration 1064, loss = 0.30269901\n",
      "Iteration 106, loss = 0.39537753\n",
      "Iteration 107, loss = 0.39419892\n",
      "Iteration 108, loss = 0.39301821\n",
      "Iteration 109, loss = 0.39192524\n",
      "Iteration 2128, loss = 0.18800282\n",
      "Iteration 2104, loss = 0.16889200\n",
      "Iteration 999, loss = 0.26729672\n",
      "Iteration 1905, loss = 0.15102965\n",
      "Iteration 110, loss = 0.39079712\n",
      "Iteration 1065, loss = 0.30259820\n",
      "Iteration 2105, loss = 0.16879248\n",
      "Iteration 2129, loss = 0.18798403\n",
      "Iteration 2106, loss = 0.16867186\n",
      "Iteration 111, loss = 0.38977827\n",
      "Iteration 2107, loss = 0.16853657\n",
      "Iteration 1091, loss = 0.27673749\n",
      "Iteration 112, loss = 0.38870818\n",
      "Iteration 1906, loss = 0.15100920\n",
      "Iteration 1000, loss = 0.26702197\n",
      "Iteration 2108, loss = 0.16854121\n",
      "Iteration 113, loss = 0.38766224\n",
      "Iteration 1066, loss = 0.30246973\n",
      "Iteration 2130, loss = 0.18770643\n",
      "Iteration 114, loss = 0.38664672\n",
      "Iteration 115, loss = 0.38562870\n",
      "Iteration 1001, loss = 0.26705776\n",
      "Iteration 2109, loss = 0.16838239\n",
      "Iteration 116, loss = 0.38466863\n",
      "Iteration 1907, loss = 0.15102064\n",
      "Iteration 1067, loss = 0.30239028\n",
      "Iteration 117, loss = 0.38368565\n",
      "Iteration 118, loss = 0.38282921\n",
      "Iteration 2131, loss = 0.18759413\n",
      "Iteration 119, loss = 0.38184903\n",
      "Iteration 2110, loss = 0.16824588\n",
      "Iteration 120, loss = 0.38093477\n",
      "Iteration 1002, loss = 0.26664059\n",
      "Iteration 1092, loss = 0.27662080\n",
      "Iteration 1068, loss = 0.30223777\n",
      "Iteration 121, loss = 0.38005350\n",
      "Iteration 2111, loss = 0.16823307\n",
      "Iteration 122, loss = 0.37917590\n",
      "Iteration 123, loss = 0.37830184\n",
      "Iteration 1003, loss = 0.26639536\n",
      "Iteration 2112, loss = 0.16811273\n",
      "Iteration 124, loss = 0.37752170\n",
      "Iteration 1069, loss = 0.30215446\n",
      "Iteration 125, loss = 0.37666775\n",
      "Iteration 2113, loss = 0.16794313\n",
      "Iteration 126, loss = 0.37589779\n",
      "Iteration 1908, loss = 0.15095737\n",
      "Iteration 1004, loss = 0.26625972\n",
      "Iteration 127, loss = 0.37503906\n",
      "Iteration 2132, loss = 0.18750522\n",
      "Iteration 128, loss = 0.37428158\n",
      "Iteration 1093, loss = 0.27639727\n",
      "Iteration 129, loss = 0.37349752\n",
      "Iteration 2114, loss = 0.16801561\n",
      "Iteration 130, loss = 0.37277935\n",
      "Iteration 1005, loss = 0.26613595\n",
      "Iteration 131, loss = 0.37199655\n",
      "Iteration 2115, loss = 0.16777498\n",
      "Iteration 1070, loss = 0.30200069\n",
      "Iteration 132, loss = 0.37128883\n",
      "Iteration 133, loss = 0.37056721\n",
      "Iteration 2133, loss = 0.18739993\n",
      "Iteration 134, loss = 0.36986533\n",
      "Iteration 1006, loss = 0.26578821\n",
      "Iteration 135, loss = 0.36919183\n",
      "Iteration 136, loss = 0.36847671\n",
      "Iteration 1071, loss = 0.30192603\n",
      "Iteration 1094, loss = 0.27619557\n",
      "Iteration 1909, loss = 0.15083006\n",
      "Iteration 137, loss = 0.36780401\n",
      "Iteration 2116, loss = 0.16765493\n",
      "Iteration 2134, loss = 0.18742400\n",
      "Iteration 138, loss = 0.36714975\n",
      "Iteration 139, loss = 0.36648355\n",
      "Iteration 2117, loss = 0.16757189\n",
      "Iteration 1007, loss = 0.26556300\n",
      "Iteration 1072, loss = 0.30178342\n",
      "Iteration 2118, loss = 0.16750244\n",
      "Iteration 2119, loss = 0.16745132\n",
      "Iteration 140, loss = 0.36586157\n",
      "Iteration 228, loss = 0.43485882\n",
      "Iteration 1095, loss = 0.27631308\n",
      "Iteration 2135, loss = 0.18723809\n",
      "Iteration 1910, loss = 0.15077587\n",
      "Iteration 141, loss = 0.36525500\n",
      "Iteration 1008, loss = 0.26530959\n",
      "Iteration 1073, loss = 0.30174971\n",
      "Iteration 142, loss = 0.36458164\n",
      "Iteration 2120, loss = 0.16725931\n",
      "Iteration 143, loss = 0.36399077\n",
      "Iteration 144, loss = 0.36337405\n",
      "Iteration 2136, loss = 0.18725393Iteration 1096, loss = 0.27597162\n",
      "\n",
      "Iteration 145, loss = 0.36278896\n",
      "Iteration 146, loss = 0.36222064\n",
      "Iteration 147, loss = 0.36164036\n",
      "Iteration 1074, loss = 0.30154181\n",
      "Iteration 1009, loss = 0.26513218\n",
      "Iteration 2121, loss = 0.16712126\n",
      "Iteration 148, loss = 0.36104696\n",
      "Iteration 149, loss = 0.36051965\n",
      "Iteration 150, loss = 0.35996517\n",
      "Iteration 151, loss = 0.35941101\n",
      "Iteration 229, loss = 0.43470451\n",
      "Iteration 1911, loss = 0.15072048\n",
      "Iteration 1010, loss = 0.26491558\n",
      "Iteration 2137, loss = 0.18713100\n",
      "Iteration 152, loss = 0.35889629\n",
      "Iteration 2122, loss = 0.16706032\n",
      "Iteration 1097, loss = 0.27573306\n",
      "Iteration 1075, loss = 0.30148287\n",
      "Iteration 2123, loss = 0.16703280\n",
      "Iteration 153, loss = 0.35835153\n",
      "Iteration 1011, loss = 0.26465784\n",
      "Iteration 154, loss = 0.35782451\n",
      "Iteration 155, loss = 0.35737587\n",
      "Iteration 1076, loss = 0.30133406\n",
      "Iteration 1912, loss = 0.15066428\n",
      "Iteration 1012, loss = 0.26451701\n",
      "Iteration 2124, loss = 0.16685560\n",
      "Iteration 156, loss = 0.35681124\n",
      "Iteration 2138, loss = 0.18698907\n",
      "Iteration 230, loss = 0.43434818\n",
      "Iteration 1013, loss = 0.26436019\n",
      "Iteration 157, loss = 0.35631506\n",
      "Iteration 158, loss = 0.35584072\n",
      "Iteration 1098, loss = 0.27578624\n",
      "Iteration 159, loss = 0.35536639\n",
      "Iteration 1077, loss = 0.30119975\n",
      "Iteration 2125, loss = 0.16674750\n",
      "Iteration 160, loss = 0.35493387\n",
      "Iteration 1014, loss = 0.26407279\n",
      "Iteration 161, loss = 0.35440332\n",
      "Iteration 2126, loss = 0.16700415\n",
      "Iteration 162, loss = 0.35395671\n",
      "Iteration 2139, loss = 0.18690457\n",
      "Iteration 163, loss = 0.35346759\n",
      "Iteration 231, loss = 0.43402406\n",
      "Iteration 164, loss = 0.35302452\n",
      "Iteration 1913, loss = 0.15064329\n",
      "Iteration 1099, loss = 0.27544674\n",
      "Iteration 165, loss = 0.35258481\n",
      "Iteration 2127, loss = 0.16658140\n",
      "Iteration 1015, loss = 0.26389062\n",
      "Iteration 166, loss = 0.35212244\n",
      "Iteration 2128, loss = 0.16640818\n",
      "Iteration 1078, loss = 0.30106371\n",
      "Iteration 167, loss = 0.35168591\n",
      "Iteration 2129, loss = 0.16638600\n",
      "Iteration 2140, loss = 0.18678469\n",
      "Iteration 168, loss = 0.35126448\n",
      "Iteration 232, loss = 0.43376962\n",
      "Iteration 1914, loss = 0.15055893\n",
      "Iteration 1016, loss = 0.26365867\n",
      "Iteration 2130, loss = 0.16620580\n",
      "Iteration 169, loss = 0.35082528\n",
      "Iteration 1100, loss = 0.27519248\n",
      "Iteration 2141, loss = 0.18689422\n",
      "Iteration 170, loss = 0.35044648\n",
      "Iteration 1079, loss = 0.30096498\n",
      "Iteration 171, loss = 0.34999723\n",
      "Iteration 1915, loss = 0.15051903\n",
      "Iteration 2142, loss = 0.18674696\n",
      "Iteration 2131, loss = 0.16609972\n",
      "Iteration 172, loss = 0.34962065\n",
      "Iteration 1017, loss = 0.26338181\n",
      "Iteration 1080, loss = 0.30089255\n",
      "Iteration 173, loss = 0.34917640\n",
      "Iteration 233, loss = 0.43349884\n",
      "Iteration 1101, loss = 0.27506623\n",
      "Iteration 2132, loss = 0.16600718\n",
      "Iteration 1916, loss = 0.15050139\n",
      "Iteration 174, loss = 0.34877461\n",
      "Iteration 2143, loss = 0.18654314\n",
      "Iteration 1018, loss = 0.26352979\n",
      "Iteration 2133, loss = 0.16593179\n",
      "Iteration 1081, loss = 0.30071073\n",
      "Iteration 175, loss = 0.34838500\n",
      "Iteration 2134, loss = 0.16589254\n",
      "Iteration 176, loss = 0.34799405\n",
      "Iteration 2135, loss = 0.16580804\n",
      "Iteration 1019, loss = 0.26295251\n",
      "Iteration 1917, loss = 0.15045467\n",
      "Iteration 177, loss = 0.34762056\n",
      "Iteration 1102, loss = 0.27480048\n",
      "Iteration 178, loss = 0.34721601\n",
      "Iteration 2136, loss = 0.16559954\n",
      "Iteration 179, loss = 0.34685982\n",
      "Iteration 180, loss = 0.34647613\n",
      "Iteration 234, loss = 0.43321693\n",
      "Iteration 1082, loss = 0.30062518\n",
      "Iteration 181, loss = 0.34614314\n",
      "Iteration 2144, loss = 0.18644950\n",
      "Iteration 1020, loss = 0.26277733\n",
      "Iteration 182, loss = 0.34575359\n",
      "Iteration 1918, loss = 0.15038636\n",
      "Iteration 183, loss = 0.34536863\n",
      "Iteration 184, loss = 0.34503881\n",
      "Iteration 2137, loss = 0.16555250\n",
      "Iteration 185, loss = 0.34467697\n",
      "Iteration 235, loss = 0.43297689\n",
      "Iteration 186, loss = 0.34432139\n",
      "Iteration 187, loss = 0.34399564\n",
      "Iteration 2138, loss = 0.16546595\n",
      "Iteration 188, loss = 0.34362992\n",
      "Iteration 2139, loss = 0.16531512\n",
      "Iteration 1103, loss = 0.27466740\n",
      "Iteration 1021, loss = 0.26251545\n",
      "Iteration 2145, loss = 0.18634891\n",
      "Iteration 1083, loss = 0.30050087\n",
      "Iteration 189, loss = 0.34330462\n",
      "Iteration 236, loss = 0.43266288\n",
      "Iteration 190, loss = 0.34296747\n",
      "Iteration 2140, loss = 0.16526633\n",
      "Iteration 191, loss = 0.34263604\n",
      "Iteration 192, loss = 0.34231899\n",
      "Iteration 2141, loss = 0.16509670\n",
      "Iteration 1919, loss = 0.15035468\n",
      "Iteration 1104, loss = 0.27442298\n",
      "Iteration 2146, loss = 0.18638310\n",
      "Iteration 193, loss = 0.34200666\n",
      "Iteration 194, loss = 0.34167728\n",
      "Iteration 2142, loss = 0.16508514\n",
      "Iteration 1084, loss = 0.30038200\n",
      "Iteration 195, loss = 0.34135484\n",
      "Iteration 2143, loss = 0.16496854\n",
      "Iteration 237, loss = 0.43237569\n",
      "Iteration 196, loss = 0.34102423\n",
      "Iteration 2144, loss = 0.16489659\n",
      "Iteration 2145, loss = 0.16472379\n",
      "Iteration 197, loss = 0.34072560\n",
      "Iteration 1105, loss = 0.27427129\n",
      "Iteration 2147, loss = 0.18612947\n",
      "Iteration 198, loss = 0.34042036\n",
      "Iteration 2146, loss = 0.16463720\n",
      "Iteration 1920, loss = 0.15029913\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1085, loss = 0.30025127\n",
      "Iteration 199, loss = 0.34012476\n",
      "Iteration 2147, loss = 0.16461750\n",
      "Iteration 238, loss = 0.43215446\n",
      "Iteration 2148, loss = 0.16444257\n",
      "Iteration 1086, loss = 0.30019715\n",
      "Iteration 200, loss = 0.33981957\n",
      "Iteration 201, loss = 0.33951540\n",
      "Iteration 2149, loss = 0.16429332\n",
      "Iteration 2148, loss = 0.18616455\n",
      "Iteration 202, loss = 0.33925773\n",
      "Iteration 1106, loss = 0.27410856\n",
      "Iteration 239, loss = 0.43188097\n",
      "Iteration 1, loss = 0.69211227\n",
      "Iteration 1087, loss = 0.30008077\n",
      "Iteration 2150, loss = 0.16420323\n",
      "Iteration 203, loss = 0.33894465\n",
      "Iteration 2, loss = 0.69039151\n",
      "Iteration 204, loss = 0.33867560\n",
      "Iteration 3, loss = 0.68769962\n",
      "Iteration 205, loss = 0.33836576\n",
      "Iteration 2151, loss = 0.16434897\n",
      "Iteration 206, loss = 0.33811001\n",
      "Iteration 2149, loss = 0.18595524\n",
      "Iteration 207, loss = 0.33782237\n",
      "Iteration 4, loss = 0.68444579\n",
      "Iteration 208, loss = 0.33754043\n",
      "Iteration 1107, loss = 0.27397211\n",
      "Iteration 2152, loss = 0.16397477\n",
      "Iteration 5, loss = 0.68084117\n",
      "Iteration 209, loss = 0.33726855\n",
      "Iteration 240, loss = 0.43162886\n",
      "Iteration 1088, loss = 0.29992242\n",
      "Iteration 210, loss = 0.33698610\n",
      "Iteration 6, loss = 0.67668567\n",
      "Iteration 211, loss = 0.33672694\n",
      "Iteration 7, loss = 0.67252318\n",
      "Iteration 2153, loss = 0.16407936\n",
      "Iteration 212, loss = 0.33645532\n",
      "Iteration 2150, loss = 0.18586754\n",
      "Iteration 213, loss = 0.33619638\n",
      "Iteration 214, loss = 0.33593667\n",
      "Iteration 8, loss = 0.66825889\n",
      "Iteration 1108, loss = 0.27368031\n",
      "Iteration 215, loss = 0.33568568\n",
      "Iteration 241, loss = 0.43137663\n",
      "Iteration 216, loss = 0.33542544\n",
      "Iteration 1089, loss = 0.29979602\n",
      "Iteration 217, loss = 0.33517787\n",
      "Iteration 2154, loss = 0.16401003\n",
      "Iteration 218, loss = 0.33493920\n",
      "Iteration 9, loss = 0.66395685\n",
      "Iteration 219, loss = 0.33465378\n",
      "Iteration 2151, loss = 0.18581541\n",
      "Iteration 2155, loss = 0.16377231\n",
      "Iteration 10, loss = 0.65952944\n",
      "Iteration 220, loss = 0.33442434\n",
      "Iteration 221, loss = 0.33418556\n",
      "Iteration 1090, loss = 0.29967333\n",
      "Iteration 2156, loss = 0.16366145\n",
      "Iteration 11, loss = 0.65508437\n",
      "Iteration 222, loss = 0.33394247\n",
      "Iteration 2152, loss = 0.18582372\n",
      "Iteration 223, loss = 0.33369019\n",
      "Iteration 2157, loss = 0.16351730\n",
      "Iteration 224, loss = 0.33345638\n",
      "Iteration 1109, loss = 0.27355931\n",
      "Iteration 242, loss = 0.43111349\n",
      "Iteration 12, loss = 0.65077069\n",
      "Iteration 225, loss = 0.33320685\n",
      "Iteration 226, loss = 0.33298311\n",
      "Iteration 13, loss = 0.64645085\n",
      "Iteration 2158, loss = 0.16340760\n",
      "Iteration 227, loss = 0.33275701\n",
      "Iteration 2153, loss = 0.18563647\n",
      "Iteration 14, loss = 0.64229255\n",
      "Iteration 228, loss = 0.33254209\n",
      "Iteration 229, loss = 0.33230257\n",
      "Iteration 15, loss = 0.63806700\n",
      "Iteration 230, loss = 0.33209075\n",
      "Iteration 1091, loss = 0.29955643\n",
      "Iteration 16, loss = 0.63381610\n",
      "Iteration 2159, loss = 0.16338556\n",
      "Iteration 231, loss = 0.33182377\n",
      "Iteration 232, loss = 0.33162194\n",
      "Iteration 2154, loss = 0.18562758\n",
      "Iteration 17, loss = 0.62968039\n",
      "Iteration 2160, loss = 0.16321721\n",
      "Iteration 1110, loss = 0.27346420\n",
      "Iteration 233, loss = 0.33138354\n",
      "Iteration 18, loss = 0.62567364\n",
      "Iteration 1092, loss = 0.29943655\n",
      "Iteration 243, loss = 0.43086653\n",
      "Iteration 234, loss = 0.33115628\n",
      "Iteration 2161, loss = 0.16312204\n",
      "Iteration 235, loss = 0.33093218\n",
      "Iteration 19, loss = 0.62170109\n",
      "Iteration 2155, loss = 0.18544046\n",
      "Iteration 20, loss = 0.61773218\n",
      "Iteration 21, loss = 0.61381176\n",
      "Iteration 236, loss = 0.33071950\n",
      "Iteration 22, loss = 0.60997154\n",
      "Iteration 244, loss = 0.43063405\n",
      "Iteration 2162, loss = 0.16309595\n",
      "Iteration 237, loss = 0.33050244\n",
      "Iteration 23, loss = 0.60614324\n",
      "Iteration 1111, loss = 0.27323764\n",
      "Iteration 1093, loss = 0.29930292\n",
      "Iteration 24, loss = 0.60227644\n",
      "Iteration 238, loss = 0.33028793\n",
      "Iteration 25, loss = 0.59866333\n",
      "Iteration 239, loss = 0.33004890\n",
      "Iteration 2156, loss = 0.18540155\n",
      "Iteration 26, loss = 0.59488439\n",
      "Iteration 2163, loss = 0.16301201\n",
      "Iteration 1022, loss = 0.26232483\n",
      "Iteration 240, loss = 0.32986982\n",
      "Iteration 245, loss = 0.43037123\n",
      "Iteration 241, loss = 0.32965972\n",
      "Iteration 27, loss = 0.59129986\n",
      "Iteration 242, loss = 0.32945198\n",
      "Iteration 2164, loss = 0.16297483\n",
      "Iteration 1094, loss = 0.29923668\n",
      "Iteration 28, loss = 0.58766645\n",
      "Iteration 243, loss = 0.32923082\n",
      "Iteration 1112, loss = 0.27304250\n",
      "Iteration 244, loss = 0.32904597\n",
      "Iteration 2165, loss = 0.16273269\n",
      "Iteration 245, loss = 0.32882219\n",
      "Iteration 246, loss = 0.32860826\n",
      "Iteration 1095, loss = 0.29909200\n",
      "Iteration 2157, loss = 0.18541617\n",
      "Iteration 247, loss = 0.32842216\n",
      "Iteration 29, loss = 0.58413258\n",
      "Iteration 248, loss = 0.32820564\n",
      "Iteration 246, loss = 0.43016692\n",
      "Iteration 30, loss = 0.58070611\n",
      "Iteration 249, loss = 0.32801841\n",
      "Iteration 250, loss = 0.32781210\n",
      "Iteration 31, loss = 0.57718802\n",
      "Iteration 1023, loss = 0.26215110\n",
      "Iteration 2166, loss = 0.16270175\n",
      "Iteration 1096, loss = 0.29896232\n",
      "Iteration 2167, loss = 0.16253059\n",
      "Iteration 2158, loss = 0.18525524\n",
      "Iteration 1113, loss = 0.27293585\n",
      "Iteration 32, loss = 0.57371281\n",
      "Iteration 251, loss = 0.32763110\n",
      "Iteration 1097, loss = 0.29894148\n",
      "Iteration 1024, loss = 0.26183748\n",
      "Iteration 252, loss = 0.32743937\n",
      "Iteration 2168, loss = 0.16241700\n",
      "Iteration 2159, loss = 0.18507889\n",
      "Iteration 253, loss = 0.32724769\n",
      "Iteration 1098, loss = 0.29878453\n",
      "Iteration 1114, loss = 0.27270925\n",
      "Iteration 254, loss = 0.32702529\n",
      "Iteration 247, loss = 0.42986625\n",
      "Iteration 255, loss = 0.32684695\n",
      "Iteration 33, loss = 0.57041624\n",
      "Iteration 1025, loss = 0.26165960\n",
      "Iteration 2169, loss = 0.16234438\n",
      "Iteration 256, loss = 0.32665979\n",
      "Iteration 2160, loss = 0.18500112\n",
      "Iteration 34, loss = 0.56699012\n",
      "Iteration 2170, loss = 0.16224235\n",
      "Iteration 257, loss = 0.32646854\n",
      "Iteration 258, loss = 0.32627309\n",
      "Iteration 1026, loss = 0.26144291\n",
      "Iteration 259, loss = 0.32608544\n",
      "Iteration 35, loss = 0.56366122\n",
      "Iteration 260, loss = 0.32590562\n",
      "Iteration 1099, loss = 0.29863168\n",
      "Iteration 261, loss = 0.32573692\n",
      "Iteration 1027, loss = 0.26138100\n",
      "Iteration 262, loss = 0.32553895\n",
      "Iteration 2161, loss = 0.18492817\n",
      "Iteration 2171, loss = 0.16214181\n",
      "Iteration 263, loss = 0.32534954\n",
      "Iteration 248, loss = 0.42980348\n",
      "Iteration 36, loss = 0.56049252\n",
      "Iteration 1115, loss = 0.27255292\n",
      "Iteration 264, loss = 0.32515962\n",
      "Iteration 37, loss = 0.55715355\n",
      "Iteration 265, loss = 0.32500669\n",
      "Iteration 2172, loss = 0.16208188\n",
      "Iteration 266, loss = 0.32481387\n",
      "Iteration 1100, loss = 0.29850057\n",
      "Iteration 267, loss = 0.32464785\n",
      "Iteration 38, loss = 0.55390902\n",
      "Iteration 1028, loss = 0.26099512\n",
      "Iteration 268, loss = 0.32445539\n",
      "Iteration 249, loss = 0.42944321\n",
      "Iteration 39, loss = 0.55074713\n",
      "Iteration 2173, loss = 0.16197931\n",
      "Iteration 2162, loss = 0.18493869\n",
      "Iteration 269, loss = 0.32429782\n",
      "Iteration 1101, loss = 0.29840147\n",
      "Iteration 1029, loss = 0.26085489\n",
      "Iteration 270, loss = 0.32412876\n",
      "Iteration 40, loss = 0.54759748\n",
      "Iteration 271, loss = 0.32395751\n",
      "Iteration 272, loss = 0.32379529\n",
      "Iteration 1030, loss = 0.26058316\n",
      "Iteration 273, loss = 0.32360370\n",
      "Iteration 274, loss = 0.32343981\n",
      "Iteration 275, loss = 0.32327128\n",
      "Iteration 2163, loss = 0.18484275\n",
      "Iteration 250, loss = 0.42917710\n",
      "Iteration 41, loss = 0.54448328\n",
      "Iteration 276, loss = 0.32309900\n",
      "Iteration 277, loss = 0.32293648\n",
      "Iteration 1031, loss = 0.26073432\n",
      "Iteration 278, loss = 0.32277380\n",
      "Iteration 2174, loss = 0.16184734\n",
      "Iteration 42, loss = 0.54140663\n",
      "Iteration 1116, loss = 0.27230199\n",
      "Iteration 279, loss = 0.32262005\n",
      "Iteration 280, loss = 0.32244062\n",
      "Iteration 43, loss = 0.53831389\n",
      "Iteration 2175, loss = 0.16176576\n",
      "Iteration 1032, loss = 0.26014947\n",
      "Iteration 1102, loss = 0.29825745\n",
      "Iteration 281, loss = 0.32229473\n",
      "Iteration 44, loss = 0.53541184\n",
      "Iteration 2164, loss = 0.18476843\n",
      "Iteration 2176, loss = 0.16167240\n",
      "Iteration 1033, loss = 0.25992592\n",
      "Iteration 251, loss = 0.42895119\n",
      "Iteration 282, loss = 0.32213124\n",
      "Iteration 45, loss = 0.53232216\n",
      "Iteration 2177, loss = 0.16161895\n",
      "Iteration 283, loss = 0.32196136\n",
      "Iteration 46, loss = 0.52935246\n",
      "Iteration 1103, loss = 0.29816493\n",
      "Iteration 284, loss = 0.32179691\n",
      "Iteration 1034, loss = 0.25966683\n",
      "Iteration 47, loss = 0.52662962\n",
      "Iteration 285, loss = 0.32165467\n",
      "Iteration 2178, loss = 0.16144961\n",
      "Iteration 286, loss = 0.32149717\n",
      "Iteration 2165, loss = 0.18455563\n",
      "Iteration 48, loss = 0.52369068\n",
      "Iteration 287, loss = 0.32133830\n",
      "Iteration 49, loss = 0.52088065\n",
      "Iteration 1035, loss = 0.25950037\n",
      "Iteration 1117, loss = 0.27214094\n",
      "Iteration 2179, loss = 0.16131660\n",
      "Iteration 1104, loss = 0.29803001\n",
      "Iteration 50, loss = 0.51806349\n",
      "Iteration 252, loss = 0.42871482\n",
      "Iteration 51, loss = 0.51535010\n",
      "Iteration 1036, loss = 0.25938999\n",
      "Iteration 52, loss = 0.51256849\n",
      "Iteration 1105, loss = 0.29793117\n",
      "Iteration 288, loss = 0.32118163\n",
      "Iteration 53, loss = 0.50995512\n",
      "Iteration 1037, loss = 0.25905446\n",
      "Iteration 2166, loss = 0.18466088\n",
      "Iteration 2180, loss = 0.16124577\n",
      "Iteration 289, loss = 0.32102533\n",
      "Iteration 54, loss = 0.50737372\n",
      "Iteration 290, loss = 0.32088933\n",
      "Iteration 2181, loss = 0.16127745\n",
      "Iteration 1118, loss = 0.27191243\n",
      "Iteration 2167, loss = 0.18437260\n",
      "Iteration 55, loss = 0.50478664\n",
      "Iteration 1038, loss = 0.25898532\n",
      "Iteration 56, loss = 0.50224221\n",
      "Iteration 291, loss = 0.32071738\n",
      "Iteration 1106, loss = 0.29779278\n",
      "Iteration 2182, loss = 0.16109146\n",
      "Iteration 57, loss = 0.49971347\n",
      "Iteration 2183, loss = 0.16109013\n",
      "Iteration 58, loss = 0.49721513\n",
      "Iteration 1039, loss = 0.25862796\n",
      "Iteration 253, loss = 0.42844970\n",
      "Iteration 2184, loss = 0.16081878\n",
      "Iteration 1040, loss = 0.25856659\n",
      "Iteration 59, loss = 0.49477568\n",
      "Iteration 292, loss = 0.32058637\n",
      "Iteration 60, loss = 0.49242473\n",
      "Iteration 1107, loss = 0.29765693\n",
      "Iteration 1119, loss = 0.27173792\n",
      "Iteration 2168, loss = 0.18432717\n",
      "Iteration 61, loss = 0.48999848\n",
      "Iteration 1041, loss = 0.25821317\n",
      "Iteration 2185, loss = 0.16079591\n",
      "Iteration 62, loss = 0.48770296\n",
      "Iteration 293, loss = 0.32042498\n",
      "Iteration 254, loss = 0.42828707\n",
      "Iteration 63, loss = 0.48531328\n",
      "Iteration 294, loss = 0.32028356\n",
      "Iteration 64, loss = 0.48314076\n",
      "Iteration 2186, loss = 0.16075769\n",
      "Iteration 2169, loss = 0.18418485\n",
      "Iteration 1108, loss = 0.29756211\n",
      "Iteration 65, loss = 0.48091070\n",
      "Iteration 1120, loss = 0.27159183\n",
      "Iteration 2187, loss = 0.16057001\n",
      "Iteration 1042, loss = 0.25801333Iteration 295, loss = 0.32012462\n",
      "\n",
      "Iteration 66, loss = 0.47870834\n",
      "Iteration 2188, loss = 0.16051446\n",
      "Iteration 67, loss = 0.47653647\n",
      "Iteration 296, loss = 0.32001896\n",
      "Iteration 255, loss = 0.42811864\n",
      "Iteration 2170, loss = 0.18414606\n",
      "Iteration 68, loss = 0.47435791\n",
      "Iteration 297, loss = 0.31984474\n",
      "Iteration 2189, loss = 0.16047976\n",
      "Iteration 298, loss = 0.31968325\n",
      "Iteration 69, loss = 0.47228361\n",
      "Iteration 1043, loss = 0.25775105\n",
      "Iteration 299, loss = 0.31954374\n",
      "Iteration 2190, loss = 0.16029425\n",
      "Iteration 300, loss = 0.31941190\n",
      "Iteration 1109, loss = 0.29744327\n",
      "Iteration 301, loss = 0.31926601\n",
      "Iteration 2191, loss = 0.16030541\n",
      "Iteration 302, loss = 0.31913455\n",
      "Iteration 1044, loss = 0.25754561\n",
      "Iteration 303, loss = 0.31897974\n",
      "Iteration 2171, loss = 0.18402100\n",
      "Iteration 304, loss = 0.31884325\n",
      "Iteration 1110, loss = 0.29731319\n",
      "Iteration 305, loss = 0.31870690\n",
      "Iteration 306, loss = 0.31856372\n",
      "Iteration 70, loss = 0.47019451\n",
      "Iteration 2192, loss = 0.16010718Iteration 307, loss = 0.31843420\n",
      "Iteration 256, loss = 0.42794168\n",
      "Iteration 71, loss = 0.46820798\n",
      "\n",
      "Iteration 308, loss = 0.31828313\n",
      "Iteration 72, loss = 0.46617705\n",
      "Iteration 309, loss = 0.31817133\n",
      "Iteration 310, loss = 0.31802402\n",
      "Iteration 73, loss = 0.46421958\n",
      "Iteration 1045, loss = 0.25741023Iteration 1121, loss = 0.27143179\n",
      "\n",
      "Iteration 2172, loss = 0.18401388\n",
      "Iteration 311, loss = 0.31790750\n",
      "Iteration 74, loss = 0.46228601\n",
      "Iteration 312, loss = 0.31776856\n",
      "Iteration 75, loss = 0.46040122\n",
      "Iteration 1111, loss = 0.29720017\n",
      "Iteration 313, loss = 0.31762420\n",
      "Iteration 76, loss = 0.45852898\n",
      "Iteration 314, loss = 0.31748586\n",
      "Iteration 77, loss = 0.45668241\n",
      "Iteration 1046, loss = 0.25707482\n",
      "Iteration 2173, loss = 0.18387226\n",
      "Iteration 315, loss = 0.31734516\n",
      "Iteration 257, loss = 0.42760245\n",
      "Iteration 78, loss = 0.45482338\n",
      "Iteration 316, loss = 0.31722742\n",
      "Iteration 79, loss = 0.45305985\n",
      "Iteration 317, loss = 0.31709471\n",
      "Iteration 318, loss = 0.31697990\n",
      "Iteration 80, loss = 0.45133957\n",
      "Iteration 319, loss = 0.31684746\n",
      "Iteration 2174, loss = 0.18395495\n",
      "Iteration 81, loss = 0.44960612\n",
      "Iteration 320, loss = 0.31673904\n",
      "Iteration 1112, loss = 0.29710926\n",
      "Iteration 1047, loss = 0.25689447\n",
      "Iteration 1122, loss = 0.27119402\n",
      "Iteration 321, loss = 0.31659391\n",
      "Iteration 82, loss = 0.44792771\n",
      "Iteration 322, loss = 0.31646153\n",
      "Iteration 1048, loss = 0.25680599\n",
      "Iteration 83, loss = 0.44628157\n",
      "Iteration 323, loss = 0.31632861\n",
      "Iteration 2175, loss = 0.18375630\n",
      "Iteration 324, loss = 0.31619829\n",
      "Iteration 84, loss = 0.44465583\n",
      "Iteration 325, loss = 0.31609802\n",
      "Iteration 326, loss = 0.31594729\n",
      "Iteration 85, loss = 0.44305327\n",
      "Iteration 1049, loss = 0.25644485\n",
      "Iteration 258, loss = 0.42733107\n",
      "Iteration 327, loss = 0.31583608\n",
      "Iteration 86, loss = 0.44141667\n",
      "Iteration 328, loss = 0.31569428\n",
      "Iteration 2176, loss = 0.18358649\n",
      "Iteration 87, loss = 0.43990676\n",
      "Iteration 329, loss = 0.31556760\n",
      "Iteration 1113, loss = 0.29698967\n",
      "Iteration 88, loss = 0.43838465\n",
      "Iteration 1123, loss = 0.27109021\n",
      "Iteration 330, loss = 0.31546225\n",
      "Iteration 2193, loss = 0.16003559\n",
      "Iteration 89, loss = 0.43685579\n",
      "Iteration 331, loss = 0.31533816\n",
      "Iteration 90, loss = 0.43540414\n",
      "Iteration 1050, loss = 0.25635822\n",
      "Iteration 2177, loss = 0.18348604\n",
      "Iteration 2194, loss = 0.16005758\n",
      "Iteration 1051, loss = 0.25604110\n",
      "Iteration 332, loss = 0.31520935\n",
      "Iteration 1124, loss = 0.27095899\n",
      "Iteration 259, loss = 0.42714052\n",
      "Iteration 1114, loss = 0.29685510\n",
      "Iteration 333, loss = 0.31509161\n",
      "Iteration 91, loss = 0.43399687\n",
      "Iteration 334, loss = 0.31496940\n",
      "Iteration 2195, loss = 0.15988412\n",
      "Iteration 335, loss = 0.31485393\n",
      "Iteration 2178, loss = 0.18352477\n",
      "Iteration 336, loss = 0.31473471\n",
      "Iteration 2196, loss = 0.15972551\n",
      "Iteration 337, loss = 0.31461647\n",
      "Iteration 92, loss = 0.43254502\n",
      "Iteration 338, loss = 0.31450421\n",
      "Iteration 1052, loss = 0.25580416\n",
      "Iteration 260, loss = 0.42692016\n",
      "Iteration 339, loss = 0.31439949\n",
      "Iteration 93, loss = 0.43110685\n",
      "Iteration 1053, loss = 0.25556756\n",
      "Iteration 94, loss = 0.42978795\n",
      "Iteration 95, loss = 0.42848195\n",
      "Iteration 1125, loss = 0.27068709\n",
      "Iteration 2197, loss = 0.15966335\n",
      "Iteration 340, loss = 0.31426493\n",
      "Iteration 341, loss = 0.31414478\n",
      "Iteration 96, loss = 0.42711821\n",
      "Iteration 1054, loss = 0.25539172\n",
      "Iteration 342, loss = 0.31403568\n",
      "Iteration 2179, loss = 0.18341013\n",
      "Iteration 2198, loss = 0.15954434\n",
      "Iteration 1115, loss = 0.29671977\n",
      "Iteration 261, loss = 0.42668509\n",
      "Iteration 343, loss = 0.31392363\n",
      "Iteration 2199, loss = 0.15959227\n",
      "Iteration 97, loss = 0.42581703\n",
      "Iteration 2200, loss = 0.15927642\n",
      "Iteration 344, loss = 0.31379662\n",
      "Iteration 1116, loss = 0.29662317\n",
      "Iteration 345, loss = 0.31369550\n",
      "Iteration 2201, loss = 0.15919992\n",
      "Iteration 1055, loss = 0.25524357\n",
      "Iteration 98, loss = 0.42450242\n",
      "Iteration 346, loss = 0.31358695\n",
      "Iteration 347, loss = 0.31346186\n",
      "Iteration 262, loss = 0.42646466\n",
      "Iteration 2180, loss = 0.18323710\n",
      "Iteration 99, loss = 0.42327661\n",
      "Iteration 100, loss = 0.42202579\n",
      "Iteration 2202, loss = 0.15913074\n",
      "Iteration 101, loss = 0.42081122\n",
      "Iteration 2181, loss = 0.18318954\n",
      "Iteration 102, loss = 0.41959302\n",
      "Iteration 1117, loss = 0.29650818\n",
      "Iteration 1056, loss = 0.25511323\n",
      "Iteration 103, loss = 0.41843682\n",
      "Iteration 2203, loss = 0.15911446\n",
      "Iteration 1126, loss = 0.27047605\n",
      "Iteration 263, loss = 0.42626359\n",
      "Iteration 348, loss = 0.31337842\n",
      "Iteration 1057, loss = 0.25491608\n",
      "Iteration 349, loss = 0.31324415\n",
      "Iteration 2204, loss = 0.15896748\n",
      "Iteration 104, loss = 0.41723847\n",
      "Iteration 350, loss = 0.31314400\n",
      "Iteration 1118, loss = 0.29640322\n",
      "Iteration 105, loss = 0.41613727\n",
      "Iteration 351, loss = 0.31301853\n",
      "Iteration 106, loss = 0.41495024\n",
      "Iteration 107, loss = 0.41386315\n",
      "Iteration 352, loss = 0.31291510\n",
      "Iteration 264, loss = 0.42618361\n",
      "Iteration 2205, loss = 0.15887234\n",
      "Iteration 1127, loss = 0.27033329\n",
      "Iteration 353, loss = 0.31280600\n",
      "Iteration 2182, loss = 0.18304925\n",
      "Iteration 108, loss = 0.41280739\n",
      "Iteration 354, loss = 0.31270269\n",
      "Iteration 1119, loss = 0.29627612\n",
      "Iteration 1058, loss = 0.25447075\n",
      "Iteration 355, loss = 0.31258369\n",
      "Iteration 109, loss = 0.41168402\n",
      "Iteration 356, loss = 0.31248873\n",
      "Iteration 357, loss = 0.31237697\n",
      "Iteration 1059, loss = 0.25429923\n",
      "Iteration 2206, loss = 0.15869996\n",
      "Iteration 2183, loss = 0.18299943\n",
      "Iteration 265, loss = 0.42581888\n",
      "Iteration 2207, loss = 0.15863973\n",
      "Iteration 1120, loss = 0.29615769\n",
      "Iteration 358, loss = 0.31227267\n",
      "Iteration 2208, loss = 0.15858142\n",
      "Iteration 1060, loss = 0.25411930\n",
      "Iteration 359, loss = 0.31216963\n",
      "Iteration 2209, loss = 0.15842903\n",
      "Iteration 2184, loss = 0.18293676\n",
      "Iteration 360, loss = 0.31206263\n",
      "Iteration 110, loss = 0.41066994\n",
      "Iteration 2210, loss = 0.15842241\n",
      "Iteration 1061, loss = 0.25386580\n",
      "Iteration 361, loss = 0.31195700\n",
      "Iteration 111, loss = 0.40963827\n",
      "Iteration 362, loss = 0.31185120\n",
      "Iteration 363, loss = 0.31175090\n",
      "Iteration 2185, loss = 0.18284817\n",
      "Iteration 364, loss = 0.31165816\n",
      "Iteration 1062, loss = 0.25380055\n",
      "Iteration 2211, loss = 0.15826647\n",
      "Iteration 365, loss = 0.31153338\n",
      "Iteration 2212, loss = 0.15824538\n",
      "Iteration 1128, loss = 0.27020405\n",
      "Iteration 266, loss = 0.42566574\n",
      "Iteration 366, loss = 0.31143524\n",
      "Iteration 112, loss = 0.40859340\n",
      "Iteration 2213, loss = 0.15808675\n",
      "Iteration 1063, loss = 0.25340811\n",
      "Iteration 367, loss = 0.31133247\n",
      "Iteration 113, loss = 0.40757389\n",
      "Iteration 2186, loss = 0.18276553\n",
      "Iteration 114, loss = 0.40660557\n",
      "Iteration 368, loss = 0.31124533\n",
      "Iteration 1121, loss = 0.29608069\n",
      "Iteration 2214, loss = 0.15798782\n",
      "Iteration 115, loss = 0.40565021\n",
      "Iteration 369, loss = 0.31112870\n",
      "Iteration 116, loss = 0.40463444\n",
      "Iteration 370, loss = 0.31101449\n",
      "Iteration 117, loss = 0.40374954\n",
      "Iteration 371, loss = 0.31093656\n",
      "Iteration 1129, loss = 0.26993750\n",
      "Iteration 1064, loss = 0.25339690\n",
      "Iteration 118, loss = 0.40275722\n",
      "Iteration 2215, loss = 0.15791582\n",
      "Iteration 119, loss = 0.40184957\n",
      "Iteration 120, loss = 0.40093135\n",
      "Iteration 2216, loss = 0.15796072\n",
      "Iteration 1065, loss = 0.25315505\n",
      "Iteration 121, loss = 0.40007410\n",
      "Iteration 267, loss = 0.42541086\n",
      "Iteration 122, loss = 0.39918071\n",
      "Iteration 2217, loss = 0.15774531\n",
      "Iteration 123, loss = 0.39828792\n",
      "Iteration 1130, loss = 0.26979583\n",
      "Iteration 2187, loss = 0.18282884\n",
      "Iteration 124, loss = 0.39745044\n",
      "Iteration 1066, loss = 0.25276249\n",
      "Iteration 372, loss = 0.31082156\n",
      "Iteration 125, loss = 0.39658873\n",
      "Iteration 1122, loss = 0.29592568\n",
      "Iteration 373, loss = 0.31072083\n",
      "Iteration 126, loss = 0.39577257\n",
      "Iteration 374, loss = 0.31062428\n",
      "Iteration 2188, loss = 0.18251893\n",
      "Iteration 1131, loss = 0.26968957\n",
      "Iteration 127, loss = 0.39491715\n",
      "Iteration 2218, loss = 0.15766237\n",
      "Iteration 268, loss = 0.42519132\n",
      "Iteration 375, loss = 0.31050675\n",
      "Iteration 128, loss = 0.39415834\n",
      "Iteration 376, loss = 0.31041818\n",
      "Iteration 2219, loss = 0.15756582\n",
      "Iteration 129, loss = 0.39333381\n",
      "Iteration 377, loss = 0.31032247\n",
      "Iteration 1123, loss = 0.29588446\n",
      "Iteration 1067, loss = 0.25266635\n",
      "Iteration 130, loss = 0.39254113\n",
      "Iteration 378, loss = 0.31022397\n",
      "Iteration 2220, loss = 0.15742630\n",
      "Iteration 2189, loss = 0.18241788\n",
      "Iteration 379, loss = 0.31012658\n",
      "Iteration 1068, loss = 0.25253046\n",
      "Iteration 1124, loss = 0.29568948\n",
      "Iteration 380, loss = 0.31002749\n",
      "Iteration 131, loss = 0.39184687\n",
      "Iteration 381, loss = 0.30991782\n",
      "Iteration 382, loss = 0.30983431\n",
      "Iteration 1069, loss = 0.25208834\n",
      "Iteration 383, loss = 0.30974165\n",
      "Iteration 2221, loss = 0.15748465\n",
      "Iteration 384, loss = 0.30963103\n",
      "Iteration 269, loss = 0.42502790\n",
      "Iteration 1132, loss = 0.26942493\n",
      "Iteration 1070, loss = 0.25187551\n",
      "Iteration 385, loss = 0.30954418\n",
      "Iteration 2222, loss = 0.15723857\n",
      "Iteration 386, loss = 0.30945229\n",
      "Iteration 1125, loss = 0.29555077\n",
      "Iteration 387, loss = 0.30935202\n",
      "Iteration 132, loss = 0.39099993\n",
      "Iteration 2190, loss = 0.18237643\n",
      "Iteration 388, loss = 0.30925315\n",
      "Iteration 389, loss = 0.30917142\n",
      "Iteration 390, loss = 0.30907350\n",
      "Iteration 1071, loss = 0.25164689\n",
      "Iteration 2223, loss = 0.15716914\n",
      "Iteration 391, loss = 0.30897481\n",
      "Iteration 133, loss = 0.39025222\n",
      "Iteration 392, loss = 0.30888540\n",
      "Iteration 1133, loss = 0.26921728\n",
      "Iteration 2191, loss = 0.18234817\n",
      "Iteration 393, loss = 0.30878453\n",
      "Iteration 134, loss = 0.38949311\n",
      "Iteration 394, loss = 0.30870319\n",
      "Iteration 135, loss = 0.38877719\n",
      "Iteration 2224, loss = 0.15700621\n",
      "Iteration 1072, loss = 0.25160959\n",
      "Iteration 395, loss = 0.30861541\n",
      "Iteration 2225, loss = 0.15690899\n",
      "Iteration 1134, loss = 0.26904934\n",
      "Iteration 136, loss = 0.38799897\n",
      "Iteration 1073, loss = 0.25124415\n",
      "Iteration 396, loss = 0.30853074\n",
      "Iteration 2226, loss = 0.15681724\n",
      "Iteration 270, loss = 0.42481060\n",
      "Iteration 2192, loss = 0.18224422\n",
      "Iteration 397, loss = 0.30842576\n",
      "Iteration 1126, loss = 0.29545642\n",
      "Iteration 398, loss = 0.30832767\n",
      "Iteration 137, loss = 0.38730789\n",
      "Iteration 1074, loss = 0.25096682\n",
      "Iteration 399, loss = 0.30824036\n",
      "Iteration 138, loss = 0.38662876\n",
      "Iteration 400, loss = 0.30816251\n",
      "Iteration 401, loss = 0.30805037\n",
      "Iteration 139, loss = 0.38590717\n",
      "Iteration 402, loss = 0.30796027\n",
      "Iteration 2193, loss = 0.18216515\n",
      "Iteration 1135, loss = 0.26902395\n",
      "Iteration 403, loss = 0.30788099\n",
      "Iteration 404, loss = 0.30781463\n",
      "Iteration 2227, loss = 0.15670793\n",
      "Iteration 271, loss = 0.42459430\n",
      "Iteration 1127, loss = 0.29535214\n",
      "Iteration 405, loss = 0.30770089\n",
      "Iteration 1075, loss = 0.25099547\n",
      "Iteration 406, loss = 0.30761708\n",
      "Iteration 2194, loss = 0.18201572\n",
      "Iteration 2228, loss = 0.15664930\n",
      "Iteration 1136, loss = 0.26879337\n",
      "Iteration 407, loss = 0.30752432\n",
      "Iteration 1076, loss = 0.25057116\n",
      "Iteration 140, loss = 0.38522497\n",
      "Iteration 408, loss = 0.30743780\n",
      "Iteration 409, loss = 0.30734499\n",
      "Iteration 1128, loss = 0.29519667\n",
      "Iteration 141, loss = 0.38454379\n",
      "Iteration 410, loss = 0.30725141\n",
      "Iteration 2229, loss = 0.15651525\n",
      "Iteration 411, loss = 0.30716498\n",
      "Iteration 272, loss = 0.42443487\n",
      "Iteration 412, loss = 0.30709788\n",
      "Iteration 1077, loss = 0.25032385\n",
      "Iteration 1137, loss = 0.26855449\n",
      "Iteration 413, loss = 0.30698936\n",
      "Iteration 2195, loss = 0.18202942\n",
      "Iteration 1129, loss = 0.29507258\n",
      "Iteration 414, loss = 0.30690145\n",
      "Iteration 142, loss = 0.38382335\n",
      "Iteration 2230, loss = 0.15641848\n",
      "Iteration 143, loss = 0.38325082\n",
      "Iteration 415, loss = 0.30681390\n",
      "Iteration 144, loss = 0.38251148\n",
      "Iteration 416, loss = 0.30672599\n",
      "Iteration 2231, loss = 0.15644653\n",
      "Iteration 417, loss = 0.30667762\n",
      "Iteration 145, loss = 0.38188148\n",
      "Iteration 418, loss = 0.30655228\n",
      "Iteration 1138, loss = 0.26831563\n",
      "Iteration 273, loss = 0.42424728\n",
      "Iteration 1078, loss = 0.25009166\n",
      "Iteration 2196, loss = 0.18184171\n",
      "Iteration 419, loss = 0.30646868\n",
      "Iteration 2232, loss = 0.15627334\n",
      "Iteration 146, loss = 0.38124550\n",
      "Iteration 1079, loss = 0.24985976\n",
      "Iteration 420, loss = 0.30636755\n",
      "Iteration 421, loss = 0.30628766\n",
      "Iteration 147, loss = 0.38063641\n",
      "Iteration 1130, loss = 0.29500293\n",
      "Iteration 1080, loss = 0.24983052\n",
      "Iteration 2233, loss = 0.15621134\n",
      "Iteration 422, loss = 0.30620174\n",
      "Iteration 1139, loss = 0.26835403\n",
      "Iteration 274, loss = 0.42402417\n",
      "Iteration 1081, loss = 0.24946910\n",
      "Iteration 148, loss = 0.37997223\n",
      "Iteration 423, loss = 0.30612470\n",
      "Iteration 1131, loss = 0.29494842\n",
      "Iteration 149, loss = 0.37934505\n",
      "Iteration 2197, loss = 0.18172951\n",
      "Iteration 424, loss = 0.30604257\n",
      "Iteration 2234, loss = 0.15604657\n",
      "Iteration 150, loss = 0.37876401\n",
      "Iteration 1082, loss = 0.24923288\n",
      "Iteration 151, loss = 0.37819036\n",
      "Iteration 425, loss = 0.30595980\n",
      "Iteration 152, loss = 0.37755161\n",
      "Iteration 275, loss = 0.42384270\n",
      "Iteration 153, loss = 0.37697464\n",
      "Iteration 426, loss = 0.30585243\n",
      "Iteration 1083, loss = 0.24916909\n",
      "Iteration 2235, loss = 0.15595743\n",
      "Iteration 154, loss = 0.37636077\n",
      "Iteration 427, loss = 0.30578142\n",
      "Iteration 1140, loss = 0.26809194\n",
      "Iteration 428, loss = 0.30569645\n",
      "Iteration 429, loss = 0.30561206\n",
      "Iteration 430, loss = 0.30551800\n",
      "Iteration 1132, loss = 0.29484573\n",
      "Iteration 2198, loss = 0.18163427\n",
      "Iteration 431, loss = 0.30545950\n",
      "Iteration 1084, loss = 0.24888588\n",
      "Iteration 2236, loss = 0.15589901\n",
      "Iteration 155, loss = 0.37584139\n",
      "Iteration 432, loss = 0.30536263\n",
      "Iteration 433, loss = 0.30527356\n",
      "Iteration 434, loss = 0.30519490\n",
      "Iteration 435, loss = 0.30511025\n",
      "Iteration 156, loss = 0.37525406\n",
      "Iteration 436, loss = 0.30502075\n",
      "Iteration 2237, loss = 0.15577082\n",
      "Iteration 2199, loss = 0.18158042\n",
      "Iteration 157, loss = 0.37467398\n",
      "Iteration 437, loss = 0.30492790\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 276, loss = 0.42361485\n",
      "Iteration 158, loss = 0.37412759\n",
      "Iteration 1141, loss = 0.26784460\n",
      "Iteration 1133, loss = 0.29464296\n",
      "Iteration 2238, loss = 0.15564197\n",
      "Iteration 1085, loss = 0.24855713\n",
      "Iteration 159, loss = 0.37357444\n",
      "Iteration 160, loss = 0.37300804\n",
      "Iteration 2239, loss = 0.15566857\n",
      "Iteration 161, loss = 0.37248426\n",
      "Iteration 162, loss = 0.37194632\n",
      "Iteration 1086, loss = 0.24848707\n",
      "Iteration 1134, loss = 0.29451684\n",
      "Iteration 163, loss = 0.37144022\n",
      "Iteration 2200, loss = 0.18152951\n",
      "Iteration 164, loss = 0.37090614\n",
      "Iteration 1142, loss = 0.26769245\n",
      "Iteration 165, loss = 0.37041259\n",
      "Iteration 1087, loss = 0.24817650\n",
      "Iteration 166, loss = 0.36991060\n",
      "Iteration 1135, loss = 0.29438026\n",
      "Iteration 1, loss = 0.74593883\n",
      "Iteration 167, loss = 0.36936131\n",
      "Iteration 2240, loss = 0.15546866\n",
      "Iteration 168, loss = 0.36887773\n",
      "Iteration 277, loss = 0.42343271\n",
      "Iteration 169, loss = 0.36838615\n",
      "Iteration 2241, loss = 0.15539877\n",
      "Iteration 2, loss = 0.74233372\n",
      "Iteration 1088, loss = 0.24785966\n",
      "Iteration 3, loss = 0.73699611\n",
      "Iteration 170, loss = 0.36790125\n",
      "Iteration 2201, loss = 0.18142060\n",
      "Iteration 171, loss = 0.36741364\n",
      "Iteration 1089, loss = 0.24777707\n",
      "Iteration 2242, loss = 0.15532577\n",
      "Iteration 4, loss = 0.73049863\n",
      "Iteration 1090, loss = 0.24760297\n",
      "Iteration 172, loss = 0.36692393\n",
      "Iteration 2243, loss = 0.15535268\n",
      "Iteration 1136, loss = 0.29429625\n",
      "Iteration 5, loss = 0.72326716\n",
      "Iteration 278, loss = 0.42321360\n",
      "Iteration 2202, loss = 0.18133555\n",
      "Iteration 1143, loss = 0.26744815\n",
      "Iteration 173, loss = 0.36644828\n",
      "Iteration 2244, loss = 0.15510253\n",
      "Iteration 6, loss = 0.71548967\n",
      "Iteration 1091, loss = 0.24740416\n",
      "Iteration 2245, loss = 0.15500579\n",
      "Iteration 2246, loss = 0.15496824\n",
      "Iteration 1137, loss = 0.29413144\n",
      "Iteration 1144, loss = 0.26729036\n",
      "Iteration 7, loss = 0.70754242\n",
      "Iteration 174, loss = 0.36598143\n",
      "Iteration 279, loss = 0.42302584\n",
      "Iteration 2247, loss = 0.15483925\n",
      "Iteration 8, loss = 0.69950272\n",
      "Iteration 175, loss = 0.36554756\n",
      "Iteration 2203, loss = 0.18123008\n",
      "Iteration 2248, loss = 0.15474117\n",
      "Iteration 176, loss = 0.36504118\n",
      "Iteration 1092, loss = 0.24696999\n",
      "Iteration 9, loss = 0.69165900\n",
      "Iteration 177, loss = 0.36460134\n",
      "Iteration 2249, loss = 0.15465760\n",
      "Iteration 280, loss = 0.42279092\n",
      "Iteration 178, loss = 0.36413535\n",
      "Iteration 2250, loss = 0.15453050\n",
      "Iteration 179, loss = 0.36371204\n",
      "Iteration 10, loss = 0.68401680\n",
      "Iteration 180, loss = 0.36325804\n",
      "Iteration 1093, loss = 0.24678166\n",
      "Iteration 1138, loss = 0.29407677\n",
      "Iteration 2251, loss = 0.15441690\n",
      "Iteration 2204, loss = 0.18117570\n",
      "Iteration 181, loss = 0.36281702\n",
      "Iteration 11, loss = 0.67652722\n",
      "Iteration 2252, loss = 0.15432518\n",
      "Iteration 1139, loss = 0.29401294\n",
      "Iteration 182, loss = 0.36239547\n",
      "Iteration 12, loss = 0.66914051\n",
      "Iteration 1145, loss = 0.26714240\n",
      "Iteration 2253, loss = 0.15424416\n",
      "Iteration 281, loss = 0.42263954\n",
      "Iteration 1094, loss = 0.24660921\n",
      "Iteration 13, loss = 0.66212577\n",
      "Iteration 2205, loss = 0.18111905\n",
      "Iteration 183, loss = 0.36194645\n",
      "Iteration 184, loss = 0.36151033\n",
      "Iteration 14, loss = 0.65527903\n",
      "Iteration 2254, loss = 0.15432457\n",
      "Iteration 1095, loss = 0.24639734\n",
      "Iteration 185, loss = 0.36110942\n",
      "Iteration 1140, loss = 0.29382902\n",
      "Iteration 186, loss = 0.36071226\n",
      "Iteration 2255, loss = 0.15405150\n",
      "Iteration 187, loss = 0.36028204\n",
      "Iteration 1096, loss = 0.24615038\n",
      "Iteration 2206, loss = 0.18103336\n",
      "Iteration 15, loss = 0.64849253\n",
      "Iteration 282, loss = 0.42242280\n",
      "Iteration 1097, loss = 0.24591297\n",
      "Iteration 1141, loss = 0.29368768\n",
      "Iteration 16, loss = 0.64210892\n",
      "Iteration 188, loss = 0.35985386\n",
      "Iteration 2256, loss = 0.15393743\n",
      "Iteration 189, loss = 0.35943139\n",
      "Iteration 190, loss = 0.35905436\n",
      "Iteration 1098, loss = 0.24573125\n",
      "Iteration 2257, loss = 0.15387315\n",
      "Iteration 17, loss = 0.63583089\n",
      "Iteration 2207, loss = 0.18088219\n",
      "Iteration 1146, loss = 0.26690139\n",
      "Iteration 191, loss = 0.35866292\n",
      "Iteration 2258, loss = 0.15382904\n",
      "Iteration 192, loss = 0.35824996\n",
      "Iteration 2259, loss = 0.15366213\n",
      "Iteration 193, loss = 0.35784262\n",
      "Iteration 18, loss = 0.62976608\n",
      "Iteration 1142, loss = 0.29355558\n",
      "Iteration 2260, loss = 0.15359604\n",
      "Iteration 194, loss = 0.35746491\n",
      "Iteration 283, loss = 0.42224593\n",
      "Iteration 1099, loss = 0.24545155\n",
      "Iteration 2261, loss = 0.15346797\n",
      "Iteration 195, loss = 0.35705953\n",
      "Iteration 19, loss = 0.62383299\n",
      "Iteration 2208, loss = 0.18082932\n",
      "Iteration 196, loss = 0.35666937\n",
      "Iteration 1100, loss = 0.24523906\n",
      "Iteration 1147, loss = 0.26669153\n",
      "Iteration 2262, loss = 0.15338540\n",
      "Iteration 1143, loss = 0.29343459\n",
      "Iteration 197, loss = 0.35633248Iteration 2263, loss = 0.15340478\n",
      "Iteration 1101, loss = 0.24497226\n",
      "\n",
      "Iteration 20, loss = 0.61801157\n",
      "Iteration 2264, loss = 0.15317893\n",
      "Iteration 1102, loss = 0.24477089\n",
      "Iteration 1148, loss = 0.26653352\n",
      "Iteration 2209, loss = 0.18080755\n",
      "Iteration 1144, loss = 0.29331291\n",
      "Iteration 198, loss = 0.35595461\n",
      "Iteration 21, loss = 0.61245718\n",
      "Iteration 199, loss = 0.35559566\n",
      "Iteration 1103, loss = 0.24452818\n",
      "Iteration 200, loss = 0.35520987\n",
      "Iteration 2265, loss = 0.15311871\n",
      "Iteration 1145, loss = 0.29319483\n",
      "Iteration 22, loss = 0.60694603\n",
      "Iteration 284, loss = 0.42212089\n",
      "Iteration 1104, loss = 0.24432392\n",
      "Iteration 201, loss = 0.35484348\n",
      "Iteration 23, loss = 0.60152314\n",
      "Iteration 2210, loss = 0.18060380\n",
      "Iteration 1105, loss = 0.24415711\n",
      "Iteration 1149, loss = 0.26635680\n",
      "Iteration 2266, loss = 0.15302272\n",
      "Iteration 202, loss = 0.35446569\n",
      "Iteration 24, loss = 0.59640421\n",
      "Iteration 25, loss = 0.59134340\n",
      "Iteration 1106, loss = 0.24383575\n",
      "Iteration 2267, loss = 0.15293497\n",
      "Iteration 2268, loss = 0.15285920\n",
      "Iteration 203, loss = 0.35410261\n",
      "Iteration 1146, loss = 0.29307303\n",
      "Iteration 285, loss = 0.42188045\n",
      "Iteration 1107, loss = 0.24367353\n",
      "Iteration 1150, loss = 0.26615147\n",
      "Iteration 26, loss = 0.58617484\n",
      "Iteration 2211, loss = 0.18052950\n",
      "Iteration 2269, loss = 0.15282330\n",
      "Iteration 204, loss = 0.35374947\n",
      "Iteration 27, loss = 0.58138139\n",
      "Iteration 205, loss = 0.35337418\n",
      "Iteration 1108, loss = 0.24346152\n",
      "Iteration 2270, loss = 0.15267630\n",
      "Iteration 28, loss = 0.57646531\n",
      "Iteration 206, loss = 0.35306356\n",
      "Iteration 29, loss = 0.57185346\n",
      "Iteration 1109, loss = 0.24317593\n",
      "Iteration 2271, loss = 0.15257299\n",
      "Iteration 30, loss = 0.56717052\n",
      "Iteration 1147, loss = 0.29299486\n",
      "Iteration 207, loss = 0.35269035\n",
      "Iteration 2272, loss = 0.15252802\n",
      "Iteration 1110, loss = 0.24297406\n",
      "Iteration 2212, loss = 0.18045814\n",
      "Iteration 31, loss = 0.56262210\n",
      "Iteration 208, loss = 0.35234443\n",
      "Iteration 286, loss = 0.42165260\n",
      "Iteration 1151, loss = 0.26596719\n",
      "Iteration 2273, loss = 0.15235919\n",
      "Iteration 1111, loss = 0.24278697\n",
      "Iteration 209, loss = 0.35201755\n",
      "Iteration 1148, loss = 0.29284859\n",
      "Iteration 32, loss = 0.55825946\n",
      "Iteration 33, loss = 0.55384459\n",
      "Iteration 1112, loss = 0.24254574\n",
      "Iteration 210, loss = 0.35170865\n",
      "Iteration 34, loss = 0.54958131\n",
      "Iteration 2274, loss = 0.15230554\n",
      "Iteration 211, loss = 0.35137050\n",
      "Iteration 2213, loss = 0.18037875\n",
      "Iteration 1149, loss = 0.29274054\n",
      "Iteration 212, loss = 0.35099465\n",
      "Iteration 2275, loss = 0.15214563\n",
      "Iteration 35, loss = 0.54538317\n",
      "Iteration 213, loss = 0.35068269\n",
      "Iteration 1113, loss = 0.24229954\n",
      "Iteration 36, loss = 0.54130294\n",
      "Iteration 2276, loss = 0.15229274\n",
      "Iteration 214, loss = 0.35035053\n",
      "Iteration 2214, loss = 0.18027903\n",
      "Iteration 1150, loss = 0.29263415\n",
      "Iteration 287, loss = 0.42149266\n",
      "Iteration 215, loss = 0.35004749\n",
      "Iteration 1152, loss = 0.26583098\n",
      "Iteration 216, loss = 0.34971832\n",
      "Iteration 217, loss = 0.34940975\n",
      "Iteration 1114, loss = 0.24207526\n",
      "Iteration 37, loss = 0.53724315\n",
      "Iteration 2277, loss = 0.15209681\n",
      "Iteration 38, loss = 0.53332630\n",
      "Iteration 218, loss = 0.34908922\n",
      "Iteration 39, loss = 0.52940476\n",
      "Iteration 2215, loss = 0.18019631\n",
      "Iteration 219, loss = 0.34873626\n",
      "Iteration 1153, loss = 0.26583918\n",
      "Iteration 220, loss = 0.34847650\n",
      "Iteration 221, loss = 0.34814544\n",
      "Iteration 40, loss = 0.52565681\n",
      "Iteration 222, loss = 0.34784987\n",
      "Iteration 1151, loss = 0.29253004\n",
      "Iteration 223, loss = 0.34754198\n",
      "Iteration 1115, loss = 0.24192448\n",
      "Iteration 2216, loss = 0.18010341\n",
      "Iteration 41, loss = 0.52193465\n",
      "Iteration 2278, loss = 0.15186999\n",
      "Iteration 224, loss = 0.34724296\n",
      "Iteration 1116, loss = 0.24161516\n",
      "Iteration 288, loss = 0.42131178\n",
      "Iteration 225, loss = 0.34693482\n",
      "Iteration 42, loss = 0.51818759\n",
      "Iteration 1154, loss = 0.26557501\n",
      "Iteration 226, loss = 0.34663970\n",
      "Iteration 2279, loss = 0.15180651\n",
      "Iteration 1117, loss = 0.24152145\n",
      "Iteration 43, loss = 0.51459517\n",
      "Iteration 227, loss = 0.34636874\n",
      "Iteration 1152, loss = 0.29237200Iteration 2280, loss = 0.15169724\n",
      "\n",
      "Iteration 44, loss = 0.51128195\n",
      "Iteration 2217, loss = 0.18005077\n",
      "Iteration 45, loss = 0.50770507\n",
      "Iteration 2281, loss = 0.15163176\n",
      "Iteration 289, loss = 0.42115889\n",
      "Iteration 46, loss = 0.50436947\n",
      "Iteration 1153, loss = 0.29229858\n",
      "Iteration 47, loss = 0.50104793\n",
      "Iteration 228, loss = 0.34606202\n",
      "Iteration 1118, loss = 0.24127905\n",
      "Iteration 229, loss = 0.34576875\n",
      "Iteration 2282, loss = 0.15160194\n",
      "Iteration 290, loss = 0.42091840\n",
      "Iteration 2218, loss = 0.17994023\n",
      "Iteration 48, loss = 0.49788668\n",
      "Iteration 2283, loss = 0.15150103\n",
      "Iteration 230, loss = 0.34547827\n",
      "Iteration 49, loss = 0.49470211\n",
      "Iteration 1155, loss = 0.26523895\n",
      "Iteration 1119, loss = 0.24101626\n",
      "Iteration 2284, loss = 0.15135256\n",
      "Iteration 50, loss = 0.49153445\n",
      "Iteration 1154, loss = 0.29217588\n",
      "Iteration 231, loss = 0.34521195\n",
      "Iteration 51, loss = 0.48859013\n",
      "Iteration 232, loss = 0.34493318\n",
      "Iteration 2219, loss = 0.17985829\n",
      "Iteration 233, loss = 0.34463589\n",
      "Iteration 2285, loss = 0.15122453\n",
      "Iteration 1120, loss = 0.24072056\n",
      "Iteration 234, loss = 0.34437142\n",
      "Iteration 2220, loss = 0.17979015\n",
      "Iteration 52, loss = 0.48550136\n",
      "Iteration 235, loss = 0.34409644\n",
      "Iteration 1155, loss = 0.29203874\n",
      "Iteration 291, loss = 0.42069876\n",
      "Iteration 1156, loss = 0.26538043\n",
      "Iteration 1121, loss = 0.24075497\n",
      "Iteration 236, loss = 0.34382353\n",
      "Iteration 1122, loss = 0.24054232\n",
      "Iteration 237, loss = 0.34355404\n",
      "Iteration 2221, loss = 0.17967590\n",
      "Iteration 1156, loss = 0.29189279\n",
      "Iteration 53, loss = 0.48256582\n",
      "Iteration 2286, loss = 0.15117464\n",
      "Iteration 238, loss = 0.34330558\n",
      "Iteration 54, loss = 0.47974072\n",
      "Iteration 1157, loss = 0.26489809\n",
      "Iteration 292, loss = 0.42053175\n",
      "Iteration 239, loss = 0.34301623\n",
      "Iteration 2287, loss = 0.15110127\n",
      "Iteration 55, loss = 0.47692351\n",
      "Iteration 1123, loss = 0.24005454\n",
      "Iteration 56, loss = 0.47421444\n",
      "Iteration 240, loss = 0.34277077\n",
      "Iteration 2288, loss = 0.15098121\n",
      "Iteration 1157, loss = 0.29179344\n",
      "Iteration 57, loss = 0.47153567\n",
      "Iteration 1158, loss = 0.26470793\n",
      "Iteration 2222, loss = 0.17968741\n",
      "Iteration 1124, loss = 0.23995277\n",
      "Iteration 2289, loss = 0.15089892\n",
      "Iteration 293, loss = 0.42033229\n",
      "Iteration 58, loss = 0.46894303\n",
      "Iteration 241, loss = 0.34250480\n",
      "Iteration 1158, loss = 0.29167838\n",
      "Iteration 2290, loss = 0.15085701\n",
      "Iteration 242, loss = 0.34226501\n",
      "Iteration 1125, loss = 0.23981683\n",
      "Iteration 2223, loss = 0.17956053\n",
      "Iteration 243, loss = 0.34197489\n",
      "Iteration 1126, loss = 0.23953032\n",
      "Iteration 1159, loss = 0.29157637\n",
      "Iteration 294, loss = 0.42020379\n",
      "Iteration 1127, loss = 0.23919653\n",
      "Iteration 2224, loss = 0.17942491\n",
      "Iteration 2291, loss = 0.15084574\n",
      "Iteration 244, loss = 0.34171180\n",
      "Iteration 59, loss = 0.46636974\n",
      "Iteration 2292, loss = 0.15060538\n",
      "Iteration 60, loss = 0.46386113\n",
      "Iteration 245, loss = 0.34148044\n",
      "Iteration 1159, loss = 0.26458279\n",
      "Iteration 1160, loss = 0.29141641\n",
      "Iteration 61, loss = 0.46146236\n",
      "Iteration 1128, loss = 0.23897948\n",
      "Iteration 295, loss = 0.41999671\n",
      "Iteration 2293, loss = 0.15049512\n",
      "Iteration 246, loss = 0.34122088\n",
      "Iteration 62, loss = 0.45906433\n",
      "Iteration 2225, loss = 0.17940666\n",
      "Iteration 247, loss = 0.34096943\n",
      "Iteration 63, loss = 0.45675458\n",
      "Iteration 248, loss = 0.34072205\n",
      "Iteration 1129, loss = 0.23880069\n",
      "Iteration 64, loss = 0.45446465\n",
      "Iteration 1161, loss = 0.29131711\n",
      "Iteration 249, loss = 0.34046303\n",
      "Iteration 65, loss = 0.45225137\n",
      "Iteration 1160, loss = 0.26437245\n",
      "Iteration 250, loss = 0.34024423\n",
      "Iteration 66, loss = 0.45011644\n",
      "Iteration 296, loss = 0.41980032\n",
      "Iteration 2294, loss = 0.15040684\n",
      "Iteration 67, loss = 0.44796842\n",
      "Iteration 2295, loss = 0.15034871\n",
      "Iteration 1162, loss = 0.29121964\n",
      "Iteration 1130, loss = 0.23847542\n",
      "Iteration 2296, loss = 0.15023032\n",
      "Iteration 251, loss = 0.34000351\n",
      "Iteration 2226, loss = 0.17931766\n",
      "Iteration 68, loss = 0.44583975\n",
      "Iteration 252, loss = 0.33973901\n",
      "Iteration 1161, loss = 0.26419759\n",
      "Iteration 69, loss = 0.44383652\n",
      "Iteration 1131, loss = 0.23826028\n",
      "Iteration 1163, loss = 0.29112121\n",
      "Iteration 70, loss = 0.44181085\n",
      "Iteration 253, loss = 0.33950820\n",
      "Iteration 2297, loss = 0.15018104\n",
      "Iteration 2227, loss = 0.17917450\n",
      "Iteration 1132, loss = 0.23812426\n",
      "Iteration 71, loss = 0.43998790\n",
      "Iteration 254, loss = 0.33925351\n",
      "Iteration 1133, loss = 0.23793896\n",
      "Iteration 255, loss = 0.33903300\n",
      "Iteration 72, loss = 0.43803620\n",
      "Iteration 256, loss = 0.33878787\n",
      "Iteration 257, loss = 0.33858227\n",
      "Iteration 1162, loss = 0.26399467\n",
      "Iteration 1134, loss = 0.23757305\n",
      "Iteration 258, loss = 0.33832295\n",
      "Iteration 73, loss = 0.43618632\n",
      "Iteration 2298, loss = 0.15009206\n",
      "Iteration 259, loss = 0.33808657\n",
      "Iteration 1135, loss = 0.23742950\n",
      "Iteration 260, loss = 0.33786368\n",
      "Iteration 2299, loss = 0.14996175\n",
      "Iteration 261, loss = 0.33763543\n",
      "Iteration 2228, loss = 0.17911429\n",
      "Iteration 74, loss = 0.43434195\n",
      "Iteration 1164, loss = 0.29094210\n",
      "Iteration 2300, loss = 0.14999771\n",
      "Iteration 297, loss = 0.41959463\n",
      "Iteration 262, loss = 0.33740339\n",
      "Iteration 1163, loss = 0.26380527\n",
      "Iteration 2229, loss = 0.17900408\n",
      "Iteration 1136, loss = 0.23728547\n",
      "Iteration 2301, loss = 0.14983321\n",
      "Iteration 75, loss = 0.43269884\n",
      "Iteration 263, loss = 0.33717867\n",
      "Iteration 1165, loss = 0.29083784\n",
      "Iteration 2302, loss = 0.14970165\n",
      "Iteration 298, loss = 0.41941471\n",
      "Iteration 76, loss = 0.43094871\n",
      "Iteration 2230, loss = 0.17895568\n",
      "Iteration 77, loss = 0.42919329\n",
      "Iteration 2231, loss = 0.17889351\n",
      "Iteration 2303, loss = 0.14961420\n",
      "Iteration 2232, loss = 0.17879905\n",
      "Iteration 2304, loss = 0.14951185\n",
      "Iteration 1137, loss = 0.23693497\n",
      "Iteration 78, loss = 0.42755543\n",
      "Iteration 1164, loss = 0.26359264\n",
      "Iteration 264, loss = 0.33696388Iteration 299, loss = 0.41921131\n",
      "\n",
      "Iteration 2305, loss = 0.14944273\n",
      "Iteration 2306, loss = 0.14932679\n",
      "Iteration 1138, loss = 0.23670382\n",
      "Iteration 79, loss = 0.42593456\n",
      "Iteration 1166, loss = 0.29070511\n",
      "Iteration 2307, loss = 0.14948689\n",
      "Iteration 265, loss = 0.33674892\n",
      "Iteration 2233, loss = 0.17866441\n",
      "Iteration 2308, loss = 0.14918323\n",
      "Iteration 266, loss = 0.33652291\n",
      "Iteration 1139, loss = 0.23643287\n",
      "Iteration 2309, loss = 0.14907113\n",
      "Iteration 80, loss = 0.42444215\n",
      "Iteration 267, loss = 0.33630564\n",
      "Iteration 1140, loss = 0.23623339\n",
      "Iteration 300, loss = 0.41905993\n",
      "Iteration 268, loss = 0.33608752\n",
      "Iteration 1165, loss = 0.26340544\n",
      "Iteration 2234, loss = 0.17860416\n",
      "Iteration 1167, loss = 0.29059915\n",
      "Iteration 269, loss = 0.33592144\n",
      "Iteration 81, loss = 0.42283667\n",
      "Iteration 1141, loss = 0.23603823\n",
      "Iteration 270, loss = 0.33567302\n",
      "Iteration 82, loss = 0.42129486\n",
      "Iteration 271, loss = 0.33545495\n",
      "Iteration 83, loss = 0.41983579\n",
      "Iteration 272, loss = 0.33525226\n",
      "Iteration 1166, loss = 0.26333129\n",
      "Iteration 273, loss = 0.33502573\n",
      "Iteration 84, loss = 0.41837628\n",
      "Iteration 1142, loss = 0.23583128\n",
      "Iteration 274, loss = 0.33481661\n",
      "Iteration 275, loss = 0.33461400\n",
      "Iteration 85, loss = 0.41696814\n",
      "Iteration 301, loss = 0.41887869\n",
      "Iteration 276, loss = 0.33440731\n",
      "Iteration 1168, loss = 0.29047398\n",
      "Iteration 277, loss = 0.33420194\n",
      "Iteration 1143, loss = 0.23567987\n",
      "Iteration 2310, loss = 0.14906245\n",
      "Iteration 2235, loss = 0.17867538\n",
      "Iteration 86, loss = 0.41560388\n",
      "Iteration 1169, loss = 0.29039596\n",
      "Iteration 1144, loss = 0.23552889\n",
      "Iteration 1167, loss = 0.26318069\n",
      "Iteration 302, loss = 0.41873681\n",
      "Iteration 2311, loss = 0.14900222\n",
      "Iteration 2236, loss = 0.17838901\n",
      "Iteration 87, loss = 0.41421710\n",
      "Iteration 278, loss = 0.33401145\n",
      "Iteration 2312, loss = 0.14882371\n",
      "Iteration 88, loss = 0.41292382\n",
      "Iteration 1168, loss = 0.26315136\n",
      "Iteration 1170, loss = 0.29021927\n",
      "Iteration 279, loss = 0.33380719\n",
      "Iteration 2313, loss = 0.14873268\n",
      "Iteration 89, loss = 0.41166001\n",
      "Iteration 280, loss = 0.33360358\n",
      "Iteration 2314, loss = 0.14860605\n",
      "Iteration 281, loss = 0.33342121\n",
      "Iteration 2237, loss = 0.17833133\n",
      "Iteration 1145, loss = 0.23514781\n",
      "Iteration 90, loss = 0.41032371\n",
      "Iteration 1171, loss = 0.29015168\n",
      "Iteration 282, loss = 0.33321675\n",
      "Iteration 2315, loss = 0.14849454\n",
      "Iteration 283, loss = 0.33302593\n",
      "Iteration 303, loss = 0.41853422\n",
      "Iteration 1146, loss = 0.23489222\n",
      "Iteration 284, loss = 0.33283398\n",
      "Iteration 91, loss = 0.40906391\n",
      "Iteration 2238, loss = 0.17823989\n",
      "Iteration 92, loss = 0.40787920\n",
      "Iteration 285, loss = 0.33261852\n",
      "Iteration 2316, loss = 0.14841866\n",
      "Iteration 286, loss = 0.33246269\n",
      "Iteration 93, loss = 0.40665989\n",
      "Iteration 287, loss = 0.33223559\n",
      "Iteration 1172, loss = 0.29007096\n",
      "Iteration 2239, loss = 0.17816660\n",
      "Iteration 288, loss = 0.33204404\n",
      "Iteration 1147, loss = 0.23479732\n",
      "Iteration 304, loss = 0.41829745\n",
      "Iteration 289, loss = 0.33188376\n",
      "Iteration 290, loss = 0.33169357\n",
      "Iteration 1173, loss = 0.28988269\n",
      "Iteration 1169, loss = 0.26283197\n",
      "Iteration 291, loss = 0.33148988\n",
      "Iteration 94, loss = 0.40550360\n",
      "Iteration 2240, loss = 0.17809054\n",
      "Iteration 1148, loss = 0.23456363\n",
      "Iteration 292, loss = 0.33130701\n",
      "Iteration 2317, loss = 0.14840878\n",
      "Iteration 293, loss = 0.33114918\n",
      "Iteration 294, loss = 0.33095765\n",
      "Iteration 95, loss = 0.40434460\n",
      "Iteration 2241, loss = 0.17798533\n",
      "Iteration 1149, loss = 0.23431957\n",
      "Iteration 2318, loss = 0.14823425\n",
      "Iteration 1174, loss = 0.28977108\n",
      "Iteration 295, loss = 0.33077991\n",
      "Iteration 96, loss = 0.40325962\n",
      "Iteration 2319, loss = 0.14815993\n",
      "Iteration 1170, loss = 0.26252395\n",
      "Iteration 97, loss = 0.40215017\n",
      "Iteration 1150, loss = 0.23405319\n",
      "Iteration 2242, loss = 0.17790693\n",
      "Iteration 296, loss = 0.33059359\n",
      "Iteration 2320, loss = 0.14803872\n",
      "Iteration 98, loss = 0.40103554\n",
      "Iteration 297, loss = 0.33039480\n",
      "Iteration 2321, loss = 0.14796796\n",
      "Iteration 1175, loss = 0.28966218\n",
      "Iteration 298, loss = 0.33023332\n",
      "Iteration 99, loss = 0.40002541\n",
      "Iteration 305, loss = 0.41814133\n",
      "Iteration 1171, loss = 0.26238994\n",
      "Iteration 1151, loss = 0.23382931\n",
      "Iteration 2243, loss = 0.17785017\n",
      "Iteration 100, loss = 0.39894819\n",
      "Iteration 299, loss = 0.33007197\n",
      "Iteration 101, loss = 0.39795191\n",
      "Iteration 1172, loss = 0.26222178\n",
      "Iteration 2322, loss = 0.14808407\n",
      "Iteration 300, loss = 0.32988559\n",
      "Iteration 1152, loss = 0.23349484\n",
      "Iteration 102, loss = 0.39697991\n",
      "Iteration 2244, loss = 0.17768935\n",
      "Iteration 103, loss = 0.39601596\n",
      "Iteration 306, loss = 0.41794286\n",
      "Iteration 104, loss = 0.39502652\n",
      "Iteration 2323, loss = 0.14789349\n",
      "Iteration 1176, loss = 0.28955859\n",
      "Iteration 301, loss = 0.32971893\n",
      "Iteration 2324, loss = 0.14771906\n",
      "Iteration 105, loss = 0.39411581\n",
      "Iteration 2325, loss = 0.14766149\n",
      "Iteration 302, loss = 0.32954870\n",
      "Iteration 106, loss = 0.39317659\n",
      "Iteration 1173, loss = 0.26194968\n",
      "Iteration 303, loss = 0.32938086\n",
      "Iteration 1153, loss = 0.23339167\n",
      "Iteration 304, loss = 0.32919674\n",
      "Iteration 307, loss = 0.41775331\n",
      "Iteration 305, loss = 0.32903316\n",
      "Iteration 2245, loss = 0.17767473\n",
      "Iteration 1154, loss = 0.23305523\n",
      "Iteration 1177, loss = 0.28939585\n",
      "Iteration 2326, loss = 0.14767154\n",
      "Iteration 107, loss = 0.39233286\n",
      "Iteration 2246, loss = 0.17759682\n",
      "Iteration 2327, loss = 0.14751054\n",
      "Iteration 108, loss = 0.39143095\n",
      "Iteration 109, loss = 0.39056179\n",
      "Iteration 2328, loss = 0.14746369\n",
      "Iteration 110, loss = 0.38970117\n",
      "Iteration 1178, loss = 0.28929872\n",
      "Iteration 306, loss = 0.32885976\n",
      "Iteration 111, loss = 0.38888498\n",
      "Iteration 307, loss = 0.32870949\n",
      "Iteration 2329, loss = 0.14730152\n",
      "Iteration 1155, loss = 0.23288271\n",
      "Iteration 1174, loss = 0.26185474\n",
      "Iteration 308, loss = 0.32854779\n",
      "Iteration 2247, loss = 0.17753921\n",
      "Iteration 309, loss = 0.32836912\n",
      "Iteration 310, loss = 0.32820406\n",
      "Iteration 311, loss = 0.32804359\n",
      "Iteration 1179, loss = 0.28915926\n",
      "Iteration 308, loss = 0.41757094\n",
      "Iteration 112, loss = 0.38804391\n",
      "Iteration 312, loss = 0.32788577\n",
      "Iteration 2330, loss = 0.14715907\n",
      "Iteration 313, loss = 0.32775929\n",
      "Iteration 2331, loss = 0.14709235\n",
      "Iteration 1175, loss = 0.26162504\n",
      "Iteration 309, loss = 0.41740556\n",
      "Iteration 113, loss = 0.38727304\n",
      "Iteration 314, loss = 0.32758268\n",
      "Iteration 1156, loss = 0.23266954\n",
      "Iteration 310, loss = 0.41721135\n",
      "Iteration 315, loss = 0.32741905\n",
      "Iteration 114, loss = 0.38646205\n",
      "Iteration 1180, loss = 0.28905730\n",
      "Iteration 316, loss = 0.32723947\n",
      "Iteration 2332, loss = 0.14697968\n",
      "Iteration 2248, loss = 0.17760665\n",
      "Iteration 317, loss = 0.32707783\n",
      "Iteration 1157, loss = 0.23243920\n",
      "Iteration 318, loss = 0.32693916\n",
      "Iteration 115, loss = 0.38567217\n",
      "Iteration 2249, loss = 0.17732813\n",
      "Iteration 319, loss = 0.32677678\n",
      "Iteration 1181, loss = 0.28889791\n",
      "Iteration 1158, loss = 0.23214849\n",
      "Iteration 116, loss = 0.38495900\n",
      "Iteration 320, loss = 0.32664307\n",
      "Iteration 321, loss = 0.32647067\n",
      "Iteration 322, loss = 0.32633191\n",
      "Iteration 2333, loss = 0.14689911\n",
      "Iteration 323, loss = 0.32618026\n",
      "Iteration 117, loss = 0.38417227\n",
      "Iteration 1159, loss = 0.23197691\n",
      "Iteration 2250, loss = 0.17721300\n",
      "Iteration 118, loss = 0.38345041\n",
      "Iteration 2334, loss = 0.14681586\n",
      "Iteration 1176, loss = 0.26145973\n",
      "Iteration 119, loss = 0.38271483\n",
      "Iteration 2335, loss = 0.14673230\n",
      "Iteration 120, loss = 0.38203119\n",
      "Iteration 1160, loss = 0.23172801\n",
      "Iteration 2336, loss = 0.14660556\n",
      "Iteration 324, loss = 0.32600829\n",
      "Iteration 1182, loss = 0.28881034\n",
      "Iteration 2337, loss = 0.14650839\n",
      "Iteration 325, loss = 0.32586042\n",
      "Iteration 121, loss = 0.38131653Iteration 2338, loss = 0.14646638\n",
      "\n",
      "Iteration 326, loss = 0.32573241\n",
      "Iteration 2339, loss = 0.14644955\n",
      "Iteration 311, loss = 0.41707115\n",
      "Iteration 122, loss = 0.38066270\n",
      "Iteration 1183, loss = 0.28876288\n",
      "Iteration 327, loss = 0.32556868\n",
      "Iteration 328, loss = 0.32542305\n",
      "Iteration 2340, loss = 0.14629683\n",
      "Iteration 329, loss = 0.32528302\n",
      "Iteration 123, loss = 0.37996805\n",
      "Iteration 1184, loss = 0.28862610\n",
      "Iteration 124, loss = 0.37933009\n",
      "Iteration 2251, loss = 0.17714764\n",
      "Iteration 330, loss = 0.32513487\n",
      "Iteration 2341, loss = 0.14613957\n",
      "Iteration 1177, loss = 0.26123988\n",
      "Iteration 331, loss = 0.32497033\n",
      "Iteration 1161, loss = 0.23157204\n",
      "Iteration 312, loss = 0.41684387\n",
      "Iteration 332, loss = 0.32483851\n",
      "Iteration 2252, loss = 0.17709481\n",
      "Iteration 333, loss = 0.32468892\n",
      "Iteration 334, loss = 0.32455033\n",
      "Iteration 1162, loss = 0.23123667\n",
      "Iteration 335, loss = 0.32440970\n",
      "Iteration 1178, loss = 0.26102173\n",
      "Iteration 2253, loss = 0.17704900\n",
      "Iteration 1163, loss = 0.23118520\n",
      "Iteration 336, loss = 0.32426510\n",
      "Iteration 125, loss = 0.37863998\n",
      "Iteration 1164, loss = 0.23086414\n",
      "Iteration 337, loss = 0.32411807\n",
      "Iteration 1185, loss = 0.28843996\n",
      "Iteration 2342, loss = 0.14612699\n",
      "Iteration 126, loss = 0.37802606\n",
      "Iteration 2254, loss = 0.17695069\n",
      "Iteration 1179, loss = 0.26086773\n",
      "Iteration 338, loss = 0.32397397\n",
      "Iteration 339, loss = 0.32383688\n",
      "Iteration 127, loss = 0.37739298\n",
      "Iteration 340, loss = 0.32373041\n",
      "Iteration 341, loss = 0.32356468\n",
      "Iteration 128, loss = 0.37674037\n",
      "Iteration 2255, loss = 0.17692762\n",
      "Iteration 2343, loss = 0.14598015\n",
      "Iteration 342, loss = 0.32342395\n",
      "Iteration 313, loss = 0.41667399\n",
      "Iteration 343, loss = 0.32329151\n",
      "Iteration 2344, loss = 0.14588598\n",
      "Iteration 344, loss = 0.32315895\n",
      "Iteration 345, loss = 0.32306329\n",
      "Iteration 1165, loss = 0.23061594\n",
      "Iteration 2345, loss = 0.14578821\n",
      "Iteration 2256, loss = 0.17669823\n",
      "Iteration 346, loss = 0.32288504\n",
      "Iteration 347, loss = 0.32274837\n",
      "Iteration 2346, loss = 0.14571786\n",
      "Iteration 129, loss = 0.37615871\n",
      "Iteration 1186, loss = 0.28832126\n",
      "Iteration 348, loss = 0.32261851\n",
      "Iteration 2347, loss = 0.14565099\n",
      "Iteration 1166, loss = 0.23043032\n",
      "Iteration 1187, loss = 0.28818601\n",
      "Iteration 349, loss = 0.32249236\n",
      "Iteration 130, loss = 0.37552579\n",
      "Iteration 314, loss = 0.41648198\n",
      "Iteration 2348, loss = 0.14564624\n",
      "Iteration 131, loss = 0.37497148\n",
      "Iteration 350, loss = 0.32235394\n",
      "Iteration 1167, loss = 0.23010811\n",
      "Iteration 1180, loss = 0.26072779\n",
      "Iteration 2257, loss = 0.17664856\n",
      "Iteration 2349, loss = 0.14546241\n",
      "Iteration 1188, loss = 0.28804883\n",
      "Iteration 315, loss = 0.41633370\n",
      "Iteration 2258, loss = 0.17664766\n",
      "Iteration 2350, loss = 0.14540374\n",
      "Iteration 1189, loss = 0.28795462\n",
      "Iteration 1168, loss = 0.22991043\n",
      "Iteration 2259, loss = 0.17657387\n",
      "Iteration 2351, loss = 0.14530268\n",
      "Iteration 1169, loss = 0.22989214\n",
      "Iteration 351, loss = 0.32221563Iteration 1181, loss = 0.26047598\n",
      "\n",
      "Iteration 1190, loss = 0.28782014\n",
      "Iteration 132, loss = 0.37437692\n",
      "Iteration 1170, loss = 0.22948908\n",
      "Iteration 2352, loss = 0.14522971\n",
      "Iteration 352, loss = 0.32209416\n",
      "Iteration 133, loss = 0.37379553\n",
      "Iteration 353, loss = 0.32196017\n",
      "Iteration 1171, loss = 0.22923515\n",
      "Iteration 316, loss = 0.41611941\n",
      "Iteration 134, loss = 0.37326799\n",
      "Iteration 2260, loss = 0.17640771\n",
      "Iteration 2353, loss = 0.14530596\n",
      "Iteration 1172, loss = 0.22907812\n",
      "Iteration 2354, loss = 0.14523615\n",
      "Iteration 1191, loss = 0.28771582\n",
      "Iteration 135, loss = 0.37266683\n",
      "Iteration 1182, loss = 0.26039288\n",
      "Iteration 136, loss = 0.37208058\n",
      "Iteration 354, loss = 0.32183132\n",
      "Iteration 317, loss = 0.41596100\n",
      "Iteration 2355, loss = 0.14496320\n",
      "Iteration 355, loss = 0.32170387\n",
      "Iteration 137, loss = 0.37155138\n",
      "Iteration 1192, loss = 0.28762346\n",
      "Iteration 356, loss = 0.32157909\n",
      "Iteration 138, loss = 0.37101628\n",
      "Iteration 357, loss = 0.32144193\n",
      "Iteration 1183, loss = 0.26017048\n",
      "Iteration 1193, loss = 0.28749596\n",
      "Iteration 358, loss = 0.32131259\n",
      "Iteration 139, loss = 0.37052161\n",
      "Iteration 359, loss = 0.32119875\n",
      "Iteration 140, loss = 0.36995574\n",
      "Iteration 360, loss = 0.32108010\n",
      "Iteration 2356, loss = 0.14487546\n",
      "Iteration 141, loss = 0.36946193\n",
      "Iteration 361, loss = 0.32094671\n",
      "Iteration 1173, loss = 0.22896932\n",
      "Iteration 2357, loss = 0.14472114\n",
      "Iteration 142, loss = 0.36896490\n",
      "Iteration 1194, loss = 0.28732360\n",
      "Iteration 2358, loss = 0.14464181\n",
      "Iteration 318, loss = 0.41579791\n",
      "Iteration 362, loss = 0.32083525\n",
      "Iteration 143, loss = 0.36843686\n",
      "Iteration 1184, loss = 0.25994484\n",
      "Iteration 2359, loss = 0.14465008\n",
      "Iteration 363, loss = 0.32069370\n",
      "Iteration 144, loss = 0.36791854\n",
      "Iteration 2360, loss = 0.14456172\n",
      "Iteration 364, loss = 0.32057274\n",
      "Iteration 145, loss = 0.36745123\n",
      "Iteration 2361, loss = 0.14437416\n",
      "Iteration 365, loss = 0.32046116\n",
      "Iteration 146, loss = 0.36697634\n",
      "Iteration 1195, loss = 0.28721578\n",
      "Iteration 319, loss = 0.41563051\n",
      "Iteration 366, loss = 0.32033099\n",
      "Iteration 2362, loss = 0.14427774\n",
      "Iteration 147, loss = 0.36650048\n",
      "Iteration 367, loss = 0.32023197\n",
      "Iteration 1185, loss = 0.25973060\n",
      "Iteration 148, loss = 0.36600373\n",
      "Iteration 368, loss = 0.32009128\n",
      "Iteration 369, loss = 0.31998907\n",
      "Iteration 2363, loss = 0.14416576\n",
      "Iteration 1196, loss = 0.28710597\n",
      "Iteration 149, loss = 0.36555791\n",
      "Iteration 2261, loss = 0.17630449\n",
      "Iteration 370, loss = 0.31985681\n",
      "Iteration 150, loss = 0.36508021\n",
      "Iteration 2364, loss = 0.14413058\n",
      "Iteration 151, loss = 0.36461959\n",
      "Iteration 371, loss = 0.31972498\n",
      "Iteration 1186, loss = 0.25958295\n",
      "Iteration 320, loss = 0.41542077\n",
      "Iteration 372, loss = 0.31962970\n",
      "Iteration 1174, loss = 0.22854946\n",
      "Iteration 152, loss = 0.36421660\n",
      "Iteration 1197, loss = 0.28698014\n",
      "Iteration 2365, loss = 0.14406746\n",
      "Iteration 373, loss = 0.31950561\n",
      "Iteration 374, loss = 0.31938459\n",
      "Iteration 375, loss = 0.31925566\n",
      "Iteration 153, loss = 0.36374861\n",
      "Iteration 2366, loss = 0.14404809\n",
      "Iteration 1198, loss = 0.28690354\n",
      "Iteration 321, loss = 0.41526710\n",
      "Iteration 154, loss = 0.36329466\n",
      "Iteration 376, loss = 0.31916176\n",
      "Iteration 2367, loss = 0.14392659\n",
      "Iteration 155, loss = 0.36288844\n",
      "Iteration 377, loss = 0.31902507\n",
      "Iteration 378, loss = 0.31895415\n",
      "Iteration 1187, loss = 0.25943601\n",
      "Iteration 1199, loss = 0.28677103\n",
      "Iteration 322, loss = 0.41508277\n",
      "Iteration 2368, loss = 0.14373655\n",
      "Iteration 379, loss = 0.31880014\n",
      "Iteration 156, loss = 0.36248467\n",
      "Iteration 2262, loss = 0.17619028\n",
      "Iteration 2369, loss = 0.14364226\n",
      "Iteration 380, loss = 0.31869300\n",
      "Iteration 157, loss = 0.36206261\n",
      "Iteration 381, loss = 0.31858888\n",
      "Iteration 1200, loss = 0.28662144\n",
      "Iteration 382, loss = 0.31846516\n",
      "Iteration 158, loss = 0.36160650\n",
      "Iteration 2370, loss = 0.14370567\n",
      "Iteration 1188, loss = 0.25920725\n",
      "Iteration 2263, loss = 0.17613804\n",
      "Iteration 383, loss = 0.31835753\n",
      "Iteration 2371, loss = 0.14363342\n",
      "Iteration 384, loss = 0.31826069\n",
      "Iteration 323, loss = 0.41491036\n",
      "Iteration 385, loss = 0.31814703\n",
      "Iteration 386, loss = 0.31802858\n",
      "Iteration 2264, loss = 0.17606144\n",
      "Iteration 159, loss = 0.36121011\n",
      "Iteration 2372, loss = 0.14342289\n",
      "Iteration 160, loss = 0.36078793\n",
      "Iteration 387, loss = 0.31792181\n",
      "Iteration 161, loss = 0.36040382\n",
      "Iteration 2373, loss = 0.14335984\n",
      "Iteration 162, loss = 0.35999902\n",
      "Iteration 1201, loss = 0.28650075\n",
      "Iteration 2265, loss = 0.17594257\n",
      "Iteration 1175, loss = 0.22848798\n",
      "Iteration 2374, loss = 0.14351588\n",
      "Iteration 163, loss = 0.35963025\n",
      "Iteration 2375, loss = 0.14317558\n",
      "Iteration 1189, loss = 0.25899698\n",
      "Iteration 1176, loss = 0.22822963\n",
      "Iteration 1202, loss = 0.28639286\n",
      "Iteration 164, loss = 0.35921247\n",
      "Iteration 2376, loss = 0.14304937\n",
      "Iteration 388, loss = 0.31781227\n",
      "Iteration 2266, loss = 0.17590441\n",
      "Iteration 165, loss = 0.35885986\n",
      "Iteration 389, loss = 0.31769688\n",
      "Iteration 390, loss = 0.31758778\n",
      "Iteration 2377, loss = 0.14308492\n",
      "Iteration 324, loss = 0.41473842\n",
      "Iteration 1190, loss = 0.25875478\n",
      "Iteration 391, loss = 0.31750679\n",
      "Iteration 166, loss = 0.35845946\n",
      "Iteration 2267, loss = 0.17588894\n",
      "Iteration 392, loss = 0.31737617\n",
      "Iteration 1203, loss = 0.28630930\n",
      "Iteration 2378, loss = 0.14292544\n",
      "Iteration 167, loss = 0.35808092\n",
      "Iteration 1177, loss = 0.22792679\n",
      "Iteration 1191, loss = 0.25863429\n",
      "Iteration 2268, loss = 0.17578632\n",
      "Iteration 2379, loss = 0.14282669\n",
      "Iteration 393, loss = 0.31726946\n",
      "Iteration 325, loss = 0.41456186\n",
      "Iteration 394, loss = 0.31717129\n",
      "Iteration 395, loss = 0.31706676\n",
      "Iteration 396, loss = 0.31697015\n",
      "Iteration 397, loss = 0.31684905\n",
      "Iteration 1192, loss = 0.25864095\n",
      "Iteration 2380, loss = 0.14271235\n",
      "Iteration 2269, loss = 0.17562223\n",
      "Iteration 1178, loss = 0.22786246\n",
      "Iteration 398, loss = 0.31674352\n",
      "Iteration 168, loss = 0.35769397\n",
      "Iteration 1204, loss = 0.28617125\n",
      "Iteration 399, loss = 0.31662688\n",
      "Iteration 1179, loss = 0.22745040\n",
      "Iteration 326, loss = 0.41440450\n",
      "Iteration 2270, loss = 0.17556216\n",
      "Iteration 169, loss = 0.35734455\n",
      "Iteration 400, loss = 0.31654153\n",
      "Iteration 170, loss = 0.35696701\n",
      "Iteration 1180, loss = 0.22723615\n",
      "Iteration 1193, loss = 0.25826477\n",
      "Iteration 401, loss = 0.31643451\n",
      "Iteration 171, loss = 0.35659978\n",
      "Iteration 402, loss = 0.31634415\n",
      "Iteration 2381, loss = 0.14266564\n",
      "Iteration 1181, loss = 0.22699285\n",
      "Iteration 172, loss = 0.35626656\n",
      "Iteration 1205, loss = 0.28602586\n",
      "Iteration 2382, loss = 0.14250121\n",
      "Iteration 403, loss = 0.31625720\n",
      "Iteration 173, loss = 0.35590694\n",
      "Iteration 2383, loss = 0.14237887\n",
      "Iteration 1182, loss = 0.22694312\n",
      "Iteration 174, loss = 0.35556972\n",
      "Iteration 404, loss = 0.31613126\n",
      "Iteration 175, loss = 0.35520277\n",
      "Iteration 1194, loss = 0.25825221\n",
      "Iteration 327, loss = 0.41423407\n",
      "Iteration 405, loss = 0.31603412\n",
      "Iteration 176, loss = 0.35484861\n",
      "Iteration 406, loss = 0.31593090\n",
      "Iteration 2384, loss = 0.14231969\n",
      "Iteration 407, loss = 0.31585283\n",
      "Iteration 177, loss = 0.35451829\n",
      "Iteration 408, loss = 0.31571025\n",
      "Iteration 2271, loss = 0.17551457\n",
      "Iteration 1183, loss = 0.22650894\n",
      "Iteration 409, loss = 0.31561955\n",
      "Iteration 2385, loss = 0.14239129\n",
      "Iteration 178, loss = 0.35416147\n",
      "Iteration 1206, loss = 0.28596683\n",
      "Iteration 2386, loss = 0.14215119\n",
      "Iteration 1195, loss = 0.25786993\n",
      "Iteration 410, loss = 0.31552463\n",
      "Iteration 328, loss = 0.41404843\n",
      "Iteration 2272, loss = 0.17541716\n",
      "Iteration 179, loss = 0.35383083\n",
      "Iteration 1184, loss = 0.22633318\n",
      "Iteration 2387, loss = 0.14242403\n",
      "Iteration 180, loss = 0.35351305\n",
      "Iteration 411, loss = 0.31543061\n",
      "Iteration 1185, loss = 0.22610089\n",
      "Iteration 181, loss = 0.35318446\n",
      "Iteration 2388, loss = 0.14198963\n",
      "Iteration 412, loss = 0.31533397\n",
      "Iteration 182, loss = 0.35285817\n",
      "Iteration 329, loss = 0.41386670\n",
      "Iteration 1186, loss = 0.22592280\n",
      "Iteration 2389, loss = 0.14193844\n",
      "Iteration 183, loss = 0.35251238\n",
      "Iteration 1196, loss = 0.25766305\n",
      "Iteration 2273, loss = 0.17534753\n",
      "Iteration 413, loss = 0.31521780\n",
      "Iteration 2390, loss = 0.14186069\n",
      "Iteration 2391, loss = 0.14177251\n",
      "Iteration 1207, loss = 0.28581144\n",
      "Iteration 330, loss = 0.41374647\n",
      "Iteration 184, loss = 0.35221811\n",
      "Iteration 414, loss = 0.31514973\n",
      "Iteration 185, loss = 0.35188957\n",
      "Iteration 1187, loss = 0.22569077\n",
      "Iteration 1197, loss = 0.25759328\n",
      "Iteration 2392, loss = 0.14161688\n",
      "Iteration 415, loss = 0.31502477\n",
      "Iteration 1208, loss = 0.28565301\n",
      "Iteration 186, loss = 0.35159359\n",
      "Iteration 416, loss = 0.31493870\n",
      "Iteration 2274, loss = 0.17528096\n",
      "Iteration 417, loss = 0.31483449\n",
      "Iteration 187, loss = 0.35127111\n",
      "Iteration 418, loss = 0.31474120\n",
      "Iteration 2393, loss = 0.14151591\n",
      "Iteration 419, loss = 0.31461817\n",
      "Iteration 188, loss = 0.35094395\n",
      "Iteration 1188, loss = 0.22540232\n",
      "Iteration 2275, loss = 0.17523646\n",
      "Iteration 2394, loss = 0.14139417\n",
      "Iteration 420, loss = 0.31453030\n",
      "Iteration 189, loss = 0.35070382\n",
      "Iteration 331, loss = 0.41360544\n",
      "Iteration 1209, loss = 0.28558319\n",
      "Iteration 1189, loss = 0.22541214\n",
      "Iteration 421, loss = 0.31446893\n",
      "Iteration 2395, loss = 0.14130586\n",
      "Iteration 190, loss = 0.35035560\n",
      "Iteration 2276, loss = 0.17508330\n",
      "Iteration 1198, loss = 0.25742825\n",
      "Iteration 1210, loss = 0.28544075\n",
      "Iteration 1190, loss = 0.22513857\n",
      "Iteration 422, loss = 0.31436096\n",
      "Iteration 423, loss = 0.31424454\n",
      "Iteration 332, loss = 0.41335749\n",
      "Iteration 1191, loss = 0.22468934\n",
      "Iteration 424, loss = 0.31413796\n",
      "Iteration 191, loss = 0.35005959\n",
      "Iteration 425, loss = 0.31409061\n",
      "Iteration 1192, loss = 0.22453471\n",
      "Iteration 426, loss = 0.31394412\n",
      "Iteration 192, loss = 0.34976047\n",
      "Iteration 427, loss = 0.31384913\n",
      "Iteration 2396, loss = 0.14133448\n",
      "Iteration 1211, loss = 0.28534221\n",
      "Iteration 1199, loss = 0.25717137\n",
      "Iteration 428, loss = 0.31375894\n",
      "Iteration 1193, loss = 0.22422278\n",
      "Iteration 429, loss = 0.31365822\n",
      "Iteration 333, loss = 0.41316213\n",
      "Iteration 2277, loss = 0.17495746\n",
      "Iteration 193, loss = 0.34946058\n",
      "Iteration 2397, loss = 0.14113893\n",
      "Iteration 1194, loss = 0.22415345\n",
      "Iteration 2398, loss = 0.14110055\n",
      "Iteration 430, loss = 0.31357545\n",
      "Iteration 1212, loss = 0.28522554\n",
      "Iteration 194, loss = 0.34919258\n",
      "Iteration 1200, loss = 0.25698432\n",
      "Iteration 2399, loss = 0.14103779\n",
      "Iteration 195, loss = 0.34889734\n",
      "Iteration 431, loss = 0.31346297\n",
      "Iteration 432, loss = 0.31337492\n",
      "Iteration 2400, loss = 0.14092525\n",
      "Iteration 334, loss = 0.41300489\n",
      "Iteration 1195, loss = 0.22382030\n",
      "Iteration 433, loss = 0.31328124\n",
      "Iteration 2278, loss = 0.17498011\n",
      "Iteration 1213, loss = 0.28512995\n",
      "Iteration 2401, loss = 0.14082767Iteration 434, loss = 0.31319582\n",
      "\n",
      "Iteration 196, loss = 0.34860166\n",
      "Iteration 435, loss = 0.31308550\n",
      "Iteration 436, loss = 0.31299650\n",
      "Iteration 2402, loss = 0.14076774\n",
      "Iteration 1201, loss = 0.25705461\n",
      "Iteration 2279, loss = 0.17482252\n",
      "Iteration 1196, loss = 0.22364600\n",
      "Iteration 1214, loss = 0.28493436\n",
      "Iteration 197, loss = 0.34832606\n",
      "Iteration 437, loss = 0.31291003\n",
      "Iteration 1197, loss = 0.22332864\n",
      "Iteration 2403, loss = 0.14076607\n",
      "Iteration 198, loss = 0.34804285\n",
      "Iteration 2280, loss = 0.17471870\n",
      "Iteration 335, loss = 0.41283918\n",
      "Iteration 438, loss = 0.31280356\n",
      "Iteration 1202, loss = 0.25659662\n",
      "Iteration 2404, loss = 0.14058456\n",
      "Iteration 199, loss = 0.34775804\n",
      "Iteration 1215, loss = 0.28484660\n",
      "Iteration 1198, loss = 0.22315983\n",
      "Iteration 439, loss = 0.31271657\n",
      "Iteration 2405, loss = 0.14046281\n",
      "Iteration 440, loss = 0.31262001\n",
      "Iteration 200, loss = 0.34748087\n",
      "Iteration 441, loss = 0.31253022\n",
      "Iteration 2406, loss = 0.14044561\n",
      "Iteration 442, loss = 0.31245345\n",
      "Iteration 1203, loss = 0.25642875\n",
      "Iteration 1216, loss = 0.28469786\n",
      "Iteration 2407, loss = 0.14065142\n",
      "Iteration 2281, loss = 0.17464784\n",
      "Iteration 443, loss = 0.31233249\n",
      "Iteration 1199, loss = 0.22285864\n",
      "Iteration 201, loss = 0.34722335\n",
      "Iteration 2408, loss = 0.14024861\n",
      "Iteration 444, loss = 0.31226562\n",
      "Iteration 336, loss = 0.41267008\n",
      "Iteration 202, loss = 0.34692059\n",
      "Iteration 1217, loss = 0.28461047\n",
      "Iteration 2409, loss = 0.14016758\n",
      "Iteration 1200, loss = 0.22306698\n",
      "Iteration 445, loss = 0.31215386\n",
      "Iteration 2410, loss = 0.14003574\n",
      "Iteration 2282, loss = 0.17459404\n",
      "Iteration 337, loss = 0.41252616\n",
      "Iteration 1218, loss = 0.28445648\n",
      "Iteration 2411, loss = 0.13995681\n",
      "Iteration 1201, loss = 0.22243183\n",
      "Iteration 1204, loss = 0.25634812Iteration 446, loss = 0.31207689\n",
      "\n",
      "Iteration 2412, loss = 0.14026364\n",
      "Iteration 447, loss = 0.31196633\n",
      "Iteration 2413, loss = 0.13976213\n",
      "Iteration 448, loss = 0.31190530\n",
      "Iteration 2414, loss = 0.13974081\n",
      "Iteration 449, loss = 0.31178144\n",
      "Iteration 338, loss = 0.41233124\n",
      "Iteration 203, loss = 0.34666566\n",
      "Iteration 1202, loss = 0.22218285\n",
      "Iteration 450, loss = 0.31169785\n",
      "Iteration 1219, loss = 0.28434015\n",
      "Iteration 2415, loss = 0.13966239\n",
      "Iteration 204, loss = 0.34639844\n",
      "Iteration 451, loss = 0.31160672\n",
      "Iteration 2283, loss = 0.17463588\n",
      "Iteration 205, loss = 0.34613276\n",
      "Iteration 452, loss = 0.31153556\n",
      "Iteration 2416, loss = 0.13959740\n",
      "Iteration 206, loss = 0.34589932\n",
      "Iteration 453, loss = 0.31141645\n",
      "Iteration 1205, loss = 0.25614677\n",
      "Iteration 1203, loss = 0.22202224\n",
      "Iteration 339, loss = 0.41214824\n",
      "Iteration 2284, loss = 0.17444808\n",
      "Iteration 207, loss = 0.34559988\n",
      "Iteration 454, loss = 0.31135404\n",
      "Iteration 2417, loss = 0.13946388\n",
      "Iteration 1220, loss = 0.28420628\n",
      "Iteration 1204, loss = 0.22179265\n",
      "Iteration 208, loss = 0.34535131\n",
      "Iteration 2418, loss = 0.13945351\n",
      "Iteration 1205, loss = 0.22152601\n",
      "Iteration 209, loss = 0.34507996\n",
      "Iteration 455, loss = 0.31123814\n",
      "Iteration 2419, loss = 0.13933980\n",
      "Iteration 210, loss = 0.34483680\n",
      "Iteration 1221, loss = 0.28413029\n",
      "Iteration 340, loss = 0.41199126\n",
      "Iteration 456, loss = 0.31114400\n",
      "Iteration 2420, loss = 0.13926235\n",
      "Iteration 457, loss = 0.31111602\n",
      "Iteration 211, loss = 0.34456069\n",
      "Iteration 1222, loss = 0.28399818\n",
      "Iteration 2421, loss = 0.13914949\n",
      "Iteration 2285, loss = 0.17433687\n",
      "Iteration 1206, loss = 0.22140271\n",
      "Iteration 458, loss = 0.31096568\n",
      "Iteration 2422, loss = 0.13910273\n",
      "Iteration 1206, loss = 0.25596749\n",
      "Iteration 212, loss = 0.34433419\n",
      "Iteration 1223, loss = 0.28398096\n",
      "Iteration 459, loss = 0.31087976\n",
      "Iteration 213, loss = 0.34408257\n",
      "Iteration 2423, loss = 0.13895071\n",
      "Iteration 460, loss = 0.31083598\n",
      "Iteration 341, loss = 0.41181380\n",
      "Iteration 214, loss = 0.34383211\n",
      "Iteration 2424, loss = 0.13889464\n",
      "Iteration 1224, loss = 0.28373985\n",
      "Iteration 215, loss = 0.34357352\n",
      "Iteration 2286, loss = 0.17420312\n",
      "Iteration 461, loss = 0.31073837\n",
      "Iteration 2425, loss = 0.13882979\n",
      "Iteration 216, loss = 0.34333686\n",
      "Iteration 217, loss = 0.34310912\n",
      "Iteration 1207, loss = 0.22113792\n",
      "Iteration 2426, loss = 0.13877573\n",
      "Iteration 218, loss = 0.34286059\n",
      "Iteration 1207, loss = 0.25585422\n",
      "Iteration 2287, loss = 0.17413218\n",
      "Iteration 1225, loss = 0.28360472\n",
      "Iteration 219, loss = 0.34264524\n",
      "Iteration 2427, loss = 0.13859446\n",
      "Iteration 462, loss = 0.31063528\n",
      "Iteration 342, loss = 0.41166409\n",
      "Iteration 1208, loss = 0.22115638\n",
      "Iteration 220, loss = 0.34239523\n",
      "Iteration 2428, loss = 0.13857742\n",
      "Iteration 221, loss = 0.34216370\n",
      "Iteration 463, loss = 0.31052917\n",
      "Iteration 464, loss = 0.31044551\n",
      "Iteration 1209, loss = 0.22061458\n",
      "Iteration 1226, loss = 0.28349924\n",
      "Iteration 222, loss = 0.34192690\n",
      "Iteration 2429, loss = 0.13842593\n",
      "Iteration 2288, loss = 0.17416467\n",
      "Iteration 465, loss = 0.31035122\n",
      "Iteration 2430, loss = 0.13847152\n",
      "Iteration 343, loss = 0.41153357\n",
      "Iteration 1208, loss = 0.25550964\n",
      "Iteration 1210, loss = 0.22070179\n",
      "Iteration 223, loss = 0.34170560\n",
      "Iteration 2431, loss = 0.13822626\n",
      "Iteration 1227, loss = 0.28354039\n",
      "Iteration 466, loss = 0.31027486\n",
      "Iteration 2289, loss = 0.17401299\n",
      "Iteration 467, loss = 0.31021082\n",
      "Iteration 224, loss = 0.34145776\n",
      "Iteration 468, loss = 0.31011084\n",
      "Iteration 2432, loss = 0.13819624\n",
      "Iteration 1228, loss = 0.28327501\n",
      "Iteration 225, loss = 0.34121931\n",
      "Iteration 1211, loss = 0.22020305\n",
      "Iteration 2290, loss = 0.17390630\n",
      "Iteration 469, loss = 0.31003610\n",
      "Iteration 1229, loss = 0.28314038\n",
      "Iteration 2433, loss = 0.13811197\n",
      "Iteration 226, loss = 0.34100350\n",
      "Iteration 470, loss = 0.30994538\n",
      "Iteration 344, loss = 0.41130587\n",
      "Iteration 2434, loss = 0.13801883\n",
      "Iteration 1230, loss = 0.28304777\n",
      "Iteration 471, loss = 0.30983866\n",
      "Iteration 227, loss = 0.34077655\n",
      "Iteration 1209, loss = 0.25536463\n",
      "Iteration 2291, loss = 0.17379950\n",
      "Iteration 1212, loss = 0.22018191\n",
      "Iteration 472, loss = 0.30979569\n",
      "Iteration 1231, loss = 0.28290760\n",
      "Iteration 2435, loss = 0.13797400\n",
      "Iteration 228, loss = 0.34056473\n",
      "Iteration 2292, loss = 0.17371048\n",
      "Iteration 345, loss = 0.41113966\n",
      "Iteration 1213, loss = 0.21980870\n",
      "Iteration 2436, loss = 0.13790817\n",
      "Iteration 229, loss = 0.34032913\n",
      "Iteration 473, loss = 0.30967913\n",
      "Iteration 1214, loss = 0.21960947\n",
      "Iteration 230, loss = 0.34012258\n",
      "Iteration 1232, loss = 0.28279296\n",
      "Iteration 474, loss = 0.30961796\n",
      "Iteration 231, loss = 0.33989672\n",
      "Iteration 475, loss = 0.30949702\n",
      "Iteration 2437, loss = 0.13782645\n",
      "Iteration 232, loss = 0.33968473\n",
      "Iteration 476, loss = 0.30941367\n",
      "Iteration 1233, loss = 0.28269927\n",
      "Iteration 477, loss = 0.30936045\n",
      "Iteration 233, loss = 0.33946377\n",
      "Iteration 1210, loss = 0.25510853\n",
      "Iteration 478, loss = 0.30925842\n",
      "Iteration 2293, loss = 0.17362092\n",
      "Iteration 234, loss = 0.33923856\n",
      "Iteration 1215, loss = 0.21932769\n",
      "Iteration 479, loss = 0.30917839\n",
      "Iteration 2438, loss = 0.13775160\n",
      "Iteration 346, loss = 0.41099865\n",
      "Iteration 480, loss = 0.30909144\n",
      "Iteration 2439, loss = 0.13777678\n",
      "Iteration 1211, loss = 0.25512387\n",
      "Iteration 2294, loss = 0.17357722\n",
      "Iteration 481, loss = 0.30900504\n",
      "Iteration 235, loss = 0.33902773\n",
      "Iteration 1216, loss = 0.21909634\n",
      "Iteration 482, loss = 0.30892133\n",
      "Iteration 236, loss = 0.33881184\n",
      "Iteration 483, loss = 0.30887148\n",
      "Iteration 237, loss = 0.33859963\n",
      "Iteration 2440, loss = 0.13754678\n",
      "Iteration 347, loss = 0.41087637\n",
      "Iteration 484, loss = 0.30875831\n",
      "Iteration 1212, loss = 0.25488100\n",
      "Iteration 2441, loss = 0.13748230\n",
      "Iteration 1217, loss = 0.21904978\n",
      "Iteration 1234, loss = 0.28256675\n",
      "Iteration 2442, loss = 0.13736258\n",
      "Iteration 238, loss = 0.33840632\n",
      "Iteration 1218, loss = 0.21859825\n",
      "Iteration 485, loss = 0.30868472\n",
      "Iteration 2295, loss = 0.17348459\n",
      "Iteration 1213, loss = 0.25458345\n",
      "Iteration 1219, loss = 0.21853952\n",
      "Iteration 2443, loss = 0.13727476\n",
      "Iteration 1235, loss = 0.28243638\n",
      "Iteration 486, loss = 0.30860924\n",
      "Iteration 239, loss = 0.33818927\n",
      "Iteration 348, loss = 0.41067241\n",
      "Iteration 487, loss = 0.30853368\n",
      "Iteration 488, loss = 0.30841086\n",
      "Iteration 489, loss = 0.30833607\n",
      "Iteration 240, loss = 0.33797980\n",
      "Iteration 490, loss = 0.30824627\n",
      "Iteration 2444, loss = 0.13721148\n",
      "Iteration 1236, loss = 0.28238028\n",
      "Iteration 349, loss = 0.41050056\n",
      "Iteration 241, loss = 0.33777445\n",
      "Iteration 2296, loss = 0.17348923\n",
      "Iteration 491, loss = 0.30816713\n",
      "Iteration 1214, loss = 0.25439632\n",
      "Iteration 1237, loss = 0.28219184\n",
      "Iteration 1220, loss = 0.21823982\n",
      "Iteration 492, loss = 0.30809712\n",
      "Iteration 2297, loss = 0.17337336\n",
      "Iteration 242, loss = 0.33755699\n",
      "Iteration 493, loss = 0.30800593\n",
      "Iteration 350, loss = 0.41036519\n",
      "Iteration 2445, loss = 0.13710047\n",
      "Iteration 243, loss = 0.33735378\n",
      "Iteration 1238, loss = 0.28211556\n",
      "Iteration 244, loss = 0.33715244\n",
      "Iteration 1221, loss = 0.21795445\n",
      "Iteration 2298, loss = 0.17352956\n",
      "Iteration 245, loss = 0.33693852\n",
      "Iteration 494, loss = 0.30791934\n",
      "Iteration 2446, loss = 0.13706807\n",
      "Iteration 495, loss = 0.30784655\n",
      "Iteration 1215, loss = 0.25430201\n",
      "Iteration 1222, loss = 0.21770709\n",
      "Iteration 351, loss = 0.41016415\n",
      "Iteration 496, loss = 0.30775793\n",
      "Iteration 2447, loss = 0.13694200\n",
      "Iteration 497, loss = 0.30767264\n",
      "Iteration 246, loss = 0.33675161\n",
      "Iteration 498, loss = 0.30759112\n",
      "Iteration 2448, loss = 0.13687161\n",
      "Iteration 499, loss = 0.30751889\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1216, loss = 0.25402373\n",
      "Iteration 1239, loss = 0.28203648\n",
      "Iteration 2299, loss = 0.17319529\n",
      "Iteration 247, loss = 0.33654924\n",
      "Iteration 1223, loss = 0.21753798\n",
      "Iteration 352, loss = 0.40994867\n",
      "Iteration 2449, loss = 0.13675615\n",
      "Iteration 2300, loss = 0.17312028\n",
      "Iteration 1224, loss = 0.21740173\n",
      "Iteration 248, loss = 0.33635282\n",
      "Iteration 1240, loss = 0.28191898\n",
      "Iteration 1225, loss = 0.21731597\n",
      "Iteration 1217, loss = 0.25385470\n",
      "Iteration 249, loss = 0.33614729\n",
      "Iteration 2450, loss = 0.13684692\n",
      "Iteration 353, loss = 0.40979358\n",
      "Iteration 1241, loss = 0.28172375\n",
      "Iteration 250, loss = 0.33594739\n",
      "Iteration 1226, loss = 0.21685701\n",
      "Iteration 251, loss = 0.33574628\n",
      "Iteration 1, loss = 0.76265095\n",
      "Iteration 2451, loss = 0.13677147\n",
      "Iteration 2301, loss = 0.17298133\n",
      "Iteration 1218, loss = 0.25370324\n",
      "Iteration 1227, loss = 0.21672916\n",
      "Iteration 2, loss = 0.76013765\n",
      "Iteration 3, loss = 0.75631211\n",
      "Iteration 2452, loss = 0.13653697\n",
      "Iteration 4, loss = 0.75201350\n",
      "Iteration 2453, loss = 0.13649116\n",
      "Iteration 5, loss = 0.74703527\n",
      "Iteration 6, loss = 0.74184937\n",
      "Iteration 354, loss = 0.40963495\n",
      "Iteration 1219, loss = 0.25351190\n",
      "Iteration 252, loss = 0.33556940\n",
      "Iteration 1228, loss = 0.21644533\n",
      "Iteration 2454, loss = 0.13638570\n",
      "Iteration 7, loss = 0.73672769\n",
      "Iteration 8, loss = 0.73157188\n",
      "Iteration 1242, loss = 0.28162277\n",
      "Iteration 2455, loss = 0.13627668\n",
      "Iteration 1229, loss = 0.21614945\n",
      "Iteration 2302, loss = 0.17297709\n",
      "Iteration 9, loss = 0.72659687\n",
      "Iteration 2456, loss = 0.13620552\n",
      "Iteration 1230, loss = 0.21592312\n",
      "Iteration 10, loss = 0.72172239\n",
      "Iteration 2457, loss = 0.13620145\n",
      "Iteration 253, loss = 0.33536032\n",
      "Iteration 11, loss = 0.71712538\n",
      "Iteration 355, loss = 0.40945440\n",
      "Iteration 2458, loss = 0.13605616\n",
      "Iteration 1243, loss = 0.28148100\n",
      "Iteration 2303, loss = 0.17281376\n",
      "Iteration 254, loss = 0.33517701\n",
      "Iteration 12, loss = 0.71268927\n",
      "Iteration 2459, loss = 0.13600289\n",
      "Iteration 1220, loss = 0.25337289\n",
      "Iteration 255, loss = 0.33497928\n",
      "Iteration 2460, loss = 0.13586598\n",
      "Iteration 13, loss = 0.70823913\n",
      "Iteration 1231, loss = 0.21567773\n",
      "Iteration 256, loss = 0.33479459\n",
      "Iteration 2461, loss = 0.13589835\n",
      "Iteration 257, loss = 0.33459296\n",
      "Iteration 1244, loss = 0.28135193\n",
      "Iteration 2304, loss = 0.17277717\n",
      "Iteration 356, loss = 0.40930222\n",
      "Iteration 14, loss = 0.70404919\n",
      "Iteration 1232, loss = 0.21546723\n",
      "Iteration 258, loss = 0.33440446\n",
      "Iteration 1233, loss = 0.21519070\n",
      "Iteration 2462, loss = 0.13585821\n",
      "Iteration 259, loss = 0.33421306\n",
      "Iteration 15, loss = 0.70002624\n",
      "Iteration 2305, loss = 0.17273375\n",
      "Iteration 260, loss = 0.33402857\n",
      "Iteration 1245, loss = 0.28126572\n",
      "Iteration 16, loss = 0.69631607\n",
      "Iteration 357, loss = 0.40927610\n",
      "Iteration 1221, loss = 0.25319946\n",
      "Iteration 17, loss = 0.69241814\n",
      "Iteration 2463, loss = 0.13565977\n",
      "Iteration 261, loss = 0.33387016\n",
      "Iteration 1234, loss = 0.21498363\n",
      "Iteration 2306, loss = 0.17274532\n",
      "Iteration 262, loss = 0.33367505\n",
      "Iteration 1246, loss = 0.28116607\n",
      "Iteration 2464, loss = 0.13564721\n",
      "Iteration 18, loss = 0.68875145\n",
      "Iteration 263, loss = 0.33349102\n",
      "Iteration 19, loss = 0.68521858\n",
      "Iteration 1235, loss = 0.21480093\n",
      "Iteration 264, loss = 0.33330235\n",
      "Iteration 2465, loss = 0.13548992\n",
      "Iteration 1247, loss = 0.28100181\n",
      "Iteration 358, loss = 0.40895273\n",
      "Iteration 2466, loss = 0.13542618\n",
      "Iteration 1236, loss = 0.21461615\n",
      "Iteration 2467, loss = 0.13530905\n",
      "Iteration 1237, loss = 0.21437125\n",
      "Iteration 20, loss = 0.68161312\n",
      "Iteration 265, loss = 0.33311456\n",
      "Iteration 21, loss = 0.67820174\n",
      "Iteration 22, loss = 0.67472038\n",
      "Iteration 266, loss = 0.33291422\n",
      "Iteration 1238, loss = 0.21419880\n",
      "Iteration 2307, loss = 0.17263437\n",
      "Iteration 2468, loss = 0.13536728\n",
      "Iteration 1248, loss = 0.28090894\n",
      "Iteration 23, loss = 0.67141615\n",
      "Iteration 359, loss = 0.40877588\n",
      "Iteration 24, loss = 0.66800717\n",
      "Iteration 267, loss = 0.33277466\n",
      "Iteration 25, loss = 0.66482704\n",
      "Iteration 1222, loss = 0.25291566\n",
      "Iteration 1239, loss = 0.21395450\n",
      "Iteration 26, loss = 0.66157948\n",
      "Iteration 27, loss = 0.65830871\n",
      "Iteration 1249, loss = 0.28080269\n",
      "Iteration 268, loss = 0.33259044\n",
      "Iteration 2469, loss = 0.13528293\n",
      "Iteration 28, loss = 0.65506905\n",
      "Iteration 269, loss = 0.33240513\n",
      "Iteration 2308, loss = 0.17249502\n",
      "Iteration 1240, loss = 0.21403170\n",
      "Iteration 29, loss = 0.65181106\n",
      "Iteration 30, loss = 0.64870729\n",
      "Iteration 270, loss = 0.33223202\n",
      "Iteration 31, loss = 0.64540578\n",
      "Iteration 1241, loss = 0.21351639\n",
      "Iteration 32, loss = 0.64221078\n",
      "Iteration 33, loss = 0.63912772\n",
      "Iteration 2470, loss = 0.13515055\n",
      "Iteration 1223, loss = 0.25308177\n",
      "Iteration 1242, loss = 0.21324181\n",
      "Iteration 1250, loss = 0.28067000\n",
      "Iteration 34, loss = 0.63581458\n",
      "Iteration 360, loss = 0.40861523\n",
      "Iteration 271, loss = 0.33205721\n",
      "Iteration 2471, loss = 0.13505100\n",
      "Iteration 35, loss = 0.63262477\n",
      "Iteration 36, loss = 0.62936034\n",
      "Iteration 272, loss = 0.33188896\n",
      "Iteration 37, loss = 0.62610238\n",
      "Iteration 2472, loss = 0.13486313\n",
      "Iteration 2309, loss = 0.17234768\n",
      "Iteration 38, loss = 0.62292206\n",
      "Iteration 39, loss = 0.61966949\n",
      "Iteration 40, loss = 0.61642484\n",
      "Iteration 1251, loss = 0.28055664\n",
      "Iteration 273, loss = 0.33169868\n",
      "Iteration 1224, loss = 0.25278674\n",
      "Iteration 2310, loss = 0.17230482Iteration 41, loss = 0.61313518\n",
      "\n",
      "Iteration 2473, loss = 0.13476624\n",
      "Iteration 274, loss = 0.33151583\n",
      "Iteration 1243, loss = 0.21309915\n",
      "Iteration 361, loss = 0.40844843\n",
      "Iteration 42, loss = 0.60983111\n",
      "Iteration 275, loss = 0.33137375\n",
      "Iteration 43, loss = 0.60650901\n",
      "Iteration 44, loss = 0.60321553\n",
      "Iteration 45, loss = 0.60005523\n",
      "Iteration 2474, loss = 0.13480612\n",
      "Iteration 1244, loss = 0.21287210\n",
      "Iteration 1225, loss = 0.25234977\n",
      "Iteration 46, loss = 0.59671130\n",
      "Iteration 276, loss = 0.33120382\n",
      "Iteration 2311, loss = 0.17219748\n",
      "Iteration 362, loss = 0.40827720\n",
      "Iteration 1245, loss = 0.21259313\n",
      "Iteration 47, loss = 0.59337093\n",
      "Iteration 277, loss = 0.33103005\n",
      "Iteration 2475, loss = 0.13475368\n",
      "Iteration 1246, loss = 0.21236689\n",
      "Iteration 48, loss = 0.59006814\n",
      "Iteration 278, loss = 0.33086156\n",
      "Iteration 1252, loss = 0.28046671\n",
      "Iteration 49, loss = 0.58669482\n",
      "Iteration 279, loss = 0.33070465\n",
      "Iteration 1247, loss = 0.21254282\n",
      "Iteration 1226, loss = 0.25222098\n",
      "Iteration 2476, loss = 0.13465247\n",
      "Iteration 50, loss = 0.58336344\n",
      "Iteration 51, loss = 0.58004398\n",
      "Iteration 2477, loss = 0.13449661\n",
      "Iteration 52, loss = 0.57657314\n",
      "Iteration 2312, loss = 0.17210834\n",
      "Iteration 280, loss = 0.33051738\n",
      "Iteration 363, loss = 0.40810530\n",
      "Iteration 1248, loss = 0.21202404\n",
      "Iteration 53, loss = 0.57339469\n",
      "Iteration 2478, loss = 0.13448867\n",
      "Iteration 281, loss = 0.33034863\n",
      "Iteration 54, loss = 0.56996206\n",
      "Iteration 2479, loss = 0.13430217\n",
      "Iteration 1227, loss = 0.25236347\n",
      "Iteration 282, loss = 0.33017641\n",
      "Iteration 2480, loss = 0.13422126\n",
      "Iteration 55, loss = 0.56658161\n",
      "Iteration 56, loss = 0.56324201\n",
      "Iteration 57, loss = 0.55995224\n",
      "Iteration 283, loss = 0.33003188\n",
      "Iteration 2481, loss = 0.13412218\n",
      "Iteration 1253, loss = 0.28035311\n",
      "Iteration 58, loss = 0.55652997\n",
      "Iteration 59, loss = 0.55317687\n",
      "Iteration 2313, loss = 0.17215838\n",
      "Iteration 1249, loss = 0.21172023\n",
      "Iteration 2482, loss = 0.13413705\n",
      "Iteration 60, loss = 0.54982037\n",
      "Iteration 61, loss = 0.54653309\n",
      "Iteration 284, loss = 0.32987264\n",
      "Iteration 62, loss = 0.54321775\n",
      "Iteration 1250, loss = 0.21148562\n",
      "Iteration 2483, loss = 0.13403212\n",
      "Iteration 1228, loss = 0.25181937\n",
      "Iteration 2314, loss = 0.17202452\n",
      "Iteration 1254, loss = 0.28024750\n",
      "Iteration 63, loss = 0.53994949\n",
      "Iteration 2484, loss = 0.13392717\n",
      "Iteration 64, loss = 0.53666666\n",
      "Iteration 1251, loss = 0.21129929\n",
      "Iteration 364, loss = 0.40804027\n",
      "Iteration 1255, loss = 0.28005125\n",
      "Iteration 2315, loss = 0.17185921\n",
      "Iteration 65, loss = 0.53349325\n",
      "Iteration 2485, loss = 0.13384023Iteration 1252, loss = 0.21117845\n",
      "\n",
      "Iteration 66, loss = 0.53021646\n",
      "Iteration 1253, loss = 0.21080460\n",
      "Iteration 285, loss = 0.32969304\n",
      "Iteration 2486, loss = 0.13373984\n",
      "Iteration 67, loss = 0.52707216\n",
      "Iteration 2316, loss = 0.17179720\n",
      "Iteration 1256, loss = 0.27994737\n",
      "Iteration 1254, loss = 0.21058009\n",
      "Iteration 68, loss = 0.52386198\n",
      "Iteration 286, loss = 0.32954901\n",
      "Iteration 2487, loss = 0.13373182\n",
      "Iteration 365, loss = 0.40776625\n",
      "Iteration 69, loss = 0.52080644\n",
      "Iteration 1229, loss = 0.25179555\n",
      "Iteration 1255, loss = 0.21051805\n",
      "Iteration 1257, loss = 0.27981158\n",
      "Iteration 70, loss = 0.51777344\n",
      "Iteration 287, loss = 0.32937876\n",
      "Iteration 1256, loss = 0.21015439\n",
      "Iteration 2317, loss = 0.17170601\n",
      "Iteration 2488, loss = 0.13369133\n",
      "Iteration 1258, loss = 0.27981379\n",
      "Iteration 71, loss = 0.51459835\n",
      "Iteration 72, loss = 0.51152442\n",
      "Iteration 1257, loss = 0.20997464\n",
      "Iteration 1230, loss = 0.25156364\n",
      "Iteration 288, loss = 0.32924298\n",
      "Iteration 73, loss = 0.50854583\n",
      "Iteration 74, loss = 0.50564799\n",
      "Iteration 1259, loss = 0.27956507\n",
      "Iteration 75, loss = 0.50267956\n",
      "Iteration 1258, loss = 0.20970433\n",
      "Iteration 76, loss = 0.49986130\n",
      "Iteration 77, loss = 0.49699584\n",
      "Iteration 78, loss = 0.49424818\n",
      "Iteration 289, loss = 0.32905663\n",
      "Iteration 366, loss = 0.40759967\n",
      "Iteration 1259, loss = 0.20948190\n",
      "Iteration 79, loss = 0.49153987\n",
      "Iteration 2489, loss = 0.13350115\n",
      "Iteration 80, loss = 0.48875441\n",
      "Iteration 81, loss = 0.48625218\n",
      "Iteration 290, loss = 0.32890241\n",
      "Iteration 1231, loss = 0.25129091\n",
      "Iteration 1260, loss = 0.20927270\n",
      "Iteration 2490, loss = 0.13362817\n",
      "Iteration 2318, loss = 0.17163511\n",
      "Iteration 82, loss = 0.48354539\n",
      "Iteration 83, loss = 0.48105977\n",
      "Iteration 291, loss = 0.32876064\n",
      "Iteration 1260, loss = 0.27946031\n",
      "Iteration 2491, loss = 0.13336907\n",
      "Iteration 367, loss = 0.40748000\n",
      "Iteration 84, loss = 0.47846222\n",
      "Iteration 1261, loss = 0.20916434\n",
      "Iteration 85, loss = 0.47595995\n",
      "Iteration 86, loss = 0.47356493\n",
      "Iteration 292, loss = 0.32857923\n",
      "Iteration 2492, loss = 0.13326996\n",
      "Iteration 1261, loss = 0.27933941\n",
      "Iteration 87, loss = 0.47123579\n",
      "Iteration 1232, loss = 0.25118912\n",
      "Iteration 88, loss = 0.46889349\n",
      "Iteration 2493, loss = 0.13333347\n",
      "Iteration 89, loss = 0.46658909\n",
      "Iteration 2319, loss = 0.17158176\n",
      "Iteration 90, loss = 0.46429165\n",
      "Iteration 293, loss = 0.32842924\n",
      "Iteration 2494, loss = 0.13312516\n",
      "Iteration 91, loss = 0.46214998\n",
      "Iteration 294, loss = 0.32831078\n",
      "Iteration 1262, loss = 0.20883364\n",
      "Iteration 92, loss = 0.45994942\n",
      "Iteration 295, loss = 0.32812924\n",
      "Iteration 368, loss = 0.40725288\n",
      "Iteration 93, loss = 0.45784439\n",
      "Iteration 94, loss = 0.45582551\n",
      "Iteration 2495, loss = 0.13319977\n",
      "Iteration 1262, loss = 0.27928763\n",
      "Iteration 1263, loss = 0.20862306\n",
      "Iteration 95, loss = 0.45376449\n",
      "Iteration 296, loss = 0.32798001\n",
      "Iteration 2320, loss = 0.17145998\n",
      "Iteration 1233, loss = 0.25103661\n",
      "Iteration 2496, loss = 0.13304906\n",
      "Iteration 96, loss = 0.45177901\n",
      "Iteration 97, loss = 0.44985211\n",
      "Iteration 1264, loss = 0.20848129\n",
      "Iteration 369, loss = 0.40718585\n",
      "Iteration 297, loss = 0.32786422\n",
      "Iteration 1263, loss = 0.27912533\n",
      "Iteration 98, loss = 0.44798390\n",
      "Iteration 2497, loss = 0.13308879\n",
      "Iteration 2321, loss = 0.17140938\n",
      "Iteration 1265, loss = 0.20822082\n",
      "Iteration 1234, loss = 0.25078020\n",
      "Iteration 99, loss = 0.44621619\n",
      "Iteration 298, loss = 0.32766919\n",
      "Iteration 100, loss = 0.44430277\n",
      "Iteration 1264, loss = 0.27896715\n",
      "Iteration 2498, loss = 0.13280754\n",
      "Iteration 370, loss = 0.40690980\n",
      "Iteration 101, loss = 0.44247526\n",
      "Iteration 299, loss = 0.32751000\n",
      "Iteration 102, loss = 0.44084351\n",
      "Iteration 1266, loss = 0.20804272\n",
      "Iteration 2322, loss = 0.17125811\n",
      "Iteration 2499, loss = 0.13276743\n",
      "Iteration 103, loss = 0.43917038\n",
      "Iteration 1235, loss = 0.25058982\n",
      "Iteration 300, loss = 0.32737935\n",
      "Iteration 104, loss = 0.43749914\n",
      "Iteration 2500, loss = 0.13271785\n",
      "Iteration 1265, loss = 0.27889243\n",
      "Iteration 105, loss = 0.43580023\n",
      "Iteration 1267, loss = 0.20777681\n",
      "Iteration 106, loss = 0.43433365\n",
      "Iteration 301, loss = 0.32724111\n",
      "Iteration 107, loss = 0.43276127\n",
      "Iteration 2323, loss = 0.17119336\n",
      "Iteration 108, loss = 0.43128481\n",
      "Iteration 302, loss = 0.32707547\n",
      "Iteration 371, loss = 0.40679112\n",
      "Iteration 1268, loss = 0.20751223\n",
      "Iteration 303, loss = 0.32694839\n",
      "Iteration 109, loss = 0.42974448\n",
      "Iteration 304, loss = 0.32676502\n",
      "Iteration 110, loss = 0.42835385\n",
      "Iteration 2501, loss = 0.13258322\n",
      "Iteration 111, loss = 0.42697345\n",
      "Iteration 305, loss = 0.32663090\n",
      "Iteration 1266, loss = 0.27874799\n",
      "Iteration 1269, loss = 0.20741448\n",
      "Iteration 112, loss = 0.42555957\n",
      "Iteration 1236, loss = 0.25051155\n",
      "Iteration 306, loss = 0.32646794\n",
      "Iteration 113, loss = 0.42418996\n",
      "Iteration 372, loss = 0.40661549\n",
      "Iteration 2324, loss = 0.17117076\n",
      "Iteration 114, loss = 0.42294913\n",
      "Iteration 307, loss = 0.32636656\n",
      "Iteration 2502, loss = 0.13255336\n",
      "Iteration 115, loss = 0.42173459\n",
      "Iteration 1237, loss = 0.25048472\n",
      "Iteration 308, loss = 0.32617677\n",
      "Iteration 116, loss = 0.42041742\n",
      "Iteration 1270, loss = 0.20715474\n",
      "Iteration 1267, loss = 0.27861339\n",
      "Iteration 2503, loss = 0.13239501\n",
      "Iteration 2325, loss = 0.17119147\n",
      "Iteration 309, loss = 0.32604242\n",
      "Iteration 117, loss = 0.41924989\n",
      "Iteration 373, loss = 0.40648469\n",
      "Iteration 118, loss = 0.41803002\n",
      "Iteration 310, loss = 0.32588559\n",
      "Iteration 2504, loss = 0.13231130\n",
      "Iteration 119, loss = 0.41684505\n",
      "Iteration 1271, loss = 0.20687079\n",
      "Iteration 120, loss = 0.41569600\n",
      "Iteration 2505, loss = 0.13234877\n",
      "Iteration 121, loss = 0.41455141\n",
      "Iteration 1268, loss = 0.27851937\n",
      "Iteration 311, loss = 0.32575703\n",
      "Iteration 2326, loss = 0.17103104\n",
      "Iteration 2506, loss = 0.13233303\n",
      "Iteration 1238, loss = 0.25010503\n",
      "Iteration 312, loss = 0.32561686\n",
      "Iteration 122, loss = 0.41349564\n",
      "Iteration 1272, loss = 0.20699349\n",
      "Iteration 123, loss = 0.41240346\n",
      "Iteration 124, loss = 0.41135488\n",
      "Iteration 1269, loss = 0.27837145\n",
      "Iteration 313, loss = 0.32546663\n",
      "Iteration 2507, loss = 0.13217466\n",
      "Iteration 374, loss = 0.40626986\n",
      "Iteration 125, loss = 0.41036392\n",
      "Iteration 314, loss = 0.32531182\n",
      "Iteration 2508, loss = 0.13203783\n",
      "Iteration 126, loss = 0.40930872\n",
      "Iteration 1273, loss = 0.20694645\n",
      "Iteration 2327, loss = 0.17092422\n",
      "Iteration 2509, loss = 0.13199188\n",
      "Iteration 127, loss = 0.40826386\n",
      "Iteration 1239, loss = 0.24988160\n",
      "Iteration 1270, loss = 0.27837347\n",
      "Iteration 315, loss = 0.32517598\n",
      "Iteration 2510, loss = 0.13199400\n",
      "Iteration 128, loss = 0.40740225\n",
      "Iteration 375, loss = 0.40611172\n",
      "Iteration 129, loss = 0.40638564\n",
      "Iteration 2328, loss = 0.17087829\n",
      "Iteration 130, loss = 0.40542664\n",
      "Iteration 2511, loss = 0.13184194\n",
      "Iteration 1271, loss = 0.27819148\n",
      "Iteration 131, loss = 0.40450211\n",
      "Iteration 316, loss = 0.32503508\n",
      "Iteration 132, loss = 0.40360909\n",
      "Iteration 1274, loss = 0.20618689\n",
      "Iteration 1272, loss = 0.27802028\n",
      "Iteration 2512, loss = 0.13180606\n",
      "Iteration 133, loss = 0.40272764\n",
      "Iteration 1240, loss = 0.24969520\n",
      "Iteration 134, loss = 0.40188453\n",
      "Iteration 317, loss = 0.32489523\n",
      "Iteration 1275, loss = 0.20602647\n",
      "Iteration 135, loss = 0.40101146\n",
      "Iteration 136, loss = 0.40016780\n",
      "Iteration 137, loss = 0.39929374\n",
      "Iteration 1273, loss = 0.27791456\n",
      "Iteration 318, loss = 0.32475581\n",
      "Iteration 2513, loss = 0.13174938\n",
      "Iteration 1276, loss = 0.20584638\n",
      "Iteration 2329, loss = 0.17077780\n",
      "Iteration 138, loss = 0.39851633\n",
      "Iteration 319, loss = 0.32461406\n",
      "Iteration 2514, loss = 0.13159109\n",
      "Iteration 376, loss = 0.40590273\n",
      "Iteration 320, loss = 0.32447504\n",
      "Iteration 139, loss = 0.39770030\n",
      "Iteration 2330, loss = 0.17068945\n",
      "Iteration 1277, loss = 0.20560892\n",
      "Iteration 140, loss = 0.39688661\n",
      "Iteration 1274, loss = 0.27790084\n",
      "Iteration 377, loss = 0.40576778\n",
      "Iteration 1241, loss = 0.24950865\n",
      "Iteration 321, loss = 0.32433556\n",
      "Iteration 2331, loss = 0.17060014\n",
      "Iteration 2515, loss = 0.13155619\n",
      "Iteration 141, loss = 0.39616032\n",
      "Iteration 322, loss = 0.32418859\n",
      "Iteration 323, loss = 0.32405331\n",
      "Iteration 2516, loss = 0.13140908\n",
      "Iteration 2332, loss = 0.17052206\n",
      "Iteration 324, loss = 0.32392570\n",
      "Iteration 142, loss = 0.39537089\n",
      "Iteration 378, loss = 0.40561770\n",
      "Iteration 325, loss = 0.32378785\n",
      "Iteration 143, loss = 0.39455767\n",
      "Iteration 326, loss = 0.32365298\n",
      "Iteration 144, loss = 0.39386554\n",
      "Iteration 1278, loss = 0.20533085\n",
      "Iteration 2333, loss = 0.17040500\n",
      "Iteration 2517, loss = 0.13133939\n",
      "Iteration 145, loss = 0.39310033\n",
      "Iteration 327, loss = 0.32352359\n",
      "Iteration 146, loss = 0.39233090\n",
      "Iteration 147, loss = 0.39168253\n",
      "Iteration 148, loss = 0.39095018\n",
      "Iteration 328, loss = 0.32339461\n",
      "Iteration 1275, loss = 0.27769801\n",
      "Iteration 1242, loss = 0.24928867\n",
      "Iteration 149, loss = 0.39024194\n",
      "Iteration 1279, loss = 0.20516878\n",
      "Iteration 150, loss = 0.38953072\n",
      "Iteration 151, loss = 0.38886669\n",
      "Iteration 2518, loss = 0.13129453\n",
      "Iteration 152, loss = 0.38815746\n",
      "Iteration 1280, loss = 0.20493403\n",
      "Iteration 1276, loss = 0.27755869\n",
      "Iteration 153, loss = 0.38754095\n",
      "Iteration 154, loss = 0.38685269\n",
      "Iteration 329, loss = 0.32327545\n",
      "Iteration 155, loss = 0.38616890\n",
      "Iteration 379, loss = 0.40542464\n",
      "Iteration 1281, loss = 0.20474940\n",
      "Iteration 2519, loss = 0.13116627\n",
      "Iteration 2334, loss = 0.17049808\n",
      "Iteration 156, loss = 0.38558740\n",
      "Iteration 1277, loss = 0.27742859\n",
      "Iteration 2520, loss = 0.13115625\n",
      "Iteration 330, loss = 0.32311418\n",
      "Iteration 157, loss = 0.38492974\n",
      "Iteration 2521, loss = 0.13120051\n",
      "Iteration 1282, loss = 0.20457264\n",
      "Iteration 380, loss = 0.40530878\n",
      "Iteration 1243, loss = 0.24910815\n",
      "Iteration 158, loss = 0.38431712\n",
      "Iteration 2522, loss = 0.13091189\n",
      "Iteration 331, loss = 0.32298248\n",
      "Iteration 159, loss = 0.38368788\n",
      "Iteration 2335, loss = 0.17025195\n",
      "Iteration 2523, loss = 0.13095089\n",
      "Iteration 160, loss = 0.38308547\n",
      "Iteration 1278, loss = 0.27735401\n",
      "Iteration 2524, loss = 0.13080698\n",
      "Iteration 332, loss = 0.32287101\n",
      "Iteration 161, loss = 0.38250011\n",
      "Iteration 162, loss = 0.38190925\n",
      "Iteration 2525, loss = 0.13083270\n",
      "Iteration 1283, loss = 0.20429273\n",
      "Iteration 163, loss = 0.38130371\n",
      "Iteration 2336, loss = 0.17024760\n",
      "Iteration 381, loss = 0.40516798\n",
      "Iteration 333, loss = 0.32272924\n",
      "Iteration 164, loss = 0.38073260\n",
      "Iteration 1284, loss = 0.20422728\n",
      "Iteration 2526, loss = 0.13064125\n",
      "Iteration 165, loss = 0.38014815\n",
      "Iteration 2337, loss = 0.17013111\n",
      "Iteration 1279, loss = 0.27723784\n",
      "Iteration 1244, loss = 0.24901667\n",
      "Iteration 166, loss = 0.37964293\n",
      "Iteration 1285, loss = 0.20393201\n",
      "Iteration 334, loss = 0.32259261\n",
      "Iteration 167, loss = 0.37903345\n",
      "Iteration 2527, loss = 0.13056608\n",
      "Iteration 382, loss = 0.40493828\n",
      "Iteration 168, loss = 0.37851224\n",
      "Iteration 2528, loss = 0.13049824\n",
      "Iteration 335, loss = 0.32247560\n",
      "Iteration 1280, loss = 0.27712008\n",
      "Iteration 2338, loss = 0.17014773\n",
      "Iteration 169, loss = 0.37798263\n",
      "Iteration 2529, loss = 0.13046229\n",
      "Iteration 1286, loss = 0.20379656\n",
      "Iteration 170, loss = 0.37743636\n",
      "Iteration 336, loss = 0.32234638\n",
      "Iteration 1281, loss = 0.27696118\n",
      "Iteration 1287, loss = 0.20357118\n",
      "Iteration 1245, loss = 0.24874168\n",
      "Iteration 383, loss = 0.40476741\n",
      "Iteration 337, loss = 0.32221380\n",
      "Iteration 171, loss = 0.37693446\n",
      "Iteration 2530, loss = 0.13047084\n",
      "Iteration 338, loss = 0.32208469\n",
      "Iteration 172, loss = 0.37634808\n",
      "Iteration 2531, loss = 0.13022654\n",
      "Iteration 1288, loss = 0.20317352\n",
      "Iteration 2532, loss = 0.13016226\n",
      "Iteration 173, loss = 0.37582174\n",
      "Iteration 2339, loss = 0.16993711\n",
      "Iteration 174, loss = 0.37535226\n",
      "Iteration 339, loss = 0.32194279\n",
      "Iteration 1289, loss = 0.20309732\n",
      "Iteration 2533, loss = 0.13010608\n",
      "Iteration 384, loss = 0.40463912\n",
      "Iteration 340, loss = 0.32182626\n",
      "Iteration 1282, loss = 0.27683597\n",
      "Iteration 175, loss = 0.37482648\n",
      "Iteration 1246, loss = 0.24859195\n",
      "Iteration 2534, loss = 0.13008914\n",
      "Iteration 176, loss = 0.37433131\n",
      "Iteration 1290, loss = 0.20279419\n",
      "Iteration 177, loss = 0.37385666\n",
      "Iteration 341, loss = 0.32169562\n",
      "Iteration 342, loss = 0.32156934\n",
      "Iteration 385, loss = 0.40443610\n",
      "Iteration 2340, loss = 0.16990208\n",
      "Iteration 178, loss = 0.37339421\n",
      "Iteration 1291, loss = 0.20274940\n",
      "Iteration 179, loss = 0.37288536\n",
      "Iteration 343, loss = 0.32145634\n",
      "Iteration 1247, loss = 0.24837524\n",
      "Iteration 180, loss = 0.37235749\n",
      "Iteration 2341, loss = 0.16996002\n",
      "Iteration 1283, loss = 0.27672686\n",
      "Iteration 1292, loss = 0.20273333\n",
      "Iteration 2535, loss = 0.12995671\n",
      "Iteration 344, loss = 0.32132999\n",
      "Iteration 181, loss = 0.37192334\n",
      "Iteration 345, loss = 0.32120530\n",
      "Iteration 182, loss = 0.37143020\n",
      "Iteration 386, loss = 0.40433183\n",
      "Iteration 2342, loss = 0.16973952\n",
      "Iteration 346, loss = 0.32107073\n",
      "Iteration 2536, loss = 0.12991738\n",
      "Iteration 1293, loss = 0.20215983\n",
      "Iteration 183, loss = 0.37094018\n",
      "Iteration 347, loss = 0.32095782\n",
      "Iteration 1248, loss = 0.24838790\n",
      "Iteration 184, loss = 0.37048670\n",
      "Iteration 2537, loss = 0.12983131\n",
      "Iteration 348, loss = 0.32082125\n",
      "Iteration 1294, loss = 0.20198077\n",
      "Iteration 185, loss = 0.37006088\n",
      "Iteration 2538, loss = 0.12974601\n",
      "Iteration 1284, loss = 0.27660734\n",
      "Iteration 186, loss = 0.36958257\n",
      "Iteration 2343, loss = 0.16971604\n",
      "Iteration 1249, loss = 0.24802614\n",
      "Iteration 2539, loss = 0.12969212\n",
      "Iteration 349, loss = 0.32070402\n",
      "Iteration 187, loss = 0.36911569\n",
      "Iteration 2540, loss = 0.12959868\n",
      "Iteration 188, loss = 0.36867447\n",
      "Iteration 1295, loss = 0.20180822\n",
      "Iteration 350, loss = 0.32059788\n",
      "Iteration 1285, loss = 0.27648373\n",
      "Iteration 189, loss = 0.36824462\n",
      "Iteration 2541, loss = 0.12950424\n",
      "Iteration 387, loss = 0.40410623\n",
      "Iteration 2344, loss = 0.16956486\n",
      "Iteration 1296, loss = 0.20152746\n",
      "Iteration 190, loss = 0.36778372\n",
      "Iteration 2542, loss = 0.12948644\n",
      "Iteration 1250, loss = 0.24785419\n",
      "Iteration 351, loss = 0.32048860\n",
      "Iteration 388, loss = 0.40396243\n",
      "Iteration 191, loss = 0.36733680\n",
      "Iteration 2345, loss = 0.16950381\n",
      "Iteration 2543, loss = 0.12954994\n",
      "Iteration 192, loss = 0.36695301\n",
      "Iteration 1286, loss = 0.27637444\n",
      "Iteration 1251, loss = 0.24783912\n",
      "Iteration 1297, loss = 0.20140735\n",
      "Iteration 193, loss = 0.36648354\n",
      "Iteration 2544, loss = 0.12933921\n",
      "Iteration 352, loss = 0.32034442\n",
      "Iteration 194, loss = 0.36605055\n",
      "Iteration 2545, loss = 0.12925047\n",
      "Iteration 389, loss = 0.40380342\n",
      "Iteration 2346, loss = 0.16945269\n",
      "Iteration 195, loss = 0.36563676\n",
      "Iteration 1252, loss = 0.24749663\n",
      "Iteration 196, loss = 0.36524231\n",
      "Iteration 1298, loss = 0.20110090\n",
      "Iteration 2546, loss = 0.12913563\n",
      "Iteration 353, loss = 0.32022383\n",
      "Iteration 197, loss = 0.36480538\n",
      "Iteration 1287, loss = 0.27624428\n",
      "Iteration 2547, loss = 0.12905798\n",
      "Iteration 1299, loss = 0.20095935\n",
      "Iteration 390, loss = 0.40362358\n",
      "Iteration 198, loss = 0.36442241\n",
      "Iteration 2548, loss = 0.12899578\n",
      "Iteration 354, loss = 0.32011664\n",
      "Iteration 199, loss = 0.36396138\n",
      "Iteration 1300, loss = 0.20073318\n",
      "Iteration 200, loss = 0.36358098\n",
      "Iteration 2347, loss = 0.16933545\n",
      "Iteration 2549, loss = 0.12892438\n",
      "Iteration 201, loss = 0.36319270\n",
      "Iteration 391, loss = 0.40346100\n",
      "Iteration 355, loss = 0.31998658\n",
      "Iteration 202, loss = 0.36278432\n",
      "Iteration 1288, loss = 0.27613308\n",
      "Iteration 1253, loss = 0.24742021\n",
      "Iteration 2348, loss = 0.16923706\n",
      "Iteration 203, loss = 0.36243105\n",
      "Iteration 1301, loss = 0.20055413\n",
      "Iteration 2550, loss = 0.12885813\n",
      "Iteration 204, loss = 0.36201604\n",
      "Iteration 205, loss = 0.36166034\n",
      "Iteration 2349, loss = 0.16917161\n",
      "Iteration 1302, loss = 0.20026859\n",
      "Iteration 2551, loss = 0.12877365\n",
      "Iteration 1289, loss = 0.27607301\n",
      "Iteration 206, loss = 0.36125589\n",
      "Iteration 392, loss = 0.40332447\n",
      "Iteration 1254, loss = 0.24715664\n",
      "Iteration 207, loss = 0.36086293\n",
      "Iteration 2350, loss = 0.16906594\n",
      "Iteration 208, loss = 0.36047562\n",
      "Iteration 1290, loss = 0.27592865\n",
      "Iteration 1303, loss = 0.19997713\n",
      "Iteration 2552, loss = 0.12869326\n",
      "Iteration 209, loss = 0.36009335\n",
      "Iteration 210, loss = 0.35973794\n",
      "Iteration 1255, loss = 0.24696248\n",
      "Iteration 2351, loss = 0.16897272\n",
      "Iteration 211, loss = 0.35935226\n",
      "Iteration 1304, loss = 0.19979094\n",
      "Iteration 212, loss = 0.35896633\n",
      "Iteration 213, loss = 0.35863129\n",
      "Iteration 356, loss = 0.31988751\n",
      "Iteration 2553, loss = 0.12872219Iteration 393, loss = 0.40318735\n",
      "\n",
      "Iteration 214, loss = 0.35825292\n",
      "Iteration 215, loss = 0.35786608\n",
      "Iteration 1305, loss = 0.19986291\n",
      "Iteration 216, loss = 0.35750359\n",
      "Iteration 217, loss = 0.35715205\n",
      "Iteration 1291, loss = 0.27576919\n",
      "Iteration 218, loss = 0.35678515\n",
      "Iteration 2352, loss = 0.16889339\n",
      "Iteration 219, loss = 0.35648063\n",
      "Iteration 2554, loss = 0.12855251\n",
      "Iteration 357, loss = 0.31974697\n",
      "Iteration 2555, loss = 0.12858661\n",
      "Iteration 358, loss = 0.31962913\n",
      "Iteration 220, loss = 0.35611568\n",
      "Iteration 1256, loss = 0.24695850\n",
      "Iteration 221, loss = 0.35572010\n",
      "Iteration 1306, loss = 0.19940881\n",
      "Iteration 222, loss = 0.35537542\n",
      "Iteration 1292, loss = 0.27566098\n",
      "Iteration 2556, loss = 0.12839395\n",
      "Iteration 359, loss = 0.31950064\n",
      "Iteration 223, loss = 0.35505205\n",
      "Iteration 224, loss = 0.35467433\n",
      "Iteration 2557, loss = 0.12840726\n",
      "Iteration 360, loss = 0.31939047\n",
      "Iteration 225, loss = 0.35434496\n",
      "Iteration 394, loss = 0.40296886\n",
      "Iteration 226, loss = 0.35401686\n",
      "Iteration 1293, loss = 0.27560295\n",
      "Iteration 2353, loss = 0.16881182\n",
      "Iteration 2558, loss = 0.12828348\n",
      "Iteration 227, loss = 0.35370020\n",
      "Iteration 1307, loss = 0.19913719\n",
      "Iteration 228, loss = 0.35337024\n",
      "Iteration 361, loss = 0.31927366\n",
      "Iteration 2559, loss = 0.12820897\n",
      "Iteration 1294, loss = 0.27541235\n",
      "Iteration 2354, loss = 0.16876215\n",
      "Iteration 362, loss = 0.31916364\n",
      "Iteration 1257, loss = 0.24670657\n",
      "Iteration 229, loss = 0.35301608\n",
      "Iteration 363, loss = 0.31905213\n",
      "Iteration 1308, loss = 0.19898358\n",
      "Iteration 2560, loss = 0.12827048\n",
      "Iteration 364, loss = 0.31893926\n",
      "Iteration 230, loss = 0.35269220\n",
      "Iteration 1295, loss = 0.27529652\n",
      "Iteration 2355, loss = 0.16868967\n",
      "Iteration 2561, loss = 0.12832555\n",
      "Iteration 231, loss = 0.35237454\n",
      "Iteration 1258, loss = 0.24641873\n",
      "Iteration 2562, loss = 0.12792031\n",
      "Iteration 232, loss = 0.35205121\n",
      "Iteration 1296, loss = 0.27519154\n",
      "Iteration 365, loss = 0.31880628\n",
      "Iteration 2356, loss = 0.16859322\n",
      "Iteration 233, loss = 0.35171781\n",
      "Iteration 2563, loss = 0.12790366\n",
      "Iteration 1309, loss = 0.19878787\n",
      "Iteration 395, loss = 0.40285607\n",
      "Iteration 234, loss = 0.35139177\n",
      "Iteration 366, loss = 0.31869858\n",
      "Iteration 235, loss = 0.35109924\n",
      "Iteration 1297, loss = 0.27503873\n",
      "Iteration 2357, loss = 0.16868809\n",
      "Iteration 236, loss = 0.35080905\n",
      "Iteration 1310, loss = 0.19865441\n",
      "Iteration 237, loss = 0.35042924\n",
      "Iteration 2564, loss = 0.12787731\n",
      "Iteration 238, loss = 0.35012210\n",
      "Iteration 367, loss = 0.31858639\n",
      "Iteration 239, loss = 0.34983174\n",
      "Iteration 240, loss = 0.34949613\n",
      "Iteration 1311, loss = 0.19870581\n",
      "Iteration 241, loss = 0.34920361\n",
      "Iteration 368, loss = 0.31847019\n",
      "Iteration 242, loss = 0.34886623\n",
      "Iteration 1298, loss = 0.27494162\n",
      "Iteration 2358, loss = 0.16844747\n",
      "Iteration 369, loss = 0.31834626\n",
      "Iteration 396, loss = 0.40271761\n",
      "Iteration 1259, loss = 0.24626960\n",
      "Iteration 2565, loss = 0.12783276\n",
      "Iteration 243, loss = 0.34857110\n",
      "Iteration 1312, loss = 0.19817382\n",
      "Iteration 244, loss = 0.34825619\n",
      "Iteration 370, loss = 0.31825648\n",
      "Iteration 371, loss = 0.31813097\n",
      "Iteration 2566, loss = 0.12767603\n",
      "Iteration 245, loss = 0.34798315\n",
      "Iteration 1299, loss = 0.27483527\n",
      "Iteration 2359, loss = 0.16836153\n",
      "Iteration 2567, loss = 0.12759458\n",
      "Iteration 397, loss = 0.40247745\n",
      "Iteration 1313, loss = 0.19790855\n",
      "Iteration 246, loss = 0.34765616\n",
      "Iteration 1260, loss = 0.24628479\n",
      "Iteration 247, loss = 0.34738274\n",
      "Iteration 248, loss = 0.34705960\n",
      "Iteration 372, loss = 0.31802387\n",
      "Iteration 249, loss = 0.34676435\n",
      "Iteration 250, loss = 0.34648611\n",
      "Iteration 398, loss = 0.40233584\n",
      "Iteration 2360, loss = 0.16825183\n",
      "Iteration 251, loss = 0.34618831\n",
      "Iteration 252, loss = 0.34590273\n",
      "Iteration 2568, loss = 0.12767216\n",
      "Iteration 373, loss = 0.31790872\n",
      "Iteration 1314, loss = 0.19775122\n",
      "Iteration 253, loss = 0.34561758\n",
      "Iteration 254, loss = 0.34534992\n",
      "Iteration 1300, loss = 0.27474481\n",
      "Iteration 2569, loss = 0.12750065\n",
      "Iteration 255, loss = 0.34504818\n",
      "Iteration 2570, loss = 0.12739067\n",
      "Iteration 1315, loss = 0.19758476\n",
      "Iteration 1261, loss = 0.24586503\n",
      "Iteration 256, loss = 0.34477652\n",
      "Iteration 1301, loss = 0.27460034\n",
      "Iteration 257, loss = 0.34448827\n",
      "Iteration 2571, loss = 0.12736260\n",
      "Iteration 399, loss = 0.40218360\n",
      "Iteration 258, loss = 0.34425108\n",
      "Iteration 374, loss = 0.31781332\n",
      "Iteration 259, loss = 0.34395486\n",
      "Iteration 2572, loss = 0.12730222\n",
      "Iteration 1316, loss = 0.19734836\n",
      "Iteration 260, loss = 0.34366988\n",
      "Iteration 2361, loss = 0.16831865\n",
      "Iteration 261, loss = 0.34339860\n",
      "Iteration 375, loss = 0.31769200\n",
      "Iteration 2573, loss = 0.12726261\n",
      "Iteration 376, loss = 0.31756485\n",
      "Iteration 1262, loss = 0.24575831\n",
      "Iteration 262, loss = 0.34312809\n",
      "Iteration 400, loss = 0.40205010\n",
      "Iteration 377, loss = 0.31746738\n",
      "Iteration 263, loss = 0.34283592\n",
      "Iteration 1317, loss = 0.19723683\n",
      "Iteration 264, loss = 0.34258542\n",
      "Iteration 378, loss = 0.31734888\n",
      "Iteration 1302, loss = 0.27446476\n",
      "Iteration 265, loss = 0.34230876\n",
      "Iteration 1263, loss = 0.24570340\n",
      "Iteration 266, loss = 0.34206552\n",
      "Iteration 1318, loss = 0.19685345\n",
      "Iteration 379, loss = 0.31723945\n",
      "Iteration 267, loss = 0.34179075\n",
      "Iteration 268, loss = 0.34152996\n",
      "Iteration 2574, loss = 0.12711245\n",
      "Iteration 1303, loss = 0.27434359\n",
      "Iteration 269, loss = 0.34128768\n",
      "Iteration 401, loss = 0.40185802\n",
      "Iteration 380, loss = 0.31714334\n",
      "Iteration 270, loss = 0.34098961\n",
      "Iteration 271, loss = 0.34076157\n",
      "Iteration 1319, loss = 0.19665517\n",
      "Iteration 1264, loss = 0.24545645\n",
      "Iteration 381, loss = 0.31703361\n",
      "Iteration 2575, loss = 0.12731514\n",
      "Iteration 2362, loss = 0.16815699\n",
      "Iteration 272, loss = 0.34050470\n",
      "Iteration 402, loss = 0.40168518\n",
      "Iteration 382, loss = 0.31692883\n",
      "Iteration 273, loss = 0.34023346\n",
      "Iteration 274, loss = 0.33998041\n",
      "Iteration 1320, loss = 0.19650965\n",
      "Iteration 275, loss = 0.33976471\n",
      "Iteration 276, loss = 0.33949869\n",
      "Iteration 383, loss = 0.31680035\n",
      "Iteration 2363, loss = 0.16814752\n",
      "Iteration 1304, loss = 0.27422307\n",
      "Iteration 2576, loss = 0.12699580\n",
      "Iteration 384, loss = 0.31670778\n",
      "Iteration 277, loss = 0.33924689\n",
      "Iteration 1265, loss = 0.24522357\n",
      "Iteration 2577, loss = 0.12691700\n",
      "Iteration 278, loss = 0.33900674\n",
      "Iteration 1305, loss = 0.27413358\n",
      "Iteration 2578, loss = 0.12684779\n",
      "Iteration 2364, loss = 0.16807526\n",
      "Iteration 385, loss = 0.31659444\n",
      "Iteration 403, loss = 0.40158343\n",
      "Iteration 1321, loss = 0.19647542\n",
      "Iteration 1306, loss = 0.27398116\n",
      "Iteration 2579, loss = 0.12674384\n",
      "Iteration 386, loss = 0.31649061\n",
      "Iteration 279, loss = 0.33876838\n",
      "Iteration 1266, loss = 0.24499332\n",
      "Iteration 404, loss = 0.40136257\n",
      "Iteration 280, loss = 0.33852170\n",
      "Iteration 2580, loss = 0.12671147\n",
      "Iteration 2365, loss = 0.16839032\n",
      "Iteration 387, loss = 0.31638063\n",
      "Iteration 281, loss = 0.33828763\n",
      "Iteration 1322, loss = 0.19598444\n",
      "Iteration 282, loss = 0.33807761\n",
      "Iteration 283, loss = 0.33778189\n",
      "Iteration 2366, loss = 0.16781669\n",
      "Iteration 2581, loss = 0.12660972\n",
      "Iteration 1307, loss = 0.27387431\n",
      "Iteration 284, loss = 0.33754207\n",
      "Iteration 388, loss = 0.31628668\n",
      "Iteration 2582, loss = 0.12652363\n",
      "Iteration 405, loss = 0.40119101\n",
      "Iteration 285, loss = 0.33734118\n",
      "Iteration 2367, loss = 0.16782684\n",
      "Iteration 2583, loss = 0.12644175\n",
      "Iteration 389, loss = 0.31616738\n",
      "Iteration 1323, loss = 0.19620664\n",
      "Iteration 1267, loss = 0.24481455\n",
      "Iteration 286, loss = 0.33708586\n",
      "Iteration 2584, loss = 0.12639743\n",
      "Iteration 287, loss = 0.33684169\n",
      "Iteration 390, loss = 0.31607239\n",
      "Iteration 288, loss = 0.33665378\n",
      "Iteration 2585, loss = 0.12632377\n",
      "Iteration 391, loss = 0.31597512\n",
      "Iteration 2368, loss = 0.16771508\n",
      "Iteration 1308, loss = 0.27384603\n",
      "Iteration 289, loss = 0.33639622\n",
      "Iteration 2586, loss = 0.12637738\n",
      "Iteration 406, loss = 0.40102143\n",
      "Iteration 1324, loss = 0.19555924\n",
      "Iteration 392, loss = 0.31586079\n",
      "Iteration 290, loss = 0.33621997\n",
      "Iteration 393, loss = 0.31576079\n",
      "Iteration 291, loss = 0.33592888\n",
      "Iteration 1325, loss = 0.19534927\n",
      "Iteration 2587, loss = 0.12618526\n",
      "Iteration 292, loss = 0.33571660\n",
      "Iteration 1309, loss = 0.27367850\n",
      "Iteration 1268, loss = 0.24475840\n",
      "Iteration 293, loss = 0.33551974\n",
      "Iteration 294, loss = 0.33533128\n",
      "Iteration 394, loss = 0.31568086\n",
      "Iteration 295, loss = 0.33505106\n",
      "Iteration 296, loss = 0.33482894\n",
      "Iteration 407, loss = 0.40090519\n",
      "Iteration 297, loss = 0.33461011\n",
      "Iteration 1310, loss = 0.27353943\n",
      "Iteration 2588, loss = 0.12610169\n",
      "Iteration 1269, loss = 0.24445042\n",
      "Iteration 2369, loss = 0.16762092\n",
      "Iteration 395, loss = 0.31556110\n",
      "Iteration 2589, loss = 0.12607158\n",
      "Iteration 1326, loss = 0.19537148\n",
      "Iteration 298, loss = 0.33440275\n",
      "Iteration 299, loss = 0.33417480\n",
      "Iteration 396, loss = 0.31545186\n",
      "Iteration 300, loss = 0.33395527\n",
      "Iteration 301, loss = 0.33374626\n",
      "Iteration 397, loss = 0.31536357\n",
      "Iteration 2370, loss = 0.16752302\n",
      "Iteration 302, loss = 0.33354082\n",
      "Iteration 2590, loss = 0.12596834\n",
      "Iteration 1311, loss = 0.27338155\n",
      "Iteration 1327, loss = 0.19499919\n",
      "Iteration 398, loss = 0.31526522\n",
      "Iteration 303, loss = 0.33330055\n",
      "Iteration 1270, loss = 0.24432488\n",
      "Iteration 2371, loss = 0.16742403\n",
      "Iteration 2591, loss = 0.12589484\n",
      "Iteration 304, loss = 0.33310482\n",
      "Iteration 1328, loss = 0.19476504\n",
      "Iteration 1312, loss = 0.27326276\n",
      "Iteration 2592, loss = 0.12582189\n",
      "Iteration 305, loss = 0.33290023\n",
      "Iteration 306, loss = 0.33269891\n",
      "Iteration 2372, loss = 0.16732157\n",
      "Iteration 307, loss = 0.33248137\n",
      "Iteration 399, loss = 0.31516239\n",
      "Iteration 1329, loss = 0.19459882\n",
      "Iteration 308, loss = 0.33227181\n",
      "Iteration 309, loss = 0.33205239\n",
      "Iteration 310, loss = 0.33188362\n",
      "Iteration 2593, loss = 0.12575398\n",
      "Iteration 1330, loss = 0.19435932\n",
      "Iteration 2373, loss = 0.16732470\n",
      "Iteration 311, loss = 0.33166365\n",
      "Iteration 400, loss = 0.31505964\n",
      "Iteration 312, loss = 0.33145698\n",
      "Iteration 408, loss = 0.40071862\n",
      "Iteration 1271, loss = 0.24419449\n",
      "Iteration 1313, loss = 0.27315403\n",
      "Iteration 2594, loss = 0.12571225\n",
      "Iteration 401, loss = 0.31494918\n",
      "Iteration 313, loss = 0.33124244\n",
      "Iteration 402, loss = 0.31486064\n",
      "Iteration 1331, loss = 0.19429166\n",
      "Iteration 314, loss = 0.33104824\n",
      "Iteration 2595, loss = 0.12561298\n",
      "Iteration 2374, loss = 0.16718809\n",
      "Iteration 403, loss = 0.31478958\n",
      "Iteration 315, loss = 0.33085307\n",
      "Iteration 316, loss = 0.33069413\n",
      "Iteration 1332, loss = 0.19402795\n",
      "Iteration 404, loss = 0.31466307\n",
      "Iteration 317, loss = 0.33048451\n",
      "Iteration 2596, loss = 0.12556326\n",
      "Iteration 1314, loss = 0.27302308\n",
      "Iteration 318, loss = 0.33026203\n",
      "Iteration 319, loss = 0.33006408\n",
      "Iteration 2375, loss = 0.16706932\n",
      "Iteration 1333, loss = 0.19402589\n",
      "Iteration 320, loss = 0.32987461\n",
      "Iteration 2597, loss = 0.12548326\n",
      "Iteration 405, loss = 0.31457076\n",
      "Iteration 2598, loss = 0.12538194\n",
      "Iteration 406, loss = 0.31445904\n",
      "Iteration 2376, loss = 0.16711727\n",
      "Iteration 321, loss = 0.32967772\n",
      "Iteration 1334, loss = 0.19387656\n",
      "Iteration 407, loss = 0.31437259\n",
      "Iteration 1272, loss = 0.24406532\n",
      "Iteration 1315, loss = 0.27292411\n",
      "Iteration 409, loss = 0.40054095\n",
      "Iteration 322, loss = 0.32950689\n",
      "Iteration 2599, loss = 0.12538331\n",
      "Iteration 1335, loss = 0.19342532\n",
      "Iteration 323, loss = 0.32927503\n",
      "Iteration 324, loss = 0.32908838\n",
      "Iteration 2377, loss = 0.16712812\n",
      "Iteration 408, loss = 0.31425573\n",
      "Iteration 325, loss = 0.32890823\n",
      "Iteration 2600, loss = 0.12526164\n",
      "Iteration 409, loss = 0.31416157\n",
      "Iteration 326, loss = 0.32870674\n",
      "Iteration 2601, loss = 0.12525284\n",
      "Iteration 2378, loss = 0.16698357\n",
      "Iteration 327, loss = 0.32850237\n",
      "Iteration 1316, loss = 0.27285444\n",
      "Iteration 410, loss = 0.31409518\n",
      "Iteration 2602, loss = 0.12508211\n",
      "Iteration 328, loss = 0.32833104\n",
      "Iteration 1336, loss = 0.19347376\n",
      "Iteration 411, loss = 0.31397254\n",
      "Iteration 1273, loss = 0.24374451\n",
      "Iteration 2603, loss = 0.12504609\n",
      "Iteration 329, loss = 0.32813633\n",
      "Iteration 1337, loss = 0.19301562\n",
      "Iteration 2604, loss = 0.12506594\n",
      "Iteration 410, loss = 0.40040790\n",
      "Iteration 330, loss = 0.32796432\n",
      "Iteration 412, loss = 0.31387837\n",
      "Iteration 1317, loss = 0.27275237\n",
      "Iteration 2605, loss = 0.12492035\n",
      "Iteration 331, loss = 0.32777437\n",
      "Iteration 332, loss = 0.32758219\n",
      "Iteration 1274, loss = 0.24359854\n",
      "Iteration 1338, loss = 0.19285475\n",
      "Iteration 413, loss = 0.31378018\n",
      "Iteration 2379, loss = 0.16675337\n",
      "Iteration 2606, loss = 0.12484913\n",
      "Iteration 333, loss = 0.32738820\n",
      "Iteration 1318, loss = 0.27257299\n",
      "Iteration 2607, loss = 0.12477710\n",
      "Iteration 334, loss = 0.32720364\n",
      "Iteration 414, loss = 0.31370116\n",
      "Iteration 335, loss = 0.32702796\n",
      "Iteration 1339, loss = 0.19256149\n",
      "Iteration 1275, loss = 0.24350716\n",
      "Iteration 2608, loss = 0.12476455\n",
      "Iteration 415, loss = 0.31358557\n",
      "Iteration 336, loss = 0.32684340\n",
      "Iteration 1340, loss = 0.19229464\n",
      "Iteration 416, loss = 0.31348998\n",
      "Iteration 1319, loss = 0.27249366\n",
      "Iteration 411, loss = 0.40022934\n",
      "Iteration 2380, loss = 0.16685367\n",
      "Iteration 337, loss = 0.32665761\n",
      "Iteration 1276, loss = 0.24320504\n",
      "Iteration 2609, loss = 0.12465485\n",
      "Iteration 417, loss = 0.31339634\n",
      "Iteration 338, loss = 0.32646963\n",
      "Iteration 1341, loss = 0.19216233\n",
      "Iteration 418, loss = 0.31331943\n",
      "Iteration 1320, loss = 0.27230184\n",
      "Iteration 339, loss = 0.32630717\n",
      "Iteration 419, loss = 0.31319875\n",
      "Iteration 2381, loss = 0.16670945\n",
      "Iteration 1342, loss = 0.19187566\n",
      "Iteration 340, loss = 0.32614424\n",
      "Iteration 2610, loss = 0.12456181\n",
      "Iteration 341, loss = 0.32594083\n",
      "Iteration 420, loss = 0.31311741\n",
      "Iteration 412, loss = 0.40005559\n",
      "Iteration 421, loss = 0.31300742\n",
      "Iteration 2611, loss = 0.12457615\n",
      "Iteration 342, loss = 0.32579365\n",
      "Iteration 1321, loss = 0.27220168\n",
      "Iteration 343, loss = 0.32559446\n",
      "Iteration 1343, loss = 0.19166828\n",
      "Iteration 2612, loss = 0.12461890\n",
      "Iteration 344, loss = 0.32539400\n",
      "Iteration 345, loss = 0.32524484\n",
      "Iteration 422, loss = 0.31292065\n",
      "Iteration 2382, loss = 0.16660123\n",
      "Iteration 346, loss = 0.32506728\n",
      "Iteration 347, loss = 0.32489285\n",
      "Iteration 1277, loss = 0.24311340\n",
      "Iteration 348, loss = 0.32472010\n",
      "Iteration 2613, loss = 0.12438035\n",
      "Iteration 413, loss = 0.39990937\n",
      "Iteration 1344, loss = 0.19148937\n",
      "Iteration 423, loss = 0.31283786\n",
      "Iteration 1322, loss = 0.27210947\n",
      "Iteration 349, loss = 0.32454567\n",
      "Iteration 424, loss = 0.31274263\n",
      "Iteration 350, loss = 0.32438730\n",
      "Iteration 2383, loss = 0.16652154\n",
      "Iteration 1345, loss = 0.19128414\n",
      "Iteration 2614, loss = 0.12433931\n",
      "Iteration 1278, loss = 0.24296209\n",
      "Iteration 425, loss = 0.31264356\n",
      "Iteration 351, loss = 0.32421525\n",
      "Iteration 2384, loss = 0.16653141\n",
      "Iteration 352, loss = 0.32402725\n",
      "Iteration 1346, loss = 0.19115239\n",
      "Iteration 426, loss = 0.31254309\n",
      "Iteration 414, loss = 0.39975516\n",
      "Iteration 2615, loss = 0.12430226\n",
      "Iteration 353, loss = 0.32388361\n",
      "Iteration 427, loss = 0.31245860\n",
      "Iteration 1323, loss = 0.27195170\n",
      "Iteration 428, loss = 0.31236710\n",
      "Iteration 354, loss = 0.32369259\n",
      "Iteration 2616, loss = 0.12418173\n",
      "Iteration 415, loss = 0.39970423\n",
      "Iteration 355, loss = 0.32356088\n",
      "Iteration 1347, loss = 0.19084110\n",
      "Iteration 429, loss = 0.31227321\n",
      "Iteration 356, loss = 0.32336392\n",
      "Iteration 1324, loss = 0.27191489\n",
      "Iteration 430, loss = 0.31218224\n",
      "Iteration 2617, loss = 0.12409582\n",
      "Iteration 1348, loss = 0.19068501\n",
      "Iteration 357, loss = 0.32321483\n",
      "Iteration 431, loss = 0.31209179\n",
      "Iteration 2618, loss = 0.12401938\n",
      "Iteration 358, loss = 0.32304700\n",
      "Iteration 2385, loss = 0.16641974\n",
      "Iteration 359, loss = 0.32288199\n",
      "Iteration 2619, loss = 0.12395138\n",
      "Iteration 1325, loss = 0.27172889\n",
      "Iteration 1279, loss = 0.24271085\n",
      "Iteration 432, loss = 0.31197208\n",
      "Iteration 360, loss = 0.32272304\n",
      "Iteration 361, loss = 0.32256688\n",
      "Iteration 416, loss = 0.39946794\n",
      "Iteration 1349, loss = 0.19045961\n",
      "Iteration 362, loss = 0.32243187\n",
      "Iteration 2620, loss = 0.12386999\n",
      "Iteration 363, loss = 0.32224833\n",
      "Iteration 433, loss = 0.31190031\n",
      "Iteration 364, loss = 0.32209083\n",
      "Iteration 365, loss = 0.32195494\n",
      "Iteration 1326, loss = 0.27160424\n",
      "Iteration 366, loss = 0.32180506\n",
      "Iteration 2621, loss = 0.12392128\n",
      "Iteration 367, loss = 0.32164519\n",
      "Iteration 2386, loss = 0.16626845\n",
      "Iteration 1350, loss = 0.19036665\n",
      "Iteration 434, loss = 0.31181018\n",
      "Iteration 368, loss = 0.32148528\n",
      "Iteration 1327, loss = 0.27149726\n",
      "Iteration 2622, loss = 0.12375267\n",
      "Iteration 1280, loss = 0.24257060\n",
      "Iteration 369, loss = 0.32132487\n",
      "Iteration 2387, loss = 0.16625205\n",
      "Iteration 435, loss = 0.31170815\n",
      "Iteration 417, loss = 0.39935454\n",
      "Iteration 2623, loss = 0.12367876\n",
      "Iteration 370, loss = 0.32117629\n",
      "Iteration 371, loss = 0.32104755\n",
      "Iteration 436, loss = 0.31162953\n",
      "Iteration 1328, loss = 0.27137896\n",
      "Iteration 2624, loss = 0.12373616\n",
      "Iteration 372, loss = 0.32088215\n",
      "Iteration 2388, loss = 0.16617747\n",
      "Iteration 437, loss = 0.31151879\n",
      "Iteration 418, loss = 0.39911186\n",
      "Iteration 1351, loss = 0.19012334\n",
      "Iteration 373, loss = 0.32073704\n",
      "Iteration 2389, loss = 0.16613518\n",
      "Iteration 374, loss = 0.32058023\n",
      "Iteration 1352, loss = 0.19013181\n",
      "Iteration 375, loss = 0.32045707Iteration 1329, loss = 0.27127428\n",
      "\n",
      "Iteration 2625, loss = 0.12356051\n",
      "Iteration 438, loss = 0.31142036\n",
      "Iteration 1281, loss = 0.24232795\n",
      "Iteration 1353, loss = 0.18974821\n",
      "Iteration 376, loss = 0.32032984\n",
      "Iteration 2390, loss = 0.16595530\n",
      "Iteration 377, loss = 0.32015248\n",
      "Iteration 439, loss = 0.31134701\n",
      "Iteration 378, loss = 0.32001309\n",
      "Iteration 379, loss = 0.31986921\n",
      "Iteration 1354, loss = 0.18955473\n",
      "Iteration 419, loss = 0.39901869\n",
      "Iteration 380, loss = 0.31970462\n",
      "Iteration 381, loss = 0.31957466\n",
      "Iteration 1282, loss = 0.24215253\n",
      "Iteration 382, loss = 0.31943140\n",
      "Iteration 2391, loss = 0.16583071\n",
      "Iteration 383, loss = 0.31933356\n",
      "Iteration 2626, loss = 0.12349164\n",
      "Iteration 384, loss = 0.31913130\n",
      "Iteration 1330, loss = 0.27121417\n",
      "Iteration 1355, loss = 0.18936364\n",
      "Iteration 1283, loss = 0.24190214\n",
      "Iteration 440, loss = 0.31125160\n",
      "Iteration 420, loss = 0.39878465\n",
      "Iteration 2627, loss = 0.12342840\n",
      "Iteration 441, loss = 0.31118629\n",
      "Iteration 385, loss = 0.31900853\n",
      "Iteration 442, loss = 0.31105185\n",
      "Iteration 386, loss = 0.31886522\n",
      "Iteration 387, loss = 0.31872500\n",
      "Iteration 2628, loss = 0.12331903\n",
      "Iteration 443, loss = 0.31096775\n",
      "Iteration 388, loss = 0.31856354\n",
      "Iteration 2392, loss = 0.16587195\n",
      "Iteration 389, loss = 0.31844567\n",
      "Iteration 1356, loss = 0.18911539\n",
      "Iteration 390, loss = 0.31828852\n",
      "Iteration 1331, loss = 0.27098861\n",
      "Iteration 1284, loss = 0.24180653\n",
      "Iteration 444, loss = 0.31088671\n",
      "Iteration 391, loss = 0.31814042\n",
      "Iteration 2629, loss = 0.12325214\n",
      "Iteration 392, loss = 0.31799403\n",
      "Iteration 1357, loss = 0.18903587\n",
      "Iteration 2630, loss = 0.12330256\n",
      "Iteration 445, loss = 0.31078668\n",
      "Iteration 393, loss = 0.31793588\n",
      "Iteration 421, loss = 0.39884846\n",
      "Iteration 394, loss = 0.31772397\n",
      "Iteration 2393, loss = 0.16569088\n",
      "Iteration 395, loss = 0.31761616\n",
      "Iteration 1332, loss = 0.27089006\n",
      "Iteration 446, loss = 0.31069630\n",
      "Iteration 396, loss = 0.31745321\n",
      "Iteration 2631, loss = 0.12318613\n",
      "Iteration 397, loss = 0.31730965\n",
      "Iteration 447, loss = 0.31060654\n",
      "Iteration 2632, loss = 0.12310140\n",
      "Iteration 1358, loss = 0.18865764\n",
      "Iteration 398, loss = 0.31718242\n",
      "Iteration 1333, loss = 0.27078876\n",
      "Iteration 1285, loss = 0.24161681\n",
      "Iteration 2633, loss = 0.12301326\n",
      "Iteration 422, loss = 0.39847191\n",
      "Iteration 1359, loss = 0.18843955\n",
      "Iteration 448, loss = 0.31051651\n",
      "Iteration 449, loss = 0.31043350\n",
      "Iteration 399, loss = 0.31703222\n",
      "Iteration 2634, loss = 0.12293702\n",
      "Iteration 1360, loss = 0.18833194\n",
      "Iteration 400, loss = 0.31691650\n",
      "Iteration 2635, loss = 0.12287203\n",
      "Iteration 1334, loss = 0.27063713\n",
      "Iteration 2394, loss = 0.16559662\n",
      "Iteration 423, loss = 0.39837144\n",
      "Iteration 450, loss = 0.31033732\n",
      "Iteration 2636, loss = 0.12280428\n",
      "Iteration 401, loss = 0.31675628\n",
      "Iteration 1361, loss = 0.18818494\n",
      "Iteration 402, loss = 0.31663367\n",
      "Iteration 451, loss = 0.31023999\n",
      "Iteration 403, loss = 0.31647119\n",
      "Iteration 2637, loss = 0.12271927\n",
      "Iteration 452, loss = 0.31016225\n",
      "Iteration 404, loss = 0.31636646\n",
      "Iteration 1335, loss = 0.27055783\n",
      "Iteration 1286, loss = 0.24142681\n",
      "Iteration 405, loss = 0.31621370\n",
      "Iteration 2638, loss = 0.12270380\n",
      "Iteration 453, loss = 0.31005890\n",
      "Iteration 454, loss = 0.30998897\n",
      "Iteration 2639, loss = 0.12267101\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 406, loss = 0.31608133\n",
      "Iteration 1362, loss = 0.18812607\n",
      "Iteration 455, loss = 0.30990848\n",
      "Iteration 407, loss = 0.31593699\n",
      "Iteration 456, loss = 0.30980485\n",
      "Iteration 2395, loss = 0.16553035\n",
      "Iteration 408, loss = 0.31582647\n",
      "Iteration 457, loss = 0.30971009\n",
      "Iteration 409, loss = 0.31577259\n",
      "Iteration 2396, loss = 0.16560016\n",
      "Iteration 424, loss = 0.39828599\n",
      "Iteration 1, loss = 0.71440917\n",
      "Iteration 1336, loss = 0.27040159\n",
      "Iteration 410, loss = 0.31554932\n",
      "Iteration 2, loss = 0.71288894\n",
      "Iteration 2397, loss = 0.16544928\n",
      "Iteration 458, loss = 0.30962376\n",
      "Iteration 1363, loss = 0.18770747\n",
      "Iteration 3, loss = 0.71049661\n",
      "Iteration 4, loss = 0.70748442\n",
      "Iteration 1364, loss = 0.18761478\n",
      "Iteration 411, loss = 0.31541182\n",
      "Iteration 425, loss = 0.39795675\n",
      "Iteration 412, loss = 0.31529326\n",
      "Iteration 413, loss = 0.31516360\n",
      "Iteration 1287, loss = 0.24125238\n",
      "Iteration 414, loss = 0.31500880\n",
      "Iteration 1365, loss = 0.18727274\n",
      "Iteration 415, loss = 0.31490300\n",
      "Iteration 1337, loss = 0.27026950\n",
      "Iteration 1366, loss = 0.18720002\n",
      "Iteration 426, loss = 0.39787627\n",
      "Iteration 459, loss = 0.30953129\n",
      "Iteration 1367, loss = 0.18693615\n",
      "Iteration 416, loss = 0.31474633\n",
      "Iteration 1368, loss = 0.18672306\n",
      "Iteration 1338, loss = 0.27015422\n",
      "Iteration 460, loss = 0.30944938\n",
      "Iteration 427, loss = 0.39766917\n",
      "Iteration 417, loss = 0.31464101\n",
      "Iteration 1369, loss = 0.18645550\n",
      "Iteration 5, loss = 0.70409064\n",
      "Iteration 2398, loss = 0.16534955\n",
      "Iteration 1339, loss = 0.27000132\n",
      "Iteration 418, loss = 0.31448207\n",
      "Iteration 461, loss = 0.30937044\n",
      "Iteration 419, loss = 0.31436755\n",
      "Iteration 1370, loss = 0.18625771\n",
      "Iteration 420, loss = 0.31422619\n",
      "Iteration 1288, loss = 0.24107536\n",
      "Iteration 421, loss = 0.31410978\n",
      "Iteration 6, loss = 0.70035221\n",
      "Iteration 422, loss = 0.31401793\n",
      "Iteration 1340, loss = 0.26990738\n",
      "Iteration 462, loss = 0.30927500\n",
      "Iteration 2399, loss = 0.16528535\n",
      "Iteration 7, loss = 0.69647225\n",
      "Iteration 1371, loss = 0.18604454\n",
      "Iteration 423, loss = 0.31385705\n",
      "Iteration 1341, loss = 0.26975958\n",
      "Iteration 424, loss = 0.31374142\n",
      "Iteration 8, loss = 0.69252584\n",
      "Iteration 1289, loss = 0.24124968\n",
      "Iteration 463, loss = 0.30919268\n",
      "Iteration 1372, loss = 0.18592832\n",
      "Iteration 2400, loss = 0.16515165\n",
      "Iteration 425, loss = 0.31360490\n",
      "Iteration 428, loss = 0.39751861\n",
      "Iteration 1342, loss = 0.26968111\n",
      "Iteration 426, loss = 0.31348657\n",
      "Iteration 427, loss = 0.31334973\n",
      "Iteration 464, loss = 0.30910068\n",
      "Iteration 9, loss = 0.68841150\n",
      "Iteration 428, loss = 0.31323012\n",
      "Iteration 1373, loss = 0.18593780\n",
      "Iteration 465, loss = 0.30901410\n",
      "Iteration 2401, loss = 0.16507272\n",
      "Iteration 10, loss = 0.68427835\n",
      "Iteration 429, loss = 0.31309461\n",
      "Iteration 1290, loss = 0.24066167\n",
      "Iteration 466, loss = 0.30892998\n",
      "Iteration 11, loss = 0.68020879\n",
      "Iteration 2402, loss = 0.16509140\n",
      "Iteration 430, loss = 0.31303571\n",
      "Iteration 467, loss = 0.30887389\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 12, loss = 0.67596800\n",
      "Iteration 431, loss = 0.31286629\n",
      "Iteration 429, loss = 0.39740924\n",
      "Iteration 13, loss = 0.67186083\n",
      "Iteration 2403, loss = 0.16495840\n",
      "Iteration 432, loss = 0.31276790\n",
      "Iteration 1291, loss = 0.24058871\n",
      "Iteration 1374, loss = 0.18546050\n",
      "Iteration 433, loss = 0.31261613\n",
      "Iteration 434, loss = 0.31250558\n",
      "Iteration 14, loss = 0.66768456\n",
      "Iteration 1343, loss = 0.26962516\n",
      "Iteration 1375, loss = 0.18544199\n",
      "Iteration 435, loss = 0.31240879\n",
      "Iteration 2404, loss = 0.16491741\n",
      "Iteration 436, loss = 0.31226277\n",
      "Iteration 437, loss = 0.31216151\n",
      "Iteration 1376, loss = 0.18516103\n",
      "Iteration 2405, loss = 0.16476919\n",
      "Iteration 1344, loss = 0.26942984\n",
      "Iteration 1, loss = 0.79047207\n",
      "Iteration 430, loss = 0.39717345\n",
      "Iteration 438, loss = 0.31202002\n",
      "Iteration 1377, loss = 0.18497704\n",
      "Iteration 439, loss = 0.31191376\n",
      "Iteration 1292, loss = 0.24037025\n",
      "Iteration 440, loss = 0.31181284\n",
      "Iteration 441, loss = 0.31168047\n",
      "Iteration 15, loss = 0.66354278\n",
      "Iteration 2406, loss = 0.16473305\n",
      "Iteration 2, loss = 0.78849883\n",
      "Iteration 442, loss = 0.31156691\n",
      "Iteration 1345, loss = 0.26931047\n",
      "Iteration 443, loss = 0.31143271\n",
      "Iteration 1378, loss = 0.18470745\n",
      "Iteration 444, loss = 0.31131299\n",
      "Iteration 1346, loss = 0.26921531\n",
      "Iteration 2407, loss = 0.16460805\n",
      "Iteration 3, loss = 0.78566131\n",
      "Iteration 16, loss = 0.65951970\n",
      "Iteration 445, loss = 0.31119827\n",
      "Iteration 431, loss = 0.39701332\n",
      "Iteration 1379, loss = 0.18451046\n",
      "Iteration 1293, loss = 0.24024498\n",
      "Iteration 17, loss = 0.65530757\n",
      "Iteration 446, loss = 0.31109402\n",
      "Iteration 1347, loss = 0.26906541\n",
      "Iteration 447, loss = 0.31097204\n",
      "Iteration 448, loss = 0.31085885\n",
      "Iteration 449, loss = 0.31074785\n",
      "Iteration 1380, loss = 0.18429425\n",
      "Iteration 450, loss = 0.31061586\n",
      "Iteration 18, loss = 0.65119970\n",
      "Iteration 432, loss = 0.39690227\n",
      "Iteration 4, loss = 0.78201390\n",
      "Iteration 1348, loss = 0.26899310\n",
      "Iteration 2408, loss = 0.16455900\n",
      "Iteration 19, loss = 0.64717491\n",
      "Iteration 451, loss = 0.31050847\n",
      "Iteration 1381, loss = 0.18405740\n",
      "Iteration 452, loss = 0.31039972\n",
      "Iteration 5, loss = 0.77815569\n",
      "Iteration 1294, loss = 0.23996953\n",
      "Iteration 453, loss = 0.31026565\n",
      "Iteration 20, loss = 0.64315789\n",
      "Iteration 6, loss = 0.77369032\n",
      "Iteration 1382, loss = 0.18406305\n",
      "Iteration 454, loss = 0.31016562\n",
      "Iteration 433, loss = 0.39668988\n",
      "Iteration 21, loss = 0.63910961\n",
      "Iteration 455, loss = 0.31006754\n",
      "Iteration 2409, loss = 0.16449372\n",
      "Iteration 1349, loss = 0.26880292\n",
      "Iteration 1383, loss = 0.18374141\n",
      "Iteration 456, loss = 0.30996899\n",
      "Iteration 22, loss = 0.63514160\n",
      "Iteration 457, loss = 0.30983889\n",
      "Iteration 458, loss = 0.30970230\n",
      "Iteration 1350, loss = 0.26871892\n",
      "Iteration 459, loss = 0.30959960\n",
      "Iteration 7, loss = 0.76940863\n",
      "Iteration 460, loss = 0.30949783\n",
      "Iteration 1384, loss = 0.18367034\n",
      "Iteration 23, loss = 0.63115058\n",
      "Iteration 461, loss = 0.30940026\n",
      "Iteration 1295, loss = 0.23978695\n",
      "Iteration 24, loss = 0.62717529\n",
      "Iteration 462, loss = 0.30926636\n",
      "Iteration 1385, loss = 0.18335918\n",
      "Iteration 463, loss = 0.30916569\n",
      "Iteration 2410, loss = 0.16437303\n",
      "Iteration 1351, loss = 0.26861869\n",
      "Iteration 25, loss = 0.62326562\n",
      "Iteration 464, loss = 0.30904686\n",
      "Iteration 465, loss = 0.30892934\n",
      "Iteration 1386, loss = 0.18319668\n",
      "Iteration 466, loss = 0.30880904\n",
      "Iteration 434, loss = 0.39654836\n",
      "Iteration 26, loss = 0.61930931\n",
      "Iteration 8, loss = 0.76520252\n",
      "Iteration 467, loss = 0.30870362\n",
      "Iteration 468, loss = 0.30859118\n",
      "Iteration 1387, loss = 0.18298918\n",
      "Iteration 469, loss = 0.30849617\n",
      "Iteration 2411, loss = 0.16430600\n",
      "Iteration 1352, loss = 0.26844081\n",
      "Iteration 470, loss = 0.30840197\n",
      "Iteration 435, loss = 0.39639976\n",
      "Iteration 27, loss = 0.61542883\n",
      "Iteration 1296, loss = 0.23976190\n",
      "Iteration 471, loss = 0.30826956\n",
      "Iteration 1353, loss = 0.26835328\n",
      "Iteration 28, loss = 0.61152761\n",
      "Iteration 1388, loss = 0.18274847\n",
      "Iteration 472, loss = 0.30814446\n",
      "Iteration 9, loss = 0.76058468\n",
      "Iteration 436, loss = 0.39630300\n",
      "Iteration 2412, loss = 0.16427440\n",
      "Iteration 473, loss = 0.30803580\n",
      "Iteration 29, loss = 0.60764678\n",
      "Iteration 1389, loss = 0.18283273\n",
      "Iteration 474, loss = 0.30793186\n",
      "Iteration 1354, loss = 0.26820582\n",
      "Iteration 475, loss = 0.30781991\n",
      "Iteration 476, loss = 0.30772376\n",
      "Iteration 10, loss = 0.75625820\n",
      "Iteration 30, loss = 0.60381609\n",
      "Iteration 1297, loss = 0.23954192\n",
      "Iteration 477, loss = 0.30760350\n",
      "Iteration 437, loss = 0.39604414\n",
      "Iteration 31, loss = 0.59999895\n",
      "Iteration 478, loss = 0.30751763\n",
      "Iteration 2413, loss = 0.16428253\n",
      "Iteration 1390, loss = 0.18266864\n",
      "Iteration 479, loss = 0.30740758\n",
      "Iteration 1355, loss = 0.26806494\n",
      "Iteration 1391, loss = 0.18229299\n",
      "Iteration 480, loss = 0.30732573\n",
      "Iteration 481, loss = 0.30716169\n",
      "Iteration 32, loss = 0.59619850\n",
      "Iteration 2414, loss = 0.16413152\n",
      "Iteration 482, loss = 0.30705132\n",
      "Iteration 11, loss = 0.75253801\n",
      "Iteration 1298, loss = 0.23938026\n",
      "Iteration 1392, loss = 0.18192998\n",
      "Iteration 2415, loss = 0.16414153\n",
      "Iteration 33, loss = 0.59237280\n",
      "Iteration 1356, loss = 0.26798224\n",
      "Iteration 483, loss = 0.30698274\n",
      "Iteration 438, loss = 0.39595280\n",
      "Iteration 1393, loss = 0.18193643\n",
      "Iteration 484, loss = 0.30684426\n",
      "Iteration 34, loss = 0.58869811\n",
      "Iteration 1299, loss = 0.23917765\n",
      "Iteration 1394, loss = 0.18167334\n",
      "Iteration 485, loss = 0.30672277\n",
      "Iteration 12, loss = 0.74798778\n",
      "Iteration 35, loss = 0.58487781\n",
      "Iteration 1395, loss = 0.18162016\n",
      "Iteration 1357, loss = 0.26787197\n",
      "Iteration 486, loss = 0.30661940\n",
      "Iteration 2416, loss = 0.16393679\n",
      "Iteration 1300, loss = 0.23906973\n",
      "Iteration 487, loss = 0.30652387\n",
      "Iteration 1396, loss = 0.18133077\n",
      "Iteration 439, loss = 0.39574226\n",
      "Iteration 36, loss = 0.58131043\n",
      "Iteration 1358, loss = 0.26770187\n",
      "Iteration 1397, loss = 0.18108406\n",
      "Iteration 13, loss = 0.74396231\n",
      "Iteration 488, loss = 0.30645670\n",
      "Iteration 489, loss = 0.30631374\n",
      "Iteration 490, loss = 0.30622489\n",
      "Iteration 37, loss = 0.57753698\n",
      "Iteration 491, loss = 0.30613022\n",
      "Iteration 492, loss = 0.30599755\n",
      "Iteration 38, loss = 0.57402355\n",
      "Iteration 493, loss = 0.30588252\n",
      "Iteration 2417, loss = 0.16398479\n",
      "Iteration 494, loss = 0.30580120\n",
      "Iteration 495, loss = 0.30566264\n",
      "Iteration 496, loss = 0.30557614\n",
      "Iteration 39, loss = 0.57031641\n",
      "Iteration 1301, loss = 0.23889287\n",
      "Iteration 497, loss = 0.30546304\n",
      "Iteration 14, loss = 0.73997565\n",
      "Iteration 2418, loss = 0.16390397\n",
      "Iteration 40, loss = 0.56673974\n",
      "Iteration 1359, loss = 0.26763103\n",
      "Iteration 1398, loss = 0.18099762\n",
      "Iteration 41, loss = 0.56314786\n",
      "Iteration 2419, loss = 0.16376488\n",
      "Iteration 498, loss = 0.30534198\n",
      "Iteration 1302, loss = 0.23903537\n",
      "Iteration 42, loss = 0.55974303\n",
      "Iteration 499, loss = 0.30525550\n",
      "Iteration 500, loss = 0.30512936\n",
      "Iteration 43, loss = 0.55617538\n",
      "Iteration 15, loss = 0.73621727\n",
      "Iteration 501, loss = 0.30502340\n",
      "Iteration 502, loss = 0.30495870\n",
      "Iteration 1399, loss = 0.18065485\n",
      "Iteration 44, loss = 0.55265380\n",
      "Iteration 2420, loss = 0.16361222\n",
      "Iteration 440, loss = 0.39562311\n",
      "Iteration 503, loss = 0.30481873\n",
      "Iteration 16, loss = 0.73243029\n",
      "Iteration 504, loss = 0.30470982\n",
      "Iteration 45, loss = 0.54929939\n",
      "Iteration 505, loss = 0.30462468\n",
      "Iteration 1400, loss = 0.18057239\n",
      "Iteration 506, loss = 0.30450058\n",
      "Iteration 1303, loss = 0.23839291\n",
      "Iteration 46, loss = 0.54591217\n",
      "Iteration 1360, loss = 0.26749366\n",
      "Iteration 507, loss = 0.30440030\n",
      "Iteration 1401, loss = 0.18027867\n",
      "Iteration 508, loss = 0.30434683\n",
      "Iteration 1361, loss = 0.26735590\n",
      "Iteration 47, loss = 0.54245117\n",
      "Iteration 441, loss = 0.39545406\n",
      "Iteration 509, loss = 0.30420213\n",
      "Iteration 17, loss = 0.72855051\n",
      "Iteration 1402, loss = 0.18008211\n",
      "Iteration 2421, loss = 0.16357396\n",
      "Iteration 510, loss = 0.30408501\n",
      "Iteration 48, loss = 0.53909029\n",
      "Iteration 1362, loss = 0.26725117\n",
      "Iteration 511, loss = 0.30399445\n",
      "Iteration 442, loss = 0.39531128\n",
      "Iteration 18, loss = 0.72488709\n",
      "Iteration 512, loss = 0.30389563\n",
      "Iteration 1304, loss = 0.23825350\n",
      "Iteration 1403, loss = 0.18001531\n",
      "Iteration 49, loss = 0.53576558\n",
      "Iteration 1363, loss = 0.26711400\n",
      "Iteration 513, loss = 0.30382396\n",
      "Iteration 514, loss = 0.30370729\n",
      "Iteration 50, loss = 0.53240782\n",
      "Iteration 19, loss = 0.72132660\n",
      "Iteration 2422, loss = 0.16348222\n",
      "Iteration 515, loss = 0.30361191\n",
      "Iteration 1364, loss = 0.26697236\n",
      "Iteration 1404, loss = 0.17970289\n",
      "Iteration 51, loss = 0.52929948\n",
      "Iteration 516, loss = 0.30348741\n",
      "Iteration 443, loss = 0.39512195\n",
      "Iteration 517, loss = 0.30337039\n",
      "Iteration 518, loss = 0.30328284\n",
      "Iteration 52, loss = 0.52594109\n",
      "Iteration 1365, loss = 0.26684725\n",
      "Iteration 519, loss = 0.30317125\n",
      "Iteration 20, loss = 0.71768255\n",
      "Iteration 520, loss = 0.30307649\n",
      "Iteration 1405, loss = 0.17961462\n",
      "Iteration 1305, loss = 0.23825376\n",
      "Iteration 521, loss = 0.30298864\n",
      "Iteration 522, loss = 0.30286036\n",
      "Iteration 523, loss = 0.30276872\n",
      "Iteration 1366, loss = 0.26684007\n",
      "Iteration 53, loss = 0.52266966\n",
      "Iteration 524, loss = 0.30268287\n",
      "Iteration 2423, loss = 0.16351198\n",
      "Iteration 1406, loss = 0.17949746\n",
      "Iteration 444, loss = 0.39495451\n",
      "Iteration 525, loss = 0.30259243\n",
      "Iteration 1407, loss = 0.17918577\n",
      "Iteration 54, loss = 0.51962039\n",
      "Iteration 1367, loss = 0.26662742\n",
      "Iteration 21, loss = 0.71417691\n",
      "Iteration 526, loss = 0.30250376\n",
      "Iteration 527, loss = 0.30239142\n",
      "Iteration 528, loss = 0.30228087\n",
      "Iteration 55, loss = 0.51647297\n",
      "Iteration 529, loss = 0.30218222\n",
      "Iteration 1306, loss = 0.23789191\n",
      "Iteration 530, loss = 0.30209638\n",
      "Iteration 2424, loss = 0.16345014\n",
      "Iteration 531, loss = 0.30200485\n",
      "Iteration 532, loss = 0.30190599\n",
      "Iteration 445, loss = 0.39479822\n",
      "Iteration 1408, loss = 0.17906793\n",
      "Iteration 533, loss = 0.30184817\n",
      "Iteration 56, loss = 0.51342775\n",
      "Iteration 534, loss = 0.30168704\n",
      "Iteration 22, loss = 0.71062753\n",
      "Iteration 535, loss = 0.30159797\n",
      "Iteration 536, loss = 0.30151188\n",
      "Iteration 1368, loss = 0.26658042\n",
      "Iteration 57, loss = 0.51029128\n",
      "Iteration 1409, loss = 0.17874769\n",
      "Iteration 2425, loss = 0.16324443\n",
      "Iteration 1369, loss = 0.26647691\n",
      "Iteration 1410, loss = 0.17863611\n",
      "Iteration 1307, loss = 0.23784736\n",
      "Iteration 446, loss = 0.39464188\n",
      "Iteration 23, loss = 0.70717453\n",
      "Iteration 1411, loss = 0.17836896\n",
      "Iteration 537, loss = 0.30140985\n",
      "Iteration 1370, loss = 0.26623634\n",
      "Iteration 538, loss = 0.30130422\n",
      "Iteration 2426, loss = 0.16316666\n",
      "Iteration 58, loss = 0.50737026\n",
      "Iteration 1412, loss = 0.17837476\n",
      "Iteration 1371, loss = 0.26613585\n",
      "Iteration 59, loss = 0.50437956\n",
      "Iteration 447, loss = 0.39455213\n",
      "Iteration 24, loss = 0.70369376\n",
      "Iteration 2427, loss = 0.16310845\n",
      "Iteration 539, loss = 0.30120595\n",
      "Iteration 1413, loss = 0.17839768\n",
      "Iteration 1308, loss = 0.23766589\n",
      "Iteration 60, loss = 0.50137375\n",
      "Iteration 540, loss = 0.30112295\n",
      "Iteration 2428, loss = 0.16302623\n",
      "Iteration 61, loss = 0.49852034\n",
      "Iteration 541, loss = 0.30105548\n",
      "Iteration 1414, loss = 0.17784911\n",
      "Iteration 1372, loss = 0.26604852\n",
      "Iteration 25, loss = 0.70027624\n",
      "Iteration 62, loss = 0.49573878\n",
      "Iteration 542, loss = 0.30093194\n",
      "Iteration 63, loss = 0.49297921\n",
      "Iteration 543, loss = 0.30084967\n",
      "Iteration 2429, loss = 0.16294572\n",
      "Iteration 544, loss = 0.30075265\n",
      "Iteration 545, loss = 0.30066315\n",
      "Iteration 1309, loss = 0.23748237\n",
      "Iteration 1415, loss = 0.17787648\n",
      "Iteration 546, loss = 0.30056535\n",
      "Iteration 1373, loss = 0.26588971\n",
      "Iteration 547, loss = 0.30048578\n",
      "Iteration 64, loss = 0.49019263\n",
      "Iteration 448, loss = 0.39436064\n",
      "Iteration 548, loss = 0.30037206Iteration 26, loss = 0.69685286\n",
      "\n",
      "Iteration 549, loss = 0.30027822\n",
      "Iteration 550, loss = 0.30017226\n",
      "Iteration 551, loss = 0.30010879\n",
      "Iteration 552, loss = 0.29999397\n",
      "Iteration 1310, loss = 0.23731250\n",
      "Iteration 2430, loss = 0.16287062\n",
      "Iteration 65, loss = 0.48748112\n",
      "Iteration 553, loss = 0.29990810\n",
      "Iteration 1416, loss = 0.17748391\n",
      "Iteration 1374, loss = 0.26579686\n",
      "Iteration 554, loss = 0.29984876\n",
      "Iteration 27, loss = 0.69343083\n",
      "Iteration 555, loss = 0.29971516\n",
      "Iteration 66, loss = 0.48482046\n",
      "Iteration 556, loss = 0.29960951\n",
      "Iteration 1417, loss = 0.17734879\n",
      "Iteration 1375, loss = 0.26561903\n",
      "Iteration 557, loss = 0.29953008\n",
      "Iteration 558, loss = 0.29943625\n",
      "Iteration 2431, loss = 0.16282934\n",
      "Iteration 559, loss = 0.29945826\n",
      "Iteration 1418, loss = 0.17712748\n",
      "Iteration 449, loss = 0.39418314\n",
      "Iteration 560, loss = 0.29926823\n",
      "Iteration 67, loss = 0.48226239\n",
      "Iteration 561, loss = 0.29917603\n",
      "Iteration 562, loss = 0.29911101\n",
      "Iteration 1376, loss = 0.26549695\n",
      "Iteration 2432, loss = 0.16273137\n",
      "Iteration 563, loss = 0.29898710\n",
      "Iteration 564, loss = 0.29888744\n",
      "Iteration 565, loss = 0.29879923\n",
      "Iteration 28, loss = 0.68999944\n",
      "Iteration 1419, loss = 0.17715606\n",
      "Iteration 2433, loss = 0.16272852\n",
      "Iteration 1377, loss = 0.26539710\n",
      "Iteration 1311, loss = 0.23700121\n",
      "Iteration 68, loss = 0.47968205\n",
      "Iteration 566, loss = 0.29878104\n",
      "Iteration 1420, loss = 0.17695387\n",
      "Iteration 567, loss = 0.29864315\n",
      "Iteration 450, loss = 0.39400795\n",
      "Iteration 69, loss = 0.47716941\n",
      "Iteration 568, loss = 0.29853434\n",
      "Iteration 70, loss = 0.47461937\n",
      "Iteration 2434, loss = 0.16259912\n",
      "Iteration 569, loss = 0.29844489\n",
      "Iteration 570, loss = 0.29834245\n",
      "Iteration 571, loss = 0.29827630\n",
      "Iteration 71, loss = 0.47224154\n",
      "Iteration 29, loss = 0.68650808\n",
      "Iteration 572, loss = 0.29818992\n",
      "Iteration 1421, loss = 0.17664013\n",
      "Iteration 1312, loss = 0.23686539\n",
      "Iteration 1378, loss = 0.26530213Iteration 573, loss = 0.29808013\n",
      "\n",
      "Iteration 574, loss = 0.29799020\n",
      "Iteration 575, loss = 0.29796745\n",
      "Iteration 576, loss = 0.29783521\n",
      "Iteration 577, loss = 0.29774285\n",
      "Iteration 72, loss = 0.46987296\n",
      "Iteration 2435, loss = 0.16253843\n",
      "Iteration 451, loss = 0.39385571\n",
      "Iteration 578, loss = 0.29767728\n",
      "Iteration 1313, loss = 0.23668574\n",
      "Iteration 1379, loss = 0.26512992\n",
      "Iteration 579, loss = 0.29755538\n",
      "Iteration 1422, loss = 0.17643468\n",
      "Iteration 452, loss = 0.39369433\n",
      "Iteration 30, loss = 0.68304104\n",
      "Iteration 73, loss = 0.46751763\n",
      "Iteration 580, loss = 0.29749813\n",
      "Iteration 1423, loss = 0.17622764\n",
      "Iteration 1314, loss = 0.23649305\n",
      "Iteration 2436, loss = 0.16246379\n",
      "Iteration 581, loss = 0.29738056\n",
      "Iteration 1380, loss = 0.26502446\n",
      "Iteration 74, loss = 0.46509829\n",
      "Iteration 582, loss = 0.29731060\n",
      "Iteration 75, loss = 0.46295322\n",
      "Iteration 1424, loss = 0.17638335\n",
      "Iteration 2437, loss = 0.16235318\n",
      "Iteration 583, loss = 0.29722619Iteration 453, loss = 0.39353961\n",
      "Iteration 76, loss = 0.46069538\n",
      "Iteration 31, loss = 0.67967383\n",
      "\n",
      "Iteration 584, loss = 0.29714889Iteration 77, loss = 0.45862416\n",
      "\n",
      "Iteration 1425, loss = 0.17605953\n",
      "Iteration 585, loss = 0.29705156\n",
      "Iteration 1381, loss = 0.26488211\n",
      "Iteration 586, loss = 0.29696526\n",
      "Iteration 78, loss = 0.45649612\n",
      "Iteration 1315, loss = 0.23627948\n",
      "Iteration 1426, loss = 0.17577965\n",
      "Iteration 79, loss = 0.45438220\n",
      "Iteration 32, loss = 0.67615378\n",
      "Iteration 1382, loss = 0.26477527\n",
      "Iteration 587, loss = 0.29687821\n",
      "Iteration 80, loss = 0.45235652\n",
      "Iteration 33, loss = 0.67280184\n",
      "Iteration 588, loss = 0.29679652\n",
      "Iteration 1427, loss = 0.17555492\n",
      "Iteration 1316, loss = 0.23616502Iteration 81, loss = 0.45026344\n",
      "\n",
      "Iteration 1383, loss = 0.26465513\n",
      "Iteration 589, loss = 0.29671121\n",
      "Iteration 2438, loss = 0.16227237\n",
      "Iteration 590, loss = 0.29661649\n",
      "Iteration 82, loss = 0.44842825\n",
      "Iteration 591, loss = 0.29652783\n",
      "Iteration 454, loss = 0.39348345\n",
      "Iteration 592, loss = 0.29646391\n",
      "Iteration 1428, loss = 0.17552268\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 2439, loss = 0.16220869\n",
      "Iteration 34, loss = 0.66934917\n",
      "Iteration 83, loss = 0.44640484\n",
      "Iteration 1384, loss = 0.26452601\n",
      "Iteration 1429, loss = 0.17514415\n",
      "Iteration 1317, loss = 0.23591528\n",
      "Iteration 2440, loss = 0.16218244\n",
      "Iteration 84, loss = 0.44456728\n",
      "Iteration 1430, loss = 0.17496339\n",
      "Iteration 455, loss = 0.39320465\n",
      "Iteration 1, loss = 0.77115115\n",
      "Iteration 35, loss = 0.66589036\n",
      "Iteration 1431, loss = 0.17483967\n",
      "Iteration 85, loss = 0.44275769\n",
      "Iteration 2, loss = 0.76979823\n",
      "Iteration 1385, loss = 0.26440593\n",
      "Iteration 3, loss = 0.76766990\n",
      "Iteration 2441, loss = 0.16213674\n",
      "Iteration 86, loss = 0.44094784\n",
      "Iteration 1432, loss = 0.17458409\n",
      "Iteration 4, loss = 0.76522216\n",
      "Iteration 1318, loss = 0.23578858\n",
      "Iteration 2442, loss = 0.16206902\n",
      "Iteration 87, loss = 0.43923331\n",
      "Iteration 1386, loss = 0.26427317\n",
      "Iteration 5, loss = 0.76252600\n",
      "Iteration 1433, loss = 0.17449194\n",
      "Iteration 6, loss = 0.75960136\n",
      "Iteration 36, loss = 0.66240165\n",
      "Iteration 7, loss = 0.75684185\n",
      "Iteration 88, loss = 0.43747536\n",
      "Iteration 1434, loss = 0.17428521\n",
      "Iteration 8, loss = 0.75394854\n",
      "Iteration 9, loss = 0.75113355\n",
      "Iteration 2443, loss = 0.16194894\n",
      "Iteration 1319, loss = 0.23559493\n",
      "Iteration 1387, loss = 0.26423992\n",
      "Iteration 1435, loss = 0.17409446\n",
      "Iteration 10, loss = 0.74855522\n",
      "Iteration 89, loss = 0.43577468\n",
      "Iteration 37, loss = 0.65903749\n",
      "Iteration 11, loss = 0.74576221\n",
      "Iteration 1436, loss = 0.17398564\n",
      "Iteration 90, loss = 0.43410273\n",
      "Iteration 12, loss = 0.74334413\n",
      "Iteration 2444, loss = 0.16203080\n",
      "Iteration 1388, loss = 0.26401824\n",
      "Iteration 38, loss = 0.65556004\n",
      "Iteration 1320, loss = 0.23539696\n",
      "Iteration 13, loss = 0.74098609\n",
      "Iteration 91, loss = 0.43250717\n",
      "Iteration 1437, loss = 0.17372894\n",
      "Iteration 14, loss = 0.73874296\n",
      "Iteration 15, loss = 0.73656670\n",
      "Iteration 92, loss = 0.43099077\n",
      "Iteration 39, loss = 0.65206737\n",
      "Iteration 2445, loss = 0.16183216\n",
      "Iteration 1389, loss = 0.26388240\n",
      "Iteration 16, loss = 0.73446594\n",
      "Iteration 93, loss = 0.42944650\n",
      "Iteration 1438, loss = 0.17360766\n",
      "Iteration 17, loss = 0.73254072\n",
      "Iteration 94, loss = 0.42797792\n",
      "Iteration 1390, loss = 0.26378787\n",
      "Iteration 40, loss = 0.64858438\n",
      "Iteration 1321, loss = 0.23528575\n",
      "Iteration 456, loss = 0.39311446\n",
      "Iteration 95, loss = 0.42643802\n",
      "Iteration 18, loss = 0.73068191\n",
      "Iteration 96, loss = 0.42509336\n",
      "Iteration 1439, loss = 0.17352448\n",
      "Iteration 2446, loss = 0.16168431\n",
      "Iteration 19, loss = 0.72885949\n",
      "Iteration 1391, loss = 0.26366680\n",
      "Iteration 20, loss = 0.72711837\n",
      "Iteration 1440, loss = 0.17320466\n",
      "Iteration 21, loss = 0.72529411\n",
      "Iteration 97, loss = 0.42369198\n",
      "Iteration 2447, loss = 0.16163847\n",
      "Iteration 1392, loss = 0.26354643\n",
      "Iteration 41, loss = 0.64515779\n",
      "Iteration 22, loss = 0.72364308\n",
      "Iteration 1322, loss = 0.23504592\n",
      "Iteration 23, loss = 0.72204116\n",
      "Iteration 24, loss = 0.72042362\n",
      "Iteration 1441, loss = 0.17311527\n",
      "Iteration 98, loss = 0.42225908\n",
      "Iteration 1393, loss = 0.26338784\n",
      "Iteration 42, loss = 0.64160910\n",
      "Iteration 25, loss = 0.71894020\n",
      "Iteration 99, loss = 0.42086897\n",
      "Iteration 1442, loss = 0.17296373\n",
      "Iteration 2448, loss = 0.16153608\n",
      "Iteration 26, loss = 0.71733198\n",
      "Iteration 1443, loss = 0.17272062\n",
      "Iteration 1394, loss = 0.26344976\n",
      "Iteration 27, loss = 0.71596087\n",
      "Iteration 43, loss = 0.63809944\n",
      "Iteration 1323, loss = 0.23483039\n",
      "Iteration 2449, loss = 0.16145189\n",
      "Iteration 100, loss = 0.41966203\n",
      "Iteration 28, loss = 0.71440010\n",
      "Iteration 29, loss = 0.71291462\n",
      "Iteration 1444, loss = 0.17303423\n",
      "Iteration 101, loss = 0.41834694\n",
      "Iteration 30, loss = 0.71150613\n",
      "Iteration 44, loss = 0.63465062\n",
      "Iteration 1395, loss = 0.26320162\n",
      "Iteration 102, loss = 0.41706575\n",
      "Iteration 1445, loss = 0.17248265\n",
      "Iteration 2450, loss = 0.16140661\n",
      "Iteration 31, loss = 0.71008631\n",
      "Iteration 1324, loss = 0.23498418\n",
      "Iteration 32, loss = 0.70870073\n",
      "Iteration 1446, loss = 0.17214373\n",
      "Iteration 33, loss = 0.70728196\n",
      "Iteration 1396, loss = 0.26309435\n",
      "Iteration 2451, loss = 0.16134292\n",
      "Iteration 34, loss = 0.70586518\n",
      "Iteration 103, loss = 0.41583357\n",
      "Iteration 45, loss = 0.63125170\n",
      "Iteration 35, loss = 0.70452294\n",
      "Iteration 1397, loss = 0.26291815\n",
      "Iteration 2452, loss = 0.16127384\n",
      "Iteration 1447, loss = 0.17213657\n",
      "Iteration 36, loss = 0.70316186Iteration 104, loss = 0.41465719\n",
      "\n",
      "Iteration 46, loss = 0.62764565\n",
      "Iteration 37, loss = 0.70175242\n",
      "Iteration 1398, loss = 0.26279496\n",
      "Iteration 1448, loss = 0.17178603\n",
      "Iteration 105, loss = 0.41349845\n",
      "Iteration 38, loss = 0.70036502\n",
      "Iteration 39, loss = 0.69905248\n",
      "Iteration 2453, loss = 0.16116580\n",
      "Iteration 1325, loss = 0.23459011\n",
      "Iteration 106, loss = 0.41228636\n",
      "Iteration 40, loss = 0.69761745\n",
      "Iteration 1449, loss = 0.17178194\n",
      "Iteration 41, loss = 0.69623588\n",
      "Iteration 1399, loss = 0.26278005\n",
      "Iteration 107, loss = 0.41117312\n",
      "Iteration 47, loss = 0.62413816\n",
      "Iteration 1450, loss = 0.17159742\n",
      "Iteration 42, loss = 0.69484646\n",
      "Iteration 108, loss = 0.41010437Iteration 1326, loss = 0.23452093\n",
      "\n",
      "Iteration 43, loss = 0.69347301\n",
      "Iteration 48, loss = 0.62067811\n",
      "Iteration 2454, loss = 0.16110114\n",
      "Iteration 1400, loss = 0.26253555\n",
      "Iteration 1451, loss = 0.17144121\n",
      "Iteration 44, loss = 0.69208256\n",
      "Iteration 45, loss = 0.69068266\n",
      "Iteration 109, loss = 0.40899616\n",
      "Iteration 1452, loss = 0.17118818\n",
      "Iteration 46, loss = 0.68927209\n",
      "Iteration 1327, loss = 0.23418207\n",
      "Iteration 47, loss = 0.68784544\n",
      "Iteration 457, loss = 0.39295016Iteration 49, loss = 0.61725198\n",
      "Iteration 2455, loss = 0.16102497\n",
      "\n",
      "Iteration 48, loss = 0.68640608\n",
      "Iteration 110, loss = 0.40792259\n",
      "Iteration 1401, loss = 0.26244723\n",
      "Iteration 49, loss = 0.68495212\n",
      "Iteration 1453, loss = 0.17089505\n",
      "Iteration 50, loss = 0.68348177\n",
      "Iteration 1402, loss = 0.26235854\n",
      "Iteration 111, loss = 0.40689234\n",
      "Iteration 458, loss = 0.39274895\n",
      "Iteration 1454, loss = 0.17081771\n",
      "Iteration 50, loss = 0.61371192\n",
      "Iteration 112, loss = 0.40583811\n",
      "Iteration 2456, loss = 0.16095211\n",
      "Iteration 51, loss = 0.68201321\n",
      "Iteration 52, loss = 0.68050744\n",
      "Iteration 113, loss = 0.40491160\n",
      "Iteration 1455, loss = 0.17070061\n",
      "Iteration 114, loss = 0.40384976\n",
      "Iteration 51, loss = 0.61025218\n",
      "Iteration 53, loss = 0.67901036\n",
      "Iteration 2457, loss = 0.16086781\n",
      "Iteration 459, loss = 0.39258695\n",
      "Iteration 54, loss = 0.67746911\n",
      "Iteration 1456, loss = 0.17042280\n",
      "Iteration 1403, loss = 0.26218955\n",
      "Iteration 55, loss = 0.67591558\n",
      "Iteration 56, loss = 0.67432675\n",
      "Iteration 115, loss = 0.40290524\n",
      "Iteration 57, loss = 0.67278808\n",
      "Iteration 2458, loss = 0.16081817\n",
      "Iteration 52, loss = 0.60677793\n",
      "Iteration 1457, loss = 0.17029534\n",
      "Iteration 58, loss = 0.67114741\n",
      "Iteration 1404, loss = 0.26207620\n",
      "Iteration 1328, loss = 0.23435978\n",
      "Iteration 116, loss = 0.40196278\n",
      "Iteration 2459, loss = 0.16071643\n",
      "Iteration 59, loss = 0.66950690\n",
      "Iteration 1458, loss = 0.17022185\n",
      "Iteration 53, loss = 0.60318577\n",
      "Iteration 117, loss = 0.40101268\n",
      "Iteration 60, loss = 0.66785355\n",
      "Iteration 1405, loss = 0.26204669\n",
      "Iteration 61, loss = 0.66617337\n",
      "Iteration 1459, loss = 0.16998488\n",
      "Iteration 62, loss = 0.66442636\n",
      "Iteration 118, loss = 0.40012675\n",
      "Iteration 63, loss = 0.66268927\n",
      "Iteration 1460, loss = 0.16968089\n",
      "Iteration 1406, loss = 0.26183740\n",
      "Iteration 2460, loss = 0.16075060\n",
      "Iteration 54, loss = 0.59981021\n",
      "Iteration 64, loss = 0.66090555\n",
      "Iteration 460, loss = 0.39244856\n",
      "Iteration 65, loss = 0.65906523\n",
      "Iteration 66, loss = 0.65734056\n",
      "Iteration 1329, loss = 0.23384691\n",
      "Iteration 119, loss = 0.39922707\n",
      "Iteration 2461, loss = 0.16060514\n",
      "Iteration 67, loss = 0.65542750\n",
      "Iteration 1461, loss = 0.16966116\n",
      "Iteration 68, loss = 0.65349732\n",
      "Iteration 1407, loss = 0.26171875\n",
      "Iteration 69, loss = 0.65153868\n",
      "Iteration 55, loss = 0.59614084\n",
      "Iteration 70, loss = 0.64961863\n",
      "Iteration 1462, loss = 0.16963272\n",
      "Iteration 71, loss = 0.64760604\n",
      "Iteration 120, loss = 0.39834380\n",
      "Iteration 72, loss = 0.64555513\n",
      "Iteration 2462, loss = 0.16048731\n",
      "Iteration 56, loss = 0.59273379\n",
      "Iteration 1408, loss = 0.26158455\n",
      "Iteration 73, loss = 0.64350366\n",
      "Iteration 1330, loss = 0.23381368\n",
      "Iteration 74, loss = 0.64144293\n",
      "Iteration 121, loss = 0.39748463\n",
      "Iteration 75, loss = 0.63926395\n",
      "Iteration 2463, loss = 0.16046574\n",
      "Iteration 461, loss = 0.39232442\n",
      "Iteration 57, loss = 0.58928769\n",
      "Iteration 76, loss = 0.63713686\n",
      "Iteration 1409, loss = 0.26144919\n",
      "Iteration 1463, loss = 0.16946459\n",
      "Iteration 122, loss = 0.39666076\n",
      "Iteration 77, loss = 0.63495962\n",
      "Iteration 123, loss = 0.39578647\n",
      "Iteration 58, loss = 0.58576271\n",
      "Iteration 1464, loss = 0.16907233\n",
      "Iteration 78, loss = 0.63263941\n",
      "Iteration 79, loss = 0.63045279\n",
      "Iteration 2464, loss = 0.16048887\n",
      "Iteration 80, loss = 0.62810697\n",
      "Iteration 1331, loss = 0.23348350\n",
      "Iteration 462, loss = 0.39216698\n",
      "Iteration 124, loss = 0.39498462\n",
      "Iteration 1465, loss = 0.16916133\n",
      "Iteration 1410, loss = 0.26131516\n",
      "Iteration 2465, loss = 0.16038151\n",
      "Iteration 81, loss = 0.62582743\n",
      "Iteration 1466, loss = 0.16880426\n",
      "Iteration 125, loss = 0.39421134\n",
      "Iteration 1332, loss = 0.23356719\n",
      "Iteration 82, loss = 0.62337829\n",
      "Iteration 126, loss = 0.39346390\n",
      "Iteration 2466, loss = 0.16023399\n",
      "Iteration 1411, loss = 0.26125093\n",
      "Iteration 127, loss = 0.39264009\n",
      "Iteration 1467, loss = 0.16914884\n",
      "Iteration 128, loss = 0.39191232\n",
      "Iteration 2467, loss = 0.16016638\n",
      "Iteration 83, loss = 0.62107838\n",
      "Iteration 59, loss = 0.58237884\n",
      "Iteration 1412, loss = 0.26107534\n",
      "Iteration 1468, loss = 0.16845240\n",
      "Iteration 2468, loss = 0.16024406\n",
      "Iteration 84, loss = 0.61862695\n",
      "Iteration 1413, loss = 0.26099362\n",
      "Iteration 85, loss = 0.61616733\n",
      "Iteration 60, loss = 0.57893821\n",
      "Iteration 1469, loss = 0.16826215\n",
      "Iteration 86, loss = 0.61368821\n",
      "Iteration 1333, loss = 0.23321433\n",
      "Iteration 463, loss = 0.39198834\n",
      "Iteration 87, loss = 0.61122311\n",
      "Iteration 129, loss = 0.39118862\n",
      "Iteration 88, loss = 0.60865558\n",
      "Iteration 130, loss = 0.39051150\n",
      "Iteration 89, loss = 0.60613366\n",
      "Iteration 1470, loss = 0.16815090\n",
      "Iteration 1414, loss = 0.26091460\n",
      "Iteration 90, loss = 0.60358634\n",
      "Iteration 61, loss = 0.57554037\n",
      "Iteration 1471, loss = 0.16795816\n",
      "Iteration 131, loss = 0.38982706\n",
      "Iteration 91, loss = 0.60093653\n",
      "Iteration 1472, loss = 0.16789419\n",
      "Iteration 92, loss = 0.59833551\n",
      "Iteration 93, loss = 0.59567134\n",
      "Iteration 464, loss = 0.39188757\n",
      "Iteration 94, loss = 0.59298023\n",
      "Iteration 132, loss = 0.38907360\n",
      "Iteration 1334, loss = 0.23296582\n",
      "Iteration 133, loss = 0.38841565\n",
      "Iteration 95, loss = 0.59031813\n",
      "Iteration 2469, loss = 0.16012871\n",
      "Iteration 96, loss = 0.58764200\n",
      "Iteration 1415, loss = 0.26069920\n",
      "Iteration 62, loss = 0.57207287\n",
      "Iteration 134, loss = 0.38778638\n",
      "Iteration 97, loss = 0.58494757\n",
      "Iteration 98, loss = 0.58218981\n",
      "Iteration 135, loss = 0.38702391\n",
      "Iteration 2470, loss = 0.16003002\n",
      "Iteration 1473, loss = 0.16752511\n",
      "Iteration 1416, loss = 0.26058886\n",
      "Iteration 136, loss = 0.38640285\n",
      "Iteration 1335, loss = 0.23279364\n",
      "Iteration 99, loss = 0.57948299\n",
      "Iteration 465, loss = 0.39167554\n",
      "Iteration 137, loss = 0.38575839\n",
      "Iteration 1474, loss = 0.16736662\n",
      "Iteration 100, loss = 0.57673682\n",
      "Iteration 1417, loss = 0.26046252\n",
      "Iteration 63, loss = 0.56875248\n",
      "Iteration 138, loss = 0.38516689\n",
      "Iteration 2471, loss = 0.15987381\n",
      "Iteration 101, loss = 0.57403534\n",
      "Iteration 1418, loss = 0.26033876\n",
      "Iteration 102, loss = 0.57126197\n",
      "Iteration 466, loss = 0.39157237\n",
      "Iteration 139, loss = 0.38459041\n",
      "Iteration 1336, loss = 0.23264813\n",
      "Iteration 2472, loss = 0.15973546\n",
      "Iteration 1419, loss = 0.26023125\n",
      "Iteration 103, loss = 0.56849228\n",
      "Iteration 140, loss = 0.38389757\n",
      "Iteration 104, loss = 0.56571815\n",
      "Iteration 1475, loss = 0.16729416\n",
      "Iteration 1420, loss = 0.26010832\n",
      "Iteration 105, loss = 0.56292550\n",
      "Iteration 64, loss = 0.56536648\n",
      "Iteration 106, loss = 0.56012185\n",
      "Iteration 1337, loss = 0.23243591\n",
      "Iteration 107, loss = 0.55734289\n",
      "Iteration 2473, loss = 0.15978441\n",
      "Iteration 141, loss = 0.38332332\n",
      "Iteration 65, loss = 0.56213569\n",
      "Iteration 108, loss = 0.55464253\n",
      "Iteration 467, loss = 0.39135900\n",
      "Iteration 142, loss = 0.38272247\n",
      "Iteration 109, loss = 0.55189002\n",
      "Iteration 1421, loss = 0.25996072\n",
      "Iteration 1476, loss = 0.16710194\n",
      "Iteration 110, loss = 0.54914205\n",
      "Iteration 2474, loss = 0.15964359\n",
      "Iteration 143, loss = 0.38216040\n",
      "Iteration 111, loss = 0.54632425\n",
      "Iteration 66, loss = 0.55875029\n",
      "Iteration 1422, loss = 0.25982863\n",
      "Iteration 468, loss = 0.39127482\n",
      "Iteration 1338, loss = 0.23227387\n",
      "Iteration 112, loss = 0.54356347\n",
      "Iteration 1477, loss = 0.16681838\n",
      "Iteration 113, loss = 0.54088313\n",
      "Iteration 114, loss = 0.53821273\n",
      "Iteration 2475, loss = 0.15950711\n",
      "Iteration 144, loss = 0.38152473\n",
      "Iteration 115, loss = 0.53547850\n",
      "Iteration 67, loss = 0.55562482\n",
      "Iteration 1423, loss = 0.25972446\n",
      "Iteration 116, loss = 0.53282036\n",
      "Iteration 1339, loss = 0.23215177\n",
      "Iteration 469, loss = 0.39116322\n",
      "Iteration 117, loss = 0.53007402\n",
      "Iteration 118, loss = 0.52745341\n",
      "Iteration 1478, loss = 0.16670458\n",
      "Iteration 145, loss = 0.38099294\n",
      "Iteration 119, loss = 0.52474907\n",
      "Iteration 146, loss = 0.38040443\n",
      "Iteration 2476, loss = 0.15954602\n",
      "Iteration 68, loss = 0.55230144\n",
      "Iteration 1479, loss = 0.16651517\n",
      "Iteration 147, loss = 0.37987767\n",
      "Iteration 120, loss = 0.52220970Iteration 1340, loss = 0.23203585\n",
      "Iteration 1424, loss = 0.25958876\n",
      "Iteration 148, loss = 0.37931793\n",
      "\n",
      "Iteration 1480, loss = 0.16657615\n",
      "Iteration 69, loss = 0.54911420\n",
      "Iteration 121, loss = 0.51961218\n",
      "Iteration 470, loss = 0.39093815\n",
      "Iteration 2477, loss = 0.15938927\n",
      "Iteration 122, loss = 0.51702586\n",
      "Iteration 1481, loss = 0.16648998\n",
      "Iteration 149, loss = 0.37878148\n",
      "Iteration 70, loss = 0.54590907\n",
      "Iteration 1482, loss = 0.16606030\n",
      "Iteration 123, loss = 0.51448069\n",
      "Iteration 1425, loss = 0.25948135\n",
      "Iteration 2478, loss = 0.15935707\n",
      "Iteration 124, loss = 0.51189736\n",
      "Iteration 150, loss = 0.37824969\n",
      "Iteration 71, loss = 0.54264795\n",
      "Iteration 125, loss = 0.50951645\n",
      "Iteration 151, loss = 0.37771610\n",
      "Iteration 126, loss = 0.50697902\n",
      "Iteration 1426, loss = 0.25937092\n",
      "Iteration 127, loss = 0.50454465\n",
      "Iteration 1483, loss = 0.16587876\n",
      "Iteration 471, loss = 0.39076401\n",
      "Iteration 72, loss = 0.53967726\n",
      "Iteration 152, loss = 0.37720440\n",
      "Iteration 1341, loss = 0.23170992\n",
      "Iteration 128, loss = 0.50220709\n",
      "Iteration 153, loss = 0.37668130\n",
      "Iteration 2479, loss = 0.15940724\n",
      "Iteration 129, loss = 0.49982982\n",
      "Iteration 154, loss = 0.37623102\n",
      "Iteration 1484, loss = 0.16582245\n",
      "Iteration 1427, loss = 0.25922655\n",
      "Iteration 130, loss = 0.49745239\n",
      "Iteration 472, loss = 0.39060792\n",
      "Iteration 73, loss = 0.53661100\n",
      "Iteration 131, loss = 0.49520021\n",
      "Iteration 155, loss = 0.37573281\n",
      "Iteration 132, loss = 0.49280901\n",
      "Iteration 1342, loss = 0.23158340\n",
      "Iteration 2480, loss = 0.15916220\n",
      "Iteration 133, loss = 0.49064790\n",
      "Iteration 156, loss = 0.37522561\n",
      "Iteration 1485, loss = 0.16566577\n",
      "Iteration 473, loss = 0.39046932\n",
      "Iteration 134, loss = 0.48849231\n",
      "Iteration 74, loss = 0.53361377\n",
      "Iteration 1428, loss = 0.25911210\n",
      "Iteration 157, loss = 0.37473159\n",
      "Iteration 135, loss = 0.48630013\n",
      "Iteration 2481, loss = 0.15909402\n",
      "Iteration 75, loss = 0.53053205\n",
      "Iteration 158, loss = 0.37427785\n",
      "Iteration 1343, loss = 0.23139618\n",
      "Iteration 136, loss = 0.48410628\n",
      "Iteration 76, loss = 0.52751106\n",
      "Iteration 137, loss = 0.48201970\n",
      "Iteration 1486, loss = 0.16541923\n",
      "Iteration 138, loss = 0.47994103\n",
      "Iteration 474, loss = 0.39029518\n",
      "Iteration 2482, loss = 0.15911574\n",
      "Iteration 77, loss = 0.52467703\n",
      "Iteration 159, loss = 0.37379878\n",
      "Iteration 139, loss = 0.47785621\n",
      "Iteration 1487, loss = 0.16528542\n",
      "Iteration 78, loss = 0.52169974\n",
      "Iteration 140, loss = 0.47588442\n",
      "Iteration 1344, loss = 0.23129637\n",
      "Iteration 160, loss = 0.37337049\n",
      "Iteration 141, loss = 0.47391849\n",
      "Iteration 1488, loss = 0.16507819\n",
      "Iteration 1429, loss = 0.25905508\n",
      "Iteration 475, loss = 0.39012288\n",
      "Iteration 142, loss = 0.47193914\n",
      "Iteration 2483, loss = 0.15894800\n",
      "Iteration 161, loss = 0.37289641\n",
      "Iteration 1489, loss = 0.16494580\n",
      "Iteration 143, loss = 0.47007328\n",
      "Iteration 1430, loss = 0.25893788\n",
      "Iteration 1490, loss = 0.16470402\n",
      "Iteration 144, loss = 0.46816344\n",
      "Iteration 145, loss = 0.46629260\n",
      "Iteration 2484, loss = 0.15886144\n",
      "Iteration 1491, loss = 0.16458348\n",
      "Iteration 146, loss = 0.46453597\n",
      "Iteration 1345, loss = 0.23109351\n",
      "Iteration 162, loss = 0.37249205\n",
      "Iteration 2485, loss = 0.15875995\n",
      "Iteration 147, loss = 0.46271588\n",
      "Iteration 79, loss = 0.51896782\n",
      "Iteration 148, loss = 0.46098543\n",
      "Iteration 163, loss = 0.37201376\n",
      "Iteration 1431, loss = 0.25874541\n",
      "Iteration 149, loss = 0.45914977\n",
      "Iteration 1346, loss = 0.23083880\n",
      "Iteration 164, loss = 0.37157098\n",
      "Iteration 150, loss = 0.45757208\n",
      "Iteration 476, loss = 0.39000804\n",
      "Iteration 151, loss = 0.45590820\n",
      "Iteration 80, loss = 0.51614437\n",
      "Iteration 2486, loss = 0.15878121Iteration 152, loss = 0.45417389\n",
      "Iteration 1432, loss = 0.25866239\n",
      "\n",
      "Iteration 165, loss = 0.37117128\n",
      "Iteration 1492, loss = 0.16464146\n",
      "Iteration 1347, loss = 0.23082990\n",
      "Iteration 166, loss = 0.37071606\n",
      "Iteration 1493, loss = 0.16426265\n",
      "Iteration 477, loss = 0.38979190\n",
      "Iteration 1494, loss = 0.16438761\n",
      "Iteration 167, loss = 0.37031643\n",
      "Iteration 153, loss = 0.45261849\n",
      "Iteration 154, loss = 0.45108237\n",
      "Iteration 1495, loss = 0.16400488\n",
      "Iteration 81, loss = 0.51328968\n",
      "Iteration 168, loss = 0.36987741\n",
      "Iteration 155, loss = 0.44955374\n",
      "Iteration 1433, loss = 0.25849409\n",
      "Iteration 169, loss = 0.36946699\n",
      "Iteration 170, loss = 0.36907435\n",
      "Iteration 478, loss = 0.38968287\n",
      "Iteration 2487, loss = 0.15880529\n",
      "Iteration 156, loss = 0.44807790\n",
      "Iteration 1496, loss = 0.16380240\n",
      "Iteration 1348, loss = 0.23064948Iteration 157, loss = 0.44662090\n",
      "\n",
      "Iteration 158, loss = 0.44514150\n",
      "Iteration 159, loss = 0.44377689\n",
      "Iteration 160, loss = 0.44230564\n",
      "Iteration 479, loss = 0.38949555\n",
      "Iteration 171, loss = 0.36871183\n",
      "Iteration 161, loss = 0.44099555\n",
      "Iteration 82, loss = 0.51066378\n",
      "Iteration 2488, loss = 0.15863482\n",
      "Iteration 1434, loss = 0.25835914\n",
      "Iteration 1497, loss = 0.16371356\n",
      "Iteration 162, loss = 0.43956058\n",
      "Iteration 163, loss = 0.43826436\n",
      "Iteration 164, loss = 0.43707221\n",
      "Iteration 172, loss = 0.36825855\n",
      "Iteration 2489, loss = 0.15846420\n",
      "Iteration 165, loss = 0.43570669\n",
      "Iteration 1435, loss = 0.25833303\n",
      "Iteration 173, loss = 0.36793423\n",
      "Iteration 1349, loss = 0.23057282\n",
      "Iteration 166, loss = 0.43451154\n",
      "Iteration 2490, loss = 0.15838512\n",
      "Iteration 1498, loss = 0.16345969\n",
      "Iteration 480, loss = 0.38933786\n",
      "Iteration 167, loss = 0.43324016\n",
      "Iteration 1436, loss = 0.25814463\n",
      "Iteration 168, loss = 0.43210629\n",
      "Iteration 174, loss = 0.36750579\n",
      "Iteration 169, loss = 0.43090131\n",
      "Iteration 1499, loss = 0.16333110\n",
      "Iteration 170, loss = 0.42977750\n",
      "Iteration 2491, loss = 0.15835849\n",
      "Iteration 1437, loss = 0.25799708\n",
      "Iteration 171, loss = 0.42863822\n",
      "Iteration 175, loss = 0.36712535\n",
      "Iteration 1350, loss = 0.23043261\n",
      "Iteration 172, loss = 0.42756317\n",
      "Iteration 176, loss = 0.36672973\n",
      "Iteration 1438, loss = 0.25791642\n",
      "Iteration 173, loss = 0.42641624\n",
      "Iteration 1500, loss = 0.16316322\n",
      "Iteration 481, loss = 0.38920936\n",
      "Iteration 177, loss = 0.36633864\n",
      "Iteration 174, loss = 0.42533049\n",
      "Iteration 175, loss = 0.42431957\n",
      "Iteration 1439, loss = 0.25777861\n",
      "Iteration 2492, loss = 0.15837561\n",
      "Iteration 176, loss = 0.42331679\n",
      "Iteration 1501, loss = 0.16333456\n",
      "Iteration 178, loss = 0.36601598\n",
      "Iteration 1351, loss = 0.22998036\n",
      "Iteration 177, loss = 0.42224249\n",
      "Iteration 1502, loss = 0.16282704\n",
      "Iteration 178, loss = 0.42125358\n",
      "Iteration 2493, loss = 0.15822746\n",
      "Iteration 179, loss = 0.36562013\n",
      "Iteration 1440, loss = 0.25763338\n",
      "Iteration 179, loss = 0.42024461\n",
      "Iteration 482, loss = 0.38907822\n",
      "Iteration 1503, loss = 0.16283564\n",
      "Iteration 83, loss = 0.50797637\n",
      "Iteration 180, loss = 0.41934609\n",
      "Iteration 2494, loss = 0.15811036\n",
      "Iteration 181, loss = 0.41834019\n",
      "Iteration 1352, loss = 0.22996773\n",
      "Iteration 1441, loss = 0.25752351\n",
      "Iteration 180, loss = 0.36529766\n",
      "Iteration 182, loss = 0.41747603\n",
      "Iteration 1504, loss = 0.16250698\n",
      "Iteration 183, loss = 0.41648886\n",
      "Iteration 2495, loss = 0.15807680\n",
      "Iteration 184, loss = 0.41557658\n",
      "Iteration 185, loss = 0.41477468\n",
      "Iteration 1353, loss = 0.22964198\n",
      "Iteration 181, loss = 0.36491238\n",
      "Iteration 186, loss = 0.41385871\n",
      "Iteration 1442, loss = 0.25746246\n",
      "Iteration 187, loss = 0.41295508\n",
      "Iteration 483, loss = 0.38887014\n",
      "Iteration 1505, loss = 0.16246526\n",
      "Iteration 188, loss = 0.41213294\n",
      "Iteration 189, loss = 0.41128633\n",
      "Iteration 182, loss = 0.36457516\n",
      "Iteration 2496, loss = 0.15798395\n",
      "Iteration 1443, loss = 0.25733945\n",
      "Iteration 190, loss = 0.41041742\n",
      "Iteration 2497, loss = 0.15792203\n",
      "Iteration 484, loss = 0.38879488\n",
      "Iteration 1444, loss = 0.25722758\n",
      "Iteration 183, loss = 0.36428761\n",
      "Iteration 191, loss = 0.40962159\n",
      "Iteration 1506, loss = 0.16237455\n",
      "Iteration 1354, loss = 0.22948930\n",
      "Iteration 192, loss = 0.40879652\n",
      "Iteration 2498, loss = 0.15783098\n",
      "Iteration 193, loss = 0.40805180\n",
      "Iteration 485, loss = 0.38861549\n",
      "Iteration 1507, loss = 0.16225497\n",
      "Iteration 194, loss = 0.40727008\n",
      "Iteration 2499, loss = 0.15778724\n",
      "Iteration 195, loss = 0.40643049\n",
      "Iteration 184, loss = 0.36388062\n",
      "Iteration 196, loss = 0.40567908\n",
      "Iteration 197, loss = 0.40490641\n",
      "Iteration 1508, loss = 0.16194098\n",
      "Iteration 198, loss = 0.40417591\n",
      "Iteration 1445, loss = 0.25700573\n",
      "Iteration 185, loss = 0.36353587\n",
      "Iteration 1509, loss = 0.16182224\n",
      "Iteration 199, loss = 0.40345564\n",
      "Iteration 186, loss = 0.36319653\n",
      "Iteration 200, loss = 0.40267209\n",
      "Iteration 2500, loss = 0.15765862\n",
      "Iteration 486, loss = 0.38849909\n",
      "Iteration 187, loss = 0.36295768\n",
      "Iteration 1510, loss = 0.16159519\n",
      "Iteration 1446, loss = 0.25695780\n",
      "Iteration 1511, loss = 0.16143467\n",
      "Iteration 2501, loss = 0.15777537\n",
      "Iteration 188, loss = 0.36256085\n",
      "Iteration 201, loss = 0.40192025Iteration 1512, loss = 0.16121299\n",
      "\n",
      "Iteration 2502, loss = 0.15753639\n",
      "Iteration 189, loss = 0.36220134\n",
      "Iteration 1355, loss = 0.22933045\n",
      "Iteration 202, loss = 0.40121817\n",
      "Iteration 2503, loss = 0.15755484\n",
      "Iteration 190, loss = 0.36191443\n",
      "Iteration 84, loss = 0.50537978\n",
      "Iteration 203, loss = 0.40044644\n",
      "Iteration 204, loss = 0.39978282\n",
      "Iteration 191, loss = 0.36158767\n",
      "Iteration 192, loss = 0.36125738\n",
      "Iteration 205, loss = 0.39908708\n",
      "Iteration 1447, loss = 0.25676666\n",
      "Iteration 206, loss = 0.39836571\n",
      "Iteration 1356, loss = 0.22915994\n",
      "Iteration 487, loss = 0.38829618\n",
      "Iteration 1513, loss = 0.16116042\n",
      "Iteration 193, loss = 0.36095282Iteration 85, loss = 0.50281185\n",
      "Iteration 207, loss = 0.39765007\n",
      "\n",
      "Iteration 1448, loss = 0.25667087\n",
      "Iteration 1357, loss = 0.22917310\n",
      "Iteration 208, loss = 0.39697370\n",
      "Iteration 194, loss = 0.36063650\n",
      "Iteration 209, loss = 0.39630522\n",
      "Iteration 86, loss = 0.50014901\n",
      "Iteration 488, loss = 0.38809966\n",
      "Iteration 210, loss = 0.39559118\n",
      "Iteration 1449, loss = 0.25653234\n",
      "Iteration 1514, loss = 0.16095761\n",
      "Iteration 211, loss = 0.39493355\n",
      "Iteration 212, loss = 0.39427956\n",
      "Iteration 213, loss = 0.39366991\n",
      "Iteration 87, loss = 0.49767065\n",
      "Iteration 489, loss = 0.38792022\n",
      "Iteration 214, loss = 0.39300851\n",
      "Iteration 1358, loss = 0.22892117\n",
      "Iteration 215, loss = 0.39231009\n",
      "Iteration 2504, loss = 0.15743536\n",
      "Iteration 195, loss = 0.36031245\n",
      "Iteration 216, loss = 0.39165819\n",
      "Iteration 217, loss = 0.39105736\n",
      "Iteration 1450, loss = 0.25648037\n",
      "Iteration 1515, loss = 0.16078640\n",
      "Iteration 218, loss = 0.39043623\n",
      "Iteration 196, loss = 0.36003445Iteration 490, loss = 0.38784172\n",
      "\n",
      "Iteration 219, loss = 0.38978444\n",
      "Iteration 1451, loss = 0.25642873\n",
      "Iteration 1359, loss = 0.22859517\n",
      "Iteration 2505, loss = 0.15739868\n",
      "Iteration 88, loss = 0.49532754\n",
      "Iteration 1516, loss = 0.16064757\n",
      "Iteration 197, loss = 0.35975258\n",
      "Iteration 220, loss = 0.38917942\n",
      "Iteration 221, loss = 0.38857618\n",
      "Iteration 491, loss = 0.38765251\n",
      "Iteration 1452, loss = 0.25622047\n",
      "Iteration 222, loss = 0.38799291\n",
      "Iteration 2506, loss = 0.15728585\n",
      "Iteration 198, loss = 0.35941201\n",
      "Iteration 1517, loss = 0.16051721\n",
      "Iteration 89, loss = 0.49283537\n",
      "Iteration 223, loss = 0.38737200\n",
      "Iteration 1360, loss = 0.22852969\n",
      "Iteration 1518, loss = 0.16074533\n",
      "Iteration 199, loss = 0.35915049\n",
      "Iteration 224, loss = 0.38677674\n",
      "Iteration 1453, loss = 0.25606208\n",
      "Iteration 200, loss = 0.35888192\n",
      "Iteration 225, loss = 0.38618909\n",
      "Iteration 492, loss = 0.38746319\n",
      "Iteration 1519, loss = 0.16034585\n",
      "Iteration 2507, loss = 0.15722221\n",
      "Iteration 90, loss = 0.49049391\n",
      "Iteration 201, loss = 0.35856152\n",
      "Iteration 226, loss = 0.38558676\n",
      "Iteration 227, loss = 0.38505829\n",
      "Iteration 1454, loss = 0.25590779\n",
      "Iteration 1361, loss = 0.22826594\n",
      "Iteration 91, loss = 0.48817786\n",
      "Iteration 228, loss = 0.38443443\n",
      "Iteration 1520, loss = 0.16017502\n",
      "Iteration 1455, loss = 0.25587491\n",
      "Iteration 2508, loss = 0.15708723\n",
      "Iteration 493, loss = 0.38729544\n",
      "Iteration 202, loss = 0.35826431\n",
      "Iteration 1362, loss = 0.22812212\n",
      "Iteration 203, loss = 0.35799961\n",
      "Iteration 229, loss = 0.38385467\n",
      "Iteration 1521, loss = 0.15991423\n",
      "Iteration 2509, loss = 0.15701365\n",
      "Iteration 230, loss = 0.38331302\n",
      "Iteration 92, loss = 0.48591655\n",
      "Iteration 231, loss = 0.38277773\n",
      "Iteration 204, loss = 0.35767569\n",
      "Iteration 232, loss = 0.38222009\n",
      "Iteration 494, loss = 0.38715392\n",
      "Iteration 1522, loss = 0.15989651\n",
      "Iteration 1363, loss = 0.22795380\n",
      "Iteration 1456, loss = 0.25567425\n",
      "Iteration 233, loss = 0.38164249\n",
      "Iteration 205, loss = 0.35743284\n",
      "Iteration 234, loss = 0.38110604\n",
      "Iteration 2510, loss = 0.15698488\n",
      "Iteration 235, loss = 0.38056448\n",
      "Iteration 1523, loss = 0.15971662\n",
      "Iteration 206, loss = 0.35713202\n",
      "Iteration 1457, loss = 0.25559939\n",
      "Iteration 236, loss = 0.38001677\n",
      "Iteration 1364, loss = 0.22775880\n",
      "Iteration 237, loss = 0.37948757\n",
      "Iteration 93, loss = 0.48368469\n",
      "Iteration 1524, loss = 0.15935322\n",
      "Iteration 238, loss = 0.37894943\n",
      "Iteration 239, loss = 0.37846053\n",
      "Iteration 207, loss = 0.35687506\n",
      "Iteration 208, loss = 0.35657838\n",
      "Iteration 240, loss = 0.37789602\n",
      "Iteration 209, loss = 0.35633850\n",
      "Iteration 1458, loss = 0.25545769\n",
      "Iteration 1365, loss = 0.22796144\n",
      "Iteration 241, loss = 0.37745005\n",
      "Iteration 1525, loss = 0.15936313\n",
      "Iteration 210, loss = 0.35605124\n",
      "Iteration 495, loss = 0.38698146\n",
      "Iteration 242, loss = 0.37692255\n",
      "Iteration 2511, loss = 0.15693698\n",
      "Iteration 94, loss = 0.48156626\n",
      "Iteration 243, loss = 0.37643124\n",
      "Iteration 1526, loss = 0.15915586\n",
      "Iteration 244, loss = 0.37585415\n",
      "Iteration 245, loss = 0.37538986\n",
      "Iteration 1459, loss = 0.25530774\n",
      "Iteration 95, loss = 0.47937881\n",
      "Iteration 211, loss = 0.35580328\n",
      "Iteration 1527, loss = 0.15895864\n",
      "Iteration 2512, loss = 0.15682871\n",
      "Iteration 1366, loss = 0.22746381\n",
      "Iteration 1528, loss = 0.15898839\n",
      "Iteration 246, loss = 0.37491099\n",
      "Iteration 212, loss = 0.35552404\n",
      "Iteration 247, loss = 0.37440022\n",
      "Iteration 1460, loss = 0.25519293\n",
      "Iteration 213, loss = 0.35525862\n",
      "Iteration 1367, loss = 0.22721775\n",
      "Iteration 496, loss = 0.38684241\n",
      "Iteration 96, loss = 0.47734368\n",
      "Iteration 214, loss = 0.35501640\n",
      "Iteration 2513, loss = 0.15679924\n",
      "Iteration 248, loss = 0.37387397\n",
      "Iteration 215, loss = 0.35473310\n",
      "Iteration 1529, loss = 0.15895708\n",
      "Iteration 249, loss = 0.37343236\n",
      "Iteration 216, loss = 0.35457131\n",
      "Iteration 250, loss = 0.37291496\n",
      "Iteration 1368, loss = 0.22735520\n",
      "Iteration 1461, loss = 0.25508770\n",
      "Iteration 217, loss = 0.35426743\n",
      "Iteration 251, loss = 0.37245222\n",
      "Iteration 2514, loss = 0.15664687\n",
      "Iteration 1530, loss = 0.15857320\n",
      "Iteration 218, loss = 0.35399794\n",
      "Iteration 97, loss = 0.47535063\n",
      "Iteration 252, loss = 0.37198840\n",
      "Iteration 219, loss = 0.35378694\n",
      "Iteration 1531, loss = 0.15862309\n",
      "Iteration 2515, loss = 0.15661715\n",
      "Iteration 220, loss = 0.35352499\n",
      "Iteration 1532, loss = 0.15818738\n",
      "Iteration 221, loss = 0.35329615\n",
      "Iteration 222, loss = 0.35302853\n",
      "Iteration 98, loss = 0.47339676\n",
      "Iteration 1369, loss = 0.22689333\n",
      "Iteration 253, loss = 0.37155800\n",
      "Iteration 223, loss = 0.35280932\n",
      "Iteration 2516, loss = 0.15651171\n",
      "Iteration 224, loss = 0.35255692\n",
      "Iteration 254, loss = 0.37103463\n",
      "Iteration 1533, loss = 0.15819576\n",
      "Iteration 99, loss = 0.47139496\n",
      "Iteration 497, loss = 0.38667766\n",
      "Iteration 255, loss = 0.37059309\n",
      "Iteration 225, loss = 0.35234401\n",
      "Iteration 1462, loss = 0.25498098\n",
      "Iteration 256, loss = 0.37014000\n",
      "Iteration 226, loss = 0.35209496\n",
      "Iteration 1463, loss = 0.25480778\n",
      "Iteration 227, loss = 0.35188509\n",
      "Iteration 2517, loss = 0.15644504\n",
      "Iteration 257, loss = 0.36965414\n",
      "Iteration 100, loss = 0.46944572\n",
      "Iteration 228, loss = 0.35164449\n",
      "Iteration 1370, loss = 0.22671325\n",
      "Iteration 258, loss = 0.36923491\n",
      "Iteration 498, loss = 0.38671368\n",
      "Iteration 1464, loss = 0.25476706\n",
      "Iteration 259, loss = 0.36875556\n",
      "Iteration 1534, loss = 0.15812733\n",
      "Iteration 260, loss = 0.36831040\n",
      "Iteration 101, loss = 0.46769898\n",
      "Iteration 1535, loss = 0.15774279\n",
      "Iteration 229, loss = 0.35141988\n",
      "Iteration 261, loss = 0.36787637\n",
      "Iteration 1536, loss = 0.15825069\n",
      "Iteration 1371, loss = 0.22681647\n",
      "Iteration 499, loss = 0.38641291\n",
      "Iteration 1465, loss = 0.25458585\n",
      "Iteration 262, loss = 0.36741861\n",
      "Iteration 1537, loss = 0.15755368\n",
      "Iteration 230, loss = 0.35122539\n",
      "Iteration 102, loss = 0.46588500\n",
      "Iteration 231, loss = 0.35100261\n",
      "Iteration 263, loss = 0.36700541\n",
      "Iteration 1538, loss = 0.15730828\n",
      "Iteration 264, loss = 0.36656406\n",
      "Iteration 103, loss = 0.46415404\n",
      "Iteration 2518, loss = 0.15643409\n",
      "Iteration 500, loss = 0.38622493\n",
      "Iteration 104, loss = 0.46241349\n",
      "Iteration 265, loss = 0.36614651\n",
      "Iteration 1372, loss = 0.22635354\n",
      "Iteration 1466, loss = 0.25448404Iteration 1539, loss = 0.15729755\n",
      "\n",
      "Iteration 232, loss = 0.35077907\n",
      "Iteration 105, loss = 0.46069407\n",
      "Iteration 266, loss = 0.36573444\n",
      "Iteration 233, loss = 0.35056711\n",
      "Iteration 1373, loss = 0.22634214\n",
      "Iteration 501, loss = 0.38609059\n",
      "Iteration 1467, loss = 0.25433358\n",
      "Iteration 267, loss = 0.36528644\n",
      "Iteration 234, loss = 0.35036480\n",
      "Iteration 268, loss = 0.36488384\n",
      "Iteration 2519, loss = 0.15629199\n",
      "Iteration 269, loss = 0.36447954\n",
      "Iteration 106, loss = 0.45901670\n",
      "Iteration 270, loss = 0.36402811\n",
      "Iteration 1468, loss = 0.25419859\n",
      "Iteration 271, loss = 0.36363007\n",
      "Iteration 1540, loss = 0.15707884\n",
      "Iteration 272, loss = 0.36319494\n",
      "Iteration 1374, loss = 0.22611350\n",
      "Iteration 273, loss = 0.36282006\n",
      "Iteration 274, loss = 0.36241570\n",
      "Iteration 235, loss = 0.35016485\n",
      "Iteration 502, loss = 0.38595365\n",
      "Iteration 2520, loss = 0.15623035\n",
      "Iteration 1469, loss = 0.25412190\n",
      "Iteration 236, loss = 0.34993024\n",
      "Iteration 275, loss = 0.36207197\n",
      "Iteration 276, loss = 0.36160678\n",
      "Iteration 1541, loss = 0.15688050\n",
      "Iteration 237, loss = 0.34974274\n",
      "Iteration 2521, loss = 0.15615394\n",
      "Iteration 1542, loss = 0.15671840\n",
      "Iteration 277, loss = 0.36120060\n",
      "Iteration 278, loss = 0.36081375\n",
      "Iteration 238, loss = 0.34954612\n",
      "Iteration 107, loss = 0.45740498\n",
      "Iteration 239, loss = 0.34933596\n",
      "Iteration 2522, loss = 0.15619587\n",
      "Iteration 1543, loss = 0.15655806\n",
      "Iteration 279, loss = 0.36042656\n",
      "Iteration 503, loss = 0.38575400\n",
      "Iteration 240, loss = 0.34922039\n",
      "Iteration 1544, loss = 0.15643191\n",
      "Iteration 280, loss = 0.36006845\n",
      "Iteration 108, loss = 0.45589915\n",
      "Iteration 241, loss = 0.34893927\n",
      "Iteration 2523, loss = 0.15600235\n",
      "Iteration 1545, loss = 0.15625863\n",
      "Iteration 281, loss = 0.35967527\n",
      "Iteration 282, loss = 0.35924187\n",
      "Iteration 1470, loss = 0.25404400\n",
      "Iteration 1375, loss = 0.22587607\n",
      "Iteration 283, loss = 0.35894927\n",
      "Iteration 242, loss = 0.34872970\n",
      "Iteration 1546, loss = 0.15612386\n",
      "Iteration 243, loss = 0.34854795\n",
      "Iteration 284, loss = 0.35853402\n",
      "Iteration 1471, loss = 0.25394999\n",
      "Iteration 109, loss = 0.45438056\n",
      "Iteration 244, loss = 0.34835028\n",
      "Iteration 1547, loss = 0.15608510\n",
      "Iteration 285, loss = 0.35811135\n",
      "Iteration 245, loss = 0.34815268\n",
      "Iteration 2524, loss = 0.15601061\n",
      "Iteration 286, loss = 0.35775285\n",
      "Iteration 1376, loss = 0.22569727\n",
      "Iteration 1548, loss = 0.15588213\n",
      "Iteration 287, loss = 0.35738095\n",
      "Iteration 504, loss = 0.38565256\n",
      "Iteration 246, loss = 0.34795547\n",
      "Iteration 288, loss = 0.35705931\n",
      "Iteration 110, loss = 0.45282638\n",
      "Iteration 2525, loss = 0.15589851\n",
      "Iteration 1472, loss = 0.25372052\n",
      "Iteration 289, loss = 0.35665978\n",
      "Iteration 290, loss = 0.35627501\n",
      "Iteration 291, loss = 0.35591746\n",
      "Iteration 1377, loss = 0.22552783\n",
      "Iteration 292, loss = 0.35557527\n",
      "Iteration 2526, loss = 0.15588022\n",
      "Iteration 1549, loss = 0.15570626\n",
      "Iteration 293, loss = 0.35519151\n",
      "Iteration 294, loss = 0.35486046\n",
      "Iteration 111, loss = 0.45134532\n",
      "Iteration 295, loss = 0.35445539\n",
      "Iteration 247, loss = 0.34779683\n",
      "Iteration 1473, loss = 0.25360590\n",
      "Iteration 1550, loss = 0.15578648\n",
      "Iteration 505, loss = 0.38548746\n",
      "Iteration 112, loss = 0.44997139\n",
      "Iteration 248, loss = 0.34760181\n",
      "Iteration 2527, loss = 0.15576501\n",
      "Iteration 1474, loss = 0.25348083\n",
      "Iteration 296, loss = 0.35409032\n",
      "Iteration 1378, loss = 0.22542402\n",
      "Iteration 506, loss = 0.38529418\n",
      "Iteration 249, loss = 0.34742543\n",
      "Iteration 1475, loss = 0.25341312\n",
      "Iteration 297, loss = 0.35377900Iteration 113, loss = 0.44842442\n",
      "Iteration 1551, loss = 0.15553500\n",
      "\n",
      "Iteration 2528, loss = 0.15569228\n",
      "Iteration 250, loss = 0.34722351\n",
      "Iteration 298, loss = 0.35336097\n",
      "Iteration 1476, loss = 0.25322824\n",
      "Iteration 251, loss = 0.34704605\n",
      "Iteration 299, loss = 0.35306963\n",
      "Iteration 252, loss = 0.34691433\n",
      "Iteration 300, loss = 0.35274844\n",
      "Iteration 1379, loss = 0.22517901\n",
      "Iteration 1552, loss = 0.15535826\n",
      "Iteration 301, loss = 0.35233787\n",
      "Iteration 2529, loss = 0.15575858\n",
      "Iteration 1477, loss = 0.25309876\n",
      "Iteration 114, loss = 0.44716709\n",
      "Iteration 507, loss = 0.38517428\n",
      "Iteration 253, loss = 0.34666077\n",
      "Iteration 302, loss = 0.35203855\n",
      "Iteration 1553, loss = 0.15528626\n",
      "Iteration 1478, loss = 0.25299058\n",
      "Iteration 2530, loss = 0.15551549\n",
      "Iteration 254, loss = 0.34647125\n",
      "Iteration 303, loss = 0.35166721\n",
      "Iteration 1380, loss = 0.22510300\n",
      "Iteration 304, loss = 0.35132460\n",
      "Iteration 305, loss = 0.35102266\n",
      "Iteration 2531, loss = 0.15543565\n",
      "Iteration 115, loss = 0.44580821\n",
      "Iteration 255, loss = 0.34632374\n",
      "Iteration 1479, loss = 0.25288572\n",
      "Iteration 1554, loss = 0.15508787\n",
      "Iteration 508, loss = 0.38498758\n",
      "Iteration 256, loss = 0.34613882\n",
      "Iteration 306, loss = 0.35069279\n",
      "Iteration 116, loss = 0.44454178\n",
      "Iteration 257, loss = 0.34596110\n",
      "Iteration 307, loss = 0.35039923\n",
      "Iteration 1555, loss = 0.15483166\n",
      "Iteration 1381, loss = 0.22488674\n",
      "Iteration 308, loss = 0.35006923\n",
      "Iteration 1480, loss = 0.25278937\n",
      "Iteration 309, loss = 0.34971285\n",
      "Iteration 258, loss = 0.34577983\n",
      "Iteration 310, loss = 0.34939631\n",
      "Iteration 2532, loss = 0.15545410\n",
      "Iteration 117, loss = 0.44322137\n",
      "Iteration 259, loss = 0.34562788\n",
      "Iteration 311, loss = 0.34908589\n",
      "Iteration 509, loss = 0.38488748\n",
      "Iteration 260, loss = 0.34543644\n",
      "Iteration 312, loss = 0.34878226\n",
      "Iteration 1556, loss = 0.15477466\n",
      "Iteration 1481, loss = 0.25261015\n",
      "Iteration 118, loss = 0.44206378\n",
      "Iteration 261, loss = 0.34523027\n",
      "Iteration 313, loss = 0.34846952\n",
      "Iteration 1382, loss = 0.22469737\n",
      "Iteration 1482, loss = 0.25249759\n",
      "Iteration 510, loss = 0.38467703\n",
      "Iteration 119, loss = 0.44075274\n",
      "Iteration 2533, loss = 0.15529971\n",
      "Iteration 1557, loss = 0.15462112\n",
      "Iteration 262, loss = 0.34508777\n",
      "Iteration 314, loss = 0.34816848\n",
      "Iteration 263, loss = 0.34491326\n",
      "Iteration 315, loss = 0.34786500\n",
      "Iteration 120, loss = 0.43957629\n",
      "Iteration 316, loss = 0.34751593\n",
      "Iteration 264, loss = 0.34474325\n",
      "Iteration 1383, loss = 0.22468113\n",
      "Iteration 317, loss = 0.34720782\n",
      "Iteration 318, loss = 0.34688647\n",
      "Iteration 319, loss = 0.34667357\n",
      "Iteration 1483, loss = 0.25247035\n",
      "Iteration 265, loss = 0.34457148\n",
      "Iteration 121, loss = 0.43841259\n",
      "Iteration 320, loss = 0.34630296\n",
      "Iteration 2534, loss = 0.15527502\n",
      "Iteration 321, loss = 0.34599189\n",
      "Iteration 1558, loss = 0.15441881\n",
      "Iteration 1384, loss = 0.22450686\n",
      "Iteration 511, loss = 0.38456099\n",
      "Iteration 122, loss = 0.43727210\n",
      "Iteration 266, loss = 0.34439162\n",
      "Iteration 322, loss = 0.34571061\n",
      "Iteration 267, loss = 0.34421677\n",
      "Iteration 323, loss = 0.34539492\n",
      "Iteration 1559, loss = 0.15437272\n",
      "Iteration 324, loss = 0.34508955\n",
      "Iteration 268, loss = 0.34407863\n",
      "Iteration 325, loss = 0.34485418\n",
      "Iteration 2535, loss = 0.15519664\n",
      "Iteration 326, loss = 0.34449684\n",
      "Iteration 1560, loss = 0.15422307\n",
      "Iteration 269, loss = 0.34389339\n",
      "Iteration 1385, loss = 0.22436517\n",
      "Iteration 327, loss = 0.34422664\n",
      "Iteration 328, loss = 0.34391946\n",
      "Iteration 270, loss = 0.34373332\n",
      "Iteration 1561, loss = 0.15391595\n",
      "Iteration 1484, loss = 0.25223447\n",
      "Iteration 271, loss = 0.34356329\n",
      "Iteration 123, loss = 0.43611486\n",
      "Iteration 272, loss = 0.34344365\n",
      "Iteration 1386, loss = 0.22407006\n",
      "Iteration 273, loss = 0.34325150\n",
      "Iteration 2536, loss = 0.15506208\n",
      "Iteration 124, loss = 0.43501029\n",
      "Iteration 329, loss = 0.34361335\n",
      "Iteration 1485, loss = 0.25214269\n",
      "Iteration 1562, loss = 0.15382114\n",
      "Iteration 512, loss = 0.38445396\n",
      "Iteration 274, loss = 0.34309229\n",
      "Iteration 330, loss = 0.34338347\n",
      "Iteration 1387, loss = 0.22386145\n",
      "Iteration 2537, loss = 0.15508060\n",
      "Iteration 275, loss = 0.34294457\n",
      "Iteration 1563, loss = 0.15361874\n",
      "Iteration 276, loss = 0.34280206\n",
      "Iteration 331, loss = 0.34305157\n",
      "Iteration 277, loss = 0.34261125\n",
      "Iteration 332, loss = 0.34275048\n",
      "Iteration 125, loss = 0.43407961\n",
      "Iteration 2538, loss = 0.15499989\n",
      "Iteration 333, loss = 0.34253965\n",
      "Iteration 1564, loss = 0.15363539\n",
      "Iteration 1388, loss = 0.22382179\n",
      "Iteration 334, loss = 0.34221691\n",
      "Iteration 1486, loss = 0.25199204\n",
      "Iteration 335, loss = 0.34192362\n",
      "Iteration 336, loss = 0.34168833\n",
      "Iteration 278, loss = 0.34247834\n",
      "Iteration 126, loss = 0.43288619\n",
      "Iteration 2539, loss = 0.15488351\n",
      "Iteration 1565, loss = 0.15335936\n",
      "Iteration 337, loss = 0.34140105\n",
      "Iteration 1389, loss = 0.22352947\n",
      "Iteration 513, loss = 0.38423492\n",
      "Iteration 279, loss = 0.34235195\n",
      "Iteration 338, loss = 0.34113938\n",
      "Iteration 339, loss = 0.34081361\n",
      "Iteration 1487, loss = 0.25186437\n",
      "Iteration 2540, loss = 0.15484794Iteration 280, loss = 0.34215000\n",
      "\n",
      "Iteration 340, loss = 0.34059874\n",
      "Iteration 1566, loss = 0.15318086\n",
      "Iteration 281, loss = 0.34200855\n",
      "Iteration 341, loss = 0.34026604\n",
      "Iteration 127, loss = 0.43188406\n",
      "Iteration 1390, loss = 0.22345374\n",
      "Iteration 1488, loss = 0.25176152\n",
      "Iteration 342, loss = 0.34002360\n",
      "Iteration 343, loss = 0.33973781\n",
      "Iteration 344, loss = 0.33949651\n",
      "Iteration 282, loss = 0.34185921\n",
      "Iteration 1567, loss = 0.15309772\n",
      "Iteration 345, loss = 0.33922491\n",
      "Iteration 514, loss = 0.38411467\n",
      "Iteration 2541, loss = 0.15487346\n",
      "Iteration 1391, loss = 0.22314878\n",
      "Iteration 346, loss = 0.33895335\n",
      "Iteration 1489, loss = 0.25177124\n",
      "Iteration 1568, loss = 0.15288646\n",
      "Iteration 128, loss = 0.43085869\n",
      "Iteration 283, loss = 0.34170701\n",
      "Iteration 347, loss = 0.33870964\n",
      "Iteration 1569, loss = 0.15283489\n",
      "Iteration 2542, loss = 0.15468382\n",
      "Iteration 348, loss = 0.33842447\n",
      "Iteration 1490, loss = 0.25149348\n",
      "Iteration 284, loss = 0.34155942\n",
      "Iteration 349, loss = 0.33815945\n",
      "Iteration 350, loss = 0.33789671\n",
      "Iteration 515, loss = 0.38393541\n",
      "Iteration 1392, loss = 0.22312175\n",
      "Iteration 351, loss = 0.33763438\n",
      "Iteration 285, loss = 0.34142912\n",
      "Iteration 2543, loss = 0.15461044\n",
      "Iteration 129, loss = 0.42988937\n",
      "Iteration 352, loss = 0.33739622\n",
      "Iteration 1570, loss = 0.15262529\n",
      "Iteration 353, loss = 0.33713619\n",
      "Iteration 1491, loss = 0.25138503\n",
      "Iteration 354, loss = 0.33690790\n",
      "Iteration 286, loss = 0.34132244\n",
      "Iteration 2544, loss = 0.15474991\n",
      "Iteration 1393, loss = 0.22309798\n",
      "Iteration 355, loss = 0.33662385\n",
      "Iteration 516, loss = 0.38378390\n",
      "Iteration 130, loss = 0.42891976\n",
      "Iteration 356, loss = 0.33639966\n",
      "Iteration 357, loss = 0.33617207\n",
      "Iteration 1492, loss = 0.25127033\n",
      "Iteration 1571, loss = 0.15260591\n",
      "Iteration 287, loss = 0.34115233\n",
      "Iteration 358, loss = 0.33595159\n",
      "Iteration 2545, loss = 0.15442783\n",
      "Iteration 359, loss = 0.33567229\n",
      "Iteration 517, loss = 0.38362752\n",
      "Iteration 288, loss = 0.34096207\n",
      "Iteration 360, loss = 0.33542993\n",
      "Iteration 131, loss = 0.42798314\n",
      "Iteration 1394, loss = 0.22278434\n",
      "Iteration 1493, loss = 0.25114127\n",
      "Iteration 361, loss = 0.33520361\n",
      "Iteration 1572, loss = 0.15251435\n",
      "Iteration 362, loss = 0.33497844\n",
      "Iteration 1494, loss = 0.25102157\n",
      "Iteration 1573, loss = 0.15223819\n",
      "Iteration 2546, loss = 0.15441195\n",
      "Iteration 363, loss = 0.33474352\n",
      "Iteration 364, loss = 0.33449666\n",
      "Iteration 289, loss = 0.34081475\n",
      "Iteration 1574, loss = 0.15221357\n",
      "Iteration 1495, loss = 0.25088815\n",
      "Iteration 518, loss = 0.38352260\n",
      "Iteration 365, loss = 0.33426537\n",
      "Iteration 132, loss = 0.42703333\n",
      "Iteration 290, loss = 0.34067610\n",
      "Iteration 366, loss = 0.33405220\n",
      "Iteration 1395, loss = 0.22250572\n",
      "Iteration 2547, loss = 0.15432503\n",
      "Iteration 1575, loss = 0.15198654\n",
      "Iteration 367, loss = 0.33377846\n",
      "Iteration 291, loss = 0.34055110\n",
      "Iteration 133, loss = 0.42610149\n",
      "Iteration 368, loss = 0.33356393\n",
      "Iteration 519, loss = 0.38334383\n",
      "Iteration 1576, loss = 0.15186977\n",
      "Iteration 1496, loss = 0.25076006\n",
      "Iteration 292, loss = 0.34039509\n",
      "Iteration 369, loss = 0.33332602Iteration 1396, loss = 0.22243014\n",
      "\n",
      "Iteration 293, loss = 0.34023853\n",
      "Iteration 370, loss = 0.33315802\n",
      "Iteration 2548, loss = 0.15428094\n",
      "Iteration 371, loss = 0.33288761\n",
      "Iteration 134, loss = 0.42522493\n",
      "Iteration 1577, loss = 0.15181095\n",
      "Iteration 372, loss = 0.33265179\n",
      "Iteration 294, loss = 0.34009392\n",
      "Iteration 373, loss = 0.33246650\n",
      "Iteration 520, loss = 0.38315711\n",
      "Iteration 1497, loss = 0.25064840\n",
      "Iteration 1578, loss = 0.15163914\n",
      "Iteration 135, loss = 0.42436674\n",
      "Iteration 1579, loss = 0.15160441\n",
      "Iteration 295, loss = 0.33997055\n",
      "Iteration 374, loss = 0.33219905\n",
      "Iteration 2549, loss = 0.15416761\n",
      "Iteration 136, loss = 0.42351641\n",
      "Iteration 1580, loss = 0.15123830\n",
      "Iteration 375, loss = 0.33205040\n",
      "Iteration 296, loss = 0.33982840\n",
      "Iteration 1397, loss = 0.22228042\n",
      "Iteration 521, loss = 0.38304853\n",
      "Iteration 376, loss = 0.33175317\n",
      "Iteration 2550, loss = 0.15425858\n",
      "Iteration 1581, loss = 0.15108084\n",
      "Iteration 137, loss = 0.42263536\n",
      "Iteration 1498, loss = 0.25052328\n",
      "Iteration 377, loss = 0.33151625\n",
      "Iteration 297, loss = 0.33966478\n",
      "Iteration 378, loss = 0.33130112\n",
      "Iteration 1582, loss = 0.15114913\n",
      "Iteration 379, loss = 0.33106151\n",
      "Iteration 380, loss = 0.33089177\n",
      "Iteration 381, loss = 0.33063877\n",
      "Iteration 1398, loss = 0.22201880\n",
      "Iteration 1583, loss = 0.15078798\n",
      "Iteration 138, loss = 0.42181637\n",
      "Iteration 382, loss = 0.33042029\n",
      "Iteration 298, loss = 0.33961576\n",
      "Iteration 383, loss = 0.33019744\n",
      "Iteration 522, loss = 0.38284848\n",
      "Iteration 384, loss = 0.32998610\n",
      "Iteration 1584, loss = 0.15068341\n",
      "Iteration 2551, loss = 0.15406660\n",
      "Iteration 385, loss = 0.32975812\n",
      "Iteration 299, loss = 0.33939549\n",
      "Iteration 1399, loss = 0.22190718\n",
      "Iteration 1585, loss = 0.15052556\n",
      "Iteration 386, loss = 0.32954084\n",
      "Iteration 139, loss = 0.42103082\n",
      "Iteration 300, loss = 0.33929442\n",
      "Iteration 523, loss = 0.38268398\n",
      "Iteration 301, loss = 0.33909906\n",
      "Iteration 2552, loss = 0.15404882\n",
      "Iteration 387, loss = 0.32929411\n",
      "Iteration 1586, loss = 0.15036679\n",
      "Iteration 302, loss = 0.33898044\n",
      "Iteration 388, loss = 0.32910445\n",
      "Iteration 1400, loss = 0.22171473\n",
      "Iteration 389, loss = 0.32887930\n",
      "Iteration 303, loss = 0.33883258\n",
      "Iteration 2553, loss = 0.15392573\n",
      "Iteration 390, loss = 0.32865755\n",
      "Iteration 140, loss = 0.42019887\n",
      "Iteration 304, loss = 0.33870216\n",
      "Iteration 391, loss = 0.32853295\n",
      "Iteration 2554, loss = 0.15382159\n",
      "Iteration 524, loss = 0.38253910\n",
      "Iteration 1587, loss = 0.15050597\n",
      "Iteration 392, loss = 0.32829931\n",
      "Iteration 305, loss = 0.33857046\n",
      "Iteration 141, loss = 0.41942767\n",
      "Iteration 393, loss = 0.32801775\n",
      "Iteration 2555, loss = 0.15384188\n",
      "Iteration 306, loss = 0.33842914\n",
      "Iteration 1401, loss = 0.22151262\n",
      "Iteration 394, loss = 0.32783223\n",
      "Iteration 525, loss = 0.38237395\n",
      "Iteration 142, loss = 0.41870075\n",
      "Iteration 395, loss = 0.32762560\n",
      "Iteration 1588, loss = 0.15009693\n",
      "Iteration 307, loss = 0.33831265\n",
      "Iteration 396, loss = 0.32742527\n",
      "Iteration 397, loss = 0.32726582\n",
      "Iteration 2556, loss = 0.15368255\n",
      "Iteration 143, loss = 0.41794132\n",
      "Iteration 398, loss = 0.32699123\n",
      "Iteration 526, loss = 0.38225973\n",
      "Iteration 308, loss = 0.33817323Iteration 399, loss = 0.32686252\n",
      "\n",
      "Iteration 400, loss = 0.32658998\n",
      "Iteration 1589, loss = 0.15002042\n",
      "Iteration 401, loss = 0.32640020\n",
      "Iteration 1402, loss = 0.22140639\n",
      "Iteration 144, loss = 0.41716749\n",
      "Iteration 402, loss = 0.32621660\n",
      "Iteration 309, loss = 0.33804807\n",
      "Iteration 403, loss = 0.32604133\n",
      "Iteration 2557, loss = 0.15386235\n",
      "Iteration 527, loss = 0.38209130\n",
      "Iteration 404, loss = 0.32580486\n",
      "Iteration 405, loss = 0.32563459\n",
      "Iteration 1590, loss = 0.15008130\n",
      "Iteration 145, loss = 0.41648127\n",
      "Iteration 406, loss = 0.32540016\n",
      "Iteration 407, loss = 0.32519272\n",
      "Iteration 310, loss = 0.33787504\n",
      "Iteration 2558, loss = 0.15356796\n",
      "Iteration 408, loss = 0.32503468\n",
      "Iteration 146, loss = 0.41580911\n",
      "Iteration 311, loss = 0.33777222\n",
      "Iteration 409, loss = 0.32480433\n",
      "Iteration 1403, loss = 0.22119639\n",
      "Iteration 410, loss = 0.32460914\n",
      "Iteration 1591, loss = 0.14969055\n",
      "Iteration 312, loss = 0.33762419\n",
      "Iteration 411, loss = 0.32441331\n",
      "Iteration 412, loss = 0.32424001\n",
      "Iteration 528, loss = 0.38194201\n",
      "Iteration 147, loss = 0.41503666\n",
      "Iteration 313, loss = 0.33748965\n",
      "Iteration 413, loss = 0.32404447\n",
      "Iteration 1592, loss = 0.14960525\n",
      "Iteration 414, loss = 0.32386008\n",
      "Iteration 314, loss = 0.33737371\n",
      "Iteration 2559, loss = 0.15348100\n",
      "Iteration 1404, loss = 0.22112654\n",
      "Iteration 1593, loss = 0.14977777\n",
      "Iteration 415, loss = 0.32365227\n",
      "Iteration 148, loss = 0.41435050\n",
      "Iteration 416, loss = 0.32346733\n",
      "Iteration 315, loss = 0.33727983\n",
      "Iteration 417, loss = 0.32330602\n",
      "Iteration 2560, loss = 0.15346296\n",
      "Iteration 1594, loss = 0.14947729\n",
      "Iteration 418, loss = 0.32310571\n",
      "Iteration 529, loss = 0.38174713\n",
      "Iteration 316, loss = 0.33712550\n",
      "Iteration 149, loss = 0.41365909\n",
      "Iteration 317, loss = 0.33698665\n",
      "Iteration 419, loss = 0.32290215Iteration 1595, loss = 0.14941838\n",
      "\n",
      "Iteration 2561, loss = 0.15336050\n",
      "Iteration 318, loss = 0.33686772\n",
      "Iteration 420, loss = 0.32273738\n",
      "Iteration 1405, loss = 0.22082213\n",
      "Iteration 421, loss = 0.32251427\n",
      "Iteration 530, loss = 0.38164148\n",
      "Iteration 422, loss = 0.32234225\n",
      "Iteration 319, loss = 0.33671737\n",
      "Iteration 423, loss = 0.32218559\n",
      "Iteration 2562, loss = 0.15327376\n",
      "Iteration 1499, loss = 0.25038945\n",
      "Iteration 1596, loss = 0.14909487\n",
      "Iteration 424, loss = 0.32196262\n",
      "Iteration 150, loss = 0.41295056\n",
      "Iteration 320, loss = 0.33658956\n",
      "Iteration 531, loss = 0.38148518\n",
      "Iteration 1406, loss = 0.22085436\n",
      "Iteration 2563, loss = 0.15323524\n",
      "Iteration 425, loss = 0.32178469\n",
      "Iteration 1597, loss = 0.14889324\n",
      "Iteration 1500, loss = 0.25026732\n",
      "Iteration 321, loss = 0.33647369\n",
      "Iteration 532, loss = 0.38139504\n",
      "Iteration 1407, loss = 0.22078628\n",
      "Iteration 151, loss = 0.41226757\n",
      "Iteration 1598, loss = 0.14888235\n",
      "Iteration 322, loss = 0.33634167\n",
      "Iteration 426, loss = 0.32161560\n",
      "Iteration 427, loss = 0.32140462\n",
      "Iteration 533, loss = 0.38115710\n",
      "Iteration 1599, loss = 0.14861955\n",
      "Iteration 1501, loss = 0.25017746\n",
      "Iteration 2564, loss = 0.15313345\n",
      "Iteration 323, loss = 0.33622498\n",
      "Iteration 428, loss = 0.32121265\n",
      "Iteration 1600, loss = 0.14863685\n",
      "Iteration 1408, loss = 0.22065069\n",
      "Iteration 429, loss = 0.32105318\n",
      "Iteration 1601, loss = 0.14848286\n",
      "Iteration 152, loss = 0.41160389\n",
      "Iteration 2565, loss = 0.15308767\n",
      "Iteration 324, loss = 0.33609871\n",
      "Iteration 430, loss = 0.32090366\n",
      "Iteration 1502, loss = 0.25006284\n",
      "Iteration 325, loss = 0.33597127\n",
      "Iteration 1409, loss = 0.22024903\n",
      "Iteration 1602, loss = 0.14827144\n",
      "Iteration 431, loss = 0.32066435\n",
      "Iteration 2566, loss = 0.15306266\n",
      "Iteration 432, loss = 0.32048985\n",
      "Iteration 534, loss = 0.38104827\n",
      "Iteration 153, loss = 0.41100236\n",
      "Iteration 326, loss = 0.33586125\n",
      "Iteration 1410, loss = 0.22007133\n",
      "Iteration 1603, loss = 0.14817225\n",
      "Iteration 1503, loss = 0.24991337\n",
      "Iteration 327, loss = 0.33573075\n",
      "Iteration 535, loss = 0.38081843\n",
      "Iteration 433, loss = 0.32030653\n",
      "Iteration 1411, loss = 0.21988595\n",
      "Iteration 1604, loss = 0.14805651\n",
      "Iteration 154, loss = 0.41030288\n",
      "Iteration 1504, loss = 0.24982539\n",
      "Iteration 1605, loss = 0.14779419\n",
      "Iteration 1505, loss = 0.24967962\n",
      "Iteration 1606, loss = 0.14772852\n",
      "Iteration 2567, loss = 0.15294282\n",
      "Iteration 434, loss = 0.32014486\n",
      "Iteration 1412, loss = 0.21972503\n",
      "Iteration 1607, loss = 0.14773328\n",
      "Iteration 1506, loss = 0.24953412\n",
      "Iteration 435, loss = 0.31994830\n",
      "Iteration 155, loss = 0.40968961\n",
      "Iteration 536, loss = 0.38069393\n",
      "Iteration 436, loss = 0.31976035\n",
      "Iteration 1608, loss = 0.14745078\n",
      "Iteration 437, loss = 0.31959890\n",
      "Iteration 328, loss = 0.33562002\n",
      "Iteration 1507, loss = 0.24947942\n",
      "Iteration 156, loss = 0.40907684\n",
      "Iteration 2568, loss = 0.15295935\n",
      "Iteration 1609, loss = 0.14735080\n",
      "Iteration 438, loss = 0.31943161\n",
      "Iteration 329, loss = 0.33548447\n",
      "Iteration 439, loss = 0.31922541\n",
      "Iteration 1610, loss = 0.14742438\n",
      "Iteration 1413, loss = 0.21993222\n",
      "Iteration 537, loss = 0.38058222\n",
      "Iteration 1611, loss = 0.14701240\n",
      "Iteration 440, loss = 0.31910521\n",
      "Iteration 1508, loss = 0.24931471\n",
      "Iteration 157, loss = 0.40841382\n",
      "Iteration 2569, loss = 0.15278582\n",
      "Iteration 330, loss = 0.33534155\n",
      "Iteration 1612, loss = 0.14696819\n",
      "Iteration 441, loss = 0.31889658\n",
      "Iteration 442, loss = 0.31875557\n",
      "Iteration 158, loss = 0.40778885\n",
      "Iteration 2570, loss = 0.15271242\n",
      "Iteration 331, loss = 0.33531186\n",
      "Iteration 1509, loss = 0.24915718\n",
      "Iteration 1414, loss = 0.21942299\n",
      "Iteration 1613, loss = 0.14678499\n",
      "Iteration 538, loss = 0.38035883\n",
      "Iteration 2571, loss = 0.15275205\n",
      "Iteration 332, loss = 0.33516495\n",
      "Iteration 1614, loss = 0.14684072\n",
      "Iteration 443, loss = 0.31853987\n",
      "Iteration 159, loss = 0.40721780\n",
      "Iteration 333, loss = 0.33498030\n",
      "Iteration 1510, loss = 0.24909869\n",
      "Iteration 444, loss = 0.31841692\n",
      "Iteration 334, loss = 0.33484738\n",
      "Iteration 1615, loss = 0.14651363\n",
      "Iteration 445, loss = 0.31822418\n",
      "Iteration 2572, loss = 0.15260154\n",
      "Iteration 1415, loss = 0.21929462\n",
      "Iteration 446, loss = 0.31803307\n",
      "Iteration 1511, loss = 0.24891435\n",
      "Iteration 539, loss = 0.38021578\n",
      "Iteration 447, loss = 0.31784766\n",
      "Iteration 335, loss = 0.33474528\n",
      "Iteration 2573, loss = 0.15254887\n",
      "Iteration 1616, loss = 0.14639414\n",
      "Iteration 160, loss = 0.40665230\n",
      "Iteration 336, loss = 0.33468546\n",
      "Iteration 448, loss = 0.31766578\n",
      "Iteration 337, loss = 0.33449715\n",
      "Iteration 449, loss = 0.31754210\n",
      "Iteration 450, loss = 0.31734487\n",
      "Iteration 1416, loss = 0.21941399\n",
      "Iteration 2574, loss = 0.15243846\n",
      "Iteration 451, loss = 0.31717131\n",
      "Iteration 1512, loss = 0.24881340\n",
      "Iteration 161, loss = 0.40602779\n",
      "Iteration 540, loss = 0.38008631\n",
      "Iteration 1617, loss = 0.14627337\n",
      "Iteration 452, loss = 0.31702656\n",
      "Iteration 338, loss = 0.33439744\n",
      "Iteration 453, loss = 0.31682928\n",
      "Iteration 339, loss = 0.33426441\n",
      "Iteration 2575, loss = 0.15235201\n",
      "Iteration 1513, loss = 0.24865503\n",
      "Iteration 340, loss = 0.33415381\n",
      "Iteration 1618, loss = 0.14616127\n",
      "Iteration 162, loss = 0.40545823\n",
      "Iteration 454, loss = 0.31668523\n",
      "Iteration 455, loss = 0.31649798\n",
      "Iteration 2576, loss = 0.15233660\n",
      "Iteration 341, loss = 0.33404637\n",
      "Iteration 1417, loss = 0.21885302\n",
      "Iteration 456, loss = 0.31634972\n",
      "Iteration 1514, loss = 0.24854356\n",
      "Iteration 457, loss = 0.31617100\n",
      "Iteration 541, loss = 0.37990209\n",
      "Iteration 1619, loss = 0.14598274\n",
      "Iteration 163, loss = 0.40490183\n",
      "Iteration 342, loss = 0.33388834\n",
      "Iteration 458, loss = 0.31600034\n",
      "Iteration 459, loss = 0.31589389\n",
      "Iteration 1620, loss = 0.14597882\n",
      "Iteration 2577, loss = 0.15222521\n",
      "Iteration 343, loss = 0.33379585\n",
      "Iteration 460, loss = 0.31571224\n",
      "Iteration 542, loss = 0.37975971\n",
      "Iteration 344, loss = 0.33368497\n",
      "Iteration 164, loss = 0.40430297\n",
      "Iteration 1621, loss = 0.14574876\n",
      "Iteration 461, loss = 0.31551080\n",
      "Iteration 1515, loss = 0.24845295\n",
      "Iteration 1622, loss = 0.14575556\n",
      "Iteration 2578, loss = 0.15222142\n",
      "Iteration 1418, loss = 0.21894413\n",
      "Iteration 345, loss = 0.33355125\n",
      "Iteration 1623, loss = 0.14552972\n",
      "Iteration 462, loss = 0.31539392\n",
      "Iteration 346, loss = 0.33342454\n",
      "Iteration 2579, loss = 0.15228911\n",
      "Iteration 463, loss = 0.31520603\n",
      "Iteration 543, loss = 0.37958314\n",
      "Iteration 464, loss = 0.31507567\n",
      "Iteration 347, loss = 0.33336048\n",
      "Iteration 165, loss = 0.40376466\n",
      "Iteration 465, loss = 0.31488347\n",
      "Iteration 348, loss = 0.33318970\n",
      "Iteration 1419, loss = 0.21856538\n",
      "Iteration 466, loss = 0.31472822\n",
      "Iteration 349, loss = 0.33310458\n",
      "Iteration 1624, loss = 0.14536927\n",
      "Iteration 2580, loss = 0.15206073\n",
      "Iteration 467, loss = 0.31455423\n",
      "Iteration 1516, loss = 0.24833183\n",
      "Iteration 350, loss = 0.33300552\n",
      "Iteration 166, loss = 0.40322595\n",
      "Iteration 1420, loss = 0.21857763\n",
      "Iteration 2581, loss = 0.15210063\n",
      "Iteration 468, loss = 0.31437819\n",
      "Iteration 2582, loss = 0.15189743\n",
      "Iteration 1517, loss = 0.24816527\n",
      "Iteration 351, loss = 0.33289452\n",
      "Iteration 469, loss = 0.31425930\n",
      "Iteration 167, loss = 0.40266731\n",
      "Iteration 470, loss = 0.31407925\n",
      "Iteration 544, loss = 0.37947424\n",
      "Iteration 471, loss = 0.31389578\n",
      "Iteration 1625, loss = 0.14536094\n",
      "Iteration 472, loss = 0.31376986\n",
      "Iteration 168, loss = 0.40213653\n",
      "Iteration 2583, loss = 0.15182365\n",
      "Iteration 352, loss = 0.33280089\n",
      "Iteration 1421, loss = 0.21862555\n",
      "Iteration 473, loss = 0.31362119\n",
      "Iteration 353, loss = 0.33266204\n",
      "Iteration 1518, loss = 0.24815180\n",
      "Iteration 1626, loss = 0.14516833\n",
      "Iteration 474, loss = 0.31344521\n",
      "Iteration 2584, loss = 0.15177165\n",
      "Iteration 354, loss = 0.33252527\n",
      "Iteration 475, loss = 0.31327744\n",
      "Iteration 169, loss = 0.40159908\n",
      "Iteration 476, loss = 0.31314774\n",
      "Iteration 477, loss = 0.31303213\n",
      "Iteration 355, loss = 0.33245669\n",
      "Iteration 545, loss = 0.37933487\n",
      "Iteration 1627, loss = 0.14502239\n",
      "Iteration 1519, loss = 0.24800029\n",
      "Iteration 478, loss = 0.31282293\n",
      "Iteration 170, loss = 0.40111542\n",
      "Iteration 479, loss = 0.31271560\n",
      "Iteration 1628, loss = 0.14497973\n",
      "Iteration 480, loss = 0.31257432\n",
      "Iteration 1422, loss = 0.21829928\n",
      "Iteration 171, loss = 0.40055097\n",
      "Iteration 1520, loss = 0.24782198\n",
      "Iteration 1629, loss = 0.14477078\n",
      "Iteration 481, loss = 0.31238882\n",
      "Iteration 2585, loss = 0.15172092\n",
      "Iteration 482, loss = 0.31218274\n",
      "Iteration 356, loss = 0.33229919\n",
      "Iteration 546, loss = 0.37912949\n",
      "Iteration 357, loss = 0.33221450\n",
      "Iteration 1521, loss = 0.24774343\n",
      "Iteration 483, loss = 0.31208023\n",
      "Iteration 1423, loss = 0.21803959\n",
      "Iteration 484, loss = 0.31188803\n",
      "Iteration 172, loss = 0.40003753\n",
      "Iteration 2586, loss = 0.15166461\n",
      "Iteration 1630, loss = 0.14475328\n",
      "Iteration 485, loss = 0.31172303\n",
      "Iteration 486, loss = 0.31155797\n",
      "Iteration 547, loss = 0.37896283\n",
      "Iteration 487, loss = 0.31144465\n",
      "Iteration 1522, loss = 0.24756623\n",
      "Iteration 173, loss = 0.39953649\n",
      "Iteration 488, loss = 0.31125407\n",
      "Iteration 489, loss = 0.31110147\n",
      "Iteration 2587, loss = 0.15166068\n",
      "Iteration 490, loss = 0.31095649\n",
      "Iteration 1424, loss = 0.21775316\n",
      "Iteration 1631, loss = 0.14456803\n",
      "Iteration 1523, loss = 0.24745814\n",
      "Iteration 491, loss = 0.31080759\n",
      "Iteration 174, loss = 0.39900884\n",
      "Iteration 492, loss = 0.31064840\n",
      "Iteration 1524, loss = 0.24732493\n",
      "Iteration 548, loss = 0.37882228\n",
      "Iteration 1632, loss = 0.14440228\n",
      "Iteration 493, loss = 0.31049633\n",
      "Iteration 1425, loss = 0.21762686\n",
      "Iteration 358, loss = 0.33212147\n",
      "Iteration 494, loss = 0.31033738\n",
      "Iteration 2588, loss = 0.15183274\n",
      "Iteration 175, loss = 0.39854393\n",
      "Iteration 495, loss = 0.31019608\n",
      "Iteration 496, loss = 0.31003785\n",
      "Iteration 1525, loss = 0.24718250\n",
      "Iteration 1633, loss = 0.14433316\n",
      "Iteration 497, loss = 0.30988123\n",
      "Iteration 176, loss = 0.39801059\n",
      "Iteration 498, loss = 0.30975228\n",
      "Iteration 2589, loss = 0.15152757\n",
      "Iteration 1426, loss = 0.21748098\n",
      "Iteration 1634, loss = 0.14416028\n",
      "Iteration 499, loss = 0.30957903\n",
      "Iteration 549, loss = 0.37863093\n",
      "Iteration 1526, loss = 0.24712788\n",
      "Iteration 500, loss = 0.30945365\n",
      "Iteration 2590, loss = 0.15143102\n",
      "Iteration 1635, loss = 0.14420230\n",
      "Iteration 501, loss = 0.30929604\n",
      "Iteration 1427, loss = 0.21729100\n",
      "Iteration 177, loss = 0.39757164\n",
      "Iteration 1527, loss = 0.24700671\n",
      "Iteration 502, loss = 0.30915465\n",
      "Iteration 2591, loss = 0.15141260\n",
      "Iteration 503, loss = 0.30899431\n",
      "Iteration 1636, loss = 0.14390898\n",
      "Iteration 504, loss = 0.30884978\n",
      "Iteration 359, loss = 0.33200407\n",
      "Iteration 505, loss = 0.30872612\n",
      "Iteration 1637, loss = 0.14377682\n",
      "Iteration 1528, loss = 0.24684664\n",
      "Iteration 1428, loss = 0.21707295\n",
      "Iteration 178, loss = 0.39709447\n",
      "Iteration 506, loss = 0.30858211\n",
      "Iteration 550, loss = 0.37852491\n",
      "Iteration 1638, loss = 0.14360040\n",
      "Iteration 2592, loss = 0.15127278\n",
      "Iteration 507, loss = 0.30844232\n",
      "Iteration 1529, loss = 0.24673268\n",
      "Iteration 1639, loss = 0.14361524\n",
      "Iteration 508, loss = 0.30828840\n",
      "Iteration 1429, loss = 0.21688133\n",
      "Iteration 179, loss = 0.39655422\n",
      "Iteration 509, loss = 0.30817260\n",
      "Iteration 551, loss = 0.37838126\n",
      "Iteration 2593, loss = 0.15112865\n",
      "Iteration 1640, loss = 0.14349299\n",
      "Iteration 510, loss = 0.30800454\n",
      "Iteration 1530, loss = 0.24659969\n",
      "Iteration 180, loss = 0.39609378\n",
      "Iteration 511, loss = 0.30786512\n",
      "Iteration 2594, loss = 0.15117746\n",
      "Iteration 512, loss = 0.30771167\n",
      "Iteration 1531, loss = 0.24657091\n",
      "Iteration 513, loss = 0.30758481\n",
      "Iteration 1641, loss = 0.14329466\n",
      "Iteration 552, loss = 0.37819051\n",
      "Iteration 514, loss = 0.30744253\n",
      "Iteration 181, loss = 0.39564004\n",
      "Iteration 2595, loss = 0.15101175\n",
      "Iteration 1642, loss = 0.14313596\n",
      "Iteration 515, loss = 0.30728841\n",
      "Iteration 1430, loss = 0.21688439\n",
      "Iteration 1532, loss = 0.24634061\n",
      "Iteration 516, loss = 0.30714270\n",
      "Iteration 360, loss = 0.33187429\n",
      "Iteration 517, loss = 0.30701169\n",
      "Iteration 518, loss = 0.30697775\n",
      "Iteration 2596, loss = 0.15096031\n",
      "Iteration 519, loss = 0.30671470\n",
      "Iteration 553, loss = 0.37802259\n",
      "Iteration 1643, loss = 0.14308898\n",
      "Iteration 182, loss = 0.39515080\n",
      "Iteration 520, loss = 0.30660387\n",
      "Iteration 361, loss = 0.33176979\n",
      "Iteration 521, loss = 0.30647516\n",
      "Iteration 522, loss = 0.30631000\n",
      "Iteration 1644, loss = 0.14299861\n",
      "Iteration 523, loss = 0.30615752\n",
      "Iteration 2597, loss = 0.15089116\n",
      "Iteration 1431, loss = 0.21662546\n",
      "Iteration 524, loss = 0.30605185\n",
      "Iteration 362, loss = 0.33165132\n",
      "Iteration 1533, loss = 0.24623679\n",
      "Iteration 525, loss = 0.30588707\n",
      "Iteration 1645, loss = 0.14294935\n",
      "Iteration 183, loss = 0.39469960\n",
      "Iteration 526, loss = 0.30581551\n",
      "Iteration 2598, loss = 0.15079689\n",
      "Iteration 554, loss = 0.37784202\n",
      "Iteration 363, loss = 0.33159453\n",
      "Iteration 527, loss = 0.30563032\n",
      "Iteration 1432, loss = 0.21639013\n",
      "Iteration 1646, loss = 0.14280306\n",
      "Iteration 528, loss = 0.30549212\n",
      "Iteration 2599, loss = 0.15072024\n",
      "Iteration 184, loss = 0.39422762\n",
      "Iteration 364, loss = 0.33146016\n",
      "Iteration 1534, loss = 0.24607886\n",
      "Iteration 529, loss = 0.30531903\n",
      "Iteration 365, loss = 0.33132541\n",
      "Iteration 185, loss = 0.39376249\n",
      "Iteration 1647, loss = 0.14254653\n",
      "Iteration 2600, loss = 0.15065444\n",
      "Iteration 555, loss = 0.37773319\n",
      "Iteration 1648, loss = 0.14248516\n",
      "Iteration 366, loss = 0.33120237\n",
      "Iteration 1535, loss = 0.24599040\n",
      "Iteration 1649, loss = 0.14228854\n",
      "Iteration 530, loss = 0.30518504\n",
      "Iteration 367, loss = 0.33110333\n",
      "Iteration 1650, loss = 0.14247226\n",
      "Iteration 531, loss = 0.30506561\n",
      "Iteration 556, loss = 0.37753244\n",
      "Iteration 368, loss = 0.33102487\n",
      "Iteration 532, loss = 0.30494900\n",
      "Iteration 369, loss = 0.33087481\n",
      "Iteration 186, loss = 0.39330699\n",
      "Iteration 1536, loss = 0.24585339\n",
      "Iteration 533, loss = 0.30482432\n",
      "Iteration 370, loss = 0.33077960\n",
      "Iteration 2601, loss = 0.15072166\n",
      "Iteration 1651, loss = 0.14205183\n",
      "Iteration 534, loss = 0.30468170\n",
      "Iteration 371, loss = 0.33066608\n",
      "Iteration 1433, loss = 0.21634956\n",
      "Iteration 187, loss = 0.39289548\n",
      "Iteration 372, loss = 0.33056450\n",
      "Iteration 535, loss = 0.30450780\n",
      "Iteration 2602, loss = 0.15054389\n",
      "Iteration 373, loss = 0.33045878\n",
      "Iteration 536, loss = 0.30437930\n",
      "Iteration 1652, loss = 0.14200751\n",
      "Iteration 537, loss = 0.30431057\n",
      "Iteration 1537, loss = 0.24571075\n",
      "Iteration 538, loss = 0.30411018\n",
      "Iteration 188, loss = 0.39244329\n",
      "Iteration 2603, loss = 0.15054499\n",
      "Iteration 539, loss = 0.30398412\n",
      "Iteration 540, loss = 0.30384110\n",
      "Iteration 557, loss = 0.37737285\n",
      "Iteration 541, loss = 0.30368561\n",
      "Iteration 1434, loss = 0.21612467\n",
      "Iteration 374, loss = 0.33035622\n",
      "Iteration 542, loss = 0.30359992\n",
      "Iteration 189, loss = 0.39204554\n",
      "Iteration 1653, loss = 0.14176927\n",
      "Iteration 2604, loss = 0.15042057\n",
      "Iteration 543, loss = 0.30342168\n",
      "Iteration 558, loss = 0.37720124\n",
      "Iteration 1538, loss = 0.24564676\n",
      "Iteration 544, loss = 0.30328003\n",
      "Iteration 545, loss = 0.30313700\n",
      "Iteration 375, loss = 0.33023551\n",
      "Iteration 190, loss = 0.39160181\n",
      "Iteration 2605, loss = 0.15034815\n",
      "Iteration 546, loss = 0.30305273\n",
      "Iteration 547, loss = 0.30290945\n",
      "Iteration 548, loss = 0.30274091\n",
      "Iteration 1435, loss = 0.21618802\n",
      "Iteration 191, loss = 0.39117072\n",
      "Iteration 376, loss = 0.33015123\n",
      "Iteration 1654, loss = 0.14174177\n",
      "Iteration 549, loss = 0.30263782\n",
      "Iteration 1539, loss = 0.24548504\n",
      "Iteration 550, loss = 0.30249125\n",
      "Iteration 2606, loss = 0.15029750\n",
      "Iteration 551, loss = 0.30238445\n",
      "Iteration 559, loss = 0.37709395\n",
      "Iteration 552, loss = 0.30223332\n",
      "Iteration 377, loss = 0.33001859\n",
      "Iteration 553, loss = 0.30207275\n",
      "Iteration 192, loss = 0.39074109\n",
      "Iteration 378, loss = 0.32991665\n",
      "Iteration 554, loss = 0.30194418\n",
      "Iteration 1540, loss = 0.24542968\n",
      "Iteration 1655, loss = 0.14160732\n",
      "Iteration 1436, loss = 0.21597127\n",
      "Iteration 379, loss = 0.32978895\n",
      "Iteration 2607, loss = 0.15023112\n",
      "Iteration 555, loss = 0.30186067\n",
      "Iteration 193, loss = 0.39031833\n",
      "Iteration 380, loss = 0.32969882\n",
      "Iteration 556, loss = 0.30168502\n",
      "Iteration 381, loss = 0.32959644\n",
      "Iteration 557, loss = 0.30158277\n",
      "Iteration 2608, loss = 0.15019409\n",
      "Iteration 194, loss = 0.38991919\n",
      "Iteration 558, loss = 0.30145485\n",
      "Iteration 1656, loss = 0.14152871\n",
      "Iteration 559, loss = 0.30130128\n",
      "Iteration 1541, loss = 0.24526783\n",
      "Iteration 560, loss = 0.30122610\n",
      "Iteration 560, loss = 0.37687732\n",
      "Iteration 382, loss = 0.32949956\n",
      "Iteration 561, loss = 0.30105054\n",
      "Iteration 2609, loss = 0.15018399\n",
      "Iteration 1542, loss = 0.24514433\n",
      "Iteration 1657, loss = 0.14136353\n",
      "Iteration 195, loss = 0.38949390\n",
      "Iteration 383, loss = 0.32940917\n",
      "Iteration 1437, loss = 0.21566333\n",
      "Iteration 1658, loss = 0.14123206\n",
      "Iteration 561, loss = 0.37672063\n",
      "Iteration 562, loss = 0.30090807\n",
      "Iteration 563, loss = 0.30080810\n",
      "Iteration 2610, loss = 0.15005997\n",
      "Iteration 1659, loss = 0.14109281\n",
      "Iteration 384, loss = 0.32928417\n",
      "Iteration 564, loss = 0.30066942\n",
      "Iteration 565, loss = 0.30055877\n",
      "Iteration 562, loss = 0.37659940\n",
      "Iteration 1543, loss = 0.24500966\n",
      "Iteration 1660, loss = 0.14102885\n",
      "Iteration 566, loss = 0.30040723\n",
      "Iteration 196, loss = 0.38911543\n",
      "Iteration 385, loss = 0.32916122\n",
      "Iteration 567, loss = 0.30028790\n",
      "Iteration 2611, loss = 0.14995422\n",
      "Iteration 563, loss = 0.37643616\n",
      "Iteration 568, loss = 0.30015167\n",
      "Iteration 1438, loss = 0.21561132\n",
      "Iteration 1544, loss = 0.24488740\n",
      "Iteration 197, loss = 0.38872459\n",
      "Iteration 386, loss = 0.32906196\n",
      "Iteration 569, loss = 0.30003292\n",
      "Iteration 1661, loss = 0.14088003\n",
      "Iteration 570, loss = 0.29992834\n",
      "Iteration 387, loss = 0.32900088\n",
      "Iteration 2612, loss = 0.14991415\n",
      "Iteration 1545, loss = 0.24475236\n",
      "Iteration 388, loss = 0.32886483\n",
      "Iteration 389, loss = 0.32875167\n",
      "Iteration 1662, loss = 0.14079797\n",
      "Iteration 390, loss = 0.32867108\n",
      "Iteration 1546, loss = 0.24462960\n",
      "Iteration 1663, loss = 0.14062808\n",
      "Iteration 391, loss = 0.32854528\n",
      "Iteration 1664, loss = 0.14049782\n",
      "Iteration 1547, loss = 0.24456174\n",
      "Iteration 392, loss = 0.32845800\n",
      "Iteration 2613, loss = 0.14983756\n",
      "Iteration 393, loss = 0.32841373\n",
      "Iteration 1665, loss = 0.14038694\n",
      "Iteration 1439, loss = 0.21531901\n",
      "Iteration 1548, loss = 0.24438320\n",
      "Iteration 394, loss = 0.32823858\n",
      "Iteration 2614, loss = 0.14980380\n",
      "Iteration 1666, loss = 0.14031451\n",
      "Iteration 571, loss = 0.29976087\n",
      "Iteration 198, loss = 0.38830636\n",
      "Iteration 395, loss = 0.32811700\n",
      "Iteration 572, loss = 0.29971228\n",
      "Iteration 1549, loss = 0.24433535Iteration 2615, loss = 0.14968038\n",
      "\n",
      "Iteration 573, loss = 0.29953199\n",
      "Iteration 574, loss = 0.29951957\n",
      "Iteration 564, loss = 0.37625538\n",
      "Iteration 575, loss = 0.29930937\n",
      "Iteration 576, loss = 0.29920522\n",
      "Iteration 199, loss = 0.38791274\n",
      "Iteration 2616, loss = 0.14967438\n",
      "Iteration 1667, loss = 0.14039112\n",
      "Iteration 396, loss = 0.32803900\n",
      "Iteration 1550, loss = 0.24417648\n",
      "Iteration 2617, loss = 0.14954153\n",
      "Iteration 1668, loss = 0.14001940\n",
      "Iteration 565, loss = 0.37615804\n",
      "Iteration 200, loss = 0.38755078\n",
      "Iteration 577, loss = 0.29901855\n",
      "Iteration 1440, loss = 0.21519852\n",
      "Iteration 578, loss = 0.29890312\n",
      "Iteration 397, loss = 0.32791616\n",
      "Iteration 1669, loss = 0.13996274\n",
      "Iteration 201, loss = 0.38714744\n",
      "Iteration 2618, loss = 0.14946750\n",
      "Iteration 579, loss = 0.29877753\n",
      "Iteration 1551, loss = 0.24402829\n",
      "Iteration 580, loss = 0.29868516\n",
      "Iteration 398, loss = 0.32782044\n",
      "Iteration 581, loss = 0.29859713\n",
      "Iteration 399, loss = 0.32773644\n",
      "Iteration 582, loss = 0.29839182\n",
      "Iteration 1552, loss = 0.24388720\n",
      "Iteration 583, loss = 0.29828694\n",
      "Iteration 2619, loss = 0.14945284\n",
      "Iteration 1670, loss = 0.13991748\n",
      "Iteration 400, loss = 0.32763136\n",
      "Iteration 584, loss = 0.29816551\n",
      "Iteration 202, loss = 0.38681407\n",
      "Iteration 566, loss = 0.37595029\n",
      "Iteration 1671, loss = 0.14011118\n",
      "Iteration 401, loss = 0.32751065\n",
      "Iteration 1441, loss = 0.21501231\n",
      "Iteration 203, loss = 0.38638430\n",
      "Iteration 402, loss = 0.32741031\n",
      "Iteration 585, loss = 0.29809078\n",
      "Iteration 1672, loss = 0.13957562\n",
      "Iteration 586, loss = 0.29791033\n",
      "Iteration 2620, loss = 0.14932880\n",
      "Iteration 1553, loss = 0.24375673\n",
      "Iteration 403, loss = 0.32735878\n",
      "Iteration 587, loss = 0.29779997\n",
      "Iteration 567, loss = 0.37580900\n",
      "Iteration 1673, loss = 0.13965871\n",
      "Iteration 588, loss = 0.29776553\n",
      "Iteration 589, loss = 0.29755792\n",
      "Iteration 2621, loss = 0.14929714\n",
      "Iteration 590, loss = 0.29751983\n",
      "Iteration 404, loss = 0.32721704\n",
      "Iteration 1554, loss = 0.24372637\n",
      "Iteration 1674, loss = 0.13938177\n",
      "Iteration 204, loss = 0.38604459\n",
      "Iteration 591, loss = 0.29738418\n",
      "Iteration 592, loss = 0.29718853\n",
      "Iteration 405, loss = 0.32709715\n",
      "Iteration 1442, loss = 0.21482014\n",
      "Iteration 2622, loss = 0.14920799\n",
      "Iteration 205, loss = 0.38568973\n",
      "Iteration 568, loss = 0.37560463\n",
      "Iteration 406, loss = 0.32703626\n",
      "Iteration 593, loss = 0.29704996\n",
      "Iteration 1555, loss = 0.24356766\n",
      "Iteration 407, loss = 0.32688879\n",
      "Iteration 1675, loss = 0.13970708\n",
      "Iteration 1443, loss = 0.21468104\n",
      "Iteration 206, loss = 0.38530357\n",
      "Iteration 594, loss = 0.29693998\n",
      "Iteration 2623, loss = 0.14917490\n",
      "Iteration 408, loss = 0.32679303\n",
      "Iteration 595, loss = 0.29683092\n",
      "Iteration 596, loss = 0.29670033\n",
      "Iteration 569, loss = 0.37556402\n",
      "Iteration 207, loss = 0.38503668\n",
      "Iteration 597, loss = 0.29657402\n",
      "Iteration 598, loss = 0.29646934\n",
      "Iteration 1676, loss = 0.13909692\n",
      "Iteration 1556, loss = 0.24338654\n",
      "Iteration 599, loss = 0.29641606\n",
      "Iteration 409, loss = 0.32671646\n",
      "Iteration 600, loss = 0.29621110\n",
      "Iteration 601, loss = 0.29610268\n",
      "Iteration 208, loss = 0.38461108\n",
      "Iteration 602, loss = 0.29600328\n",
      "Iteration 410, loss = 0.32657497\n",
      "Iteration 2624, loss = 0.14918892\n",
      "Iteration 603, loss = 0.29585299\n",
      "Iteration 411, loss = 0.32650493\n",
      "Iteration 604, loss = 0.29572894\n",
      "Iteration 570, loss = 0.37530554\n",
      "Iteration 1444, loss = 0.21448719\n",
      "Iteration 412, loss = 0.32637591\n",
      "Iteration 605, loss = 0.29561322\n",
      "Iteration 209, loss = 0.38423189\n",
      "Iteration 413, loss = 0.32631321\n",
      "Iteration 606, loss = 0.29550702\n",
      "Iteration 1677, loss = 0.13899205\n",
      "Iteration 1557, loss = 0.24349860\n",
      "Iteration 414, loss = 0.32619798\n",
      "Iteration 2625, loss = 0.14907027\n",
      "Iteration 415, loss = 0.32608135\n",
      "Iteration 607, loss = 0.29539699\n",
      "Iteration 1678, loss = 0.13889402\n",
      "Iteration 571, loss = 0.37514313\n",
      "Iteration 416, loss = 0.32599371\n",
      "Iteration 2626, loss = 0.14894681\n",
      "Iteration 417, loss = 0.32587022\n",
      "Iteration 608, loss = 0.29524626\n",
      "Iteration 1679, loss = 0.13879787\n",
      "Iteration 418, loss = 0.32577614\n",
      "Iteration 609, loss = 0.29516729\n",
      "Iteration 419, loss = 0.32568923\n",
      "Iteration 610, loss = 0.29503053\n",
      "Iteration 210, loss = 0.38393543\n",
      "Iteration 1445, loss = 0.21437425\n",
      "Iteration 1680, loss = 0.13866789\n",
      "Iteration 2627, loss = 0.14890202\n",
      "Iteration 572, loss = 0.37501160\n",
      "Iteration 1558, loss = 0.24321896\n",
      "Iteration 611, loss = 0.29490952\n",
      "Iteration 420, loss = 0.32560672\n",
      "Iteration 1681, loss = 0.13859096\n",
      "Iteration 612, loss = 0.29478784\n",
      "Iteration 613, loss = 0.29468269\n",
      "Iteration 614, loss = 0.29458022\n",
      "Iteration 421, loss = 0.32547950\n",
      "Iteration 1446, loss = 0.21420340\n",
      "Iteration 615, loss = 0.29445462\n",
      "Iteration 1559, loss = 0.24313956\n",
      "Iteration 211, loss = 0.38356622\n",
      "Iteration 616, loss = 0.29434713\n",
      "Iteration 617, loss = 0.29422617\n",
      "Iteration 1682, loss = 0.13842971\n",
      "Iteration 618, loss = 0.29410953\n",
      "Iteration 1560, loss = 0.24296085\n",
      "Iteration 573, loss = 0.37480204\n",
      "Iteration 1447, loss = 0.21412647\n",
      "Iteration 422, loss = 0.32536782\n",
      "Iteration 212, loss = 0.38320677\n",
      "Iteration 423, loss = 0.32528731\n",
      "Iteration 2628, loss = 0.14881687\n",
      "Iteration 619, loss = 0.29395906\n",
      "Iteration 620, loss = 0.29384756\n",
      "Iteration 574, loss = 0.37468605\n",
      "Iteration 621, loss = 0.29372129\n",
      "Iteration 213, loss = 0.38289675\n",
      "Iteration 1561, loss = 0.24277832\n",
      "Iteration 622, loss = 0.29363151\n",
      "Iteration 2629, loss = 0.14876739\n",
      "Iteration 424, loss = 0.32517522\n",
      "Iteration 1683, loss = 0.13839790\n",
      "Iteration 623, loss = 0.29351055\n",
      "Iteration 1448, loss = 0.21393192\n",
      "Iteration 624, loss = 0.29340067\n",
      "Iteration 625, loss = 0.29323719\n",
      "Iteration 2630, loss = 0.14869443\n",
      "Iteration 1684, loss = 0.13832267\n",
      "Iteration 575, loss = 0.37447830\n",
      "Iteration 626, loss = 0.29312471\n",
      "Iteration 214, loss = 0.38254566\n",
      "Iteration 627, loss = 0.29301534\n",
      "Iteration 1562, loss = 0.24271415\n",
      "Iteration 2631, loss = 0.14866061\n",
      "Iteration 628, loss = 0.29293404\n",
      "Iteration 629, loss = 0.29280365\n",
      "Iteration 1449, loss = 0.21368235\n",
      "Iteration 1685, loss = 0.13812066\n",
      "Iteration 630, loss = 0.29265849\n",
      "Iteration 425, loss = 0.32507511\n",
      "Iteration 2632, loss = 0.14856755\n",
      "Iteration 631, loss = 0.29256928\n",
      "Iteration 1686, loss = 0.13802432\n",
      "Iteration 215, loss = 0.38220643\n",
      "Iteration 576, loss = 0.37431777\n",
      "Iteration 632, loss = 0.29246551\n",
      "Iteration 1563, loss = 0.24257484\n",
      "Iteration 1687, loss = 0.13798661\n",
      "Iteration 426, loss = 0.32499792\n",
      "Iteration 1688, loss = 0.13799531\n",
      "Iteration 633, loss = 0.29230082\n",
      "Iteration 1450, loss = 0.21350746\n",
      "Iteration 634, loss = 0.29228418\n",
      "Iteration 1564, loss = 0.24243869\n",
      "Iteration 427, loss = 0.32491815\n",
      "Iteration 577, loss = 0.37424258\n",
      "Iteration 2633, loss = 0.14846759\n",
      "Iteration 635, loss = 0.29215391\n",
      "Iteration 1565, loss = 0.24228944\n",
      "Iteration 216, loss = 0.38189474\n",
      "Iteration 1689, loss = 0.13773248\n",
      "Iteration 428, loss = 0.32478223\n",
      "Iteration 636, loss = 0.29197544\n",
      "Iteration 2634, loss = 0.14844142\n",
      "Iteration 637, loss = 0.29182987\n",
      "Iteration 638, loss = 0.29172720\n",
      "Iteration 639, loss = 0.29158863\n",
      "Iteration 429, loss = 0.32470253\n",
      "Iteration 640, loss = 0.29146690\n",
      "Iteration 1690, loss = 0.13768414\n",
      "Iteration 641, loss = 0.29139636\n",
      "Iteration 430, loss = 0.32459032\n",
      "Iteration 642, loss = 0.29127161\n",
      "Iteration 217, loss = 0.38158578\n",
      "Iteration 1691, loss = 0.13750477\n",
      "Iteration 643, loss = 0.29111088\n",
      "Iteration 1451, loss = 0.21335651\n",
      "Iteration 1566, loss = 0.24222683\n",
      "Iteration 2635, loss = 0.14841675\n",
      "Iteration 431, loss = 0.32450992\n",
      "Iteration 1692, loss = 0.13746846\n",
      "Iteration 644, loss = 0.29101040\n",
      "Iteration 578, loss = 0.37401144\n",
      "Iteration 645, loss = 0.29090217\n",
      "Iteration 646, loss = 0.29076939\n",
      "Iteration 218, loss = 0.38123743\n",
      "Iteration 647, loss = 0.29070295\n",
      "Iteration 1693, loss = 0.13721807\n",
      "Iteration 648, loss = 0.29050554\n",
      "Iteration 1567, loss = 0.24205445\n",
      "Iteration 649, loss = 0.29043999\n",
      "Iteration 432, loss = 0.32438217\n",
      "Iteration 1452, loss = 0.21310943\n",
      "Iteration 1694, loss = 0.13709959\n",
      "Iteration 2636, loss = 0.14833213\n",
      "Iteration 433, loss = 0.32429606\n",
      "Iteration 650, loss = 0.29038412\n",
      "Iteration 1695, loss = 0.13705558\n",
      "Iteration 651, loss = 0.29015870\n",
      "Iteration 219, loss = 0.38093665\n",
      "Iteration 652, loss = 0.29003868\n",
      "Iteration 434, loss = 0.32417986\n",
      "Iteration 1453, loss = 0.21312660\n",
      "Iteration 1568, loss = 0.24196340\n",
      "Iteration 1696, loss = 0.13694417\n",
      "Iteration 435, loss = 0.32407538Iteration 653, loss = 0.28991391\n",
      "\n",
      "Iteration 220, loss = 0.38057318\n",
      "Iteration 2637, loss = 0.14822365\n",
      "Iteration 654, loss = 0.28979158\n",
      "Iteration 436, loss = 0.32399554\n",
      "Iteration 655, loss = 0.28970601\n",
      "Iteration 656, loss = 0.28961625\n",
      "Iteration 437, loss = 0.32390347\n",
      "Iteration 657, loss = 0.28955442\n",
      "Iteration 1454, loss = 0.21283533\n",
      "Iteration 221, loss = 0.38027792\n",
      "Iteration 1569, loss = 0.24183457\n",
      "Iteration 438, loss = 0.32380349\n",
      "Iteration 658, loss = 0.28936394\n",
      "Iteration 1697, loss = 0.13678349\n",
      "Iteration 439, loss = 0.32369028\n",
      "Iteration 1570, loss = 0.24170194\n",
      "Iteration 659, loss = 0.28926750\n",
      "Iteration 1455, loss = 0.21275374\n",
      "Iteration 660, loss = 0.28910004\n",
      "Iteration 579, loss = 0.37395200\n",
      "Iteration 440, loss = 0.32361736\n",
      "Iteration 661, loss = 0.28895329\n",
      "Iteration 662, loss = 0.28883949\n",
      "Iteration 441, loss = 0.32351644\n",
      "Iteration 1698, loss = 0.13670597\n",
      "Iteration 222, loss = 0.37999213\n",
      "Iteration 663, loss = 0.28873428\n",
      "Iteration 2638, loss = 0.14820352\n",
      "Iteration 442, loss = 0.32339676\n",
      "Iteration 580, loss = 0.37375623\n",
      "Iteration 443, loss = 0.32330530\n",
      "Iteration 1699, loss = 0.13667662\n",
      "Iteration 1456, loss = 0.21258811\n",
      "Iteration 2639, loss = 0.14822067\n",
      "Iteration 664, loss = 0.28862548\n",
      "Iteration 444, loss = 0.32321908\n",
      "Iteration 665, loss = 0.28849729\n",
      "Iteration 666, loss = 0.28836569\n",
      "Iteration 445, loss = 0.32313026\n",
      "Iteration 1700, loss = 0.13649837\n",
      "Iteration 667, loss = 0.28829016\n",
      "Iteration 668, loss = 0.28812844\n",
      "Iteration 446, loss = 0.32301235\n",
      "Iteration 581, loss = 0.37353893\n",
      "Iteration 223, loss = 0.37965007\n",
      "Iteration 669, loss = 0.28800024\n",
      "Iteration 447, loss = 0.32293615\n",
      "Iteration 2640, loss = 0.14803790\n",
      "Iteration 1571, loss = 0.24162032\n",
      "Iteration 670, loss = 0.28790093\n",
      "Iteration 671, loss = 0.28777533\n",
      "Iteration 224, loss = 0.37939134\n",
      "Iteration 672, loss = 0.28766415\n",
      "Iteration 1701, loss = 0.13637413\n",
      "Iteration 1457, loss = 0.21241189\n",
      "Iteration 673, loss = 0.28765050\n",
      "Iteration 448, loss = 0.32286596\n",
      "Iteration 1702, loss = 0.13640062\n",
      "Iteration 674, loss = 0.28747774\n",
      "Iteration 2641, loss = 0.14797556\n",
      "Iteration 1572, loss = 0.24143705\n",
      "Iteration 225, loss = 0.37905768\n",
      "Iteration 1703, loss = 0.13630553\n",
      "Iteration 675, loss = 0.28728873\n",
      "Iteration 582, loss = 0.37334956\n",
      "Iteration 449, loss = 0.32271561\n",
      "Iteration 1704, loss = 0.13604560\n",
      "Iteration 676, loss = 0.28728446\n",
      "Iteration 1458, loss = 0.21266040\n",
      "Iteration 1573, loss = 0.24133033\n",
      "Iteration 2642, loss = 0.14791540\n",
      "Iteration 677, loss = 0.28706980\n",
      "Iteration 226, loss = 0.37874358\n",
      "Iteration 1705, loss = 0.13588410\n",
      "Iteration 583, loss = 0.37320388\n",
      "Iteration 2643, loss = 0.14785495\n",
      "Iteration 678, loss = 0.28695755\n",
      "Iteration 1574, loss = 0.24120849\n",
      "Iteration 227, loss = 0.37843220\n",
      "Iteration 450, loss = 0.32268901\n",
      "Iteration 2644, loss = 0.14778377\n",
      "Iteration 1575, loss = 0.24107362\n",
      "Iteration 1459, loss = 0.21205341\n",
      "Iteration 451, loss = 0.32255679\n",
      "Iteration 1706, loss = 0.13592499\n",
      "Iteration 584, loss = 0.37304785\n",
      "Iteration 452, loss = 0.32243001\n",
      "Iteration 228, loss = 0.37815862\n",
      "Iteration 2645, loss = 0.14777089\n",
      "Iteration 679, loss = 0.28684059\n",
      "Iteration 1576, loss = 0.24092418\n",
      "Iteration 1460, loss = 0.21185298\n",
      "Iteration 1707, loss = 0.13575533\n",
      "Iteration 680, loss = 0.28679425\n",
      "Iteration 453, loss = 0.32234240\n",
      "Iteration 585, loss = 0.37293652\n",
      "Iteration 229, loss = 0.37786244\n",
      "Iteration 454, loss = 0.32226121\n",
      "Iteration 1708, loss = 0.13568814\n",
      "Iteration 1577, loss = 0.24081291\n",
      "Iteration 455, loss = 0.32218084\n",
      "Iteration 1461, loss = 0.21196054\n",
      "Iteration 681, loss = 0.28660005\n",
      "Iteration 230, loss = 0.37756152\n",
      "Iteration 1709, loss = 0.13547448\n",
      "Iteration 682, loss = 0.28648857\n",
      "Iteration 683, loss = 0.28636347\n",
      "Iteration 1578, loss = 0.24071501\n",
      "Iteration 684, loss = 0.28625025\n",
      "Iteration 456, loss = 0.32207726\n",
      "Iteration 231, loss = 0.37725695\n",
      "Iteration 685, loss = 0.28613670\n",
      "Iteration 586, loss = 0.37269744\n",
      "Iteration 1710, loss = 0.13538053\n",
      "Iteration 686, loss = 0.28601894\n",
      "Iteration 687, loss = 0.28593811\n",
      "Iteration 457, loss = 0.32195526\n",
      "Iteration 1579, loss = 0.24058810\n",
      "Iteration 2646, loss = 0.14763423\n",
      "Iteration 1711, loss = 0.13538680\n",
      "Iteration 232, loss = 0.37701456\n",
      "Iteration 688, loss = 0.28577113\n",
      "Iteration 458, loss = 0.32186885\n",
      "Iteration 689, loss = 0.28570402\n",
      "Iteration 690, loss = 0.28556982\n",
      "Iteration 691, loss = 0.28547904\n",
      "Iteration 1712, loss = 0.13548684\n",
      "Iteration 459, loss = 0.32178445\n",
      "Iteration 692, loss = 0.28530659\n",
      "Iteration 587, loss = 0.37255850\n",
      "Iteration 693, loss = 0.28535035\n",
      "Iteration 233, loss = 0.37668499\n",
      "Iteration 694, loss = 0.28509450\n",
      "Iteration 695, loss = 0.28498643\n",
      "Iteration 460, loss = 0.32167108\n",
      "Iteration 2647, loss = 0.14757902\n",
      "Iteration 1462, loss = 0.21155108\n",
      "Iteration 1713, loss = 0.13503320\n",
      "Iteration 1580, loss = 0.24045147\n",
      "Iteration 696, loss = 0.28484010\n",
      "Iteration 697, loss = 0.28472000\n",
      "Iteration 461, loss = 0.32159730\n",
      "Iteration 234, loss = 0.37642780\n",
      "Iteration 698, loss = 0.28466602\n",
      "Iteration 1581, loss = 0.24037082\n",
      "Iteration 699, loss = 0.28451693\n",
      "Iteration 462, loss = 0.32148258\n",
      "Iteration 235, loss = 0.37611463\n",
      "Iteration 700, loss = 0.28442540\n",
      "Iteration 588, loss = 0.37242547\n",
      "Iteration 463, loss = 0.32139491\n",
      "Iteration 701, loss = 0.28428195\n",
      "Iteration 1582, loss = 0.24025563\n",
      "Iteration 1714, loss = 0.13502497\n",
      "Iteration 2648, loss = 0.14755546\n",
      "Iteration 702, loss = 0.28419126\n",
      "Iteration 464, loss = 0.32135705\n",
      "Iteration 236, loss = 0.37585461\n",
      "Iteration 1463, loss = 0.21142064\n",
      "Iteration 703, loss = 0.28406004\n",
      "Iteration 2649, loss = 0.14747847\n",
      "Iteration 704, loss = 0.28399171\n",
      "Iteration 705, loss = 0.28384067\n",
      "Iteration 465, loss = 0.32119254\n",
      "Iteration 1583, loss = 0.24009789\n",
      "Iteration 706, loss = 0.28376409\n",
      "Iteration 466, loss = 0.32110771\n",
      "Iteration 707, loss = 0.28363181\n",
      "Iteration 589, loss = 0.37221522\n",
      "Iteration 1464, loss = 0.21123707\n",
      "Iteration 1715, loss = 0.13486240\n",
      "Iteration 467, loss = 0.32100180\n",
      "Iteration 2650, loss = 0.14737682\n",
      "Iteration 708, loss = 0.28354448\n",
      "Iteration 709, loss = 0.28344271\n",
      "Iteration 710, loss = 0.28326969\n",
      "Iteration 1716, loss = 0.13498930\n",
      "Iteration 1584, loss = 0.23995476\n",
      "Iteration 2651, loss = 0.14731034\n",
      "Iteration 468, loss = 0.32094916\n",
      "Iteration 1585, loss = 0.23980868\n",
      "Iteration 590, loss = 0.37204503\n",
      "Iteration 711, loss = 0.28317966\n",
      "Iteration 1717, loss = 0.13464584\n",
      "Iteration 237, loss = 0.37558573\n",
      "Iteration 469, loss = 0.32081816\n",
      "Iteration 712, loss = 0.28305716\n",
      "Iteration 1586, loss = 0.23971458\n",
      "Iteration 713, loss = 0.28293421\n",
      "Iteration 714, loss = 0.28295431\n",
      "Iteration 591, loss = 0.37188057\n",
      "Iteration 1465, loss = 0.21119175\n",
      "Iteration 1718, loss = 0.13453971\n",
      "Iteration 715, loss = 0.28274784\n",
      "Iteration 470, loss = 0.32071780\n",
      "Iteration 238, loss = 0.37526566\n",
      "Iteration 1587, loss = 0.23961439\n",
      "Iteration 716, loss = 0.28262627\n",
      "Iteration 717, loss = 0.28254459\n",
      "Iteration 1719, loss = 0.13441870\n",
      "Iteration 2652, loss = 0.14725557\n",
      "Iteration 718, loss = 0.28240246\n",
      "Iteration 1588, loss = 0.23949583\n",
      "Iteration 471, loss = 0.32061988\n",
      "Iteration 1466, loss = 0.21121928\n",
      "Iteration 2653, loss = 0.14723766\n",
      "Iteration 1589, loss = 0.23932173\n",
      "Iteration 1720, loss = 0.13430881\n",
      "Iteration 239, loss = 0.37498862\n",
      "Iteration 472, loss = 0.32054424\n",
      "Iteration 592, loss = 0.37170080\n",
      "Iteration 719, loss = 0.28230299\n",
      "Iteration 473, loss = 0.32042898\n",
      "Iteration 720, loss = 0.28217609\n",
      "Iteration 1467, loss = 0.21089349\n",
      "Iteration 474, loss = 0.32035519\n",
      "Iteration 721, loss = 0.28206478\n",
      "Iteration 2654, loss = 0.14718067\n",
      "Iteration 240, loss = 0.37472498\n",
      "Iteration 1721, loss = 0.13423594\n",
      "Iteration 722, loss = 0.28192776\n",
      "Iteration 723, loss = 0.28185727\n",
      "Iteration 475, loss = 0.32026432\n",
      "Iteration 724, loss = 0.28170163\n",
      "Iteration 1722, loss = 0.13413883\n",
      "Iteration 593, loss = 0.37165316\n",
      "Iteration 725, loss = 0.28160478\n",
      "Iteration 726, loss = 0.28147535\n",
      "Iteration 2655, loss = 0.14706056\n",
      "Iteration 727, loss = 0.28137189\n",
      "Iteration 1590, loss = 0.23921586\n",
      "Iteration 1723, loss = 0.13404565\n",
      "Iteration 728, loss = 0.28126907\n",
      "Iteration 476, loss = 0.32014998\n",
      "Iteration 2656, loss = 0.14697237\n",
      "Iteration 729, loss = 0.28122659\n",
      "Iteration 1468, loss = 0.21059506\n",
      "Iteration 594, loss = 0.37144067\n",
      "Iteration 730, loss = 0.28108635\n",
      "Iteration 241, loss = 0.37447489\n",
      "Iteration 1591, loss = 0.23913706\n",
      "Iteration 1724, loss = 0.13395709\n",
      "Iteration 2657, loss = 0.14692991\n",
      "Iteration 477, loss = 0.32008408\n",
      "Iteration 731, loss = 0.28102328\n",
      "Iteration 2658, loss = 0.14685396\n",
      "Iteration 478, loss = 0.31995877\n",
      "Iteration 242, loss = 0.37417212\n",
      "Iteration 1725, loss = 0.13381044\n",
      "Iteration 1592, loss = 0.23894072\n",
      "Iteration 732, loss = 0.28082971\n",
      "Iteration 479, loss = 0.31987649\n",
      "Iteration 733, loss = 0.28077304\n",
      "Iteration 595, loss = 0.37120734\n",
      "Iteration 1726, loss = 0.13366501\n",
      "Iteration 2659, loss = 0.14686219\n",
      "Iteration 1469, loss = 0.21064488\n",
      "Iteration 734, loss = 0.28058598\n",
      "Iteration 1593, loss = 0.23882726\n",
      "Iteration 480, loss = 0.31977423\n",
      "Iteration 1727, loss = 0.13371644\n",
      "Iteration 596, loss = 0.37105736\n",
      "Iteration 243, loss = 0.37387924\n",
      "Iteration 481, loss = 0.31968528\n",
      "Iteration 2660, loss = 0.14689588\n",
      "Iteration 735, loss = 0.28053415\n",
      "Iteration 1470, loss = 0.21036341\n",
      "Iteration 736, loss = 0.28036752\n",
      "Iteration 1728, loss = 0.13352867\n",
      "Iteration 1594, loss = 0.23884530\n",
      "Iteration 244, loss = 0.37364498\n",
      "Iteration 2661, loss = 0.14671920\n",
      "Iteration 482, loss = 0.31962416\n",
      "Iteration 597, loss = 0.37095787\n",
      "Iteration 737, loss = 0.28024287\n",
      "Iteration 1729, loss = 0.13346944\n",
      "Iteration 483, loss = 0.31950246\n",
      "Iteration 1595, loss = 0.23858794\n",
      "Iteration 1730, loss = 0.13337394\n",
      "Iteration 2662, loss = 0.14668195\n",
      "Iteration 738, loss = 0.28011587\n",
      "Iteration 1596, loss = 0.23851989\n",
      "Iteration 484, loss = 0.31940465\n",
      "Iteration 1471, loss = 0.21026049\n",
      "Iteration 739, loss = 0.28010375\n",
      "Iteration 245, loss = 0.37337731\n",
      "Iteration 598, loss = 0.37073053\n",
      "Iteration 740, loss = 0.27991671\n",
      "Iteration 1731, loss = 0.13315340\n",
      "Iteration 485, loss = 0.31935711\n",
      "Iteration 741, loss = 0.27991364\n",
      "Iteration 1472, loss = 0.21008336\n",
      "Iteration 1597, loss = 0.23846653\n",
      "Iteration 742, loss = 0.27967109\n",
      "Iteration 2663, loss = 0.14655590\n",
      "Iteration 246, loss = 0.37311146\n",
      "Iteration 743, loss = 0.27955242\n",
      "Iteration 599, loss = 0.37067051\n",
      "Iteration 1732, loss = 0.13315119\n",
      "Iteration 744, loss = 0.27946701\n",
      "Iteration 745, loss = 0.27957433\n",
      "Iteration 247, loss = 0.37283589\n",
      "Iteration 2664, loss = 0.14651844\n",
      "Iteration 486, loss = 0.31928955\n",
      "Iteration 746, loss = 0.27923138\n",
      "Iteration 1733, loss = 0.13298905\n",
      "Iteration 1473, loss = 0.20988012\n",
      "Iteration 248, loss = 0.37258926\n",
      "Iteration 487, loss = 0.31923862\n",
      "Iteration 747, loss = 0.27915787\n",
      "Iteration 600, loss = 0.37049328\n",
      "Iteration 2665, loss = 0.14645075\n",
      "Iteration 1734, loss = 0.13291503\n",
      "Iteration 748, loss = 0.27902001\n",
      "Iteration 488, loss = 0.31904162\n",
      "Iteration 749, loss = 0.27890734\n",
      "Iteration 489, loss = 0.31893492\n",
      "Iteration 1598, loss = 0.23824055\n",
      "Iteration 249, loss = 0.37235698\n",
      "Iteration 2666, loss = 0.14637363\n",
      "Iteration 1735, loss = 0.13286552\n",
      "Iteration 601, loss = 0.37021876\n",
      "Iteration 490, loss = 0.31884843\n",
      "Iteration 1599, loss = 0.23809712\n",
      "Iteration 491, loss = 0.31877415\n",
      "Iteration 250, loss = 0.37208624\n",
      "Iteration 750, loss = 0.27881907\n",
      "Iteration 1736, loss = 0.13261269\n",
      "Iteration 751, loss = 0.27868224\n",
      "Iteration 1600, loss = 0.23799028\n",
      "Iteration 492, loss = 0.31867537\n",
      "Iteration 1474, loss = 0.20991116\n",
      "Iteration 752, loss = 0.27859675\n",
      "Iteration 2667, loss = 0.14631881\n",
      "Iteration 602, loss = 0.37008187\n",
      "Iteration 753, loss = 0.27847263\n",
      "Iteration 754, loss = 0.27835184\n",
      "Iteration 1737, loss = 0.13260724\n",
      "Iteration 493, loss = 0.31861274\n",
      "Iteration 251, loss = 0.37183054\n",
      "Iteration 1475, loss = 0.20951574\n",
      "Iteration 755, loss = 0.27825292\n",
      "Iteration 494, loss = 0.31855263\n",
      "Iteration 1601, loss = 0.23786649\n",
      "Iteration 1738, loss = 0.13261546\n",
      "Iteration 252, loss = 0.37158524\n",
      "Iteration 495, loss = 0.31840480\n",
      "Iteration 1739, loss = 0.13242724\n",
      "Iteration 756, loss = 0.27824011\n",
      "Iteration 2668, loss = 0.14622603\n",
      "Iteration 1740, loss = 0.13222146\n",
      "Iteration 496, loss = 0.31829991\n",
      "Iteration 757, loss = 0.27802626\n",
      "Iteration 497, loss = 0.31824460\n",
      "Iteration 1602, loss = 0.23774376\n",
      "Iteration 253, loss = 0.37132397\n",
      "Iteration 1741, loss = 0.13213990\n",
      "Iteration 603, loss = 0.36994864\n",
      "Iteration 758, loss = 0.27792375\n",
      "Iteration 1476, loss = 0.20938570\n",
      "Iteration 759, loss = 0.27784495\n",
      "Iteration 760, loss = 0.27767214\n",
      "Iteration 1603, loss = 0.23760930\n",
      "Iteration 498, loss = 0.31813460\n",
      "Iteration 2669, loss = 0.14626201\n",
      "Iteration 761, loss = 0.27756847\n",
      "Iteration 254, loss = 0.37108882\n",
      "Iteration 604, loss = 0.36973532\n",
      "Iteration 1742, loss = 0.13215037\n",
      "Iteration 762, loss = 0.27756529\n",
      "Iteration 763, loss = 0.27736822\n",
      "Iteration 499, loss = 0.31802056\n",
      "Iteration 1477, loss = 0.20922004\n",
      "Iteration 1743, loss = 0.13198451\n",
      "Iteration 764, loss = 0.27728340\n",
      "Iteration 255, loss = 0.37085082\n",
      "Iteration 2670, loss = 0.14608139\n",
      "Iteration 1604, loss = 0.23752316\n",
      "Iteration 1744, loss = 0.13182564\n",
      "Iteration 765, loss = 0.27717848\n",
      "Iteration 256, loss = 0.37068146\n",
      "Iteration 766, loss = 0.27707282\n",
      "Iteration 2671, loss = 0.14608783\n",
      "Iteration 500, loss = 0.31794731Iteration 1745, loss = 0.13182863\n",
      "\n",
      "Iteration 767, loss = 0.27701225\n",
      "Iteration 1605, loss = 0.23734863\n",
      "Iteration 768, loss = 0.27683472\n",
      "Iteration 769, loss = 0.27674810\n",
      "Iteration 1478, loss = 0.20921043\n",
      "Iteration 770, loss = 0.27670191\n",
      "Iteration 605, loss = 0.36956555\n",
      "Iteration 1746, loss = 0.13172422\n",
      "Iteration 501, loss = 0.31784211\n",
      "Iteration 771, loss = 0.27648712\n",
      "Iteration 772, loss = 0.27641846\n",
      "Iteration 2672, loss = 0.14613662\n",
      "Iteration 257, loss = 0.37036142\n",
      "Iteration 502, loss = 0.31774924\n",
      "Iteration 773, loss = 0.27643461\n",
      "Iteration 774, loss = 0.27619660\n",
      "Iteration 1606, loss = 0.23725528\n",
      "Iteration 503, loss = 0.31770447\n",
      "Iteration 1479, loss = 0.20893774\n",
      "Iteration 775, loss = 0.27608487\n",
      "Iteration 258, loss = 0.37010574\n",
      "Iteration 776, loss = 0.27598048\n",
      "Iteration 1747, loss = 0.13154572\n",
      "Iteration 606, loss = 0.36940622\n",
      "Iteration 504, loss = 0.31764885\n",
      "Iteration 777, loss = 0.27586568\n",
      "Iteration 2673, loss = 0.14594737\n",
      "Iteration 505, loss = 0.31748002\n",
      "Iteration 778, loss = 0.27577486\n",
      "Iteration 259, loss = 0.36989259\n",
      "Iteration 1607, loss = 0.23721266\n",
      "Iteration 1480, loss = 0.20884227\n",
      "Iteration 779, loss = 0.27571016\n",
      "Iteration 506, loss = 0.31738784\n",
      "Iteration 1748, loss = 0.13144143\n",
      "Iteration 780, loss = 0.27554419\n",
      "Iteration 781, loss = 0.27544542\n",
      "Iteration 2674, loss = 0.14600228\n",
      "Iteration 1608, loss = 0.23706229\n",
      "Iteration 782, loss = 0.27535356\n",
      "Iteration 1749, loss = 0.13135621\n",
      "Iteration 783, loss = 0.27527832\n",
      "Iteration 507, loss = 0.31736686\n",
      "Iteration 784, loss = 0.27513632\n",
      "Iteration 1609, loss = 0.23690790\n",
      "Iteration 1750, loss = 0.13125774\n",
      "Iteration 785, loss = 0.27520549\n",
      "Iteration 786, loss = 0.27497172\n",
      "Iteration 607, loss = 0.36927016\n",
      "Iteration 508, loss = 0.31719957\n",
      "Iteration 260, loss = 0.36963763\n",
      "Iteration 787, loss = 0.27492503\n",
      "Iteration 509, loss = 0.31711492\n",
      "Iteration 1481, loss = 0.20853927\n",
      "Iteration 1610, loss = 0.23675739\n",
      "Iteration 2675, loss = 0.14591677\n",
      "Iteration 788, loss = 0.27470359\n",
      "Iteration 789, loss = 0.27459602\n",
      "Iteration 1751, loss = 0.13115215\n",
      "Iteration 790, loss = 0.27452783\n",
      "Iteration 510, loss = 0.31705104\n",
      "Iteration 2676, loss = 0.14574924\n",
      "Iteration 1752, loss = 0.13105134\n",
      "Iteration 261, loss = 0.36940718\n",
      "Iteration 511, loss = 0.31694171\n",
      "Iteration 1482, loss = 0.20841372\n",
      "Iteration 1753, loss = 0.13097575\n",
      "Iteration 512, loss = 0.31683283\n",
      "Iteration 608, loss = 0.36907339\n",
      "Iteration 791, loss = 0.27440316\n",
      "Iteration 2677, loss = 0.14568571\n",
      "Iteration 1611, loss = 0.23661143\n",
      "Iteration 1754, loss = 0.13082841\n",
      "Iteration 792, loss = 0.27430310\n",
      "Iteration 1755, loss = 0.13076592\n",
      "Iteration 513, loss = 0.31680381\n",
      "Iteration 793, loss = 0.27418000\n",
      "Iteration 262, loss = 0.36924940\n",
      "Iteration 1612, loss = 0.23651759\n",
      "Iteration 609, loss = 0.36898248\n",
      "Iteration 794, loss = 0.27408108\n",
      "Iteration 795, loss = 0.27395152\n",
      "Iteration 514, loss = 0.31666123\n",
      "Iteration 1483, loss = 0.20829876\n",
      "Iteration 796, loss = 0.27394733\n",
      "Iteration 1756, loss = 0.13065502\n",
      "Iteration 797, loss = 0.27377356\n",
      "Iteration 263, loss = 0.36893829\n",
      "Iteration 2678, loss = 0.14567901\n",
      "Iteration 515, loss = 0.31658809Iteration 1613, loss = 0.23646747\n",
      "\n",
      "Iteration 1757, loss = 0.13048981\n",
      "Iteration 798, loss = 0.27364517\n",
      "Iteration 610, loss = 0.36874415\n",
      "Iteration 799, loss = 0.27357536\n",
      "Iteration 516, loss = 0.31652512\n",
      "Iteration 1758, loss = 0.13040788\n",
      "Iteration 800, loss = 0.27344623\n",
      "Iteration 2679, loss = 0.14554877\n",
      "Iteration 264, loss = 0.36870853\n",
      "Iteration 801, loss = 0.27333075\n",
      "Iteration 517, loss = 0.31640971\n",
      "Iteration 1484, loss = 0.20814449\n",
      "Iteration 802, loss = 0.27329788\n",
      "Iteration 803, loss = 0.27322933\n",
      "Iteration 1759, loss = 0.13038852\n",
      "Iteration 804, loss = 0.27304838\n",
      "Iteration 611, loss = 0.36859196\n",
      "Iteration 805, loss = 0.27291992\n",
      "Iteration 2680, loss = 0.14556351\n",
      "Iteration 1760, loss = 0.13048937\n",
      "Iteration 518, loss = 0.31632854\n",
      "Iteration 806, loss = 0.27288477\n",
      "Iteration 1614, loss = 0.23627946\n",
      "Iteration 265, loss = 0.36848346\n",
      "Iteration 807, loss = 0.27271549\n",
      "Iteration 1761, loss = 0.13029366\n",
      "Iteration 808, loss = 0.27261617\n",
      "Iteration 809, loss = 0.27259362\n",
      "Iteration 1762, loss = 0.13000715\n",
      "Iteration 519, loss = 0.31624553\n",
      "Iteration 2681, loss = 0.14545782Iteration 810, loss = 0.27244083\n",
      "\n",
      "Iteration 1485, loss = 0.20795476\n",
      "Iteration 1615, loss = 0.23616866\n",
      "Iteration 520, loss = 0.31612268\n",
      "Iteration 266, loss = 0.36825508\n",
      "Iteration 612, loss = 0.36844194\n",
      "Iteration 521, loss = 0.31603988\n",
      "Iteration 1486, loss = 0.20830040\n",
      "Iteration 2682, loss = 0.14539972\n",
      "Iteration 1763, loss = 0.12992164\n",
      "Iteration 811, loss = 0.27229389\n",
      "Iteration 267, loss = 0.36805097\n",
      "Iteration 1616, loss = 0.23603920\n",
      "Iteration 812, loss = 0.27218922\n",
      "Iteration 813, loss = 0.27211842\n",
      "Iteration 814, loss = 0.27199146\n",
      "Iteration 1764, loss = 0.12981201\n",
      "Iteration 815, loss = 0.27190525\n",
      "Iteration 2683, loss = 0.14530520\n",
      "Iteration 613, loss = 0.36825512\n",
      "Iteration 522, loss = 0.31596894\n",
      "Iteration 1487, loss = 0.20770701\n",
      "Iteration 2684, loss = 0.14532768\n",
      "Iteration 268, loss = 0.36782962\n",
      "Iteration 816, loss = 0.27177420\n",
      "Iteration 614, loss = 0.36807636\n",
      "Iteration 817, loss = 0.27176208\n",
      "Iteration 523, loss = 0.31587731\n",
      "Iteration 2685, loss = 0.14518923\n",
      "Iteration 1617, loss = 0.23589204\n",
      "Iteration 818, loss = 0.27161886\n",
      "Iteration 1765, loss = 0.12979376\n",
      "Iteration 819, loss = 0.27151501\n",
      "Iteration 1488, loss = 0.20760162\n",
      "Iteration 269, loss = 0.36760310\n",
      "Iteration 615, loss = 0.36794791\n",
      "Iteration 524, loss = 0.31582735\n",
      "Iteration 820, loss = 0.27137401\n",
      "Iteration 2686, loss = 0.14511819\n",
      "Iteration 1618, loss = 0.23579398\n",
      "Iteration 525, loss = 0.31572588\n",
      "Iteration 1766, loss = 0.12965281\n",
      "Iteration 821, loss = 0.27134582\n",
      "Iteration 822, loss = 0.27114868\n",
      "Iteration 526, loss = 0.31561114\n",
      "Iteration 616, loss = 0.36772962\n",
      "Iteration 1767, loss = 0.12950194\n",
      "Iteration 823, loss = 0.27107185\n",
      "Iteration 527, loss = 0.31550112\n",
      "Iteration 2687, loss = 0.14510308\n",
      "Iteration 528, loss = 0.31544090\n",
      "Iteration 1489, loss = 0.20738200\n",
      "Iteration 1768, loss = 0.12945496\n",
      "Iteration 824, loss = 0.27095272\n",
      "Iteration 529, loss = 0.31532553\n",
      "Iteration 530, loss = 0.31524140\n",
      "Iteration 2688, loss = 0.14500746\n",
      "Iteration 1769, loss = 0.12947578\n",
      "Iteration 825, loss = 0.27088173\n",
      "Iteration 617, loss = 0.36759476\n",
      "Iteration 531, loss = 0.31517535\n",
      "Iteration 1619, loss = 0.23570614\n",
      "Iteration 2689, loss = 0.14497709\n",
      "Iteration 270, loss = 0.36736956\n",
      "Iteration 532, loss = 0.31506369\n",
      "Iteration 826, loss = 0.27084929\n",
      "Iteration 1770, loss = 0.12926842\n",
      "Iteration 1490, loss = 0.20715284\n",
      "Iteration 827, loss = 0.27064627\n",
      "Iteration 2690, loss = 0.14489144\n",
      "Iteration 1771, loss = 0.12916841\n",
      "Iteration 828, loss = 0.27058731\n",
      "Iteration 271, loss = 0.36714736\n",
      "Iteration 1620, loss = 0.23555659\n",
      "Iteration 829, loss = 0.27046513\n",
      "Iteration 533, loss = 0.31499826\n",
      "Iteration 1772, loss = 0.12911267\n",
      "Iteration 534, loss = 0.31490332\n",
      "Iteration 830, loss = 0.27035039\n",
      "Iteration 2691, loss = 0.14484552\n",
      "Iteration 618, loss = 0.36740834\n",
      "Iteration 1491, loss = 0.20705273Iteration 535, loss = 0.31481580\n",
      "Iteration 831, loss = 0.27026117\n",
      "\n",
      "Iteration 1773, loss = 0.12895756\n",
      "Iteration 536, loss = 0.31470995\n",
      "Iteration 832, loss = 0.27022956\n",
      "Iteration 272, loss = 0.36694227\n",
      "Iteration 1621, loss = 0.23537870\n",
      "Iteration 833, loss = 0.27005933\n",
      "Iteration 834, loss = 0.26997058\n",
      "Iteration 2692, loss = 0.14476151\n",
      "Iteration 537, loss = 0.31463499\n",
      "Iteration 273, loss = 0.36671648\n",
      "Iteration 619, loss = 0.36725232\n",
      "Iteration 835, loss = 0.26985356\n",
      "Iteration 836, loss = 0.26974507\n",
      "Iteration 1774, loss = 0.12891569\n",
      "Iteration 538, loss = 0.31453338\n",
      "Iteration 837, loss = 0.26962326\n",
      "Iteration 1492, loss = 0.20688371\n",
      "Iteration 539, loss = 0.31446847\n",
      "Iteration 1622, loss = 0.23525627\n",
      "Iteration 2693, loss = 0.14467184\n",
      "Iteration 1775, loss = 0.12880834\n",
      "Iteration 274, loss = 0.36650910\n",
      "Iteration 838, loss = 0.26955150\n",
      "Iteration 620, loss = 0.36710302\n",
      "Iteration 540, loss = 0.31436002\n",
      "Iteration 839, loss = 0.26943564\n",
      "Iteration 840, loss = 0.26933291\n",
      "Iteration 541, loss = 0.31426521\n",
      "Iteration 841, loss = 0.26923830\n",
      "Iteration 542, loss = 0.31419718\n",
      "Iteration 1493, loss = 0.20672817\n",
      "Iteration 1623, loss = 0.23515099\n",
      "Iteration 842, loss = 0.26914170\n",
      "Iteration 1776, loss = 0.12891659\n",
      "Iteration 2694, loss = 0.14463575\n",
      "Iteration 275, loss = 0.36632954\n",
      "Iteration 843, loss = 0.26909265\n",
      "Iteration 844, loss = 0.26891741\n",
      "Iteration 845, loss = 0.26886340\n",
      "Iteration 2695, loss = 0.14453672\n",
      "Iteration 543, loss = 0.31408703\n",
      "Iteration 846, loss = 0.26875885\n",
      "Iteration 1777, loss = 0.12860908\n",
      "Iteration 847, loss = 0.26862223\n",
      "Iteration 544, loss = 0.31402996\n",
      "Iteration 1778, loss = 0.12848143\n",
      "Iteration 1624, loss = 0.23500062\n",
      "Iteration 848, loss = 0.26856485\n",
      "Iteration 276, loss = 0.36608052\n",
      "Iteration 545, loss = 0.31393116\n",
      "Iteration 621, loss = 0.36699887\n",
      "Iteration 849, loss = 0.26841552\n",
      "Iteration 1494, loss = 0.20656739\n",
      "Iteration 850, loss = 0.26829544\n",
      "Iteration 2696, loss = 0.14447322\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 851, loss = 0.26824875\n",
      "Iteration 546, loss = 0.31385326\n",
      "Iteration 277, loss = 0.36589961\n",
      "Iteration 852, loss = 0.26815285\n",
      "Iteration 547, loss = 0.31373435\n",
      "Iteration 853, loss = 0.26803762\n",
      "Iteration 1625, loss = 0.23495417\n",
      "Iteration 854, loss = 0.26796649\n",
      "Iteration 1779, loss = 0.12847164\n",
      "Iteration 855, loss = 0.26779858\n",
      "Iteration 1780, loss = 0.12847111\n",
      "Iteration 1626, loss = 0.23476133\n",
      "Iteration 856, loss = 0.26770262\n",
      "Iteration 1495, loss = 0.20643730\n",
      "Iteration 622, loss = 0.36675689\n",
      "Iteration 857, loss = 0.26760296\n",
      "Iteration 548, loss = 0.31365838\n",
      "Iteration 278, loss = 0.36565965\n",
      "Iteration 1781, loss = 0.12824785\n",
      "Iteration 549, loss = 0.31356896\n",
      "Iteration 858, loss = 0.26759463\n",
      "Iteration 1496, loss = 0.20631758\n",
      "Iteration 279, loss = 0.36549079\n",
      "Iteration 859, loss = 0.26739646\n",
      "Iteration 1627, loss = 0.23465874\n",
      "Iteration 860, loss = 0.26729612\n",
      "Iteration 550, loss = 0.31346664\n",
      "Iteration 280, loss = 0.36522636\n",
      "Iteration 861, loss = 0.26723865\n",
      "Iteration 862, loss = 0.26713147\n",
      "Iteration 1782, loss = 0.12823560\n",
      "Iteration 623, loss = 0.36660108\n",
      "Iteration 1, loss = 0.78177133\n",
      "Iteration 551, loss = 0.31341463\n",
      "Iteration 863, loss = 0.26699480\n",
      "Iteration 1783, loss = 0.12813342\n",
      "Iteration 1628, loss = 0.23455786\n",
      "Iteration 1497, loss = 0.20616889\n",
      "Iteration 864, loss = 0.26698783\n",
      "Iteration 552, loss = 0.31329730\n",
      "Iteration 1784, loss = 0.12796737\n",
      "Iteration 865, loss = 0.26685693\n",
      "Iteration 866, loss = 0.26670451\n",
      "Iteration 553, loss = 0.31321233\n",
      "Iteration 2, loss = 0.78035291\n",
      "Iteration 867, loss = 0.26671260\n",
      "Iteration 1629, loss = 0.23454774\n",
      "Iteration 281, loss = 0.36503102\n",
      "Iteration 554, loss = 0.31320116\n",
      "Iteration 624, loss = 0.36647766\n",
      "Iteration 1630, loss = 0.23435326\n",
      "Iteration 3, loss = 0.77809653\n",
      "Iteration 4, loss = 0.77566547\n",
      "Iteration 868, loss = 0.26667985\n",
      "Iteration 282, loss = 0.36484155\n",
      "Iteration 1785, loss = 0.12793430\n",
      "Iteration 869, loss = 0.26640833\n",
      "Iteration 555, loss = 0.31305081\n",
      "Iteration 1631, loss = 0.23417026\n",
      "Iteration 870, loss = 0.26633459\n",
      "Iteration 556, loss = 0.31298377\n",
      "Iteration 1786, loss = 0.12775287\n",
      "Iteration 1498, loss = 0.20599348\n",
      "Iteration 871, loss = 0.26623926\n",
      "Iteration 625, loss = 0.36634910\n",
      "Iteration 872, loss = 0.26611573\n",
      "Iteration 1787, loss = 0.12776186\n",
      "Iteration 557, loss = 0.31292846\n",
      "Iteration 873, loss = 0.26603347\n",
      "Iteration 283, loss = 0.36462332\n",
      "Iteration 874, loss = 0.26592110\n",
      "Iteration 5, loss = 0.77256155\n",
      "Iteration 875, loss = 0.26585191\n",
      "Iteration 1788, loss = 0.12758449\n",
      "Iteration 1632, loss = 0.23409527\n",
      "Iteration 558, loss = 0.31281629\n",
      "Iteration 876, loss = 0.26577186\n",
      "Iteration 6, loss = 0.76943309\n",
      "Iteration 1789, loss = 0.12750192\n",
      "Iteration 1499, loss = 0.20577882\n",
      "Iteration 877, loss = 0.26572365\n",
      "Iteration 878, loss = 0.26550995\n",
      "Iteration 559, loss = 0.31277041\n",
      "Iteration 284, loss = 0.36442823\n",
      "Iteration 1633, loss = 0.23393624\n",
      "Iteration 1790, loss = 0.12738199\n",
      "Iteration 879, loss = 0.26550139\n",
      "Iteration 560, loss = 0.31261746\n",
      "Iteration 7, loss = 0.76612554\n",
      "Iteration 880, loss = 0.26532633\n",
      "Iteration 1791, loss = 0.12734025\n",
      "Iteration 285, loss = 0.36420117\n",
      "Iteration 561, loss = 0.31253776\n",
      "Iteration 881, loss = 0.26527088\n",
      "Iteration 1792, loss = 0.12718718\n",
      "Iteration 882, loss = 0.26511828\n",
      "Iteration 562, loss = 0.31244764\n",
      "Iteration 883, loss = 0.26502511\n",
      "Iteration 1793, loss = 0.12715638\n",
      "Iteration 1500, loss = 0.20566963\n",
      "Iteration 286, loss = 0.36403779\n",
      "Iteration 626, loss = 0.36607553\n",
      "Iteration 563, loss = 0.31234853\n",
      "Iteration 1634, loss = 0.23384319\n",
      "Iteration 564, loss = 0.31228590\n",
      "Iteration 287, loss = 0.36380453\n",
      "Iteration 884, loss = 0.26494660\n",
      "Iteration 1794, loss = 0.12713985\n",
      "Iteration 885, loss = 0.26484650\n",
      "Iteration 886, loss = 0.26473274\n",
      "Iteration 1795, loss = 0.12697000\n",
      "Iteration 887, loss = 0.26465074\n",
      "Iteration 565, loss = 0.31220811\n",
      "Iteration 1501, loss = 0.20572174\n",
      "Iteration 1635, loss = 0.23371918\n",
      "Iteration 888, loss = 0.26453359\n",
      "Iteration 889, loss = 0.26443153\n",
      "Iteration 566, loss = 0.31210503\n",
      "Iteration 8, loss = 0.76281811\n",
      "Iteration 890, loss = 0.26434424\n",
      "Iteration 891, loss = 0.26427015\n",
      "Iteration 1796, loss = 0.12690351\n",
      "Iteration 627, loss = 0.36592182\n",
      "Iteration 892, loss = 0.26420063\n",
      "Iteration 567, loss = 0.31204575\n",
      "Iteration 288, loss = 0.36361359\n",
      "Iteration 893, loss = 0.26402977\n",
      "Iteration 894, loss = 0.26396610\n",
      "Iteration 1636, loss = 0.23356538\n",
      "Iteration 1502, loss = 0.20541431\n",
      "Iteration 568, loss = 0.31195159\n",
      "Iteration 895, loss = 0.26383907\n",
      "Iteration 896, loss = 0.26378370\n",
      "Iteration 897, loss = 0.26365079\n",
      "Iteration 569, loss = 0.31195106Iteration 1797, loss = 0.12679844\n",
      "\n",
      "Iteration 628, loss = 0.36579484\n",
      "Iteration 898, loss = 0.26353645\n",
      "Iteration 570, loss = 0.31177839\n",
      "Iteration 1637, loss = 0.23347403\n",
      "Iteration 1798, loss = 0.12668903\n",
      "Iteration 899, loss = 0.26346763\n",
      "Iteration 571, loss = 0.31168378\n",
      "Iteration 900, loss = 0.26338789\n",
      "Iteration 629, loss = 0.36559565\n",
      "Iteration 901, loss = 0.26329826\n",
      "Iteration 1799, loss = 0.12674476\n",
      "Iteration 902, loss = 0.26319928\n",
      "Iteration 289, loss = 0.36344643\n",
      "Iteration 1503, loss = 0.20518996\n",
      "Iteration 1638, loss = 0.23331156\n",
      "Iteration 572, loss = 0.31159458\n",
      "Iteration 903, loss = 0.26305468\n",
      "Iteration 904, loss = 0.26295059\n",
      "Iteration 290, loss = 0.36322349\n",
      "Iteration 905, loss = 0.26286992\n",
      "Iteration 573, loss = 0.31150018\n",
      "Iteration 906, loss = 0.26279483\n",
      "Iteration 907, loss = 0.26267141\n",
      "Iteration 1800, loss = 0.12654693\n",
      "Iteration 908, loss = 0.26257882\n",
      "Iteration 909, loss = 0.26247474\n",
      "Iteration 1504, loss = 0.20506463\n",
      "Iteration 1639, loss = 0.23318998\n",
      "Iteration 1801, loss = 0.12645916\n",
      "Iteration 910, loss = 0.26239691\n",
      "Iteration 574, loss = 0.31140143\n",
      "Iteration 1802, loss = 0.12634274\n",
      "Iteration 575, loss = 0.31135475\n",
      "Iteration 1640, loss = 0.23303923\n",
      "Iteration 911, loss = 0.26229004\n",
      "Iteration 630, loss = 0.36557163\n",
      "Iteration 291, loss = 0.36301025\n",
      "Iteration 576, loss = 0.31122533\n",
      "Iteration 1803, loss = 0.12641436\n",
      "Iteration 912, loss = 0.26228828\n",
      "Iteration 577, loss = 0.31115743\n",
      "Iteration 1804, loss = 0.12618455\n",
      "Iteration 1505, loss = 0.20486567\n",
      "Iteration 913, loss = 0.26212518\n",
      "Iteration 1641, loss = 0.23293286\n",
      "Iteration 292, loss = 0.36281577\n",
      "Iteration 9, loss = 0.75969025\n",
      "Iteration 578, loss = 0.31111624\n",
      "Iteration 1642, loss = 0.23280747\n",
      "Iteration 914, loss = 0.26198862\n",
      "Iteration 1506, loss = 0.20475908\n",
      "Iteration 915, loss = 0.26189059\n",
      "Iteration 293, loss = 0.36264797\n",
      "Iteration 1805, loss = 0.12610301\n",
      "Iteration 631, loss = 0.36527148\n",
      "Iteration 916, loss = 0.26178813\n",
      "Iteration 917, loss = 0.26166095\n",
      "Iteration 579, loss = 0.31099101\n",
      "Iteration 918, loss = 0.26164072\n",
      "Iteration 1643, loss = 0.23266928\n",
      "Iteration 580, loss = 0.31089479\n",
      "Iteration 1507, loss = 0.20472403\n",
      "Iteration 294, loss = 0.36243433\n",
      "Iteration 581, loss = 0.31080922\n",
      "Iteration 919, loss = 0.26150479\n",
      "Iteration 1806, loss = 0.12597733\n",
      "Iteration 582, loss = 0.31075396\n",
      "Iteration 583, loss = 0.31067473\n",
      "Iteration 1508, loss = 0.20441632\n",
      "Iteration 1644, loss = 0.23258761\n",
      "Iteration 295, loss = 0.36226754\n",
      "Iteration 10, loss = 0.75638708\n",
      "Iteration 920, loss = 0.26138259\n",
      "Iteration 1807, loss = 0.12592715\n",
      "Iteration 921, loss = 0.26133605\n",
      "Iteration 584, loss = 0.31054440\n",
      "Iteration 1808, loss = 0.12582078\n",
      "Iteration 585, loss = 0.31045285\n",
      "Iteration 1645, loss = 0.23242739\n",
      "Iteration 922, loss = 0.26122156\n",
      "Iteration 632, loss = 0.36514567\n",
      "Iteration 296, loss = 0.36205850\n",
      "Iteration 586, loss = 0.31044184\n",
      "Iteration 923, loss = 0.26115137\n",
      "Iteration 297, loss = 0.36189095\n",
      "Iteration 587, loss = 0.31034924\n",
      "Iteration 588, loss = 0.31022105\n",
      "Iteration 1509, loss = 0.20430676\n",
      "Iteration 298, loss = 0.36170466\n",
      "Iteration 924, loss = 0.26101411\n",
      "Iteration 1646, loss = 0.23237303Iteration 1809, loss = 0.12577540\n",
      "\n",
      "Iteration 11, loss = 0.75334789\n",
      "Iteration 925, loss = 0.26088999\n",
      "Iteration 633, loss = 0.36491344\n",
      "Iteration 1810, loss = 0.12589752\n",
      "Iteration 926, loss = 0.26080052\n",
      "Iteration 12, loss = 0.75011089Iteration 1647, loss = 0.23221542\n",
      "Iteration 1811, loss = 0.12564603\n",
      "Iteration 589, loss = 0.31017004\n",
      "Iteration 927, loss = 0.26072950\n",
      "\n",
      "Iteration 928, loss = 0.26062737\n",
      "Iteration 13, loss = 0.74710850\n",
      "Iteration 929, loss = 0.26052972\n",
      "Iteration 1648, loss = 0.23206051\n",
      "Iteration 1510, loss = 0.20413064\n",
      "Iteration 930, loss = 0.26043693\n",
      "Iteration 14, loss = 0.74396275\n",
      "Iteration 299, loss = 0.36152742\n",
      "Iteration 634, loss = 0.36475041\n",
      "Iteration 1649, loss = 0.23190605\n",
      "Iteration 590, loss = 0.31004442\n",
      "Iteration 931, loss = 0.26032032\n",
      "Iteration 1812, loss = 0.12560593\n",
      "Iteration 591, loss = 0.30995278\n",
      "Iteration 1511, loss = 0.20396430\n",
      "Iteration 592, loss = 0.30986442\n",
      "Iteration 932, loss = 0.26020562\n",
      "Iteration 1813, loss = 0.12550046\n",
      "Iteration 300, loss = 0.36134450\n",
      "Iteration 593, loss = 0.30981200\n",
      "Iteration 933, loss = 0.26015567\n",
      "Iteration 934, loss = 0.26001968\n",
      "Iteration 1814, loss = 0.12538312\n",
      "Iteration 594, loss = 0.30971223\n",
      "Iteration 1650, loss = 0.23182201\n",
      "Iteration 595, loss = 0.30963351\n",
      "Iteration 935, loss = 0.25994817\n",
      "Iteration 1815, loss = 0.12521804\n",
      "Iteration 635, loss = 0.36458484\n",
      "Iteration 301, loss = 0.36114820\n",
      "Iteration 15, loss = 0.74104163\n",
      "Iteration 1651, loss = 0.23168120\n",
      "Iteration 596, loss = 0.30955807\n",
      "Iteration 1816, loss = 0.12517556\n",
      "Iteration 597, loss = 0.30943579\n",
      "Iteration 1512, loss = 0.20407420\n",
      "Iteration 936, loss = 0.25984552\n",
      "Iteration 16, loss = 0.73812846\n",
      "Iteration 1652, loss = 0.23155694\n",
      "Iteration 598, loss = 0.30935142\n",
      "Iteration 599, loss = 0.30925177\n",
      "Iteration 302, loss = 0.36095810\n",
      "Iteration 937, loss = 0.25975059\n",
      "Iteration 1513, loss = 0.20370340\n",
      "Iteration 636, loss = 0.36441111\n",
      "Iteration 938, loss = 0.25963325\n",
      "Iteration 1653, loss = 0.23144673\n",
      "Iteration 1817, loss = 0.12516950\n",
      "Iteration 939, loss = 0.25951320\n",
      "Iteration 940, loss = 0.25948710\n",
      "Iteration 941, loss = 0.25938791\n",
      "Iteration 600, loss = 0.30923142\n",
      "Iteration 942, loss = 0.25925009\n",
      "Iteration 17, loss = 0.73531653\n",
      "Iteration 1818, loss = 0.12519443\n",
      "Iteration 943, loss = 0.25912238\n",
      "Iteration 601, loss = 0.30911656\n",
      "Iteration 944, loss = 0.25903469\n",
      "Iteration 945, loss = 0.25904341\n",
      "Iteration 18, loss = 0.73225999\n",
      "Iteration 946, loss = 0.25893391\n",
      "Iteration 303, loss = 0.36081450\n",
      "Iteration 1514, loss = 0.20354204\n",
      "Iteration 1654, loss = 0.23132149\n",
      "Iteration 637, loss = 0.36425367\n",
      "Iteration 304, loss = 0.36058137\n",
      "Iteration 19, loss = 0.72941050\n",
      "Iteration 602, loss = 0.30900324\n",
      "Iteration 1655, loss = 0.23123763\n",
      "Iteration 603, loss = 0.30893185\n",
      "Iteration 305, loss = 0.36040815\n",
      "Iteration 638, loss = 0.36415743\n",
      "Iteration 1656, loss = 0.23119662\n",
      "Iteration 604, loss = 0.30886800\n",
      "Iteration 605, loss = 0.30882932\n",
      "Iteration 306, loss = 0.36033305\n",
      "Iteration 947, loss = 0.25875367\n",
      "Iteration 1819, loss = 0.12492350\n",
      "Iteration 20, loss = 0.72670381\n",
      "Iteration 1515, loss = 0.20345725\n",
      "Iteration 948, loss = 0.25865419\n",
      "Iteration 21, loss = 0.72389959\n",
      "Iteration 1820, loss = 0.12480918\n",
      "Iteration 22, loss = 0.72110611\n",
      "Iteration 949, loss = 0.25862161\n",
      "Iteration 950, loss = 0.25854432\n",
      "Iteration 1821, loss = 0.12469662\n",
      "Iteration 23, loss = 0.71830941\n",
      "Iteration 951, loss = 0.25843339\n",
      "Iteration 1822, loss = 0.12481735\n",
      "Iteration 606, loss = 0.30865617\n",
      "Iteration 24, loss = 0.71564005Iteration 952, loss = 0.25829345\n",
      "\n",
      "Iteration 1823, loss = 0.12462367\n",
      "Iteration 607, loss = 0.30860528\n",
      "Iteration 639, loss = 0.36391551\n",
      "Iteration 25, loss = 0.71292934\n",
      "Iteration 953, loss = 0.25836011\n",
      "Iteration 1657, loss = 0.23104447\n",
      "Iteration 1516, loss = 0.20325929\n",
      "Iteration 26, loss = 0.71030142\n",
      "Iteration 27, loss = 0.70750391\n",
      "Iteration 608, loss = 0.30849472\n",
      "Iteration 640, loss = 0.36374353\n",
      "Iteration 954, loss = 0.25808260\n",
      "Iteration 1824, loss = 0.12457651\n",
      "Iteration 307, loss = 0.36005800\n",
      "Iteration 1517, loss = 0.20335140\n",
      "Iteration 955, loss = 0.25806515\n",
      "Iteration 28, loss = 0.70477116\n",
      "Iteration 609, loss = 0.30841454\n",
      "Iteration 956, loss = 0.25790649\n",
      "Iteration 308, loss = 0.35992009\n",
      "Iteration 1658, loss = 0.23086357\n",
      "Iteration 1518, loss = 0.20335039\n",
      "Iteration 1825, loss = 0.12449783\n",
      "Iteration 610, loss = 0.30836928\n",
      "Iteration 957, loss = 0.25778852\n",
      "Iteration 1659, loss = 0.23068170\n",
      "Iteration 641, loss = 0.36366715\n",
      "Iteration 958, loss = 0.25787070\n",
      "Iteration 1826, loss = 0.12435545\n",
      "Iteration 959, loss = 0.25759778\n",
      "Iteration 960, loss = 0.25754137\n",
      "Iteration 961, loss = 0.25739713\n",
      "Iteration 309, loss = 0.35969656\n",
      "Iteration 962, loss = 0.25729027\n",
      "Iteration 963, loss = 0.25720721\n",
      "Iteration 1660, loss = 0.23055861\n",
      "Iteration 1827, loss = 0.12432553\n",
      "Iteration 964, loss = 0.25712608\n",
      "Iteration 965, loss = 0.25706030\n",
      "Iteration 310, loss = 0.35953151\n",
      "Iteration 1519, loss = 0.20278042\n",
      "Iteration 611, loss = 0.30830045\n",
      "Iteration 29, loss = 0.70211563\n",
      "Iteration 1661, loss = 0.23049504\n",
      "Iteration 966, loss = 0.25690126\n",
      "Iteration 612, loss = 0.30817834\n",
      "Iteration 1520, loss = 0.20281595\n",
      "Iteration 613, loss = 0.30811855\n",
      "Iteration 1662, loss = 0.23039298\n",
      "Iteration 967, loss = 0.25679693\n",
      "Iteration 30, loss = 0.69934830\n",
      "Iteration 614, loss = 0.30800845\n",
      "Iteration 311, loss = 0.35935618Iteration 1663, loss = 0.23017790\n",
      "Iteration 31, loss = 0.69663567\n",
      "Iteration 615, loss = 0.30793605\n",
      "\n",
      "Iteration 968, loss = 0.25679464\n",
      "Iteration 1664, loss = 0.23010084\n",
      "Iteration 642, loss = 0.36342010\n",
      "Iteration 969, loss = 0.25660712\n",
      "Iteration 616, loss = 0.30785215\n",
      "Iteration 1521, loss = 0.20254588\n",
      "Iteration 32, loss = 0.69380619\n",
      "Iteration 970, loss = 0.25650878\n",
      "Iteration 971, loss = 0.25641448\n",
      "Iteration 972, loss = 0.25632258\n",
      "Iteration 1828, loss = 0.12421492\n",
      "Iteration 643, loss = 0.36324984\n",
      "Iteration 973, loss = 0.25623794\n",
      "Iteration 312, loss = 0.35918085\n",
      "Iteration 617, loss = 0.30780679\n",
      "Iteration 974, loss = 0.25610634\n",
      "Iteration 1829, loss = 0.12415266\n",
      "Iteration 33, loss = 0.69104002\n",
      "Iteration 1522, loss = 0.20230574\n",
      "Iteration 975, loss = 0.25602157\n",
      "Iteration 618, loss = 0.30769356\n",
      "Iteration 976, loss = 0.25593798\n",
      "Iteration 34, loss = 0.68823764\n",
      "Iteration 1830, loss = 0.12402174\n",
      "Iteration 977, loss = 0.25589854\n",
      "Iteration 644, loss = 0.36309184\n",
      "Iteration 619, loss = 0.30763377\n",
      "Iteration 313, loss = 0.35902906\n",
      "Iteration 35, loss = 0.68547787\n",
      "Iteration 1831, loss = 0.12397898\n",
      "Iteration 1523, loss = 0.20221037\n",
      "Iteration 620, loss = 0.30753458\n",
      "Iteration 36, loss = 0.68264671\n",
      "Iteration 621, loss = 0.30746880\n",
      "Iteration 314, loss = 0.35882882\n",
      "Iteration 622, loss = 0.30734513\n",
      "Iteration 1832, loss = 0.12389656\n",
      "Iteration 37, loss = 0.67982471\n",
      "Iteration 1665, loss = 0.22992569\n",
      "Iteration 978, loss = 0.25573426\n",
      "Iteration 1833, loss = 0.12380881\n",
      "Iteration 623, loss = 0.30728205\n",
      "Iteration 979, loss = 0.25566609\n",
      "Iteration 315, loss = 0.35864418\n",
      "Iteration 1524, loss = 0.20224221\n",
      "Iteration 980, loss = 0.25554760\n",
      "Iteration 645, loss = 0.36290576\n",
      "Iteration 38, loss = 0.67702096\n",
      "Iteration 981, loss = 0.25551418\n",
      "Iteration 624, loss = 0.30718716\n",
      "Iteration 316, loss = 0.35849086\n",
      "Iteration 1666, loss = 0.22982195\n",
      "Iteration 1834, loss = 0.12374659\n",
      "Iteration 982, loss = 0.25536458\n",
      "Iteration 625, loss = 0.30715126\n",
      "Iteration 983, loss = 0.25528964\n",
      "Iteration 646, loss = 0.36280476\n",
      "Iteration 39, loss = 0.67405443\n",
      "Iteration 317, loss = 0.35831548\n",
      "Iteration 1667, loss = 0.22967785\n",
      "Iteration 1835, loss = 0.12374463\n",
      "Iteration 626, loss = 0.30702093\n",
      "Iteration 984, loss = 0.25516062\n",
      "Iteration 985, loss = 0.25507211\n",
      "Iteration 1836, loss = 0.12362744\n",
      "Iteration 1525, loss = 0.20204942\n",
      "Iteration 647, loss = 0.36253748\n",
      "Iteration 318, loss = 0.35812854\n",
      "Iteration 986, loss = 0.25495014\n",
      "Iteration 1837, loss = 0.12345798\n",
      "Iteration 40, loss = 0.67110226\n",
      "Iteration 987, loss = 0.25491434\n",
      "Iteration 627, loss = 0.30692768\n",
      "Iteration 1668, loss = 0.22966422\n",
      "Iteration 41, loss = 0.66830276\n",
      "Iteration 628, loss = 0.30685715\n",
      "Iteration 988, loss = 0.25478149\n",
      "Iteration 1838, loss = 0.12337898\n",
      "Iteration 648, loss = 0.36238508\n",
      "Iteration 1669, loss = 0.22945334\n",
      "Iteration 42, loss = 0.66533508\n",
      "Iteration 989, loss = 0.25467267\n",
      "Iteration 629, loss = 0.30679609\n",
      "Iteration 1526, loss = 0.20175599\n",
      "Iteration 990, loss = 0.25456771\n",
      "Iteration 630, loss = 0.30668949\n",
      "Iteration 649, loss = 0.36227574\n",
      "Iteration 43, loss = 0.66240123\n",
      "Iteration 991, loss = 0.25452030\n",
      "Iteration 992, loss = 0.25440594\n",
      "Iteration 631, loss = 0.30662514\n",
      "Iteration 1527, loss = 0.20159905\n",
      "Iteration 319, loss = 0.35798746\n",
      "Iteration 993, loss = 0.25433501\n",
      "Iteration 44, loss = 0.65938940\n",
      "Iteration 632, loss = 0.30651131\n",
      "Iteration 1670, loss = 0.22931504\n",
      "Iteration 994, loss = 0.25419342\n",
      "Iteration 633, loss = 0.30644577\n",
      "Iteration 1839, loss = 0.12330231\n",
      "Iteration 650, loss = 0.36203544\n",
      "Iteration 995, loss = 0.25423293\n",
      "Iteration 1528, loss = 0.20148706\n",
      "Iteration 320, loss = 0.35779684\n",
      "Iteration 996, loss = 0.25399823\n",
      "Iteration 45, loss = 0.65639762\n",
      "Iteration 634, loss = 0.30634820\n",
      "Iteration 1840, loss = 0.12331502\n",
      "Iteration 997, loss = 0.25388314\n",
      "Iteration 998, loss = 0.25380789\n",
      "Iteration 46, loss = 0.65345637\n",
      "Iteration 999, loss = 0.25377015\n",
      "Iteration 1841, loss = 0.12325014\n",
      "Iteration 1529, loss = 0.20146762\n",
      "Iteration 1671, loss = 0.22920036\n",
      "Iteration 635, loss = 0.30629404\n",
      "Iteration 651, loss = 0.36192015\n",
      "Iteration 1000, loss = 0.25360874\n",
      "Iteration 1842, loss = 0.12327132\n",
      "Iteration 1001, loss = 0.25349141\n",
      "Iteration 636, loss = 0.30621244\n",
      "Iteration 321, loss = 0.35762689\n",
      "Iteration 1002, loss = 0.25344033\n",
      "Iteration 47, loss = 0.65036171\n",
      "Iteration 637, loss = 0.30611440\n",
      "Iteration 1843, loss = 0.12297358\n",
      "Iteration 652, loss = 0.36172896\n",
      "Iteration 1003, loss = 0.25336072\n",
      "Iteration 638, loss = 0.30601748\n",
      "Iteration 1672, loss = 0.22909135\n",
      "Iteration 1004, loss = 0.25325557\n",
      "Iteration 322, loss = 0.35746095\n",
      "Iteration 1005, loss = 0.25315043\n",
      "Iteration 639, loss = 0.30595999\n",
      "Iteration 48, loss = 0.64730983\n",
      "Iteration 1006, loss = 0.25305373\n",
      "Iteration 1844, loss = 0.12293575\n",
      "Iteration 640, loss = 0.30585457\n",
      "Iteration 1007, loss = 0.25307350\n",
      "Iteration 1673, loss = 0.22899568\n",
      "Iteration 323, loss = 0.35732276\n",
      "Iteration 653, loss = 0.36152196\n",
      "Iteration 641, loss = 0.30579789\n",
      "Iteration 1008, loss = 0.25298034\n",
      "Iteration 49, loss = 0.64430071\n",
      "Iteration 1530, loss = 0.20124645Iteration 1009, loss = 0.25274014\n",
      "\n",
      "Iteration 642, loss = 0.30571405\n",
      "Iteration 1010, loss = 0.25285613\n",
      "Iteration 643, loss = 0.30561760\n",
      "Iteration 1674, loss = 0.22882408\n",
      "Iteration 50, loss = 0.64116759\n",
      "Iteration 1011, loss = 0.25261966\n",
      "Iteration 654, loss = 0.36140352\n",
      "Iteration 644, loss = 0.30555224\n",
      "Iteration 1012, loss = 0.25247607\n",
      "Iteration 51, loss = 0.63808790\n",
      "Iteration 1845, loss = 0.12294586\n",
      "Iteration 1013, loss = 0.25237661\n",
      "Iteration 52, loss = 0.63500086\n",
      "Iteration 645, loss = 0.30554311\n",
      "Iteration 1014, loss = 0.25233958\n",
      "Iteration 1846, loss = 0.12281157\n",
      "Iteration 1531, loss = 0.20145990\n",
      "Iteration 1015, loss = 0.25223201\n",
      "Iteration 53, loss = 0.63187619\n",
      "Iteration 1016, loss = 0.25207272\n",
      "Iteration 324, loss = 0.35712624\n",
      "Iteration 1847, loss = 0.12270271\n",
      "Iteration 1017, loss = 0.25209508\n",
      "Iteration 1532, loss = 0.20085227\n",
      "Iteration 646, loss = 0.30538421\n",
      "Iteration 325, loss = 0.35697922\n",
      "Iteration 1018, loss = 0.25194737\n",
      "Iteration 1675, loss = 0.22872927\n",
      "Iteration 647, loss = 0.30532463\n",
      "Iteration 655, loss = 0.36119178\n",
      "Iteration 1019, loss = 0.25181844\n",
      "Iteration 648, loss = 0.30520396\n",
      "Iteration 1676, loss = 0.22866656\n",
      "Iteration 1533, loss = 0.20081267\n",
      "Iteration 1020, loss = 0.25170598\n",
      "Iteration 1021, loss = 0.25164840\n",
      "Iteration 649, loss = 0.30514945\n",
      "Iteration 54, loss = 0.62875360Iteration 1022, loss = 0.25152069\n",
      "\n",
      "Iteration 1023, loss = 0.25142961\n",
      "Iteration 656, loss = 0.36111074\n",
      "Iteration 1677, loss = 0.22847690\n",
      "Iteration 650, loss = 0.30509290\n",
      "Iteration 326, loss = 0.35682081\n",
      "Iteration 1848, loss = 0.12265485\n",
      "Iteration 55, loss = 0.62552199\n",
      "Iteration 1024, loss = 0.25133095\n",
      "Iteration 1678, loss = 0.22835835\n",
      "Iteration 1025, loss = 0.25121016\n",
      "Iteration 1849, loss = 0.12251698\n",
      "Iteration 56, loss = 0.62239801\n",
      "Iteration 1026, loss = 0.25114007\n",
      "Iteration 1027, loss = 0.25102943\n",
      "Iteration 657, loss = 0.36083907\n",
      "Iteration 327, loss = 0.35664359\n",
      "Iteration 57, loss = 0.61922682\n",
      "Iteration 1028, loss = 0.25098007\n",
      "Iteration 1029, loss = 0.25101241\n",
      "Iteration 1030, loss = 0.25075110\n",
      "Iteration 328, loss = 0.35649954\n",
      "Iteration 1031, loss = 0.25066164\n",
      "Iteration 58, loss = 0.61598613\n",
      "Iteration 1850, loss = 0.12246150\n",
      "Iteration 329, loss = 0.35633288\n",
      "Iteration 1534, loss = 0.20072028\n",
      "Iteration 658, loss = 0.36070742\n",
      "Iteration 1851, loss = 0.12245308\n",
      "Iteration 1679, loss = 0.22826323\n",
      "Iteration 651, loss = 0.30495831\n",
      "Iteration 652, loss = 0.30489168\n",
      "Iteration 1852, loss = 0.12247390\n",
      "Iteration 1032, loss = 0.25056732\n",
      "Iteration 1680, loss = 0.22813136\n",
      "Iteration 653, loss = 0.30479632\n",
      "Iteration 1033, loss = 0.25044235\n",
      "Iteration 1681, loss = 0.22799520\n",
      "Iteration 659, loss = 0.36051111\n",
      "Iteration 330, loss = 0.35617565\n",
      "Iteration 59, loss = 0.61283809\n",
      "Iteration 1034, loss = 0.25034300Iteration 1682, loss = 0.22785766\n",
      "Iteration 1535, loss = 0.20046898\n",
      "\n",
      "Iteration 654, loss = 0.30474330\n",
      "Iteration 1853, loss = 0.12223431\n",
      "Iteration 655, loss = 0.30465755\n",
      "Iteration 1683, loss = 0.22778732\n",
      "Iteration 60, loss = 0.60955942\n",
      "Iteration 656, loss = 0.30457213\n",
      "Iteration 1536, loss = 0.20030837\n",
      "Iteration 657, loss = 0.30448124\n",
      "Iteration 658, loss = 0.30438384\n",
      "Iteration 331, loss = 0.35605528\n",
      "Iteration 659, loss = 0.30432209\n",
      "Iteration 1035, loss = 0.25027746\n",
      "Iteration 332, loss = 0.35586216\n",
      "Iteration 1684, loss = 0.22761277\n",
      "Iteration 1854, loss = 0.12221339\n",
      "Iteration 1036, loss = 0.25032290\n",
      "Iteration 61, loss = 0.60640111\n",
      "Iteration 1037, loss = 0.25007586\n",
      "Iteration 333, loss = 0.35570992\n",
      "Iteration 660, loss = 0.36031176\n",
      "Iteration 1537, loss = 0.20016290\n",
      "Iteration 1038, loss = 0.24998279\n",
      "Iteration 661, loss = 0.36014605\n",
      "Iteration 1538, loss = 0.20004084\n",
      "Iteration 1039, loss = 0.24987703\n",
      "Iteration 1040, loss = 0.24979831\n",
      "Iteration 62, loss = 0.60317406\n",
      "Iteration 660, loss = 0.30424503\n",
      "Iteration 1041, loss = 0.24972146\n",
      "Iteration 1855, loss = 0.12216217Iteration 661, loss = 0.30414623\n",
      "\n",
      "Iteration 1685, loss = 0.22749386\n",
      "Iteration 334, loss = 0.35553559\n",
      "Iteration 1042, loss = 0.24972624\n",
      "Iteration 63, loss = 0.59990998\n",
      "Iteration 1043, loss = 0.24949645\n",
      "Iteration 662, loss = 0.30406726\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1044, loss = 0.24937298\n",
      "Iteration 1856, loss = 0.12204581\n",
      "Iteration 1686, loss = 0.22738534\n",
      "Iteration 1045, loss = 0.24936293\n",
      "Iteration 64, loss = 0.59663242\n",
      "Iteration 1046, loss = 0.24917943\n",
      "Iteration 1047, loss = 0.24908970\n",
      "Iteration 1048, loss = 0.24907962\n",
      "Iteration 335, loss = 0.35538915\n",
      "Iteration 1857, loss = 0.12201161\n",
      "Iteration 1539, loss = 0.19995981\n",
      "Iteration 336, loss = 0.35522241\n",
      "Iteration 1858, loss = 0.12184210\n",
      "Iteration 662, loss = 0.35999523\n",
      "Iteration 1, loss = 0.80610376\n",
      "Iteration 65, loss = 0.59339354\n",
      "Iteration 1859, loss = 0.12178352\n",
      "Iteration 1687, loss = 0.22731917\n",
      "Iteration 1049, loss = 0.24894012\n",
      "Iteration 1860, loss = 0.12166474\n",
      "Iteration 1540, loss = 0.19967365\n",
      "Iteration 66, loss = 0.59014345\n",
      "Iteration 1050, loss = 0.24880831\n",
      "Iteration 1861, loss = 0.12165130\n",
      "Iteration 2, loss = 0.80540424\n",
      "Iteration 67, loss = 0.58682683\n",
      "Iteration 1051, loss = 0.24873253\n",
      "Iteration 1862, loss = 0.12163849\n",
      "Iteration 1688, loss = 0.22717518\n",
      "Iteration 1052, loss = 0.24860433\n",
      "Iteration 663, loss = 0.35984589\n",
      "Iteration 1053, loss = 0.24853737\n",
      "Iteration 337, loss = 0.35505925\n",
      "Iteration 1054, loss = 0.24848305\n",
      "Iteration 3, loss = 0.80432074\n",
      "Iteration 68, loss = 0.58365110\n",
      "Iteration 1055, loss = 0.24831675\n",
      "Iteration 1863, loss = 0.12165636\n",
      "Iteration 1689, loss = 0.22704015\n",
      "Iteration 1056, loss = 0.24835957\n",
      "Iteration 1541, loss = 0.19949225\n",
      "Iteration 664, loss = 0.35973315\n",
      "Iteration 1864, loss = 0.12153282\n",
      "Iteration 1057, loss = 0.24816220\n",
      "Iteration 1690, loss = 0.22693316\n",
      "Iteration 69, loss = 0.58042453\n",
      "Iteration 1058, loss = 0.24805568\n",
      "Iteration 338, loss = 0.35491664\n",
      "Iteration 1865, loss = 0.12132453\n",
      "Iteration 1059, loss = 0.24796497\n",
      "Iteration 1060, loss = 0.24794628\n",
      "Iteration 4, loss = 0.80293221\n",
      "Iteration 665, loss = 0.35946237\n",
      "Iteration 1691, loss = 0.22681971\n",
      "Iteration 1866, loss = 0.12134895\n",
      "Iteration 1061, loss = 0.24775169\n",
      "Iteration 70, loss = 0.57716084\n",
      "Iteration 1542, loss = 0.19940926\n",
      "Iteration 339, loss = 0.35478735\n",
      "Iteration 1692, loss = 0.22662735\n",
      "Iteration 1867, loss = 0.12120063\n",
      "Iteration 1062, loss = 0.24763260\n",
      "Iteration 340, loss = 0.35462046\n",
      "Iteration 5, loss = 0.80136745\n",
      "Iteration 71, loss = 0.57386913\n",
      "Iteration 1063, loss = 0.24752447\n",
      "Iteration 1543, loss = 0.19932016\n",
      "Iteration 1064, loss = 0.24751373\n",
      "Iteration 1868, loss = 0.12119257\n",
      "Iteration 1065, loss = 0.24740317\n",
      "Iteration 341, loss = 0.35444265\n",
      "Iteration 72, loss = 0.57066690\n",
      "Iteration 1066, loss = 0.24725702\n",
      "Iteration 666, loss = 0.35930986\n",
      "Iteration 1693, loss = 0.22665359\n",
      "Iteration 1869, loss = 0.12105629\n",
      "Iteration 1067, loss = 0.24722612\n",
      "Iteration 1068, loss = 0.24713816\n",
      "Iteration 342, loss = 0.35430436\n",
      "Iteration 1870, loss = 0.12105478\n",
      "Iteration 1069, loss = 0.24696092\n",
      "Iteration 667, loss = 0.35911370\n",
      "Iteration 1694, loss = 0.22641048\n",
      "Iteration 1070, loss = 0.24691692\n",
      "Iteration 1871, loss = 0.12090982\n",
      "Iteration 1544, loss = 0.19911184\n",
      "Iteration 6, loss = 0.79967265\n",
      "Iteration 1872, loss = 0.12090199\n",
      "Iteration 1071, loss = 0.24679558\n",
      "Iteration 668, loss = 0.35893842\n",
      "Iteration 343, loss = 0.35420903\n",
      "Iteration 73, loss = 0.56746498\n",
      "Iteration 1695, loss = 0.22629462\n",
      "Iteration 1873, loss = 0.12076871\n",
      "Iteration 74, loss = 0.56419190\n",
      "Iteration 1072, loss = 0.24670395\n",
      "Iteration 7, loss = 0.79791140\n",
      "Iteration 1874, loss = 0.12077964\n",
      "Iteration 75, loss = 0.56110061\n",
      "Iteration 1073, loss = 0.24656613\n",
      "Iteration 344, loss = 0.35400360\n",
      "Iteration 1875, loss = 0.12062025\n",
      "Iteration 1696, loss = 0.22621026\n",
      "Iteration 76, loss = 0.55788788\n",
      "Iteration 1545, loss = 0.19890950\n",
      "Iteration 669, loss = 0.35877135\n",
      "Iteration 1074, loss = 0.24647357\n",
      "Iteration 77, loss = 0.55477707\n",
      "Iteration 345, loss = 0.35384350\n",
      "Iteration 1075, loss = 0.24644724\n",
      "Iteration 1076, loss = 0.24635234\n",
      "Iteration 8, loss = 0.79605308\n",
      "Iteration 1697, loss = 0.22608806\n",
      "Iteration 1077, loss = 0.24619955\n",
      "Iteration 1546, loss = 0.19885845\n",
      "Iteration 346, loss = 0.35373920\n",
      "Iteration 1078, loss = 0.24622125\n",
      "Iteration 1876, loss = 0.12060210\n",
      "Iteration 78, loss = 0.55160025\n",
      "Iteration 1079, loss = 0.24601242\n",
      "Iteration 1080, loss = 0.24589831\n",
      "Iteration 9, loss = 0.79427289\n",
      "Iteration 347, loss = 0.35356089\n",
      "Iteration 79, loss = 0.54841328\n",
      "Iteration 1877, loss = 0.12053377Iteration 670, loss = 0.35863131\n",
      "\n",
      "Iteration 1081, loss = 0.24579175\n",
      "Iteration 1082, loss = 0.24570910\n",
      "Iteration 1698, loss = 0.22593369\n",
      "Iteration 1547, loss = 0.19883478\n",
      "Iteration 1083, loss = 0.24560357\n",
      "Iteration 1084, loss = 0.24552965\n",
      "Iteration 348, loss = 0.35339055\n",
      "Iteration 1085, loss = 0.24547672\n",
      "Iteration 80, loss = 0.54533997\n",
      "Iteration 1878, loss = 0.12040667\n",
      "Iteration 671, loss = 0.35839011\n",
      "Iteration 1086, loss = 0.24546373\n",
      "Iteration 349, loss = 0.35324392\n",
      "Iteration 1087, loss = 0.24526409\n",
      "Iteration 1699, loss = 0.22587529\n",
      "Iteration 10, loss = 0.79229606\n",
      "Iteration 81, loss = 0.54224650\n",
      "Iteration 672, loss = 0.35832819\n",
      "Iteration 350, loss = 0.35309380\n",
      "Iteration 1088, loss = 0.24520160\n",
      "Iteration 1548, loss = 0.19852395\n",
      "Iteration 1879, loss = 0.12038188\n",
      "Iteration 82, loss = 0.53920191\n",
      "Iteration 1089, loss = 0.24506002\n",
      "Iteration 1700, loss = 0.22568944\n",
      "Iteration 673, loss = 0.35807293\n",
      "Iteration 351, loss = 0.35295511\n",
      "Iteration 83, loss = 0.53615212\n",
      "Iteration 11, loss = 0.79036472\n",
      "Iteration 1090, loss = 0.24496950\n",
      "Iteration 674, loss = 0.35791227\n",
      "Iteration 1880, loss = 0.12027792\n",
      "Iteration 1091, loss = 0.24488348\n",
      "Iteration 1701, loss = 0.22556879\n",
      "Iteration 352, loss = 0.35281217\n",
      "Iteration 1092, loss = 0.24479624\n",
      "Iteration 84, loss = 0.53316799\n",
      "Iteration 1093, loss = 0.24468430\n",
      "Iteration 12, loss = 0.78850006\n",
      "Iteration 1702, loss = 0.22551423\n",
      "Iteration 1094, loss = 0.24470213\n",
      "Iteration 1881, loss = 0.12019674\n",
      "Iteration 85, loss = 0.53012280\n",
      "Iteration 1095, loss = 0.24449908\n",
      "Iteration 1549, loss = 0.19841844\n",
      "Iteration 1096, loss = 0.24437347\n",
      "Iteration 1097, loss = 0.24430694\n",
      "Iteration 86, loss = 0.52725781\n",
      "Iteration 353, loss = 0.35267054\n",
      "Iteration 1098, loss = 0.24420107\n",
      "Iteration 1882, loss = 0.12011634\n",
      "Iteration 1099, loss = 0.24413052\n",
      "Iteration 675, loss = 0.35769866\n",
      "Iteration 1100, loss = 0.24413227\n",
      "Iteration 13, loss = 0.78664537\n",
      "Iteration 1101, loss = 0.24390389\n",
      "Iteration 1703, loss = 0.22543321\n",
      "Iteration 1102, loss = 0.24393219\n",
      "Iteration 1103, loss = 0.24376432\n",
      "Iteration 676, loss = 0.35751314\n",
      "Iteration 87, loss = 0.52436041\n",
      "Iteration 1704, loss = 0.22519869\n",
      "Iteration 1104, loss = 0.24363176\n",
      "Iteration 1550, loss = 0.19830574\n",
      "Iteration 1883, loss = 0.12007226\n",
      "Iteration 1105, loss = 0.24353981\n",
      "Iteration 1705, loss = 0.22510086\n",
      "Iteration 1106, loss = 0.24349708Iteration 354, loss = 0.35252045\n",
      "\n",
      "Iteration 14, loss = 0.78469690\n",
      "Iteration 1884, loss = 0.12003268\n",
      "Iteration 88, loss = 0.52150460\n",
      "Iteration 1107, loss = 0.24339733\n",
      "Iteration 1551, loss = 0.19822233\n",
      "Iteration 1108, loss = 0.24327587\n",
      "Iteration 1885, loss = 0.11994338\n",
      "Iteration 1706, loss = 0.22495464\n",
      "Iteration 1109, loss = 0.24315067\n",
      "Iteration 15, loss = 0.78282509\n",
      "Iteration 1552, loss = 0.19793030\n",
      "Iteration 677, loss = 0.35738983\n",
      "Iteration 1886, loss = 0.11993335\n",
      "Iteration 1110, loss = 0.24307141\n",
      "Iteration 89, loss = 0.51880493\n",
      "Iteration 355, loss = 0.35236708\n",
      "Iteration 1707, loss = 0.22485655\n",
      "Iteration 1111, loss = 0.24300337\n",
      "Iteration 90, loss = 0.51593576\n",
      "Iteration 16, loss = 0.78090077\n",
      "Iteration 356, loss = 0.35221491\n",
      "Iteration 91, loss = 0.51321633\n",
      "Iteration 1708, loss = 0.22476711\n",
      "Iteration 1112, loss = 0.24289388\n",
      "Iteration 678, loss = 0.35720466\n",
      "Iteration 357, loss = 0.35211174\n",
      "Iteration 1887, loss = 0.11985892\n",
      "Iteration 1113, loss = 0.24281177\n",
      "Iteration 92, loss = 0.51054272\n",
      "Iteration 1888, loss = 0.11996748\n",
      "Iteration 1114, loss = 0.24270066\n",
      "Iteration 1115, loss = 0.24263488\n",
      "Iteration 1553, loss = 0.19783259\n",
      "Iteration 1116, loss = 0.24255277\n",
      "Iteration 1889, loss = 0.11984239\n",
      "Iteration 358, loss = 0.35197109\n",
      "Iteration 93, loss = 0.50795063\n",
      "Iteration 1117, loss = 0.24241597\n",
      "Iteration 1890, loss = 0.11964739\n",
      "Iteration 1709, loss = 0.22460054\n",
      "Iteration 94, loss = 0.50530524\n",
      "Iteration 679, loss = 0.35701735\n",
      "Iteration 17, loss = 0.77903680\n",
      "Iteration 1118, loss = 0.24231252\n",
      "Iteration 1119, loss = 0.24227519\n",
      "Iteration 1120, loss = 0.24222624\n",
      "Iteration 1554, loss = 0.19778714\n",
      "Iteration 359, loss = 0.35180322\n",
      "Iteration 1121, loss = 0.24208757\n",
      "Iteration 1891, loss = 0.11949099\n",
      "Iteration 1122, loss = 0.24199875\n",
      "Iteration 1123, loss = 0.24190642\n",
      "Iteration 95, loss = 0.50272987\n",
      "Iteration 1124, loss = 0.24179040\n",
      "Iteration 18, loss = 0.77718024\n",
      "Iteration 680, loss = 0.35687457\n",
      "Iteration 1555, loss = 0.19759566\n",
      "Iteration 1710, loss = 0.22457648\n",
      "Iteration 1892, loss = 0.11945091\n",
      "Iteration 1125, loss = 0.24166469\n",
      "Iteration 360, loss = 0.35167079\n",
      "Iteration 1126, loss = 0.24154756\n",
      "Iteration 96, loss = 0.50026337\n",
      "Iteration 1893, loss = 0.11949551\n",
      "Iteration 1127, loss = 0.24151069\n",
      "Iteration 681, loss = 0.35668592\n",
      "Iteration 1128, loss = 0.24135305\n",
      "Iteration 1711, loss = 0.22445542\n",
      "Iteration 97, loss = 0.49782689\n",
      "Iteration 1129, loss = 0.24125535\n",
      "Iteration 19, loss = 0.77528366\n",
      "Iteration 1556, loss = 0.19758406\n",
      "Iteration 1130, loss = 0.24118380\n",
      "Iteration 361, loss = 0.35152363\n",
      "Iteration 1894, loss = 0.11934047\n",
      "Iteration 1131, loss = 0.24111341\n",
      "Iteration 682, loss = 0.35654421\n",
      "Iteration 1132, loss = 0.24097273\n",
      "Iteration 1557, loss = 0.19718642\n",
      "Iteration 1133, loss = 0.24086383\n",
      "Iteration 98, loss = 0.49536683\n",
      "Iteration 20, loss = 0.77341433\n",
      "Iteration 1712, loss = 0.22425910\n",
      "Iteration 1134, loss = 0.24076699\n",
      "Iteration 1895, loss = 0.11923751\n",
      "Iteration 683, loss = 0.35633385\n",
      "Iteration 99, loss = 0.49302307\n",
      "Iteration 1135, loss = 0.24070725\n",
      "Iteration 1558, loss = 0.19708717\n",
      "Iteration 362, loss = 0.35140558\n",
      "Iteration 100, loss = 0.49069436\n",
      "Iteration 1136, loss = 0.24062356\n",
      "Iteration 21, loss = 0.77151137\n",
      "Iteration 1896, loss = 0.11918749\n",
      "Iteration 684, loss = 0.35614175\n",
      "Iteration 1713, loss = 0.22412672\n",
      "Iteration 1137, loss = 0.24061460\n",
      "Iteration 1138, loss = 0.24041600\n",
      "Iteration 101, loss = 0.48835803\n",
      "Iteration 1559, loss = 0.19696712\n",
      "Iteration 1897, loss = 0.11911688\n",
      "Iteration 363, loss = 0.35123681\n",
      "Iteration 102, loss = 0.48608422\n",
      "Iteration 22, loss = 0.76961205\n",
      "Iteration 1139, loss = 0.24039031\n",
      "Iteration 1898, loss = 0.11899823\n",
      "Iteration 103, loss = 0.48393659\n",
      "Iteration 1140, loss = 0.24019806\n",
      "Iteration 1714, loss = 0.22399807\n",
      "Iteration 1141, loss = 0.24010575\n",
      "Iteration 1142, loss = 0.23998247\n",
      "Iteration 1560, loss = 0.19685955\n",
      "Iteration 685, loss = 0.35596957\n",
      "Iteration 1715, loss = 0.22396205\n",
      "Iteration 364, loss = 0.35105847\n",
      "Iteration 1899, loss = 0.11902585\n",
      "Iteration 1143, loss = 0.23991168\n",
      "Iteration 104, loss = 0.48170894\n",
      "Iteration 1144, loss = 0.23986707\n",
      "Iteration 1145, loss = 0.23973065\n",
      "Iteration 1900, loss = 0.11887605\n",
      "Iteration 1561, loss = 0.19665324\n",
      "Iteration 1146, loss = 0.23959127\n",
      "Iteration 365, loss = 0.35093932\n",
      "Iteration 1716, loss = 0.22376077\n",
      "Iteration 1147, loss = 0.23953435\n",
      "Iteration 1148, loss = 0.23942986\n",
      "Iteration 1901, loss = 0.11889247\n",
      "Iteration 1149, loss = 0.23938781\n",
      "Iteration 1150, loss = 0.23924000\n",
      "Iteration 686, loss = 0.35577038\n",
      "Iteration 366, loss = 0.35078384\n",
      "Iteration 1151, loss = 0.23915451\n",
      "Iteration 1152, loss = 0.23904443\n",
      "Iteration 1902, loss = 0.11878497\n",
      "Iteration 1717, loss = 0.22364180\n",
      "Iteration 23, loss = 0.76769696\n",
      "Iteration 1562, loss = 0.19671319\n",
      "Iteration 1153, loss = 0.23892071\n",
      "Iteration 367, loss = 0.35066277\n",
      "Iteration 1154, loss = 0.23884266\n",
      "Iteration 105, loss = 0.47955087\n",
      "Iteration 687, loss = 0.35559945\n",
      "Iteration 1718, loss = 0.22351703\n",
      "Iteration 368, loss = 0.35051261\n",
      "Iteration 1155, loss = 0.23878796\n",
      "Iteration 1903, loss = 0.11880849\n",
      "Iteration 1156, loss = 0.23867854\n",
      "Iteration 1157, loss = 0.23854811\n",
      "Iteration 1904, loss = 0.11872748\n",
      "Iteration 688, loss = 0.35556448\n",
      "Iteration 1563, loss = 0.19636734\n",
      "Iteration 1158, loss = 0.23840730\n",
      "Iteration 1719, loss = 0.22340511\n",
      "Iteration 369, loss = 0.35040676\n",
      "Iteration 1159, loss = 0.23839455\n",
      "Iteration 106, loss = 0.47747449\n",
      "Iteration 1160, loss = 0.23822892\n",
      "Iteration 1161, loss = 0.23821884\n",
      "Iteration 370, loss = 0.35023228\n",
      "Iteration 1162, loss = 0.23808069\n",
      "Iteration 1905, loss = 0.11868699\n",
      "Iteration 1564, loss = 0.19627539\n",
      "Iteration 1163, loss = 0.23797617\n",
      "Iteration 107, loss = 0.47541467\n",
      "Iteration 1720, loss = 0.22328598\n",
      "Iteration 1164, loss = 0.23787481\n",
      "Iteration 24, loss = 0.76585337\n",
      "Iteration 1165, loss = 0.23774242\n",
      "Iteration 1906, loss = 0.11853555\n",
      "Iteration 1166, loss = 0.23765029\n",
      "Iteration 371, loss = 0.35012379\n",
      "Iteration 1565, loss = 0.19608431\n",
      "Iteration 1167, loss = 0.23760315\n",
      "Iteration 689, loss = 0.35524232\n",
      "Iteration 1168, loss = 0.23756386\n",
      "Iteration 1721, loss = 0.22316762\n",
      "Iteration 1907, loss = 0.11858382\n",
      "Iteration 1169, loss = 0.23736547\n",
      "Iteration 1170, loss = 0.23730799\n",
      "Iteration 1171, loss = 0.23715676\n",
      "Iteration 1172, loss = 0.23712233\n",
      "Iteration 1908, loss = 0.11838711\n",
      "Iteration 372, loss = 0.34994126\n",
      "Iteration 1173, loss = 0.23700422\n",
      "Iteration 25, loss = 0.76386440\n",
      "Iteration 1722, loss = 0.22303544\n",
      "Iteration 1566, loss = 0.19600006\n",
      "Iteration 1174, loss = 0.23694873\n",
      "Iteration 690, loss = 0.35507007\n",
      "Iteration 373, loss = 0.34981469\n",
      "Iteration 1723, loss = 0.22296818\n",
      "Iteration 1175, loss = 0.23678229\n",
      "Iteration 1909, loss = 0.11832701\n",
      "Iteration 1176, loss = 0.23667096\n",
      "Iteration 1177, loss = 0.23663564\n",
      "Iteration 691, loss = 0.35494679\n",
      "Iteration 108, loss = 0.47338369\n",
      "Iteration 1724, loss = 0.22289252\n",
      "Iteration 1178, loss = 0.23652080\n",
      "Iteration 374, loss = 0.34970152\n",
      "Iteration 1179, loss = 0.23641382\n",
      "Iteration 26, loss = 0.76190795\n",
      "Iteration 1725, loss = 0.22274747\n",
      "Iteration 1567, loss = 0.19605549\n",
      "Iteration 1910, loss = 0.11826897\n",
      "Iteration 692, loss = 0.35472839\n",
      "Iteration 1180, loss = 0.23634338\n",
      "Iteration 375, loss = 0.34956140\n",
      "Iteration 1181, loss = 0.23622668\n",
      "Iteration 1726, loss = 0.22256489\n",
      "Iteration 1182, loss = 0.23608167\n",
      "Iteration 1568, loss = 0.19587409\n",
      "Iteration 1911, loss = 0.11817661\n",
      "Iteration 376, loss = 0.34940888\n",
      "Iteration 27, loss = 0.75992119\n",
      "Iteration 1183, loss = 0.23599915\n",
      "Iteration 693, loss = 0.35457532\n",
      "Iteration 1912, loss = 0.11816996Iteration 1184, loss = 0.23589137\n",
      "\n",
      "Iteration 1185, loss = 0.23579796\n",
      "Iteration 377, loss = 0.34930746\n",
      "Iteration 1727, loss = 0.22248591\n",
      "Iteration 1186, loss = 0.23573497\n",
      "Iteration 1187, loss = 0.23559325\n",
      "Iteration 28, loss = 0.75790622\n",
      "Iteration 1913, loss = 0.11807667\n",
      "Iteration 1188, loss = 0.23551379\n",
      "Iteration 1728, loss = 0.22232290\n",
      "Iteration 694, loss = 0.35437472\n",
      "Iteration 1569, loss = 0.19596829\n",
      "Iteration 1914, loss = 0.11799349\n",
      "Iteration 1189, loss = 0.23544564\n",
      "Iteration 378, loss = 0.34912584\n",
      "Iteration 1190, loss = 0.23536138\n",
      "Iteration 109, loss = 0.47147548Iteration 1729, loss = 0.22232238\n",
      "\n",
      "Iteration 1915, loss = 0.11792230\n",
      "Iteration 29, loss = 0.75588529\n",
      "Iteration 695, loss = 0.35416865\n",
      "Iteration 1191, loss = 0.23525545\n",
      "Iteration 1916, loss = 0.11804726\n",
      "Iteration 379, loss = 0.34899405\n",
      "Iteration 1192, loss = 0.23518634\n",
      "Iteration 110, loss = 0.46948670\n",
      "Iteration 1193, loss = 0.23505672\n",
      "Iteration 1194, loss = 0.23495002\n",
      "Iteration 1570, loss = 0.19535290\n",
      "Iteration 1730, loss = 0.22216976\n",
      "Iteration 1195, loss = 0.23485680\n",
      "Iteration 1196, loss = 0.23479627\n",
      "Iteration 30, loss = 0.75387920\n",
      "Iteration 380, loss = 0.34884917\n",
      "Iteration 1917, loss = 0.11781719\n",
      "Iteration 1197, loss = 0.23463675\n",
      "Iteration 1731, loss = 0.22206716\n",
      "Iteration 111, loss = 0.46759348\n",
      "Iteration 1198, loss = 0.23456532\n",
      "Iteration 696, loss = 0.35401469\n",
      "Iteration 1199, loss = 0.23465733\n",
      "Iteration 1918, loss = 0.11780020\n",
      "Iteration 381, loss = 0.34874712\n",
      "Iteration 1200, loss = 0.23441933\n",
      "Iteration 1732, loss = 0.22183795\n",
      "Iteration 31, loss = 0.75170470\n",
      "Iteration 1571, loss = 0.19525500\n",
      "Iteration 1201, loss = 0.23431705\n",
      "Iteration 112, loss = 0.46576669\n",
      "Iteration 1202, loss = 0.23425006\n",
      "Iteration 113, loss = 0.46386001\n",
      "Iteration 1919, loss = 0.11763827\n",
      "Iteration 1733, loss = 0.22184792\n",
      "Iteration 1203, loss = 0.23410133\n",
      "Iteration 114, loss = 0.46213297\n",
      "Iteration 1920, loss = 0.11770982\n",
      "Iteration 382, loss = 0.34857718\n",
      "Iteration 1204, loss = 0.23395829\n",
      "Iteration 1572, loss = 0.19510231\n",
      "Iteration 697, loss = 0.35385040\n",
      "Iteration 1921, loss = 0.11758896\n",
      "Iteration 1734, loss = 0.22160257\n",
      "Iteration 32, loss = 0.74965161\n",
      "Iteration 1922, loss = 0.11743876\n",
      "Iteration 1205, loss = 0.23384603\n",
      "Iteration 1735, loss = 0.22151301\n",
      "Iteration 1923, loss = 0.11756192\n",
      "Iteration 115, loss = 0.46038310\n",
      "Iteration 1206, loss = 0.23376005\n",
      "Iteration 1207, loss = 0.23366962\n",
      "Iteration 698, loss = 0.35373253\n",
      "Iteration 1924, loss = 0.11739618\n",
      "Iteration 1208, loss = 0.23359481\n",
      "Iteration 1573, loss = 0.19493459\n",
      "Iteration 1209, loss = 0.23348369\n",
      "Iteration 1736, loss = 0.22144016\n",
      "Iteration 383, loss = 0.34854513\n",
      "Iteration 1210, loss = 0.23350637\n",
      "Iteration 116, loss = 0.45860250\n",
      "Iteration 33, loss = 0.74748098\n",
      "Iteration 1925, loss = 0.11735470\n",
      "Iteration 1574, loss = 0.19486842\n",
      "Iteration 1211, loss = 0.23335817\n",
      "Iteration 384, loss = 0.34834506\n",
      "Iteration 1926, loss = 0.11723793\n",
      "Iteration 1212, loss = 0.23319671\n",
      "Iteration 699, loss = 0.35350887\n",
      "Iteration 1213, loss = 0.23314740\n",
      "Iteration 385, loss = 0.34817687\n",
      "Iteration 117, loss = 0.45697984\n",
      "Iteration 1927, loss = 0.11714976Iteration 34, loss = 0.74534272\n",
      "Iteration 1214, loss = 0.23297199\n",
      "\n",
      "Iteration 1575, loss = 0.19473090\n",
      "Iteration 1737, loss = 0.22130659\n",
      "Iteration 700, loss = 0.35335395\n",
      "Iteration 1215, loss = 0.23288630\n",
      "Iteration 1928, loss = 0.11712432\n",
      "Iteration 1216, loss = 0.23282002\n",
      "Iteration 1217, loss = 0.23285186\n",
      "Iteration 35, loss = 0.74312446\n",
      "Iteration 386, loss = 0.34805541\n",
      "Iteration 1929, loss = 0.11701941\n",
      "Iteration 118, loss = 0.45534139\n",
      "Iteration 1738, loss = 0.22117191\n",
      "Iteration 1218, loss = 0.23265047\n",
      "Iteration 701, loss = 0.35322904\n",
      "Iteration 1219, loss = 0.23258910\n",
      "Iteration 1930, loss = 0.11700546\n",
      "Iteration 1576, loss = 0.19459685\n",
      "Iteration 1220, loss = 0.23248945\n",
      "Iteration 119, loss = 0.45368736\n",
      "Iteration 36, loss = 0.74087388\n",
      "Iteration 1739, loss = 0.22106917\n",
      "Iteration 387, loss = 0.34790498\n",
      "Iteration 1931, loss = 0.11698234\n",
      "Iteration 1221, loss = 0.23231636\n",
      "Iteration 120, loss = 0.45214684\n",
      "Iteration 1932, loss = 0.11686094\n",
      "Iteration 1222, loss = 0.23223123\n",
      "Iteration 388, loss = 0.34779221\n",
      "Iteration 702, loss = 0.35300620\n",
      "Iteration 121, loss = 0.45056147\n",
      "Iteration 1740, loss = 0.22093438\n",
      "Iteration 1223, loss = 0.23216754\n",
      "Iteration 37, loss = 0.73865138\n",
      "Iteration 1933, loss = 0.11685544\n",
      "Iteration 122, loss = 0.44910869\n",
      "Iteration 1224, loss = 0.23201696\n",
      "Iteration 1741, loss = 0.22093575\n",
      "Iteration 1934, loss = 0.11674850\n",
      "Iteration 389, loss = 0.34765962\n",
      "Iteration 1225, loss = 0.23199321\n",
      "Iteration 1577, loss = 0.19491280\n",
      "Iteration 123, loss = 0.44752669\n",
      "Iteration 1742, loss = 0.22071258\n",
      "Iteration 1226, loss = 0.23183564\n",
      "Iteration 38, loss = 0.73633570\n",
      "Iteration 390, loss = 0.34755046\n",
      "Iteration 124, loss = 0.44610686\n",
      "Iteration 1935, loss = 0.11666948\n",
      "Iteration 1227, loss = 0.23174857\n",
      "Iteration 1743, loss = 0.22055497\n",
      "Iteration 1228, loss = 0.23166782\n",
      "Iteration 1578, loss = 0.19427315\n",
      "Iteration 703, loss = 0.35277111\n",
      "Iteration 1229, loss = 0.23155123\n",
      "Iteration 125, loss = 0.44466511\n",
      "Iteration 391, loss = 0.34738393\n",
      "Iteration 1936, loss = 0.11668289\n",
      "Iteration 1230, loss = 0.23149624\n",
      "Iteration 1231, loss = 0.23143050\n",
      "Iteration 1744, loss = 0.22045100\n",
      "Iteration 1232, loss = 0.23126548\n",
      "Iteration 1937, loss = 0.11655077\n",
      "Iteration 392, loss = 0.34724379\n",
      "Iteration 1233, loss = 0.23116244\n",
      "Iteration 1234, loss = 0.23108330\n",
      "Iteration 126, loss = 0.44324941\n",
      "Iteration 1235, loss = 0.23102106\n",
      "Iteration 39, loss = 0.73394315\n",
      "Iteration 1938, loss = 0.11650079\n",
      "Iteration 1236, loss = 0.23089955\n",
      "Iteration 1237, loss = 0.23092804\n",
      "Iteration 1579, loss = 0.19425164\n",
      "Iteration 1745, loss = 0.22037609\n",
      "Iteration 704, loss = 0.35263543\n",
      "Iteration 1238, loss = 0.23073239\n",
      "Iteration 127, loss = 0.44184378\n",
      "Iteration 393, loss = 0.34712726\n",
      "Iteration 1939, loss = 0.11650622\n",
      "Iteration 1746, loss = 0.22022860\n",
      "Iteration 1239, loss = 0.23061161\n",
      "Iteration 705, loss = 0.35248401\n",
      "Iteration 40, loss = 0.73158109\n",
      "Iteration 1240, loss = 0.23046186\n",
      "Iteration 394, loss = 0.34698750\n",
      "Iteration 1747, loss = 0.22006946\n",
      "Iteration 1940, loss = 0.11640033\n",
      "Iteration 1241, loss = 0.23038404\n",
      "Iteration 128, loss = 0.44050584\n",
      "Iteration 1242, loss = 0.23032164\n",
      "Iteration 1580, loss = 0.19402020\n",
      "Iteration 1243, loss = 0.23025086\n",
      "Iteration 1941, loss = 0.11632531\n",
      "Iteration 129, loss = 0.43917926\n",
      "Iteration 1748, loss = 0.21996773\n",
      "Iteration 1244, loss = 0.23010324\n",
      "Iteration 41, loss = 0.72916324\n",
      "Iteration 395, loss = 0.34689783\n",
      "Iteration 1245, loss = 0.23010890\n",
      "Iteration 1942, loss = 0.11629935\n",
      "Iteration 1749, loss = 0.21987521\n",
      "Iteration 706, loss = 0.35225848\n",
      "Iteration 1246, loss = 0.22995861\n",
      "Iteration 130, loss = 0.43784801\n",
      "Iteration 1943, loss = 0.11615140\n",
      "Iteration 1247, loss = 0.22985323\n",
      "Iteration 1248, loss = 0.22975222\n",
      "Iteration 1249, loss = 0.22969462\n",
      "Iteration 1944, loss = 0.11610856\n",
      "Iteration 1750, loss = 0.21974157\n",
      "Iteration 42, loss = 0.72670274\n",
      "Iteration 1581, loss = 0.19397987\n",
      "Iteration 1250, loss = 0.22954607\n",
      "Iteration 396, loss = 0.34675657\n",
      "Iteration 131, loss = 0.43659667\n",
      "Iteration 1251, loss = 0.22942305\n",
      "Iteration 707, loss = 0.35210848\n",
      "Iteration 1252, loss = 0.22933227\n",
      "Iteration 1751, loss = 0.21959019\n",
      "Iteration 1253, loss = 0.22939182\n",
      "Iteration 1945, loss = 0.11604770\n",
      "Iteration 1254, loss = 0.22920293\n",
      "Iteration 43, loss = 0.72424977\n",
      "Iteration 397, loss = 0.34658817\n",
      "Iteration 1255, loss = 0.22904180\n",
      "Iteration 132, loss = 0.43531253\n",
      "Iteration 1256, loss = 0.22898337\n",
      "Iteration 1946, loss = 0.11597712\n",
      "Iteration 1752, loss = 0.21949671\n",
      "Iteration 1582, loss = 0.19387241\n",
      "Iteration 1257, loss = 0.22881485\n",
      "Iteration 133, loss = 0.43415933\n",
      "Iteration 1947, loss = 0.11599304\n",
      "Iteration 1258, loss = 0.22874777\n",
      "Iteration 44, loss = 0.72164722\n",
      "Iteration 1259, loss = 0.22865725\n",
      "Iteration 1753, loss = 0.21937629\n",
      "Iteration 1948, loss = 0.11599766\n",
      "Iteration 398, loss = 0.34646363\n",
      "Iteration 134, loss = 0.43286136\n",
      "Iteration 708, loss = 0.35194710\n",
      "Iteration 1260, loss = 0.22861877\n",
      "Iteration 1754, loss = 0.21927639\n",
      "Iteration 135, loss = 0.43171343\n",
      "Iteration 1261, loss = 0.22842910\n",
      "Iteration 399, loss = 0.34638223\n",
      "Iteration 1949, loss = 0.11589098\n",
      "Iteration 1583, loss = 0.19368175\n",
      "Iteration 1755, loss = 0.21920131\n",
      "Iteration 1262, loss = 0.22833818\n",
      "Iteration 709, loss = 0.35177424\n",
      "Iteration 136, loss = 0.43054030\n",
      "Iteration 45, loss = 0.71908145\n",
      "Iteration 1263, loss = 0.22825549\n",
      "Iteration 1264, loss = 0.22816556\n",
      "Iteration 1265, loss = 0.22806531\n",
      "Iteration 1950, loss = 0.11578784\n",
      "Iteration 1756, loss = 0.21903054\n",
      "Iteration 137, loss = 0.42938954\n",
      "Iteration 400, loss = 0.34623521\n",
      "Iteration 1266, loss = 0.22795896\n",
      "Iteration 138, loss = 0.42825327\n",
      "Iteration 1584, loss = 0.19347239\n",
      "Iteration 710, loss = 0.35155911\n",
      "Iteration 1267, loss = 0.22789894\n",
      "Iteration 1757, loss = 0.21892644\n",
      "Iteration 139, loss = 0.42720039\n",
      "Iteration 401, loss = 0.34608881\n",
      "Iteration 1268, loss = 0.22777638\n",
      "Iteration 140, loss = 0.42610769\n",
      "Iteration 1585, loss = 0.19337798\n",
      "Iteration 46, loss = 0.71645130\n",
      "Iteration 1269, loss = 0.22768713\n",
      "Iteration 402, loss = 0.34596729\n",
      "Iteration 1951, loss = 0.11582656\n",
      "Iteration 1270, loss = 0.22759627\n",
      "Iteration 1271, loss = 0.22745830\n",
      "Iteration 711, loss = 0.35141407\n",
      "Iteration 1952, loss = 0.11569303\n",
      "Iteration 141, loss = 0.42501792\n",
      "Iteration 1272, loss = 0.22742624\n",
      "Iteration 1758, loss = 0.21890813\n",
      "Iteration 1273, loss = 0.22732074\n",
      "Iteration 1953, loss = 0.11558451\n",
      "Iteration 47, loss = 0.71370320\n",
      "Iteration 712, loss = 0.35129242\n",
      "Iteration 1274, loss = 0.22716543\n",
      "Iteration 142, loss = 0.42397326\n",
      "Iteration 1275, loss = 0.22712753\n",
      "Iteration 1954, loss = 0.11553501\n",
      "Iteration 403, loss = 0.34585038\n",
      "Iteration 143, loss = 0.42296467\n",
      "Iteration 1276, loss = 0.22706236\n",
      "Iteration 713, loss = 0.35106538\n",
      "Iteration 1586, loss = 0.19328546\n",
      "Iteration 1955, loss = 0.11555740\n",
      "Iteration 1277, loss = 0.22692918\n",
      "Iteration 48, loss = 0.71104574\n",
      "Iteration 1278, loss = 0.22674664\n",
      "Iteration 404, loss = 0.34571355\n",
      "Iteration 1759, loss = 0.21870741\n",
      "Iteration 1279, loss = 0.22667626\n",
      "Iteration 1280, loss = 0.22661466\n",
      "Iteration 1281, loss = 0.22654046\n",
      "Iteration 1282, loss = 0.22643155\n",
      "Iteration 144, loss = 0.42195091\n",
      "Iteration 1283, loss = 0.22634469\n",
      "Iteration 714, loss = 0.35085740\n",
      "Iteration 1956, loss = 0.11553863\n",
      "Iteration 1587, loss = 0.19318022\n",
      "Iteration 405, loss = 0.34562137\n",
      "Iteration 1284, loss = 0.22615021\n",
      "Iteration 1285, loss = 0.22604431\n",
      "Iteration 145, loss = 0.42094812\n",
      "Iteration 49, loss = 0.70826425\n",
      "Iteration 1286, loss = 0.22597990\n",
      "Iteration 1287, loss = 0.22588051\n",
      "Iteration 1760, loss = 0.21853357\n",
      "Iteration 1288, loss = 0.22575663\n",
      "Iteration 715, loss = 0.35088059\n",
      "Iteration 1957, loss = 0.11561510\n",
      "Iteration 1588, loss = 0.19286951\n",
      "Iteration 146, loss = 0.41996539\n",
      "Iteration 406, loss = 0.34546865\n",
      "Iteration 1289, loss = 0.22566083\n",
      "Iteration 1290, loss = 0.22558658\n",
      "Iteration 50, loss = 0.70549529\n",
      "Iteration 716, loss = 0.35052814\n",
      "Iteration 407, loss = 0.34535195\n",
      "Iteration 1761, loss = 0.21846540\n",
      "Iteration 1291, loss = 0.22546179\n",
      "Iteration 1292, loss = 0.22537593\n",
      "Iteration 147, loss = 0.41906119\n",
      "Iteration 1293, loss = 0.22526137\n",
      "Iteration 1958, loss = 0.11532612\n",
      "Iteration 1294, loss = 0.22522685\n",
      "Iteration 1589, loss = 0.19275788\n",
      "Iteration 1762, loss = 0.21841682\n",
      "Iteration 1295, loss = 0.22505096\n",
      "Iteration 408, loss = 0.34520328\n",
      "Iteration 1296, loss = 0.22494050\n",
      "Iteration 51, loss = 0.70267976\n",
      "Iteration 148, loss = 0.41810527\n",
      "Iteration 1297, loss = 0.22484574\n",
      "Iteration 1959, loss = 0.11524757\n",
      "Iteration 1298, loss = 0.22483249\n",
      "Iteration 149, loss = 0.41711128\n",
      "Iteration 1299, loss = 0.22470707\n",
      "Iteration 1590, loss = 0.19265778\n",
      "Iteration 1300, loss = 0.22455527\n",
      "Iteration 1960, loss = 0.11520897\n",
      "Iteration 1301, loss = 0.22445625\n",
      "Iteration 150, loss = 0.41626966\n",
      "Iteration 1763, loss = 0.21820236\n",
      "Iteration 1961, loss = 0.11512403\n",
      "Iteration 1302, loss = 0.22439253\n",
      "Iteration 409, loss = 0.34510024\n",
      "Iteration 151, loss = 0.41532057\n",
      "Iteration 52, loss = 0.69970519\n",
      "Iteration 1303, loss = 0.22423073\n",
      "Iteration 410, loss = 0.34496257\n",
      "Iteration 1304, loss = 0.22415788\n",
      "Iteration 1591, loss = 0.19259342\n",
      "Iteration 152, loss = 0.41442003\n",
      "Iteration 1305, loss = 0.22406929\n",
      "Iteration 1962, loss = 0.11510591\n",
      "Iteration 1306, loss = 0.22405442\n",
      "Iteration 717, loss = 0.35031000\n",
      "Iteration 1764, loss = 0.21811045\n",
      "Iteration 411, loss = 0.34484048\n",
      "Iteration 53, loss = 0.69684423\n",
      "Iteration 1307, loss = 0.22384379\n",
      "Iteration 153, loss = 0.41358395\n",
      "Iteration 1308, loss = 0.22376619\n",
      "Iteration 1963, loss = 0.11501958\n",
      "Iteration 1309, loss = 0.22364098\n",
      "Iteration 412, loss = 0.34470682\n",
      "Iteration 1592, loss = 0.19247122\n",
      "Iteration 1310, loss = 0.22364014\n",
      "Iteration 1964, loss = 0.11497283\n",
      "Iteration 1311, loss = 0.22350104\n",
      "Iteration 1312, loss = 0.22344177\n",
      "Iteration 154, loss = 0.41273283\n",
      "Iteration 1313, loss = 0.22324958\n",
      "Iteration 1765, loss = 0.21795299\n",
      "Iteration 413, loss = 0.34459028\n",
      "Iteration 1965, loss = 0.11494107\n",
      "Iteration 1314, loss = 0.22321441\n",
      "Iteration 155, loss = 0.41191406\n",
      "Iteration 54, loss = 0.69391564\n",
      "Iteration 1966, loss = 0.11487864\n",
      "Iteration 1593, loss = 0.19217005\n",
      "Iteration 1315, loss = 0.22305209\n",
      "Iteration 414, loss = 0.34445347\n",
      "Iteration 156, loss = 0.41102946\n",
      "Iteration 1316, loss = 0.22297267\n",
      "Iteration 1766, loss = 0.21783072\n",
      "Iteration 1317, loss = 0.22286275\n",
      "Iteration 157, loss = 0.41026362\n",
      "Iteration 415, loss = 0.34435073\n",
      "Iteration 55, loss = 0.69095412\n",
      "Iteration 1318, loss = 0.22287378\n",
      "Iteration 1767, loss = 0.21770860\n",
      "Iteration 158, loss = 0.40946801\n",
      "Iteration 1967, loss = 0.11478258\n",
      "Iteration 1319, loss = 0.22265685\n",
      "Iteration 1594, loss = 0.19204247\n",
      "Iteration 1320, loss = 0.22254896\n",
      "Iteration 416, loss = 0.34427436\n",
      "Iteration 1768, loss = 0.21762389\n",
      "Iteration 1321, loss = 0.22246286\n",
      "Iteration 1968, loss = 0.11476204\n",
      "Iteration 1322, loss = 0.22232320\n",
      "Iteration 56, loss = 0.68793769\n",
      "Iteration 159, loss = 0.40865983\n",
      "Iteration 1323, loss = 0.22222387\n",
      "Iteration 1595, loss = 0.19194866\n",
      "Iteration 160, loss = 0.40786545\n",
      "Iteration 1969, loss = 0.11477643\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1769, loss = 0.21747633\n",
      "Iteration 1324, loss = 0.22218522\n",
      "Iteration 417, loss = 0.34409217\n",
      "Iteration 1325, loss = 0.22207518\n",
      "Iteration 161, loss = 0.40710031\n",
      "Iteration 1596, loss = 0.19188448\n",
      "Iteration 1326, loss = 0.22193081\n",
      "Iteration 1770, loss = 0.21737686\n",
      "Iteration 718, loss = 0.35013667\n",
      "Iteration 57, loss = 0.68474317\n",
      "Iteration 1327, loss = 0.22183618\n",
      "Iteration 418, loss = 0.34395998\n",
      "Iteration 162, loss = 0.40636640\n",
      "Iteration 1771, loss = 0.21729364\n",
      "Iteration 1597, loss = 0.19165025\n",
      "Iteration 1328, loss = 0.22171204\n",
      "Iteration 419, loss = 0.34384890\n",
      "Iteration 1329, loss = 0.22162227\n",
      "Iteration 1772, loss = 0.21717123\n",
      "Iteration 1330, loss = 0.22156133\n",
      "Iteration 163, loss = 0.40565307\n",
      "Iteration 58, loss = 0.68172081\n",
      "Iteration 1331, loss = 0.22145503\n",
      "Iteration 164, loss = 0.40488295\n",
      "Iteration 1773, loss = 0.21701123\n",
      "Iteration 1332, loss = 0.22129719\n",
      "Iteration 1598, loss = 0.19185278\n",
      "Iteration 420, loss = 0.34373862\n",
      "Iteration 719, loss = 0.34993007\n",
      "Iteration 1333, loss = 0.22122710\n",
      "Iteration 1334, loss = 0.22113748\n",
      "Iteration 165, loss = 0.40410843\n",
      "Iteration 1335, loss = 0.22097342\n",
      "Iteration 421, loss = 0.34361150\n",
      "Iteration 1336, loss = 0.22087599\n",
      "Iteration 720, loss = 0.34975929Iteration 1337, loss = 0.22076659\n",
      "\n",
      "Iteration 1774, loss = 0.21688330\n",
      "Iteration 1338, loss = 0.22070013\n",
      "Iteration 422, loss = 0.34353669\n",
      "Iteration 59, loss = 0.67854920\n",
      "Iteration 166, loss = 0.40342433\n",
      "Iteration 1339, loss = 0.22058266\n",
      "Iteration 721, loss = 0.34961693\n",
      "Iteration 423, loss = 0.34337520\n",
      "Iteration 1599, loss = 0.19140813\n",
      "Iteration 167, loss = 0.40270465\n",
      "Iteration 60, loss = 0.67532323\n",
      "Iteration 1340, loss = 0.22049713\n",
      "Iteration 424, loss = 0.34323589\n",
      "Iteration 1341, loss = 0.22043903\n",
      "Iteration 1342, loss = 0.22026352\n",
      "Iteration 1775, loss = 0.21683530\n",
      "Iteration 1600, loss = 0.19130923\n",
      "Iteration 1343, loss = 0.22017740\n",
      "Iteration 168, loss = 0.40200813\n",
      "Iteration 61, loss = 0.67213484\n",
      "Iteration 1344, loss = 0.22002258\n",
      "Iteration 722, loss = 0.34943813\n",
      "Iteration 1776, loss = 0.21671083\n",
      "Iteration 1345, loss = 0.21994760\n",
      "Iteration 425, loss = 0.34312993\n",
      "Iteration 1346, loss = 0.21985372\n",
      "Iteration 169, loss = 0.40134811\n",
      "Iteration 1347, loss = 0.21975345\n",
      "Iteration 723, loss = 0.34924923\n",
      "Iteration 1348, loss = 0.21962311\n",
      "Iteration 1601, loss = 0.19117104\n",
      "Iteration 170, loss = 0.40066244\n",
      "Iteration 1777, loss = 0.21666329\n",
      "Iteration 1349, loss = 0.21956572\n",
      "Iteration 62, loss = 0.66896557\n",
      "Iteration 1350, loss = 0.21942463\n",
      "Iteration 1351, loss = 0.21930330\n",
      "Iteration 171, loss = 0.40001618\n",
      "Iteration 1352, loss = 0.21929742\n",
      "Iteration 1353, loss = 0.21918773\n",
      "Iteration 426, loss = 0.34302343\n",
      "Iteration 1602, loss = 0.19102447\n",
      "Iteration 1354, loss = 0.21899927\n",
      "Iteration 1778, loss = 0.21644458\n",
      "Iteration 63, loss = 0.66564614\n",
      "Iteration 1355, loss = 0.21896019\n",
      "Iteration 724, loss = 0.34905673\n",
      "Iteration 172, loss = 0.39935689\n",
      "Iteration 1356, loss = 0.21884477\n",
      "Iteration 1779, loss = 0.21630134\n",
      "Iteration 1357, loss = 0.21874276\n",
      "Iteration 1358, loss = 0.21867117\n",
      "Iteration 427, loss = 0.34287952\n",
      "Iteration 1603, loss = 0.19104287\n",
      "Iteration 173, loss = 0.39869635\n",
      "Iteration 1359, loss = 0.21863795\n",
      "Iteration 1780, loss = 0.21620836\n",
      "Iteration 1360, loss = 0.21841083\n",
      "Iteration 1361, loss = 0.21823979\n",
      "Iteration 64, loss = 0.66228875\n",
      "Iteration 1781, loss = 0.21609281\n",
      "Iteration 1604, loss = 0.19067579\n",
      "Iteration 725, loss = 0.34889162\n",
      "Iteration 1362, loss = 0.21816500\n",
      "Iteration 174, loss = 0.39807832\n",
      "Iteration 428, loss = 0.34288744\n",
      "Iteration 1782, loss = 0.21596829\n",
      "Iteration 1363, loss = 0.21804155\n",
      "Iteration 1364, loss = 0.21789356\n",
      "Iteration 1605, loss = 0.19057446\n",
      "Iteration 175, loss = 0.39745142\n",
      "Iteration 726, loss = 0.34879213\n",
      "Iteration 429, loss = 0.34264518\n",
      "Iteration 1365, loss = 0.21780535\n",
      "Iteration 65, loss = 0.65901286\n",
      "Iteration 1783, loss = 0.21587489\n",
      "Iteration 1366, loss = 0.21784291\n",
      "Iteration 176, loss = 0.39683137\n",
      "Iteration 430, loss = 0.34258415\n",
      "Iteration 1367, loss = 0.21769101\n",
      "Iteration 1606, loss = 0.19038402\n",
      "Iteration 1368, loss = 0.21747974\n",
      "Iteration 66, loss = 0.65564482\n",
      "Iteration 1369, loss = 0.21738416\n",
      "Iteration 1784, loss = 0.21571429\n",
      "Iteration 727, loss = 0.34847641\n",
      "Iteration 1370, loss = 0.21726922\n",
      "Iteration 177, loss = 0.39625695\n",
      "Iteration 1371, loss = 0.21718126\n",
      "Iteration 1372, loss = 0.21705836\n",
      "Iteration 431, loss = 0.34246098\n",
      "Iteration 1607, loss = 0.19036537\n",
      "Iteration 178, loss = 0.39562056\n",
      "Iteration 1373, loss = 0.21695842\n",
      "Iteration 1785, loss = 0.21563072\n",
      "Iteration 728, loss = 0.34829513\n",
      "Iteration 1374, loss = 0.21683456\n",
      "Iteration 432, loss = 0.34230930\n",
      "Iteration 1375, loss = 0.21675681\n",
      "Iteration 1376, loss = 0.21665644\n",
      "Iteration 67, loss = 0.65218320\n",
      "Iteration 1377, loss = 0.21651826\n",
      "Iteration 179, loss = 0.39514108\n",
      "Iteration 1608, loss = 0.19022372\n",
      "Iteration 1786, loss = 0.21550726\n",
      "Iteration 433, loss = 0.34216641\n",
      "Iteration 1378, loss = 0.21641662\n",
      "Iteration 180, loss = 0.39445151\n",
      "Iteration 1379, loss = 0.21633772\n",
      "Iteration 1380, loss = 0.21617281\n",
      "Iteration 729, loss = 0.34817143\n",
      "Iteration 1609, loss = 0.18999338\n",
      "Iteration 181, loss = 0.39386932\n",
      "Iteration 1381, loss = 0.21609642\n",
      "Iteration 1382, loss = 0.21598244\n",
      "Iteration 68, loss = 0.64874945\n",
      "Iteration 434, loss = 0.34203343\n",
      "Iteration 1383, loss = 0.21593293\n",
      "Iteration 182, loss = 0.39330988\n",
      "Iteration 1787, loss = 0.21539380\n",
      "Iteration 1384, loss = 0.21581313\n",
      "Iteration 435, loss = 0.34193126\n",
      "Iteration 183, loss = 0.39275717\n",
      "Iteration 1385, loss = 0.21565217\n",
      "Iteration 1788, loss = 0.21526350\n",
      "Iteration 184, loss = 0.39223345\n",
      "Iteration 1386, loss = 0.21553407\n",
      "Iteration 1387, loss = 0.21546562\n",
      "Iteration 730, loss = 0.34797066\n",
      "Iteration 436, loss = 0.34180819\n",
      "Iteration 185, loss = 0.39166940\n",
      "Iteration 1388, loss = 0.21533472\n",
      "Iteration 1789, loss = 0.21513181\n",
      "Iteration 1610, loss = 0.18999629\n",
      "Iteration 69, loss = 0.64538534\n",
      "Iteration 1389, loss = 0.21523417\n",
      "Iteration 437, loss = 0.34166962\n",
      "Iteration 1790, loss = 0.21507376\n",
      "Iteration 731, loss = 0.34782197\n",
      "Iteration 1390, loss = 0.21509127\n",
      "Iteration 186, loss = 0.39121623\n",
      "Iteration 1391, loss = 0.21506898\n",
      "Iteration 1611, loss = 0.18990855\n",
      "Iteration 1392, loss = 0.21492240\n",
      "Iteration 1393, loss = 0.21481347\n",
      "Iteration 438, loss = 0.34154454\n",
      "Iteration 70, loss = 0.64190237\n",
      "Iteration 1394, loss = 0.21476170\n",
      "Iteration 732, loss = 0.34761546\n",
      "Iteration 1395, loss = 0.21469624\n",
      "Iteration 187, loss = 0.39062998\n",
      "Iteration 1396, loss = 0.21460981\n",
      "Iteration 1791, loss = 0.21496651\n",
      "Iteration 1397, loss = 0.21437730\n",
      "Iteration 439, loss = 0.34147223\n",
      "Iteration 1612, loss = 0.18960635\n",
      "Iteration 1398, loss = 0.21458428\n",
      "Iteration 71, loss = 0.63843342\n",
      "Iteration 1399, loss = 0.21418715\n",
      "Iteration 188, loss = 0.39014519\n",
      "Iteration 1400, loss = 0.21405356\n",
      "Iteration 1401, loss = 0.21391885\n",
      "Iteration 440, loss = 0.34131518\n",
      "Iteration 1402, loss = 0.21384458\n",
      "Iteration 189, loss = 0.38961920\n",
      "Iteration 733, loss = 0.34759581\n",
      "Iteration 1403, loss = 0.21374447\n",
      "Iteration 1792, loss = 0.21483970\n",
      "Iteration 72, loss = 0.63497646\n",
      "Iteration 1404, loss = 0.21373474\n",
      "Iteration 1613, loss = 0.18947807\n",
      "Iteration 1405, loss = 0.21359815\n",
      "Iteration 441, loss = 0.34119740\n",
      "Iteration 1406, loss = 0.21342804\n",
      "Iteration 190, loss = 0.38915028\n",
      "Iteration 1407, loss = 0.21327736\n",
      "Iteration 1793, loss = 0.21469193\n",
      "Iteration 1408, loss = 0.21325837\n",
      "Iteration 734, loss = 0.34721633\n",
      "Iteration 191, loss = 0.38865188\n",
      "Iteration 1409, loss = 0.21307754\n",
      "Iteration 442, loss = 0.34110304\n",
      "Iteration 1410, loss = 0.21314137\n",
      "Iteration 192, loss = 0.38816625\n",
      "Iteration 73, loss = 0.63134682\n",
      "Iteration 1794, loss = 0.21459990\n",
      "Iteration 1411, loss = 0.21292268\n",
      "Iteration 443, loss = 0.34096755\n",
      "Iteration 1412, loss = 0.21289384\n",
      "Iteration 193, loss = 0.38771700\n",
      "Iteration 735, loss = 0.34709814\n",
      "Iteration 1413, loss = 0.21276792\n",
      "Iteration 1414, loss = 0.21256987\n",
      "Iteration 74, loss = 0.62782070\n",
      "Iteration 1795, loss = 0.21442023\n",
      "Iteration 194, loss = 0.38725714\n",
      "Iteration 1415, loss = 0.21241030\n",
      "Iteration 444, loss = 0.34084450\n",
      "Iteration 1614, loss = 0.18937404\n",
      "Iteration 736, loss = 0.34687573\n",
      "Iteration 1416, loss = 0.21236787\n",
      "Iteration 75, loss = 0.62439654\n",
      "Iteration 1417, loss = 0.21222829\n",
      "Iteration 195, loss = 0.38672639Iteration 1796, loss = 0.21430496\n",
      "\n",
      "Iteration 1615, loss = 0.18943570\n",
      "Iteration 1418, loss = 0.21224970\n",
      "Iteration 445, loss = 0.34072091\n",
      "Iteration 1419, loss = 0.21197803\n",
      "Iteration 196, loss = 0.38628966\n",
      "Iteration 1420, loss = 0.21186298\n",
      "Iteration 76, loss = 0.62083208\n",
      "Iteration 1797, loss = 0.21425665\n",
      "Iteration 1421, loss = 0.21181046\n",
      "Iteration 737, loss = 0.34676747\n",
      "Iteration 1616, loss = 0.18911899\n",
      "Iteration 197, loss = 0.38584168\n",
      "Iteration 1422, loss = 0.21167117\n",
      "Iteration 446, loss = 0.34063512\n",
      "Iteration 1798, loss = 0.21408845\n",
      "Iteration 1423, loss = 0.21156576\n",
      "Iteration 77, loss = 0.61723302\n",
      "Iteration 198, loss = 0.38541542\n",
      "Iteration 1424, loss = 0.21160998\n",
      "Iteration 738, loss = 0.34656772\n",
      "Iteration 1425, loss = 0.21138067\n",
      "Iteration 1799, loss = 0.21396717\n",
      "Iteration 1426, loss = 0.21128363\n",
      "Iteration 447, loss = 0.34055844\n",
      "Iteration 1427, loss = 0.21109648\n",
      "Iteration 1428, loss = 0.21103331\n",
      "Iteration 199, loss = 0.38494000\n",
      "Iteration 1617, loss = 0.18901950\n",
      "Iteration 78, loss = 0.61375085\n",
      "Iteration 1800, loss = 0.21386102\n",
      "Iteration 1429, loss = 0.21088737\n",
      "Iteration 448, loss = 0.34039901\n",
      "Iteration 200, loss = 0.38459502\n",
      "Iteration 739, loss = 0.34646084\n",
      "Iteration 1430, loss = 0.21090237\n",
      "Iteration 1431, loss = 0.21072044\n",
      "Iteration 1618, loss = 0.18892547\n",
      "Iteration 79, loss = 0.61016517\n",
      "Iteration 1432, loss = 0.21055899\n",
      "Iteration 1801, loss = 0.21375583\n",
      "Iteration 1433, loss = 0.21043367\n",
      "Iteration 740, loss = 0.34618010\n",
      "Iteration 449, loss = 0.34026683\n",
      "Iteration 201, loss = 0.38414359\n",
      "Iteration 1434, loss = 0.21037161\n",
      "Iteration 80, loss = 0.60650004\n",
      "Iteration 1435, loss = 0.21021850\n",
      "Iteration 1619, loss = 0.18878672\n",
      "Iteration 1802, loss = 0.21367072\n",
      "Iteration 741, loss = 0.34600299\n",
      "Iteration 202, loss = 0.38361889\n",
      "Iteration 1436, loss = 0.21015704\n",
      "Iteration 1437, loss = 0.21000134\n",
      "Iteration 450, loss = 0.34016965\n",
      "Iteration 1803, loss = 0.21353984\n",
      "Iteration 81, loss = 0.60317997\n",
      "Iteration 1438, loss = 0.20996817\n",
      "Iteration 203, loss = 0.38323547\n",
      "Iteration 1620, loss = 0.18856137\n",
      "Iteration 1804, loss = 0.21338354\n",
      "Iteration 1439, loss = 0.20976289\n",
      "Iteration 204, loss = 0.38282989\n",
      "Iteration 451, loss = 0.34003517\n",
      "Iteration 1440, loss = 0.20967417\n",
      "Iteration 205, loss = 0.38240659\n",
      "Iteration 1805, loss = 0.21324894\n",
      "Iteration 742, loss = 0.34581340\n",
      "Iteration 1441, loss = 0.20957742\n",
      "Iteration 1621, loss = 0.18847917\n",
      "Iteration 82, loss = 0.59963636\n",
      "Iteration 1442, loss = 0.20954247\n",
      "Iteration 206, loss = 0.38199291\n",
      "Iteration 1806, loss = 0.21313814\n",
      "Iteration 452, loss = 0.33993856\n",
      "Iteration 1443, loss = 0.20970034\n",
      "Iteration 1444, loss = 0.20920309\n",
      "Iteration 743, loss = 0.34566685\n",
      "Iteration 1445, loss = 0.20914132\n",
      "Iteration 207, loss = 0.38160680\n",
      "Iteration 1446, loss = 0.20919861\n",
      "Iteration 453, loss = 0.33979390\n",
      "Iteration 1807, loss = 0.21303111\n",
      "Iteration 208, loss = 0.38121851\n",
      "Iteration 83, loss = 0.59609767\n",
      "Iteration 1447, loss = 0.20888602\n",
      "Iteration 1622, loss = 0.18829164\n",
      "Iteration 1808, loss = 0.21294860\n",
      "Iteration 744, loss = 0.34546514\n",
      "Iteration 1448, loss = 0.20880110\n",
      "Iteration 209, loss = 0.38081880\n",
      "Iteration 454, loss = 0.33968485\n",
      "Iteration 84, loss = 0.59278575\n",
      "Iteration 1809, loss = 0.21282703\n",
      "Iteration 1449, loss = 0.20863244\n",
      "Iteration 1623, loss = 0.18831224\n",
      "Iteration 210, loss = 0.38042138\n",
      "Iteration 745, loss = 0.34534403\n",
      "Iteration 1450, loss = 0.20859317\n",
      "Iteration 1810, loss = 0.21266952\n",
      "Iteration 1451, loss = 0.20839052\n",
      "Iteration 85, loss = 0.58927094\n",
      "Iteration 455, loss = 0.33957995\n",
      "Iteration 1452, loss = 0.20837235\n",
      "Iteration 1811, loss = 0.21256554\n",
      "Iteration 211, loss = 0.38004224\n",
      "Iteration 1453, loss = 0.20817541\n",
      "Iteration 212, loss = 0.37967735\n",
      "Iteration 1454, loss = 0.20808189\n",
      "Iteration 746, loss = 0.34511659\n",
      "Iteration 1624, loss = 0.18817752\n",
      "Iteration 456, loss = 0.33945993\n",
      "Iteration 86, loss = 0.58584624\n",
      "Iteration 1455, loss = 0.20803334\n",
      "Iteration 213, loss = 0.37927731\n",
      "Iteration 1456, loss = 0.20786659\n",
      "Iteration 1812, loss = 0.21247409\n",
      "Iteration 457, loss = 0.33933008\n",
      "Iteration 1457, loss = 0.20773308\n",
      "Iteration 214, loss = 0.37890821\n",
      "Iteration 1625, loss = 0.18791567\n",
      "Iteration 1458, loss = 0.20760090\n",
      "Iteration 1459, loss = 0.20752043\n",
      "Iteration 1460, loss = 0.20739777\n",
      "Iteration 1461, loss = 0.20743949\n",
      "Iteration 87, loss = 0.58247740\n",
      "Iteration 215, loss = 0.37853523\n",
      "Iteration 1462, loss = 0.20721328\n",
      "Iteration 1813, loss = 0.21232380\n",
      "Iteration 458, loss = 0.33924133\n",
      "Iteration 1626, loss = 0.18785906\n",
      "Iteration 747, loss = 0.34504901\n",
      "Iteration 1463, loss = 0.20714583\n",
      "Iteration 1464, loss = 0.20705917\n",
      "Iteration 216, loss = 0.37827718\n",
      "Iteration 1465, loss = 0.20679961\n",
      "Iteration 1466, loss = 0.20673043\n",
      "Iteration 217, loss = 0.37787778\n",
      "Iteration 1467, loss = 0.20669331\n",
      "Iteration 1814, loss = 0.21222324\n",
      "Iteration 459, loss = 0.33911902\n",
      "Iteration 1468, loss = 0.20650963\n",
      "Iteration 88, loss = 0.57911944\n",
      "Iteration 748, loss = 0.34473687\n",
      "Iteration 1469, loss = 0.20641963\n",
      "Iteration 1627, loss = 0.18790489\n",
      "Iteration 1470, loss = 0.20640431\n",
      "Iteration 1471, loss = 0.20613248\n",
      "Iteration 1472, loss = 0.20603549\n",
      "Iteration 460, loss = 0.33899642\n",
      "Iteration 218, loss = 0.37747830\n",
      "Iteration 1815, loss = 0.21209692\n",
      "Iteration 1473, loss = 0.20604715\n",
      "Iteration 1474, loss = 0.20586021\n",
      "Iteration 749, loss = 0.34455234\n",
      "Iteration 89, loss = 0.57588102\n",
      "Iteration 219, loss = 0.37713535\n",
      "Iteration 461, loss = 0.33890002\n",
      "Iteration 1628, loss = 0.18772931\n",
      "Iteration 1475, loss = 0.20569450\n",
      "Iteration 1816, loss = 0.21199988\n",
      "Iteration 1476, loss = 0.20565088\n",
      "Iteration 462, loss = 0.33875877\n",
      "Iteration 1477, loss = 0.20565965\n",
      "Iteration 90, loss = 0.57264745\n",
      "Iteration 750, loss = 0.34437184\n",
      "Iteration 1817, loss = 0.21200625\n",
      "Iteration 1478, loss = 0.20532782\n",
      "Iteration 463, loss = 0.33868025\n",
      "Iteration 1629, loss = 0.18740865\n",
      "Iteration 1479, loss = 0.20524898\n",
      "Iteration 220, loss = 0.37680109\n",
      "Iteration 1480, loss = 0.20515551\n",
      "Iteration 464, loss = 0.33854704\n",
      "Iteration 91, loss = 0.56935661\n",
      "Iteration 1818, loss = 0.21179305\n",
      "Iteration 1481, loss = 0.20500672\n",
      "Iteration 751, loss = 0.34423270\n",
      "Iteration 221, loss = 0.37643046\n",
      "Iteration 1482, loss = 0.20492676\n",
      "Iteration 1630, loss = 0.18724956\n",
      "Iteration 465, loss = 0.33842745\n",
      "Iteration 222, loss = 0.37611270\n",
      "Iteration 1483, loss = 0.20478286\n",
      "Iteration 1819, loss = 0.21167740\n",
      "Iteration 1484, loss = 0.20467787\n",
      "Iteration 223, loss = 0.37578865\n",
      "Iteration 1485, loss = 0.20456538\n",
      "Iteration 1486, loss = 0.20443960\n",
      "Iteration 92, loss = 0.56622544\n",
      "Iteration 752, loss = 0.34407841\n",
      "Iteration 1487, loss = 0.20431698\n",
      "Iteration 466, loss = 0.33843069\n",
      "Iteration 224, loss = 0.37547521\n",
      "Iteration 1488, loss = 0.20419984\n",
      "Iteration 1820, loss = 0.21154060\n",
      "Iteration 1489, loss = 0.20423079\n",
      "Iteration 1631, loss = 0.18718467Iteration 93, loss = 0.56297698\n",
      "\n",
      "Iteration 225, loss = 0.37510778\n",
      "Iteration 1490, loss = 0.20398011\n",
      "Iteration 1491, loss = 0.20390706\n",
      "Iteration 1821, loss = 0.21142835\n",
      "Iteration 226, loss = 0.37480256\n",
      "Iteration 467, loss = 0.33821697\n",
      "Iteration 1492, loss = 0.20377285\n",
      "Iteration 753, loss = 0.34403974\n",
      "Iteration 1822, loss = 0.21126534\n",
      "Iteration 227, loss = 0.37452024\n",
      "Iteration 1493, loss = 0.20365511\n",
      "Iteration 468, loss = 0.33809495\n",
      "Iteration 94, loss = 0.55995016\n",
      "Iteration 228, loss = 0.37419459\n",
      "Iteration 1494, loss = 0.20360947\n",
      "Iteration 1632, loss = 0.18713157\n",
      "Iteration 1823, loss = 0.21122275\n",
      "Iteration 1495, loss = 0.20349937\n",
      "Iteration 229, loss = 0.37389206\n",
      "Iteration 469, loss = 0.33810548\n",
      "Iteration 230, loss = 0.37358865\n",
      "Iteration 754, loss = 0.34370543\n",
      "Iteration 1496, loss = 0.20332072\n",
      "Iteration 95, loss = 0.55705504\n",
      "Iteration 1497, loss = 0.20325159\n",
      "Iteration 1498, loss = 0.20310887\n",
      "Iteration 1824, loss = 0.21107041\n",
      "Iteration 1633, loss = 0.18702410\n",
      "Iteration 1499, loss = 0.20301244\n",
      "Iteration 470, loss = 0.33789960\n",
      "Iteration 231, loss = 0.37327091\n",
      "Iteration 1500, loss = 0.20294445\n",
      "Iteration 755, loss = 0.34350511\n",
      "Iteration 1501, loss = 0.20275870\n",
      "Iteration 96, loss = 0.55400004\n",
      "Iteration 1502, loss = 0.20267631\n",
      "Iteration 471, loss = 0.33778154\n",
      "Iteration 1503, loss = 0.20262000\n",
      "Iteration 232, loss = 0.37301841\n",
      "Iteration 1825, loss = 0.21098218\n",
      "Iteration 1634, loss = 0.18678499\n",
      "Iteration 1504, loss = 0.20241613\n",
      "Iteration 233, loss = 0.37272011\n",
      "Iteration 756, loss = 0.34336078\n",
      "Iteration 1505, loss = 0.20235372\n",
      "Iteration 234, loss = 0.37238032\n",
      "Iteration 1506, loss = 0.20216991\n",
      "Iteration 1507, loss = 0.20215868\n",
      "Iteration 1635, loss = 0.18663442\n",
      "Iteration 97, loss = 0.55100499\n",
      "Iteration 1508, loss = 0.20198484Iteration 235, loss = 0.37209844\n",
      "\n",
      "Iteration 1826, loss = 0.21081030\n",
      "Iteration 472, loss = 0.33767909\n",
      "Iteration 1509, loss = 0.20184695\n",
      "Iteration 98, loss = 0.54819844\n",
      "Iteration 1510, loss = 0.20171247\n",
      "Iteration 1827, loss = 0.21072364\n",
      "Iteration 1636, loss = 0.18650976\n",
      "Iteration 757, loss = 0.34314501\n",
      "Iteration 1511, loss = 0.20167665\n",
      "Iteration 236, loss = 0.37183597\n",
      "Iteration 473, loss = 0.33753354\n",
      "Iteration 1512, loss = 0.20149942\n",
      "Iteration 1513, loss = 0.20144254\n",
      "Iteration 1828, loss = 0.21066010\n",
      "Iteration 237, loss = 0.37152464\n",
      "Iteration 1514, loss = 0.20124698\n",
      "Iteration 99, loss = 0.54539664\n",
      "Iteration 1637, loss = 0.18636506\n",
      "Iteration 1515, loss = 0.20132147\n",
      "Iteration 474, loss = 0.33744086\n",
      "Iteration 1516, loss = 0.20103478\n",
      "Iteration 1829, loss = 0.21064679\n",
      "Iteration 1517, loss = 0.20091814\n",
      "Iteration 1518, loss = 0.20079095\n",
      "Iteration 758, loss = 0.34303116\n",
      "Iteration 238, loss = 0.37124262\n",
      "Iteration 1519, loss = 0.20071750\n",
      "Iteration 475, loss = 0.33732396\n",
      "Iteration 1520, loss = 0.20056727\n",
      "Iteration 100, loss = 0.54256130\n",
      "Iteration 239, loss = 0.37095854\n",
      "Iteration 1521, loss = 0.20043498\n",
      "Iteration 1522, loss = 0.20034091\n",
      "Iteration 1830, loss = 0.21039749\n",
      "Iteration 1523, loss = 0.20027384\n",
      "Iteration 759, loss = 0.34279544\n",
      "Iteration 1524, loss = 0.20011249\n",
      "Iteration 240, loss = 0.37071260\n",
      "Iteration 1525, loss = 0.19999086\n",
      "Iteration 1638, loss = 0.18630267\n",
      "Iteration 101, loss = 0.53988235\n",
      "Iteration 1526, loss = 0.19987503\n",
      "Iteration 1831, loss = 0.21028599\n",
      "Iteration 1527, loss = 0.19982839\n",
      "Iteration 476, loss = 0.33721867\n",
      "Iteration 1528, loss = 0.19968763\n",
      "Iteration 1529, loss = 0.19954949\n",
      "Iteration 760, loss = 0.34265806\n",
      "Iteration 1530, loss = 0.19943652\n",
      "Iteration 1832, loss = 0.21027369\n",
      "Iteration 241, loss = 0.37040232\n",
      "Iteration 1531, loss = 0.19940708\n",
      "Iteration 477, loss = 0.33711707\n",
      "Iteration 1532, loss = 0.19927988\n",
      "Iteration 102, loss = 0.53722960\n",
      "Iteration 1639, loss = 0.18619459\n",
      "Iteration 1533, loss = 0.19909302\n",
      "Iteration 1833, loss = 0.21011796\n",
      "Iteration 1534, loss = 0.19901676\n",
      "Iteration 242, loss = 0.37013430\n",
      "Iteration 1535, loss = 0.19888096\n",
      "Iteration 761, loss = 0.34245415\n",
      "Iteration 1536, loss = 0.19873182\n",
      "Iteration 1537, loss = 0.19861772\n",
      "Iteration 1640, loss = 0.18599946\n",
      "Iteration 1834, loss = 0.20991848\n",
      "Iteration 478, loss = 0.33700111\n",
      "Iteration 243, loss = 0.36989420\n",
      "Iteration 103, loss = 0.53463893\n",
      "Iteration 1538, loss = 0.19859434\n",
      "Iteration 244, loss = 0.36959723\n",
      "Iteration 1835, loss = 0.20989461\n",
      "Iteration 1539, loss = 0.19843324\n",
      "Iteration 762, loss = 0.34237428Iteration 1540, loss = 0.19828312\n",
      "\n",
      "Iteration 1541, loss = 0.19830208\n",
      "Iteration 1542, loss = 0.19810400\n",
      "Iteration 1836, loss = 0.20972424\n",
      "Iteration 104, loss = 0.53214688\n",
      "Iteration 1641, loss = 0.18602648\n",
      "Iteration 245, loss = 0.36930603\n",
      "Iteration 479, loss = 0.33688541\n",
      "Iteration 1543, loss = 0.19793545\n",
      "Iteration 1544, loss = 0.19804591\n",
      "Iteration 246, loss = 0.36905577\n",
      "Iteration 1545, loss = 0.19776965\n",
      "Iteration 1837, loss = 0.20956571\n",
      "Iteration 1546, loss = 0.19767206\n",
      "Iteration 763, loss = 0.34226322\n",
      "Iteration 247, loss = 0.36887753\n",
      "Iteration 1547, loss = 0.19752520\n",
      "Iteration 1548, loss = 0.19741113\n",
      "Iteration 1642, loss = 0.18582785\n",
      "Iteration 248, loss = 0.36850879\n",
      "Iteration 480, loss = 0.33678969\n",
      "Iteration 1549, loss = 0.19733449\n",
      "Iteration 105, loss = 0.52960381\n",
      "Iteration 249, loss = 0.36830924\n",
      "Iteration 1550, loss = 0.19724477Iteration 1838, loss = 0.20946712\n",
      "\n",
      "Iteration 1643, loss = 0.18595692\n",
      "Iteration 1551, loss = 0.19712527\n",
      "Iteration 250, loss = 0.36803164\n",
      "Iteration 764, loss = 0.34196125\n",
      "Iteration 481, loss = 0.33666230\n",
      "Iteration 106, loss = 0.52721281\n",
      "Iteration 1552, loss = 0.19696008\n",
      "Iteration 1553, loss = 0.19685419\n",
      "Iteration 1839, loss = 0.20947663\n",
      "Iteration 1644, loss = 0.18553720\n",
      "Iteration 1554, loss = 0.19677251\n",
      "Iteration 1555, loss = 0.19674738\n",
      "Iteration 1556, loss = 0.19650018\n",
      "Iteration 251, loss = 0.36781926\n",
      "Iteration 1557, loss = 0.19638827\n",
      "Iteration 482, loss = 0.33656021\n",
      "Iteration 1558, loss = 0.19625911\n",
      "Iteration 1645, loss = 0.18549665\n",
      "Iteration 107, loss = 0.52488522\n",
      "Iteration 1559, loss = 0.19616867\n",
      "Iteration 765, loss = 0.34175434\n",
      "Iteration 1560, loss = 0.19609144\n",
      "Iteration 1840, loss = 0.20929471\n",
      "Iteration 252, loss = 0.36759916\n",
      "Iteration 1561, loss = 0.19596846\n",
      "Iteration 1562, loss = 0.19581372\n",
      "Iteration 483, loss = 0.33644321\n",
      "Iteration 1841, loss = 0.20913742\n",
      "Iteration 1563, loss = 0.19588115\n",
      "Iteration 1564, loss = 0.19560591\n",
      "Iteration 108, loss = 0.52242041\n",
      "Iteration 1646, loss = 0.18525319\n",
      "Iteration 253, loss = 0.36735879\n",
      "Iteration 1565, loss = 0.19554999\n",
      "Iteration 766, loss = 0.34160360\n",
      "Iteration 1842, loss = 0.20908713\n",
      "Iteration 484, loss = 0.33635928\n",
      "Iteration 1566, loss = 0.19537241\n",
      "Iteration 254, loss = 0.36707607\n",
      "Iteration 1567, loss = 0.19526390\n",
      "Iteration 1568, loss = 0.19521361\n",
      "Iteration 1843, loss = 0.20890700\n",
      "Iteration 1647, loss = 0.18517093\n",
      "Iteration 1569, loss = 0.19518878\n",
      "Iteration 109, loss = 0.52027502\n",
      "Iteration 1570, loss = 0.19489328\n",
      "Iteration 1571, loss = 0.19489150\n",
      "Iteration 767, loss = 0.34144643\n",
      "Iteration 255, loss = 0.36683228\n",
      "Iteration 1572, loss = 0.19484173\n",
      "Iteration 1573, loss = 0.19478585Iteration 1844, loss = 0.20876214\n",
      "Iteration 485, loss = 0.33627001\n",
      "\n",
      "Iteration 110, loss = 0.51802935\n",
      "Iteration 768, loss = 0.34140305\n",
      "Iteration 486, loss = 0.33611850\n",
      "Iteration 256, loss = 0.36660781\n",
      "Iteration 1845, loss = 0.20865201\n",
      "Iteration 111, loss = 0.51576254\n",
      "Iteration 487, loss = 0.33602015\n",
      "Iteration 1648, loss = 0.18504560\n",
      "Iteration 1574, loss = 0.19442451\n",
      "Iteration 257, loss = 0.36636472\n",
      "Iteration 1575, loss = 0.19434734\n",
      "Iteration 112, loss = 0.51365855\n",
      "Iteration 1576, loss = 0.19424269\n",
      "Iteration 1649, loss = 0.18483854\n",
      "Iteration 769, loss = 0.34110480\n",
      "Iteration 488, loss = 0.33590566\n",
      "Iteration 1846, loss = 0.20852223\n",
      "Iteration 258, loss = 0.36615064\n",
      "Iteration 1577, loss = 0.19418565\n",
      "Iteration 1578, loss = 0.19396763\n",
      "Iteration 1579, loss = 0.19399034\n",
      "Iteration 1580, loss = 0.19376951\n",
      "Iteration 259, loss = 0.36589152\n",
      "Iteration 489, loss = 0.33579478\n",
      "Iteration 1581, loss = 0.19387192\n",
      "Iteration 770, loss = 0.34092243\n",
      "Iteration 1582, loss = 0.19358549\n",
      "Iteration 113, loss = 0.51175045\n",
      "Iteration 1650, loss = 0.18471331\n",
      "Iteration 1583, loss = 0.19342846\n",
      "Iteration 1847, loss = 0.20850478\n",
      "Iteration 490, loss = 0.33569042\n",
      "Iteration 1584, loss = 0.19333298\n",
      "Iteration 260, loss = 0.36564488\n",
      "Iteration 1585, loss = 0.19328747\n",
      "Iteration 1586, loss = 0.19320845\n",
      "Iteration 261, loss = 0.36546009\n",
      "Iteration 1587, loss = 0.19305751\n",
      "Iteration 771, loss = 0.34068286\n",
      "Iteration 1588, loss = 0.19287459\n",
      "Iteration 1848, loss = 0.20830901\n",
      "Iteration 1589, loss = 0.19277127\n",
      "Iteration 1651, loss = 0.18462815\n",
      "Iteration 1590, loss = 0.19262716\n",
      "Iteration 262, loss = 0.36518990\n",
      "Iteration 491, loss = 0.33557634\n",
      "Iteration 1591, loss = 0.19258939\n",
      "Iteration 114, loss = 0.50964325\n",
      "Iteration 1592, loss = 0.19242002\n",
      "Iteration 1593, loss = 0.19227688\n",
      "Iteration 1594, loss = 0.19234347\n",
      "Iteration 492, loss = 0.33552764\n",
      "Iteration 1595, loss = 0.19202004\n",
      "Iteration 1596, loss = 0.19195231\n",
      "Iteration 772, loss = 0.34053933\n",
      "Iteration 1597, loss = 0.19177661\n",
      "Iteration 493, loss = 0.33536275\n",
      "Iteration 1849, loss = 0.20827127\n",
      "Iteration 1598, loss = 0.19171573\n",
      "Iteration 263, loss = 0.36500802\n",
      "Iteration 1599, loss = 0.19169659\n",
      "Iteration 115, loss = 0.50779313\n",
      "Iteration 1600, loss = 0.19143235\n",
      "Iteration 1601, loss = 0.19145951\n",
      "Iteration 1652, loss = 0.18445824\n",
      "Iteration 264, loss = 0.36472954\n",
      "Iteration 494, loss = 0.33526317\n",
      "Iteration 1602, loss = 0.19121812\n",
      "Iteration 1850, loss = 0.20809916\n",
      "Iteration 773, loss = 0.34053432\n",
      "Iteration 1603, loss = 0.19117205\n",
      "Iteration 265, loss = 0.36454731\n",
      "Iteration 1604, loss = 0.19106338\n",
      "Iteration 116, loss = 0.50578779\n",
      "Iteration 495, loss = 0.33516664\n",
      "Iteration 1851, loss = 0.20797898\n",
      "Iteration 1653, loss = 0.18437711\n",
      "Iteration 1605, loss = 0.19086472\n",
      "Iteration 266, loss = 0.36433866Iteration 774, loss = 0.34017427\n",
      "\n",
      "Iteration 1852, loss = 0.20789642\n",
      "Iteration 267, loss = 0.36411185\n",
      "Iteration 117, loss = 0.50389950\n",
      "Iteration 496, loss = 0.33506454\n",
      "Iteration 1654, loss = 0.18422447\n",
      "Iteration 1606, loss = 0.19075199\n",
      "Iteration 268, loss = 0.36386732\n",
      "Iteration 1607, loss = 0.19076111\n",
      "Iteration 775, loss = 0.34010463\n",
      "Iteration 1853, loss = 0.20776913\n",
      "Iteration 269, loss = 0.36381794\n",
      "Iteration 1608, loss = 0.19049945\n",
      "Iteration 1609, loss = 0.19043220\n",
      "Iteration 1655, loss = 0.18425885\n",
      "Iteration 497, loss = 0.33497341\n",
      "Iteration 270, loss = 0.36349622\n",
      "Iteration 1610, loss = 0.19029563\n",
      "Iteration 1854, loss = 0.20766487\n",
      "Iteration 118, loss = 0.50204990\n",
      "Iteration 1611, loss = 0.19018106\n",
      "Iteration 776, loss = 0.33985276\n",
      "Iteration 271, loss = 0.36321228\n",
      "Iteration 1612, loss = 0.19001979\n",
      "Iteration 498, loss = 0.33484912\n",
      "Iteration 1656, loss = 0.18401590\n",
      "Iteration 272, loss = 0.36299697\n",
      "Iteration 1613, loss = 0.19001618\n",
      "Iteration 1855, loss = 0.20752317\n",
      "Iteration 1614, loss = 0.18989086\n",
      "Iteration 499, loss = 0.33477128\n",
      "Iteration 777, loss = 0.33964792\n",
      "Iteration 273, loss = 0.36280954\n",
      "Iteration 1615, loss = 0.18968660\n",
      "Iteration 119, loss = 0.50023809\n",
      "Iteration 1616, loss = 0.18957538\n",
      "Iteration 1856, loss = 0.20743373\n",
      "Iteration 274, loss = 0.36257324\n",
      "Iteration 1617, loss = 0.18946591\n",
      "Iteration 1657, loss = 0.18398107\n",
      "Iteration 275, loss = 0.36238829\n",
      "Iteration 500, loss = 0.33462160\n",
      "Iteration 1618, loss = 0.18943820\n",
      "Iteration 276, loss = 0.36214140\n",
      "Iteration 778, loss = 0.33946759\n",
      "Iteration 1857, loss = 0.20729152\n",
      "Iteration 1619, loss = 0.18927895\n",
      "Iteration 501, loss = 0.33461960\n",
      "Iteration 120, loss = 0.49854510\n",
      "Iteration 1620, loss = 0.18907716\n",
      "Iteration 1621, loss = 0.18896559\n",
      "Iteration 277, loss = 0.36194938\n",
      "Iteration 1858, loss = 0.20722369\n",
      "Iteration 502, loss = 0.33443585\n",
      "Iteration 1658, loss = 0.18383712\n",
      "Iteration 779, loss = 0.33933627\n",
      "Iteration 1622, loss = 0.18881087\n",
      "Iteration 278, loss = 0.36180930\n",
      "Iteration 1859, loss = 0.20709343\n",
      "Iteration 1623, loss = 0.18878708\n",
      "Iteration 503, loss = 0.33430704\n",
      "Iteration 1624, loss = 0.18859706\n",
      "Iteration 1625, loss = 0.18862098\n",
      "Iteration 121, loss = 0.49689244\n",
      "Iteration 1626, loss = 0.18838317\n",
      "Iteration 279, loss = 0.36153727\n",
      "Iteration 1627, loss = 0.18825786\n",
      "Iteration 504, loss = 0.33425365\n",
      "Iteration 1860, loss = 0.20702121\n",
      "Iteration 1628, loss = 0.18815223\n",
      "Iteration 280, loss = 0.36133592\n",
      "Iteration 1629, loss = 0.18801793\n",
      "Iteration 1630, loss = 0.18798853\n",
      "Iteration 1631, loss = 0.18801122\n",
      "Iteration 281, loss = 0.36113793\n",
      "Iteration 780, loss = 0.33918713\n",
      "Iteration 1861, loss = 0.20687872\n",
      "Iteration 1632, loss = 0.18769094\n",
      "Iteration 1659, loss = 0.18382395\n",
      "Iteration 1633, loss = 0.18752187\n",
      "Iteration 282, loss = 0.36091164\n",
      "Iteration 1634, loss = 0.18744636\n",
      "Iteration 1862, loss = 0.20682115\n",
      "Iteration 505, loss = 0.33410854\n",
      "Iteration 283, loss = 0.36076191\n",
      "Iteration 1635, loss = 0.18733121\n",
      "Iteration 122, loss = 0.49524533\n",
      "Iteration 781, loss = 0.33893604\n",
      "Iteration 1636, loss = 0.18726173\n",
      "Iteration 506, loss = 0.33401653\n",
      "Iteration 284, loss = 0.36052771\n",
      "Iteration 1660, loss = 0.18360915\n",
      "Iteration 1863, loss = 0.20664891\n",
      "Iteration 1637, loss = 0.18709240\n",
      "Iteration 285, loss = 0.36029661\n",
      "Iteration 1638, loss = 0.18699853\n",
      "Iteration 1864, loss = 0.20654949\n",
      "Iteration 286, loss = 0.36011251\n",
      "Iteration 1661, loss = 0.18362353\n",
      "Iteration 507, loss = 0.33388383\n",
      "Iteration 1639, loss = 0.18685965\n",
      "Iteration 782, loss = 0.33898139\n",
      "Iteration 287, loss = 0.35990214\n",
      "Iteration 1640, loss = 0.18676577\n",
      "Iteration 1641, loss = 0.18679654Iteration 123, loss = 0.49358612\n",
      "\n",
      "Iteration 1865, loss = 0.20642039\n",
      "Iteration 1642, loss = 0.18651080\n",
      "Iteration 1643, loss = 0.18641256\n",
      "Iteration 783, loss = 0.33871724\n",
      "Iteration 288, loss = 0.35972123\n",
      "Iteration 1866, loss = 0.20629109\n",
      "Iteration 508, loss = 0.33379430\n",
      "Iteration 1662, loss = 0.18333338\n",
      "Iteration 1644, loss = 0.18638757\n",
      "Iteration 289, loss = 0.35950167\n",
      "Iteration 1645, loss = 0.18617210\n",
      "Iteration 1646, loss = 0.18608797\n",
      "Iteration 290, loss = 0.35932648\n",
      "Iteration 784, loss = 0.33840883\n",
      "Iteration 509, loss = 0.33367191\n",
      "Iteration 1647, loss = 0.18591237\n",
      "Iteration 1867, loss = 0.20619902\n",
      "Iteration 291, loss = 0.35910791\n",
      "Iteration 124, loss = 0.49208354\n",
      "Iteration 1648, loss = 0.18583340\n",
      "Iteration 1663, loss = 0.18318601\n",
      "Iteration 292, loss = 0.35894641\n",
      "Iteration 1649, loss = 0.18566941\n",
      "Iteration 1868, loss = 0.20616518\n",
      "Iteration 293, loss = 0.35872051\n",
      "Iteration 785, loss = 0.33832799\n",
      "Iteration 1650, loss = 0.18550944\n",
      "Iteration 510, loss = 0.33358267\n",
      "Iteration 1664, loss = 0.18309262\n",
      "Iteration 294, loss = 0.35863816\n",
      "Iteration 1869, loss = 0.20602860\n",
      "Iteration 1651, loss = 0.18546039\n",
      "Iteration 295, loss = 0.35836059\n",
      "Iteration 125, loss = 0.49050606\n",
      "Iteration 1652, loss = 0.18537069\n",
      "Iteration 786, loss = 0.33809601\n",
      "Iteration 1653, loss = 0.18524696\n",
      "Iteration 511, loss = 0.33346795\n",
      "Iteration 1654, loss = 0.18512684\n",
      "Iteration 296, loss = 0.35817206\n",
      "Iteration 1655, loss = 0.18500674\n",
      "Iteration 1656, loss = 0.18486323\n",
      "Iteration 297, loss = 0.35796208\n",
      "Iteration 512, loss = 0.33337382\n",
      "Iteration 1657, loss = 0.18476175\n",
      "Iteration 1658, loss = 0.18460232\n",
      "Iteration 1870, loss = 0.20593118\n",
      "Iteration 298, loss = 0.35778449\n",
      "Iteration 1665, loss = 0.18290596\n",
      "Iteration 513, loss = 0.33329639\n",
      "Iteration 1659, loss = 0.18446242\n",
      "Iteration 1660, loss = 0.18443830\n",
      "Iteration 126, loss = 0.48901479\n",
      "Iteration 299, loss = 0.35759857\n",
      "Iteration 1871, loss = 0.20580657\n",
      "Iteration 1666, loss = 0.18280520\n",
      "Iteration 1661, loss = 0.18423765\n",
      "Iteration 787, loss = 0.33789184\n",
      "Iteration 300, loss = 0.35743245\n",
      "Iteration 1872, loss = 0.20569488\n",
      "Iteration 514, loss = 0.33317234\n",
      "Iteration 1662, loss = 0.18412076\n",
      "Iteration 127, loss = 0.48757769\n",
      "Iteration 1663, loss = 0.18401818\n",
      "Iteration 301, loss = 0.35719891\n",
      "Iteration 1873, loss = 0.20556053\n",
      "Iteration 515, loss = 0.33304221\n",
      "Iteration 1664, loss = 0.18397902\n",
      "Iteration 1667, loss = 0.18262349\n",
      "Iteration 302, loss = 0.35704597\n",
      "Iteration 1665, loss = 0.18380141\n",
      "Iteration 516, loss = 0.33293408\n",
      "Iteration 128, loss = 0.48611686\n",
      "Iteration 303, loss = 0.35683372\n",
      "Iteration 1666, loss = 0.18384686\n",
      "Iteration 788, loss = 0.33780612\n",
      "Iteration 1668, loss = 0.18258886\n",
      "Iteration 1667, loss = 0.18354126\n",
      "Iteration 1874, loss = 0.20546299\n",
      "Iteration 1668, loss = 0.18348160\n",
      "Iteration 304, loss = 0.35667173\n",
      "Iteration 1669, loss = 0.18337757\n",
      "Iteration 1670, loss = 0.18326668\n",
      "Iteration 129, loss = 0.48467495\n",
      "Iteration 305, loss = 0.35648129\n",
      "Iteration 1671, loss = 0.18307323\n",
      "Iteration 517, loss = 0.33283663\n",
      "Iteration 1672, loss = 0.18295144\n",
      "Iteration 789, loss = 0.33752917\n",
      "Iteration 1875, loss = 0.20537556\n",
      "Iteration 1673, loss = 0.18293262\n",
      "Iteration 1674, loss = 0.18272972\n",
      "Iteration 1669, loss = 0.18238816\n",
      "Iteration 306, loss = 0.35628998\n",
      "Iteration 130, loss = 0.48335165\n",
      "Iteration 518, loss = 0.33273669\n",
      "Iteration 1675, loss = 0.18259115\n",
      "Iteration 1676, loss = 0.18253426\n",
      "Iteration 790, loss = 0.33735021\n",
      "Iteration 1677, loss = 0.18235548\n",
      "Iteration 1678, loss = 0.18223297\n",
      "Iteration 1876, loss = 0.20522821\n",
      "Iteration 519, loss = 0.33261442\n",
      "Iteration 1679, loss = 0.18210306\n",
      "Iteration 307, loss = 0.35611782\n",
      "Iteration 1680, loss = 0.18199963\n",
      "Iteration 1681, loss = 0.18194770\n",
      "Iteration 1682, loss = 0.18186524\n",
      "Iteration 1683, loss = 0.18164089\n",
      "Iteration 1670, loss = 0.18231116\n",
      "Iteration 1684, loss = 0.18163045\n",
      "Iteration 308, loss = 0.35590642\n",
      "Iteration 1877, loss = 0.20508822\n",
      "Iteration 1685, loss = 0.18162081\n",
      "Iteration 791, loss = 0.33720778\n",
      "Iteration 520, loss = 0.33256247\n",
      "Iteration 1686, loss = 0.18137660\n",
      "Iteration 309, loss = 0.35573523\n",
      "Iteration 131, loss = 0.48199199\n",
      "Iteration 1687, loss = 0.18119602\n",
      "Iteration 1878, loss = 0.20499406\n",
      "Iteration 1671, loss = 0.18222534\n",
      "Iteration 310, loss = 0.35554352\n",
      "Iteration 1688, loss = 0.18102994\n",
      "Iteration 1689, loss = 0.18101854\n",
      "Iteration 1690, loss = 0.18088214\n",
      "Iteration 1879, loss = 0.20486293\n",
      "Iteration 1691, loss = 0.18067294\n",
      "Iteration 311, loss = 0.35540372\n",
      "Iteration 521, loss = 0.33243293\n",
      "Iteration 1692, loss = 0.18055651\n",
      "Iteration 312, loss = 0.35521547\n",
      "Iteration 1693, loss = 0.18049344\n",
      "Iteration 132, loss = 0.48072709\n",
      "Iteration 792, loss = 0.33704103\n",
      "Iteration 1694, loss = 0.18031144\n",
      "Iteration 1880, loss = 0.20476455\n",
      "Iteration 1695, loss = 0.18020628\n",
      "Iteration 313, loss = 0.35507885\n",
      "Iteration 1696, loss = 0.18013593\n",
      "Iteration 1672, loss = 0.18213378\n",
      "Iteration 1697, loss = 0.18002333\n",
      "Iteration 522, loss = 0.33241890\n",
      "Iteration 314, loss = 0.35485360\n",
      "Iteration 1881, loss = 0.20462630\n",
      "Iteration 1698, loss = 0.17990275\n",
      "Iteration 1699, loss = 0.17995971\n",
      "Iteration 133, loss = 0.47943186\n",
      "Iteration 315, loss = 0.35468756\n",
      "Iteration 1700, loss = 0.17970338\n",
      "Iteration 1882, loss = 0.20451678\n",
      "Iteration 1701, loss = 0.17965352\n",
      "Iteration 1702, loss = 0.17934478\n",
      "Iteration 793, loss = 0.33695460\n",
      "Iteration 1673, loss = 0.18196277\n",
      "Iteration 1703, loss = 0.17941556\n",
      "Iteration 316, loss = 0.35452187\n",
      "Iteration 1704, loss = 0.17954478\n",
      "Iteration 523, loss = 0.33220941\n",
      "Iteration 134, loss = 0.47815128\n",
      "Iteration 1705, loss = 0.17896400\n",
      "Iteration 1706, loss = 0.17882766\n",
      "Iteration 317, loss = 0.35432367\n",
      "Iteration 794, loss = 0.33663975\n",
      "Iteration 1674, loss = 0.18192929\n",
      "Iteration 1883, loss = 0.20441137\n",
      "Iteration 1707, loss = 0.17871503\n",
      "Iteration 1708, loss = 0.17887995\n",
      "Iteration 1709, loss = 0.17852695\n",
      "Iteration 318, loss = 0.35423331\n",
      "Iteration 1710, loss = 0.17836314\n",
      "Iteration 524, loss = 0.33212996\n",
      "Iteration 1711, loss = 0.17828477\n",
      "Iteration 1884, loss = 0.20432939\n",
      "Iteration 1712, loss = 0.17817718\n",
      "Iteration 1713, loss = 0.17808901\n",
      "Iteration 795, loss = 0.33658250\n",
      "Iteration 319, loss = 0.35400518\n",
      "Iteration 1714, loss = 0.17805573\n",
      "Iteration 1675, loss = 0.18171931\n",
      "Iteration 1885, loss = 0.20417997\n",
      "Iteration 525, loss = 0.33198194\n",
      "Iteration 135, loss = 0.47690550\n",
      "Iteration 1715, loss = 0.17779257\n",
      "Iteration 1716, loss = 0.17769666\n",
      "Iteration 1717, loss = 0.17783333\n",
      "Iteration 796, loss = 0.33635242\n",
      "Iteration 320, loss = 0.35387734\n",
      "Iteration 1718, loss = 0.17745755\n",
      "Iteration 1719, loss = 0.17729373\n",
      "Iteration 1886, loss = 0.20410006\n",
      "Iteration 526, loss = 0.33191950\n",
      "Iteration 1720, loss = 0.17744731\n",
      "Iteration 1676, loss = 0.18161901\n",
      "Iteration 1721, loss = 0.17709105\n",
      "Iteration 136, loss = 0.47571100\n",
      "Iteration 1722, loss = 0.17700626\n",
      "Iteration 321, loss = 0.35364707\n",
      "Iteration 1887, loss = 0.20396670\n",
      "Iteration 1723, loss = 0.17695951\n",
      "Iteration 527, loss = 0.33184363\n",
      "Iteration 1724, loss = 0.17672317\n",
      "Iteration 797, loss = 0.33612696\n",
      "Iteration 322, loss = 0.35351697\n",
      "Iteration 137, loss = 0.47460529\n",
      "Iteration 1725, loss = 0.17663343\n",
      "Iteration 1726, loss = 0.17661016\n",
      "Iteration 1677, loss = 0.18166290\n",
      "Iteration 323, loss = 0.35327079\n",
      "Iteration 1727, loss = 0.17631475\n",
      "Iteration 1888, loss = 0.20394479\n",
      "Iteration 528, loss = 0.33172049\n",
      "Iteration 1728, loss = 0.17625577\n",
      "Iteration 324, loss = 0.35313878\n",
      "Iteration 1729, loss = 0.17613741\n",
      "Iteration 1730, loss = 0.17598996\n",
      "Iteration 325, loss = 0.35294877\n",
      "Iteration 1889, loss = 0.20372005\n",
      "Iteration 138, loss = 0.47334025\n",
      "Iteration 529, loss = 0.33159729\n",
      "Iteration 1731, loss = 0.17585472\n",
      "Iteration 1678, loss = 0.18129190\n",
      "Iteration 1732, loss = 0.17574684\n",
      "Iteration 798, loss = 0.33600311\n",
      "Iteration 1733, loss = 0.17561099\n",
      "Iteration 326, loss = 0.35276512\n",
      "Iteration 1734, loss = 0.17556341\n",
      "Iteration 1735, loss = 0.17543847\n",
      "Iteration 1890, loss = 0.20363280\n",
      "Iteration 139, loss = 0.47222116\n",
      "Iteration 327, loss = 0.35265318\n",
      "Iteration 530, loss = 0.33148098\n",
      "Iteration 1736, loss = 0.17532605\n",
      "Iteration 799, loss = 0.33579095\n",
      "Iteration 1737, loss = 0.17512542\n",
      "Iteration 1679, loss = 0.18123008\n",
      "Iteration 1738, loss = 0.17504641\n",
      "Iteration 1739, loss = 0.17498747\n",
      "Iteration 140, loss = 0.47115081\n",
      "Iteration 328, loss = 0.35243609\n",
      "Iteration 1891, loss = 0.20354056\n",
      "Iteration 531, loss = 0.33141329\n",
      "Iteration 1740, loss = 0.17478134\n",
      "Iteration 1741, loss = 0.17471497\n",
      "Iteration 1742, loss = 0.17461987\n",
      "Iteration 1892, loss = 0.20344162\n",
      "Iteration 1743, loss = 0.17473030\n",
      "Iteration 800, loss = 0.33562042\n",
      "Iteration 532, loss = 0.33128480\n",
      "Iteration 1744, loss = 0.17456714\n",
      "Iteration 329, loss = 0.35227142\n",
      "Iteration 1745, loss = 0.17422689\n",
      "Iteration 1746, loss = 0.17409820\n",
      "Iteration 141, loss = 0.47006156\n",
      "Iteration 1680, loss = 0.18110012\n",
      "Iteration 1747, loss = 0.17393232\n",
      "Iteration 330, loss = 0.35209456\n",
      "Iteration 1748, loss = 0.17385415\n",
      "Iteration 533, loss = 0.33116513\n",
      "Iteration 1749, loss = 0.17371924\n",
      "Iteration 1893, loss = 0.20333315\n",
      "Iteration 801, loss = 0.33541582\n",
      "Iteration 1750, loss = 0.17365702\n",
      "Iteration 142, loss = 0.46896706\n",
      "Iteration 331, loss = 0.35192072\n",
      "Iteration 1751, loss = 0.17365891\n",
      "Iteration 1681, loss = 0.18103998\n",
      "Iteration 1752, loss = 0.17342274\n",
      "Iteration 534, loss = 0.33113452\n",
      "Iteration 1894, loss = 0.20321455\n",
      "Iteration 332, loss = 0.35176095\n",
      "Iteration 1753, loss = 0.17332285\n",
      "Iteration 1754, loss = 0.17312761\n",
      "Iteration 143, loss = 0.46792854\n",
      "Iteration 333, loss = 0.35157504\n",
      "Iteration 535, loss = 0.33097745\n",
      "Iteration 1755, loss = 0.17297914\n",
      "Iteration 1895, loss = 0.20308355\n",
      "Iteration 802, loss = 0.33525006\n",
      "Iteration 334, loss = 0.35147114\n",
      "Iteration 1756, loss = 0.17290410\n",
      "Iteration 1682, loss = 0.18094961\n",
      "Iteration 536, loss = 0.33084504\n",
      "Iteration 1757, loss = 0.17291129\n",
      "Iteration 1896, loss = 0.20296931\n",
      "Iteration 335, loss = 0.35127197\n",
      "Iteration 1758, loss = 0.17266025\n",
      "Iteration 1759, loss = 0.17254587\n",
      "Iteration 336, loss = 0.35108099\n",
      "Iteration 1760, loss = 0.17239347\n",
      "Iteration 144, loss = 0.46687646\n",
      "Iteration 1761, loss = 0.17232773\n",
      "Iteration 1683, loss = 0.18076334\n",
      "Iteration 803, loss = 0.33516356\n",
      "Iteration 1897, loss = 0.20295865\n",
      "Iteration 337, loss = 0.35092440\n",
      "Iteration 1762, loss = 0.17216275\n",
      "Iteration 1763, loss = 0.17208024\n",
      "Iteration 537, loss = 0.33080540\n",
      "Iteration 338, loss = 0.35085400\n",
      "Iteration 1764, loss = 0.17195989\n",
      "Iteration 1765, loss = 0.17198340\n",
      "Iteration 1684, loss = 0.18071211\n",
      "Iteration 1766, loss = 0.17179814\n",
      "Iteration 339, loss = 0.35061564\n",
      "Iteration 1898, loss = 0.20282045\n",
      "Iteration 1767, loss = 0.17154578\n",
      "Iteration 1768, loss = 0.17141297\n",
      "Iteration 145, loss = 0.46586987\n",
      "Iteration 1769, loss = 0.17168953\n",
      "Iteration 1770, loss = 0.17126676\n",
      "Iteration 1899, loss = 0.20265705\n",
      "Iteration 1685, loss = 0.18068451\n",
      "Iteration 538, loss = 0.33069970\n",
      "Iteration 340, loss = 0.35047075\n",
      "Iteration 1771, loss = 0.17115183\n",
      "Iteration 804, loss = 0.33490390\n",
      "Iteration 1772, loss = 0.17093485\n",
      "Iteration 341, loss = 0.35031466\n",
      "Iteration 1773, loss = 0.17082852\n",
      "Iteration 1900, loss = 0.20253669\n",
      "Iteration 1774, loss = 0.17073869\n",
      "Iteration 1775, loss = 0.17069163\n",
      "Iteration 539, loss = 0.33056422\n",
      "Iteration 146, loss = 0.46488634\n",
      "Iteration 1776, loss = 0.17060889\n",
      "Iteration 805, loss = 0.33473352\n",
      "Iteration 1777, loss = 0.17046751\n",
      "Iteration 342, loss = 0.35013110\n",
      "Iteration 1901, loss = 0.20249660\n",
      "Iteration 1778, loss = 0.17024430\n",
      "Iteration 343, loss = 0.34998802\n",
      "Iteration 147, loss = 0.46389097\n",
      "Iteration 1902, loss = 0.20230954\n",
      "Iteration 1779, loss = 0.17010562\n",
      "Iteration 1780, loss = 0.16994799\n",
      "Iteration 1686, loss = 0.18058225\n",
      "Iteration 1781, loss = 0.16985216\n",
      "Iteration 344, loss = 0.34985615\n",
      "Iteration 1782, loss = 0.16973002\n",
      "Iteration 1783, loss = 0.16963684\n",
      "Iteration 345, loss = 0.34964119\n",
      "Iteration 1903, loss = 0.20220441\n",
      "Iteration 1784, loss = 0.16958182\n",
      "Iteration 1785, loss = 0.16933433\n",
      "Iteration 806, loss = 0.33462863\n",
      "Iteration 346, loss = 0.34952433\n",
      "Iteration 1786, loss = 0.16921296\n",
      "Iteration 540, loss = 0.33051274\n",
      "Iteration 1904, loss = 0.20212459\n",
      "Iteration 347, loss = 0.34932263\n",
      "Iteration 1787, loss = 0.16931341\n",
      "Iteration 807, loss = 0.33434976\n",
      "Iteration 1687, loss = 0.18023764\n",
      "Iteration 348, loss = 0.34914500\n",
      "Iteration 148, loss = 0.46290482\n",
      "Iteration 1788, loss = 0.16909766\n",
      "Iteration 1789, loss = 0.16891173Iteration 349, loss = 0.34897476\n",
      "\n",
      "Iteration 1905, loss = 0.20199882\n",
      "Iteration 541, loss = 0.33036821\n",
      "Iteration 1790, loss = 0.16876666\n",
      "Iteration 1688, loss = 0.18022591\n",
      "Iteration 149, loss = 0.46199596\n",
      "Iteration 1791, loss = 0.16860671\n",
      "Iteration 350, loss = 0.34884644\n",
      "Iteration 542, loss = 0.33028184\n",
      "Iteration 1792, loss = 0.16865261\n",
      "Iteration 808, loss = 0.33421486\n",
      "Iteration 1793, loss = 0.16854167\n",
      "Iteration 351, loss = 0.34865990\n",
      "Iteration 1689, loss = 0.18001509\n",
      "Iteration 1906, loss = 0.20193894Iteration 150, loss = 0.46105046\n",
      "\n",
      "Iteration 1794, loss = 0.16842388\n",
      "Iteration 543, loss = 0.33014047\n",
      "Iteration 1795, loss = 0.16835213\n",
      "Iteration 352, loss = 0.34850259\n",
      "Iteration 1796, loss = 0.16803299\n",
      "Iteration 1907, loss = 0.20189559\n",
      "Iteration 151, loss = 0.46014215\n",
      "Iteration 1797, loss = 0.16786247\n",
      "Iteration 809, loss = 0.33402374\n",
      "Iteration 1690, loss = 0.17989601\n",
      "Iteration 1798, loss = 0.16785804\n",
      "Iteration 353, loss = 0.34835267\n",
      "Iteration 1799, loss = 0.16765669\n",
      "Iteration 544, loss = 0.33002398\n",
      "Iteration 1800, loss = 0.16758374\n",
      "Iteration 1908, loss = 0.20164351\n",
      "Iteration 1801, loss = 0.16744939\n",
      "Iteration 354, loss = 0.34820931\n",
      "Iteration 1802, loss = 0.16728349\n",
      "Iteration 1803, loss = 0.16728005\n",
      "Iteration 152, loss = 0.45931029\n",
      "Iteration 1691, loss = 0.17981876\n",
      "Iteration 355, loss = 0.34807526\n",
      "Iteration 1804, loss = 0.16700926\n",
      "Iteration 1805, loss = 0.16693982\n",
      "Iteration 545, loss = 0.32992680\n",
      "Iteration 1909, loss = 0.20156103\n",
      "Iteration 356, loss = 0.34785682\n",
      "Iteration 810, loss = 0.33389600\n",
      "Iteration 1806, loss = 0.16687064\n",
      "Iteration 153, loss = 0.45841059\n",
      "Iteration 1807, loss = 0.16674100\n",
      "Iteration 357, loss = 0.34768429\n",
      "Iteration 1692, loss = 0.17987396\n",
      "Iteration 1808, loss = 0.16659956\n",
      "Iteration 1910, loss = 0.20142113\n",
      "Iteration 546, loss = 0.32987285\n",
      "Iteration 1809, loss = 0.16658048\n",
      "Iteration 811, loss = 0.33365397\n",
      "Iteration 1810, loss = 0.16630142Iteration 547, loss = 0.32973638\n",
      "\n",
      "Iteration 358, loss = 0.34755315\n",
      "Iteration 1693, loss = 0.17954252\n",
      "Iteration 1811, loss = 0.16622187\n",
      "Iteration 154, loss = 0.45752923\n",
      "Iteration 1812, loss = 0.16640662\n",
      "Iteration 1911, loss = 0.20148326\n",
      "Iteration 359, loss = 0.34738520\n",
      "Iteration 812, loss = 0.33346254\n",
      "Iteration 1813, loss = 0.16598642\n",
      "Iteration 360, loss = 0.34722128\n",
      "Iteration 548, loss = 0.32962533\n",
      "Iteration 1694, loss = 0.17950647\n",
      "Iteration 1814, loss = 0.16583355\n",
      "Iteration 155, loss = 0.45669888\n",
      "Iteration 1815, loss = 0.16568934\n",
      "Iteration 1912, loss = 0.20121998\n",
      "Iteration 361, loss = 0.34709219\n",
      "Iteration 1816, loss = 0.16573442\n",
      "Iteration 1817, loss = 0.16548438\n",
      "Iteration 813, loss = 0.33331180\n",
      "Iteration 549, loss = 0.32953300\n",
      "Iteration 1818, loss = 0.16538733\n",
      "Iteration 362, loss = 0.34690695\n",
      "Iteration 1913, loss = 0.20118217\n",
      "Iteration 1819, loss = 0.16522640\n",
      "Iteration 550, loss = 0.32940788\n",
      "Iteration 1820, loss = 0.16525709\n",
      "Iteration 1821, loss = 0.16497936\n",
      "Iteration 1914, loss = 0.20099586\n",
      "Iteration 1695, loss = 0.17932356\n",
      "Iteration 156, loss = 0.45586214\n",
      "Iteration 363, loss = 0.34677895\n",
      "Iteration 1822, loss = 0.16489202\n",
      "Iteration 551, loss = 0.32932493\n",
      "Iteration 1915, loss = 0.20090842\n",
      "Iteration 1823, loss = 0.16479795\n",
      "Iteration 814, loss = 0.33312487\n",
      "Iteration 1824, loss = 0.16459409\n",
      "Iteration 1696, loss = 0.17951626\n",
      "Iteration 364, loss = 0.34666546\n",
      "Iteration 1825, loss = 0.16488970\n",
      "Iteration 1826, loss = 0.16439611\n",
      "Iteration 365, loss = 0.34656792\n",
      "Iteration 157, loss = 0.45503414\n",
      "Iteration 1827, loss = 0.16439378\n",
      "Iteration 1828, loss = 0.16441681\n",
      "Iteration 1916, loss = 0.20080095\n",
      "Iteration 815, loss = 0.33294824\n",
      "Iteration 1829, loss = 0.16401011\n",
      "Iteration 366, loss = 0.34627706\n",
      "Iteration 1697, loss = 0.17912742\n",
      "Iteration 1830, loss = 0.16389902\n",
      "Iteration 552, loss = 0.32921156\n",
      "Iteration 1831, loss = 0.16379316\n",
      "Iteration 1917, loss = 0.20067291\n",
      "Iteration 1832, loss = 0.16366250\n",
      "Iteration 1833, loss = 0.16358064\n",
      "Iteration 1698, loss = 0.17902600\n",
      "Iteration 1834, loss = 0.16340136\n",
      "Iteration 158, loss = 0.45431098\n",
      "Iteration 553, loss = 0.32910525\n",
      "Iteration 1835, loss = 0.16330376\n",
      "Iteration 1918, loss = 0.20065841\n",
      "Iteration 1836, loss = 0.16318637\n",
      "Iteration 367, loss = 0.34614099\n",
      "Iteration 816, loss = 0.33276829\n",
      "Iteration 368, loss = 0.34599302\n",
      "Iteration 1837, loss = 0.16309386\n",
      "Iteration 159, loss = 0.45350731\n",
      "Iteration 1838, loss = 0.16292401\n",
      "Iteration 369, loss = 0.34584644\n",
      "Iteration 1839, loss = 0.16289718\n",
      "Iteration 1919, loss = 0.20049665\n",
      "Iteration 1840, loss = 0.16268927\n",
      "Iteration 554, loss = 0.32901338\n",
      "Iteration 1841, loss = 0.16273162\n",
      "Iteration 1699, loss = 0.17904654\n",
      "Iteration 1842, loss = 0.16247761\n",
      "Iteration 160, loss = 0.45268865\n",
      "Iteration 817, loss = 0.33256856\n",
      "Iteration 1843, loss = 0.16241239\n",
      "Iteration 1844, loss = 0.16248624\n",
      "Iteration 1920, loss = 0.20044965\n",
      "Iteration 370, loss = 0.34567016\n",
      "Iteration 1845, loss = 0.16208549\n",
      "Iteration 1846, loss = 0.16213483\n",
      "Iteration 1847, loss = 0.16190289\n",
      "Iteration 1921, loss = 0.20022899\n",
      "Iteration 371, loss = 0.34550383\n",
      "Iteration 1848, loss = 0.16169556\n",
      "Iteration 555, loss = 0.32890826\n",
      "Iteration 1700, loss = 0.17877717\n",
      "Iteration 818, loss = 0.33244842\n",
      "Iteration 161, loss = 0.45195362\n",
      "Iteration 1849, loss = 0.16173041\n",
      "Iteration 1850, loss = 0.16149659\n",
      "Iteration 372, loss = 0.34543832\n",
      "Iteration 1851, loss = 0.16154735\n",
      "Iteration 1922, loss = 0.20014804\n",
      "Iteration 1852, loss = 0.16133076\n",
      "Iteration 373, loss = 0.34518741\n",
      "Iteration 556, loss = 0.32887701\n",
      "Iteration 819, loss = 0.33221676\n",
      "Iteration 1853, loss = 0.16112043\n",
      "Iteration 1923, loss = 0.20000502\n",
      "Iteration 1701, loss = 0.17879332\n",
      "Iteration 1854, loss = 0.16112452\n",
      "Iteration 374, loss = 0.34505497\n",
      "Iteration 1855, loss = 0.16089842\n",
      "Iteration 1856, loss = 0.16086343\n",
      "Iteration 1924, loss = 0.19988755\n",
      "Iteration 162, loss = 0.45115250\n",
      "Iteration 375, loss = 0.34490639\n",
      "Iteration 1857, loss = 0.16071405\n",
      "Iteration 1858, loss = 0.16056019\n",
      "Iteration 557, loss = 0.32871176\n",
      "Iteration 1702, loss = 0.17872764\n",
      "Iteration 376, loss = 0.34482462\n",
      "Iteration 1859, loss = 0.16052517\n",
      "Iteration 1925, loss = 0.19983285\n",
      "Iteration 1860, loss = 0.16032653\n",
      "Iteration 820, loss = 0.33206275\n",
      "Iteration 558, loss = 0.32861020\n",
      "Iteration 377, loss = 0.34459329\n",
      "Iteration 1861, loss = 0.16018661\n",
      "Iteration 1862, loss = 0.16007153\n",
      "Iteration 1863, loss = 0.15999517\n",
      "Iteration 1926, loss = 0.19969678\n",
      "Iteration 1864, loss = 0.15980933\n",
      "Iteration 163, loss = 0.45043922\n",
      "Iteration 559, loss = 0.32849659\n",
      "Iteration 1703, loss = 0.17841077\n",
      "Iteration 378, loss = 0.34446396\n",
      "Iteration 1865, loss = 0.15969312\n",
      "Iteration 821, loss = 0.33195609\n",
      "Iteration 1927, loss = 0.19955919\n",
      "Iteration 1866, loss = 0.15959212\n",
      "Iteration 560, loss = 0.32839679\n",
      "Iteration 1867, loss = 0.15950862\n",
      "Iteration 379, loss = 0.34434482\n",
      "Iteration 1704, loss = 0.17842151\n",
      "Iteration 1868, loss = 0.15937937\n",
      "Iteration 1928, loss = 0.19951698\n",
      "Iteration 822, loss = 0.33168300\n",
      "Iteration 164, loss = 0.44969914\n",
      "Iteration 1869, loss = 0.15940458\n",
      "Iteration 380, loss = 0.34414670\n",
      "Iteration 561, loss = 0.32828943\n",
      "Iteration 1870, loss = 0.15910817\n",
      "Iteration 381, loss = 0.34397178\n",
      "Iteration 1871, loss = 0.15899014\n",
      "Iteration 1705, loss = 0.17838946Iteration 1872, loss = 0.15891024\n",
      "\n",
      "Iteration 1873, loss = 0.15889519\n",
      "Iteration 1874, loss = 0.15878961\n",
      "Iteration 562, loss = 0.32819993\n",
      "Iteration 382, loss = 0.34383907\n",
      "Iteration 165, loss = 0.44897017\n",
      "Iteration 1875, loss = 0.15858967\n",
      "Iteration 1929, loss = 0.19947023\n",
      "Iteration 563, loss = 0.32810032\n",
      "Iteration 1876, loss = 0.15843622\n",
      "Iteration 1706, loss = 0.17813867\n",
      "Iteration 383, loss = 0.34372161\n",
      "Iteration 823, loss = 0.33151361\n",
      "Iteration 166, loss = 0.44823062\n",
      "Iteration 1877, loss = 0.15844722\n",
      "Iteration 384, loss = 0.34357800\n",
      "Iteration 1878, loss = 0.15820946\n",
      "Iteration 385, loss = 0.34339625Iteration 1930, loss = 0.19948233\n",
      "\n",
      "Iteration 564, loss = 0.32799221\n",
      "Iteration 1879, loss = 0.15817105\n",
      "Iteration 167, loss = 0.44751585\n",
      "Iteration 824, loss = 0.33132661\n",
      "Iteration 1880, loss = 0.15793631\n",
      "Iteration 1931, loss = 0.19917892\n",
      "Iteration 1881, loss = 0.15786324\n",
      "Iteration 1707, loss = 0.17801404\n",
      "Iteration 386, loss = 0.34323779\n",
      "Iteration 565, loss = 0.32787668\n",
      "Iteration 1882, loss = 0.15779035\n",
      "Iteration 825, loss = 0.33113845\n",
      "Iteration 1883, loss = 0.15795432\n",
      "Iteration 168, loss = 0.44692783\n",
      "Iteration 1932, loss = 0.19910721\n",
      "Iteration 566, loss = 0.32780151\n",
      "Iteration 387, loss = 0.34312157\n",
      "Iteration 1708, loss = 0.17791140\n",
      "Iteration 1884, loss = 0.15761939\n",
      "Iteration 388, loss = 0.34298307\n",
      "Iteration 1885, loss = 0.15738028Iteration 567, loss = 0.32768408\n",
      "\n",
      "Iteration 1933, loss = 0.19892573\n",
      "Iteration 1886, loss = 0.15726539\n",
      "Iteration 1887, loss = 0.15719585\n",
      "Iteration 826, loss = 0.33092893\n",
      "Iteration 1709, loss = 0.17782184\n",
      "Iteration 169, loss = 0.44621857\n",
      "Iteration 1934, loss = 0.19881021\n",
      "Iteration 1888, loss = 0.15704173\n",
      "Iteration 1889, loss = 0.15697894\n",
      "Iteration 1890, loss = 0.15687385\n",
      "Iteration 1935, loss = 0.19872671\n",
      "Iteration 827, loss = 0.33080461\n",
      "Iteration 1891, loss = 0.15666994\n",
      "Iteration 170, loss = 0.44556718\n",
      "Iteration 389, loss = 0.34281461\n",
      "Iteration 568, loss = 0.32759480\n",
      "Iteration 1892, loss = 0.15654097\n",
      "Iteration 1893, loss = 0.15650823\n",
      "Iteration 1710, loss = 0.17769805\n",
      "Iteration 828, loss = 0.33055671\n",
      "Iteration 1894, loss = 0.15632905\n",
      "Iteration 390, loss = 0.34264942\n",
      "Iteration 569, loss = 0.32749798\n",
      "Iteration 1936, loss = 0.19861479\n",
      "Iteration 171, loss = 0.44490259\n",
      "Iteration 1711, loss = 0.17751892\n",
      "Iteration 1895, loss = 0.15621202\n",
      "Iteration 570, loss = 0.32743805\n",
      "Iteration 1937, loss = 0.19847894\n",
      "Iteration 1896, loss = 0.15615327\n",
      "Iteration 391, loss = 0.34255202\n",
      "Iteration 1897, loss = 0.15627756\n",
      "Iteration 1898, loss = 0.15588135\n",
      "Iteration 1899, loss = 0.15582468\n",
      "Iteration 1900, loss = 0.15572370\n",
      "Iteration 172, loss = 0.44426846\n",
      "Iteration 1901, loss = 0.15550560\n",
      "Iteration 1902, loss = 0.15547714\n",
      "Iteration 829, loss = 0.33042547\n",
      "Iteration 1903, loss = 0.15529823\n",
      "Iteration 1938, loss = 0.19834845\n",
      "Iteration 571, loss = 0.32728756\n",
      "Iteration 392, loss = 0.34241116\n",
      "Iteration 1904, loss = 0.15529017\n",
      "Iteration 1905, loss = 0.15512748\n",
      "Iteration 173, loss = 0.44363509\n",
      "Iteration 830, loss = 0.33027390\n",
      "Iteration 572, loss = 0.32720773\n",
      "Iteration 1906, loss = 0.15495400\n",
      "Iteration 1939, loss = 0.19825951\n",
      "Iteration 393, loss = 0.34222464\n",
      "Iteration 1907, loss = 0.15485040\n",
      "Iteration 1908, loss = 0.15475883\n",
      "Iteration 1712, loss = 0.17746528\n",
      "Iteration 174, loss = 0.44297270\n",
      "Iteration 831, loss = 0.33002606\n",
      "Iteration 573, loss = 0.32706158\n",
      "Iteration 1909, loss = 0.15465482\n",
      "Iteration 1940, loss = 0.19824441\n",
      "Iteration 1910, loss = 0.15448931\n",
      "Iteration 574, loss = 0.32696211\n",
      "Iteration 1713, loss = 0.17754434\n",
      "Iteration 1911, loss = 0.15453295\n",
      "Iteration 394, loss = 0.34205472\n",
      "Iteration 175, loss = 0.44239067\n",
      "Iteration 832, loss = 0.32987088\n",
      "Iteration 1941, loss = 0.19809528\n",
      "Iteration 1912, loss = 0.15426897\n",
      "Iteration 395, loss = 0.34190782\n",
      "Iteration 1913, loss = 0.15420844\n",
      "Iteration 575, loss = 0.32687454\n",
      "Iteration 1914, loss = 0.15400214\n",
      "Iteration 176, loss = 0.44171621\n",
      "Iteration 1942, loss = 0.19791059\n",
      "Iteration 1915, loss = 0.15390384\n",
      "Iteration 396, loss = 0.34180600\n",
      "Iteration 1916, loss = 0.15376254\n",
      "Iteration 1917, loss = 0.15364645\n",
      "Iteration 1714, loss = 0.17743624\n",
      "Iteration 576, loss = 0.32677342\n",
      "Iteration 397, loss = 0.34164776\n",
      "Iteration 1943, loss = 0.19794228\n",
      "Iteration 833, loss = 0.32972963\n",
      "Iteration 1918, loss = 0.15353245\n",
      "Iteration 1944, loss = 0.19775278\n",
      "Iteration 177, loss = 0.44113802\n",
      "Iteration 1919, loss = 0.15343320\n",
      "Iteration 577, loss = 0.32668936\n",
      "Iteration 834, loss = 0.32951164\n",
      "Iteration 1715, loss = 0.17706122Iteration 398, loss = 0.34150670\n",
      "\n",
      "Iteration 1945, loss = 0.19756898\n",
      "Iteration 1920, loss = 0.15332205\n",
      "Iteration 578, loss = 0.32655754\n",
      "Iteration 399, loss = 0.34133850\n",
      "Iteration 1921, loss = 0.15333686\n",
      "Iteration 1946, loss = 0.19748622\n",
      "Iteration 400, loss = 0.34119731\n",
      "Iteration 1922, loss = 0.15308471\n",
      "Iteration 579, loss = 0.32646204\n",
      "Iteration 835, loss = 0.32931228\n",
      "Iteration 1923, loss = 0.15297971\n",
      "Iteration 178, loss = 0.44050763Iteration 1716, loss = 0.17697223\n",
      "\n",
      "Iteration 1924, loss = 0.15282802\n",
      "Iteration 401, loss = 0.34106889\n",
      "Iteration 1925, loss = 0.15280010\n",
      "Iteration 1947, loss = 0.19737413\n",
      "Iteration 1926, loss = 0.15267949\n",
      "Iteration 836, loss = 0.32917269\n",
      "Iteration 580, loss = 0.32635874\n",
      "Iteration 1927, loss = 0.15264184\n",
      "Iteration 1948, loss = 0.19729167\n",
      "Iteration 402, loss = 0.34094895\n",
      "Iteration 1928, loss = 0.15242259\n",
      "Iteration 581, loss = 0.32628025\n",
      "Iteration 1717, loss = 0.17698221\n",
      "Iteration 837, loss = 0.32894896\n",
      "Iteration 179, loss = 0.43996919\n",
      "Iteration 403, loss = 0.34074524\n",
      "Iteration 1949, loss = 0.19716008\n",
      "Iteration 582, loss = 0.32616612\n",
      "Iteration 1718, loss = 0.17680194\n",
      "Iteration 1929, loss = 0.15243485\n",
      "Iteration 583, loss = 0.32606770\n",
      "Iteration 404, loss = 0.34070565\n",
      "Iteration 1930, loss = 0.15248183\n",
      "Iteration 1950, loss = 0.19715772\n",
      "Iteration 180, loss = 0.43933008\n",
      "Iteration 1931, loss = 0.15215423\n",
      "Iteration 405, loss = 0.34048190\n",
      "Iteration 584, loss = 0.32595004\n",
      "Iteration 1951, loss = 0.19705659\n",
      "Iteration 1932, loss = 0.15196991\n",
      "Iteration 1933, loss = 0.15187285\n",
      "Iteration 838, loss = 0.32876682\n",
      "Iteration 1719, loss = 0.17660904\n",
      "Iteration 1934, loss = 0.15171105\n",
      "Iteration 406, loss = 0.34030945\n",
      "Iteration 181, loss = 0.43877064\n",
      "Iteration 1935, loss = 0.15174510\n",
      "Iteration 1936, loss = 0.15166068\n",
      "Iteration 1952, loss = 0.19682007\n",
      "Iteration 1937, loss = 0.15147315\n",
      "Iteration 1938, loss = 0.15126897\n",
      "Iteration 1720, loss = 0.17658093\n",
      "Iteration 585, loss = 0.32587422\n",
      "Iteration 1939, loss = 0.15113374\n",
      "Iteration 839, loss = 0.32863891\n",
      "Iteration 1940, loss = 0.15099022\n",
      "Iteration 407, loss = 0.34020118\n",
      "Iteration 1941, loss = 0.15112277\n",
      "Iteration 1942, loss = 0.15080755\n",
      "Iteration 1953, loss = 0.19673551\n",
      "Iteration 1943, loss = 0.15065591\n",
      "Iteration 1944, loss = 0.15049610\n",
      "Iteration 182, loss = 0.43822069\n",
      "Iteration 1721, loss = 0.17652769\n",
      "Iteration 1945, loss = 0.15066997\n",
      "Iteration 840, loss = 0.32853878\n",
      "Iteration 1954, loss = 0.19661727\n",
      "Iteration 1946, loss = 0.15044104\n",
      "Iteration 586, loss = 0.32573213\n",
      "Iteration 408, loss = 0.34014370\n",
      "Iteration 1947, loss = 0.15019145\n",
      "Iteration 1955, loss = 0.19652621\n",
      "Iteration 409, loss = 0.33988696\n",
      "Iteration 1948, loss = 0.15003877\n",
      "Iteration 1722, loss = 0.17641217\n",
      "Iteration 587, loss = 0.32564916\n",
      "Iteration 183, loss = 0.43764154\n",
      "Iteration 1949, loss = 0.14995120\n",
      "Iteration 1956, loss = 0.19639973\n",
      "Iteration 841, loss = 0.32819570\n",
      "Iteration 410, loss = 0.33975172\n",
      "Iteration 1950, loss = 0.14989361\n",
      "Iteration 1951, loss = 0.14976683\n",
      "Iteration 1723, loss = 0.17634788\n",
      "Iteration 1952, loss = 0.14987987\n",
      "Iteration 411, loss = 0.33958882\n",
      "Iteration 588, loss = 0.32558921\n",
      "Iteration 1957, loss = 0.19632346\n",
      "Iteration 1953, loss = 0.14946492\n",
      "Iteration 1954, loss = 0.14937450\n",
      "Iteration 842, loss = 0.32808168\n",
      "Iteration 412, loss = 0.33946415\n",
      "Iteration 184, loss = 0.43711591\n",
      "Iteration 589, loss = 0.32543701\n",
      "Iteration 1955, loss = 0.14921916\n",
      "Iteration 413, loss = 0.33933064\n",
      "Iteration 1958, loss = 0.19619948\n",
      "Iteration 1724, loss = 0.17609775\n",
      "Iteration 1956, loss = 0.14919867\n",
      "Iteration 414, loss = 0.33922712\n",
      "Iteration 590, loss = 0.32532036\n",
      "Iteration 1957, loss = 0.14894602\n",
      "Iteration 1958, loss = 0.14889786\n",
      "Iteration 1959, loss = 0.14882975\n",
      "Iteration 415, loss = 0.33905532\n",
      "Iteration 843, loss = 0.32786676\n",
      "Iteration 185, loss = 0.43651479\n",
      "Iteration 591, loss = 0.32523377\n",
      "Iteration 1959, loss = 0.19617379\n",
      "Iteration 1960, loss = 0.14869880\n",
      "Iteration 1961, loss = 0.14865222\n",
      "Iteration 1962, loss = 0.14849642\n",
      "Iteration 1725, loss = 0.17595598\n",
      "Iteration 1960, loss = 0.19600645\n",
      "Iteration 416, loss = 0.33886546\n",
      "Iteration 1963, loss = 0.14828269\n",
      "Iteration 592, loss = 0.32514927\n",
      "Iteration 1964, loss = 0.14819574\n",
      "Iteration 417, loss = 0.33879077\n",
      "Iteration 1965, loss = 0.14806510\n",
      "Iteration 844, loss = 0.32768647\n",
      "Iteration 186, loss = 0.43597033\n",
      "Iteration 1966, loss = 0.14793941\n",
      "Iteration 1726, loss = 0.17590431\n",
      "Iteration 1967, loss = 0.14785690\n",
      "Iteration 1968, loss = 0.14786703\n",
      "Iteration 1961, loss = 0.19585140\n",
      "Iteration 418, loss = 0.33859430\n",
      "Iteration 593, loss = 0.32503705\n",
      "Iteration 1969, loss = 0.14772951\n",
      "Iteration 1970, loss = 0.14746543\n",
      "Iteration 845, loss = 0.32754936\n",
      "Iteration 419, loss = 0.33858441\n",
      "Iteration 1971, loss = 0.14755232\n",
      "Iteration 1972, loss = 0.14723530\n",
      "Iteration 1962, loss = 0.19574485\n",
      "Iteration 1973, loss = 0.14729276\n",
      "Iteration 1727, loss = 0.17577003\n",
      "Iteration 187, loss = 0.43541879\n",
      "Iteration 594, loss = 0.32493958\n",
      "Iteration 1974, loss = 0.14703855\n",
      "Iteration 420, loss = 0.33837568\n",
      "Iteration 1975, loss = 0.14691352\n",
      "Iteration 846, loss = 0.32733440\n",
      "Iteration 1976, loss = 0.14691683\n",
      "Iteration 1963, loss = 0.19561760\n",
      "Iteration 1977, loss = 0.14674364\n",
      "Iteration 421, loss = 0.33823520\n",
      "Iteration 1978, loss = 0.14659556\n",
      "Iteration 1728, loss = 0.17566226\n",
      "Iteration 595, loss = 0.32489412\n",
      "Iteration 188, loss = 0.43489088\n",
      "Iteration 422, loss = 0.33803795\n",
      "Iteration 1979, loss = 0.14657518\n",
      "Iteration 1980, loss = 0.14638346\n",
      "Iteration 847, loss = 0.32711106\n",
      "Iteration 1981, loss = 0.14618058\n",
      "Iteration 423, loss = 0.33794602\n",
      "Iteration 596, loss = 0.32476607\n",
      "Iteration 1964, loss = 0.19554366\n",
      "Iteration 1982, loss = 0.14617620\n",
      "Iteration 1729, loss = 0.17560892\n",
      "Iteration 1983, loss = 0.14600593\n",
      "Iteration 424, loss = 0.33772637\n",
      "Iteration 1984, loss = 0.14581483\n",
      "Iteration 597, loss = 0.32462944\n",
      "Iteration 425, loss = 0.33762654\n",
      "Iteration 1985, loss = 0.14576629\n",
      "Iteration 189, loss = 0.43437207\n",
      "Iteration 848, loss = 0.32699757\n",
      "Iteration 1986, loss = 0.14589352\n",
      "Iteration 1730, loss = 0.17548888\n",
      "Iteration 1965, loss = 0.19544196\n",
      "Iteration 426, loss = 0.33754814\n",
      "Iteration 1987, loss = 0.14559511\n",
      "Iteration 427, loss = 0.33732944\n",
      "Iteration 190, loss = 0.43386770\n",
      "Iteration 849, loss = 0.32677327\n",
      "Iteration 1988, loss = 0.14534838\n",
      "Iteration 598, loss = 0.32452349\n",
      "Iteration 1966, loss = 0.19533557\n",
      "Iteration 1989, loss = 0.14528065\n",
      "Iteration 1731, loss = 0.17535041\n",
      "Iteration 1990, loss = 0.14518669Iteration 428, loss = 0.33718956\n",
      "\n",
      "Iteration 1991, loss = 0.14502578\n",
      "Iteration 429, loss = 0.33704961\n",
      "Iteration 1992, loss = 0.14497634\n",
      "Iteration 1967, loss = 0.19528205\n",
      "Iteration 1993, loss = 0.14479088\n",
      "Iteration 1732, loss = 0.17525606\n",
      "Iteration 599, loss = 0.32453035\n",
      "Iteration 850, loss = 0.32656529\n",
      "Iteration 1994, loss = 0.14469617\n",
      "Iteration 430, loss = 0.33695665\n",
      "Iteration 191, loss = 0.43332869\n",
      "Iteration 1995, loss = 0.14481531\n",
      "Iteration 431, loss = 0.33676587\n",
      "Iteration 600, loss = 0.32434261\n",
      "Iteration 851, loss = 0.32638039\n",
      "Iteration 1733, loss = 0.17517966\n",
      "Iteration 1968, loss = 0.19508947\n",
      "Iteration 1996, loss = 0.14463656\n",
      "Iteration 432, loss = 0.33665659\n",
      "Iteration 192, loss = 0.43280919\n",
      "Iteration 1997, loss = 0.14435701\n",
      "Iteration 601, loss = 0.32426767\n",
      "Iteration 1998, loss = 0.14416742\n",
      "Iteration 602, loss = 0.32413713\n",
      "Iteration 1999, loss = 0.14434982\n",
      "Iteration 2000, loss = 0.14401215\n",
      "Iteration 1969, loss = 0.19512493\n",
      "Iteration 193, loss = 0.43227319\n",
      "Iteration 1734, loss = 0.17524833\n",
      "Iteration 2001, loss = 0.14388160\n",
      "Iteration 2002, loss = 0.14397356\n",
      "Iteration 603, loss = 0.32408768\n",
      "Iteration 433, loss = 0.33648058\n",
      "Iteration 1970, loss = 0.19505288\n",
      "Iteration 2003, loss = 0.14380977\n",
      "Iteration 604, loss = 0.32393150\n",
      "Iteration 194, loss = 0.43184091\n",
      "Iteration 2004, loss = 0.14352029\n",
      "Iteration 1735, loss = 0.17496793\n",
      "Iteration 434, loss = 0.33635844\n",
      "Iteration 852, loss = 0.32625385\n",
      "Iteration 2005, loss = 0.14340991\n",
      "Iteration 435, loss = 0.33623475\n",
      "Iteration 2006, loss = 0.14334736\n",
      "Iteration 2007, loss = 0.14320227\n",
      "Iteration 605, loss = 0.32387013\n",
      "Iteration 1971, loss = 0.19480373\n",
      "Iteration 2008, loss = 0.14307948\n",
      "Iteration 853, loss = 0.32601775\n",
      "Iteration 436, loss = 0.33607629\n",
      "Iteration 2009, loss = 0.14300633\n",
      "Iteration 2010, loss = 0.14292338\n",
      "Iteration 437, loss = 0.33596549\n",
      "Iteration 2011, loss = 0.14295196\n",
      "Iteration 1736, loss = 0.17479681\n",
      "Iteration 2012, loss = 0.14261446\n",
      "Iteration 2013, loss = 0.14264656\n",
      "Iteration 1972, loss = 0.19467168\n",
      "Iteration 2014, loss = 0.14238343\n",
      "Iteration 438, loss = 0.33579232\n",
      "Iteration 195, loss = 0.43129637\n",
      "Iteration 854, loss = 0.32583934\n",
      "Iteration 606, loss = 0.32371996\n",
      "Iteration 1973, loss = 0.19458205\n",
      "Iteration 1737, loss = 0.17469413\n",
      "Iteration 2015, loss = 0.14229821\n",
      "Iteration 439, loss = 0.33563322\n",
      "Iteration 2016, loss = 0.14219303\n",
      "Iteration 440, loss = 0.33557405\n",
      "Iteration 2017, loss = 0.14210037\n",
      "Iteration 2018, loss = 0.14201475\n",
      "Iteration 196, loss = 0.43080274\n",
      "Iteration 1974, loss = 0.19445866\n",
      "Iteration 855, loss = 0.32569105\n",
      "Iteration 441, loss = 0.33534130\n",
      "Iteration 607, loss = 0.32364613\n",
      "Iteration 2019, loss = 0.14196456\n",
      "Iteration 2020, loss = 0.14166337\n",
      "Iteration 1975, loss = 0.19443222\n",
      "Iteration 442, loss = 0.33521617\n",
      "Iteration 197, loss = 0.43036718\n",
      "Iteration 856, loss = 0.32552876\n",
      "Iteration 1738, loss = 0.17463970\n",
      "Iteration 2021, loss = 0.14158399\n",
      "Iteration 2022, loss = 0.14142135\n",
      "Iteration 443, loss = 0.33510062\n",
      "Iteration 2023, loss = 0.14139025\n",
      "Iteration 198, loss = 0.42984422\n",
      "Iteration 608, loss = 0.32352766\n",
      "Iteration 2024, loss = 0.14125476\n",
      "Iteration 2025, loss = 0.14114505\n",
      "Iteration 444, loss = 0.33496360\n",
      "Iteration 2026, loss = 0.14104635\n",
      "Iteration 1976, loss = 0.19423360\n",
      "Iteration 2027, loss = 0.14095855\n",
      "Iteration 857, loss = 0.32532524\n",
      "Iteration 2028, loss = 0.14074895\n",
      "Iteration 1739, loss = 0.17455623\n",
      "Iteration 2029, loss = 0.14068720\n",
      "Iteration 609, loss = 0.32343429\n",
      "Iteration 2030, loss = 0.14078105\n",
      "Iteration 199, loss = 0.42936307\n",
      "Iteration 2031, loss = 0.14050292\n",
      "Iteration 1977, loss = 0.19414814\n",
      "Iteration 2032, loss = 0.14031894\n",
      "Iteration 610, loss = 0.32333348\n",
      "Iteration 858, loss = 0.32511955\n",
      "Iteration 2033, loss = 0.14016298\n",
      "Iteration 1740, loss = 0.17437221\n",
      "Iteration 445, loss = 0.33481585\n",
      "Iteration 2034, loss = 0.14004527\n",
      "Iteration 611, loss = 0.32326368\n",
      "Iteration 2035, loss = 0.13993387\n",
      "Iteration 2036, loss = 0.13978915\n",
      "Iteration 1978, loss = 0.19409202\n",
      "Iteration 2037, loss = 0.13971185\n",
      "Iteration 1741, loss = 0.17443950\n",
      "Iteration 612, loss = 0.32313312\n",
      "Iteration 200, loss = 0.42889814\n",
      "Iteration 2038, loss = 0.13964750\n",
      "Iteration 2039, loss = 0.13946838\n",
      "Iteration 859, loss = 0.32488421\n",
      "Iteration 2040, loss = 0.13952534\n",
      "Iteration 2041, loss = 0.13927351\n",
      "Iteration 613, loss = 0.32306160\n",
      "Iteration 1979, loss = 0.19391260\n",
      "Iteration 1742, loss = 0.17442368\n",
      "Iteration 446, loss = 0.33469373\n",
      "Iteration 201, loss = 0.42844939\n",
      "Iteration 2042, loss = 0.13923792\n",
      "Iteration 2043, loss = 0.13915910\n",
      "Iteration 1980, loss = 0.19384735\n",
      "Iteration 2044, loss = 0.13907550\n",
      "Iteration 860, loss = 0.32471166\n",
      "Iteration 2045, loss = 0.13894424\n",
      "Iteration 447, loss = 0.33461433\n",
      "Iteration 1743, loss = 0.17416308\n",
      "Iteration 614, loss = 0.32296863\n",
      "Iteration 1981, loss = 0.19375122\n",
      "Iteration 2046, loss = 0.13877106\n",
      "Iteration 2047, loss = 0.13861087\n",
      "Iteration 2048, loss = 0.13851209\n",
      "Iteration 861, loss = 0.32457013\n",
      "Iteration 615, loss = 0.32287250\n",
      "Iteration 448, loss = 0.33441185\n",
      "Iteration 1982, loss = 0.19360843\n",
      "Iteration 2049, loss = 0.13833321\n",
      "Iteration 449, loss = 0.33424149\n",
      "Iteration 202, loss = 0.42798379\n",
      "Iteration 1744, loss = 0.17393223\n",
      "Iteration 1983, loss = 0.19354796\n",
      "Iteration 2050, loss = 0.13832712\n",
      "Iteration 616, loss = 0.32275737\n",
      "Iteration 2051, loss = 0.13820875\n",
      "Iteration 2052, loss = 0.13804267\n",
      "Iteration 450, loss = 0.33417168\n",
      "Iteration 862, loss = 0.32444931\n",
      "Iteration 2053, loss = 0.13821971\n",
      "Iteration 1984, loss = 0.19342933\n",
      "Iteration 617, loss = 0.32263600\n",
      "Iteration 2054, loss = 0.13786401\n",
      "Iteration 2055, loss = 0.13766769\n",
      "Iteration 2056, loss = 0.13757148\n",
      "Iteration 451, loss = 0.33397807\n",
      "Iteration 863, loss = 0.32419012\n",
      "Iteration 1745, loss = 0.17398330\n",
      "Iteration 2057, loss = 0.13748862\n",
      "Iteration 203, loss = 0.42754903\n",
      "Iteration 1985, loss = 0.19330497\n",
      "Iteration 452, loss = 0.33383714\n",
      "Iteration 2058, loss = 0.13731278\n",
      "Iteration 618, loss = 0.32255968\n",
      "Iteration 864, loss = 0.32399804\n",
      "Iteration 1986, loss = 0.19318754\n",
      "Iteration 2059, loss = 0.13724602\n",
      "Iteration 204, loss = 0.42710695\n",
      "Iteration 2060, loss = 0.13718306\n",
      "Iteration 453, loss = 0.33375696\n",
      "Iteration 1746, loss = 0.17377707\n",
      "Iteration 2061, loss = 0.13699614\n",
      "Iteration 619, loss = 0.32244609\n",
      "Iteration 2062, loss = 0.13692965\n",
      "Iteration 2063, loss = 0.13696124\n",
      "Iteration 1987, loss = 0.19314551\n",
      "Iteration 454, loss = 0.33363528\n",
      "Iteration 865, loss = 0.32383695\n",
      "Iteration 2064, loss = 0.13667903\n",
      "Iteration 1747, loss = 0.17367771\n",
      "Iteration 620, loss = 0.32238582\n",
      "Iteration 2065, loss = 0.13658276\n",
      "Iteration 205, loss = 0.42668494\n",
      "Iteration 455, loss = 0.33341607\n",
      "Iteration 2066, loss = 0.13644938\n",
      "Iteration 1988, loss = 0.19295873\n",
      "Iteration 866, loss = 0.32360840\n",
      "Iteration 456, loss = 0.33327643\n",
      "Iteration 1748, loss = 0.17359131\n",
      "Iteration 457, loss = 0.33316592\n",
      "Iteration 2067, loss = 0.13630773\n",
      "Iteration 206, loss = 0.42619847\n",
      "Iteration 2068, loss = 0.13623132\n",
      "Iteration 1989, loss = 0.19287354\n",
      "Iteration 867, loss = 0.32357385\n",
      "Iteration 621, loss = 0.32226868\n",
      "Iteration 2069, loss = 0.13630767\n",
      "Iteration 2070, loss = 0.13615571\n",
      "Iteration 458, loss = 0.33303931\n",
      "Iteration 1990, loss = 0.19282525\n",
      "Iteration 2071, loss = 0.13595518\n",
      "Iteration 1749, loss = 0.17342690\n",
      "Iteration 2072, loss = 0.13605984\n",
      "Iteration 622, loss = 0.32218243\n",
      "Iteration 459, loss = 0.33295822\n",
      "Iteration 207, loss = 0.42580177\n",
      "Iteration 2073, loss = 0.13566134\n",
      "Iteration 2074, loss = 0.13552159\n",
      "Iteration 868, loss = 0.32332014\n",
      "Iteration 1991, loss = 0.19269456\n",
      "Iteration 2075, loss = 0.13549679\n",
      "Iteration 460, loss = 0.33273590\n",
      "Iteration 208, loss = 0.42536847\n",
      "Iteration 2076, loss = 0.13550468\n",
      "Iteration 2077, loss = 0.13529463\n",
      "Iteration 461, loss = 0.33260760\n",
      "Iteration 2078, loss = 0.13511883\n",
      "Iteration 1750, loss = 0.17331214\n",
      "Iteration 623, loss = 0.32205603\n",
      "Iteration 2079, loss = 0.13497119\n",
      "Iteration 2080, loss = 0.13491263\n",
      "Iteration 209, loss = 0.42494710\n",
      "Iteration 869, loss = 0.32307801\n",
      "Iteration 462, loss = 0.33248969\n",
      "Iteration 2081, loss = 0.13492452\n",
      "Iteration 1992, loss = 0.19260155\n",
      "Iteration 624, loss = 0.32194009\n",
      "Iteration 2082, loss = 0.13473990\n",
      "Iteration 1751, loss = 0.17344985\n",
      "Iteration 463, loss = 0.33233997\n",
      "Iteration 210, loss = 0.42454759\n",
      "Iteration 625, loss = 0.32188695\n",
      "Iteration 2083, loss = 0.13473707\n",
      "Iteration 1993, loss = 0.19250794\n",
      "Iteration 2084, loss = 0.13443977\n",
      "Iteration 464, loss = 0.33221051\n",
      "Iteration 2085, loss = 0.13437617\n",
      "Iteration 626, loss = 0.32184202\n",
      "Iteration 1994, loss = 0.19237961\n",
      "Iteration 211, loss = 0.42414730\n",
      "Iteration 2086, loss = 0.13418327\n",
      "Iteration 465, loss = 0.33211795\n",
      "Iteration 870, loss = 0.32282981\n",
      "Iteration 2087, loss = 0.13406666\n",
      "Iteration 2088, loss = 0.13403909\n",
      "Iteration 627, loss = 0.32171460\n",
      "Iteration 466, loss = 0.33194928\n",
      "Iteration 1995, loss = 0.19225922\n",
      "Iteration 212, loss = 0.42372098\n",
      "Iteration 2089, loss = 0.13390535\n",
      "Iteration 467, loss = 0.33183498\n",
      "Iteration 1752, loss = 0.17316102\n",
      "Iteration 628, loss = 0.32153951\n",
      "Iteration 871, loss = 0.32265562\n",
      "Iteration 2090, loss = 0.13375824\n",
      "Iteration 2091, loss = 0.13382169\n",
      "Iteration 468, loss = 0.33176464\n",
      "Iteration 1996, loss = 0.19213249\n",
      "Iteration 213, loss = 0.42326311\n",
      "Iteration 2092, loss = 0.13351274\n",
      "Iteration 469, loss = 0.33154018\n",
      "Iteration 629, loss = 0.32144997\n",
      "Iteration 2093, loss = 0.13365335\n",
      "Iteration 872, loss = 0.32255006\n",
      "Iteration 1753, loss = 0.17307516\n",
      "Iteration 2094, loss = 0.13333335\n",
      "Iteration 2095, loss = 0.13314762\n",
      "Iteration 2096, loss = 0.13309764\n",
      "Iteration 1997, loss = 0.19204990\n",
      "Iteration 630, loss = 0.32135781\n",
      "Iteration 2097, loss = 0.13302184\n",
      "Iteration 470, loss = 0.33144396\n",
      "Iteration 2098, loss = 0.13279565\n",
      "Iteration 2099, loss = 0.13275196\n",
      "Iteration 214, loss = 0.42288351\n",
      "Iteration 873, loss = 0.32231164\n",
      "Iteration 1998, loss = 0.19193281\n",
      "Iteration 471, loss = 0.33128539\n",
      "Iteration 631, loss = 0.32133139\n",
      "Iteration 1754, loss = 0.17292426\n",
      "Iteration 2100, loss = 0.13261922\n",
      "Iteration 2101, loss = 0.13247132\n",
      "Iteration 2102, loss = 0.13234442\n",
      "Iteration 472, loss = 0.33117415\n",
      "Iteration 215, loss = 0.42251243\n",
      "Iteration 1999, loss = 0.19186558\n",
      "Iteration 2103, loss = 0.13226154\n",
      "Iteration 632, loss = 0.32114990\n",
      "Iteration 2104, loss = 0.13221099\n",
      "Iteration 473, loss = 0.33102208\n",
      "Iteration 2105, loss = 0.13200714\n",
      "Iteration 2000, loss = 0.19169687\n",
      "Iteration 874, loss = 0.32212034\n",
      "Iteration 1755, loss = 0.17289268\n",
      "Iteration 474, loss = 0.33084017\n",
      "Iteration 216, loss = 0.42211323\n",
      "Iteration 2106, loss = 0.13190567\n",
      "Iteration 2001, loss = 0.19161071\n",
      "Iteration 2107, loss = 0.13187061\n",
      "Iteration 475, loss = 0.33071962\n",
      "Iteration 633, loss = 0.32104463\n",
      "Iteration 2108, loss = 0.13170090\n",
      "Iteration 2002, loss = 0.19154950\n",
      "Iteration 875, loss = 0.32189984\n",
      "Iteration 2109, loss = 0.13169696\n",
      "Iteration 1756, loss = 0.17273890\n",
      "Iteration 2110, loss = 0.13162372\n",
      "Iteration 2111, loss = 0.13142287\n",
      "Iteration 2112, loss = 0.13123477\n",
      "Iteration 2003, loss = 0.19138640\n",
      "Iteration 476, loss = 0.33059557\n",
      "Iteration 2113, loss = 0.13112401\n",
      "Iteration 2114, loss = 0.13120906\n",
      "Iteration 1757, loss = 0.17260431\n",
      "Iteration 634, loss = 0.32097056\n",
      "Iteration 2115, loss = 0.13091225\n",
      "Iteration 477, loss = 0.33045310\n",
      "Iteration 217, loss = 0.42170738\n",
      "Iteration 2116, loss = 0.13122825\n",
      "Iteration 2004, loss = 0.19140382\n",
      "Iteration 2117, loss = 0.13063808\n",
      "Iteration 2118, loss = 0.13057398\n",
      "Iteration 876, loss = 0.32172655\n",
      "Iteration 1758, loss = 0.17250249\n",
      "Iteration 478, loss = 0.33038483\n",
      "Iteration 2119, loss = 0.13068991\n",
      "Iteration 635, loss = 0.32084840\n",
      "Iteration 2120, loss = 0.13042856\n",
      "Iteration 479, loss = 0.33016141\n",
      "Iteration 2121, loss = 0.13019386\n",
      "Iteration 2005, loss = 0.19124451\n",
      "Iteration 2122, loss = 0.13019926\n",
      "Iteration 636, loss = 0.32076686\n",
      "Iteration 480, loss = 0.33002360\n",
      "Iteration 2123, loss = 0.12997119\n",
      "Iteration 1759, loss = 0.17242539\n",
      "Iteration 2124, loss = 0.12990086\n",
      "Iteration 877, loss = 0.32152006\n",
      "Iteration 637, loss = 0.32066144\n",
      "Iteration 2125, loss = 0.12978954\n",
      "Iteration 2126, loss = 0.12963710\n",
      "Iteration 218, loss = 0.42134448\n",
      "Iteration 2127, loss = 0.12957426\n",
      "Iteration 2006, loss = 0.19107594\n",
      "Iteration 2128, loss = 0.12945577\n",
      "Iteration 481, loss = 0.33006819\n",
      "Iteration 1760, loss = 0.17255090\n",
      "Iteration 638, loss = 0.32054684\n",
      "Iteration 2129, loss = 0.12931955\n",
      "Iteration 878, loss = 0.32131568\n",
      "Iteration 2130, loss = 0.12918044\n",
      "Iteration 2131, loss = 0.12902778\n",
      "Iteration 219, loss = 0.42102705\n",
      "Iteration 2132, loss = 0.12902289\n",
      "Iteration 482, loss = 0.32983829\n",
      "Iteration 1761, loss = 0.17228561\n",
      "Iteration 639, loss = 0.32050108\n",
      "Iteration 2007, loss = 0.19100951\n",
      "Iteration 2133, loss = 0.12880420\n",
      "Iteration 2134, loss = 0.12873753\n",
      "Iteration 483, loss = 0.32965363\n",
      "Iteration 640, loss = 0.32047587\n",
      "Iteration 879, loss = 0.32110665\n",
      "Iteration 220, loss = 0.42057555\n",
      "Iteration 2135, loss = 0.12858556\n",
      "Iteration 2136, loss = 0.12845239\n",
      "Iteration 484, loss = 0.32952473\n",
      "Iteration 2137, loss = 0.12833869\n",
      "Iteration 2008, loss = 0.19094643\n",
      "Iteration 2138, loss = 0.12862207\n",
      "Iteration 641, loss = 0.32025075\n",
      "Iteration 1762, loss = 0.17217810\n",
      "Iteration 2139, loss = 0.12814123\n",
      "Iteration 880, loss = 0.32099837\n",
      "Iteration 485, loss = 0.32936549\n",
      "Iteration 2140, loss = 0.12818045\n",
      "Iteration 2009, loss = 0.19078007\n",
      "Iteration 2141, loss = 0.12796469\n",
      "Iteration 2142, loss = 0.12796541\n",
      "Iteration 221, loss = 0.42018550\n",
      "Iteration 1763, loss = 0.17213494\n",
      "Iteration 642, loss = 0.32016486\n",
      "Iteration 486, loss = 0.32924287\n",
      "Iteration 2143, loss = 0.12766975\n",
      "Iteration 2144, loss = 0.12758497\n",
      "Iteration 2010, loss = 0.19072594\n",
      "Iteration 2145, loss = 0.12758720\n",
      "Iteration 2146, loss = 0.12734110\n",
      "Iteration 643, loss = 0.32006881\n",
      "Iteration 2147, loss = 0.12737383\n",
      "Iteration 881, loss = 0.32073030\n",
      "Iteration 487, loss = 0.32908723\n",
      "Iteration 2148, loss = 0.12708874\n",
      "Iteration 644, loss = 0.31995769\n",
      "Iteration 222, loss = 0.41981449\n",
      "Iteration 1764, loss = 0.17195588\n",
      "Iteration 2149, loss = 0.12704843\n",
      "Iteration 2150, loss = 0.12698777\n",
      "Iteration 2011, loss = 0.19059542\n",
      "Iteration 2151, loss = 0.12687613\n",
      "Iteration 645, loss = 0.31987856\n",
      "Iteration 488, loss = 0.32896172\n",
      "Iteration 2152, loss = 0.12666515\n",
      "Iteration 223, loss = 0.41941324\n",
      "Iteration 2153, loss = 0.12664724\n",
      "Iteration 1765, loss = 0.17181272\n",
      "Iteration 2012, loss = 0.19047333\n",
      "Iteration 882, loss = 0.32071440\n",
      "Iteration 2154, loss = 0.12647936\n",
      "Iteration 489, loss = 0.32881811\n",
      "Iteration 2155, loss = 0.12647426\n",
      "Iteration 646, loss = 0.31978704\n",
      "Iteration 2013, loss = 0.19033040\n",
      "Iteration 224, loss = 0.41908244\n",
      "Iteration 2156, loss = 0.12623376\n",
      "Iteration 490, loss = 0.32870846\n",
      "Iteration 2157, loss = 0.12613090\n",
      "Iteration 2158, loss = 0.12619112\n",
      "Iteration 1766, loss = 0.17165023\n",
      "Iteration 491, loss = 0.32855932\n",
      "Iteration 2159, loss = 0.12589349\n",
      "Iteration 2160, loss = 0.12594663\n",
      "Iteration 2014, loss = 0.19026785\n",
      "Iteration 883, loss = 0.32043208\n",
      "Iteration 647, loss = 0.31979858\n",
      "Iteration 2161, loss = 0.12572838\n",
      "Iteration 2162, loss = 0.12563662\n",
      "Iteration 225, loss = 0.41872609\n",
      "Iteration 2163, loss = 0.12545112\n",
      "Iteration 648, loss = 0.31958209\n",
      "Iteration 492, loss = 0.32840483\n",
      "Iteration 2164, loss = 0.12548832\n",
      "Iteration 2015, loss = 0.19020006\n",
      "Iteration 493, loss = 0.32827141\n",
      "Iteration 2165, loss = 0.12535360\n",
      "Iteration 649, loss = 0.31949629\n",
      "Iteration 2166, loss = 0.12518986\n",
      "Iteration 1767, loss = 0.17154365\n",
      "Iteration 884, loss = 0.32018915\n",
      "Iteration 494, loss = 0.32812208\n",
      "Iteration 2167, loss = 0.12504030\n",
      "Iteration 2016, loss = 0.19005580\n",
      "Iteration 226, loss = 0.41834067\n",
      "Iteration 495, loss = 0.32801376\n",
      "Iteration 2168, loss = 0.12510292\n",
      "Iteration 2169, loss = 0.12492443\n",
      "Iteration 496, loss = 0.32784742\n",
      "Iteration 2170, loss = 0.12489743\n",
      "Iteration 650, loss = 0.31941568\n",
      "Iteration 1768, loss = 0.17153574\n",
      "Iteration 2171, loss = 0.12468435\n",
      "Iteration 2017, loss = 0.18991972\n",
      "Iteration 497, loss = 0.32771639\n",
      "Iteration 2172, loss = 0.12447669\n",
      "Iteration 885, loss = 0.32001680\n",
      "Iteration 651, loss = 0.31928569\n",
      "Iteration 227, loss = 0.41795729\n",
      "Iteration 2018, loss = 0.18983650\n",
      "Iteration 2173, loss = 0.12437140\n",
      "Iteration 498, loss = 0.32758126\n",
      "Iteration 652, loss = 0.31917837\n",
      "Iteration 2174, loss = 0.12436438\n",
      "Iteration 2175, loss = 0.12416462\n",
      "Iteration 886, loss = 0.31982531\n",
      "Iteration 2176, loss = 0.12406948\n",
      "Iteration 2019, loss = 0.18970613\n",
      "Iteration 499, loss = 0.32744741\n",
      "Iteration 2177, loss = 0.12402486\n",
      "Iteration 1769, loss = 0.17134207\n",
      "Iteration 2178, loss = 0.12403942\n",
      "Iteration 653, loss = 0.31908344\n",
      "Iteration 500, loss = 0.32730796\n",
      "Iteration 2179, loss = 0.12387021\n",
      "Iteration 228, loss = 0.41759685\n",
      "Iteration 2180, loss = 0.12368003\n",
      "Iteration 2181, loss = 0.12357909\n",
      "Iteration 887, loss = 0.31957744\n",
      "Iteration 501, loss = 0.32720256\n",
      "Iteration 2182, loss = 0.12345567\n",
      "Iteration 1770, loss = 0.17136139\n",
      "Iteration 2183, loss = 0.12336867\n",
      "Iteration 2020, loss = 0.18962139\n",
      "Iteration 2184, loss = 0.12320096\n",
      "Iteration 2185, loss = 0.12306456\n",
      "Iteration 654, loss = 0.31901232\n",
      "Iteration 2186, loss = 0.12302680\n",
      "Iteration 502, loss = 0.32706347\n",
      "Iteration 229, loss = 0.41726105\n",
      "Iteration 2187, loss = 0.12304174\n",
      "Iteration 888, loss = 0.31938870\n",
      "Iteration 655, loss = 0.31888391\n",
      "Iteration 2021, loss = 0.18955042\n",
      "Iteration 1771, loss = 0.17120002\n",
      "Iteration 503, loss = 0.32696392\n",
      "Iteration 2188, loss = 0.12284133\n",
      "Iteration 230, loss = 0.41692475\n",
      "Iteration 2189, loss = 0.12269561\n",
      "Iteration 504, loss = 0.32679536\n",
      "Iteration 2190, loss = 0.12265165\n",
      "Iteration 1772, loss = 0.17123974\n",
      "Iteration 2022, loss = 0.18939543\n",
      "Iteration 2191, loss = 0.12243699\n",
      "Iteration 2192, loss = 0.12256922\n",
      "Iteration 2193, loss = 0.12226416\n",
      "Iteration 889, loss = 0.31922871\n",
      "Iteration 656, loss = 0.31877744\n",
      "Iteration 505, loss = 0.32664853\n",
      "Iteration 2194, loss = 0.12239459\n",
      "Iteration 2023, loss = 0.18930036\n",
      "Iteration 2195, loss = 0.12223675\n",
      "Iteration 231, loss = 0.41672698\n",
      "Iteration 657, loss = 0.31869893\n",
      "Iteration 506, loss = 0.32653597\n",
      "Iteration 2024, loss = 0.18919067\n",
      "Iteration 1773, loss = 0.17095459\n",
      "Iteration 890, loss = 0.31903382\n",
      "Iteration 2196, loss = 0.12188672\n",
      "Iteration 507, loss = 0.32637495\n",
      "Iteration 2197, loss = 0.12196259\n",
      "Iteration 508, loss = 0.32623666\n",
      "Iteration 2198, loss = 0.12197158\n",
      "Iteration 658, loss = 0.31861369\n",
      "Iteration 2199, loss = 0.12159687\n",
      "Iteration 2025, loss = 0.18912388\n",
      "Iteration 509, loss = 0.32610071\n",
      "Iteration 232, loss = 0.41619312\n",
      "Iteration 891, loss = 0.31884693\n",
      "Iteration 2200, loss = 0.12155326\n",
      "Iteration 1774, loss = 0.17091389\n",
      "Iteration 2201, loss = 0.12147291\n",
      "Iteration 510, loss = 0.32597108\n",
      "Iteration 659, loss = 0.31848798\n",
      "Iteration 2202, loss = 0.12139384\n",
      "Iteration 2203, loss = 0.12128925\n",
      "Iteration 511, loss = 0.32584979\n",
      "Iteration 2026, loss = 0.18907018\n",
      "Iteration 2204, loss = 0.12107078\n",
      "Iteration 660, loss = 0.31840275\n",
      "Iteration 2205, loss = 0.12100115\n",
      "Iteration 512, loss = 0.32577178\n",
      "Iteration 2206, loss = 0.12088081\n",
      "Iteration 2027, loss = 0.18900429\n",
      "Iteration 2207, loss = 0.12079557\n",
      "Iteration 661, loss = 0.31828733\n",
      "Iteration 233, loss = 0.41587939\n",
      "Iteration 1775, loss = 0.17079597\n",
      "Iteration 2208, loss = 0.12064625\n",
      "Iteration 513, loss = 0.32568453\n",
      "Iteration 892, loss = 0.31875517\n",
      "Iteration 2209, loss = 0.12058043\n",
      "Iteration 2028, loss = 0.18886611\n",
      "Iteration 662, loss = 0.31819147\n",
      "Iteration 514, loss = 0.32543705\n",
      "Iteration 2210, loss = 0.12054697\n",
      "Iteration 1776, loss = 0.17078354\n",
      "Iteration 234, loss = 0.41552483\n",
      "Iteration 2211, loss = 0.12047940\n",
      "Iteration 663, loss = 0.31809956\n",
      "Iteration 2029, loss = 0.18869876\n",
      "Iteration 2212, loss = 0.12032249\n",
      "Iteration 515, loss = 0.32531441\n",
      "Iteration 2213, loss = 0.12017267\n",
      "Iteration 2214, loss = 0.12023481\n",
      "Iteration 235, loss = 0.41521167\n",
      "Iteration 1777, loss = 0.17063926\n",
      "Iteration 516, loss = 0.32522773\n",
      "Iteration 2215, loss = 0.11998411\n",
      "Iteration 664, loss = 0.31800690\n",
      "Iteration 2030, loss = 0.18855864\n",
      "Iteration 2216, loss = 0.11992989\n",
      "Iteration 893, loss = 0.31843875\n",
      "Iteration 2217, loss = 0.11980274\n",
      "Iteration 517, loss = 0.32512342\n",
      "Iteration 2218, loss = 0.11981035\n",
      "Iteration 2031, loss = 0.18846216\n",
      "Iteration 2219, loss = 0.11959554\n",
      "Iteration 236, loss = 0.41485003\n",
      "Iteration 665, loss = 0.31797827\n",
      "Iteration 1778, loss = 0.17044733\n",
      "Iteration 2220, loss = 0.11952908\n",
      "Iteration 518, loss = 0.32487802\n",
      "Iteration 894, loss = 0.31830087\n",
      "Iteration 2221, loss = 0.11965691\n",
      "Iteration 2032, loss = 0.18836958\n",
      "Iteration 519, loss = 0.32477145\n",
      "Iteration 2222, loss = 0.11943635\n",
      "Iteration 666, loss = 0.31789163\n",
      "Iteration 2223, loss = 0.11927748\n",
      "Iteration 1779, loss = 0.17045077\n",
      "Iteration 237, loss = 0.41451910\n",
      "Iteration 2224, loss = 0.11911641\n",
      "Iteration 520, loss = 0.32462582\n",
      "Iteration 2033, loss = 0.18829499\n",
      "Iteration 667, loss = 0.31773585\n",
      "Iteration 521, loss = 0.32449492\n",
      "Iteration 895, loss = 0.31813828\n",
      "Iteration 2225, loss = 0.11893897\n",
      "Iteration 2226, loss = 0.11881229\n",
      "Iteration 1780, loss = 0.17028447\n",
      "Iteration 668, loss = 0.31761103\n",
      "Iteration 522, loss = 0.32434719\n",
      "Iteration 2034, loss = 0.18817157\n",
      "Iteration 2227, loss = 0.11875109\n",
      "Iteration 2228, loss = 0.11860580\n",
      "Iteration 2229, loss = 0.11855635\n",
      "Iteration 238, loss = 0.41418021\n",
      "Iteration 523, loss = 0.32425434\n",
      "Iteration 2230, loss = 0.11850816\n",
      "Iteration 669, loss = 0.31751800\n",
      "Iteration 2231, loss = 0.11835361\n",
      "Iteration 524, loss = 0.32409766\n",
      "Iteration 2232, loss = 0.11828426\n",
      "Iteration 896, loss = 0.31795825\n",
      "Iteration 2035, loss = 0.18806782\n",
      "Iteration 670, loss = 0.31743138\n",
      "Iteration 1781, loss = 0.17019699\n",
      "Iteration 239, loss = 0.41387527\n",
      "Iteration 2233, loss = 0.11809174\n",
      "Iteration 525, loss = 0.32393609\n",
      "Iteration 2234, loss = 0.11816336\n",
      "Iteration 2036, loss = 0.18795326\n",
      "Iteration 2235, loss = 0.11789215\n",
      "Iteration 671, loss = 0.31730789\n",
      "Iteration 2236, loss = 0.11795682\n",
      "Iteration 897, loss = 0.31769009\n",
      "Iteration 2237, loss = 0.11771325\n",
      "Iteration 526, loss = 0.32388860\n",
      "Iteration 2238, loss = 0.11765065\n",
      "Iteration 1782, loss = 0.17012841\n",
      "Iteration 2239, loss = 0.11761491\n",
      "Iteration 2037, loss = 0.18787111\n",
      "Iteration 672, loss = 0.31727001\n",
      "Iteration 240, loss = 0.41358298\n",
      "Iteration 2240, loss = 0.11751579\n",
      "Iteration 2241, loss = 0.11729094\n",
      "Iteration 527, loss = 0.32368397\n",
      "Iteration 898, loss = 0.31760906\n",
      "Iteration 2242, loss = 0.11719287\n",
      "Iteration 673, loss = 0.31721802\n",
      "Iteration 2038, loss = 0.18776701\n",
      "Iteration 2243, loss = 0.11714427\n",
      "Iteration 1783, loss = 0.17019109\n",
      "Iteration 241, loss = 0.41326604\n",
      "Iteration 2244, loss = 0.11704618\n",
      "Iteration 528, loss = 0.32359833\n",
      "Iteration 2245, loss = 0.11691570\n",
      "Iteration 2246, loss = 0.11683658\n",
      "Iteration 674, loss = 0.31702128\n",
      "Iteration 529, loss = 0.32343067\n",
      "Iteration 2039, loss = 0.18767885\n",
      "Iteration 2247, loss = 0.11680972\n",
      "Iteration 242, loss = 0.41292175\n",
      "Iteration 2248, loss = 0.11672973\n",
      "Iteration 899, loss = 0.31744134\n",
      "Iteration 2249, loss = 0.11678773\n",
      "Iteration 2250, loss = 0.11643091\n",
      "Iteration 2040, loss = 0.18753320\n",
      "Iteration 1784, loss = 0.16992912\n",
      "Iteration 2251, loss = 0.11631822\n",
      "Iteration 675, loss = 0.31694751\n",
      "Iteration 530, loss = 0.32333723\n",
      "Iteration 2252, loss = 0.11622600\n",
      "Iteration 243, loss = 0.41261027\n",
      "Iteration 2253, loss = 0.11610647\n",
      "Iteration 1785, loss = 0.16976382\n",
      "Iteration 2041, loss = 0.18746889\n",
      "Iteration 531, loss = 0.32317150\n",
      "Iteration 2254, loss = 0.11607643\n",
      "Iteration 676, loss = 0.31683543\n",
      "Iteration 900, loss = 0.31717351\n",
      "Iteration 2255, loss = 0.11601566\n",
      "Iteration 2042, loss = 0.18734954\n",
      "Iteration 244, loss = 0.41230322\n",
      "Iteration 532, loss = 0.32307168\n",
      "Iteration 2256, loss = 0.11602353\n",
      "Iteration 1786, loss = 0.16965566\n",
      "Iteration 2257, loss = 0.11578182\n",
      "Iteration 677, loss = 0.31679227\n",
      "Iteration 2043, loss = 0.18724128\n",
      "Iteration 2258, loss = 0.11570688\n",
      "Iteration 533, loss = 0.32294638\n",
      "Iteration 2259, loss = 0.11555860\n",
      "Iteration 2260, loss = 0.11542417\n",
      "Iteration 901, loss = 0.31698555\n",
      "Iteration 678, loss = 0.31663206\n",
      "Iteration 2261, loss = 0.11549124\n",
      "Iteration 245, loss = 0.41197929\n",
      "Iteration 2262, loss = 0.11528405\n",
      "Iteration 534, loss = 0.32276455\n",
      "Iteration 1787, loss = 0.16967548\n",
      "Iteration 2263, loss = 0.11545013\n",
      "Iteration 2044, loss = 0.18715614\n",
      "Iteration 2264, loss = 0.11513172\n",
      "Iteration 902, loss = 0.31674212\n",
      "Iteration 679, loss = 0.31654572\n",
      "Iteration 535, loss = 0.32265582\n",
      "Iteration 2265, loss = 0.11499835\n",
      "Iteration 2266, loss = 0.11484867\n",
      "Iteration 1788, loss = 0.16946853\n",
      "Iteration 246, loss = 0.41167828\n",
      "Iteration 2267, loss = 0.11476900\n",
      "Iteration 2045, loss = 0.18713399\n",
      "Iteration 680, loss = 0.31646876\n",
      "Iteration 536, loss = 0.32251521\n",
      "Iteration 2268, loss = 0.11468198\n",
      "Iteration 903, loss = 0.31655798\n",
      "Iteration 2269, loss = 0.11456812\n",
      "Iteration 2270, loss = 0.11450780\n",
      "Iteration 2271, loss = 0.11438412\n",
      "Iteration 1789, loss = 0.16948483\n",
      "Iteration 2046, loss = 0.18697402\n",
      "Iteration 537, loss = 0.32236383\n",
      "Iteration 2272, loss = 0.11439841\n",
      "Iteration 681, loss = 0.31633944\n",
      "Iteration 2273, loss = 0.11431239\n",
      "Iteration 2274, loss = 0.11411458\n",
      "Iteration 247, loss = 0.41138164\n",
      "Iteration 538, loss = 0.32224008\n",
      "Iteration 2275, loss = 0.11413557\n",
      "Iteration 904, loss = 0.31649533\n",
      "Iteration 539, loss = 0.32210349\n",
      "Iteration 2276, loss = 0.11390760\n",
      "Iteration 2047, loss = 0.18683983\n",
      "Iteration 2277, loss = 0.11377787\n",
      "Iteration 682, loss = 0.31622493\n",
      "Iteration 1790, loss = 0.16930587\n",
      "Iteration 2278, loss = 0.11378356\n",
      "Iteration 248, loss = 0.41101840\n",
      "Iteration 2279, loss = 0.11358668\n",
      "Iteration 540, loss = 0.32198698\n",
      "Iteration 905, loss = 0.31649883\n",
      "Iteration 2280, loss = 0.11350372\n",
      "Iteration 2048, loss = 0.18671100\n",
      "Iteration 541, loss = 0.32192416\n",
      "Iteration 2281, loss = 0.11343805\n",
      "Iteration 2282, loss = 0.11331858\n",
      "Iteration 542, loss = 0.32170936\n",
      "Iteration 683, loss = 0.31616593\n",
      "Iteration 1791, loss = 0.16919859\n",
      "Iteration 2283, loss = 0.11327742\n",
      "Iteration 249, loss = 0.41074775\n",
      "Iteration 2284, loss = 0.11313284\n",
      "Iteration 2049, loss = 0.18662696\n",
      "Iteration 543, loss = 0.32171537\n",
      "Iteration 684, loss = 0.31603782\n",
      "Iteration 544, loss = 0.32146463\n",
      "Iteration 906, loss = 0.31601536\n",
      "Iteration 2285, loss = 0.11310814\n",
      "Iteration 1792, loss = 0.16909733\n",
      "Iteration 2286, loss = 0.11299049\n",
      "Iteration 545, loss = 0.32136808\n",
      "Iteration 685, loss = 0.31598945\n",
      "Iteration 250, loss = 0.41047548\n",
      "Iteration 2287, loss = 0.11286217\n",
      "Iteration 2050, loss = 0.18663559\n",
      "Iteration 907, loss = 0.31581730\n",
      "Iteration 2288, loss = 0.11308892\n",
      "Iteration 686, loss = 0.31584327\n",
      "Iteration 1793, loss = 0.16898800\n",
      "Iteration 2289, loss = 0.11268814\n",
      "Iteration 2051, loss = 0.18656078\n",
      "Iteration 546, loss = 0.32119325\n",
      "Iteration 687, loss = 0.31581377\n",
      "Iteration 2290, loss = 0.11270737\n",
      "Iteration 251, loss = 0.41014131\n",
      "Iteration 2052, loss = 0.18632167\n",
      "Iteration 2291, loss = 0.11253642\n",
      "Iteration 2292, loss = 0.11240567\n",
      "Iteration 1794, loss = 0.16900022\n",
      "Iteration 2293, loss = 0.11224937\n",
      "Iteration 908, loss = 0.31564453\n",
      "Iteration 547, loss = 0.32108098\n",
      "Iteration 2053, loss = 0.18629243\n",
      "Iteration 2294, loss = 0.11230343\n",
      "Iteration 252, loss = 0.40984456\n",
      "Iteration 688, loss = 0.31564455\n",
      "Iteration 2295, loss = 0.11232811\n",
      "Iteration 2296, loss = 0.11218855\n",
      "Iteration 1795, loss = 0.16878016\n",
      "Iteration 2297, loss = 0.11192839\n",
      "Iteration 2054, loss = 0.18610233\n",
      "Iteration 253, loss = 0.40956472\n",
      "Iteration 2298, loss = 0.11195985\n",
      "Iteration 548, loss = 0.32093803\n",
      "Iteration 2299, loss = 0.11175829\n",
      "Iteration 2300, loss = 0.11174607\n",
      "Iteration 2055, loss = 0.18597955\n",
      "Iteration 689, loss = 0.31555907\n",
      "Iteration 1796, loss = 0.16913711\n",
      "Iteration 2301, loss = 0.11172690\n",
      "Iteration 909, loss = 0.31542946\n",
      "Iteration 2302, loss = 0.11161681\n",
      "Iteration 549, loss = 0.32083542\n",
      "Iteration 2056, loss = 0.18592484\n",
      "Iteration 2303, loss = 0.11142714\n",
      "Iteration 1797, loss = 0.16858054\n",
      "Iteration 2304, loss = 0.11147959\n",
      "Iteration 690, loss = 0.31545133\n",
      "Iteration 550, loss = 0.32081664\n",
      "Iteration 254, loss = 0.40931372\n",
      "Iteration 2305, loss = 0.11125628\n",
      "Iteration 2306, loss = 0.11141388\n",
      "Iteration 2307, loss = 0.11110672\n",
      "Iteration 691, loss = 0.31535592\n",
      "Iteration 2308, loss = 0.11096750\n",
      "Iteration 2309, loss = 0.11090005\n",
      "Iteration 1798, loss = 0.16857125\n",
      "Iteration 551, loss = 0.32058974\n",
      "Iteration 2310, loss = 0.11076811\n",
      "Iteration 2057, loss = 0.18585266\n",
      "Iteration 255, loss = 0.40901587\n",
      "Iteration 910, loss = 0.31527173\n",
      "Iteration 692, loss = 0.31524558\n",
      "Iteration 2311, loss = 0.11082435\n",
      "Iteration 552, loss = 0.32045641\n",
      "Iteration 2312, loss = 0.11054245\n",
      "Iteration 693, loss = 0.31514451\n",
      "Iteration 2313, loss = 0.11044288\n",
      "Iteration 2314, loss = 0.11040639\n",
      "Iteration 553, loss = 0.32033928Iteration 911, loss = 0.31506964\n",
      "\n",
      "Iteration 2058, loss = 0.18571446\n",
      "Iteration 1799, loss = 0.16856689\n",
      "Iteration 2315, loss = 0.11033145\n",
      "Iteration 256, loss = 0.40868718\n",
      "Iteration 2316, loss = 0.11015763\n",
      "Iteration 2317, loss = 0.11028239\n",
      "Iteration 554, loss = 0.32018699\n",
      "Iteration 694, loss = 0.31508402\n",
      "Iteration 2059, loss = 0.18562110\n",
      "Iteration 912, loss = 0.31488785\n",
      "Iteration 2318, loss = 0.10998039\n",
      "Iteration 2319, loss = 0.10991002\n",
      "Iteration 1800, loss = 0.16837257\n",
      "Iteration 2060, loss = 0.18554935\n",
      "Iteration 555, loss = 0.32014152\n",
      "Iteration 2320, loss = 0.10977069\n",
      "Iteration 257, loss = 0.40852760\n",
      "Iteration 2321, loss = 0.10980153\n",
      "Iteration 2322, loss = 0.10963988\n",
      "Iteration 2323, loss = 0.10960618\n",
      "Iteration 695, loss = 0.31492501\n",
      "Iteration 2061, loss = 0.18544979\n",
      "Iteration 2324, loss = 0.10962814\n",
      "Iteration 556, loss = 0.31994710\n",
      "Iteration 258, loss = 0.40812551\n",
      "Iteration 913, loss = 0.31464096\n",
      "Iteration 2325, loss = 0.10944170\n",
      "Iteration 696, loss = 0.31483619\n",
      "Iteration 2062, loss = 0.18532653\n",
      "Iteration 557, loss = 0.31986147\n",
      "Iteration 2326, loss = 0.10937972\n",
      "Iteration 2327, loss = 0.10919751\n",
      "Iteration 1801, loss = 0.16860516\n",
      "Iteration 697, loss = 0.31473535\n",
      "Iteration 2328, loss = 0.10931689\n",
      "Iteration 558, loss = 0.31966308\n",
      "Iteration 2063, loss = 0.18530487\n",
      "Iteration 2329, loss = 0.10907631\n",
      "Iteration 2330, loss = 0.10890674\n",
      "Iteration 259, loss = 0.40785022\n",
      "Iteration 2331, loss = 0.10884367\n",
      "Iteration 698, loss = 0.31465759\n",
      "Iteration 2332, loss = 0.10876881\n",
      "Iteration 559, loss = 0.31958354\n",
      "Iteration 2333, loss = 0.10863973\n",
      "Iteration 1802, loss = 0.16818696\n",
      "Iteration 2064, loss = 0.18524581\n",
      "Iteration 914, loss = 0.31460855\n",
      "Iteration 560, loss = 0.31944715\n",
      "Iteration 260, loss = 0.40756804\n",
      "Iteration 2334, loss = 0.10860494\n",
      "Iteration 699, loss = 0.31456589\n",
      "Iteration 2065, loss = 0.18500359\n",
      "Iteration 561, loss = 0.31928088\n",
      "Iteration 2335, loss = 0.10851367\n",
      "Iteration 1803, loss = 0.16801640\n",
      "Iteration 700, loss = 0.31444349\n",
      "Iteration 915, loss = 0.31433340\n",
      "Iteration 261, loss = 0.40732344\n",
      "Iteration 2336, loss = 0.10838905\n",
      "Iteration 2337, loss = 0.10828151\n",
      "Iteration 562, loss = 0.31921810\n",
      "Iteration 701, loss = 0.31437205\n",
      "Iteration 2338, loss = 0.10818108\n",
      "Iteration 2066, loss = 0.18490572\n",
      "Iteration 262, loss = 0.40702786\n",
      "Iteration 2339, loss = 0.10818421\n",
      "Iteration 1804, loss = 0.16800424\n",
      "Iteration 563, loss = 0.31906228\n",
      "Iteration 2340, loss = 0.10801533\n",
      "Iteration 2341, loss = 0.10814952\n",
      "Iteration 916, loss = 0.31408133\n",
      "Iteration 702, loss = 0.31425256\n",
      "Iteration 564, loss = 0.31896412\n",
      "Iteration 2342, loss = 0.10783784\n",
      "Iteration 2067, loss = 0.18483421\n",
      "Iteration 2343, loss = 0.10773364\n",
      "Iteration 2344, loss = 0.10773923\n",
      "Iteration 263, loss = 0.40674783\n",
      "Iteration 565, loss = 0.31881460\n",
      "Iteration 1805, loss = 0.16786112\n",
      "Iteration 2345, loss = 0.10760194\n",
      "Iteration 2346, loss = 0.10746801\n",
      "Iteration 917, loss = 0.31393273\n",
      "Iteration 2068, loss = 0.18469957\n",
      "Iteration 703, loss = 0.31415496\n",
      "Iteration 2347, loss = 0.10743895\n",
      "Iteration 264, loss = 0.40645901\n",
      "Iteration 566, loss = 0.31869978\n",
      "Iteration 2069, loss = 0.18466869\n",
      "Iteration 2348, loss = 0.10729385\n",
      "Iteration 2349, loss = 0.10731240\n",
      "Iteration 918, loss = 0.31376893\n",
      "Iteration 2350, loss = 0.10710176\n",
      "Iteration 1806, loss = 0.16773784\n",
      "Iteration 567, loss = 0.31857578\n",
      "Iteration 2351, loss = 0.10700566\n",
      "Iteration 704, loss = 0.31404199\n",
      "Iteration 2352, loss = 0.10697365\n",
      "Iteration 2353, loss = 0.10691550\n",
      "Iteration 2070, loss = 0.18457158\n",
      "Iteration 568, loss = 0.31845122\n",
      "Iteration 2354, loss = 0.10683732\n",
      "Iteration 705, loss = 0.31401537\n",
      "Iteration 919, loss = 0.31365349\n",
      "Iteration 265, loss = 0.40621783\n",
      "Iteration 2071, loss = 0.18441649\n",
      "Iteration 1807, loss = 0.16782813\n",
      "Iteration 2355, loss = 0.10670853\n",
      "Iteration 569, loss = 0.31830883\n",
      "Iteration 706, loss = 0.31385803\n",
      "Iteration 2072, loss = 0.18428825\n",
      "Iteration 266, loss = 0.40593097\n",
      "Iteration 2356, loss = 0.10674614\n",
      "Iteration 570, loss = 0.31817532\n",
      "Iteration 2357, loss = 0.10651104\n",
      "Iteration 920, loss = 0.31334220\n",
      "Iteration 2358, loss = 0.10660517\n",
      "Iteration 1808, loss = 0.16754636\n",
      "Iteration 2073, loss = 0.18417784\n",
      "Iteration 2359, loss = 0.10631013\n",
      "Iteration 707, loss = 0.31375594\n",
      "Iteration 267, loss = 0.40563167\n",
      "Iteration 2360, loss = 0.10624055\n",
      "Iteration 2361, loss = 0.10619426\n",
      "Iteration 571, loss = 0.31807809\n",
      "Iteration 2362, loss = 0.10611096\n",
      "Iteration 921, loss = 0.31324121\n",
      "Iteration 708, loss = 0.31363996\n",
      "Iteration 1809, loss = 0.16746973\n",
      "Iteration 572, loss = 0.31795439\n",
      "Iteration 2074, loss = 0.18419188\n",
      "Iteration 2363, loss = 0.10605601\n",
      "Iteration 2364, loss = 0.10594451\n",
      "Iteration 709, loss = 0.31357373\n",
      "Iteration 2365, loss = 0.10591349\n",
      "Iteration 2366, loss = 0.10585628\n",
      "Iteration 573, loss = 0.31780136\n",
      "Iteration 922, loss = 0.31296487\n",
      "Iteration 2075, loss = 0.18403753\n",
      "Iteration 1810, loss = 0.16738749\n",
      "Iteration 268, loss = 0.40537963\n",
      "Iteration 710, loss = 0.31343653\n",
      "Iteration 2367, loss = 0.10578550\n",
      "Iteration 574, loss = 0.31765977\n",
      "Iteration 2368, loss = 0.10583808\n",
      "Iteration 2369, loss = 0.10552116\n",
      "Iteration 2370, loss = 0.10552402\n",
      "Iteration 2371, loss = 0.10546494\n",
      "Iteration 269, loss = 0.40512833\n",
      "Iteration 575, loss = 0.31754767\n",
      "Iteration 2076, loss = 0.18391545\n",
      "Iteration 711, loss = 0.31339731\n",
      "Iteration 2372, loss = 0.10526882\n",
      "Iteration 923, loss = 0.31273968\n",
      "Iteration 576, loss = 0.31741588\n",
      "Iteration 1811, loss = 0.16748298\n",
      "Iteration 2373, loss = 0.10539703\n",
      "Iteration 2374, loss = 0.10516250\n",
      "Iteration 270, loss = 0.40485469\n",
      "Iteration 2375, loss = 0.10509887\n",
      "Iteration 712, loss = 0.31326755\n",
      "Iteration 2376, loss = 0.10486688\n",
      "Iteration 577, loss = 0.31739587\n",
      "Iteration 2377, loss = 0.10491966\n",
      "Iteration 2378, loss = 0.10480907\n",
      "Iteration 2077, loss = 0.18379410\n",
      "Iteration 924, loss = 0.31254541\n",
      "Iteration 2379, loss = 0.10462818\n",
      "Iteration 271, loss = 0.40457593\n",
      "Iteration 2380, loss = 0.10485705\n",
      "Iteration 578, loss = 0.31716837\n",
      "Iteration 713, loss = 0.31319395\n",
      "Iteration 2078, loss = 0.18369792\n",
      "Iteration 1812, loss = 0.16718998\n",
      "Iteration 2381, loss = 0.10448344\n",
      "Iteration 579, loss = 0.31706544\n",
      "Iteration 925, loss = 0.31236833\n",
      "Iteration 714, loss = 0.31305387\n",
      "Iteration 2382, loss = 0.10440922\n",
      "Iteration 272, loss = 0.40430300\n",
      "Iteration 2383, loss = 0.10436269\n",
      "Iteration 2384, loss = 0.10425580\n",
      "Iteration 2079, loss = 0.18360649\n",
      "Iteration 715, loss = 0.31298580\n",
      "Iteration 580, loss = 0.31692507\n",
      "Iteration 2385, loss = 0.10419091\n",
      "Iteration 926, loss = 0.31222483\n",
      "Iteration 2386, loss = 0.10412194\n",
      "Iteration 2387, loss = 0.10404888\n",
      "Iteration 581, loss = 0.31680725\n",
      "Iteration 716, loss = 0.31287208\n",
      "Iteration 2388, loss = 0.10416235\n",
      "Iteration 273, loss = 0.40405775\n",
      "Iteration 1813, loss = 0.16724876\n",
      "Iteration 2389, loss = 0.10385322\n",
      "Iteration 2390, loss = 0.10387239\n",
      "Iteration 582, loss = 0.31666711\n",
      "Iteration 2080, loss = 0.18352384\n",
      "Iteration 2391, loss = 0.10364377\n",
      "Iteration 717, loss = 0.31281823\n",
      "Iteration 274, loss = 0.40377524\n",
      "Iteration 2392, loss = 0.10356738\n",
      "Iteration 927, loss = 0.31197749\n",
      "Iteration 2393, loss = 0.10349692\n",
      "Iteration 1814, loss = 0.16700222\n",
      "Iteration 2394, loss = 0.10355071\n",
      "Iteration 583, loss = 0.31653842\n",
      "Iteration 2081, loss = 0.18342440\n",
      "Iteration 2395, loss = 0.10345016\n",
      "Iteration 718, loss = 0.31268026\n",
      "Iteration 2396, loss = 0.10325930\n",
      "Iteration 2397, loss = 0.10325321\n",
      "Iteration 584, loss = 0.31639033\n",
      "Iteration 2082, loss = 0.18337091\n",
      "Iteration 2398, loss = 0.10328584\n",
      "Iteration 275, loss = 0.40357013\n",
      "Iteration 2399, loss = 0.10298344\n",
      "Iteration 928, loss = 0.31189140\n",
      "Iteration 585, loss = 0.31626986\n",
      "Iteration 2400, loss = 0.10289739\n",
      "Iteration 719, loss = 0.31256334\n",
      "Iteration 1815, loss = 0.16693989\n",
      "Iteration 2083, loss = 0.18326627\n",
      "Iteration 2401, loss = 0.10285128\n",
      "Iteration 586, loss = 0.31611424\n",
      "Iteration 2402, loss = 0.10280452\n",
      "Iteration 2403, loss = 0.10293491\n",
      "Iteration 2404, loss = 0.10275011\n",
      "Iteration 587, loss = 0.31599799\n",
      "Iteration 2405, loss = 0.10253388\n",
      "Iteration 929, loss = 0.31164320\n",
      "Iteration 2406, loss = 0.10242474\n",
      "Iteration 276, loss = 0.40328363\n",
      "Iteration 588, loss = 0.31589637\n",
      "Iteration 720, loss = 0.31246195\n",
      "Iteration 589, loss = 0.31572366\n",
      "Iteration 2084, loss = 0.18315377\n",
      "Iteration 2407, loss = 0.10231829\n",
      "Iteration 1816, loss = 0.16683349\n",
      "Iteration 277, loss = 0.40300791\n",
      "Iteration 930, loss = 0.31141610\n",
      "Iteration 590, loss = 0.31560895\n",
      "Iteration 2408, loss = 0.10228127\n",
      "Iteration 2085, loss = 0.18325327\n",
      "Iteration 721, loss = 0.31237448\n",
      "Iteration 2409, loss = 0.10217055\n",
      "Iteration 591, loss = 0.31547110\n",
      "Iteration 2410, loss = 0.10209775\n",
      "Iteration 2411, loss = 0.10211568\n",
      "Iteration 2412, loss = 0.10194512\n",
      "Iteration 722, loss = 0.31226274\n",
      "Iteration 2413, loss = 0.10190390\n",
      "Iteration 1817, loss = 0.16680378\n",
      "Iteration 2414, loss = 0.10185364\n",
      "Iteration 2086, loss = 0.18298903\n",
      "Iteration 592, loss = 0.31534012\n",
      "Iteration 2415, loss = 0.10161789\n",
      "Iteration 2416, loss = 0.10162220\n",
      "Iteration 931, loss = 0.31119715\n",
      "Iteration 2417, loss = 0.10155730\n",
      "Iteration 2418, loss = 0.10158077\n",
      "Iteration 2419, loss = 0.10144429\n",
      "Iteration 278, loss = 0.40275616\n",
      "Iteration 2087, loss = 0.18282958\n",
      "Iteration 1818, loss = 0.16666444\n",
      "Iteration 593, loss = 0.31521287\n",
      "Iteration 723, loss = 0.31217235\n",
      "Iteration 2420, loss = 0.10129017\n",
      "Iteration 2421, loss = 0.10128584\n",
      "Iteration 724, loss = 0.31215457\n",
      "Iteration 2422, loss = 0.10112991\n",
      "Iteration 2088, loss = 0.18275706\n",
      "Iteration 932, loss = 0.31100523\n",
      "Iteration 2423, loss = 0.10105136\n",
      "Iteration 2424, loss = 0.10104395\n",
      "Iteration 594, loss = 0.31510617\n",
      "Iteration 725, loss = 0.31197098\n",
      "Iteration 1819, loss = 0.16661660\n",
      "Iteration 2425, loss = 0.10116095\n",
      "Iteration 726, loss = 0.31188639\n",
      "Iteration 2426, loss = 0.10083377\n",
      "Iteration 2427, loss = 0.10070822\n",
      "Iteration 595, loss = 0.31500961\n",
      "Iteration 2089, loss = 0.18263950\n",
      "Iteration 279, loss = 0.40253407\n",
      "Iteration 933, loss = 0.31089897\n",
      "Iteration 2428, loss = 0.10061796\n",
      "Iteration 1820, loss = 0.16648066\n",
      "Iteration 2429, loss = 0.10061423\n",
      "Iteration 596, loss = 0.31486762\n",
      "Iteration 2430, loss = 0.10048117\n",
      "Iteration 597, loss = 0.31469837\n",
      "Iteration 2431, loss = 0.10037706\n",
      "Iteration 934, loss = 0.31067024\n",
      "Iteration 280, loss = 0.40222980\n",
      "Iteration 2432, loss = 0.10035273\n",
      "Iteration 2090, loss = 0.18262431\n",
      "Iteration 1821, loss = 0.16634622\n",
      "Iteration 727, loss = 0.31178256\n",
      "Iteration 2433, loss = 0.10024573\n",
      "Iteration 598, loss = 0.31458008\n",
      "Iteration 2434, loss = 0.10017138\n",
      "Iteration 2435, loss = 0.10017326\n",
      "Iteration 281, loss = 0.40202754\n",
      "Iteration 599, loss = 0.31460127\n",
      "Iteration 2091, loss = 0.18242710\n",
      "Iteration 935, loss = 0.31047436\n",
      "Iteration 2436, loss = 0.10007244\n",
      "Iteration 1822, loss = 0.16639087\n",
      "Iteration 728, loss = 0.31167164\n",
      "Iteration 2437, loss = 0.10013223\n",
      "Iteration 2092, loss = 0.18242952\n",
      "Iteration 2438, loss = 0.09983792\n",
      "Iteration 282, loss = 0.40171984\n",
      "Iteration 936, loss = 0.31023306\n",
      "Iteration 2439, loss = 0.09994217\n",
      "Iteration 729, loss = 0.31163532\n",
      "Iteration 600, loss = 0.31431178\n",
      "Iteration 2440, loss = 0.09982200\n",
      "Iteration 2441, loss = 0.09967634\n",
      "Iteration 2442, loss = 0.09954273\n",
      "Iteration 1823, loss = 0.16620296\n",
      "Iteration 2443, loss = 0.09947957\n",
      "Iteration 730, loss = 0.31153072\n",
      "Iteration 2093, loss = 0.18225193\n",
      "Iteration 601, loss = 0.31431789\n",
      "Iteration 2444, loss = 0.09961550\n",
      "Iteration 937, loss = 0.31003980\n",
      "Iteration 602, loss = 0.31407001\n",
      "Iteration 283, loss = 0.40152393\n",
      "Iteration 2445, loss = 0.09941219\n",
      "Iteration 603, loss = 0.31392948\n",
      "Iteration 2446, loss = 0.09924804\n",
      "Iteration 731, loss = 0.31140004\n",
      "Iteration 604, loss = 0.31380273\n",
      "Iteration 2094, loss = 0.18212590\n",
      "Iteration 1824, loss = 0.16611297\n",
      "Iteration 2447, loss = 0.09917619\n",
      "Iteration 284, loss = 0.40125051\n",
      "Iteration 938, loss = 0.30979386\n",
      "Iteration 2448, loss = 0.09910936\n",
      "Iteration 605, loss = 0.31370090\n",
      "Iteration 2449, loss = 0.09919702\n",
      "Iteration 732, loss = 0.31131517\n",
      "Iteration 2450, loss = 0.09897377\n",
      "Iteration 606, loss = 0.31358080\n",
      "Iteration 2095, loss = 0.18212946\n",
      "Iteration 2451, loss = 0.09890487\n",
      "Iteration 285, loss = 0.40098919\n",
      "Iteration 2452, loss = 0.09874725\n",
      "Iteration 1825, loss = 0.16600079\n",
      "Iteration 2453, loss = 0.09872640\n",
      "Iteration 939, loss = 0.30964778\n",
      "Iteration 2454, loss = 0.09865081\n",
      "Iteration 607, loss = 0.31341837\n",
      "Iteration 2455, loss = 0.09858949\n",
      "Iteration 2096, loss = 0.18192549\n",
      "Iteration 2456, loss = 0.09851446\n",
      "Iteration 286, loss = 0.40074964Iteration 608, loss = 0.31331404\n",
      "Iteration 733, loss = 0.31118352\n",
      "\n",
      "Iteration 1826, loss = 0.16596716\n",
      "Iteration 2457, loss = 0.09855581\n",
      "Iteration 2458, loss = 0.09830309\n",
      "Iteration 609, loss = 0.31324277\n",
      "Iteration 2097, loss = 0.18191444\n",
      "Iteration 610, loss = 0.31304359\n",
      "Iteration 734, loss = 0.31111960\n",
      "Iteration 940, loss = 0.30954516\n",
      "Iteration 2098, loss = 0.18170765\n",
      "Iteration 611, loss = 0.31290721\n",
      "Iteration 2459, loss = 0.09828594\n",
      "Iteration 2460, loss = 0.09817896\n",
      "Iteration 735, loss = 0.31104616\n",
      "Iteration 612, loss = 0.31279253\n",
      "Iteration 2461, loss = 0.09808234\n",
      "Iteration 287, loss = 0.40049873\n",
      "Iteration 1827, loss = 0.16598138\n",
      "Iteration 613, loss = 0.31266816\n",
      "Iteration 2462, loss = 0.09802998\n",
      "Iteration 2463, loss = 0.09794967\n",
      "Iteration 2099, loss = 0.18162575\n",
      "Iteration 736, loss = 0.31090650\n",
      "Iteration 288, loss = 0.40027582\n",
      "Iteration 1828, loss = 0.16573804\n",
      "Iteration 2464, loss = 0.09785713\n",
      "Iteration 614, loss = 0.31254082\n",
      "Iteration 2100, loss = 0.18152239\n",
      "Iteration 941, loss = 0.30928520\n",
      "Iteration 737, loss = 0.31081978\n",
      "Iteration 289, loss = 0.40006128\n",
      "Iteration 615, loss = 0.31242571\n",
      "Iteration 2101, loss = 0.18155189\n",
      "Iteration 1829, loss = 0.16571663\n",
      "Iteration 616, loss = 0.31236385\n",
      "Iteration 2102, loss = 0.18137250\n",
      "Iteration 2465, loss = 0.09771634\n",
      "Iteration 738, loss = 0.31073834\n",
      "Iteration 942, loss = 0.30907920\n",
      "Iteration 2466, loss = 0.09773830\n",
      "Iteration 2103, loss = 0.18122745\n",
      "Iteration 1830, loss = 0.16573930\n",
      "Iteration 2467, loss = 0.09765943\n",
      "Iteration 290, loss = 0.39976019\n",
      "Iteration 2468, loss = 0.09788165\n",
      "Iteration 617, loss = 0.31215262\n",
      "Iteration 739, loss = 0.31063627\n",
      "Iteration 618, loss = 0.31205683\n",
      "Iteration 2104, loss = 0.18114068\n",
      "Iteration 619, loss = 0.31196158\n",
      "Iteration 943, loss = 0.30881588\n",
      "Iteration 1831, loss = 0.16563452\n",
      "Iteration 740, loss = 0.31050288\n",
      "Iteration 2469, loss = 0.09748608\n",
      "Iteration 620, loss = 0.31194186\n",
      "Iteration 2470, loss = 0.09738727\n",
      "Iteration 944, loss = 0.30863503\n",
      "Iteration 621, loss = 0.31164073\n",
      "Iteration 2471, loss = 0.09736055\n",
      "Iteration 2105, loss = 0.18105759\n",
      "Iteration 741, loss = 0.31043550\n",
      "Iteration 291, loss = 0.39958374\n",
      "Iteration 622, loss = 0.31158466\n",
      "Iteration 1832, loss = 0.16542597\n",
      "Iteration 2472, loss = 0.09723889\n",
      "Iteration 623, loss = 0.31140149\n",
      "Iteration 2473, loss = 0.09723839\n",
      "Iteration 2474, loss = 0.09713846\n",
      "Iteration 2106, loss = 0.18091972\n",
      "Iteration 945, loss = 0.30845664\n",
      "Iteration 2475, loss = 0.09709071\n",
      "Iteration 742, loss = 0.31031273\n",
      "Iteration 292, loss = 0.39929184\n",
      "Iteration 2476, loss = 0.09715811\n",
      "Iteration 624, loss = 0.31126538\n",
      "Iteration 2477, loss = 0.09696008\n",
      "Iteration 2107, loss = 0.18089555\n",
      "Iteration 2478, loss = 0.09680165\n",
      "Iteration 946, loss = 0.30832626\n",
      "Iteration 1833, loss = 0.16535798\n",
      "Iteration 2479, loss = 0.09671274\n",
      "Iteration 625, loss = 0.31116741\n",
      "Iteration 2480, loss = 0.09665950\n",
      "Iteration 2108, loss = 0.18074169\n",
      "Iteration 293, loss = 0.39905679\n",
      "Iteration 2481, loss = 0.09672073\n",
      "Iteration 743, loss = 0.31030982\n",
      "Iteration 2482, loss = 0.09666799\n",
      "Iteration 1834, loss = 0.16523893\n",
      "Iteration 947, loss = 0.30815124\n",
      "Iteration 2109, loss = 0.18063930\n",
      "Iteration 626, loss = 0.31105151\n",
      "Iteration 2483, loss = 0.09661643\n",
      "Iteration 2484, loss = 0.09642442\n",
      "Iteration 2485, loss = 0.09625856\n",
      "Iteration 1835, loss = 0.16515684\n",
      "Iteration 744, loss = 0.31012679\n",
      "Iteration 2486, loss = 0.09620198\n",
      "Iteration 294, loss = 0.39881936\n",
      "Iteration 2110, loss = 0.18059048\n",
      "Iteration 948, loss = 0.30790483\n",
      "Iteration 2487, loss = 0.09624101\n",
      "Iteration 627, loss = 0.31087783\n",
      "Iteration 2488, loss = 0.09621380\n",
      "Iteration 745, loss = 0.31005163\n",
      "Iteration 2489, loss = 0.09610124\n",
      "Iteration 1836, loss = 0.16529853\n",
      "Iteration 2490, loss = 0.09597149\n",
      "Iteration 949, loss = 0.30770682\n",
      "Iteration 2111, loss = 0.18046794\n",
      "Iteration 628, loss = 0.31078924\n",
      "Iteration 2491, loss = 0.09593228\n",
      "Iteration 295, loss = 0.39860406\n",
      "Iteration 746, loss = 0.30994766\n",
      "Iteration 2492, loss = 0.09573931\n",
      "Iteration 2493, loss = 0.09596370\n",
      "Iteration 2494, loss = 0.09570732\n",
      "Iteration 629, loss = 0.31067872\n",
      "Iteration 1837, loss = 0.16501659\n",
      "Iteration 950, loss = 0.30752817\n",
      "Iteration 2112, loss = 0.18059655\n",
      "Iteration 747, loss = 0.30983498\n",
      "Iteration 2495, loss = 0.09559669\n",
      "Iteration 296, loss = 0.39834620\n",
      "Iteration 2496, loss = 0.09546982\n",
      "Iteration 2497, loss = 0.09538933\n",
      "Iteration 630, loss = 0.31052395\n",
      "Iteration 748, loss = 0.30975393\n",
      "Iteration 951, loss = 0.30728558\n",
      "Iteration 2498, loss = 0.09545134\n",
      "Iteration 1838, loss = 0.16485596\n",
      "Iteration 2499, loss = 0.09526779\n",
      "Iteration 2113, loss = 0.18027790\n",
      "Iteration 297, loss = 0.39816241\n",
      "Iteration 631, loss = 0.31047908\n",
      "Iteration 749, loss = 0.30970707\n",
      "Iteration 2500, loss = 0.09529655\n",
      "Iteration 2501, loss = 0.09519866\n",
      "Iteration 952, loss = 0.30711837\n",
      "Iteration 2502, loss = 0.09501549\n",
      "Iteration 632, loss = 0.31026340\n",
      "Iteration 2503, loss = 0.09501272\n",
      "Iteration 2504, loss = 0.09493715\n",
      "Iteration 1839, loss = 0.16481673\n",
      "Iteration 2114, loss = 0.18022211\n",
      "Iteration 2505, loss = 0.09483977\n",
      "Iteration 750, loss = 0.30954447\n",
      "Iteration 298, loss = 0.39790633\n",
      "Iteration 953, loss = 0.30690466\n",
      "Iteration 2506, loss = 0.09483653\n",
      "Iteration 2507, loss = 0.09470497\n",
      "Iteration 633, loss = 0.31015962\n",
      "Iteration 751, loss = 0.30945008\n",
      "Iteration 2115, loss = 0.18009650\n",
      "Iteration 634, loss = 0.30999527\n",
      "Iteration 299, loss = 0.39767778\n",
      "Iteration 954, loss = 0.30672131\n",
      "Iteration 2116, loss = 0.17998972\n",
      "Iteration 1840, loss = 0.16472699\n",
      "Iteration 635, loss = 0.30993284\n",
      "Iteration 300, loss = 0.39745301\n",
      "Iteration 2508, loss = 0.09464279\n",
      "Iteration 2117, loss = 0.17989752\n",
      "Iteration 636, loss = 0.30983221\n",
      "Iteration 2509, loss = 0.09461262\n",
      "Iteration 1841, loss = 0.16461063\n",
      "Iteration 2510, loss = 0.09456858\n",
      "Iteration 955, loss = 0.30649686\n",
      "Iteration 752, loss = 0.30938064\n",
      "Iteration 2511, loss = 0.09442294\n",
      "Iteration 1842, loss = 0.16456440\n",
      "Iteration 2512, loss = 0.09443087\n",
      "Iteration 956, loss = 0.30633980\n",
      "Iteration 753, loss = 0.30927626\n",
      "Iteration 2513, loss = 0.09429123\n",
      "Iteration 2514, loss = 0.09423774\n",
      "Iteration 637, loss = 0.30962810\n",
      "Iteration 2515, loss = 0.09416933\n",
      "Iteration 2516, loss = 0.09406620\n",
      "Iteration 957, loss = 0.30617282\n",
      "Iteration 638, loss = 0.30950073\n",
      "Iteration 754, loss = 0.30916292\n",
      "Iteration 2118, loss = 0.17980094\n",
      "Iteration 2517, loss = 0.09399016\n",
      "Iteration 639, loss = 0.30934910\n",
      "Iteration 301, loss = 0.39726466\n",
      "Iteration 2518, loss = 0.09398667\n",
      "Iteration 640, loss = 0.30931377\n",
      "Iteration 1843, loss = 0.16448694\n",
      "Iteration 2519, loss = 0.09393953\n",
      "Iteration 958, loss = 0.30588070\n",
      "Iteration 641, loss = 0.30918423\n",
      "Iteration 2119, loss = 0.17970252\n",
      "Iteration 2520, loss = 0.09381472\n",
      "Iteration 2521, loss = 0.09382981Iteration 1844, loss = 0.16435324\n",
      "Iteration 642, loss = 0.30896936\n",
      "\n",
      "Iteration 2522, loss = 0.09371861\n",
      "Iteration 643, loss = 0.30891322\n",
      "Iteration 302, loss = 0.39697352\n",
      "Iteration 2523, loss = 0.09361434\n",
      "Iteration 2120, loss = 0.17971703\n",
      "Iteration 644, loss = 0.30876278\n",
      "Iteration 2524, loss = 0.09355019\n",
      "Iteration 959, loss = 0.30572970\n",
      "Iteration 1845, loss = 0.16441716\n",
      "Iteration 2525, loss = 0.09347058\n",
      "Iteration 2526, loss = 0.09354829\n",
      "Iteration 2121, loss = 0.17958138\n",
      "Iteration 2527, loss = 0.09340598\n",
      "Iteration 645, loss = 0.30860228\n",
      "Iteration 2528, loss = 0.09321388\n",
      "Iteration 2529, loss = 0.09320858\n",
      "Iteration 646, loss = 0.30845272\n",
      "Iteration 303, loss = 0.39674871\n",
      "Iteration 2122, loss = 0.17941418\n",
      "Iteration 960, loss = 0.30553513\n",
      "Iteration 2530, loss = 0.09314638\n",
      "Iteration 1846, loss = 0.16417474\n",
      "Iteration 2531, loss = 0.09305724\n",
      "Iteration 647, loss = 0.30834364\n",
      "Iteration 2123, loss = 0.17931217\n",
      "Iteration 2532, loss = 0.09294095\n",
      "Iteration 961, loss = 0.30535007\n",
      "Iteration 2533, loss = 0.09292612\n",
      "Iteration 304, loss = 0.39656879\n",
      "Iteration 2534, loss = 0.09285617\n",
      "Iteration 648, loss = 0.30841803\n",
      "Iteration 2535, loss = 0.09281187\n",
      "Iteration 2124, loss = 0.17927883\n",
      "Iteration 1847, loss = 0.16422472\n",
      "Iteration 2536, loss = 0.09271820\n",
      "Iteration 2125, loss = 0.17911570\n",
      "Iteration 305, loss = 0.39646821\n",
      "Iteration 2537, loss = 0.09265627\n",
      "Iteration 962, loss = 0.30519582\n",
      "Iteration 649, loss = 0.30811097\n",
      "Iteration 2538, loss = 0.09258094\n",
      "Iteration 1848, loss = 0.16410629\n",
      "Iteration 650, loss = 0.30798223\n",
      "Iteration 2539, loss = 0.09261682\n",
      "Iteration 2126, loss = 0.17903225\n",
      "Iteration 2540, loss = 0.09252371\n",
      "Iteration 651, loss = 0.30786798\n",
      "Iteration 2541, loss = 0.09251462\n",
      "Iteration 306, loss = 0.39609808\n",
      "Iteration 963, loss = 0.30491711\n",
      "Iteration 2542, loss = 0.09236998\n",
      "Iteration 652, loss = 0.30779976\n",
      "Iteration 2543, loss = 0.09240850\n",
      "Iteration 2127, loss = 0.17894492\n",
      "Iteration 2544, loss = 0.09219549\n",
      "Iteration 755, loss = 0.30905872\n",
      "Iteration 1849, loss = 0.16402678\n",
      "Iteration 653, loss = 0.30759167\n",
      "Iteration 2545, loss = 0.09216105\n",
      "Iteration 2546, loss = 0.09228250\n",
      "Iteration 307, loss = 0.39583644\n",
      "Iteration 964, loss = 0.30470780\n",
      "Iteration 2547, loss = 0.09202245\n",
      "Iteration 2128, loss = 0.17883049\n",
      "Iteration 2548, loss = 0.09195005\n",
      "Iteration 654, loss = 0.30757649\n",
      "Iteration 756, loss = 0.30899351\n",
      "Iteration 2549, loss = 0.09190287\n",
      "Iteration 2129, loss = 0.17879726\n",
      "Iteration 1850, loss = 0.16393873\n",
      "Iteration 655, loss = 0.30748411\n",
      "Iteration 308, loss = 0.39562933\n",
      "Iteration 757, loss = 0.30888714\n",
      "Iteration 758, loss = 0.30877619\n",
      "Iteration 656, loss = 0.30732121\n",
      "Iteration 1851, loss = 0.16374594\n",
      "Iteration 2550, loss = 0.09177372\n",
      "Iteration 2130, loss = 0.17862200\n",
      "Iteration 309, loss = 0.39544721\n",
      "Iteration 759, loss = 0.30869274\n",
      "Iteration 2551, loss = 0.09182147\n",
      "Iteration 965, loss = 0.30458759\n",
      "Iteration 2552, loss = 0.09171286\n",
      "Iteration 657, loss = 0.30709069\n",
      "Iteration 2131, loss = 0.17849895\n",
      "Iteration 2553, loss = 0.09161554\n",
      "Iteration 2554, loss = 0.09155574\n",
      "Iteration 658, loss = 0.30712532\n",
      "Iteration 2555, loss = 0.09151137\n",
      "Iteration 310, loss = 0.39521396\n",
      "Iteration 2556, loss = 0.09162134\n",
      "Iteration 659, loss = 0.30687564\n",
      "Iteration 966, loss = 0.30436052\n",
      "Iteration 2132, loss = 0.17844198\n",
      "Iteration 2557, loss = 0.09132047\n",
      "Iteration 2558, loss = 0.09128176\n",
      "Iteration 2559, loss = 0.09126159\n",
      "Iteration 2560, loss = 0.09121477\n",
      "Iteration 660, loss = 0.30671009\n",
      "Iteration 2561, loss = 0.09112181\n",
      "Iteration 2133, loss = 0.17838322\n",
      "Iteration 760, loss = 0.30861184\n",
      "Iteration 2562, loss = 0.09113462\n",
      "Iteration 1852, loss = 0.16368348\n",
      "Iteration 2563, loss = 0.09097786\n",
      "Iteration 661, loss = 0.30660258\n",
      "Iteration 2564, loss = 0.09089240\n",
      "Iteration 2565, loss = 0.09086581\n",
      "Iteration 311, loss = 0.39499888\n",
      "Iteration 2566, loss = 0.09079789\n",
      "Iteration 662, loss = 0.30647324\n",
      "Iteration 967, loss = 0.30417782\n",
      "Iteration 2134, loss = 0.17834063\n",
      "Iteration 663, loss = 0.30637620\n",
      "Iteration 2567, loss = 0.09066217\n",
      "Iteration 664, loss = 0.30618016\n",
      "Iteration 1853, loss = 0.16362437\n",
      "Iteration 2135, loss = 0.17813824\n",
      "Iteration 968, loss = 0.30407194\n",
      "Iteration 665, loss = 0.30607004\n",
      "Iteration 761, loss = 0.30849682\n",
      "Iteration 1854, loss = 0.16352000\n",
      "Iteration 2568, loss = 0.09071735\n",
      "Iteration 2136, loss = 0.17809284\n",
      "Iteration 2569, loss = 0.09060709\n",
      "Iteration 312, loss = 0.39485896\n",
      "Iteration 2570, loss = 0.09076074\n",
      "Iteration 2571, loss = 0.09045610\n",
      "Iteration 762, loss = 0.30839413\n",
      "Iteration 969, loss = 0.30373479\n",
      "Iteration 2572, loss = 0.09041350\n",
      "Iteration 1855, loss = 0.16341969\n",
      "Iteration 666, loss = 0.30596636\n",
      "Iteration 313, loss = 0.39464442\n",
      "Iteration 2137, loss = 0.17800888\n",
      "Iteration 667, loss = 0.30584536\n",
      "Iteration 2573, loss = 0.09032948\n",
      "Iteration 763, loss = 0.30837544\n",
      "Iteration 2574, loss = 0.09031990\n",
      "Iteration 2575, loss = 0.09022119\n",
      "Iteration 970, loss = 0.30352446\n",
      "Iteration 668, loss = 0.30585298\n",
      "Iteration 2138, loss = 0.17787691\n",
      "Iteration 1856, loss = 0.16336602\n",
      "Iteration 314, loss = 0.39432153\n",
      "Iteration 2576, loss = 0.09027990\n",
      "Iteration 2577, loss = 0.09011085\n",
      "Iteration 2578, loss = 0.09004311\n",
      "Iteration 764, loss = 0.30822301\n",
      "Iteration 2579, loss = 0.08999481\n",
      "Iteration 669, loss = 0.30560492\n",
      "Iteration 1857, loss = 0.16331426\n",
      "Iteration 2580, loss = 0.08990710\n",
      "Iteration 971, loss = 0.30351487\n",
      "Iteration 2581, loss = 0.08981080\n",
      "Iteration 2139, loss = 0.17781120\n",
      "Iteration 315, loss = 0.39415774\n",
      "Iteration 2582, loss = 0.08974548\n",
      "Iteration 670, loss = 0.30545720\n",
      "Iteration 765, loss = 0.30810160\n",
      "Iteration 2583, loss = 0.08975419\n",
      "Iteration 1858, loss = 0.16317595\n",
      "Iteration 671, loss = 0.30530220\n",
      "Iteration 972, loss = 0.30322236\n",
      "Iteration 766, loss = 0.30803734\n",
      "Iteration 2584, loss = 0.08970405\n",
      "Iteration 2140, loss = 0.17768662\n",
      "Iteration 2585, loss = 0.08964125\n",
      "Iteration 316, loss = 0.39395920\n",
      "Iteration 2586, loss = 0.08954985\n",
      "Iteration 672, loss = 0.30526064\n",
      "Iteration 2587, loss = 0.08953311\n",
      "Iteration 2588, loss = 0.08934277\n",
      "Iteration 767, loss = 0.30791937\n",
      "Iteration 673, loss = 0.30506920\n",
      "Iteration 2589, loss = 0.08939680\n",
      "Iteration 1859, loss = 0.16311564\n",
      "Iteration 2141, loss = 0.17772406\n",
      "Iteration 973, loss = 0.30299928\n",
      "Iteration 674, loss = 0.30490666\n",
      "Iteration 2590, loss = 0.08927233\n",
      "Iteration 2591, loss = 0.08916660\n",
      "Iteration 768, loss = 0.30791484\n",
      "Iteration 675, loss = 0.30490173\n",
      "Iteration 2592, loss = 0.08920059\n",
      "Iteration 2142, loss = 0.17754272\n",
      "Iteration 1860, loss = 0.16297843\n",
      "Iteration 317, loss = 0.39365399\n",
      "Iteration 2593, loss = 0.08912767\n",
      "Iteration 676, loss = 0.30466482\n",
      "Iteration 2594, loss = 0.08901080\n",
      "Iteration 2143, loss = 0.17744811\n",
      "Iteration 2595, loss = 0.08891554\n",
      "Iteration 769, loss = 0.30773305\n",
      "Iteration 2596, loss = 0.08904338\n",
      "Iteration 974, loss = 0.30288004\n",
      "Iteration 2597, loss = 0.08887242\n",
      "Iteration 2598, loss = 0.08876665\n",
      "Iteration 2599, loss = 0.08872122\n",
      "Iteration 318, loss = 0.39346068\n",
      "Iteration 2600, loss = 0.08864214\n",
      "Iteration 2144, loss = 0.17735391\n",
      "Iteration 2601, loss = 0.08857626\n",
      "Iteration 1861, loss = 0.16294690\n",
      "Iteration 770, loss = 0.30762092\n",
      "Iteration 2602, loss = 0.08858835\n",
      "Iteration 677, loss = 0.30454180\n",
      "Iteration 2145, loss = 0.17724152\n",
      "Iteration 975, loss = 0.30263629\n",
      "Iteration 771, loss = 0.30757192\n",
      "Iteration 319, loss = 0.39325303\n",
      "Iteration 2603, loss = 0.08853845\n",
      "Iteration 678, loss = 0.30442922\n",
      "Iteration 2604, loss = 0.08842868\n",
      "Iteration 772, loss = 0.30744083\n",
      "Iteration 2605, loss = 0.08837783\n",
      "Iteration 1862, loss = 0.16282647\n",
      "Iteration 2606, loss = 0.08841634\n",
      "Iteration 2607, loss = 0.08817915\n",
      "Iteration 320, loss = 0.39302495\n",
      "Iteration 2608, loss = 0.08817722\n",
      "Iteration 2609, loss = 0.08815667\n",
      "Iteration 1863, loss = 0.16284384\n",
      "Iteration 2610, loss = 0.08810470\n",
      "Iteration 773, loss = 0.30739354\n",
      "Iteration 2611, loss = 0.08794066\n",
      "Iteration 2146, loss = 0.17724540\n",
      "Iteration 679, loss = 0.30434840\n",
      "Iteration 976, loss = 0.30237922\n",
      "Iteration 774, loss = 0.30723750\n",
      "Iteration 2612, loss = 0.08793969\n",
      "Iteration 680, loss = 0.30417299\n",
      "Iteration 2613, loss = 0.08802424\n",
      "Iteration 321, loss = 0.39282723\n",
      "Iteration 2614, loss = 0.08785756\n",
      "Iteration 681, loss = 0.30405690\n",
      "Iteration 977, loss = 0.30219635\n",
      "Iteration 2147, loss = 0.17701462\n",
      "Iteration 1864, loss = 0.16278080\n",
      "Iteration 775, loss = 0.30715518\n",
      "Iteration 2615, loss = 0.08775624\n",
      "Iteration 322, loss = 0.39260074\n",
      "Iteration 2616, loss = 0.08773283\n",
      "Iteration 682, loss = 0.30393181\n",
      "Iteration 2617, loss = 0.08767702\n",
      "Iteration 2148, loss = 0.17697681\n",
      "Iteration 2618, loss = 0.08755292\n",
      "Iteration 776, loss = 0.30707694\n",
      "Iteration 978, loss = 0.30193552\n",
      "Iteration 2619, loss = 0.08762020\n",
      "Iteration 1865, loss = 0.16290384\n",
      "Iteration 2620, loss = 0.08741585\n",
      "Iteration 683, loss = 0.30378364\n",
      "Iteration 2149, loss = 0.17682088\n",
      "Iteration 2621, loss = 0.08745168\n",
      "Iteration 2622, loss = 0.08745037\n",
      "Iteration 2623, loss = 0.08729538\n",
      "Iteration 684, loss = 0.30366701\n",
      "Iteration 2624, loss = 0.08720621\n",
      "Iteration 2625, loss = 0.08727226\n",
      "Iteration 777, loss = 0.30696940\n",
      "Iteration 979, loss = 0.30175950\n",
      "Iteration 2626, loss = 0.08720184Iteration 2150, loss = 0.17673689\n",
      "\n",
      "Iteration 1866, loss = 0.16253432\n",
      "Iteration 2627, loss = 0.08713471\n",
      "Iteration 685, loss = 0.30357632\n",
      "Iteration 2628, loss = 0.08703239\n",
      "Iteration 778, loss = 0.30694005\n",
      "Iteration 2629, loss = 0.08685940\n",
      "Iteration 323, loss = 0.39240296\n",
      "Iteration 2151, loss = 0.17664338\n",
      "Iteration 2630, loss = 0.08690015\n",
      "Iteration 1867, loss = 0.16245043\n",
      "Iteration 686, loss = 0.30337363\n",
      "Iteration 980, loss = 0.30158436\n",
      "Iteration 2631, loss = 0.08681767\n",
      "Iteration 779, loss = 0.30674714\n",
      "Iteration 687, loss = 0.30326783\n",
      "Iteration 2632, loss = 0.08684614\n",
      "Iteration 324, loss = 0.39218240\n",
      "Iteration 1868, loss = 0.16235991\n",
      "Iteration 2152, loss = 0.17657052\n",
      "Iteration 780, loss = 0.30668034\n",
      "Iteration 688, loss = 0.30314731\n",
      "Iteration 2633, loss = 0.08667989\n",
      "Iteration 981, loss = 0.30139510\n",
      "Iteration 2634, loss = 0.08685015\n",
      "Iteration 689, loss = 0.30300349\n",
      "Iteration 2153, loss = 0.17645811\n",
      "Iteration 2635, loss = 0.08672479\n",
      "Iteration 781, loss = 0.30657982\n",
      "Iteration 325, loss = 0.39197491\n",
      "Iteration 2636, loss = 0.08672847\n",
      "Iteration 690, loss = 0.30287504\n",
      "Iteration 982, loss = 0.30123379\n",
      "Iteration 2154, loss = 0.17659453\n",
      "Iteration 782, loss = 0.30650149\n",
      "Iteration 2637, loss = 0.08664037\n",
      "Iteration 2638, loss = 0.08644797\n",
      "Iteration 1869, loss = 0.16225402\n",
      "Iteration 2639, loss = 0.08635516\n",
      "Iteration 691, loss = 0.30272130\n",
      "Iteration 2640, loss = 0.08627600\n",
      "Iteration 326, loss = 0.39176424\n",
      "Iteration 2641, loss = 0.08621795\n",
      "Iteration 2155, loss = 0.17626937\n",
      "Iteration 983, loss = 0.30105445\n",
      "Iteration 2642, loss = 0.08620482\n",
      "Iteration 2643, loss = 0.08613425\n",
      "Iteration 1870, loss = 0.16218803\n",
      "Iteration 692, loss = 0.30269463\n",
      "Iteration 2644, loss = 0.08607037\n",
      "Iteration 783, loss = 0.30639859\n",
      "Iteration 2645, loss = 0.08599297\n",
      "Iteration 2156, loss = 0.17620608\n",
      "Iteration 693, loss = 0.30251320\n",
      "Iteration 2646, loss = 0.08611217\n",
      "Iteration 327, loss = 0.39155756\n",
      "Iteration 784, loss = 0.30628794\n",
      "Iteration 2647, loss = 0.08601063\n",
      "Iteration 1871, loss = 0.16206062\n",
      "Iteration 984, loss = 0.30076259\n",
      "Iteration 694, loss = 0.30247396\n",
      "Iteration 2648, loss = 0.08584834\n",
      "Iteration 785, loss = 0.30621359\n",
      "Iteration 2649, loss = 0.08586118\n",
      "Iteration 328, loss = 0.39141260\n",
      "Iteration 695, loss = 0.30230723\n",
      "Iteration 2157, loss = 0.17618910\n",
      "Iteration 1872, loss = 0.16200551\n",
      "Iteration 2650, loss = 0.08570425\n",
      "Iteration 786, loss = 0.30609043\n",
      "Iteration 696, loss = 0.30210982\n",
      "Iteration 2651, loss = 0.08566243\n",
      "Iteration 2158, loss = 0.17603072\n",
      "Iteration 985, loss = 0.30057354\n",
      "Iteration 2652, loss = 0.08561651\n",
      "Iteration 787, loss = 0.30601465\n",
      "Iteration 697, loss = 0.30199315\n",
      "Iteration 329, loss = 0.39117448\n",
      "Iteration 2653, loss = 0.08562408\n",
      "Iteration 2654, loss = 0.08550956\n",
      "Iteration 1873, loss = 0.16205110\n",
      "Iteration 2655, loss = 0.08552138\n",
      "Iteration 698, loss = 0.30183021\n",
      "Iteration 2159, loss = 0.17589082\n",
      "Iteration 2656, loss = 0.08542520\n",
      "Iteration 699, loss = 0.30175192\n",
      "Iteration 330, loss = 0.39094400\n",
      "Iteration 986, loss = 0.30045769\n",
      "Iteration 2657, loss = 0.08534515\n",
      "Iteration 2160, loss = 0.17578144\n",
      "Iteration 788, loss = 0.30589984\n",
      "Iteration 2658, loss = 0.08526533\n",
      "Iteration 700, loss = 0.30159218\n",
      "Iteration 2659, loss = 0.08523555\n",
      "Iteration 789, loss = 0.30581763\n",
      "Iteration 701, loss = 0.30144359\n",
      "Iteration 2161, loss = 0.17573532\n",
      "Iteration 331, loss = 0.39072580\n",
      "Iteration 2660, loss = 0.08524171\n",
      "Iteration 1874, loss = 0.16186402\n",
      "Iteration 2661, loss = 0.08516581\n",
      "Iteration 2662, loss = 0.08507717\n",
      "Iteration 987, loss = 0.30029015\n",
      "Iteration 2663, loss = 0.08502921\n",
      "Iteration 790, loss = 0.30572371\n",
      "Iteration 702, loss = 0.30145490\n",
      "Iteration 2162, loss = 0.17579000\n",
      "Iteration 2664, loss = 0.08506428\n",
      "Iteration 2665, loss = 0.08490699\n",
      "Iteration 703, loss = 0.30124693\n",
      "Iteration 2666, loss = 0.08501292\n",
      "Iteration 332, loss = 0.39055022\n",
      "Iteration 1875, loss = 0.16176407\n",
      "Iteration 2163, loss = 0.17558807\n",
      "Iteration 791, loss = 0.30562932\n",
      "Iteration 2667, loss = 0.08481875\n",
      "Iteration 704, loss = 0.30115667\n",
      "Iteration 988, loss = 0.29999287\n",
      "Iteration 2668, loss = 0.08481148\n",
      "Iteration 2164, loss = 0.17544398\n",
      "Iteration 792, loss = 0.30553248\n",
      "Iteration 2669, loss = 0.08472392\n",
      "Iteration 705, loss = 0.30092490\n",
      "Iteration 2670, loss = 0.08480740\n",
      "Iteration 2671, loss = 0.08458831\n",
      "Iteration 2165, loss = 0.17535398\n",
      "Iteration 2672, loss = 0.08456087\n",
      "Iteration 706, loss = 0.30086127\n",
      "Iteration 989, loss = 0.29985766\n",
      "Iteration 333, loss = 0.39034663\n",
      "Iteration 2673, loss = 0.08446451\n",
      "Iteration 1876, loss = 0.16171249\n",
      "Iteration 793, loss = 0.30547808\n",
      "Iteration 2674, loss = 0.08444664\n",
      "Iteration 707, loss = 0.30073262\n",
      "Iteration 2675, loss = 0.08445114\n",
      "Iteration 2166, loss = 0.17548742\n",
      "Iteration 2676, loss = 0.08437445\n",
      "Iteration 1877, loss = 0.16161391\n",
      "Iteration 2677, loss = 0.08429095\n",
      "Iteration 708, loss = 0.30064425\n",
      "Iteration 2678, loss = 0.08430743\n",
      "Iteration 794, loss = 0.30538084\n",
      "Iteration 990, loss = 0.29957789\n",
      "Iteration 2679, loss = 0.08420213\n",
      "Iteration 2167, loss = 0.17517991\n",
      "Iteration 334, loss = 0.39013192\n",
      "Iteration 2680, loss = 0.08420651\n",
      "Iteration 709, loss = 0.30042122\n",
      "Iteration 2681, loss = 0.08415901\n",
      "Iteration 795, loss = 0.30529233\n",
      "Iteration 2168, loss = 0.17504298\n",
      "Iteration 991, loss = 0.29940142\n",
      "Iteration 2682, loss = 0.08400161\n",
      "Iteration 335, loss = 0.38991080\n",
      "Iteration 1878, loss = 0.16176356\n",
      "Iteration 2683, loss = 0.08394392\n",
      "Iteration 2684, loss = 0.08397599\n",
      "Iteration 796, loss = 0.30517470\n",
      "Iteration 710, loss = 0.30029529\n",
      "Iteration 2169, loss = 0.17493791\n",
      "Iteration 992, loss = 0.29926014\n",
      "Iteration 2685, loss = 0.08384264\n",
      "Iteration 711, loss = 0.30019030\n",
      "Iteration 336, loss = 0.38974419\n",
      "Iteration 1879, loss = 0.16156780\n",
      "Iteration 2686, loss = 0.08391370\n",
      "Iteration 712, loss = 0.30017738\n",
      "Iteration 2687, loss = 0.08374282\n",
      "Iteration 2170, loss = 0.17496865\n",
      "Iteration 993, loss = 0.29898830\n",
      "Iteration 2688, loss = 0.08368585\n",
      "Iteration 797, loss = 0.30505740\n",
      "Iteration 2689, loss = 0.08365673\n",
      "Iteration 2690, loss = 0.08361474\n",
      "Iteration 2171, loss = 0.17478937\n",
      "Iteration 713, loss = 0.30003749\n",
      "Iteration 2691, loss = 0.08361236\n",
      "Iteration 2692, loss = 0.08372194\n",
      "Iteration 994, loss = 0.29878112\n",
      "Iteration 714, loss = 0.29979009\n",
      "Iteration 337, loss = 0.38948781\n",
      "Iteration 2693, loss = 0.08342884\n",
      "Iteration 1880, loss = 0.16140061\n",
      "Iteration 2172, loss = 0.17472640\n",
      "Iteration 798, loss = 0.30497950\n",
      "Iteration 715, loss = 0.29970022\n",
      "Iteration 2694, loss = 0.08336717\n",
      "Iteration 716, loss = 0.29956272\n",
      "Iteration 2695, loss = 0.08332385\n",
      "Iteration 2173, loss = 0.17459775\n",
      "Iteration 799, loss = 0.30486388\n",
      "Iteration 2696, loss = 0.08333012\n",
      "Iteration 1881, loss = 0.16134396\n",
      "Iteration 2697, loss = 0.08324065\n",
      "Iteration 995, loss = 0.29863972\n",
      "Iteration 717, loss = 0.29951374\n",
      "Iteration 800, loss = 0.30478067\n",
      "Iteration 2698, loss = 0.08322490\n",
      "Iteration 338, loss = 0.38950594\n",
      "Iteration 2699, loss = 0.08316790\n",
      "Iteration 2700, loss = 0.08307098\n",
      "Iteration 801, loss = 0.30470936\n",
      "Iteration 2701, loss = 0.08301204\n",
      "Iteration 2174, loss = 0.17453586\n",
      "Iteration 996, loss = 0.29837000\n",
      "Iteration 718, loss = 0.29933063\n",
      "Iteration 2702, loss = 0.08295522\n",
      "Iteration 2703, loss = 0.08303989\n",
      "Iteration 802, loss = 0.30462939\n",
      "Iteration 2704, loss = 0.08310424\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1882, loss = 0.16128917\n",
      "Iteration 997, loss = 0.29834031Iteration 719, loss = 0.29914277\n",
      "\n",
      "Iteration 720, loss = 0.29906973\n",
      "Iteration 339, loss = 0.38915555\n",
      "Iteration 2175, loss = 0.17442420\n",
      "Iteration 1883, loss = 0.16121854\n",
      "Iteration 803, loss = 0.30451862\n",
      "Iteration 998, loss = 0.29806879\n",
      "Iteration 721, loss = 0.29913381\n",
      "Iteration 2176, loss = 0.17433587\n",
      "Iteration 804, loss = 0.30452128\n",
      "Iteration 722, loss = 0.29877711\n",
      "Iteration 340, loss = 0.38891924\n",
      "Iteration 1884, loss = 0.16111289\n",
      "Iteration 999, loss = 0.29782946\n",
      "Iteration 2177, loss = 0.17420704\n",
      "Iteration 723, loss = 0.29866276\n",
      "Iteration 805, loss = 0.30430255\n",
      "Iteration 724, loss = 0.29849527\n",
      "Iteration 341, loss = 0.38870061\n",
      "Iteration 2178, loss = 0.17421500\n",
      "Iteration 1000, loss = 0.29764168\n",
      "Iteration 806, loss = 0.30420567\n",
      "Iteration 1885, loss = 0.16105698\n",
      "Iteration 725, loss = 0.29839245\n",
      "Iteration 2179, loss = 0.17407798\n",
      "Iteration 807, loss = 0.30414134\n",
      "Iteration 726, loss = 0.29827941\n",
      "Iteration 342, loss = 0.38848758\n",
      "Iteration 1001, loss = 0.29761118\n",
      "Iteration 727, loss = 0.29816317\n",
      "Iteration 1886, loss = 0.16095502\n",
      "Iteration 2180, loss = 0.17394235\n",
      "Iteration 808, loss = 0.30402601\n",
      "Iteration 728, loss = 0.29803944\n",
      "Iteration 343, loss = 0.38828505\n",
      "Iteration 809, loss = 0.30394146\n",
      "Iteration 729, loss = 0.29790980\n",
      "Iteration 1887, loss = 0.16082838\n",
      "Iteration 2181, loss = 0.17385443\n",
      "Iteration 1002, loss = 0.29722134\n",
      "Iteration 730, loss = 0.29781301\n",
      "Iteration 810, loss = 0.30384971\n",
      "Iteration 344, loss = 0.38810621\n",
      "Iteration 2182, loss = 0.17373997\n",
      "Iteration 1888, loss = 0.16079657\n",
      "Iteration 1003, loss = 0.29707393\n",
      "Iteration 731, loss = 0.29764246\n",
      "Iteration 811, loss = 0.30387571\n",
      "Iteration 2183, loss = 0.17364614\n",
      "Iteration 345, loss = 0.38792395\n",
      "Iteration 732, loss = 0.29752372\n",
      "Iteration 812, loss = 0.30365102\n",
      "Iteration 1004, loss = 0.29692501\n",
      "Iteration 1889, loss = 0.16066974\n",
      "Iteration 2184, loss = 0.17365631\n",
      "Iteration 733, loss = 0.29751348\n",
      "Iteration 734, loss = 0.29733979\n",
      "Iteration 813, loss = 0.30357637\n",
      "Iteration 346, loss = 0.38768727\n",
      "Iteration 1005, loss = 0.29660320\n",
      "Iteration 1890, loss = 0.16061513\n",
      "Iteration 2185, loss = 0.17349922\n",
      "Iteration 735, loss = 0.29722707\n",
      "Iteration 814, loss = 0.30347875\n",
      "Iteration 736, loss = 0.29700979\n",
      "Iteration 347, loss = 0.38745777\n",
      "Iteration 1006, loss = 0.29653406\n",
      "Iteration 2186, loss = 0.17340577\n",
      "Iteration 1891, loss = 0.16074869\n",
      "Iteration 737, loss = 0.29690245\n",
      "Iteration 815, loss = 0.30338567\n",
      "Iteration 348, loss = 0.38726908\n",
      "Iteration 738, loss = 0.29677178\n",
      "Iteration 816, loss = 0.30332008\n",
      "Iteration 739, loss = 0.29671482\n",
      "Iteration 1007, loss = 0.29634197\n",
      "Iteration 2187, loss = 0.17340218\n",
      "Iteration 349, loss = 0.38712834\n",
      "Iteration 1892, loss = 0.16043974\n",
      "Iteration 740, loss = 0.29652983\n",
      "Iteration 817, loss = 0.30322850\n",
      "Iteration 2188, loss = 0.17320313\n",
      "Iteration 741, loss = 0.29639532\n",
      "Iteration 1893, loss = 0.16041065\n",
      "Iteration 1008, loss = 0.29628523\n",
      "Iteration 350, loss = 0.38688923\n",
      "Iteration 742, loss = 0.29631475\n",
      "Iteration 2189, loss = 0.17309655\n",
      "Iteration 818, loss = 0.30317165\n",
      "Iteration 743, loss = 0.29614242\n",
      "Iteration 2190, loss = 0.17307896\n",
      "Iteration 351, loss = 0.38671403\n",
      "Iteration 819, loss = 0.30309356\n",
      "Iteration 1894, loss = 0.16028906\n",
      "Iteration 744, loss = 0.29601523\n",
      "Iteration 1009, loss = 0.29591332\n",
      "Iteration 820, loss = 0.30294385\n",
      "Iteration 2191, loss = 0.17313411\n",
      "Iteration 1895, loss = 0.16025555\n",
      "Iteration 745, loss = 0.29587548\n",
      "Iteration 821, loss = 0.30289738\n",
      "Iteration 2192, loss = 0.17284739\n",
      "Iteration 352, loss = 0.38646737\n",
      "Iteration 1010, loss = 0.29556698\n",
      "Iteration 822, loss = 0.30273644\n",
      "Iteration 746, loss = 0.29579784\n",
      "Iteration 747, loss = 0.29566834\n",
      "Iteration 2193, loss = 0.17277427\n",
      "Iteration 823, loss = 0.30263915\n",
      "Iteration 1896, loss = 0.16020151\n",
      "Iteration 1011, loss = 0.29549708\n",
      "Iteration 353, loss = 0.38634384\n",
      "Iteration 748, loss = 0.29550255\n",
      "Iteration 2194, loss = 0.17265553\n",
      "Iteration 354, loss = 0.38610983\n",
      "Iteration 1012, loss = 0.29517935\n",
      "Iteration 749, loss = 0.29546428\n",
      "Iteration 824, loss = 0.30260181\n",
      "Iteration 1897, loss = 0.16008581\n",
      "Iteration 2195, loss = 0.17259014\n",
      "Iteration 825, loss = 0.30248948\n",
      "Iteration 750, loss = 0.29527647\n",
      "Iteration 355, loss = 0.38588533\n",
      "Iteration 1013, loss = 0.29510591\n",
      "Iteration 2196, loss = 0.17247229\n",
      "Iteration 826, loss = 0.30237065\n",
      "Iteration 1898, loss = 0.16004629\n",
      "Iteration 751, loss = 0.29524504\n",
      "Iteration 356, loss = 0.38582455\n",
      "Iteration 1899, loss = 0.15997310\n",
      "Iteration 827, loss = 0.30225637\n",
      "Iteration 1014, loss = 0.29481026\n",
      "Iteration 2197, loss = 0.17237606\n",
      "Iteration 752, loss = 0.29529336\n",
      "Iteration 2198, loss = 0.17228621\n",
      "Iteration 1900, loss = 0.15999014\n",
      "Iteration 357, loss = 0.38552187\n",
      "Iteration 828, loss = 0.30218183\n",
      "Iteration 753, loss = 0.29496950\n",
      "Iteration 1015, loss = 0.29456058\n",
      "Iteration 2199, loss = 0.17223057\n",
      "Iteration 754, loss = 0.29490368\n",
      "Iteration 829, loss = 0.30212213\n",
      "Iteration 358, loss = 0.38531057\n",
      "Iteration 755, loss = 0.29479391\n",
      "Iteration 1901, loss = 0.15990349\n",
      "Iteration 1016, loss = 0.29437773\n",
      "Iteration 2200, loss = 0.17214829\n",
      "Iteration 830, loss = 0.30198324\n",
      "Iteration 756, loss = 0.29468044\n",
      "Iteration 2201, loss = 0.17204083\n",
      "Iteration 757, loss = 0.29441265\n",
      "Iteration 831, loss = 0.30198498\n",
      "Iteration 1017, loss = 0.29424535\n",
      "Iteration 1902, loss = 0.15971919\n",
      "Iteration 359, loss = 0.38509129\n",
      "Iteration 2202, loss = 0.17194431\n",
      "Iteration 758, loss = 0.29429575\n",
      "Iteration 2203, loss = 0.17188191\n",
      "Iteration 1018, loss = 0.29408635\n",
      "Iteration 832, loss = 0.30187721\n",
      "Iteration 1903, loss = 0.15971608\n",
      "Iteration 360, loss = 0.38494384\n",
      "Iteration 759, loss = 0.29425260\n",
      "Iteration 2204, loss = 0.17177442\n",
      "Iteration 1904, loss = 0.15968473\n",
      "Iteration 1019, loss = 0.29375653\n",
      "Iteration 833, loss = 0.30176574\n",
      "Iteration 760, loss = 0.29404326\n",
      "Iteration 361, loss = 0.38470473\n",
      "Iteration 761, loss = 0.29392242\n",
      "Iteration 2205, loss = 0.17165545\n",
      "Iteration 834, loss = 0.30172125\n",
      "Iteration 1020, loss = 0.29362701\n",
      "Iteration 762, loss = 0.29376653\n",
      "Iteration 1905, loss = 0.15946421\n",
      "Iteration 362, loss = 0.38454214\n",
      "Iteration 2206, loss = 0.17159354\n",
      "Iteration 763, loss = 0.29369162\n",
      "Iteration 835, loss = 0.30153048\n",
      "Iteration 764, loss = 0.29363623\n",
      "Iteration 1021, loss = 0.29342390\n",
      "Iteration 836, loss = 0.30145273\n",
      "Iteration 765, loss = 0.29339011\n",
      "Iteration 1906, loss = 0.15938637\n",
      "Iteration 2207, loss = 0.17149692\n",
      "Iteration 766, loss = 0.29327772\n",
      "Iteration 837, loss = 0.30133424\n",
      "Iteration 363, loss = 0.38436477\n",
      "Iteration 2208, loss = 0.17136793\n",
      "Iteration 1022, loss = 0.29317585\n",
      "Iteration 767, loss = 0.29318268\n",
      "Iteration 838, loss = 0.30130507\n",
      "Iteration 1907, loss = 0.15940333\n",
      "Iteration 768, loss = 0.29302858\n",
      "Iteration 2209, loss = 0.17143862\n",
      "Iteration 839, loss = 0.30116089\n",
      "Iteration 364, loss = 0.38414375\n",
      "Iteration 769, loss = 0.29295607\n",
      "Iteration 840, loss = 0.30106746\n",
      "Iteration 1908, loss = 0.15936945\n",
      "Iteration 1023, loss = 0.29309034\n",
      "Iteration 2210, loss = 0.17117701\n",
      "Iteration 770, loss = 0.29283113\n",
      "Iteration 841, loss = 0.30102069\n",
      "Iteration 365, loss = 0.38402282\n",
      "Iteration 1909, loss = 0.15919533\n",
      "Iteration 771, loss = 0.29265654\n",
      "Iteration 2211, loss = 0.17115228\n",
      "Iteration 1024, loss = 0.29281331\n",
      "Iteration 842, loss = 0.30101031\n",
      "Iteration 772, loss = 0.29253834\n",
      "Iteration 366, loss = 0.38376910\n",
      "Iteration 773, loss = 0.29242037\n",
      "Iteration 1910, loss = 0.15910783\n",
      "Iteration 2212, loss = 0.17107667\n",
      "Iteration 843, loss = 0.30078504\n",
      "Iteration 1025, loss = 0.29277694\n",
      "Iteration 774, loss = 0.29227995\n",
      "Iteration 367, loss = 0.38360028\n",
      "Iteration 775, loss = 0.29217337\n",
      "Iteration 1911, loss = 0.15905705\n",
      "Iteration 2213, loss = 0.17094898\n",
      "Iteration 844, loss = 0.30071548\n",
      "Iteration 776, loss = 0.29203907\n",
      "Iteration 1026, loss = 0.29234519\n",
      "Iteration 845, loss = 0.30061211\n",
      "Iteration 368, loss = 0.38349464\n",
      "Iteration 777, loss = 0.29191333\n",
      "Iteration 2214, loss = 0.17091926\n",
      "Iteration 778, loss = 0.29183789\n",
      "Iteration 846, loss = 0.30056154\n",
      "Iteration 1912, loss = 0.15893486\n",
      "Iteration 779, loss = 0.29166324\n",
      "Iteration 1027, loss = 0.29223348\n",
      "Iteration 847, loss = 0.30047561\n",
      "Iteration 2215, loss = 0.17078212\n",
      "Iteration 780, loss = 0.29157298\n",
      "Iteration 369, loss = 0.38323636\n",
      "Iteration 2216, loss = 0.17065642\n",
      "Iteration 1913, loss = 0.15886351\n",
      "Iteration 848, loss = 0.30033927\n",
      "Iteration 781, loss = 0.29142450\n",
      "Iteration 849, loss = 0.30029697\n",
      "Iteration 2217, loss = 0.17059518\n",
      "Iteration 1028, loss = 0.29195693\n",
      "Iteration 370, loss = 0.38303551\n",
      "Iteration 1914, loss = 0.15883767\n",
      "Iteration 782, loss = 0.29132160\n",
      "Iteration 2218, loss = 0.17056477\n",
      "Iteration 1915, loss = 0.15878329\n",
      "Iteration 850, loss = 0.30014846\n",
      "Iteration 783, loss = 0.29112994\n",
      "Iteration 371, loss = 0.38290825\n",
      "Iteration 2219, loss = 0.17045130\n",
      "Iteration 784, loss = 0.29102978\n",
      "Iteration 1029, loss = 0.29177060\n",
      "Iteration 851, loss = 0.30005239\n",
      "Iteration 785, loss = 0.29110349\n",
      "Iteration 852, loss = 0.29997178\n",
      "Iteration 1030, loss = 0.29163505\n",
      "Iteration 786, loss = 0.29078077\n",
      "Iteration 2220, loss = 0.17036947\n",
      "Iteration 1916, loss = 0.15865093\n",
      "Iteration 372, loss = 0.38265716\n",
      "Iteration 787, loss = 0.29071698\n",
      "Iteration 853, loss = 0.29987114\n",
      "Iteration 2221, loss = 0.17021145\n",
      "Iteration 788, loss = 0.29051624\n",
      "Iteration 1031, loss = 0.29142552\n",
      "Iteration 1917, loss = 0.15860798\n",
      "Iteration 373, loss = 0.38251196\n",
      "Iteration 854, loss = 0.29978258\n",
      "Iteration 2222, loss = 0.17017884\n",
      "Iteration 789, loss = 0.29041552\n",
      "Iteration 2223, loss = 0.17010064\n",
      "Iteration 1032, loss = 0.29119436\n",
      "Iteration 374, loss = 0.38235061\n",
      "Iteration 1918, loss = 0.15858139\n",
      "Iteration 855, loss = 0.29974565\n",
      "Iteration 790, loss = 0.29024767\n",
      "Iteration 856, loss = 0.29965368\n",
      "Iteration 791, loss = 0.29013217\n",
      "Iteration 2224, loss = 0.16997566\n",
      "Iteration 1033, loss = 0.29105845\n",
      "Iteration 375, loss = 0.38209567\n",
      "Iteration 792, loss = 0.29006553\n",
      "Iteration 1919, loss = 0.15843671\n",
      "Iteration 2225, loss = 0.16990460\n",
      "Iteration 857, loss = 0.29952819\n",
      "Iteration 793, loss = 0.28992738\n",
      "Iteration 376, loss = 0.38191891\n",
      "Iteration 1034, loss = 0.29083764\n",
      "Iteration 858, loss = 0.29944029\n",
      "Iteration 1920, loss = 0.15842778\n",
      "Iteration 2226, loss = 0.16981524\n",
      "Iteration 794, loss = 0.28977815\n",
      "Iteration 1035, loss = 0.29057037\n",
      "Iteration 795, loss = 0.28963558\n",
      "Iteration 377, loss = 0.38174898\n",
      "Iteration 859, loss = 0.29932868\n",
      "Iteration 796, loss = 0.28963222\n",
      "Iteration 1921, loss = 0.15862771\n",
      "Iteration 2227, loss = 0.16969324\n",
      "Iteration 860, loss = 0.29926628\n",
      "Iteration 378, loss = 0.38154132\n",
      "Iteration 1036, loss = 0.29039917\n",
      "Iteration 797, loss = 0.28945652\n",
      "Iteration 2228, loss = 0.16968861\n",
      "Iteration 861, loss = 0.29915280\n",
      "Iteration 1037, loss = 0.29017856\n",
      "Iteration 798, loss = 0.28929008\n",
      "Iteration 2229, loss = 0.16955896\n",
      "Iteration 1922, loss = 0.15822852\n",
      "Iteration 379, loss = 0.38135005\n",
      "Iteration 799, loss = 0.28916945\n",
      "Iteration 2230, loss = 0.16945818\n",
      "Iteration 862, loss = 0.29904496\n",
      "Iteration 1923, loss = 0.15819043\n",
      "Iteration 380, loss = 0.38121268\n",
      "Iteration 2231, loss = 0.16935487\n",
      "Iteration 800, loss = 0.28916542\n",
      "Iteration 1038, loss = 0.28996242\n",
      "Iteration 863, loss = 0.29900493\n",
      "Iteration 801, loss = 0.28889473\n",
      "Iteration 2232, loss = 0.16927578\n",
      "Iteration 381, loss = 0.38101442\n",
      "Iteration 864, loss = 0.29887136\n",
      "Iteration 1924, loss = 0.15809944\n",
      "Iteration 1039, loss = 0.28983115\n",
      "Iteration 802, loss = 0.28875034\n",
      "Iteration 803, loss = 0.28861659\n",
      "Iteration 2233, loss = 0.16917215\n",
      "Iteration 865, loss = 0.29876976\n",
      "Iteration 382, loss = 0.38082656\n",
      "Iteration 1925, loss = 0.15804270\n",
      "Iteration 804, loss = 0.28851018\n",
      "Iteration 1040, loss = 0.28956627\n",
      "Iteration 2234, loss = 0.16909099\n",
      "Iteration 866, loss = 0.29868211\n",
      "Iteration 805, loss = 0.28839159\n",
      "Iteration 383, loss = 0.38061877\n",
      "Iteration 806, loss = 0.28826217\n",
      "Iteration 867, loss = 0.29867300\n",
      "Iteration 2235, loss = 0.16908240\n",
      "Iteration 1041, loss = 0.28944482\n",
      "Iteration 807, loss = 0.28814302\n",
      "Iteration 1926, loss = 0.15802530\n",
      "Iteration 384, loss = 0.38043185\n",
      "Iteration 808, loss = 0.28798339\n",
      "Iteration 1042, loss = 0.28923057\n",
      "Iteration 2236, loss = 0.16892489\n",
      "Iteration 868, loss = 0.29850465\n",
      "Iteration 1927, loss = 0.15796396\n",
      "Iteration 869, loss = 0.29841064\n",
      "Iteration 809, loss = 0.28787883\n",
      "Iteration 1043, loss = 0.28894230\n",
      "Iteration 385, loss = 0.38024406\n",
      "Iteration 810, loss = 0.28777300\n",
      "Iteration 2237, loss = 0.16882867\n",
      "Iteration 870, loss = 0.29833811\n",
      "Iteration 811, loss = 0.28764613\n",
      "Iteration 1928, loss = 0.15802245\n",
      "Iteration 1044, loss = 0.28874035\n",
      "Iteration 2238, loss = 0.16874080\n",
      "Iteration 871, loss = 0.29826263\n",
      "Iteration 386, loss = 0.38008673\n",
      "Iteration 2239, loss = 0.16866680\n",
      "Iteration 812, loss = 0.28746503\n",
      "Iteration 1929, loss = 0.15778054\n",
      "Iteration 1045, loss = 0.28853979\n",
      "Iteration 872, loss = 0.29817095\n",
      "Iteration 2240, loss = 0.16854815\n",
      "Iteration 387, loss = 0.37992259\n",
      "Iteration 1046, loss = 0.28833494\n",
      "Iteration 873, loss = 0.29804875\n",
      "Iteration 813, loss = 0.28734464\n",
      "Iteration 1930, loss = 0.15783930\n",
      "Iteration 2241, loss = 0.16850372\n",
      "Iteration 814, loss = 0.28734575\n",
      "Iteration 874, loss = 0.29801247\n",
      "Iteration 388, loss = 0.37973748\n",
      "Iteration 815, loss = 0.28714550\n",
      "Iteration 1931, loss = 0.15765520\n",
      "Iteration 1047, loss = 0.28820210\n",
      "Iteration 2242, loss = 0.16839284\n",
      "Iteration 875, loss = 0.29794920\n",
      "Iteration 389, loss = 0.37952775\n",
      "Iteration 1048, loss = 0.28800634\n",
      "Iteration 816, loss = 0.28699945\n",
      "Iteration 2243, loss = 0.16834327\n",
      "Iteration 1932, loss = 0.15757105\n",
      "Iteration 876, loss = 0.29779625\n",
      "Iteration 390, loss = 0.37934139\n",
      "Iteration 877, loss = 0.29769823\n",
      "Iteration 817, loss = 0.28690578\n",
      "Iteration 1049, loss = 0.28800262\n",
      "Iteration 818, loss = 0.28688385\n",
      "Iteration 2244, loss = 0.16822573\n",
      "Iteration 1933, loss = 0.15748766\n",
      "Iteration 878, loss = 0.29768639\n",
      "Iteration 819, loss = 0.28659698\n",
      "Iteration 391, loss = 0.37919199\n",
      "Iteration 820, loss = 0.28683519\n",
      "Iteration 2245, loss = 0.16820171\n",
      "Iteration 879, loss = 0.29750793\n",
      "Iteration 1934, loss = 0.15744370\n",
      "Iteration 821, loss = 0.28639762\n",
      "Iteration 1050, loss = 0.28758679\n",
      "Iteration 2246, loss = 0.16814451\n",
      "Iteration 822, loss = 0.28625646\n",
      "Iteration 392, loss = 0.37897308\n",
      "Iteration 880, loss = 0.29741060\n",
      "Iteration 1935, loss = 0.15742813\n",
      "Iteration 2247, loss = 0.16796572\n",
      "Iteration 1051, loss = 0.28731720\n",
      "Iteration 881, loss = 0.29743688\n",
      "Iteration 393, loss = 0.37880310\n",
      "Iteration 823, loss = 0.28613073\n",
      "Iteration 2248, loss = 0.16793657\n",
      "Iteration 1936, loss = 0.15730415\n",
      "Iteration 824, loss = 0.28602783\n",
      "Iteration 2249, loss = 0.16775200\n",
      "Iteration 825, loss = 0.28593988\n",
      "Iteration 1052, loss = 0.28714632\n",
      "Iteration 882, loss = 0.29722754\n",
      "Iteration 394, loss = 0.37861679\n",
      "Iteration 826, loss = 0.28573599\n",
      "Iteration 883, loss = 0.29719342\n",
      "Iteration 2250, loss = 0.16768049\n",
      "Iteration 1053, loss = 0.28698247\n",
      "Iteration 1937, loss = 0.15723157\n",
      "Iteration 827, loss = 0.28566900\n",
      "Iteration 884, loss = 0.29707218\n",
      "Iteration 395, loss = 0.37845988\n",
      "Iteration 1054, loss = 0.28677722\n",
      "Iteration 885, loss = 0.29697816\n",
      "Iteration 2251, loss = 0.16761035\n",
      "Iteration 1938, loss = 0.15714799\n",
      "Iteration 828, loss = 0.28555232\n",
      "Iteration 396, loss = 0.37825761\n",
      "Iteration 1939, loss = 0.15709489\n",
      "Iteration 886, loss = 0.29688632\n",
      "Iteration 1055, loss = 0.28650082\n",
      "Iteration 2252, loss = 0.16753788\n",
      "Iteration 829, loss = 0.28536374\n",
      "Iteration 397, loss = 0.37807278\n",
      "Iteration 887, loss = 0.29678017\n",
      "Iteration 2253, loss = 0.16746735\n",
      "Iteration 830, loss = 0.28519685\n",
      "Iteration 1940, loss = 0.15701209\n",
      "Iteration 398, loss = 0.37794021\n",
      "Iteration 1056, loss = 0.28628234\n",
      "Iteration 888, loss = 0.29669987\n",
      "Iteration 2254, loss = 0.16735278\n",
      "Iteration 1941, loss = 0.15694273\n",
      "Iteration 831, loss = 0.28511934\n",
      "Iteration 1057, loss = 0.28614852\n",
      "Iteration 399, loss = 0.37769759\n",
      "Iteration 2255, loss = 0.16734420\n",
      "Iteration 889, loss = 0.29659327\n",
      "Iteration 832, loss = 0.28501535\n",
      "Iteration 1942, loss = 0.15702012\n",
      "Iteration 2256, loss = 0.16718651\n",
      "Iteration 890, loss = 0.29652333\n",
      "Iteration 400, loss = 0.37757012\n",
      "Iteration 833, loss = 0.28493786\n",
      "Iteration 1058, loss = 0.28587084\n",
      "Iteration 2257, loss = 0.16709009\n",
      "Iteration 1943, loss = 0.15690982\n",
      "Iteration 891, loss = 0.29640379\n",
      "Iteration 834, loss = 0.28470746\n",
      "Iteration 401, loss = 0.37743914\n",
      "Iteration 1059, loss = 0.28571429\n",
      "Iteration 2258, loss = 0.16707699\n",
      "Iteration 1944, loss = 0.15682987\n",
      "Iteration 892, loss = 0.29632017\n",
      "Iteration 2259, loss = 0.16693227\n",
      "Iteration 835, loss = 0.28471031\n",
      "Iteration 1060, loss = 0.28556431\n",
      "Iteration 836, loss = 0.28447512\n",
      "Iteration 893, loss = 0.29622739\n",
      "Iteration 2260, loss = 0.16684896\n",
      "Iteration 1945, loss = 0.15671591\n",
      "Iteration 402, loss = 0.37721015\n",
      "Iteration 837, loss = 0.28437836\n",
      "Iteration 838, loss = 0.28425596\n",
      "Iteration 2261, loss = 0.16676490\n",
      "Iteration 1061, loss = 0.28530435\n",
      "Iteration 1946, loss = 0.15662201\n",
      "Iteration 894, loss = 0.29616357\n",
      "Iteration 839, loss = 0.28433697\n",
      "Iteration 403, loss = 0.37705358\n",
      "Iteration 2262, loss = 0.16669868\n",
      "Iteration 840, loss = 0.28399967\n",
      "Iteration 1062, loss = 0.28502674\n",
      "Iteration 895, loss = 0.29604542\n",
      "Iteration 1947, loss = 0.15663775\n",
      "Iteration 841, loss = 0.28383034\n",
      "Iteration 404, loss = 0.37689736\n",
      "Iteration 2263, loss = 0.16662791\n",
      "Iteration 896, loss = 0.29597119\n",
      "Iteration 1948, loss = 0.15655222\n",
      "Iteration 842, loss = 0.28373417\n",
      "Iteration 897, loss = 0.29587588\n",
      "Iteration 1063, loss = 0.28484961\n",
      "Iteration 2264, loss = 0.16652483\n",
      "Iteration 405, loss = 0.37669943\n",
      "Iteration 843, loss = 0.28365253\n",
      "Iteration 1949, loss = 0.15645731\n",
      "Iteration 1064, loss = 0.28466342\n",
      "Iteration 898, loss = 0.29578377\n",
      "Iteration 844, loss = 0.28345381\n",
      "Iteration 406, loss = 0.37663547\n",
      "Iteration 2265, loss = 0.16639518\n",
      "Iteration 845, loss = 0.28332276\n",
      "Iteration 899, loss = 0.29566899\n",
      "Iteration 1950, loss = 0.15640268\n",
      "Iteration 2266, loss = 0.16633635\n",
      "Iteration 846, loss = 0.28324170\n",
      "Iteration 407, loss = 0.37627842\n",
      "Iteration 1065, loss = 0.28441730\n",
      "Iteration 847, loss = 0.28307141\n",
      "Iteration 900, loss = 0.29562128\n",
      "Iteration 2267, loss = 0.16642081\n",
      "Iteration 848, loss = 0.28294179\n",
      "Iteration 408, loss = 0.37615972\n",
      "Iteration 1951, loss = 0.15643332\n",
      "Iteration 901, loss = 0.29550040\n",
      "Iteration 2268, loss = 0.16617326\n",
      "Iteration 1066, loss = 0.28424262\n",
      "Iteration 902, loss = 0.29544475\n",
      "Iteration 849, loss = 0.28290249\n",
      "Iteration 409, loss = 0.37595817\n",
      "Iteration 2269, loss = 0.16606138\n",
      "Iteration 1952, loss = 0.15624370\n",
      "Iteration 903, loss = 0.29533928\n",
      "Iteration 1067, loss = 0.28401646\n",
      "Iteration 850, loss = 0.28273292\n",
      "Iteration 2270, loss = 0.16597748\n",
      "Iteration 904, loss = 0.29522362\n",
      "Iteration 410, loss = 0.37577021\n",
      "Iteration 851, loss = 0.28258528\n",
      "Iteration 1953, loss = 0.15614750\n",
      "Iteration 2271, loss = 0.16599938\n",
      "Iteration 852, loss = 0.28250478\n",
      "Iteration 905, loss = 0.29513319\n",
      "Iteration 1068, loss = 0.28390781\n",
      "Iteration 853, loss = 0.28234395\n",
      "Iteration 2272, loss = 0.16582020\n",
      "Iteration 1954, loss = 0.15612252\n",
      "Iteration 411, loss = 0.37559374\n",
      "Iteration 906, loss = 0.29504313\n",
      "Iteration 854, loss = 0.28228601\n",
      "Iteration 1069, loss = 0.28365620\n",
      "Iteration 2273, loss = 0.16576512\n",
      "Iteration 855, loss = 0.28208478\n",
      "Iteration 907, loss = 0.29501982\n",
      "Iteration 1955, loss = 0.15605183\n",
      "Iteration 856, loss = 0.28212810\n",
      "Iteration 2274, loss = 0.16565917\n",
      "Iteration 412, loss = 0.37541550\n",
      "Iteration 1070, loss = 0.28338901\n",
      "Iteration 908, loss = 0.29486215\n",
      "Iteration 2275, loss = 0.16561929\n",
      "Iteration 857, loss = 0.28185603\n",
      "Iteration 1956, loss = 0.15597936\n",
      "Iteration 413, loss = 0.37534755\n",
      "Iteration 2276, loss = 0.16553266\n",
      "Iteration 858, loss = 0.28177816\n",
      "Iteration 1071, loss = 0.28317328\n",
      "Iteration 909, loss = 0.29479755\n",
      "Iteration 1957, loss = 0.15590641\n",
      "Iteration 859, loss = 0.28160902\n",
      "Iteration 2277, loss = 0.16541455\n",
      "Iteration 860, loss = 0.28159298\n",
      "Iteration 414, loss = 0.37514064\n",
      "Iteration 910, loss = 0.29470828\n",
      "Iteration 1072, loss = 0.28301607\n",
      "Iteration 2278, loss = 0.16531742\n",
      "Iteration 1958, loss = 0.15581701\n",
      "Iteration 861, loss = 0.28134335\n",
      "Iteration 911, loss = 0.29459210\n",
      "Iteration 2279, loss = 0.16541683\n",
      "Iteration 862, loss = 0.28121535\n",
      "Iteration 415, loss = 0.37487711\n",
      "Iteration 1073, loss = 0.28283252\n",
      "Iteration 1959, loss = 0.15580776\n",
      "Iteration 912, loss = 0.29457315\n",
      "Iteration 2280, loss = 0.16525346\n",
      "Iteration 863, loss = 0.28107342\n",
      "Iteration 416, loss = 0.37473987\n",
      "Iteration 913, loss = 0.29441416\n",
      "Iteration 1074, loss = 0.28258094\n",
      "Iteration 864, loss = 0.28096300\n",
      "Iteration 1960, loss = 0.15568826\n",
      "Iteration 865, loss = 0.28084023\n",
      "Iteration 2281, loss = 0.16507655\n",
      "Iteration 914, loss = 0.29431828\n",
      "Iteration 1961, loss = 0.15561690\n",
      "Iteration 417, loss = 0.37458408\n",
      "Iteration 2282, loss = 0.16497931\n",
      "Iteration 1075, loss = 0.28239636\n",
      "Iteration 866, loss = 0.28069206\n",
      "Iteration 915, loss = 0.29422689\n",
      "Iteration 867, loss = 0.28061065\n",
      "Iteration 418, loss = 0.37437851\n",
      "Iteration 1076, loss = 0.28222774\n",
      "Iteration 916, loss = 0.29412665\n",
      "Iteration 1962, loss = 0.15559772\n",
      "Iteration 2283, loss = 0.16498912\n",
      "Iteration 868, loss = 0.28073734\n",
      "Iteration 1077, loss = 0.28193375\n",
      "Iteration 917, loss = 0.29404988\n",
      "Iteration 2284, loss = 0.16482129\n",
      "Iteration 419, loss = 0.37419876\n",
      "Iteration 1963, loss = 0.15552523\n",
      "Iteration 869, loss = 0.28030871\n",
      "Iteration 918, loss = 0.29394749\n",
      "Iteration 1078, loss = 0.28173698\n",
      "Iteration 2285, loss = 0.16474736\n",
      "Iteration 420, loss = 0.37407117\n",
      "Iteration 1964, loss = 0.15543100\n",
      "Iteration 870, loss = 0.28018423\n",
      "Iteration 2286, loss = 0.16467005\n",
      "Iteration 919, loss = 0.29388850\n",
      "Iteration 421, loss = 0.37387844\n",
      "Iteration 2287, loss = 0.16458584\n",
      "Iteration 871, loss = 0.28020951\n",
      "Iteration 1079, loss = 0.28152490\n",
      "Iteration 1965, loss = 0.15536236\n",
      "Iteration 920, loss = 0.29375352\n",
      "Iteration 872, loss = 0.27997574\n",
      "Iteration 422, loss = 0.37373874\n",
      "Iteration 921, loss = 0.29367741\n",
      "Iteration 2288, loss = 0.16453440\n",
      "Iteration 873, loss = 0.27994180\n",
      "Iteration 1080, loss = 0.28128092\n",
      "Iteration 1966, loss = 0.15541598\n",
      "Iteration 922, loss = 0.29357940\n",
      "Iteration 874, loss = 0.27991836\n",
      "Iteration 423, loss = 0.37355256\n",
      "Iteration 875, loss = 0.27958249\n",
      "Iteration 2289, loss = 0.16446593\n",
      "Iteration 923, loss = 0.29350822\n",
      "Iteration 1967, loss = 0.15525947\n",
      "Iteration 1081, loss = 0.28110916\n",
      "Iteration 2290, loss = 0.16437647\n",
      "Iteration 876, loss = 0.27944467\n",
      "Iteration 424, loss = 0.37336157\n",
      "Iteration 1968, loss = 0.15518195\n",
      "Iteration 2291, loss = 0.16425133\n",
      "Iteration 924, loss = 0.29337711\n",
      "Iteration 1082, loss = 0.28085044\n",
      "Iteration 877, loss = 0.27941771\n",
      "Iteration 2292, loss = 0.16417595\n",
      "Iteration 925, loss = 0.29329335\n",
      "Iteration 425, loss = 0.37320825\n",
      "Iteration 1969, loss = 0.15512298\n",
      "Iteration 878, loss = 0.27922621\n",
      "Iteration 1083, loss = 0.28070495\n",
      "Iteration 926, loss = 0.29325522\n",
      "Iteration 2293, loss = 0.16411755\n",
      "Iteration 879, loss = 0.27907470\n",
      "Iteration 426, loss = 0.37305655\n",
      "Iteration 1970, loss = 0.15517435\n",
      "Iteration 927, loss = 0.29312284\n",
      "Iteration 880, loss = 0.27905694\n",
      "Iteration 1084, loss = 0.28046445\n",
      "Iteration 2294, loss = 0.16405255\n",
      "Iteration 427, loss = 0.37297292\n",
      "Iteration 881, loss = 0.27888131\n",
      "Iteration 1971, loss = 0.15502623\n",
      "Iteration 882, loss = 0.27889025\n",
      "Iteration 928, loss = 0.29303178\n",
      "Iteration 883, loss = 0.27859600\n",
      "Iteration 2295, loss = 0.16389801\n",
      "Iteration 1085, loss = 0.28031090\n",
      "Iteration 428, loss = 0.37271807\n",
      "Iteration 929, loss = 0.29297109\n",
      "Iteration 1972, loss = 0.15491836\n",
      "Iteration 2296, loss = 0.16380625\n",
      "Iteration 884, loss = 0.27857714\n",
      "Iteration 930, loss = 0.29285683\n",
      "Iteration 2297, loss = 0.16372363\n",
      "Iteration 1086, loss = 0.28005688\n",
      "Iteration 1973, loss = 0.15489987\n",
      "Iteration 931, loss = 0.29274281\n",
      "Iteration 429, loss = 0.37264997\n",
      "Iteration 885, loss = 0.27848941\n",
      "Iteration 2298, loss = 0.16372204\n",
      "Iteration 932, loss = 0.29268813\n",
      "Iteration 886, loss = 0.27827507\n",
      "Iteration 1087, loss = 0.27986578\n",
      "Iteration 887, loss = 0.27808591\n",
      "Iteration 1974, loss = 0.15482142\n",
      "Iteration 2299, loss = 0.16357687\n",
      "Iteration 430, loss = 0.37237034\n",
      "Iteration 888, loss = 0.27798190\n",
      "Iteration 933, loss = 0.29257038\n",
      "Iteration 1088, loss = 0.27971833\n",
      "Iteration 1975, loss = 0.15477931\n",
      "Iteration 2300, loss = 0.16350120\n",
      "Iteration 889, loss = 0.27788081\n",
      "Iteration 934, loss = 0.29245995\n",
      "Iteration 890, loss = 0.27772322\n",
      "Iteration 431, loss = 0.37218590\n",
      "Iteration 2301, loss = 0.16342740\n",
      "Iteration 891, loss = 0.27757829\n",
      "Iteration 1089, loss = 0.27957559\n",
      "Iteration 935, loss = 0.29240225\n",
      "Iteration 892, loss = 0.27749389\n",
      "Iteration 1976, loss = 0.15470232\n",
      "Iteration 2302, loss = 0.16348921\n",
      "Iteration 432, loss = 0.37208601\n",
      "Iteration 893, loss = 0.27738123\n",
      "Iteration 936, loss = 0.29231453\n",
      "Iteration 2303, loss = 0.16329421\n",
      "Iteration 894, loss = 0.27721255\n",
      "Iteration 433, loss = 0.37184737\n",
      "Iteration 937, loss = 0.29221467\n",
      "Iteration 1090, loss = 0.27928489\n",
      "Iteration 2304, loss = 0.16323289\n",
      "Iteration 895, loss = 0.27707844\n",
      "Iteration 1977, loss = 0.15462463\n",
      "Iteration 2305, loss = 0.16311476\n",
      "Iteration 434, loss = 0.37169851\n",
      "Iteration 1091, loss = 0.27902827\n",
      "Iteration 938, loss = 0.29211090\n",
      "Iteration 896, loss = 0.27691587\n",
      "Iteration 435, loss = 0.37149195\n",
      "Iteration 939, loss = 0.29206664\n",
      "Iteration 1978, loss = 0.15461035\n",
      "Iteration 1092, loss = 0.27883177\n",
      "Iteration 2306, loss = 0.16304224\n",
      "Iteration 897, loss = 0.27685404\n",
      "Iteration 2307, loss = 0.16290944\n",
      "Iteration 1979, loss = 0.15451205\n",
      "Iteration 940, loss = 0.29192940\n",
      "Iteration 898, loss = 0.27669827\n",
      "Iteration 1093, loss = 0.27866057\n",
      "Iteration 436, loss = 0.37133505\n",
      "Iteration 899, loss = 0.27653037\n",
      "Iteration 941, loss = 0.29185591\n",
      "Iteration 2308, loss = 0.16290936\n",
      "Iteration 1094, loss = 0.27840662\n",
      "Iteration 900, loss = 0.27639880\n",
      "Iteration 1980, loss = 0.15453164\n",
      "Iteration 437, loss = 0.37120396\n",
      "Iteration 901, loss = 0.27627163\n",
      "Iteration 2309, loss = 0.16281350\n",
      "Iteration 902, loss = 0.27617099\n",
      "Iteration 942, loss = 0.29175160\n",
      "Iteration 1095, loss = 0.27840453\n",
      "Iteration 2310, loss = 0.16272086\n",
      "Iteration 1981, loss = 0.15450283\n",
      "Iteration 438, loss = 0.37102517\n",
      "Iteration 903, loss = 0.27603368\n",
      "Iteration 2311, loss = 0.16261624\n",
      "Iteration 1096, loss = 0.27811237\n",
      "Iteration 943, loss = 0.29178979\n",
      "Iteration 904, loss = 0.27589593\n",
      "Iteration 2312, loss = 0.16252745\n",
      "Iteration 1982, loss = 0.15445967\n",
      "Iteration 1097, loss = 0.27782905\n",
      "Iteration 439, loss = 0.37083059\n",
      "Iteration 2313, loss = 0.16262857\n",
      "Iteration 905, loss = 0.27575906\n",
      "Iteration 944, loss = 0.29159412\n",
      "Iteration 1983, loss = 0.15429756\n",
      "Iteration 2314, loss = 0.16245473\n",
      "Iteration 945, loss = 0.29145831\n",
      "Iteration 906, loss = 0.27569207\n",
      "Iteration 1098, loss = 0.27767262\n",
      "Iteration 440, loss = 0.37068428\n",
      "Iteration 2315, loss = 0.16229087\n",
      "Iteration 946, loss = 0.29141556\n",
      "Iteration 907, loss = 0.27557221\n",
      "Iteration 1984, loss = 0.15444737\n",
      "Iteration 908, loss = 0.27539903\n",
      "Iteration 1099, loss = 0.27753308\n",
      "Iteration 2316, loss = 0.16223845\n",
      "Iteration 441, loss = 0.37051670\n",
      "Iteration 947, loss = 0.29130235\n",
      "Iteration 909, loss = 0.27544022\n",
      "Iteration 1985, loss = 0.15420011\n",
      "Iteration 948, loss = 0.29120923\n",
      "Iteration 910, loss = 0.27512613\n",
      "Iteration 442, loss = 0.37037551\n",
      "Iteration 2317, loss = 0.16220337\n",
      "Iteration 1100, loss = 0.27715855\n",
      "Iteration 949, loss = 0.29111309\n",
      "Iteration 2318, loss = 0.16203529\n",
      "Iteration 911, loss = 0.27504706\n",
      "Iteration 443, loss = 0.37022075\n",
      "Iteration 1986, loss = 0.15410818\n",
      "Iteration 2319, loss = 0.16197615\n",
      "Iteration 950, loss = 0.29101966\n",
      "Iteration 1101, loss = 0.27705029\n",
      "Iteration 912, loss = 0.27490332\n",
      "Iteration 2320, loss = 0.16188809\n",
      "Iteration 913, loss = 0.27474547\n",
      "Iteration 444, loss = 0.37000291\n",
      "Iteration 951, loss = 0.29091439\n",
      "Iteration 1987, loss = 0.15404364\n",
      "Iteration 914, loss = 0.27461755\n",
      "Iteration 2321, loss = 0.16191995\n",
      "Iteration 1102, loss = 0.27680750\n",
      "Iteration 915, loss = 0.27450550\n",
      "Iteration 445, loss = 0.36988909\n",
      "Iteration 952, loss = 0.29094977\n",
      "Iteration 2322, loss = 0.16175509\n",
      "Iteration 1988, loss = 0.15401859\n",
      "Iteration 916, loss = 0.27440503\n",
      "Iteration 1103, loss = 0.27659825\n",
      "Iteration 953, loss = 0.29073946\n",
      "Iteration 446, loss = 0.36975099\n",
      "Iteration 917, loss = 0.27424526\n",
      "Iteration 2323, loss = 0.16170295\n",
      "Iteration 1989, loss = 0.15390154\n",
      "Iteration 1104, loss = 0.27652813\n",
      "Iteration 954, loss = 0.29067470\n",
      "Iteration 2324, loss = 0.16159415\n",
      "Iteration 447, loss = 0.36954895\n",
      "Iteration 918, loss = 0.27420360\n",
      "Iteration 1990, loss = 0.15385017\n",
      "Iteration 955, loss = 0.29057459\n",
      "Iteration 2325, loss = 0.16151162\n",
      "Iteration 1105, loss = 0.27610194\n",
      "Iteration 919, loss = 0.27397737\n",
      "Iteration 448, loss = 0.36939890\n",
      "Iteration 956, loss = 0.29047144\n",
      "Iteration 2326, loss = 0.16148206\n",
      "Iteration 920, loss = 0.27386992\n",
      "Iteration 1991, loss = 0.15377051\n",
      "Iteration 957, loss = 0.29040929\n",
      "Iteration 1106, loss = 0.27592339\n",
      "Iteration 449, loss = 0.36919739\n",
      "Iteration 921, loss = 0.27374759\n",
      "Iteration 2327, loss = 0.16134736\n",
      "Iteration 922, loss = 0.27362281\n",
      "Iteration 958, loss = 0.29029395\n",
      "Iteration 2328, loss = 0.16127820\n",
      "Iteration 1992, loss = 0.15379315\n",
      "Iteration 1107, loss = 0.27583374\n",
      "Iteration 450, loss = 0.36906057\n",
      "Iteration 923, loss = 0.27348773\n",
      "Iteration 959, loss = 0.29020519\n",
      "Iteration 2329, loss = 0.16119646\n",
      "Iteration 924, loss = 0.27339831\n",
      "Iteration 1993, loss = 0.15364433\n",
      "Iteration 1108, loss = 0.27554432\n",
      "Iteration 960, loss = 0.29010501\n",
      "Iteration 451, loss = 0.36893929\n",
      "Iteration 2330, loss = 0.16108259\n",
      "Iteration 925, loss = 0.27319042\n",
      "Iteration 961, loss = 0.28999546\n",
      "Iteration 1994, loss = 0.15365662\n",
      "Iteration 926, loss = 0.27309474\n",
      "Iteration 1109, loss = 0.27524999\n",
      "Iteration 962, loss = 0.28997493\n",
      "Iteration 452, loss = 0.36878737\n",
      "Iteration 2331, loss = 0.16100011\n",
      "Iteration 927, loss = 0.27298034\n",
      "Iteration 1110, loss = 0.27512039\n",
      "Iteration 1995, loss = 0.15358577\n",
      "Iteration 963, loss = 0.28984554\n",
      "Iteration 2332, loss = 0.16097299\n",
      "Iteration 453, loss = 0.36860575\n",
      "Iteration 928, loss = 0.27281630\n",
      "Iteration 2333, loss = 0.16085062\n",
      "Iteration 964, loss = 0.28974552\n",
      "Iteration 929, loss = 0.27268645\n",
      "Iteration 1996, loss = 0.15351649\n",
      "Iteration 1111, loss = 0.27491330\n",
      "Iteration 965, loss = 0.28966572\n",
      "Iteration 454, loss = 0.36839907\n",
      "Iteration 2334, loss = 0.16078616\n",
      "Iteration 930, loss = 0.27257794\n",
      "Iteration 1997, loss = 0.15346449\n",
      "Iteration 931, loss = 0.27243993\n",
      "Iteration 2335, loss = 0.16068446\n",
      "Iteration 455, loss = 0.36823185\n",
      "Iteration 1998, loss = 0.15356421\n",
      "Iteration 1112, loss = 0.27476485\n",
      "Iteration 966, loss = 0.28956331\n",
      "Iteration 2336, loss = 0.16066136\n",
      "Iteration 932, loss = 0.27228928\n",
      "Iteration 456, loss = 0.36808008\n",
      "Iteration 967, loss = 0.28955423\n",
      "Iteration 1999, loss = 0.15335219\n",
      "Iteration 933, loss = 0.27219850\n",
      "Iteration 2337, loss = 0.16059072\n",
      "Iteration 968, loss = 0.28944480\n",
      "Iteration 1113, loss = 0.27448770\n",
      "Iteration 934, loss = 0.27204599\n",
      "Iteration 457, loss = 0.36789721\n",
      "Iteration 2000, loss = 0.15327434\n",
      "Iteration 969, loss = 0.28927265\n",
      "Iteration 935, loss = 0.27192982\n",
      "Iteration 2338, loss = 0.16050995\n",
      "Iteration 1114, loss = 0.27428603\n",
      "Iteration 936, loss = 0.27183907\n",
      "Iteration 970, loss = 0.28925593\n",
      "Iteration 458, loss = 0.36773746\n",
      "Iteration 937, loss = 0.27166587\n",
      "Iteration 2001, loss = 0.15322524\n",
      "Iteration 2339, loss = 0.16039145\n",
      "Iteration 971, loss = 0.28915174\n",
      "Iteration 2340, loss = 0.16036067\n",
      "Iteration 1115, loss = 0.27401526\n",
      "Iteration 459, loss = 0.36755865\n",
      "Iteration 938, loss = 0.27150629\n",
      "Iteration 2002, loss = 0.15317732\n",
      "Iteration 939, loss = 0.27137136\n",
      "Iteration 2341, loss = 0.16048351\n",
      "Iteration 972, loss = 0.28918554\n",
      "Iteration 460, loss = 0.36738931\n",
      "Iteration 940, loss = 0.27129914\n",
      "Iteration 1116, loss = 0.27387251\n",
      "Iteration 941, loss = 0.27126540\n",
      "Iteration 2342, loss = 0.16025810\n",
      "Iteration 2003, loss = 0.15310863\n",
      "Iteration 1117, loss = 0.27359316\n",
      "Iteration 942, loss = 0.27100370\n",
      "Iteration 973, loss = 0.28895183\n",
      "Iteration 461, loss = 0.36722725\n",
      "Iteration 943, loss = 0.27087874\n",
      "Iteration 2343, loss = 0.16010149\n",
      "Iteration 944, loss = 0.27074486\n",
      "Iteration 974, loss = 0.28883763\n",
      "Iteration 1118, loss = 0.27337908\n",
      "Iteration 2004, loss = 0.15303204\n",
      "Iteration 2344, loss = 0.16003090\n",
      "Iteration 462, loss = 0.36710107\n",
      "Iteration 945, loss = 0.27062930\n",
      "Iteration 2345, loss = 0.15994159\n",
      "Iteration 975, loss = 0.28876945\n",
      "Iteration 1119, loss = 0.27316891\n",
      "Iteration 2005, loss = 0.15299330\n",
      "Iteration 946, loss = 0.27077075\n",
      "Iteration 976, loss = 0.28866047\n",
      "Iteration 947, loss = 0.27034186\n",
      "Iteration 2346, loss = 0.16003793\n",
      "Iteration 463, loss = 0.36702413\n",
      "Iteration 2347, loss = 0.15982603\n",
      "Iteration 948, loss = 0.27024315\n",
      "Iteration 977, loss = 0.28855621\n",
      "Iteration 2006, loss = 0.15290879\n",
      "Iteration 1120, loss = 0.27300549\n",
      "Iteration 2348, loss = 0.15972503\n",
      "Iteration 978, loss = 0.28846146\n",
      "Iteration 949, loss = 0.27006612\n",
      "Iteration 464, loss = 0.36676413\n",
      "Iteration 950, loss = 0.27006685\n",
      "Iteration 979, loss = 0.28836728\n",
      "Iteration 2349, loss = 0.15965602\n",
      "Iteration 2007, loss = 0.15289211\n",
      "Iteration 1121, loss = 0.27288862\n",
      "Iteration 2350, loss = 0.15952908\n",
      "Iteration 980, loss = 0.28835875\n",
      "Iteration 951, loss = 0.26978854\n",
      "Iteration 465, loss = 0.36660412\n",
      "Iteration 2351, loss = 0.15947500\n",
      "Iteration 952, loss = 0.26966311\n",
      "Iteration 981, loss = 0.28821867\n",
      "Iteration 1122, loss = 0.27260011\n",
      "Iteration 2008, loss = 0.15284256\n",
      "Iteration 466, loss = 0.36641413\n",
      "Iteration 953, loss = 0.26962293\n",
      "Iteration 982, loss = 0.28821844\n",
      "Iteration 2352, loss = 0.15936716\n",
      "Iteration 954, loss = 0.26938191\n",
      "Iteration 467, loss = 0.36629546\n",
      "Iteration 1123, loss = 0.27235433\n",
      "Iteration 2353, loss = 0.15932359\n",
      "Iteration 983, loss = 0.28802480\n",
      "Iteration 2009, loss = 0.15274081\n",
      "Iteration 955, loss = 0.26927462\n",
      "Iteration 2354, loss = 0.15922434\n",
      "Iteration 984, loss = 0.28791126\n",
      "Iteration 2010, loss = 0.15265747\n",
      "Iteration 468, loss = 0.36608968\n",
      "Iteration 1124, loss = 0.27221235\n",
      "Iteration 956, loss = 0.26925372\n",
      "Iteration 985, loss = 0.28781528\n",
      "Iteration 2355, loss = 0.15921947\n",
      "Iteration 957, loss = 0.26903143\n",
      "Iteration 2011, loss = 0.15271405\n",
      "Iteration 958, loss = 0.26886944\n",
      "Iteration 1125, loss = 0.27196581\n",
      "Iteration 469, loss = 0.36597437\n",
      "Iteration 2356, loss = 0.15907314\n",
      "Iteration 986, loss = 0.28774991\n",
      "Iteration 959, loss = 0.26872945\n",
      "Iteration 470, loss = 0.36586312\n",
      "Iteration 2012, loss = 0.15262482\n",
      "Iteration 2357, loss = 0.15900939\n",
      "Iteration 1126, loss = 0.27169382\n",
      "Iteration 960, loss = 0.26861097\n",
      "Iteration 987, loss = 0.28766165\n",
      "Iteration 961, loss = 0.26849332\n",
      "Iteration 2358, loss = 0.15897795\n",
      "Iteration 988, loss = 0.28755928\n",
      "Iteration 2013, loss = 0.15254233\n",
      "Iteration 471, loss = 0.36567337\n",
      "Iteration 1127, loss = 0.27163431\n",
      "Iteration 962, loss = 0.26835773\n",
      "Iteration 989, loss = 0.28759917\n",
      "Iteration 2359, loss = 0.15891053\n",
      "Iteration 2014, loss = 0.15244805\n",
      "Iteration 963, loss = 0.26815880\n",
      "Iteration 472, loss = 0.36549337\n",
      "Iteration 990, loss = 0.28740459\n",
      "Iteration 964, loss = 0.26807838\n",
      "Iteration 2360, loss = 0.15887681\n",
      "Iteration 965, loss = 0.26791804\n",
      "Iteration 1128, loss = 0.27133865\n",
      "Iteration 2015, loss = 0.15248413\n",
      "Iteration 991, loss = 0.28727608\n",
      "Iteration 2361, loss = 0.15877698\n",
      "Iteration 966, loss = 0.26777337\n",
      "Iteration 1129, loss = 0.27108319\n",
      "Iteration 473, loss = 0.36533635\n",
      "Iteration 967, loss = 0.26767909\n",
      "Iteration 2016, loss = 0.15238970\n",
      "Iteration 992, loss = 0.28719574\n",
      "Iteration 968, loss = 0.26748684\n",
      "Iteration 2362, loss = 0.15864097\n",
      "Iteration 1130, loss = 0.27089161\n",
      "Iteration 969, loss = 0.26736965\n",
      "Iteration 2363, loss = 0.15853927\n",
      "Iteration 474, loss = 0.36515204\n",
      "Iteration 1131, loss = 0.27067402\n",
      "Iteration 993, loss = 0.28710067\n",
      "Iteration 2017, loss = 0.15241911\n",
      "Iteration 970, loss = 0.26725133\n",
      "Iteration 475, loss = 0.36500031\n",
      "Iteration 2364, loss = 0.15847122\n",
      "Iteration 994, loss = 0.28702036\n",
      "Iteration 1132, loss = 0.27048644\n",
      "Iteration 971, loss = 0.26720490\n",
      "Iteration 2018, loss = 0.15229451\n",
      "Iteration 995, loss = 0.28693530\n",
      "Iteration 2365, loss = 0.15864654\n",
      "Iteration 476, loss = 0.36481868\n",
      "Iteration 2366, loss = 0.15832746\n",
      "Iteration 972, loss = 0.26698377\n",
      "Iteration 2019, loss = 0.15219004\n",
      "Iteration 1133, loss = 0.27024696\n",
      "Iteration 973, loss = 0.26689206\n",
      "Iteration 996, loss = 0.28684292\n",
      "Iteration 2367, loss = 0.15825865\n",
      "Iteration 477, loss = 0.36466257\n",
      "Iteration 2020, loss = 0.15214901\n",
      "Iteration 2368, loss = 0.15821417\n",
      "Iteration 974, loss = 0.26675963\n",
      "Iteration 997, loss = 0.28678645\n",
      "Iteration 975, loss = 0.26658728\n",
      "Iteration 2021, loss = 0.15209052\n",
      "Iteration 1134, loss = 0.27006701\n",
      "Iteration 998, loss = 0.28663972\n",
      "Iteration 478, loss = 0.36449114\n",
      "Iteration 2369, loss = 0.15820134\n",
      "Iteration 976, loss = 0.26643104\n",
      "Iteration 479, loss = 0.36432797\n",
      "Iteration 1135, loss = 0.26993590\n",
      "Iteration 2370, loss = 0.15799997\n",
      "Iteration 999, loss = 0.28653026\n",
      "Iteration 977, loss = 0.26633541\n",
      "Iteration 2022, loss = 0.15208395\n",
      "Iteration 2371, loss = 0.15791763\n",
      "Iteration 480, loss = 0.36416275\n",
      "Iteration 978, loss = 0.26616005\n",
      "Iteration 1000, loss = 0.28651321\n",
      "Iteration 1136, loss = 0.26969520\n",
      "Iteration 2372, loss = 0.15785493\n",
      "Iteration 1001, loss = 0.28641141\n",
      "Iteration 979, loss = 0.26604199\n",
      "Iteration 1137, loss = 0.26950130\n",
      "Iteration 2373, loss = 0.15779487\n",
      "Iteration 2023, loss = 0.15197027\n",
      "Iteration 481, loss = 0.36398992\n",
      "Iteration 1002, loss = 0.28626197\n",
      "Iteration 980, loss = 0.26595101\n",
      "Iteration 2374, loss = 0.15769899\n",
      "Iteration 1003, loss = 0.28625655\n",
      "Iteration 981, loss = 0.26575481\n",
      "Iteration 2024, loss = 0.15191728\n",
      "Iteration 982, loss = 0.26567434\n",
      "Iteration 1004, loss = 0.28610980\n",
      "Iteration 2375, loss = 0.15761302\n",
      "Iteration 1138, loss = 0.26923391\n",
      "Iteration 482, loss = 0.36388258\n",
      "Iteration 2376, loss = 0.15763005\n",
      "Iteration 983, loss = 0.26556980\n",
      "Iteration 1005, loss = 0.28603086\n",
      "Iteration 2025, loss = 0.15195427\n",
      "Iteration 2377, loss = 0.15754750\n",
      "Iteration 984, loss = 0.26538960\n",
      "Iteration 483, loss = 0.36371293\n",
      "Iteration 1139, loss = 0.26901308\n",
      "Iteration 1006, loss = 0.28607758\n",
      "Iteration 985, loss = 0.26520792\n",
      "Iteration 2378, loss = 0.15770197\n",
      "Iteration 2026, loss = 0.15187698\n",
      "Iteration 1140, loss = 0.26890526\n",
      "Iteration 986, loss = 0.26504248\n",
      "Iteration 1007, loss = 0.28583097\n",
      "Iteration 484, loss = 0.36356226\n",
      "Iteration 2379, loss = 0.15729859\n",
      "Iteration 987, loss = 0.26498479\n",
      "Iteration 1008, loss = 0.28575822\n",
      "Iteration 2027, loss = 0.15174261\n",
      "Iteration 988, loss = 0.26486363\n",
      "Iteration 1141, loss = 0.26867112\n",
      "Iteration 2380, loss = 0.15731467\n",
      "Iteration 1009, loss = 0.28564454\n",
      "Iteration 989, loss = 0.26467630\n",
      "Iteration 485, loss = 0.36336972\n",
      "Iteration 1142, loss = 0.26851552\n",
      "Iteration 2381, loss = 0.15726232\n",
      "Iteration 2028, loss = 0.15167942\n",
      "Iteration 1010, loss = 0.28552961Iteration 990, loss = 0.26453378\n",
      "\n",
      "Iteration 1143, loss = 0.26825041\n",
      "Iteration 486, loss = 0.36321405\n",
      "Iteration 2382, loss = 0.15711300\n",
      "Iteration 991, loss = 0.26441421\n",
      "Iteration 1144, loss = 0.26802678\n",
      "Iteration 2029, loss = 0.15165153\n",
      "Iteration 2383, loss = 0.15712079\n",
      "Iteration 487, loss = 0.36310836\n",
      "Iteration 992, loss = 0.26423763\n",
      "Iteration 1011, loss = 0.28544450\n",
      "Iteration 1145, loss = 0.26781624\n",
      "Iteration 2030, loss = 0.15156428\n",
      "Iteration 993, loss = 0.26409282\n",
      "Iteration 1012, loss = 0.28540935\n",
      "Iteration 488, loss = 0.36292485\n",
      "Iteration 994, loss = 0.26396390\n",
      "Iteration 1013, loss = 0.28526932\n",
      "Iteration 2384, loss = 0.15699184\n",
      "Iteration 995, loss = 0.26397177\n",
      "Iteration 2031, loss = 0.15156974\n",
      "Iteration 1146, loss = 0.26762738\n",
      "Iteration 2385, loss = 0.15689337\n",
      "Iteration 1014, loss = 0.28521705\n",
      "Iteration 489, loss = 0.36276470\n",
      "Iteration 996, loss = 0.26371421\n",
      "Iteration 1015, loss = 0.28520237\n",
      "Iteration 2386, loss = 0.15681899\n",
      "Iteration 2032, loss = 0.15148580\n",
      "Iteration 1147, loss = 0.26736774\n",
      "Iteration 490, loss = 0.36255092\n",
      "Iteration 997, loss = 0.26357661\n",
      "Iteration 1016, loss = 0.28502914\n",
      "Iteration 1148, loss = 0.26713427\n",
      "Iteration 998, loss = 0.26341586\n",
      "Iteration 1017, loss = 0.28489994\n",
      "Iteration 2033, loss = 0.15139649\n",
      "Iteration 2387, loss = 0.15681758\n",
      "Iteration 491, loss = 0.36237838\n",
      "Iteration 999, loss = 0.26328312\n",
      "Iteration 1018, loss = 0.28482056\n",
      "Iteration 2388, loss = 0.15670126\n",
      "Iteration 1149, loss = 0.26689915\n",
      "Iteration 1000, loss = 0.26310806\n",
      "Iteration 2034, loss = 0.15145720\n",
      "Iteration 492, loss = 0.36224897\n",
      "Iteration 1019, loss = 0.28471697\n",
      "Iteration 2389, loss = 0.15664151\n",
      "Iteration 1150, loss = 0.26672201\n",
      "Iteration 2035, loss = 0.15147502\n",
      "Iteration 1001, loss = 0.26303094\n",
      "Iteration 493, loss = 0.36206517\n",
      "Iteration 2390, loss = 0.15651238\n",
      "Iteration 1020, loss = 0.28464806\n",
      "Iteration 1151, loss = 0.26651993\n",
      "Iteration 1002, loss = 0.26291226\n",
      "Iteration 2036, loss = 0.15127778\n",
      "Iteration 1003, loss = 0.26275589\n",
      "Iteration 2391, loss = 0.15639756\n",
      "Iteration 1021, loss = 0.28453858\n",
      "Iteration 494, loss = 0.36191661\n",
      "Iteration 1004, loss = 0.26263288\n",
      "Iteration 1152, loss = 0.26632321\n",
      "Iteration 1022, loss = 0.28450660\n",
      "Iteration 2037, loss = 0.15127658\n",
      "Iteration 495, loss = 0.36178798\n",
      "Iteration 2392, loss = 0.15635183\n",
      "Iteration 1005, loss = 0.26253984\n",
      "Iteration 1023, loss = 0.28434210\n",
      "Iteration 1153, loss = 0.26620253\n",
      "Iteration 1006, loss = 0.26240115\n",
      "Iteration 2393, loss = 0.15627815\n",
      "Iteration 496, loss = 0.36156228\n",
      "Iteration 2038, loss = 0.15116422\n",
      "Iteration 2394, loss = 0.15618262\n",
      "Iteration 1024, loss = 0.28428360\n",
      "Iteration 497, loss = 0.36141411\n",
      "Iteration 1007, loss = 0.26231150\n",
      "Iteration 1154, loss = 0.26600820\n",
      "Iteration 2395, loss = 0.15612169\n",
      "Iteration 1008, loss = 0.26213487\n",
      "Iteration 1025, loss = 0.28419597\n",
      "Iteration 2039, loss = 0.15125458\n",
      "Iteration 1155, loss = 0.26570676\n",
      "Iteration 1009, loss = 0.26207668\n",
      "Iteration 1026, loss = 0.28409535\n",
      "Iteration 2396, loss = 0.15611321\n",
      "Iteration 498, loss = 0.36125208\n",
      "Iteration 2040, loss = 0.15107872\n",
      "Iteration 1010, loss = 0.26173526\n",
      "Iteration 1156, loss = 0.26566013\n",
      "Iteration 2397, loss = 0.15603575\n",
      "Iteration 1027, loss = 0.28402370\n",
      "Iteration 1011, loss = 0.26160357\n",
      "Iteration 499, loss = 0.36112691\n",
      "Iteration 2041, loss = 0.15099097\n",
      "Iteration 1012, loss = 0.26152505\n",
      "Iteration 1157, loss = 0.26527853\n",
      "Iteration 1028, loss = 0.28390127\n",
      "Iteration 2398, loss = 0.15593918\n",
      "Iteration 500, loss = 0.36092840\n",
      "Iteration 1013, loss = 0.26133668\n",
      "Iteration 2042, loss = 0.15099409\n",
      "Iteration 2399, loss = 0.15599242\n",
      "Iteration 1158, loss = 0.26506377\n",
      "Iteration 1029, loss = 0.28382130\n",
      "Iteration 1014, loss = 0.26125593\n",
      "Iteration 2043, loss = 0.15095772\n",
      "Iteration 501, loss = 0.36082516\n",
      "Iteration 1015, loss = 0.26111303\n",
      "Iteration 1030, loss = 0.28370489\n",
      "Iteration 2400, loss = 0.15579124\n",
      "Iteration 2044, loss = 0.15084066\n",
      "Iteration 1159, loss = 0.26486253\n",
      "Iteration 1031, loss = 0.28365051\n",
      "Iteration 1016, loss = 0.26092429\n",
      "Iteration 502, loss = 0.36065322\n",
      "Iteration 2401, loss = 0.15571578\n",
      "Iteration 1032, loss = 0.28354941\n",
      "Iteration 1017, loss = 0.26087609\n",
      "Iteration 1160, loss = 0.26473891\n",
      "Iteration 2045, loss = 0.15078250\n",
      "Iteration 2402, loss = 0.15560584\n",
      "Iteration 1018, loss = 0.26069032\n",
      "Iteration 1033, loss = 0.28343353\n",
      "Iteration 2403, loss = 0.15551981\n",
      "Iteration 2046, loss = 0.15077605\n",
      "Iteration 503, loss = 0.36046177\n",
      "Iteration 1019, loss = 0.26052163\n",
      "Iteration 1034, loss = 0.28334127\n",
      "Iteration 1020, loss = 0.26037935\n",
      "Iteration 1161, loss = 0.26447611\n",
      "Iteration 1021, loss = 0.26023087\n",
      "Iteration 2047, loss = 0.15072851\n",
      "Iteration 504, loss = 0.36029878\n",
      "Iteration 2404, loss = 0.15548209\n",
      "Iteration 1035, loss = 0.28327326\n",
      "Iteration 1022, loss = 0.26008985\n",
      "Iteration 2405, loss = 0.15550922\n",
      "Iteration 1162, loss = 0.26423669\n",
      "Iteration 1023, loss = 0.25998032\n",
      "Iteration 2048, loss = 0.15063553\n",
      "Iteration 1036, loss = 0.28317413\n",
      "Iteration 2406, loss = 0.15538462\n",
      "Iteration 505, loss = 0.36014395\n",
      "Iteration 1024, loss = 0.25984719\n",
      "Iteration 1163, loss = 0.26402192\n",
      "Iteration 2407, loss = 0.15522642\n",
      "Iteration 1025, loss = 0.25965987\n",
      "Iteration 1037, loss = 0.28314807\n",
      "Iteration 1164, loss = 0.26384706\n",
      "Iteration 506, loss = 0.35998714\n",
      "Iteration 1026, loss = 0.25954659\n",
      "Iteration 2049, loss = 0.15064335\n",
      "Iteration 2408, loss = 0.15517212\n",
      "Iteration 1038, loss = 0.28312249\n",
      "Iteration 1165, loss = 0.26357145\n",
      "Iteration 1027, loss = 0.25945954\n",
      "Iteration 2409, loss = 0.15510671\n",
      "Iteration 1039, loss = 0.28289577\n",
      "Iteration 1028, loss = 0.25925133\n",
      "Iteration 2050, loss = 0.15053829\n",
      "Iteration 1166, loss = 0.26343735\n",
      "Iteration 507, loss = 0.35991317\n",
      "Iteration 1029, loss = 0.25907398\n",
      "Iteration 2410, loss = 0.15503969\n",
      "Iteration 1040, loss = 0.28281005\n",
      "Iteration 2411, loss = 0.15495894\n",
      "Iteration 1030, loss = 0.25903556\n",
      "Iteration 2051, loss = 0.15052368\n",
      "Iteration 1167, loss = 0.26315715\n",
      "Iteration 1031, loss = 0.25882464\n",
      "Iteration 1041, loss = 0.28272856\n",
      "Iteration 2412, loss = 0.15489368\n",
      "Iteration 508, loss = 0.35966213\n",
      "Iteration 1032, loss = 0.25883975\n",
      "Iteration 2413, loss = 0.15480198\n",
      "Iteration 1042, loss = 0.28262700\n",
      "Iteration 2052, loss = 0.15040603\n",
      "Iteration 1168, loss = 0.26300777\n",
      "Iteration 2414, loss = 0.15477406\n",
      "Iteration 1033, loss = 0.25851900\n",
      "Iteration 509, loss = 0.35951454\n",
      "Iteration 1034, loss = 0.25844424\n",
      "Iteration 1169, loss = 0.26275329\n",
      "Iteration 1035, loss = 0.25831968\n",
      "Iteration 2053, loss = 0.15037067\n",
      "Iteration 1043, loss = 0.28254262\n",
      "Iteration 510, loss = 0.35943056\n",
      "Iteration 1036, loss = 0.25828308\n",
      "Iteration 1044, loss = 0.28246059\n",
      "Iteration 2415, loss = 0.15466246\n",
      "Iteration 2054, loss = 0.15048482\n",
      "Iteration 1037, loss = 0.25807109\n",
      "Iteration 1170, loss = 0.26252022\n",
      "Iteration 1045, loss = 0.28234494\n",
      "Iteration 511, loss = 0.35916035\n",
      "Iteration 1038, loss = 0.25794001\n",
      "Iteration 2416, loss = 0.15460436\n",
      "Iteration 1171, loss = 0.26235557\n",
      "Iteration 2055, loss = 0.15026011\n",
      "Iteration 1039, loss = 0.25775222\n",
      "Iteration 1046, loss = 0.28226775\n",
      "Iteration 512, loss = 0.35901953\n",
      "Iteration 1040, loss = 0.25765013\n",
      "Iteration 2056, loss = 0.15028286\n",
      "Iteration 2417, loss = 0.15463656\n",
      "Iteration 1041, loss = 0.25749958\n",
      "Iteration 513, loss = 0.35892953\n",
      "Iteration 1047, loss = 0.28225706\n",
      "Iteration 1042, loss = 0.25727362\n",
      "Iteration 1172, loss = 0.26210896Iteration 1043, loss = 0.25720805\n",
      "\n",
      "Iteration 2418, loss = 0.15448766\n",
      "Iteration 1048, loss = 0.28207429\n",
      "Iteration 2057, loss = 0.15021493\n",
      "Iteration 1044, loss = 0.25697206\n",
      "Iteration 1049, loss = 0.28197989\n",
      "Iteration 2419, loss = 0.15442870\n",
      "Iteration 514, loss = 0.35873249\n",
      "Iteration 1173, loss = 0.26203615Iteration 1045, loss = 0.25689664\n",
      "\n",
      "Iteration 1050, loss = 0.28191463\n",
      "Iteration 515, loss = 0.35854036\n",
      "Iteration 2058, loss = 0.15009926\n",
      "Iteration 1046, loss = 0.25674797\n",
      "Iteration 2420, loss = 0.15430495\n",
      "Iteration 1174, loss = 0.26183980\n",
      "Iteration 1051, loss = 0.28181578\n",
      "Iteration 1047, loss = 0.25657442\n",
      "Iteration 2421, loss = 0.15423434\n",
      "Iteration 1175, loss = 0.26150609\n",
      "Iteration 1048, loss = 0.25639203\n",
      "Iteration 516, loss = 0.35850184\n",
      "Iteration 1049, loss = 0.25625109\n",
      "Iteration 2059, loss = 0.15009109\n",
      "Iteration 2422, loss = 0.15417725\n",
      "Iteration 1052, loss = 0.28174501\n",
      "Iteration 1176, loss = 0.26130974\n",
      "Iteration 1050, loss = 0.25641592\n",
      "Iteration 2060, loss = 0.14999806\n",
      "Iteration 2423, loss = 0.15413778\n",
      "Iteration 1053, loss = 0.28162493\n",
      "Iteration 1051, loss = 0.25604522\n",
      "Iteration 517, loss = 0.35822105\n",
      "Iteration 1052, loss = 0.25597624\n",
      "Iteration 2424, loss = 0.15407325\n",
      "Iteration 1177, loss = 0.26107137\n",
      "Iteration 1054, loss = 0.28157502\n",
      "Iteration 2061, loss = 0.15007989\n",
      "Iteration 1053, loss = 0.25578602\n",
      "Iteration 1054, loss = 0.25555788\n",
      "Iteration 1055, loss = 0.28146464\n",
      "Iteration 1178, loss = 0.26084138\n",
      "Iteration 2425, loss = 0.15393046\n",
      "Iteration 518, loss = 0.35808069\n",
      "Iteration 1056, loss = 0.28136853\n",
      "Iteration 1055, loss = 0.25540396\n",
      "Iteration 2062, loss = 0.14989051\n",
      "Iteration 2426, loss = 0.15386859\n",
      "Iteration 519, loss = 0.35806156\n",
      "Iteration 1056, loss = 0.25536089\n",
      "Iteration 2063, loss = 0.14986743\n",
      "Iteration 1179, loss = 0.26062635\n",
      "Iteration 2427, loss = 0.15384018\n",
      "Iteration 1057, loss = 0.28136098\n",
      "Iteration 1057, loss = 0.25525275\n",
      "Iteration 1058, loss = 0.28119168\n",
      "Iteration 520, loss = 0.35775617\n",
      "Iteration 1058, loss = 0.25501799\n",
      "Iteration 1180, loss = 0.26044986\n",
      "Iteration 2064, loss = 0.14979089\n",
      "Iteration 2428, loss = 0.15374308\n",
      "Iteration 1059, loss = 0.25500309\n",
      "Iteration 2429, loss = 0.15366566\n",
      "Iteration 1059, loss = 0.28115231\n",
      "Iteration 521, loss = 0.35755022\n",
      "Iteration 1060, loss = 0.25471415\n",
      "Iteration 1181, loss = 0.26028656\n",
      "Iteration 2065, loss = 0.14979808\n",
      "Iteration 1060, loss = 0.28099057\n",
      "Iteration 2430, loss = 0.15360444\n",
      "Iteration 1061, loss = 0.25454592\n",
      "Iteration 522, loss = 0.35745104\n",
      "Iteration 1062, loss = 0.25451566\n",
      "Iteration 1182, loss = 0.26003590\n",
      "Iteration 2431, loss = 0.15357204\n",
      "Iteration 2066, loss = 0.14968855\n",
      "Iteration 1061, loss = 0.28091825\n",
      "Iteration 523, loss = 0.35723439\n",
      "Iteration 1063, loss = 0.25431376\n",
      "Iteration 1062, loss = 0.28082765\n",
      "Iteration 1183, loss = 0.25982262\n",
      "Iteration 2432, loss = 0.15342289\n",
      "Iteration 1064, loss = 0.25411166\n",
      "Iteration 2067, loss = 0.14968311\n",
      "Iteration 1065, loss = 0.25416827\n",
      "Iteration 1063, loss = 0.28071998\n",
      "Iteration 524, loss = 0.35708188\n",
      "Iteration 1184, loss = 0.25966849\n",
      "Iteration 2433, loss = 0.15338976\n",
      "Iteration 1066, loss = 0.25386368\n",
      "Iteration 1067, loss = 0.25390350\n",
      "Iteration 1064, loss = 0.28063272\n",
      "Iteration 2434, loss = 0.15331846\n",
      "Iteration 1185, loss = 0.25943142\n",
      "Iteration 1068, loss = 0.25357844\n",
      "Iteration 2068, loss = 0.14964547\n",
      "Iteration 525, loss = 0.35693542\n",
      "Iteration 1065, loss = 0.28062088\n",
      "Iteration 1069, loss = 0.25361617\n",
      "Iteration 2435, loss = 0.15324763\n",
      "Iteration 1070, loss = 0.25333333\n",
      "Iteration 1066, loss = 0.28053751\n",
      "Iteration 526, loss = 0.35674546\n",
      "Iteration 2069, loss = 0.14953778\n",
      "Iteration 1186, loss = 0.25936894\n",
      "Iteration 1071, loss = 0.25325040\n",
      "Iteration 1067, loss = 0.28042269\n",
      "Iteration 2436, loss = 0.15317134\n",
      "Iteration 2070, loss = 0.14955699\n",
      "Iteration 1068, loss = 0.28028796\n",
      "Iteration 1072, loss = 0.25297619\n",
      "Iteration 2437, loss = 0.15307945\n",
      "Iteration 527, loss = 0.35661142\n",
      "Iteration 1187, loss = 0.25903208\n",
      "Iteration 1073, loss = 0.25281809\n",
      "Iteration 1069, loss = 0.28021712\n",
      "Iteration 1074, loss = 0.25275083\n",
      "Iteration 2438, loss = 0.15301964\n",
      "Iteration 528, loss = 0.35655395\n",
      "Iteration 1070, loss = 0.28013596\n",
      "Iteration 2071, loss = 0.14956029\n",
      "Iteration 1188, loss = 0.25877670\n",
      "Iteration 1075, loss = 0.25280954\n",
      "Iteration 2439, loss = 0.15296336\n",
      "Iteration 529, loss = 0.35632142\n",
      "Iteration 1076, loss = 0.25249293\n",
      "Iteration 1071, loss = 0.28004691\n",
      "Iteration 2072, loss = 0.14937812\n",
      "Iteration 1077, loss = 0.25227545\n",
      "Iteration 2440, loss = 0.15287297\n",
      "Iteration 1189, loss = 0.25860522\n",
      "Iteration 1078, loss = 0.25213209\n",
      "Iteration 1072, loss = 0.27991237\n",
      "Iteration 530, loss = 0.35614244\n",
      "Iteration 2441, loss = 0.15283263\n",
      "Iteration 1190, loss = 0.25845304\n",
      "Iteration 2073, loss = 0.14936310\n",
      "Iteration 1073, loss = 0.27982853\n",
      "Iteration 1079, loss = 0.25198182\n",
      "Iteration 1080, loss = 0.25193952\n",
      "Iteration 531, loss = 0.35600198\n",
      "Iteration 2442, loss = 0.15274310\n",
      "Iteration 1074, loss = 0.27974099\n",
      "Iteration 1191, loss = 0.25817266\n",
      "Iteration 1081, loss = 0.25170189\n",
      "Iteration 1082, loss = 0.25171125\n",
      "Iteration 2074, loss = 0.14930914\n",
      "Iteration 2443, loss = 0.15268247\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1075, loss = 0.27976544\n",
      "Iteration 1083, loss = 0.25143706\n",
      "Iteration 532, loss = 0.35581048\n",
      "Iteration 1192, loss = 0.25802702\n",
      "Iteration 1076, loss = 0.27964948\n",
      "Iteration 1084, loss = 0.25136714\n",
      "Iteration 2075, loss = 0.14922228\n",
      "Iteration 1085, loss = 0.25111165\n",
      "Iteration 533, loss = 0.35574469\n",
      "Iteration 1077, loss = 0.27950293\n",
      "Iteration 1193, loss = 0.25781389\n",
      "Iteration 1086, loss = 0.25096637\n",
      "Iteration 2076, loss = 0.14927745\n",
      "Iteration 1078, loss = 0.27937329\n",
      "Iteration 534, loss = 0.35554102\n",
      "Iteration 1087, loss = 0.25094606\n",
      "Iteration 1194, loss = 0.25767891\n",
      "Iteration 2077, loss = 0.14920383\n",
      "Iteration 1088, loss = 0.25078337\n",
      "Iteration 1079, loss = 0.27928146\n",
      "Iteration 1089, loss = 0.25078116\n",
      "Iteration 535, loss = 0.35533569\n",
      "Iteration 1195, loss = 0.25744169\n",
      "Iteration 2078, loss = 0.14917967\n",
      "Iteration 1090, loss = 0.25061375\n",
      "Iteration 1080, loss = 0.27923883\n",
      "Iteration 1091, loss = 0.25022985\n",
      "Iteration 536, loss = 0.35515107\n",
      "Iteration 1081, loss = 0.27915079\n",
      "Iteration 1196, loss = 0.25715853\n",
      "Iteration 1092, loss = 0.25019229\n",
      "Iteration 2079, loss = 0.14910050\n",
      "Iteration 1082, loss = 0.27902548\n",
      "Iteration 1093, loss = 0.25007238\n",
      "Iteration 537, loss = 0.35506595\n",
      "Iteration 1197, loss = 0.25702003\n",
      "Iteration 1083, loss = 0.27906436\n",
      "Iteration 1094, loss = 0.24981423\n",
      "Iteration 2080, loss = 0.14899936\n",
      "Iteration 538, loss = 0.35486243\n",
      "Iteration 1095, loss = 0.24972276\n",
      "Iteration 1084, loss = 0.27891575\n",
      "Iteration 1096, loss = 0.24959090\n",
      "Iteration 2081, loss = 0.14895739\n",
      "Iteration 1198, loss = 0.25679011\n",
      "Iteration 539, loss = 0.35471782\n",
      "Iteration 1097, loss = 0.24937389\n",
      "Iteration 1085, loss = 0.27875401\n",
      "Iteration 1098, loss = 0.24919724\n",
      "Iteration 1086, loss = 0.27868038\n",
      "Iteration 540, loss = 0.35454528\n",
      "Iteration 2082, loss = 0.14898310\n",
      "Iteration 1099, loss = 0.24906350\n",
      "Iteration 1199, loss = 0.25661080\n",
      "Iteration 1087, loss = 0.27859975\n",
      "Iteration 1100, loss = 0.24889524\n",
      "Iteration 1088, loss = 0.27848745\n",
      "Iteration 2083, loss = 0.14892017\n",
      "Iteration 1101, loss = 0.24882473\n",
      "Iteration 1200, loss = 0.25632499\n",
      "Iteration 541, loss = 0.35436275\n",
      "Iteration 1102, loss = 0.24877675\n",
      "Iteration 1089, loss = 0.27838571\n",
      "Iteration 2084, loss = 0.14881541\n",
      "Iteration 1103, loss = 0.24871501\n",
      "Iteration 542, loss = 0.35427557\n",
      "Iteration 1104, loss = 0.24843373\n",
      "Iteration 1201, loss = 0.25622173\n",
      "Iteration 1090, loss = 0.27831209\n",
      "Iteration 1105, loss = 0.24823901\n",
      "Iteration 2085, loss = 0.14881797\n",
      "Iteration 1202, loss = 0.25594235\n",
      "Iteration 1091, loss = 0.27824308\n",
      "Iteration 543, loss = 0.35405950\n",
      "Iteration 1106, loss = 0.24829273\n",
      "Iteration 1107, loss = 0.24813791\n",
      "Iteration 2086, loss = 0.14870727\n",
      "Iteration 1092, loss = 0.27811486\n",
      "Iteration 544, loss = 0.35389512\n",
      "Iteration 1108, loss = 0.24776637\n",
      "Iteration 1203, loss = 0.25575554\n",
      "Iteration 1109, loss = 0.24760943\n",
      "Iteration 1093, loss = 0.27803419\n",
      "Iteration 2087, loss = 0.14865064\n",
      "Iteration 545, loss = 0.35375358\n",
      "Iteration 1110, loss = 0.24748212\n",
      "Iteration 1204, loss = 0.25556629\n",
      "Iteration 1094, loss = 0.27795372\n",
      "Iteration 1111, loss = 0.24734477\n",
      "Iteration 546, loss = 0.35359669\n",
      "Iteration 2088, loss = 0.14862007\n",
      "Iteration 1095, loss = 0.27789056\n",
      "Iteration 1112, loss = 0.24717760\n",
      "Iteration 1205, loss = 0.25540633\n",
      "Iteration 547, loss = 0.35345984\n",
      "Iteration 1096, loss = 0.27775960\n",
      "Iteration 1113, loss = 0.24705023\n",
      "Iteration 2089, loss = 0.14866441\n",
      "Iteration 1206, loss = 0.25537003\n",
      "Iteration 1114, loss = 0.24688990\n",
      "Iteration 1097, loss = 0.27767581\n",
      "Iteration 548, loss = 0.35337590\n",
      "Iteration 1115, loss = 0.24672103\n",
      "Iteration 1207, loss = 0.25494419\n",
      "Iteration 2090, loss = 0.14854332\n",
      "Iteration 1116, loss = 0.24659786\n",
      "Iteration 1098, loss = 0.27772628\n",
      "Iteration 549, loss = 0.35315509\n",
      "Iteration 1117, loss = 0.24651108\n",
      "Iteration 1099, loss = 0.27749842\n",
      "Iteration 2091, loss = 0.14846946\n",
      "Iteration 1208, loss = 0.25475124\n",
      "Iteration 1118, loss = 0.24632730\n",
      "Iteration 550, loss = 0.35292735\n",
      "Iteration 1100, loss = 0.27742588\n",
      "Iteration 1119, loss = 0.24621149\n",
      "Iteration 2092, loss = 0.14845462\n",
      "Iteration 1209, loss = 0.25450036\n",
      "Iteration 1120, loss = 0.24601278\n",
      "Iteration 1101, loss = 0.27733463\n",
      "Iteration 551, loss = 0.35277838\n",
      "Iteration 2093, loss = 0.14839491\n",
      "Iteration 1102, loss = 0.27728667\n",
      "Iteration 1121, loss = 0.24586834\n",
      "Iteration 1210, loss = 0.25429484\n",
      "Iteration 1122, loss = 0.24576754\n",
      "Iteration 2094, loss = 0.14834960\n",
      "Iteration 1103, loss = 0.27718443\n",
      "Iteration 552, loss = 0.35271092\n",
      "Iteration 1123, loss = 0.24558142\n",
      "Iteration 1104, loss = 0.27706537\n",
      "Iteration 1124, loss = 0.24560295\n",
      "Iteration 1211, loss = 0.25425080\n",
      "Iteration 2095, loss = 0.14829310\n",
      "Iteration 1105, loss = 0.27696870\n",
      "Iteration 553, loss = 0.35250844\n",
      "Iteration 1212, loss = 0.25397816\n",
      "Iteration 1125, loss = 0.24554160\n",
      "Iteration 1126, loss = 0.24522576\n",
      "Iteration 1106, loss = 0.27692517\n",
      "Iteration 2096, loss = 0.14822028\n",
      "Iteration 1107, loss = 0.27678991\n",
      "Iteration 1127, loss = 0.24522982\n",
      "Iteration 554, loss = 0.35231940\n",
      "Iteration 1128, loss = 0.24489035\n",
      "Iteration 1213, loss = 0.25367885\n",
      "Iteration 1108, loss = 0.27674481\n",
      "Iteration 1129, loss = 0.24477077\n",
      "Iteration 555, loss = 0.35218027\n",
      "Iteration 1130, loss = 0.24455006\n",
      "Iteration 2097, loss = 0.14821650\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1109, loss = 0.27660728\n",
      "Iteration 1214, loss = 0.25346328\n",
      "Iteration 1131, loss = 0.24436928\n",
      "Iteration 556, loss = 0.35204623\n",
      "Iteration 1110, loss = 0.27659737\n",
      "Iteration 1132, loss = 0.24440808\n",
      "Iteration 1133, loss = 0.24412916\n",
      "Iteration 1215, loss = 0.25331133\n",
      "Iteration 1134, loss = 0.24396134\n",
      "Iteration 1111, loss = 0.27644381\n",
      "Iteration 557, loss = 0.35189828\n",
      "Iteration 1112, loss = 0.27643118\n",
      "Iteration 1135, loss = 0.24391828\n",
      "Iteration 1216, loss = 0.25308830\n",
      "Iteration 558, loss = 0.35169240\n",
      "Iteration 1136, loss = 0.24376347\n",
      "Iteration 1113, loss = 0.27625370\n",
      "Iteration 1114, loss = 0.27617938\n",
      "Iteration 1137, loss = 0.24357489\n",
      "Iteration 559, loss = 0.35159723\n",
      "Iteration 1217, loss = 0.25286636\n",
      "Iteration 1138, loss = 0.24337861\n",
      "Iteration 1115, loss = 0.27608128\n",
      "Iteration 560, loss = 0.35135840\n",
      "Iteration 1116, loss = 0.27610062\n",
      "Iteration 1139, loss = 0.24360526\n",
      "Iteration 1218, loss = 0.25266251\n",
      "Iteration 561, loss = 0.35122399\n",
      "Iteration 1117, loss = 0.27594518\n",
      "Iteration 1140, loss = 0.24305253\n",
      "Iteration 1141, loss = 0.24289824\n",
      "Iteration 1118, loss = 0.27583676\n",
      "Iteration 562, loss = 0.35106689\n",
      "Iteration 1219, loss = 0.25244549\n",
      "Iteration 1142, loss = 0.24278408\n",
      "Iteration 1143, loss = 0.24270214\n",
      "Iteration 1119, loss = 0.27580273\n",
      "Iteration 1220, loss = 0.25222847\n",
      "Iteration 1144, loss = 0.24242370\n",
      "Iteration 563, loss = 0.35091514\n",
      "Iteration 1120, loss = 0.27564475\n",
      "Iteration 1145, loss = 0.24227521\n",
      "Iteration 1221, loss = 0.25212243\n",
      "Iteration 1146, loss = 0.24219796\n",
      "Iteration 564, loss = 0.35079040\n",
      "Iteration 1121, loss = 0.27554899\n",
      "Iteration 1147, loss = 0.24200402\n",
      "Iteration 1222, loss = 0.25183122\n",
      "Iteration 1122, loss = 0.27547585\n",
      "Iteration 1148, loss = 0.24188976\n",
      "Iteration 565, loss = 0.35061464\n",
      "Iteration 1223, loss = 0.25173082\n",
      "Iteration 1149, loss = 0.24174783\n",
      "Iteration 1123, loss = 0.27537774\n",
      "Iteration 1150, loss = 0.24154468\n",
      "Iteration 566, loss = 0.35042958\n",
      "Iteration 1124, loss = 0.27528294\n",
      "Iteration 1224, loss = 0.25152241\n",
      "Iteration 1151, loss = 0.24143462\n",
      "Iteration 1125, loss = 0.27519422\n",
      "Iteration 1152, loss = 0.24130213\n",
      "Iteration 567, loss = 0.35031417\n",
      "Iteration 1225, loss = 0.25122335\n",
      "Iteration 1153, loss = 0.24111166\n",
      "Iteration 1126, loss = 0.27511105\n",
      "Iteration 568, loss = 0.35025004\n",
      "Iteration 1154, loss = 0.24126900\n",
      "Iteration 1127, loss = 0.27501637\n",
      "Iteration 1226, loss = 0.25106626\n",
      "Iteration 1155, loss = 0.24079655\n",
      "Iteration 569, loss = 0.35002408\n",
      "Iteration 1156, loss = 0.24074675\n",
      "Iteration 1128, loss = 0.27491755\n",
      "Iteration 1157, loss = 0.24045152\n",
      "Iteration 1227, loss = 0.25106700\n",
      "Iteration 570, loss = 0.34983603\n",
      "Iteration 1129, loss = 0.27484436\n",
      "Iteration 1158, loss = 0.24034644\n",
      "Iteration 1228, loss = 0.25063295\n",
      "Iteration 1159, loss = 0.24028818\n",
      "Iteration 1130, loss = 0.27477886\n",
      "Iteration 571, loss = 0.34971294\n",
      "Iteration 1160, loss = 0.24004681\n",
      "Iteration 1161, loss = 0.23988297\n",
      "Iteration 1131, loss = 0.27472920\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1229, loss = 0.25044483\n",
      "Iteration 1162, loss = 0.23965952\n",
      "Iteration 572, loss = 0.34966394\n",
      "Iteration 1163, loss = 0.23963019\n",
      "Iteration 1230, loss = 0.25031707\n",
      "Iteration 1164, loss = 0.23938731\n",
      "Iteration 1165, loss = 0.23936430\n",
      "Iteration 573, loss = 0.34935809\n",
      "Iteration 1231, loss = 0.25000648\n",
      "Iteration 1166, loss = 0.23910074\n",
      "Iteration 1167, loss = 0.23899485\n",
      "Iteration 574, loss = 0.34925546\n",
      "Iteration 1232, loss = 0.24981934\n",
      "Iteration 1168, loss = 0.23883202\n",
      "Iteration 1169, loss = 0.23884071\n",
      "Iteration 575, loss = 0.34905398\n",
      "Iteration 1170, loss = 0.23844617\n",
      "Iteration 1233, loss = 0.24969042\n",
      "Iteration 576, loss = 0.34892771\n",
      "Iteration 1171, loss = 0.23841772\n",
      "Iteration 577, loss = 0.34874292\n",
      "Iteration 1172, loss = 0.23814049\n",
      "Iteration 1234, loss = 0.24952630\n",
      "Iteration 1173, loss = 0.23814347\n",
      "Iteration 578, loss = 0.34859730\n",
      "Iteration 1235, loss = 0.24917667\n",
      "Iteration 1174, loss = 0.23797798\n",
      "Iteration 1175, loss = 0.23771576\n",
      "Iteration 1236, loss = 0.24907655\n",
      "Iteration 579, loss = 0.34846138\n",
      "Iteration 1176, loss = 0.23753691\n",
      "Iteration 1177, loss = 0.23753039\n",
      "Iteration 1237, loss = 0.24902527\n",
      "Iteration 1178, loss = 0.23725097\n",
      "Iteration 580, loss = 0.34838821\n",
      "Iteration 1179, loss = 0.23706209\n",
      "Iteration 1238, loss = 0.24863374\n",
      "Iteration 1180, loss = 0.23693964\n",
      "Iteration 581, loss = 0.34815930\n",
      "Iteration 1181, loss = 0.23684221\n",
      "Iteration 1182, loss = 0.23665158Iteration 1239, loss = 0.24842446\n",
      "\n",
      "Iteration 582, loss = 0.34801766\n",
      "Iteration 1183, loss = 0.23650571\n",
      "Iteration 1240, loss = 0.24822399\n",
      "Iteration 1184, loss = 0.23642658\n",
      "Iteration 583, loss = 0.34787663\n",
      "Iteration 1185, loss = 0.23613052\n",
      "Iteration 1241, loss = 0.24798716\n",
      "Iteration 584, loss = 0.34767203\n",
      "Iteration 1186, loss = 0.23609581\n",
      "Iteration 1187, loss = 0.23582826\n",
      "Iteration 1242, loss = 0.24784250\n",
      "Iteration 585, loss = 0.34755780\n",
      "Iteration 1188, loss = 0.23569271\n",
      "Iteration 1189, loss = 0.23559294\n",
      "Iteration 1243, loss = 0.24759859\n",
      "Iteration 586, loss = 0.34734462\n",
      "Iteration 1190, loss = 0.23538247\n",
      "Iteration 1191, loss = 0.23523877\n",
      "Iteration 587, loss = 0.34726350\n",
      "Iteration 1192, loss = 0.23510384\n",
      "Iteration 1244, loss = 0.24756844\n",
      "Iteration 1193, loss = 0.23511200\n",
      "Iteration 588, loss = 0.34706682\n",
      "Iteration 1194, loss = 0.23471965\n",
      "Iteration 1245, loss = 0.24714524\n",
      "Iteration 1195, loss = 0.23454370\n",
      "Iteration 589, loss = 0.34692531\n",
      "Iteration 1196, loss = 0.23434441\n",
      "Iteration 1246, loss = 0.24701772\n",
      "Iteration 1197, loss = 0.23436813\n",
      "Iteration 590, loss = 0.34674375\n",
      "Iteration 1198, loss = 0.23412062\n",
      "Iteration 1247, loss = 0.24674938\n",
      "Iteration 1199, loss = 0.23409641\n",
      "Iteration 591, loss = 0.34660872\n",
      "Iteration 1248, loss = 0.24665005\n",
      "Iteration 1200, loss = 0.23384520\n",
      "Iteration 1201, loss = 0.23371581\n",
      "Iteration 1249, loss = 0.24634635\n",
      "Iteration 1202, loss = 0.23352051\n",
      "Iteration 592, loss = 0.34646795\n",
      "Iteration 1203, loss = 0.23345127\n",
      "Iteration 1250, loss = 0.24624001\n",
      "Iteration 1204, loss = 0.23318248\n",
      "Iteration 593, loss = 0.34627804\n",
      "Iteration 1205, loss = 0.23297652\n",
      "Iteration 1251, loss = 0.24606089\n",
      "Iteration 594, loss = 0.34618036\n",
      "Iteration 1206, loss = 0.23283603\n",
      "Iteration 1207, loss = 0.23276773\n",
      "Iteration 1252, loss = 0.24576002\n",
      "Iteration 595, loss = 0.34598106\n",
      "Iteration 1208, loss = 0.23249715\n",
      "Iteration 1209, loss = 0.23229000\n",
      "Iteration 1253, loss = 0.24566916\n",
      "Iteration 596, loss = 0.34582363\n",
      "Iteration 1210, loss = 0.23219749\n",
      "Iteration 1211, loss = 0.23206479\n",
      "Iteration 1254, loss = 0.24564927\n",
      "Iteration 597, loss = 0.34568161\n",
      "Iteration 1212, loss = 0.23193482\n",
      "Iteration 1213, loss = 0.23173390\n",
      "Iteration 1255, loss = 0.24516289\n",
      "Iteration 598, loss = 0.34551565\n",
      "Iteration 1214, loss = 0.23164536\n",
      "Iteration 1215, loss = 0.23133923\n",
      "Iteration 1256, loss = 0.24503241\n",
      "Iteration 599, loss = 0.34538813\n",
      "Iteration 1216, loss = 0.23119792\n",
      "Iteration 1217, loss = 0.23106463\n",
      "Iteration 1257, loss = 0.24473733\n",
      "Iteration 600, loss = 0.34522493\n",
      "Iteration 1218, loss = 0.23107729\n",
      "Iteration 1258, loss = 0.24469041\n",
      "Iteration 1219, loss = 0.23074703\n",
      "Iteration 601, loss = 0.34506060\n",
      "Iteration 1220, loss = 0.23070646\n",
      "Iteration 1259, loss = 0.24438978\n",
      "Iteration 1221, loss = 0.23051310\n",
      "Iteration 602, loss = 0.34489534\n",
      "Iteration 1222, loss = 0.23035704\n",
      "Iteration 1260, loss = 0.24420176\n",
      "Iteration 603, loss = 0.34480291\n",
      "Iteration 1223, loss = 0.23013726\n",
      "Iteration 1224, loss = 0.22996529\n",
      "Iteration 1261, loss = 0.24394692\n",
      "Iteration 604, loss = 0.34459626\n",
      "Iteration 1225, loss = 0.22992659\n",
      "Iteration 1226, loss = 0.22960517\n",
      "Iteration 1262, loss = 0.24380361\n",
      "Iteration 605, loss = 0.34446507\n",
      "Iteration 1227, loss = 0.22962677\n",
      "Iteration 1228, loss = 0.22938155\n",
      "Iteration 1263, loss = 0.24356123\n",
      "Iteration 606, loss = 0.34430146\n",
      "Iteration 1229, loss = 0.22912845\n",
      "Iteration 1230, loss = 0.22902543\n",
      "Iteration 1264, loss = 0.24376841\n",
      "Iteration 607, loss = 0.34412374\n",
      "Iteration 1231, loss = 0.22889513\n",
      "Iteration 1232, loss = 0.22874653\n",
      "Iteration 608, loss = 0.34401139\n",
      "Iteration 1265, loss = 0.24319316\n",
      "Iteration 1233, loss = 0.22853943\n",
      "Iteration 1234, loss = 0.22850271\n",
      "Iteration 609, loss = 0.34395361\n",
      "Iteration 1266, loss = 0.24297628\n",
      "Iteration 1235, loss = 0.22823753\n",
      "Iteration 1236, loss = 0.22814570\n",
      "Iteration 610, loss = 0.34373822\n",
      "Iteration 1267, loss = 0.24275587\n",
      "Iteration 1237, loss = 0.22796218\n",
      "Iteration 1238, loss = 0.22799158\n",
      "Iteration 611, loss = 0.34353939\n",
      "Iteration 1268, loss = 0.24268231\n",
      "Iteration 1239, loss = 0.22756721\n",
      "Iteration 1240, loss = 0.22741028\n",
      "Iteration 612, loss = 0.34338128\n",
      "Iteration 1269, loss = 0.24243734\n",
      "Iteration 1241, loss = 0.22733493\n",
      "Iteration 1242, loss = 0.22707072\n",
      "Iteration 1270, loss = 0.24234632\n",
      "Iteration 613, loss = 0.34322511\n",
      "Iteration 1243, loss = 0.22688100\n",
      "Iteration 1244, loss = 0.22689814\n",
      "Iteration 1271, loss = 0.24216500\n",
      "Iteration 614, loss = 0.34305080\n",
      "Iteration 1245, loss = 0.22659712\n",
      "Iteration 1246, loss = 0.22669462\n",
      "Iteration 1272, loss = 0.24177732\n",
      "Iteration 615, loss = 0.34289911\n",
      "Iteration 1247, loss = 0.22629248\n",
      "Iteration 1248, loss = 0.22620064\n",
      "Iteration 1273, loss = 0.24155066\n",
      "Iteration 616, loss = 0.34276576\n",
      "Iteration 1249, loss = 0.22620409\n",
      "Iteration 1250, loss = 0.22585240\n",
      "Iteration 1274, loss = 0.24135308\n",
      "Iteration 617, loss = 0.34260468\n",
      "Iteration 1251, loss = 0.22563354\n",
      "Iteration 1252, loss = 0.22566280\n",
      "Iteration 1275, loss = 0.24121218\n",
      "Iteration 618, loss = 0.34242805\n",
      "Iteration 1253, loss = 0.22542321\n",
      "Iteration 1254, loss = 0.22533590\n",
      "Iteration 1276, loss = 0.24100548\n",
      "Iteration 1255, loss = 0.22496225\n",
      "Iteration 619, loss = 0.34229218\n",
      "Iteration 1256, loss = 0.22488291\n",
      "Iteration 1277, loss = 0.24077808\n",
      "Iteration 1257, loss = 0.22466412\n",
      "Iteration 620, loss = 0.34212427\n",
      "Iteration 1258, loss = 0.22450906\n",
      "Iteration 1259, loss = 0.22434149\n",
      "Iteration 1278, loss = 0.24070639\n",
      "Iteration 621, loss = 0.34206288\n",
      "Iteration 1260, loss = 0.22437425\n",
      "Iteration 1261, loss = 0.22400779\n",
      "Iteration 1279, loss = 0.24047453\n",
      "Iteration 622, loss = 0.34183764\n",
      "Iteration 1262, loss = 0.22400194\n",
      "Iteration 623, loss = 0.34166860\n",
      "Iteration 1263, loss = 0.22377118\n",
      "Iteration 1280, loss = 0.24031466\n",
      "Iteration 1264, loss = 0.22363397\n",
      "Iteration 624, loss = 0.34162814\n",
      "Iteration 1265, loss = 0.22361009\n",
      "Iteration 1281, loss = 0.24002591\n",
      "Iteration 1266, loss = 0.22322704\n",
      "Iteration 625, loss = 0.34135020\n",
      "Iteration 1282, loss = 0.23982431\n",
      "Iteration 1267, loss = 0.22319766\n",
      "Iteration 1268, loss = 0.22302580\n",
      "Iteration 626, loss = 0.34121583\n",
      "Iteration 1283, loss = 0.23963165\n",
      "Iteration 1269, loss = 0.22279637\n",
      "Iteration 1270, loss = 0.22262952\n",
      "Iteration 627, loss = 0.34110425\n",
      "Iteration 1284, loss = 0.23945027\n",
      "Iteration 1271, loss = 0.22244861\n",
      "Iteration 1272, loss = 0.22226274\n",
      "Iteration 628, loss = 0.34086842\n",
      "Iteration 1285, loss = 0.23927313\n",
      "Iteration 1273, loss = 0.22211368\n",
      "Iteration 1274, loss = 0.22192800\n",
      "Iteration 629, loss = 0.34076224\n",
      "Iteration 1286, loss = 0.23907682\n",
      "Iteration 1275, loss = 0.22175931\n",
      "Iteration 1276, loss = 0.22165721\n",
      "Iteration 630, loss = 0.34060831\n",
      "Iteration 1287, loss = 0.23882826\n",
      "Iteration 1277, loss = 0.22144698\n",
      "Iteration 1278, loss = 0.22134371\n",
      "Iteration 631, loss = 0.34048435\n",
      "Iteration 1288, loss = 0.23864793\n",
      "Iteration 1279, loss = 0.22118383\n",
      "Iteration 1280, loss = 0.22098305\n",
      "Iteration 632, loss = 0.34029747\n",
      "Iteration 1289, loss = 0.23885819\n",
      "Iteration 1281, loss = 0.22086130\n",
      "Iteration 1282, loss = 0.22079292\n",
      "Iteration 633, loss = 0.34014225\n",
      "Iteration 1290, loss = 0.23828384\n",
      "Iteration 1283, loss = 0.22061608\n",
      "Iteration 1284, loss = 0.22039434\n",
      "Iteration 634, loss = 0.34015094\n",
      "Iteration 1291, loss = 0.23808377\n",
      "Iteration 1285, loss = 0.22032491\n",
      "Iteration 1286, loss = 0.22004237\n",
      "Iteration 635, loss = 0.33984345\n",
      "Iteration 1292, loss = 0.23791726\n",
      "Iteration 1287, loss = 0.21984674\n",
      "Iteration 1288, loss = 0.21993418\n",
      "Iteration 636, loss = 0.33967166\n",
      "Iteration 1293, loss = 0.23802562\n",
      "Iteration 1289, loss = 0.21975237\n",
      "Iteration 637, loss = 0.33956915\n",
      "Iteration 1290, loss = 0.21938951\n",
      "Iteration 1294, loss = 0.23749197\n",
      "Iteration 1291, loss = 0.21916563\n",
      "Iteration 638, loss = 0.33943765\n",
      "Iteration 1292, loss = 0.21914427\n",
      "Iteration 1295, loss = 0.23723783\n",
      "Iteration 1293, loss = 0.21883964\n",
      "Iteration 639, loss = 0.33923394\n",
      "Iteration 1294, loss = 0.21881807\n",
      "Iteration 1296, loss = 0.23720991\n",
      "Iteration 1295, loss = 0.21856530\n",
      "Iteration 640, loss = 0.33905916\n",
      "Iteration 1296, loss = 0.21834087\n",
      "Iteration 1297, loss = 0.23695854\n",
      "Iteration 1297, loss = 0.21829400\n",
      "Iteration 641, loss = 0.33896478\n",
      "Iteration 1298, loss = 0.21815621\n",
      "Iteration 1298, loss = 0.23673317\n",
      "Iteration 1299, loss = 0.21788731\n",
      "Iteration 642, loss = 0.33878219\n",
      "Iteration 1300, loss = 0.21778049\n",
      "Iteration 1299, loss = 0.23662797\n",
      "Iteration 1301, loss = 0.21777004\n",
      "Iteration 643, loss = 0.33862037\n",
      "Iteration 1302, loss = 0.21735896\n",
      "Iteration 1300, loss = 0.23639675\n",
      "Iteration 1303, loss = 0.21730719\n",
      "Iteration 644, loss = 0.33846276\n",
      "Iteration 1304, loss = 0.21709169\n",
      "Iteration 1301, loss = 0.23619943\n",
      "Iteration 1305, loss = 0.21698545\n",
      "Iteration 645, loss = 0.33842852\n",
      "Iteration 1306, loss = 0.21680358\n",
      "Iteration 1302, loss = 0.23598863\n",
      "Iteration 1307, loss = 0.21650452\n",
      "Iteration 646, loss = 0.33815038\n",
      "Iteration 1308, loss = 0.21678909\n",
      "Iteration 1303, loss = 0.23582273\n",
      "Iteration 1309, loss = 0.21656874\n",
      "Iteration 647, loss = 0.33804806\n",
      "Iteration 1310, loss = 0.21603207\n",
      "Iteration 1304, loss = 0.23553227\n",
      "Iteration 1311, loss = 0.21591211\n",
      "Iteration 648, loss = 0.33786924\n",
      "Iteration 1312, loss = 0.21572938\n",
      "Iteration 1305, loss = 0.23562835\n",
      "Iteration 1313, loss = 0.21559382\n",
      "Iteration 649, loss = 0.33777984\n",
      "Iteration 1314, loss = 0.21552624\n",
      "Iteration 1306, loss = 0.23527458\n",
      "Iteration 650, loss = 0.33754945\n",
      "Iteration 1315, loss = 0.21532551\n",
      "Iteration 1316, loss = 0.21506847\n",
      "Iteration 1307, loss = 0.23504376\n",
      "Iteration 651, loss = 0.33738412\n",
      "Iteration 1317, loss = 0.21502791\n",
      "Iteration 1318, loss = 0.21491140\n",
      "Iteration 1308, loss = 0.23487279\n",
      "Iteration 652, loss = 0.33727251\n",
      "Iteration 1319, loss = 0.21495054\n",
      "Iteration 1320, loss = 0.21443249\n",
      "Iteration 1309, loss = 0.23458760\n",
      "Iteration 653, loss = 0.33716458\n",
      "Iteration 1321, loss = 0.21422251\n",
      "Iteration 1322, loss = 0.21405395\n",
      "Iteration 1310, loss = 0.23444499\n",
      "Iteration 654, loss = 0.33706518\n",
      "Iteration 1323, loss = 0.21388328\n",
      "Iteration 1311, loss = 0.23426338\n",
      "Iteration 1324, loss = 0.21374176\n",
      "Iteration 655, loss = 0.33688184\n",
      "Iteration 1325, loss = 0.21360128\n",
      "Iteration 1326, loss = 0.21345174\n",
      "Iteration 1312, loss = 0.23417046\n",
      "Iteration 656, loss = 0.33662304\n",
      "Iteration 1327, loss = 0.21319344\n",
      "Iteration 1313, loss = 0.23384458\n",
      "Iteration 1328, loss = 0.21311607\n",
      "Iteration 657, loss = 0.33670610\n",
      "Iteration 1329, loss = 0.21290893\n",
      "Iteration 1330, loss = 0.21272666\n",
      "Iteration 1314, loss = 0.23362819\n",
      "Iteration 658, loss = 0.33637097\n",
      "Iteration 1331, loss = 0.21275953\n",
      "Iteration 1332, loss = 0.21245618\n",
      "Iteration 1315, loss = 0.23341332\n",
      "Iteration 659, loss = 0.33633198\n",
      "Iteration 1333, loss = 0.21224455\n",
      "Iteration 1334, loss = 0.21204966\n",
      "Iteration 1316, loss = 0.23327042\n",
      "Iteration 660, loss = 0.33607393\n",
      "Iteration 1335, loss = 0.21198835\n",
      "Iteration 1336, loss = 0.21165189\n",
      "Iteration 1317, loss = 0.23305038\n",
      "Iteration 661, loss = 0.33600121\n",
      "Iteration 1337, loss = 0.21151162\n",
      "Iteration 1338, loss = 0.21137880\n",
      "Iteration 1318, loss = 0.23302960\n",
      "Iteration 1339, loss = 0.21125817\n",
      "Iteration 662, loss = 0.33573876\n",
      "Iteration 1340, loss = 0.21098375\n",
      "Iteration 1319, loss = 0.23281798\n",
      "Iteration 1341, loss = 0.21083935\n",
      "Iteration 663, loss = 0.33571284\n",
      "Iteration 1342, loss = 0.21073503\n",
      "Iteration 1320, loss = 0.23246754\n",
      "Iteration 1343, loss = 0.21053454\n",
      "Iteration 664, loss = 0.33545857\n",
      "Iteration 1344, loss = 0.21031355\n",
      "Iteration 1321, loss = 0.23228366\n",
      "Iteration 1345, loss = 0.21013792\n",
      "Iteration 665, loss = 0.33535285\n",
      "Iteration 1346, loss = 0.20995705\n",
      "Iteration 1322, loss = 0.23215373\n",
      "Iteration 666, loss = 0.33518381\n",
      "Iteration 1347, loss = 0.20980148\n",
      "Iteration 1348, loss = 0.20969741\n",
      "Iteration 1323, loss = 0.23197332\n",
      "Iteration 667, loss = 0.33500909\n",
      "Iteration 1349, loss = 0.20948940\n",
      "Iteration 1350, loss = 0.20937967\n",
      "Iteration 1324, loss = 0.23184262\n",
      "Iteration 668, loss = 0.33484705\n",
      "Iteration 1351, loss = 0.20922964\n",
      "Iteration 1352, loss = 0.20897222\n",
      "Iteration 1325, loss = 0.23168706\n",
      "Iteration 669, loss = 0.33471261\n",
      "Iteration 1353, loss = 0.20897868\n",
      "Iteration 1326, loss = 0.23138857\n",
      "Iteration 1354, loss = 0.20866614\n",
      "Iteration 670, loss = 0.33455708\n",
      "Iteration 1355, loss = 0.20853213\n",
      "Iteration 1327, loss = 0.23119506\n",
      "Iteration 1356, loss = 0.20833499\n",
      "Iteration 671, loss = 0.33439809\n",
      "Iteration 1357, loss = 0.20813701\n",
      "Iteration 1328, loss = 0.23112122\n",
      "Iteration 1358, loss = 0.20803709\n",
      "Iteration 672, loss = 0.33425669\n",
      "Iteration 1359, loss = 0.20782870\n",
      "Iteration 1329, loss = 0.23090803\n",
      "Iteration 1360, loss = 0.20799945\n",
      "Iteration 673, loss = 0.33408962\n",
      "Iteration 1361, loss = 0.20743961\n",
      "Iteration 1330, loss = 0.23066871\n",
      "Iteration 1362, loss = 0.20755538\n",
      "Iteration 674, loss = 0.33395773\n",
      "Iteration 1363, loss = 0.20715659\n",
      "Iteration 1331, loss = 0.23043294\n",
      "Iteration 1364, loss = 0.20696332\n",
      "Iteration 675, loss = 0.33394162\n",
      "Iteration 1365, loss = 0.20677338\n",
      "Iteration 1332, loss = 0.23055928\n",
      "Iteration 1366, loss = 0.20674098\n",
      "Iteration 676, loss = 0.33365890\n",
      "Iteration 1367, loss = 0.20654876\n",
      "Iteration 1333, loss = 0.23018189\n",
      "Iteration 1368, loss = 0.20627733\n",
      "Iteration 677, loss = 0.33349719\n",
      "Iteration 1369, loss = 0.20608866\n",
      "Iteration 1334, loss = 0.22981988\n",
      "Iteration 678, loss = 0.33341070\n",
      "Iteration 1370, loss = 0.20594470\n",
      "Iteration 1371, loss = 0.20575647\n",
      "Iteration 1335, loss = 0.22970110\n",
      "Iteration 679, loss = 0.33323658\n",
      "Iteration 1372, loss = 0.20567505\n",
      "Iteration 1373, loss = 0.20547959\n",
      "Iteration 1336, loss = 0.22948376\n",
      "Iteration 680, loss = 0.33304191\n",
      "Iteration 1374, loss = 0.20532403\n",
      "Iteration 1375, loss = 0.20511372\n",
      "Iteration 1337, loss = 0.22930043\n",
      "Iteration 1376, loss = 0.20491773\n",
      "Iteration 681, loss = 0.33295178\n",
      "Iteration 1377, loss = 0.20485787\n",
      "Iteration 1338, loss = 0.22916357\n",
      "Iteration 1378, loss = 0.20474669\n",
      "Iteration 682, loss = 0.33275290\n",
      "Iteration 1379, loss = 0.20444152\n",
      "Iteration 1339, loss = 0.22892708\n",
      "Iteration 1380, loss = 0.20428000\n",
      "Iteration 683, loss = 0.33261527\n",
      "Iteration 1381, loss = 0.20405382\n",
      "Iteration 1340, loss = 0.22883960\n",
      "Iteration 1382, loss = 0.20401315\n",
      "Iteration 684, loss = 0.33250508\n",
      "Iteration 1383, loss = 0.20390616\n",
      "Iteration 1341, loss = 0.22853645\n",
      "Iteration 1384, loss = 0.20362100\n",
      "Iteration 685, loss = 0.33238668\n",
      "Iteration 1385, loss = 0.20339704\n",
      "Iteration 1342, loss = 0.22849908\n",
      "Iteration 1386, loss = 0.20326299\n",
      "Iteration 686, loss = 0.33217809\n",
      "Iteration 1387, loss = 0.20311123\n",
      "Iteration 1343, loss = 0.22822480\n",
      "Iteration 1388, loss = 0.20293959\n",
      "Iteration 687, loss = 0.33211392\n",
      "Iteration 1389, loss = 0.20278433\n",
      "Iteration 1344, loss = 0.22805312\n",
      "Iteration 1390, loss = 0.20261817\n",
      "Iteration 688, loss = 0.33187021\n",
      "Iteration 1391, loss = 0.20239458\n",
      "Iteration 1345, loss = 0.22776712\n",
      "Iteration 1392, loss = 0.20218382\n",
      "Iteration 689, loss = 0.33178723\n",
      "Iteration 1393, loss = 0.20239571\n",
      "Iteration 1346, loss = 0.22759661\n",
      "Iteration 1394, loss = 0.20190755\n",
      "Iteration 690, loss = 0.33159717\n",
      "Iteration 1395, loss = 0.20182891\n",
      "Iteration 1347, loss = 0.22757456\n",
      "Iteration 1396, loss = 0.20148851\n",
      "Iteration 691, loss = 0.33145119\n",
      "Iteration 1397, loss = 0.20133660\n",
      "Iteration 1348, loss = 0.22738980\n",
      "Iteration 1398, loss = 0.20119307\n",
      "Iteration 692, loss = 0.33127883\n",
      "Iteration 1399, loss = 0.20105577\n",
      "Iteration 1349, loss = 0.22702652\n",
      "Iteration 1400, loss = 0.20081824\n",
      "Iteration 693, loss = 0.33111844\n",
      "Iteration 1401, loss = 0.20067613\n",
      "Iteration 1350, loss = 0.22698380\n",
      "Iteration 1402, loss = 0.20053623\n",
      "Iteration 694, loss = 0.33097149\n",
      "Iteration 1403, loss = 0.20043580\n",
      "Iteration 1351, loss = 0.22668852\n",
      "Iteration 1404, loss = 0.20019118\n",
      "Iteration 695, loss = 0.33087962\n",
      "Iteration 1405, loss = 0.20024798\n",
      "Iteration 1406, loss = 0.19985704\n",
      "Iteration 1352, loss = 0.22658783\n",
      "Iteration 696, loss = 0.33071598\n",
      "Iteration 1407, loss = 0.19966406\n",
      "Iteration 1408, loss = 0.19957579\n",
      "Iteration 1353, loss = 0.22627667\n",
      "Iteration 697, loss = 0.33056017\n",
      "Iteration 1409, loss = 0.19933186\n",
      "Iteration 1410, loss = 0.19916524\n",
      "Iteration 1354, loss = 0.22617450\n",
      "Iteration 698, loss = 0.33035664\n",
      "Iteration 1411, loss = 0.19919567\n",
      "Iteration 1412, loss = 0.19883248\n",
      "Iteration 1355, loss = 0.22597134\n",
      "Iteration 699, loss = 0.33020559\n",
      "Iteration 1413, loss = 0.19872550\n",
      "Iteration 1414, loss = 0.19850217\n",
      "Iteration 1356, loss = 0.22577452\n",
      "Iteration 700, loss = 0.33007107\n",
      "Iteration 1415, loss = 0.19831416\n",
      "Iteration 1416, loss = 0.19823049\n",
      "Iteration 1357, loss = 0.22587014\n",
      "Iteration 701, loss = 0.32992614\n",
      "Iteration 1417, loss = 0.19803390\n",
      "Iteration 1418, loss = 0.19788427\n",
      "Iteration 1419, loss = 0.19790479\n",
      "Iteration 1358, loss = 0.22552125\n",
      "Iteration 702, loss = 0.32999029\n",
      "Iteration 1420, loss = 0.19778618\n",
      "Iteration 1421, loss = 0.19729757\n",
      "Iteration 703, loss = 0.32961130\n",
      "Iteration 1359, loss = 0.22513500\n",
      "Iteration 1422, loss = 0.19715980\n",
      "Iteration 1423, loss = 0.19699801\n",
      "Iteration 704, loss = 0.32948752\n",
      "Iteration 1360, loss = 0.22508163\n",
      "Iteration 1424, loss = 0.19697179\n",
      "Iteration 1425, loss = 0.19664207\n",
      "Iteration 1361, loss = 0.22479728\n",
      "Iteration 705, loss = 0.32932359\n",
      "Iteration 1426, loss = 0.19688862\n",
      "Iteration 706, loss = 0.32924291\n",
      "Iteration 1362, loss = 0.22464379\n",
      "Iteration 1427, loss = 0.19645856\n",
      "Iteration 1428, loss = 0.19629845\n",
      "Iteration 707, loss = 0.32913052\n",
      "Iteration 1429, loss = 0.19614732\n",
      "Iteration 1363, loss = 0.22462186\n",
      "Iteration 1430, loss = 0.19600685\n",
      "Iteration 708, loss = 0.32894917\n",
      "Iteration 1431, loss = 0.19561150\n",
      "Iteration 1364, loss = 0.22425590\n",
      "Iteration 1432, loss = 0.19545313\n",
      "Iteration 709, loss = 0.32875549\n",
      "Iteration 1433, loss = 0.19540469\n",
      "Iteration 1365, loss = 0.22450728\n",
      "Iteration 1434, loss = 0.19509652\n",
      "Iteration 1435, loss = 0.19506198\n",
      "Iteration 710, loss = 0.32873717\n",
      "Iteration 1366, loss = 0.22392750\n",
      "Iteration 1436, loss = 0.19485503\n",
      "Iteration 1437, loss = 0.19462215\n",
      "Iteration 711, loss = 0.32854392\n",
      "Iteration 1367, loss = 0.22373278\n",
      "Iteration 1438, loss = 0.19450627\n",
      "Iteration 712, loss = 0.32837971\n",
      "Iteration 1439, loss = 0.19436897\n",
      "Iteration 1368, loss = 0.22365572\n",
      "Iteration 1440, loss = 0.19427029\n",
      "Iteration 713, loss = 0.32813693\n",
      "Iteration 1441, loss = 0.19400850\n",
      "Iteration 1369, loss = 0.22336935\n",
      "Iteration 1442, loss = 0.19387799\n",
      "Iteration 1443, loss = 0.19381166\n",
      "Iteration 714, loss = 0.32823719\n",
      "Iteration 1370, loss = 0.22321591\n",
      "Iteration 1444, loss = 0.19377478\n",
      "Iteration 1445, loss = 0.19343240\n",
      "Iteration 715, loss = 0.32801811\n",
      "Iteration 1446, loss = 0.19327729\n",
      "Iteration 1371, loss = 0.22310419\n",
      "Iteration 1447, loss = 0.19311360\n",
      "Iteration 716, loss = 0.32775437\n",
      "Iteration 1448, loss = 0.19279669\n",
      "Iteration 1372, loss = 0.22279542\n",
      "Iteration 1449, loss = 0.19270878\n",
      "Iteration 717, loss = 0.32758051\n",
      "Iteration 1373, loss = 0.22261915\n",
      "Iteration 1450, loss = 0.19249779\n",
      "Iteration 1451, loss = 0.19232674\n",
      "Iteration 718, loss = 0.32746952\n",
      "Iteration 1374, loss = 0.22248757\n",
      "Iteration 1452, loss = 0.19215476\n",
      "Iteration 719, loss = 0.32725839\n",
      "Iteration 1375, loss = 0.22228075\n",
      "Iteration 1453, loss = 0.19201304\n",
      "Iteration 1454, loss = 0.19190078\n",
      "Iteration 720, loss = 0.32710200\n",
      "Iteration 1455, loss = 0.19174782\n",
      "Iteration 1376, loss = 0.22204934\n",
      "Iteration 721, loss = 0.32712362\n",
      "Iteration 1456, loss = 0.19163625\n",
      "Iteration 1377, loss = 0.22187817\n",
      "Iteration 1457, loss = 0.19142042\n",
      "Iteration 722, loss = 0.32685349\n",
      "Iteration 1458, loss = 0.19124590\n",
      "Iteration 1459, loss = 0.19102986\n",
      "Iteration 1378, loss = 0.22172437\n",
      "Iteration 723, loss = 0.32667938\n",
      "Iteration 1460, loss = 0.19083849\n",
      "Iteration 1461, loss = 0.19074838\n",
      "Iteration 1379, loss = 0.22150617\n",
      "Iteration 1462, loss = 0.19063926\n",
      "Iteration 724, loss = 0.32649248\n",
      "Iteration 1463, loss = 0.19044868\n",
      "Iteration 1380, loss = 0.22138658\n",
      "Iteration 1464, loss = 0.19022287\n",
      "Iteration 725, loss = 0.32638653\n",
      "Iteration 1465, loss = 0.19006203\n",
      "Iteration 1466, loss = 0.18987014\n",
      "Iteration 1381, loss = 0.22123666\n",
      "Iteration 726, loss = 0.32622763\n",
      "Iteration 1467, loss = 0.18970647\n",
      "Iteration 1468, loss = 0.18955939\n",
      "Iteration 1382, loss = 0.22097502\n",
      "Iteration 727, loss = 0.32607395\n",
      "Iteration 1469, loss = 0.18945436\n",
      "Iteration 1383, loss = 0.22092400\n",
      "Iteration 728, loss = 0.32598941\n",
      "Iteration 1470, loss = 0.18921557\n",
      "Iteration 1471, loss = 0.18949916\n",
      "Iteration 1384, loss = 0.22074532\n",
      "Iteration 729, loss = 0.32581660\n",
      "Iteration 1472, loss = 0.18899667\n",
      "Iteration 1473, loss = 0.18881583\n",
      "Iteration 1385, loss = 0.22046913\n",
      "Iteration 730, loss = 0.32558850\n",
      "Iteration 1474, loss = 0.18861019\n",
      "Iteration 1386, loss = 0.22030021\n",
      "Iteration 731, loss = 0.32551117\n",
      "Iteration 1475, loss = 0.18844580\n",
      "Iteration 1476, loss = 0.18830954\n",
      "Iteration 1387, loss = 0.22008879\n",
      "Iteration 732, loss = 0.32528229\n",
      "Iteration 1477, loss = 0.18810100\n",
      "Iteration 1388, loss = 0.22006931\n",
      "Iteration 1478, loss = 0.18806194\n",
      "Iteration 733, loss = 0.32514433\n",
      "Iteration 1479, loss = 0.18813340\n",
      "Iteration 1389, loss = 0.21970911\n",
      "Iteration 734, loss = 0.32512835\n",
      "Iteration 1480, loss = 0.18763644\n",
      "Iteration 1390, loss = 0.21969762\n",
      "Iteration 1481, loss = 0.18741477\n",
      "Iteration 735, loss = 0.32486146\n",
      "Iteration 1482, loss = 0.18735531\n",
      "Iteration 1483, loss = 0.18708122\n",
      "Iteration 1391, loss = 0.21932054\n",
      "Iteration 736, loss = 0.32468045\n",
      "Iteration 1484, loss = 0.18699872\n",
      "Iteration 1485, loss = 0.18694308\n",
      "Iteration 1392, loss = 0.21921017\n",
      "Iteration 1486, loss = 0.18661642\n",
      "Iteration 737, loss = 0.32460530\n",
      "Iteration 1487, loss = 0.18658096\n",
      "Iteration 1393, loss = 0.21913209\n",
      "Iteration 738, loss = 0.32440949\n",
      "Iteration 1488, loss = 0.18659113\n",
      "Iteration 1489, loss = 0.18618624\n",
      "Iteration 739, loss = 0.32428220\n",
      "Iteration 1490, loss = 0.18594576\n",
      "Iteration 1394, loss = 0.21894153\n",
      "Iteration 1491, loss = 0.18577178\n",
      "Iteration 1395, loss = 0.21865622\n",
      "Iteration 1492, loss = 0.18563428\n",
      "Iteration 740, loss = 0.32410070\n",
      "Iteration 1493, loss = 0.18553391\n",
      "Iteration 1396, loss = 0.21860698\n",
      "Iteration 1494, loss = 0.18535341\n",
      "Iteration 741, loss = 0.32395875\n",
      "Iteration 1495, loss = 0.18521805\n",
      "Iteration 1397, loss = 0.21837684\n",
      "Iteration 1496, loss = 0.18514037\n",
      "Iteration 742, loss = 0.32384381\n",
      "Iteration 1497, loss = 0.18481276\n",
      "Iteration 1498, loss = 0.18465847\n",
      "Iteration 1398, loss = 0.21816412\n",
      "Iteration 743, loss = 0.32365581\n",
      "Iteration 1499, loss = 0.18452780\n",
      "Iteration 1399, loss = 0.21797900\n",
      "Iteration 1500, loss = 0.18442095\n",
      "Iteration 744, loss = 0.32352200\n",
      "Iteration 1501, loss = 0.18411727\n",
      "Iteration 1502, loss = 0.18398019\n",
      "Iteration 1400, loss = 0.21779866\n",
      "Iteration 745, loss = 0.32345632\n",
      "Iteration 1503, loss = 0.18386932\n",
      "Iteration 1504, loss = 0.18367582\n",
      "Iteration 746, loss = 0.32322282\n",
      "Iteration 1401, loss = 0.21765164\n",
      "Iteration 1505, loss = 0.18356318\n",
      "Iteration 1506, loss = 0.18353509\n",
      "Iteration 747, loss = 0.32301680\n",
      "Iteration 1507, loss = 0.18327309\n",
      "Iteration 1402, loss = 0.21749060\n",
      "Iteration 1508, loss = 0.18304576\n",
      "Iteration 748, loss = 0.32287125\n",
      "Iteration 1509, loss = 0.18285940\n",
      "Iteration 1403, loss = 0.21725864\n",
      "Iteration 1510, loss = 0.18276200\n",
      "Iteration 1511, loss = 0.18261344\n",
      "Iteration 749, loss = 0.32274310\n",
      "Iteration 1404, loss = 0.21707612\n",
      "Iteration 1512, loss = 0.18240018\n",
      "Iteration 750, loss = 0.32263088\n",
      "Iteration 1513, loss = 0.18238077\n",
      "Iteration 1405, loss = 0.21691217\n",
      "Iteration 1514, loss = 0.18205183\n",
      "Iteration 751, loss = 0.32243377\n",
      "Iteration 1515, loss = 0.18193557\n",
      "Iteration 1516, loss = 0.18177185\n",
      "Iteration 1406, loss = 0.21678725\n",
      "Iteration 752, loss = 0.32232170\n",
      "Iteration 1517, loss = 0.18159171\n",
      "Iteration 1518, loss = 0.18148013\n",
      "Iteration 753, loss = 0.32216325\n",
      "Iteration 1407, loss = 0.21673701\n",
      "Iteration 1519, loss = 0.18123771\n",
      "Iteration 754, loss = 0.32198937\n",
      "Iteration 1520, loss = 0.18107440\n",
      "Iteration 1408, loss = 0.21648625\n",
      "Iteration 1521, loss = 0.18092103\n",
      "Iteration 755, loss = 0.32191813\n",
      "Iteration 1522, loss = 0.18071093\n",
      "Iteration 1409, loss = 0.21623831\n",
      "Iteration 1523, loss = 0.18088064\n",
      "Iteration 756, loss = 0.32170600\n",
      "Iteration 1524, loss = 0.18046016\n",
      "Iteration 1410, loss = 0.21614841\n",
      "Iteration 1525, loss = 0.18028242\n",
      "Iteration 1411, loss = 0.21587137\n",
      "Iteration 757, loss = 0.32154541\n",
      "Iteration 1526, loss = 0.18014673\n",
      "Iteration 1527, loss = 0.17994921\n",
      "Iteration 1412, loss = 0.21573271\n",
      "Iteration 758, loss = 0.32137730\n",
      "Iteration 1528, loss = 0.17977933\n",
      "Iteration 1529, loss = 0.17968173\n",
      "Iteration 1413, loss = 0.21576433\n",
      "Iteration 1530, loss = 0.17944637\n",
      "Iteration 759, loss = 0.32128812\n",
      "Iteration 1531, loss = 0.17935203\n",
      "Iteration 1532, loss = 0.17922246\n",
      "Iteration 1414, loss = 0.21540231\n",
      "Iteration 760, loss = 0.32118216\n",
      "Iteration 1533, loss = 0.17893694\n",
      "Iteration 1415, loss = 0.21522121\n",
      "Iteration 1534, loss = 0.17885533\n",
      "Iteration 761, loss = 0.32104814\n",
      "Iteration 1535, loss = 0.17876333\n",
      "Iteration 1416, loss = 0.21505210\n",
      "Iteration 1536, loss = 0.17853912\n",
      "Iteration 762, loss = 0.32082065\n",
      "Iteration 1537, loss = 0.17831229\n",
      "Iteration 1417, loss = 0.21489024\n",
      "Iteration 1538, loss = 0.17820669\n",
      "Iteration 763, loss = 0.32064642\n",
      "Iteration 1539, loss = 0.17804440\n",
      "Iteration 1418, loss = 0.21499989\n",
      "Iteration 1540, loss = 0.17786473\n",
      "Iteration 764, loss = 0.32047346\n",
      "Iteration 1541, loss = 0.17770970\n",
      "Iteration 1419, loss = 0.21450999\n",
      "Iteration 1542, loss = 0.17777228\n",
      "Iteration 765, loss = 0.32028571\n",
      "Iteration 1543, loss = 0.17733269\n",
      "Iteration 1420, loss = 0.21437887\n",
      "Iteration 1544, loss = 0.17726756\n",
      "Iteration 766, loss = 0.32021563\n",
      "Iteration 1545, loss = 0.17712570\n",
      "Iteration 1421, loss = 0.21443416\n",
      "Iteration 1546, loss = 0.17690597\n",
      "Iteration 767, loss = 0.31998074\n",
      "Iteration 1547, loss = 0.17677263\n",
      "Iteration 1422, loss = 0.21429766\n",
      "Iteration 1548, loss = 0.17669413\n",
      "Iteration 768, loss = 0.31987634\n",
      "Iteration 1423, loss = 0.21403272\n",
      "Iteration 1549, loss = 0.17640893\n",
      "Iteration 1550, loss = 0.17636492\n",
      "Iteration 769, loss = 0.31971847\n",
      "Iteration 1551, loss = 0.17626322\n",
      "Iteration 1424, loss = 0.21362700\n",
      "Iteration 1552, loss = 0.17598215\n",
      "Iteration 770, loss = 0.31958978\n",
      "Iteration 1553, loss = 0.17599718\n",
      "Iteration 1425, loss = 0.21349907\n",
      "Iteration 1554, loss = 0.17581480\n",
      "Iteration 771, loss = 0.31938305\n",
      "Iteration 1426, loss = 0.21331384\n",
      "Iteration 1555, loss = 0.17551546\n",
      "Iteration 772, loss = 0.31925149\n",
      "Iteration 1556, loss = 0.17541072\n",
      "Iteration 1427, loss = 0.21317506\n",
      "Iteration 1557, loss = 0.17533432\n",
      "Iteration 773, loss = 0.31912754\n",
      "Iteration 1558, loss = 0.17512196\n",
      "Iteration 1428, loss = 0.21301459\n",
      "Iteration 1559, loss = 0.17513158\n",
      "Iteration 774, loss = 0.31892699\n",
      "Iteration 1560, loss = 0.17472412\n",
      "Iteration 1429, loss = 0.21293349\n",
      "Iteration 1561, loss = 0.17457673\n",
      "Iteration 775, loss = 0.31881469\n",
      "Iteration 1562, loss = 0.17463390\n",
      "Iteration 1430, loss = 0.21261192\n",
      "Iteration 1563, loss = 0.17426508\n",
      "Iteration 776, loss = 0.31865657\n",
      "Iteration 1564, loss = 0.17416438\n",
      "Iteration 1431, loss = 0.21248656\n",
      "Iteration 1565, loss = 0.17398770\n",
      "Iteration 777, loss = 0.31847111\n",
      "Iteration 1566, loss = 0.17378751\n",
      "Iteration 1432, loss = 0.21235942\n",
      "Iteration 1567, loss = 0.17369842\n",
      "Iteration 778, loss = 0.31832696\n",
      "Iteration 1568, loss = 0.17366587\n",
      "Iteration 1433, loss = 0.21229699\n",
      "Iteration 1569, loss = 0.17357865\n",
      "Iteration 779, loss = 0.31816584\n",
      "Iteration 1570, loss = 0.17328431\n",
      "Iteration 1434, loss = 0.21205619\n",
      "Iteration 1571, loss = 0.17303419\n",
      "Iteration 780, loss = 0.31817848\n",
      "Iteration 1572, loss = 0.17288038\n",
      "Iteration 1435, loss = 0.21181962\n",
      "Iteration 1573, loss = 0.17272473\n",
      "Iteration 781, loss = 0.31797332\n",
      "Iteration 1574, loss = 0.17256388\n",
      "Iteration 1436, loss = 0.21172358\n",
      "Iteration 1575, loss = 0.17245260\n",
      "Iteration 782, loss = 0.31774654\n",
      "Iteration 1576, loss = 0.17228701\n",
      "Iteration 1437, loss = 0.21145391\n",
      "Iteration 1577, loss = 0.17210152\n",
      "Iteration 783, loss = 0.31760094\n",
      "Iteration 1578, loss = 0.17202585\n",
      "Iteration 1438, loss = 0.21133665\n",
      "Iteration 1579, loss = 0.17181997\n",
      "Iteration 784, loss = 0.31742295\n",
      "Iteration 1580, loss = 0.17165656\n",
      "Iteration 1439, loss = 0.21118451\n",
      "Iteration 1581, loss = 0.17149307\n",
      "Iteration 785, loss = 0.31730178\n",
      "Iteration 1582, loss = 0.17138951\n",
      "Iteration 1440, loss = 0.21097140\n",
      "Iteration 1583, loss = 0.17119981\n",
      "Iteration 786, loss = 0.31715982\n",
      "Iteration 1584, loss = 0.17112122\n",
      "Iteration 1441, loss = 0.21083123\n",
      "Iteration 1585, loss = 0.17093719\n",
      "Iteration 787, loss = 0.31697697\n",
      "Iteration 1586, loss = 0.17086842\n",
      "Iteration 1442, loss = 0.21065618\n",
      "Iteration 1587, loss = 0.17060871\n",
      "Iteration 788, loss = 0.31683824\n",
      "Iteration 1588, loss = 0.17060594\n",
      "Iteration 1443, loss = 0.21061849\n",
      "Iteration 1589, loss = 0.17032155\n",
      "Iteration 789, loss = 0.31669043\n",
      "Iteration 1590, loss = 0.17019018\n",
      "Iteration 1444, loss = 0.21039560\n",
      "Iteration 1591, loss = 0.17027164\n",
      "Iteration 790, loss = 0.31654024\n",
      "Iteration 1592, loss = 0.16984743\n",
      "Iteration 1593, loss = 0.16984227\n",
      "Iteration 1445, loss = 0.21017238\n",
      "Iteration 791, loss = 0.31642132\n",
      "Iteration 1594, loss = 0.16964534\n",
      "Iteration 1446, loss = 0.21000244\n",
      "Iteration 1595, loss = 0.16941581\n",
      "Iteration 792, loss = 0.31627023\n",
      "Iteration 1596, loss = 0.16941393\n",
      "Iteration 1447, loss = 0.20984574\n",
      "Iteration 1597, loss = 0.16918325\n",
      "Iteration 793, loss = 0.31612638\n",
      "Iteration 1598, loss = 0.16896054\n",
      "Iteration 1448, loss = 0.20968305\n",
      "Iteration 1599, loss = 0.16893701\n",
      "Iteration 794, loss = 0.31592692\n",
      "Iteration 1600, loss = 0.16875366\n",
      "Iteration 1449, loss = 0.20946189\n",
      "Iteration 1601, loss = 0.16855617\n",
      "Iteration 795, loss = 0.31581561\n",
      "Iteration 1602, loss = 0.16839232\n",
      "Iteration 1450, loss = 0.20937334\n",
      "Iteration 1603, loss = 0.16827305\n",
      "Iteration 796, loss = 0.31568326\n",
      "Iteration 1604, loss = 0.16824599\n",
      "Iteration 1451, loss = 0.20916825\n",
      "Iteration 1605, loss = 0.16803834\n",
      "Iteration 797, loss = 0.31576299\n",
      "Iteration 1606, loss = 0.16785325\n",
      "Iteration 1452, loss = 0.20910111\n",
      "Iteration 1607, loss = 0.16775550\n",
      "Iteration 798, loss = 0.31536989\n",
      "Iteration 1453, loss = 0.20893692\n",
      "Iteration 1608, loss = 0.16756554\n",
      "Iteration 799, loss = 0.31516870\n",
      "Iteration 1609, loss = 0.16751004\n",
      "Iteration 1454, loss = 0.20863301\n",
      "Iteration 1610, loss = 0.16723383\n",
      "Iteration 800, loss = 0.31509565\n",
      "Iteration 1611, loss = 0.16714825\n",
      "Iteration 1455, loss = 0.20868225\n",
      "Iteration 1612, loss = 0.16695747\n",
      "Iteration 801, loss = 0.31488990\n",
      "Iteration 1613, loss = 0.16691319\n",
      "Iteration 1456, loss = 0.20829777\n",
      "Iteration 1614, loss = 0.16668335\n",
      "Iteration 802, loss = 0.31495237\n",
      "Iteration 1615, loss = 0.16663746\n",
      "Iteration 1457, loss = 0.20814912\n",
      "Iteration 1616, loss = 0.16651602\n",
      "Iteration 803, loss = 0.31454529\n",
      "Iteration 1617, loss = 0.16626862\n",
      "Iteration 1458, loss = 0.20806131\n",
      "Iteration 1618, loss = 0.16620679\n",
      "Iteration 804, loss = 0.31447570\n",
      "Iteration 1619, loss = 0.16601952\n",
      "Iteration 1459, loss = 0.20782313\n",
      "Iteration 1620, loss = 0.16598464\n",
      "Iteration 805, loss = 0.31424752\n",
      "Iteration 1621, loss = 0.16570697\n",
      "Iteration 1460, loss = 0.20763410\n",
      "Iteration 1622, loss = 0.16582684\n",
      "Iteration 806, loss = 0.31416104\n",
      "Iteration 1623, loss = 0.16543144\n",
      "Iteration 1461, loss = 0.20771110\n",
      "Iteration 1624, loss = 0.16536440\n",
      "Iteration 807, loss = 0.31403694\n",
      "Iteration 1625, loss = 0.16515834\n",
      "Iteration 1462, loss = 0.20736129\n",
      "Iteration 1626, loss = 0.16498963\n",
      "Iteration 808, loss = 0.31400425\n",
      "Iteration 1627, loss = 0.16518968\n",
      "Iteration 1463, loss = 0.20725493\n",
      "Iteration 1628, loss = 0.16484799\n",
      "Iteration 809, loss = 0.31371078\n",
      "Iteration 1629, loss = 0.16456881\n",
      "Iteration 1464, loss = 0.20701161\n",
      "Iteration 1630, loss = 0.16463694\n",
      "Iteration 810, loss = 0.31350655\n",
      "Iteration 1631, loss = 0.16429423\n",
      "Iteration 1465, loss = 0.20694456\n",
      "Iteration 1632, loss = 0.16417634\n",
      "Iteration 811, loss = 0.31336289\n",
      "Iteration 1633, loss = 0.16417597\n",
      "Iteration 1466, loss = 0.20664841\n",
      "Iteration 1634, loss = 0.16392869\n",
      "Iteration 812, loss = 0.31325588\n",
      "Iteration 1635, loss = 0.16370196\n",
      "Iteration 1467, loss = 0.20653018\n",
      "Iteration 1636, loss = 0.16358958\n",
      "Iteration 813, loss = 0.31312190\n",
      "Iteration 1637, loss = 0.16351614\n",
      "Iteration 1468, loss = 0.20637391\n",
      "Iteration 1638, loss = 0.16334250\n",
      "Iteration 814, loss = 0.31293210\n",
      "Iteration 1639, loss = 0.16317240\n",
      "Iteration 1469, loss = 0.20627715\n",
      "Iteration 1640, loss = 0.16303398\n",
      "Iteration 815, loss = 0.31279095\n",
      "Iteration 1470, loss = 0.20606442\n",
      "Iteration 1641, loss = 0.16291872\n",
      "Iteration 1642, loss = 0.16319497\n",
      "Iteration 816, loss = 0.31267864\n",
      "Iteration 1471, loss = 0.20594850\n",
      "Iteration 1643, loss = 0.16270465\n",
      "Iteration 1644, loss = 0.16271561\n",
      "Iteration 817, loss = 0.31249280\n",
      "Iteration 1472, loss = 0.20590986\n",
      "Iteration 1645, loss = 0.16235059\n",
      "Iteration 1646, loss = 0.16224767\n",
      "Iteration 818, loss = 0.31236437\n",
      "Iteration 1473, loss = 0.20571592\n",
      "Iteration 1647, loss = 0.16223613\n",
      "Iteration 1648, loss = 0.16230257\n",
      "Iteration 819, loss = 0.31221863\n",
      "Iteration 1474, loss = 0.20554788\n",
      "Iteration 1649, loss = 0.16180099\n",
      "Iteration 1650, loss = 0.16170423\n",
      "Iteration 820, loss = 0.31203661\n",
      "Iteration 1475, loss = 0.20522433\n",
      "Iteration 1651, loss = 0.16160999\n",
      "Iteration 1652, loss = 0.16166135\n",
      "Iteration 821, loss = 0.31183700\n",
      "Iteration 1476, loss = 0.20510696\n",
      "Iteration 1653, loss = 0.16127737\n",
      "Iteration 1654, loss = 0.16119778\n",
      "Iteration 822, loss = 0.31174857\n",
      "Iteration 1477, loss = 0.20502882\n",
      "Iteration 1655, loss = 0.16099889\n",
      "Iteration 1656, loss = 0.16085050\n",
      "Iteration 823, loss = 0.31157543\n",
      "Iteration 1478, loss = 0.20488825\n",
      "Iteration 1657, loss = 0.16081716\n",
      "Iteration 1658, loss = 0.16064149\n",
      "Iteration 824, loss = 0.31143329\n",
      "Iteration 1479, loss = 0.20465841\n",
      "Iteration 1659, loss = 0.16051997\n",
      "Iteration 1660, loss = 0.16035834\n",
      "Iteration 825, loss = 0.31131805\n",
      "Iteration 1480, loss = 0.20451932\n",
      "Iteration 1661, loss = 0.16024233\n",
      "Iteration 1662, loss = 0.16013286\n",
      "Iteration 826, loss = 0.31110501\n",
      "Iteration 1481, loss = 0.20433975\n",
      "Iteration 1663, loss = 0.16002039\n",
      "Iteration 1664, loss = 0.15980055\n",
      "Iteration 827, loss = 0.31107013\n",
      "Iteration 1482, loss = 0.20412535\n",
      "Iteration 1665, loss = 0.15979909\n",
      "Iteration 1666, loss = 0.15948831\n",
      "Iteration 828, loss = 0.31094656\n",
      "Iteration 1483, loss = 0.20398575\n",
      "Iteration 1667, loss = 0.15948322\n",
      "Iteration 1668, loss = 0.15932276\n",
      "Iteration 829, loss = 0.31070986\n",
      "Iteration 1484, loss = 0.20380954\n",
      "Iteration 1669, loss = 0.15942086\n",
      "Iteration 1670, loss = 0.15895375\n",
      "Iteration 830, loss = 0.31051078\n",
      "Iteration 1485, loss = 0.20379603\n",
      "Iteration 1671, loss = 0.15895189\n",
      "Iteration 831, loss = 0.31044422\n",
      "Iteration 1672, loss = 0.15879527\n",
      "Iteration 1486, loss = 0.20425080\n",
      "Iteration 1673, loss = 0.15887892\n",
      "Iteration 832, loss = 0.31044847\n",
      "Iteration 1674, loss = 0.15858075\n",
      "Iteration 1487, loss = 0.20339491\n",
      "Iteration 1675, loss = 0.15837600\n",
      "Iteration 833, loss = 0.31012308\n",
      "Iteration 1676, loss = 0.15853277\n",
      "Iteration 1488, loss = 0.20329835\n",
      "Iteration 1677, loss = 0.15822954\n",
      "Iteration 1678, loss = 0.15813057\n",
      "Iteration 834, loss = 0.30990756\n",
      "Iteration 1489, loss = 0.20321155\n",
      "Iteration 1679, loss = 0.15796636\n",
      "Iteration 1680, loss = 0.15781379\n",
      "Iteration 835, loss = 0.30987840\n",
      "Iteration 1490, loss = 0.20288504\n",
      "Iteration 1681, loss = 0.15758517\n",
      "Iteration 1682, loss = 0.15746289\n",
      "Iteration 836, loss = 0.30968096\n",
      "Iteration 1491, loss = 0.20271137\n",
      "Iteration 1683, loss = 0.15738923\n",
      "Iteration 1684, loss = 0.15719983\n",
      "Iteration 837, loss = 0.30947519\n",
      "Iteration 1492, loss = 0.20269984\n",
      "Iteration 1685, loss = 0.15719695\n",
      "Iteration 1686, loss = 0.15698339\n",
      "Iteration 838, loss = 0.30939849\n",
      "Iteration 1493, loss = 0.20249312\n",
      "Iteration 1687, loss = 0.15687156\n",
      "Iteration 1688, loss = 0.15677358\n",
      "Iteration 839, loss = 0.30935824\n",
      "Iteration 1494, loss = 0.20235277\n",
      "Iteration 1689, loss = 0.15664878\n",
      "Iteration 1690, loss = 0.15641860\n",
      "Iteration 1495, loss = 0.20212910\n",
      "Iteration 840, loss = 0.30897980\n",
      "Iteration 1691, loss = 0.15645024\n",
      "Iteration 1692, loss = 0.15621820\n",
      "Iteration 1496, loss = 0.20207971\n",
      "Iteration 841, loss = 0.30890705\n",
      "Iteration 1693, loss = 0.15606570\n",
      "Iteration 1694, loss = 0.15603570\n",
      "Iteration 1497, loss = 0.20184192\n",
      "Iteration 842, loss = 0.30872190\n",
      "Iteration 1695, loss = 0.15584310\n",
      "Iteration 1696, loss = 0.15572097\n",
      "Iteration 1498, loss = 0.20171113\n",
      "Iteration 843, loss = 0.30874491\n",
      "Iteration 1697, loss = 0.15568756\n",
      "Iteration 1499, loss = 0.20154000\n",
      "Iteration 1698, loss = 0.15550281\n",
      "Iteration 844, loss = 0.30840769\n",
      "Iteration 1699, loss = 0.15554934\n",
      "Iteration 1700, loss = 0.15533464\n",
      "Iteration 1500, loss = 0.20142963\n",
      "Iteration 845, loss = 0.30826715\n",
      "Iteration 1701, loss = 0.15504864\n",
      "Iteration 1702, loss = 0.15509197\n",
      "Iteration 846, loss = 0.30809807\n",
      "Iteration 1501, loss = 0.20128963\n",
      "Iteration 1703, loss = 0.15480101\n",
      "Iteration 1704, loss = 0.15467779\n",
      "Iteration 1502, loss = 0.20101168\n",
      "Iteration 847, loss = 0.30797307\n",
      "Iteration 1705, loss = 0.15465785\n",
      "Iteration 1706, loss = 0.15445902\n",
      "Iteration 1503, loss = 0.20094341\n",
      "Iteration 848, loss = 0.30780721\n",
      "Iteration 1707, loss = 0.15443452\n",
      "Iteration 1708, loss = 0.15417257\n",
      "Iteration 1504, loss = 0.20074004\n",
      "Iteration 849, loss = 0.30765158\n",
      "Iteration 1709, loss = 0.15406905\n",
      "Iteration 1710, loss = 0.15390514\n",
      "Iteration 1505, loss = 0.20058538\n",
      "Iteration 850, loss = 0.30752420\n",
      "Iteration 1711, loss = 0.15387713\n",
      "Iteration 1712, loss = 0.15392211\n",
      "Iteration 1506, loss = 0.20059156\n",
      "Iteration 851, loss = 0.30739560\n",
      "Iteration 1713, loss = 0.15374625\n",
      "Iteration 1714, loss = 0.15345994\n",
      "Iteration 1507, loss = 0.20031497\n",
      "Iteration 852, loss = 0.30738885\n",
      "Iteration 1715, loss = 0.15336682\n",
      "Iteration 1716, loss = 0.15330081\n",
      "Iteration 1508, loss = 0.20012583\n",
      "Iteration 853, loss = 0.30703742\n",
      "Iteration 1717, loss = 0.15318110\n",
      "Iteration 1718, loss = 0.15306068\n",
      "Iteration 1509, loss = 0.20005057\n",
      "Iteration 854, loss = 0.30689450\n",
      "Iteration 1719, loss = 0.15293842\n",
      "Iteration 1510, loss = 0.19984355\n",
      "Iteration 1720, loss = 0.15281946\n",
      "Iteration 855, loss = 0.30676993\n",
      "Iteration 1721, loss = 0.15260369\n",
      "Iteration 1511, loss = 0.19974565\n",
      "Iteration 1722, loss = 0.15252745\n",
      "Iteration 856, loss = 0.30662139\n",
      "Iteration 1723, loss = 0.15244097\n",
      "Iteration 1512, loss = 0.19956452\n",
      "Iteration 1724, loss = 0.15256085\n",
      "Iteration 857, loss = 0.30641008\n",
      "Iteration 1725, loss = 0.15217462\n",
      "Iteration 1513, loss = 0.19940735\n",
      "Iteration 858, loss = 0.30634209\n",
      "Iteration 1726, loss = 0.15198381\n",
      "Iteration 1727, loss = 0.15203628\n",
      "Iteration 1514, loss = 0.19931515\n",
      "Iteration 859, loss = 0.30612896\n",
      "Iteration 1728, loss = 0.15177629\n",
      "Iteration 1729, loss = 0.15175616\n",
      "Iteration 1515, loss = 0.19908697\n",
      "Iteration 1730, loss = 0.15155046Iteration 860, loss = 0.30598842\n",
      "\n",
      "Iteration 1731, loss = 0.15149628\n",
      "Iteration 1516, loss = 0.19898938\n",
      "Iteration 861, loss = 0.30581394\n",
      "Iteration 1732, loss = 0.15134538\n",
      "Iteration 1733, loss = 0.15116517\n",
      "Iteration 1517, loss = 0.19914272\n",
      "Iteration 862, loss = 0.30563694\n",
      "Iteration 1734, loss = 0.15117493\n",
      "Iteration 1735, loss = 0.15092986\n",
      "Iteration 1518, loss = 0.19890810\n",
      "Iteration 863, loss = 0.30560872\n",
      "Iteration 1736, loss = 0.15087828\n",
      "Iteration 1737, loss = 0.15098495\n",
      "Iteration 1519, loss = 0.19845880\n",
      "Iteration 864, loss = 0.30535021\n",
      "Iteration 1738, loss = 0.15069415\n",
      "Iteration 1739, loss = 0.15047355\n",
      "Iteration 1520, loss = 0.19848245\n",
      "Iteration 865, loss = 0.30521802\n",
      "Iteration 1740, loss = 0.15037869\n",
      "Iteration 1741, loss = 0.15028719\n",
      "Iteration 1521, loss = 0.19825521\n",
      "Iteration 866, loss = 0.30504698\n",
      "Iteration 1742, loss = 0.15013904\n",
      "Iteration 1743, loss = 0.15025329\n",
      "Iteration 1522, loss = 0.19799474\n",
      "Iteration 867, loss = 0.30491697\n",
      "Iteration 1744, loss = 0.15001689\n",
      "Iteration 1745, loss = 0.14977859\n",
      "Iteration 1523, loss = 0.19791011\n",
      "Iteration 868, loss = 0.30476159\n",
      "Iteration 1746, loss = 0.14967984\n",
      "Iteration 1747, loss = 0.14964003\n",
      "Iteration 1524, loss = 0.19772390\n",
      "Iteration 869, loss = 0.30458302\n",
      "Iteration 1748, loss = 0.14946412\n",
      "Iteration 1749, loss = 0.14950092\n",
      "Iteration 1525, loss = 0.19767760\n",
      "Iteration 870, loss = 0.30463256\n",
      "Iteration 1750, loss = 0.14920893\n",
      "Iteration 1751, loss = 0.14914033\n",
      "Iteration 1526, loss = 0.19747150\n",
      "Iteration 871, loss = 0.30428550\n",
      "Iteration 1752, loss = 0.14895517\n",
      "Iteration 1753, loss = 0.14893496\n",
      "Iteration 1527, loss = 0.19737239\n",
      "Iteration 872, loss = 0.30414766\n",
      "Iteration 1754, loss = 0.14877141\n",
      "Iteration 1755, loss = 0.14874540\n",
      "Iteration 1528, loss = 0.19726403\n",
      "Iteration 873, loss = 0.30397757\n",
      "Iteration 1756, loss = 0.14850906\n",
      "Iteration 1529, loss = 0.19707718\n",
      "Iteration 1757, loss = 0.14838914\n",
      "Iteration 874, loss = 0.30383408\n",
      "Iteration 1758, loss = 0.14832474\n",
      "Iteration 1530, loss = 0.19698665\n",
      "Iteration 1759, loss = 0.14817338\n",
      "Iteration 1760, loss = 0.14815685\n",
      "Iteration 875, loss = 0.30364721\n",
      "Iteration 1531, loss = 0.19728411\n",
      "Iteration 1761, loss = 0.14797722\n",
      "Iteration 876, loss = 0.30367977\n",
      "Iteration 1532, loss = 0.19664781\n",
      "Iteration 1762, loss = 0.14784459\n",
      "Iteration 1763, loss = 0.14794144\n",
      "Iteration 877, loss = 0.30353735\n",
      "Iteration 1533, loss = 0.19656353\n",
      "Iteration 1764, loss = 0.14768176\n",
      "Iteration 1765, loss = 0.14750624\n",
      "Iteration 878, loss = 0.30318753\n",
      "Iteration 1534, loss = 0.19635822\n",
      "Iteration 1766, loss = 0.14740289\n",
      "Iteration 1767, loss = 0.14742344\n",
      "Iteration 879, loss = 0.30304068\n",
      "Iteration 1535, loss = 0.19611652\n",
      "Iteration 1768, loss = 0.14728602\n",
      "Iteration 1769, loss = 0.14708767\n",
      "Iteration 880, loss = 0.30295081\n",
      "Iteration 1536, loss = 0.19621027\n",
      "Iteration 1770, loss = 0.14695222\n",
      "Iteration 1771, loss = 0.14683710\n",
      "Iteration 881, loss = 0.30276319\n",
      "Iteration 1537, loss = 0.19589124\n",
      "Iteration 1772, loss = 0.14683768\n",
      "Iteration 882, loss = 0.30263625\n",
      "Iteration 1773, loss = 0.14668630\n",
      "Iteration 1538, loss = 0.19583967\n",
      "Iteration 1774, loss = 0.14672291\n",
      "Iteration 883, loss = 0.30244809\n",
      "Iteration 1775, loss = 0.14642804\n",
      "Iteration 1539, loss = 0.19557344\n",
      "Iteration 1776, loss = 0.14632946\n",
      "Iteration 884, loss = 0.30241869\n",
      "Iteration 1777, loss = 0.14619958\n",
      "Iteration 1540, loss = 0.19538081\n",
      "Iteration 1778, loss = 0.14610039\n",
      "Iteration 885, loss = 0.30222999\n",
      "Iteration 1779, loss = 0.14597100\n",
      "Iteration 1541, loss = 0.19527799\n",
      "Iteration 1780, loss = 0.14586785\n",
      "Iteration 886, loss = 0.30207522\n",
      "Iteration 1781, loss = 0.14579127\n",
      "Iteration 1542, loss = 0.19516069\n",
      "Iteration 1782, loss = 0.14568429\n",
      "Iteration 887, loss = 0.30182903\n",
      "Iteration 1543, loss = 0.19502364\n",
      "Iteration 1783, loss = 0.14556664\n",
      "Iteration 1784, loss = 0.14547227\n",
      "Iteration 888, loss = 0.30168324\n",
      "Iteration 1544, loss = 0.19487617\n",
      "Iteration 1785, loss = 0.14554392\n",
      "Iteration 1786, loss = 0.14520397\n",
      "Iteration 889, loss = 0.30150380\n",
      "Iteration 1787, loss = 0.14510741\n",
      "Iteration 1545, loss = 0.19469801\n",
      "Iteration 1788, loss = 0.14506295\n",
      "Iteration 890, loss = 0.30142862\n",
      "Iteration 1546, loss = 0.19458550\n",
      "Iteration 1789, loss = 0.14486002\n",
      "Iteration 1790, loss = 0.14478343\n",
      "Iteration 891, loss = 0.30113715\n",
      "Iteration 1547, loss = 0.19447797\n",
      "Iteration 1791, loss = 0.14484301\n",
      "Iteration 1792, loss = 0.14466809\n",
      "Iteration 892, loss = 0.30100574\n",
      "Iteration 1548, loss = 0.19436692\n",
      "Iteration 1793, loss = 0.14453016\n",
      "Iteration 1794, loss = 0.14431845\n",
      "Iteration 893, loss = 0.30085284\n",
      "Iteration 1549, loss = 0.19416492\n",
      "Iteration 1795, loss = 0.14425602\n",
      "Iteration 1796, loss = 0.14412351\n",
      "Iteration 894, loss = 0.30069202\n",
      "Iteration 1550, loss = 0.19406585\n",
      "Iteration 1797, loss = 0.14399520\n",
      "Iteration 895, loss = 0.30056604\n",
      "Iteration 1798, loss = 0.14386079\n",
      "Iteration 1551, loss = 0.19400490\n",
      "Iteration 1799, loss = 0.14386028\n",
      "Iteration 896, loss = 0.30038738\n",
      "Iteration 1800, loss = 0.14395027\n",
      "Iteration 1552, loss = 0.19378389\n",
      "Iteration 1801, loss = 0.14398360\n",
      "Iteration 897, loss = 0.30023065\n",
      "Iteration 1802, loss = 0.14349641\n",
      "Iteration 1553, loss = 0.19372689\n",
      "Iteration 1803, loss = 0.14338784\n",
      "Iteration 898, loss = 0.30006582\n",
      "Iteration 1804, loss = 0.14342094\n",
      "Iteration 1554, loss = 0.19353976\n",
      "Iteration 1805, loss = 0.14316515\n",
      "Iteration 899, loss = 0.29996898\n",
      "Iteration 1806, loss = 0.14307586\n",
      "Iteration 1555, loss = 0.19329153\n",
      "Iteration 1807, loss = 0.14305351\n",
      "Iteration 900, loss = 0.29974971\n",
      "Iteration 1808, loss = 0.14299718\n",
      "Iteration 1556, loss = 0.19323266\n",
      "Iteration 1809, loss = 0.14301783\n",
      "Iteration 901, loss = 0.29974663\n",
      "Iteration 1810, loss = 0.14274972\n",
      "Iteration 1557, loss = 0.19313738\n",
      "Iteration 1811, loss = 0.14255991\n",
      "Iteration 902, loss = 0.29946026\n",
      "Iteration 1812, loss = 0.14244887\n",
      "Iteration 1813, loss = 0.14237207\n",
      "Iteration 1558, loss = 0.19286475\n",
      "Iteration 903, loss = 0.29933074\n",
      "Iteration 1814, loss = 0.14226485\n",
      "Iteration 1559, loss = 0.19273756\n",
      "Iteration 1815, loss = 0.14209308\n",
      "Iteration 904, loss = 0.29908619\n",
      "Iteration 1816, loss = 0.14227707\n",
      "Iteration 1560, loss = 0.19258092\n",
      "Iteration 1817, loss = 0.14200710\n",
      "Iteration 1818, loss = 0.14178503\n",
      "Iteration 905, loss = 0.29897421\n",
      "Iteration 1819, loss = 0.14174554\n",
      "Iteration 1561, loss = 0.19247483\n",
      "Iteration 1820, loss = 0.14171944\n",
      "Iteration 906, loss = 0.29892903\n",
      "Iteration 1562, loss = 0.19242163\n",
      "Iteration 1821, loss = 0.14150918\n",
      "Iteration 907, loss = 0.29873351\n",
      "Iteration 1822, loss = 0.14157156\n",
      "Iteration 1563, loss = 0.19216929\n",
      "Iteration 1823, loss = 0.14136772\n",
      "Iteration 908, loss = 0.29854563\n",
      "Iteration 1824, loss = 0.14121783\n",
      "Iteration 1564, loss = 0.19210344\n",
      "Iteration 1825, loss = 0.14117004\n",
      "Iteration 909, loss = 0.29839773\n",
      "Iteration 1826, loss = 0.14107353\n",
      "Iteration 1827, loss = 0.14103265\n",
      "Iteration 1565, loss = 0.19191577\n",
      "Iteration 1828, loss = 0.14086299\n",
      "Iteration 910, loss = 0.29821530\n",
      "Iteration 1829, loss = 0.14071632\n",
      "Iteration 1566, loss = 0.19174809\n",
      "Iteration 1830, loss = 0.14061424\n",
      "Iteration 911, loss = 0.29802587\n",
      "Iteration 1567, loss = 0.19175084\n",
      "Iteration 1831, loss = 0.14060769\n",
      "Iteration 912, loss = 0.29788085\n",
      "Iteration 1832, loss = 0.14055788\n",
      "Iteration 1568, loss = 0.19175884\n",
      "Iteration 1833, loss = 0.14030493\n",
      "Iteration 913, loss = 0.29773812\n",
      "Iteration 1834, loss = 0.14024445\n",
      "Iteration 1569, loss = 0.19167146\n",
      "Iteration 1835, loss = 0.14033180\n",
      "Iteration 914, loss = 0.29757921\n",
      "Iteration 1836, loss = 0.14019107\n",
      "Iteration 1570, loss = 0.19123362\n",
      "Iteration 1837, loss = 0.13993086\n",
      "Iteration 915, loss = 0.29747896\n",
      "Iteration 1838, loss = 0.13984632\n",
      "Iteration 1571, loss = 0.19110317\n",
      "Iteration 1839, loss = 0.13976460\n",
      "Iteration 916, loss = 0.29726012\n",
      "Iteration 1840, loss = 0.13973676\n",
      "Iteration 1572, loss = 0.19096116\n",
      "Iteration 1841, loss = 0.13956994\n",
      "Iteration 917, loss = 0.29714947\n",
      "Iteration 1842, loss = 0.13948115\n",
      "Iteration 1573, loss = 0.19080965\n",
      "Iteration 1843, loss = 0.13937430\n",
      "Iteration 918, loss = 0.29692576\n",
      "Iteration 1844, loss = 0.13941387\n",
      "Iteration 1574, loss = 0.19066598\n",
      "Iteration 1845, loss = 0.13928663\n",
      "Iteration 919, loss = 0.29674890\n",
      "Iteration 1846, loss = 0.13905848\n",
      "Iteration 1575, loss = 0.19056428\n",
      "Iteration 1847, loss = 0.13891964\n",
      "Iteration 920, loss = 0.29663618\n",
      "Iteration 1848, loss = 0.13881704\n",
      "Iteration 1576, loss = 0.19049238\n",
      "Iteration 1849, loss = 0.13870066\n",
      "Iteration 921, loss = 0.29647761\n",
      "Iteration 1850, loss = 0.13861738\n",
      "Iteration 1577, loss = 0.19041270\n",
      "Iteration 1851, loss = 0.13868177\n",
      "Iteration 922, loss = 0.29625110\n",
      "Iteration 1852, loss = 0.13845764\n",
      "Iteration 1578, loss = 0.19023631\n",
      "Iteration 1853, loss = 0.13839857\n",
      "Iteration 923, loss = 0.29610223\n",
      "Iteration 1854, loss = 0.13823543\n",
      "Iteration 1579, loss = 0.19001625\n",
      "Iteration 1855, loss = 0.13821202\n",
      "Iteration 924, loss = 0.29620586\n",
      "Iteration 1856, loss = 0.13813273\n",
      "Iteration 1580, loss = 0.19003247\n",
      "Iteration 1857, loss = 0.13833128\n",
      "Iteration 925, loss = 0.29578016\n",
      "Iteration 1858, loss = 0.13790290\n",
      "Iteration 1859, loss = 0.13789705\n",
      "Iteration 926, loss = 0.29566306\n",
      "Iteration 1581, loss = 0.18994321\n",
      "Iteration 1860, loss = 0.13795327\n",
      "Iteration 1861, loss = 0.13759337\n",
      "Iteration 1582, loss = 0.18970944\n",
      "Iteration 927, loss = 0.29555573\n",
      "Iteration 1862, loss = 0.13750703\n",
      "Iteration 1583, loss = 0.18949544\n",
      "Iteration 928, loss = 0.29530969\n",
      "Iteration 1863, loss = 0.13745794\n",
      "Iteration 1864, loss = 0.13746299\n",
      "Iteration 1584, loss = 0.18935820\n",
      "Iteration 929, loss = 0.29519638\n",
      "Iteration 1865, loss = 0.13722823\n",
      "Iteration 1866, loss = 0.13715482\n",
      "Iteration 1585, loss = 0.18927115\n",
      "Iteration 930, loss = 0.29498403\n",
      "Iteration 1867, loss = 0.13702504\n",
      "Iteration 1868, loss = 0.13704209\n",
      "Iteration 1586, loss = 0.18923979\n",
      "Iteration 931, loss = 0.29481976\n",
      "Iteration 1869, loss = 0.13687697\n",
      "Iteration 1870, loss = 0.13676526\n",
      "Iteration 1587, loss = 0.18899861\n",
      "Iteration 932, loss = 0.29478215\n",
      "Iteration 1871, loss = 0.13673579\n",
      "Iteration 1872, loss = 0.13657487\n",
      "Iteration 1588, loss = 0.18880227\n",
      "Iteration 933, loss = 0.29453480\n",
      "Iteration 1873, loss = 0.13651545\n",
      "Iteration 1874, loss = 0.13654577\n",
      "Iteration 1589, loss = 0.18871102\n",
      "Iteration 934, loss = 0.29436861\n",
      "Iteration 1875, loss = 0.13648477\n",
      "Iteration 1876, loss = 0.13620569\n",
      "Iteration 1590, loss = 0.18854816\n",
      "Iteration 935, loss = 0.29427931\n",
      "Iteration 1877, loss = 0.13634888\n",
      "Iteration 1878, loss = 0.13612976\n",
      "Iteration 1591, loss = 0.18844864\n",
      "Iteration 936, loss = 0.29409165\n",
      "Iteration 1879, loss = 0.13607756\n",
      "Iteration 1880, loss = 0.13594858\n",
      "Iteration 1592, loss = 0.18840943\n",
      "Iteration 1881, loss = 0.13580598\n",
      "Iteration 937, loss = 0.29388845\n",
      "Iteration 1882, loss = 0.13567060\n",
      "Iteration 938, loss = 0.29374300\n",
      "Iteration 1593, loss = 0.18817622\n",
      "Iteration 1883, loss = 0.13570480\n",
      "Iteration 1884, loss = 0.13556059\n",
      "Iteration 1594, loss = 0.18805698\n",
      "Iteration 939, loss = 0.29359931\n",
      "Iteration 1885, loss = 0.13545910\n",
      "Iteration 1886, loss = 0.13538238\n",
      "Iteration 1595, loss = 0.18794430\n",
      "Iteration 940, loss = 0.29342234\n",
      "Iteration 1887, loss = 0.13532150\n",
      "Iteration 1888, loss = 0.13512142\n",
      "Iteration 1596, loss = 0.18787446\n",
      "Iteration 941, loss = 0.29329571\n",
      "Iteration 1889, loss = 0.13514360\n",
      "Iteration 1890, loss = 0.13504728\n",
      "Iteration 1597, loss = 0.18772320\n",
      "Iteration 942, loss = 0.29308019\n",
      "Iteration 1891, loss = 0.13487718\n",
      "Iteration 1892, loss = 0.13482650\n",
      "Iteration 1598, loss = 0.18791424\n",
      "Iteration 943, loss = 0.29298000\n",
      "Iteration 1893, loss = 0.13473725\n",
      "Iteration 1894, loss = 0.13460004\n",
      "Iteration 1599, loss = 0.18735078\n",
      "Iteration 944, loss = 0.29278927\n",
      "Iteration 1895, loss = 0.13451204\n",
      "Iteration 1896, loss = 0.13446286\n",
      "Iteration 1600, loss = 0.18733430\n",
      "Iteration 945, loss = 0.29259164\n",
      "Iteration 1897, loss = 0.13456059\n",
      "Iteration 1898, loss = 0.13426025\n",
      "Iteration 1899, loss = 0.13440581\n",
      "Iteration 946, loss = 0.29242911\n",
      "Iteration 1601, loss = 0.18721727\n",
      "Iteration 1900, loss = 0.13416709\n",
      "Iteration 1901, loss = 0.13397279\n",
      "Iteration 947, loss = 0.29230642\n",
      "Iteration 1902, loss = 0.13393751\n",
      "Iteration 1602, loss = 0.18711127\n",
      "Iteration 1903, loss = 0.13394483\n",
      "Iteration 1904, loss = 0.13373799\n",
      "Iteration 1603, loss = 0.18699743\n",
      "Iteration 948, loss = 0.29234306\n",
      "Iteration 1905, loss = 0.13376025\n",
      "Iteration 1604, loss = 0.18673615\n",
      "Iteration 949, loss = 0.29199242\n",
      "Iteration 1906, loss = 0.13370248\n",
      "Iteration 1907, loss = 0.13345081\n",
      "Iteration 1908, loss = 0.13336988\n",
      "Iteration 950, loss = 0.29179526\n",
      "Iteration 1605, loss = 0.18662377\n",
      "Iteration 1909, loss = 0.13331831\n",
      "Iteration 1910, loss = 0.13323575\n",
      "Iteration 1911, loss = 0.13320050\n",
      "Iteration 951, loss = 0.29172708\n",
      "Iteration 1606, loss = 0.18654048\n",
      "Iteration 1912, loss = 0.13304151\n",
      "Iteration 1913, loss = 0.13306778\n",
      "Iteration 1914, loss = 0.13296216\n",
      "Iteration 952, loss = 0.29183868\n",
      "Iteration 1607, loss = 0.18634961\n",
      "Iteration 1915, loss = 0.13284600\n",
      "Iteration 1916, loss = 0.13273334\n",
      "Iteration 953, loss = 0.29133183\n",
      "Iteration 1608, loss = 0.18632522\n",
      "Iteration 1917, loss = 0.13291381\n",
      "Iteration 954, loss = 0.29115207\n",
      "Iteration 1918, loss = 0.13262881\n",
      "Iteration 1609, loss = 0.18608012\n",
      "Iteration 1919, loss = 0.13277497\n",
      "Iteration 955, loss = 0.29107557\n",
      "Iteration 1920, loss = 0.13242905\n",
      "Iteration 1610, loss = 0.18605285\n",
      "Iteration 1921, loss = 0.13258864\n",
      "Iteration 956, loss = 0.29085760\n",
      "Iteration 1922, loss = 0.13233278\n",
      "Iteration 1611, loss = 0.18603779\n",
      "Iteration 1923, loss = 0.13218022\n",
      "Iteration 957, loss = 0.29075344\n",
      "Iteration 1924, loss = 0.13222180\n",
      "Iteration 1925, loss = 0.13200445\n",
      "Iteration 958, loss = 0.29057819\n",
      "Iteration 1612, loss = 0.18572466\n",
      "Iteration 1926, loss = 0.13193266\n",
      "Iteration 959, loss = 0.29036302\n",
      "Iteration 1927, loss = 0.13191609\n",
      "Iteration 1928, loss = 0.13180996\n",
      "Iteration 1613, loss = 0.18564159\n",
      "Iteration 960, loss = 0.29019845\n",
      "Iteration 1929, loss = 0.13172400\n",
      "Iteration 1930, loss = 0.13181366\n",
      "Iteration 1614, loss = 0.18551910\n",
      "Iteration 961, loss = 0.28999755\n",
      "Iteration 1931, loss = 0.13183253\n",
      "Iteration 1932, loss = 0.13141126\n",
      "Iteration 1615, loss = 0.18550261\n",
      "Iteration 962, loss = 0.28986922\n",
      "Iteration 1933, loss = 0.13134926\n",
      "Iteration 1616, loss = 0.18521069\n",
      "Iteration 1934, loss = 0.13121130\n",
      "Iteration 963, loss = 0.28967833\n",
      "Iteration 1935, loss = 0.13118528\n",
      "Iteration 1936, loss = 0.13115041\n",
      "Iteration 1617, loss = 0.18513359\n",
      "Iteration 964, loss = 0.28963741\n",
      "Iteration 1937, loss = 0.13105962\n",
      "Iteration 1938, loss = 0.13108149\n",
      "Iteration 1618, loss = 0.18498917\n",
      "Iteration 965, loss = 0.28938412\n",
      "Iteration 1939, loss = 0.13104415\n",
      "Iteration 1940, loss = 0.13078798\n",
      "Iteration 1619, loss = 0.18489585\n",
      "Iteration 966, loss = 0.28920089\n",
      "Iteration 1941, loss = 0.13076173\n",
      "Iteration 1942, loss = 0.13059384\n",
      "Iteration 1620, loss = 0.18467598\n",
      "Iteration 967, loss = 0.28905543\n",
      "Iteration 1943, loss = 0.13049775\n",
      "Iteration 1944, loss = 0.13047703\n",
      "Iteration 1621, loss = 0.18458936\n",
      "Iteration 968, loss = 0.28885462\n",
      "Iteration 1945, loss = 0.13062539\n",
      "Iteration 1946, loss = 0.13046506\n",
      "Iteration 1622, loss = 0.18446544\n",
      "Iteration 969, loss = 0.28883665\n",
      "Iteration 1947, loss = 0.13035526\n",
      "Iteration 1948, loss = 0.13011371\n",
      "Iteration 1623, loss = 0.18434030\n",
      "Iteration 970, loss = 0.28853678\n",
      "Iteration 1949, loss = 0.13016647\n",
      "Iteration 1950, loss = 0.13000889\n",
      "Iteration 1624, loss = 0.18424560\n",
      "Iteration 971, loss = 0.28836271\n",
      "Iteration 1951, loss = 0.12986981\n",
      "Iteration 1952, loss = 0.12982067\n",
      "Iteration 1625, loss = 0.18414346\n",
      "Iteration 972, loss = 0.28825794\n",
      "Iteration 1953, loss = 0.12984153\n",
      "Iteration 1954, loss = 0.12965556\n",
      "Iteration 1626, loss = 0.18400360\n",
      "Iteration 973, loss = 0.28808215\n",
      "Iteration 1955, loss = 0.12955311\n",
      "Iteration 1956, loss = 0.12950028\n",
      "Iteration 1627, loss = 0.18386359\n",
      "Iteration 974, loss = 0.28788780\n",
      "Iteration 1957, loss = 0.12948462\n",
      "Iteration 1958, loss = 0.12944766\n",
      "Iteration 1628, loss = 0.18376753\n",
      "Iteration 975, loss = 0.28775341\n",
      "Iteration 1959, loss = 0.12931351\n",
      "Iteration 1960, loss = 0.12926547\n",
      "Iteration 1629, loss = 0.18362045\n",
      "Iteration 976, loss = 0.28759707\n",
      "Iteration 1961, loss = 0.12917148\n",
      "Iteration 1962, loss = 0.12909638\n",
      "Iteration 1630, loss = 0.18353603\n",
      "Iteration 977, loss = 0.28737839\n",
      "Iteration 1963, loss = 0.12904222\n",
      "Iteration 1964, loss = 0.12900622\n",
      "Iteration 1631, loss = 0.18338209\n",
      "Iteration 978, loss = 0.28726593\n",
      "Iteration 1965, loss = 0.12891567\n",
      "Iteration 1966, loss = 0.12897628\n",
      "Iteration 1632, loss = 0.18323517\n",
      "Iteration 979, loss = 0.28707734\n",
      "Iteration 1967, loss = 0.12868920\n",
      "Iteration 1968, loss = 0.12858577\n",
      "Iteration 1633, loss = 0.18323989\n",
      "Iteration 980, loss = 0.28693163\n",
      "Iteration 1969, loss = 0.12855210\n",
      "Iteration 1970, loss = 0.12851619\n",
      "Iteration 1634, loss = 0.18306701\n",
      "Iteration 981, loss = 0.28673999\n",
      "Iteration 1971, loss = 0.12857920\n",
      "Iteration 1972, loss = 0.12831400\n",
      "Iteration 1635, loss = 0.18292915\n",
      "Iteration 982, loss = 0.28663387\n",
      "Iteration 1973, loss = 0.12833613\n",
      "Iteration 1974, loss = 0.12818221\n",
      "Iteration 1636, loss = 0.18280315\n",
      "Iteration 983, loss = 0.28640580\n",
      "Iteration 1975, loss = 0.12826112\n",
      "Iteration 1976, loss = 0.12802354\n",
      "Iteration 1637, loss = 0.18264620\n",
      "Iteration 984, loss = 0.28634555\n",
      "Iteration 1977, loss = 0.12800658\n",
      "Iteration 1978, loss = 0.12801786\n",
      "Iteration 1638, loss = 0.18260146\n",
      "Iteration 985, loss = 0.28609869\n",
      "Iteration 1979, loss = 0.12777685\n",
      "Iteration 1639, loss = 0.18253459\n",
      "Iteration 1980, loss = 0.12773511\n",
      "Iteration 986, loss = 0.28596954\n",
      "Iteration 1981, loss = 0.12775363\n",
      "Iteration 1640, loss = 0.18232778\n",
      "Iteration 1982, loss = 0.12758061\n",
      "Iteration 987, loss = 0.28577397\n",
      "Iteration 1983, loss = 0.12760816\n",
      "Iteration 1641, loss = 0.18219525\n",
      "Iteration 1984, loss = 0.12754224\n",
      "Iteration 988, loss = 0.28558924\n",
      "Iteration 1985, loss = 0.12738332\n",
      "Iteration 1642, loss = 0.18211628\n",
      "Iteration 1986, loss = 0.12725165\n",
      "Iteration 989, loss = 0.28551338\n",
      "Iteration 1987, loss = 0.12729599\n",
      "Iteration 1643, loss = 0.18215581\n",
      "Iteration 1988, loss = 0.12731809\n",
      "Iteration 990, loss = 0.28528800\n",
      "Iteration 1989, loss = 0.12706791\n",
      "Iteration 1644, loss = 0.18180972\n",
      "Iteration 1990, loss = 0.12703975\n",
      "Iteration 991, loss = 0.28511013\n",
      "Iteration 1991, loss = 0.12707910\n",
      "Iteration 1645, loss = 0.18178521\n",
      "Iteration 1992, loss = 0.12692226\n",
      "Iteration 992, loss = 0.28506578\n",
      "Iteration 1993, loss = 0.12686103\n",
      "Iteration 1646, loss = 0.18156490\n",
      "Iteration 1994, loss = 0.12675508\n",
      "Iteration 993, loss = 0.28483003\n",
      "Iteration 1995, loss = 0.12663372\n",
      "Iteration 1647, loss = 0.18152986\n",
      "Iteration 1996, loss = 0.12663139\n",
      "Iteration 994, loss = 0.28461102\n",
      "Iteration 1997, loss = 0.12653070\n",
      "Iteration 1648, loss = 0.18137201\n",
      "Iteration 1998, loss = 0.12640508\n",
      "Iteration 995, loss = 0.28447654\n",
      "Iteration 1999, loss = 0.12644711\n",
      "Iteration 1649, loss = 0.18120797\n",
      "Iteration 2000, loss = 0.12648357\n",
      "Iteration 996, loss = 0.28426683\n",
      "Iteration 2001, loss = 0.12622505\n",
      "Iteration 1650, loss = 0.18108379\n",
      "Iteration 2002, loss = 0.12612207\n",
      "Iteration 997, loss = 0.28409722\n",
      "Iteration 2003, loss = 0.12606754\n",
      "Iteration 1651, loss = 0.18103190\n",
      "Iteration 2004, loss = 0.12612937\n",
      "Iteration 998, loss = 0.28394923\n",
      "Iteration 2005, loss = 0.12602345\n",
      "Iteration 1652, loss = 0.18095425\n",
      "Iteration 2006, loss = 0.12592855\n",
      "Iteration 999, loss = 0.28381553\n",
      "Iteration 1653, loss = 0.18082437\n",
      "Iteration 2007, loss = 0.12583724\n",
      "Iteration 2008, loss = 0.12587811\n",
      "Iteration 1000, loss = 0.28363046\n",
      "Iteration 1654, loss = 0.18068585\n",
      "Iteration 2009, loss = 0.12572994\n",
      "Iteration 2010, loss = 0.12572358\n",
      "Iteration 1001, loss = 0.28344660\n",
      "Iteration 1655, loss = 0.18090001\n",
      "Iteration 2011, loss = 0.12558194\n",
      "Iteration 2012, loss = 0.12553016\n",
      "Iteration 1002, loss = 0.28326274\n",
      "Iteration 1656, loss = 0.18040060\n",
      "Iteration 2013, loss = 0.12547362\n",
      "Iteration 2014, loss = 0.12543641\n",
      "Iteration 1003, loss = 0.28313661\n",
      "Iteration 1657, loss = 0.18032313\n",
      "Iteration 2015, loss = 0.12539313\n",
      "Iteration 2016, loss = 0.12522432\n",
      "Iteration 1004, loss = 0.28303275\n",
      "Iteration 1658, loss = 0.18042765\n",
      "Iteration 2017, loss = 0.12516348\n",
      "Iteration 2018, loss = 0.12512572\n",
      "Iteration 1005, loss = 0.28286150\n",
      "Iteration 1659, loss = 0.18018818\n",
      "Iteration 2019, loss = 0.12512230\n",
      "Iteration 2020, loss = 0.12491440\n",
      "Iteration 1006, loss = 0.28266348\n",
      "Iteration 1660, loss = 0.18004310\n",
      "Iteration 2021, loss = 0.12501643\n",
      "Iteration 2022, loss = 0.12484960\n",
      "Iteration 1007, loss = 0.28246420\n",
      "Iteration 1661, loss = 0.17989527\n",
      "Iteration 2023, loss = 0.12473060\n",
      "Iteration 2024, loss = 0.12480950\n",
      "Iteration 1008, loss = 0.28227499\n",
      "Iteration 1662, loss = 0.17975194\n",
      "Iteration 2025, loss = 0.12458569\n",
      "Iteration 2026, loss = 0.12460640\n",
      "Iteration 1009, loss = 0.28215522\n",
      "Iteration 2027, loss = 0.12471539\n",
      "Iteration 1663, loss = 0.17964934\n",
      "Iteration 2028, loss = 0.12442730\n",
      "Iteration 1010, loss = 0.28195799\n",
      "Iteration 2029, loss = 0.12437336\n",
      "Iteration 1664, loss = 0.17965913\n",
      "Iteration 2030, loss = 0.12446019\n",
      "Iteration 1011, loss = 0.28179968\n",
      "Iteration 1665, loss = 0.17956758\n",
      "Iteration 2031, loss = 0.12431496\n",
      "Iteration 2032, loss = 0.12428523\n",
      "Iteration 1012, loss = 0.28164409\n",
      "Iteration 2033, loss = 0.12428780\n",
      "Iteration 1666, loss = 0.17934835\n",
      "Iteration 2034, loss = 0.12405507\n",
      "Iteration 1013, loss = 0.28143218\n",
      "Iteration 2035, loss = 0.12396389\n",
      "Iteration 1667, loss = 0.17921237\n",
      "Iteration 2036, loss = 0.12388747\n",
      "Iteration 1014, loss = 0.28130591\n",
      "Iteration 1668, loss = 0.17914910\n",
      "Iteration 2037, loss = 0.12385080\n",
      "Iteration 2038, loss = 0.12394043\n",
      "Iteration 1015, loss = 0.28122435\n",
      "Iteration 2039, loss = 0.12368449\n",
      "Iteration 1669, loss = 0.17900656\n",
      "Iteration 2040, loss = 0.12369015\n",
      "Iteration 1016, loss = 0.28096142\n",
      "Iteration 1670, loss = 0.17893306\n",
      "Iteration 2041, loss = 0.12354848\n",
      "Iteration 2042, loss = 0.12360628\n",
      "Iteration 1017, loss = 0.28087497\n",
      "Iteration 1671, loss = 0.17877151\n",
      "Iteration 2043, loss = 0.12349075\n",
      "Iteration 2044, loss = 0.12340871\n",
      "Iteration 1018, loss = 0.28086884\n",
      "Iteration 1672, loss = 0.17859564\n",
      "Iteration 2045, loss = 0.12333859\n",
      "Iteration 1019, loss = 0.28052632\n",
      "Iteration 2046, loss = 0.12327046\n",
      "Iteration 1673, loss = 0.17852562\n",
      "Iteration 2047, loss = 0.12321192\n",
      "Iteration 1020, loss = 0.28033367\n",
      "Iteration 2048, loss = 0.12321057\n",
      "Iteration 1674, loss = 0.17856526\n",
      "Iteration 2049, loss = 0.12307932\n",
      "Iteration 1021, loss = 0.28026153\n",
      "Iteration 2050, loss = 0.12311267\n",
      "Iteration 1675, loss = 0.17826768\n",
      "Iteration 2051, loss = 0.12294536\n",
      "Iteration 1022, loss = 0.27998447\n",
      "Iteration 2052, loss = 0.12285119\n",
      "Iteration 1676, loss = 0.17822929\n",
      "Iteration 2053, loss = 0.12290302\n",
      "Iteration 1023, loss = 0.27984221\n",
      "Iteration 2054, loss = 0.12301182\n",
      "Iteration 1677, loss = 0.17804934\n",
      "Iteration 2055, loss = 0.12273542\n",
      "Iteration 1024, loss = 0.27971029\n",
      "Iteration 2056, loss = 0.12271731\n",
      "Iteration 1678, loss = 0.17794953\n",
      "Iteration 2057, loss = 0.12255755\n",
      "Iteration 1025, loss = 0.27982537\n",
      "Iteration 2058, loss = 0.12249722\n",
      "Iteration 1679, loss = 0.17786607\n",
      "Iteration 2059, loss = 0.12246273\n",
      "Iteration 1026, loss = 0.27933665\n",
      "Iteration 2060, loss = 0.12240204\n",
      "Iteration 1680, loss = 0.17770669\n",
      "Iteration 2061, loss = 0.12241520\n",
      "Iteration 1027, loss = 0.27915206\n",
      "Iteration 2062, loss = 0.12225861\n",
      "Iteration 1681, loss = 0.17762594\n",
      "Iteration 2063, loss = 0.12226853\n",
      "Iteration 1028, loss = 0.27899201\n",
      "Iteration 2064, loss = 0.12221164\n",
      "Iteration 1682, loss = 0.17757493\n",
      "Iteration 2065, loss = 0.12204480\n",
      "Iteration 1029, loss = 0.27880721\n",
      "Iteration 2066, loss = 0.12197733\n",
      "Iteration 1683, loss = 0.17740567\n",
      "Iteration 2067, loss = 0.12204305\n",
      "Iteration 1030, loss = 0.27868472\n",
      "Iteration 2068, loss = 0.12189598\n",
      "Iteration 1684, loss = 0.17731847\n",
      "Iteration 2069, loss = 0.12185685\n",
      "Iteration 1031, loss = 0.27851971\n",
      "Iteration 2070, loss = 0.12176608\n",
      "Iteration 1685, loss = 0.17722686\n",
      "Iteration 2071, loss = 0.12179789\n",
      "Iteration 1032, loss = 0.27843149\n",
      "Iteration 2072, loss = 0.12161717\n",
      "Iteration 1686, loss = 0.17714523\n",
      "Iteration 2073, loss = 0.12176528\n",
      "Iteration 1033, loss = 0.27827554\n",
      "Iteration 1687, loss = 0.17698970\n",
      "Iteration 2074, loss = 0.12154224\n",
      "Iteration 2075, loss = 0.12156813\n",
      "Iteration 1034, loss = 0.27811011\n",
      "Iteration 1688, loss = 0.17689392\n",
      "Iteration 2076, loss = 0.12149139\n",
      "Iteration 2077, loss = 0.12139681\n",
      "Iteration 1035, loss = 0.27784576\n",
      "Iteration 1689, loss = 0.17683133\n",
      "Iteration 2078, loss = 0.12135206\n",
      "Iteration 2079, loss = 0.12125631\n",
      "Iteration 1036, loss = 0.27767320\n",
      "Iteration 1690, loss = 0.17664235\n",
      "Iteration 2080, loss = 0.12127409\n",
      "Iteration 2081, loss = 0.12110519\n",
      "Iteration 1037, loss = 0.27761788\n",
      "Iteration 1691, loss = 0.17654024\n",
      "Iteration 2082, loss = 0.12110983\n",
      "Iteration 2083, loss = 0.12105745\n",
      "Iteration 1038, loss = 0.27733756\n",
      "Iteration 1692, loss = 0.17645504\n",
      "Iteration 2084, loss = 0.12089265\n",
      "Iteration 2085, loss = 0.12084488\n",
      "Iteration 1039, loss = 0.27719345\n",
      "Iteration 1693, loss = 0.17630093\n",
      "Iteration 2086, loss = 0.12081867\n",
      "Iteration 2087, loss = 0.12084058\n",
      "Iteration 1040, loss = 0.27715682\n",
      "Iteration 1694, loss = 0.17627286\n",
      "Iteration 2088, loss = 0.12070154\n",
      "Iteration 2089, loss = 0.12069498\n",
      "Iteration 1041, loss = 0.27684290\n",
      "Iteration 1695, loss = 0.17609093\n",
      "Iteration 2090, loss = 0.12106089\n",
      "Iteration 2091, loss = 0.12053458\n",
      "Iteration 1042, loss = 0.27675647\n",
      "Iteration 1696, loss = 0.17630043\n",
      "Iteration 2092, loss = 0.12051829\n",
      "Iteration 2093, loss = 0.12044076\n",
      "Iteration 1043, loss = 0.27655262\n",
      "Iteration 1697, loss = 0.17590506\n",
      "Iteration 2094, loss = 0.12037958\n",
      "Iteration 2095, loss = 0.12030442\n",
      "Iteration 1044, loss = 0.27646087\n",
      "Iteration 1698, loss = 0.17588421\n",
      "Iteration 2096, loss = 0.12033690\n",
      "Iteration 1045, loss = 0.27620776\n",
      "Iteration 2097, loss = 0.12022839\n",
      "Iteration 1699, loss = 0.17571686\n",
      "Iteration 2098, loss = 0.12018132\n",
      "Iteration 2099, loss = 0.12008142\n",
      "Iteration 1046, loss = 0.27602531\n",
      "Iteration 1700, loss = 0.17566162\n",
      "Iteration 2100, loss = 0.12003761\n",
      "Iteration 2101, loss = 0.12018555\n",
      "Iteration 1047, loss = 0.27583542\n",
      "Iteration 2102, loss = 0.12000692\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1701, loss = 0.17560565\n",
      "Iteration 1048, loss = 0.27580584\n",
      "Iteration 1702, loss = 0.17544027\n",
      "Iteration 1049, loss = 0.27554900\n",
      "Iteration 1703, loss = 0.17529707\n",
      "Iteration 1050, loss = 0.27530069\n",
      "Iteration 1704, loss = 0.17537657\n",
      "Iteration 1051, loss = 0.27522526\n",
      "Iteration 1705, loss = 0.17526545\n",
      "Iteration 1052, loss = 0.27504934\n",
      "Iteration 1706, loss = 0.17492992\n",
      "Iteration 1053, loss = 0.27499518\n",
      "Iteration 1707, loss = 0.17485560\n",
      "Iteration 1054, loss = 0.27474711\n",
      "Iteration 1708, loss = 0.17473999\n",
      "Iteration 1055, loss = 0.27449967\n",
      "Iteration 1709, loss = 0.17462474\n",
      "Iteration 1056, loss = 0.27437947\n",
      "Iteration 1710, loss = 0.17458968\n",
      "Iteration 1057, loss = 0.27427745\n",
      "Iteration 1711, loss = 0.17445949\n",
      "Iteration 1058, loss = 0.27409395\n",
      "Iteration 1712, loss = 0.17435158\n",
      "Iteration 1059, loss = 0.27387478\n",
      "Iteration 1713, loss = 0.17456471\n",
      "Iteration 1060, loss = 0.27378883\n",
      "Iteration 1714, loss = 0.17442939\n",
      "Iteration 1061, loss = 0.27350581\n",
      "Iteration 1715, loss = 0.17396967\n",
      "Iteration 1062, loss = 0.27343586\n",
      "Iteration 1716, loss = 0.17393881\n",
      "Iteration 1063, loss = 0.27340347\n",
      "Iteration 1717, loss = 0.17394863\n",
      "Iteration 1064, loss = 0.27299864\n",
      "Iteration 1718, loss = 0.17373007\n",
      "Iteration 1065, loss = 0.27282402\n",
      "Iteration 1719, loss = 0.17359648\n",
      "Iteration 1066, loss = 0.27267948\n",
      "Iteration 1720, loss = 0.17364345\n",
      "Iteration 1067, loss = 0.27266537\n",
      "Iteration 1721, loss = 0.17347942\n",
      "Iteration 1068, loss = 0.27234569\n",
      "Iteration 1722, loss = 0.17336480\n",
      "Iteration 1069, loss = 0.27216941\n",
      "Iteration 1723, loss = 0.17326078\n",
      "Iteration 1070, loss = 0.27202067\n",
      "Iteration 1724, loss = 0.17308478\n",
      "Iteration 1071, loss = 0.27199388\n",
      "Iteration 1725, loss = 0.17299790\n",
      "Iteration 1072, loss = 0.27176868\n",
      "Iteration 1726, loss = 0.17294877\n",
      "Iteration 1073, loss = 0.27158043\n",
      "Iteration 1727, loss = 0.17287941\n",
      "Iteration 1074, loss = 0.27138452\n",
      "Iteration 1728, loss = 0.17269283\n",
      "Iteration 1075, loss = 0.27124103\n",
      "Iteration 1729, loss = 0.17290267\n",
      "Iteration 1076, loss = 0.27104911\n",
      "Iteration 1730, loss = 0.17253494\n",
      "Iteration 1077, loss = 0.27088265\n",
      "Iteration 1731, loss = 0.17240770\n",
      "Iteration 1732, loss = 0.17233307\n",
      "Iteration 1078, loss = 0.27073561\n",
      "Iteration 1733, loss = 0.17229397\n",
      "Iteration 1079, loss = 0.27059389\n",
      "Iteration 1734, loss = 0.17217312\n",
      "Iteration 1080, loss = 0.27039101\n",
      "Iteration 1081, loss = 0.27038475\n",
      "Iteration 1735, loss = 0.17202576\n",
      "Iteration 1082, loss = 0.27002930\n",
      "Iteration 1736, loss = 0.17197147\n",
      "Iteration 1737, loss = 0.17185318\n",
      "Iteration 1083, loss = 0.26995944\n",
      "Iteration 1738, loss = 0.17171011\n",
      "Iteration 1084, loss = 0.26987150\n",
      "Iteration 1739, loss = 0.17172234\n",
      "Iteration 1085, loss = 0.26950868\n",
      "Iteration 1740, loss = 0.17156876\n",
      "Iteration 1086, loss = 0.26935633\n",
      "Iteration 1741, loss = 0.17156504\n",
      "Iteration 1087, loss = 0.26926797\n",
      "Iteration 1742, loss = 0.17140876\n",
      "Iteration 1088, loss = 0.26910300\n",
      "Iteration 1743, loss = 0.17120712\n",
      "Iteration 1089, loss = 0.26888178\n",
      "Iteration 1744, loss = 0.17115012\n",
      "Iteration 1090, loss = 0.26880880\n",
      "Iteration 1745, loss = 0.17104588\n",
      "Iteration 1091, loss = 0.26858232\n",
      "Iteration 1746, loss = 0.17092770\n",
      "Iteration 1092, loss = 0.26833144\n",
      "Iteration 1747, loss = 0.17086998\n",
      "Iteration 1093, loss = 0.26824933\n",
      "Iteration 1748, loss = 0.17085968\n",
      "Iteration 1094, loss = 0.26807955\n",
      "Iteration 1749, loss = 0.17069917\n",
      "Iteration 1095, loss = 0.26784922\n",
      "Iteration 1750, loss = 0.17054139\n",
      "Iteration 1096, loss = 0.26786537\n",
      "Iteration 1097, loss = 0.26752205\n",
      "Iteration 1751, loss = 0.17055106\n",
      "Iteration 1752, loss = 0.17041467\n",
      "Iteration 1098, loss = 0.26737248\n",
      "Iteration 1753, loss = 0.17031588\n",
      "Iteration 1099, loss = 0.26715816\n",
      "Iteration 1754, loss = 0.17013641\n",
      "Iteration 1100, loss = 0.26702230\n",
      "Iteration 1101, loss = 0.26699130\n",
      "Iteration 1755, loss = 0.17007440\n",
      "Iteration 1756, loss = 0.16995856\n",
      "Iteration 1102, loss = 0.26682443\n",
      "Iteration 1103, loss = 0.26648142\n",
      "Iteration 1757, loss = 0.16996839\n",
      "Iteration 1758, loss = 0.16981421\n",
      "Iteration 1104, loss = 0.26641209\n",
      "Iteration 1759, loss = 0.16992117\n",
      "Iteration 1105, loss = 0.26639023\n",
      "Iteration 1760, loss = 0.16982700\n",
      "Iteration 1106, loss = 0.26610037\n",
      "Iteration 1761, loss = 0.16953064\n",
      "Iteration 1107, loss = 0.26597698\n",
      "Iteration 1762, loss = 0.16976800\n",
      "Iteration 1108, loss = 0.26580619\n",
      "Iteration 1763, loss = 0.16964938\n",
      "Iteration 1109, loss = 0.26549633\n",
      "Iteration 1764, loss = 0.16933418\n",
      "Iteration 1110, loss = 0.26548913\n",
      "Iteration 1765, loss = 0.16916428\n",
      "Iteration 1111, loss = 0.26518250\n",
      "Iteration 1766, loss = 0.16913189\n",
      "Iteration 1112, loss = 0.26498396\n",
      "Iteration 1767, loss = 0.16894736\n",
      "Iteration 1113, loss = 0.26485538\n",
      "Iteration 1768, loss = 0.16892053\n",
      "Iteration 1114, loss = 0.26487242\n",
      "Iteration 1769, loss = 0.16884280\n",
      "Iteration 1115, loss = 0.26452879\n",
      "Iteration 1770, loss = 0.16883309\n",
      "Iteration 1116, loss = 0.26433094\n",
      "Iteration 1771, loss = 0.16861295\n",
      "Iteration 1117, loss = 0.26414955\n",
      "Iteration 1772, loss = 0.16855814\n",
      "Iteration 1118, loss = 0.26400216\n",
      "Iteration 1773, loss = 0.16838865\n",
      "Iteration 1119, loss = 0.26386693\n",
      "Iteration 1774, loss = 0.16835389\n",
      "Iteration 1120, loss = 0.26365377\n",
      "Iteration 1775, loss = 0.16824929\n",
      "Iteration 1121, loss = 0.26355067\n",
      "Iteration 1776, loss = 0.16818836\n",
      "Iteration 1122, loss = 0.26346014\n",
      "Iteration 1777, loss = 0.16804355\n",
      "Iteration 1123, loss = 0.26317359\n",
      "Iteration 1778, loss = 0.16795619\n",
      "Iteration 1124, loss = 0.26297103\n",
      "Iteration 1779, loss = 0.16791246\n",
      "Iteration 1125, loss = 0.26279812\n",
      "Iteration 1780, loss = 0.16778473\n",
      "Iteration 1126, loss = 0.26271683\n",
      "Iteration 1781, loss = 0.16775793\n",
      "Iteration 1127, loss = 0.26247246\n",
      "Iteration 1782, loss = 0.16768825\n",
      "Iteration 1128, loss = 0.26225657\n",
      "Iteration 1783, loss = 0.16775410\n",
      "Iteration 1129, loss = 0.26211289\n",
      "Iteration 1784, loss = 0.16755933\n",
      "Iteration 1130, loss = 0.26191133\n",
      "Iteration 1785, loss = 0.16735854\n",
      "Iteration 1131, loss = 0.26192198\n",
      "Iteration 1786, loss = 0.16735356\n",
      "Iteration 1132, loss = 0.26159707\n",
      "Iteration 1787, loss = 0.16714284\n",
      "Iteration 1133, loss = 0.26142593\n",
      "Iteration 1788, loss = 0.16719003\n",
      "Iteration 1134, loss = 0.26122658\n",
      "Iteration 1789, loss = 0.16702042\n",
      "Iteration 1135, loss = 0.26106839\n",
      "Iteration 1790, loss = 0.16688122\n",
      "Iteration 1136, loss = 0.26087927\n",
      "Iteration 1791, loss = 0.16700592\n",
      "Iteration 1137, loss = 0.26079185\n",
      "Iteration 1792, loss = 0.16682019\n",
      "Iteration 1138, loss = 0.26053703\n",
      "Iteration 1793, loss = 0.16663908\n",
      "Iteration 1139, loss = 0.26034519\n",
      "Iteration 1794, loss = 0.16662143\n",
      "Iteration 1140, loss = 0.26024228\n",
      "Iteration 1795, loss = 0.16650938\n",
      "Iteration 1141, loss = 0.26002567\n",
      "Iteration 1796, loss = 0.16648325\n",
      "Iteration 1142, loss = 0.25983384\n",
      "Iteration 1797, loss = 0.16633479\n",
      "Iteration 1143, loss = 0.25973162\n",
      "Iteration 1798, loss = 0.16623100\n",
      "Iteration 1144, loss = 0.25947297\n",
      "Iteration 1799, loss = 0.16629609\n",
      "Iteration 1145, loss = 0.25938505\n",
      "Iteration 1800, loss = 0.16611940\n",
      "Iteration 1146, loss = 0.25918156\n",
      "Iteration 1801, loss = 0.16622103\n",
      "Iteration 1147, loss = 0.25901009\n",
      "Iteration 1802, loss = 0.16599366\n",
      "Iteration 1803, loss = 0.16589689\n",
      "Iteration 1148, loss = 0.25897702\n",
      "Iteration 1804, loss = 0.16572384\n",
      "Iteration 1149, loss = 0.25889059\n",
      "Iteration 1805, loss = 0.16566752\n",
      "Iteration 1150, loss = 0.25852563\n",
      "Iteration 1806, loss = 0.16551358\n",
      "Iteration 1151, loss = 0.25839164\n",
      "Iteration 1807, loss = 0.16548049\n",
      "Iteration 1152, loss = 0.25824458\n",
      "Iteration 1808, loss = 0.16534648\n",
      "Iteration 1153, loss = 0.25796638\n",
      "Iteration 1809, loss = 0.16530506\n",
      "Iteration 1154, loss = 0.25782412\n",
      "Iteration 1810, loss = 0.16523479\n",
      "Iteration 1155, loss = 0.25762332\n",
      "Iteration 1811, loss = 0.16509749\n",
      "Iteration 1156, loss = 0.25750434\n",
      "Iteration 1812, loss = 0.16504436\n",
      "Iteration 1157, loss = 0.25732984\n",
      "Iteration 1813, loss = 0.16502584\n",
      "Iteration 1158, loss = 0.25715662\n",
      "Iteration 1814, loss = 0.16487520\n",
      "Iteration 1159, loss = 0.25703008\n",
      "Iteration 1815, loss = 0.16484858\n",
      "Iteration 1160, loss = 0.25680571\n",
      "Iteration 1816, loss = 0.16476613\n",
      "Iteration 1161, loss = 0.25671562\n",
      "Iteration 1817, loss = 0.16458225\n",
      "Iteration 1162, loss = 0.25651257\n",
      "Iteration 1818, loss = 0.16454466\n",
      "Iteration 1163, loss = 0.25635379\n",
      "Iteration 1819, loss = 0.16452169\n",
      "Iteration 1164, loss = 0.25608494\n",
      "Iteration 1820, loss = 0.16434628\n",
      "Iteration 1165, loss = 0.25596027\n",
      "Iteration 1821, loss = 0.16427168\n",
      "Iteration 1166, loss = 0.25583191\n",
      "Iteration 1822, loss = 0.16420402\n",
      "Iteration 1167, loss = 0.25568527\n",
      "Iteration 1823, loss = 0.16417043\n",
      "Iteration 1168, loss = 0.25556431\n",
      "Iteration 1824, loss = 0.16403892\n",
      "Iteration 1169, loss = 0.25523365\n",
      "Iteration 1825, loss = 0.16396668\n",
      "Iteration 1170, loss = 0.25516098\n",
      "Iteration 1826, loss = 0.16389433\n",
      "Iteration 1171, loss = 0.25492822\n",
      "Iteration 1827, loss = 0.16395715\n",
      "Iteration 1172, loss = 0.25478656\n",
      "Iteration 1828, loss = 0.16370629\n",
      "Iteration 1173, loss = 0.25457090\n",
      "Iteration 1829, loss = 0.16365285\n",
      "Iteration 1174, loss = 0.25441164\n",
      "Iteration 1830, loss = 0.16369286\n",
      "Iteration 1175, loss = 0.25437899\n",
      "Iteration 1831, loss = 0.16353307\n",
      "Iteration 1176, loss = 0.25424156\n",
      "Iteration 1832, loss = 0.16344344\n",
      "Iteration 1177, loss = 0.25403039\n",
      "Iteration 1833, loss = 0.16332899\n",
      "Iteration 1178, loss = 0.25377567\n",
      "Iteration 1834, loss = 0.16322741\n",
      "Iteration 1179, loss = 0.25362331\n",
      "Iteration 1835, loss = 0.16326949\n",
      "Iteration 1180, loss = 0.25339780\n",
      "Iteration 1836, loss = 0.16335401\n",
      "Iteration 1181, loss = 0.25325839\n",
      "Iteration 1837, loss = 0.16300506\n",
      "Iteration 1182, loss = 0.25310534\n",
      "Iteration 1838, loss = 0.16296395\n",
      "Iteration 1183, loss = 0.25298797\n",
      "Iteration 1839, loss = 0.16286702\n",
      "Iteration 1184, loss = 0.25280181\n",
      "Iteration 1840, loss = 0.16276269\n",
      "Iteration 1185, loss = 0.25253311\n",
      "Iteration 1186, loss = 0.25243164\n",
      "Iteration 1841, loss = 0.16268511\n",
      "Iteration 1187, loss = 0.25232357\n",
      "Iteration 1842, loss = 0.16264628\n",
      "Iteration 1843, loss = 0.16253972\n",
      "Iteration 1188, loss = 0.25210868\n",
      "Iteration 1844, loss = 0.16247908\n",
      "Iteration 1189, loss = 0.25189947\n",
      "Iteration 1845, loss = 0.16242834\n",
      "Iteration 1190, loss = 0.25192228\n",
      "Iteration 1846, loss = 0.16230951\n",
      "Iteration 1191, loss = 0.25159299\n",
      "Iteration 1847, loss = 0.16227817\n",
      "Iteration 1192, loss = 0.25143727\n",
      "Iteration 1848, loss = 0.16217584\n",
      "Iteration 1193, loss = 0.25158934\n",
      "Iteration 1849, loss = 0.16229218\n",
      "Iteration 1194, loss = 0.25110445\n",
      "Iteration 1850, loss = 0.16212218\n",
      "Iteration 1195, loss = 0.25113228\n",
      "Iteration 1851, loss = 0.16189413\n",
      "Iteration 1196, loss = 0.25072267\n",
      "Iteration 1852, loss = 0.16189377\n",
      "Iteration 1197, loss = 0.25053379\n",
      "Iteration 1853, loss = 0.16194031\n",
      "Iteration 1198, loss = 0.25045350\n",
      "Iteration 1854, loss = 0.16167947\n",
      "Iteration 1199, loss = 0.25018362\n",
      "Iteration 1855, loss = 0.16164727\n",
      "Iteration 1200, loss = 0.25008121\n",
      "Iteration 1856, loss = 0.16156735\n",
      "Iteration 1201, loss = 0.24987060\n",
      "Iteration 1857, loss = 0.16144640\n",
      "Iteration 1202, loss = 0.24967812\n",
      "Iteration 1858, loss = 0.16140445\n",
      "Iteration 1203, loss = 0.24972735\n",
      "Iteration 1859, loss = 0.16130738\n",
      "Iteration 1204, loss = 0.24948796\n",
      "Iteration 1860, loss = 0.16127201\n",
      "Iteration 1205, loss = 0.24927878\n",
      "Iteration 1861, loss = 0.16122301\n",
      "Iteration 1206, loss = 0.24908031\n",
      "Iteration 1862, loss = 0.16104555\n",
      "Iteration 1207, loss = 0.24890181\n",
      "Iteration 1863, loss = 0.16100901\n",
      "Iteration 1208, loss = 0.24879336\n",
      "Iteration 1864, loss = 0.16101353\n",
      "Iteration 1209, loss = 0.24853259\n",
      "Iteration 1865, loss = 0.16092070\n",
      "Iteration 1210, loss = 0.24837450\n",
      "Iteration 1866, loss = 0.16075490\n",
      "Iteration 1211, loss = 0.24823605\n",
      "Iteration 1867, loss = 0.16074500\n",
      "Iteration 1212, loss = 0.24811053\n",
      "Iteration 1868, loss = 0.16063807\n",
      "Iteration 1213, loss = 0.24805558\n",
      "Iteration 1869, loss = 0.16053824\n",
      "Iteration 1214, loss = 0.24768452\n",
      "Iteration 1870, loss = 0.16047130\n",
      "Iteration 1215, loss = 0.24755142\n",
      "Iteration 1871, loss = 0.16038074\n",
      "Iteration 1216, loss = 0.24742458\n",
      "Iteration 1872, loss = 0.16033050\n",
      "Iteration 1217, loss = 0.24717469\n",
      "Iteration 1873, loss = 0.16045731\n",
      "Iteration 1218, loss = 0.24701936\n",
      "Iteration 1874, loss = 0.16023172\n",
      "Iteration 1219, loss = 0.24685326\n",
      "Iteration 1875, loss = 0.16017585\n",
      "Iteration 1220, loss = 0.24673861\n",
      "Iteration 1876, loss = 0.16001136\n",
      "Iteration 1221, loss = 0.24653425\n",
      "Iteration 1877, loss = 0.15999135\n",
      "Iteration 1222, loss = 0.24638446\n",
      "Iteration 1878, loss = 0.16024899\n",
      "Iteration 1223, loss = 0.24623539\n",
      "Iteration 1879, loss = 0.15996813\n",
      "Iteration 1224, loss = 0.24605269\n",
      "Iteration 1880, loss = 0.15977752\n",
      "Iteration 1225, loss = 0.24579116\n",
      "Iteration 1881, loss = 0.15972396\n",
      "Iteration 1226, loss = 0.24580056\n",
      "Iteration 1882, loss = 0.15968011\n",
      "Iteration 1227, loss = 0.24556264\n",
      "Iteration 1883, loss = 0.15950997\n",
      "Iteration 1228, loss = 0.24534361\n",
      "Iteration 1884, loss = 0.15961495\n",
      "Iteration 1229, loss = 0.24515203\n",
      "Iteration 1885, loss = 0.15941830\n",
      "Iteration 1230, loss = 0.24498038\n",
      "Iteration 1886, loss = 0.15930333\n",
      "Iteration 1231, loss = 0.24479990\n",
      "Iteration 1887, loss = 0.15925451\n",
      "Iteration 1232, loss = 0.24464408\n",
      "Iteration 1888, loss = 0.15925855\n",
      "Iteration 1233, loss = 0.24451212\n",
      "Iteration 1889, loss = 0.15911092\n",
      "Iteration 1234, loss = 0.24432580\n",
      "Iteration 1890, loss = 0.15909060\n",
      "Iteration 1235, loss = 0.24415936\n",
      "Iteration 1891, loss = 0.15929613\n",
      "Iteration 1236, loss = 0.24405512\n",
      "Iteration 1892, loss = 0.15889965\n",
      "Iteration 1237, loss = 0.24381980\n",
      "Iteration 1893, loss = 0.15896514\n",
      "Iteration 1238, loss = 0.24368727\n",
      "Iteration 1894, loss = 0.15876638\n",
      "Iteration 1239, loss = 0.24357176\n",
      "Iteration 1895, loss = 0.15870064\n",
      "Iteration 1240, loss = 0.24329916\n",
      "Iteration 1896, loss = 0.15863486\n",
      "Iteration 1241, loss = 0.24320923\n",
      "Iteration 1897, loss = 0.15852772\n",
      "Iteration 1242, loss = 0.24303271\n",
      "Iteration 1898, loss = 0.15843602\n",
      "Iteration 1243, loss = 0.24291815\n",
      "Iteration 1899, loss = 0.15849864\n",
      "Iteration 1244, loss = 0.24282860\n",
      "Iteration 1900, loss = 0.15833800\n",
      "Iteration 1245, loss = 0.24247004\n",
      "Iteration 1901, loss = 0.15828922\n",
      "Iteration 1246, loss = 0.24236719\n",
      "Iteration 1902, loss = 0.15815627\n",
      "Iteration 1247, loss = 0.24222147\n",
      "Iteration 1903, loss = 0.15815624\n",
      "Iteration 1248, loss = 0.24200411\n",
      "Iteration 1904, loss = 0.15806157\n",
      "Iteration 1249, loss = 0.24184109\n",
      "Iteration 1905, loss = 0.15799983\n",
      "Iteration 1250, loss = 0.24171676\n",
      "Iteration 1906, loss = 0.15799974\n",
      "Iteration 1251, loss = 0.24149358\n",
      "Iteration 1907, loss = 0.15786275\n",
      "Iteration 1252, loss = 0.24139608\n",
      "Iteration 1908, loss = 0.15779348\n",
      "Iteration 1253, loss = 0.24123082\n",
      "Iteration 1909, loss = 0.15771129\n",
      "Iteration 1254, loss = 0.24098072\n",
      "Iteration 1910, loss = 0.15761506\n",
      "Iteration 1255, loss = 0.24086537\n",
      "Iteration 1911, loss = 0.15757605\n",
      "Iteration 1256, loss = 0.24066001\n",
      "Iteration 1912, loss = 0.15751264\n",
      "Iteration 1257, loss = 0.24047328\n",
      "Iteration 1913, loss = 0.15750709\n",
      "Iteration 1258, loss = 0.24031465\n",
      "Iteration 1914, loss = 0.15735439\n",
      "Iteration 1259, loss = 0.24026802\n",
      "Iteration 1915, loss = 0.15729840\n",
      "Iteration 1260, loss = 0.24009659\n",
      "Iteration 1261, loss = 0.23983595\n",
      "Iteration 1916, loss = 0.15721503\n",
      "Iteration 1262, loss = 0.23966767\n",
      "Iteration 1917, loss = 0.15724185\n",
      "Iteration 1263, loss = 0.23946651\n",
      "Iteration 1918, loss = 0.15708450\n",
      "Iteration 1264, loss = 0.23938416\n",
      "Iteration 1919, loss = 0.15702920\n",
      "Iteration 1265, loss = 0.23914670\n",
      "Iteration 1920, loss = 0.15699229\n",
      "Iteration 1266, loss = 0.23912979\n",
      "Iteration 1921, loss = 0.15706259\n",
      "Iteration 1267, loss = 0.23926972\n",
      "Iteration 1922, loss = 0.15692301\n",
      "Iteration 1268, loss = 0.23870469\n",
      "Iteration 1923, loss = 0.15685155\n",
      "Iteration 1269, loss = 0.23858535\n",
      "Iteration 1924, loss = 0.15679921\n",
      "Iteration 1270, loss = 0.23836351\n",
      "Iteration 1925, loss = 0.15662101\n",
      "Iteration 1271, loss = 0.23816844\n",
      "Iteration 1926, loss = 0.15663716\n",
      "Iteration 1272, loss = 0.23811495\n",
      "Iteration 1927, loss = 0.15653897\n",
      "Iteration 1273, loss = 0.23780450\n",
      "Iteration 1928, loss = 0.15647202\n",
      "Iteration 1274, loss = 0.23770607\n",
      "Iteration 1929, loss = 0.15634972\n",
      "Iteration 1275, loss = 0.23754342\n",
      "Iteration 1930, loss = 0.15638201\n",
      "Iteration 1276, loss = 0.23742012\n",
      "Iteration 1931, loss = 0.15621632\n",
      "Iteration 1277, loss = 0.23734816\n",
      "Iteration 1932, loss = 0.15633957\n",
      "Iteration 1278, loss = 0.23703351\n",
      "Iteration 1933, loss = 0.15611222\n",
      "Iteration 1279, loss = 0.23685225\n",
      "Iteration 1934, loss = 0.15603746\n",
      "Iteration 1280, loss = 0.23675029\n",
      "Iteration 1935, loss = 0.15604704\n",
      "Iteration 1281, loss = 0.23649052\n",
      "Iteration 1936, loss = 0.15599296\n",
      "Iteration 1282, loss = 0.23631652\n",
      "Iteration 1937, loss = 0.15594858\n",
      "Iteration 1283, loss = 0.23621248\n",
      "Iteration 1938, loss = 0.15580882\n",
      "Iteration 1284, loss = 0.23605663\n",
      "Iteration 1939, loss = 0.15572213\n",
      "Iteration 1285, loss = 0.23593139\n",
      "Iteration 1940, loss = 0.15565956\n",
      "Iteration 1286, loss = 0.23570222\n",
      "Iteration 1941, loss = 0.15560950\n",
      "Iteration 1287, loss = 0.23557586\n",
      "Iteration 1942, loss = 0.15558620\n",
      "Iteration 1288, loss = 0.23533090\n",
      "Iteration 1943, loss = 0.15550196\n",
      "Iteration 1289, loss = 0.23520464\n",
      "Iteration 1944, loss = 0.15556004\n",
      "Iteration 1290, loss = 0.23502962\n",
      "Iteration 1945, loss = 0.15542300\n",
      "Iteration 1291, loss = 0.23482133\n",
      "Iteration 1946, loss = 0.15531074\n",
      "Iteration 1292, loss = 0.23470392\n",
      "Iteration 1293, loss = 0.23451775\n",
      "Iteration 1947, loss = 0.15531347\n",
      "Iteration 1294, loss = 0.23456879\n",
      "Iteration 1948, loss = 0.15518840\n",
      "Iteration 1295, loss = 0.23418829\n",
      "Iteration 1949, loss = 0.15513150\n",
      "Iteration 1296, loss = 0.23405221\n",
      "Iteration 1950, loss = 0.15509586\n",
      "Iteration 1297, loss = 0.23393537\n",
      "Iteration 1951, loss = 0.15505281\n",
      "Iteration 1298, loss = 0.23385393\n",
      "Iteration 1952, loss = 0.15488729\n",
      "Iteration 1299, loss = 0.23352857\n",
      "Iteration 1953, loss = 0.15483292\n",
      "Iteration 1300, loss = 0.23346977\n",
      "Iteration 1954, loss = 0.15478384\n",
      "Iteration 1301, loss = 0.23320769\n",
      "Iteration 1955, loss = 0.15477075\n",
      "Iteration 1302, loss = 0.23361689\n",
      "Iteration 1956, loss = 0.15463682\n",
      "Iteration 1303, loss = 0.23291954\n",
      "Iteration 1957, loss = 0.15459878\n",
      "Iteration 1304, loss = 0.23265075\n",
      "Iteration 1958, loss = 0.15457426\n",
      "Iteration 1305, loss = 0.23263619\n",
      "Iteration 1959, loss = 0.15454945\n",
      "Iteration 1306, loss = 0.23250791\n",
      "Iteration 1960, loss = 0.15449278\n",
      "Iteration 1961, loss = 0.15441308\n",
      "Iteration 1307, loss = 0.23222989\n",
      "Iteration 1308, loss = 0.23202890\n",
      "Iteration 1962, loss = 0.15432856\n",
      "Iteration 1309, loss = 0.23185795\n",
      "Iteration 1963, loss = 0.15428949\n",
      "Iteration 1310, loss = 0.23179844\n",
      "Iteration 1964, loss = 0.15417616\n",
      "Iteration 1311, loss = 0.23164433\n",
      "Iteration 1965, loss = 0.15409869\n",
      "Iteration 1312, loss = 0.23138807\n",
      "Iteration 1966, loss = 0.15410288\n",
      "Iteration 1313, loss = 0.23139718\n",
      "Iteration 1967, loss = 0.15396628\n",
      "Iteration 1314, loss = 0.23118467\n",
      "Iteration 1968, loss = 0.15392433\n",
      "Iteration 1315, loss = 0.23094422\n",
      "Iteration 1969, loss = 0.15386429\n",
      "Iteration 1316, loss = 0.23093272\n",
      "Iteration 1970, loss = 0.15390022\n",
      "Iteration 1317, loss = 0.23059567\n",
      "Iteration 1971, loss = 0.15376171\n",
      "Iteration 1318, loss = 0.23044839\n",
      "Iteration 1972, loss = 0.15370510\n",
      "Iteration 1319, loss = 0.23021738\n",
      "Iteration 1973, loss = 0.15363102\n",
      "Iteration 1320, loss = 0.23008157\n",
      "Iteration 1974, loss = 0.15359752\n",
      "Iteration 1321, loss = 0.22993561\n",
      "Iteration 1975, loss = 0.15353913\n",
      "Iteration 1322, loss = 0.22971152\n",
      "Iteration 1976, loss = 0.15347081\n",
      "Iteration 1323, loss = 0.22961681\n",
      "Iteration 1977, loss = 0.15338734\n",
      "Iteration 1324, loss = 0.22938416\n",
      "Iteration 1978, loss = 0.15333560\n",
      "Iteration 1325, loss = 0.22932721\n",
      "Iteration 1979, loss = 0.15332975\n",
      "Iteration 1326, loss = 0.22914551\n",
      "Iteration 1980, loss = 0.15325000\n",
      "Iteration 1327, loss = 0.22890068\n",
      "Iteration 1981, loss = 0.15330946\n",
      "Iteration 1328, loss = 0.22884003\n",
      "Iteration 1329, loss = 0.22872797\n",
      "Iteration 1982, loss = 0.15313909\n",
      "Iteration 1330, loss = 0.22849208\n",
      "Iteration 1983, loss = 0.15305776\n",
      "Iteration 1331, loss = 0.22824839\n",
      "Iteration 1984, loss = 0.15302758\n",
      "Iteration 1332, loss = 0.22810410\n",
      "Iteration 1985, loss = 0.15301480\n",
      "Iteration 1333, loss = 0.22808692\n",
      "Iteration 1986, loss = 0.15306957\n",
      "Iteration 1334, loss = 0.22775951\n",
      "Iteration 1987, loss = 0.15279401\n",
      "Iteration 1335, loss = 0.22768249\n",
      "Iteration 1988, loss = 0.15280052\n",
      "Iteration 1336, loss = 0.22769376\n",
      "Iteration 1989, loss = 0.15269461\n",
      "Iteration 1337, loss = 0.22738770\n",
      "Iteration 1990, loss = 0.15265991\n",
      "Iteration 1991, loss = 0.15256964\n",
      "Iteration 1338, loss = 0.22716345\n",
      "Iteration 1992, loss = 0.15261929\n",
      "Iteration 1339, loss = 0.22714986\n",
      "Iteration 1993, loss = 0.15244969\n",
      "Iteration 1340, loss = 0.22692185\n",
      "Iteration 1994, loss = 0.15241011\n",
      "Iteration 1341, loss = 0.22694780\n",
      "Iteration 1995, loss = 0.15245238\n",
      "Iteration 1342, loss = 0.22655410\n",
      "Iteration 1996, loss = 0.15231851\n",
      "Iteration 1343, loss = 0.22639170\n",
      "Iteration 1997, loss = 0.15226043\n",
      "Iteration 1344, loss = 0.22627993\n",
      "Iteration 1998, loss = 0.15228101\n",
      "Iteration 1345, loss = 0.22607126\n",
      "Iteration 1999, loss = 0.15214636\n",
      "Iteration 1346, loss = 0.22599247\n",
      "Iteration 2000, loss = 0.15205285\n",
      "Iteration 1347, loss = 0.22572505\n",
      "Iteration 2001, loss = 0.15200241\n",
      "Iteration 1348, loss = 0.22555330\n",
      "Iteration 2002, loss = 0.15201153\n",
      "Iteration 1349, loss = 0.22559803\n",
      "Iteration 2003, loss = 0.15191867\n",
      "Iteration 1350, loss = 0.22532178\n",
      "Iteration 2004, loss = 0.15192540\n",
      "Iteration 1351, loss = 0.22506729\n",
      "Iteration 2005, loss = 0.15184466\n",
      "Iteration 1352, loss = 0.22497769\n",
      "Iteration 2006, loss = 0.15178934\n",
      "Iteration 1353, loss = 0.22487102\n",
      "Iteration 2007, loss = 0.15171888\n",
      "Iteration 1354, loss = 0.22468693\n",
      "Iteration 2008, loss = 0.15163699\n",
      "Iteration 1355, loss = 0.22469744\n",
      "Iteration 1356, loss = 0.22431117\n",
      "Iteration 2009, loss = 0.15157368\n",
      "Iteration 2010, loss = 0.15153418\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1357, loss = 0.22417611\n",
      "Iteration 1358, loss = 0.22419074\n",
      "Iteration 1359, loss = 0.22385470\n",
      "Iteration 1360, loss = 0.22361087\n",
      "Iteration 1361, loss = 0.22343468\n",
      "Iteration 1362, loss = 0.22343729\n",
      "Iteration 1363, loss = 0.22328610\n",
      "Iteration 1364, loss = 0.22297398\n",
      "Iteration 1365, loss = 0.22299402\n",
      "Iteration 1366, loss = 0.22277467\n",
      "Iteration 1367, loss = 0.22248291\n",
      "Iteration 1368, loss = 0.22248564\n",
      "Iteration 1369, loss = 0.22219539\n",
      "Iteration 1370, loss = 0.22220120\n",
      "Iteration 1371, loss = 0.22190321\n",
      "Iteration 1372, loss = 0.22169696\n",
      "Iteration 1373, loss = 0.22162056\n",
      "Iteration 1374, loss = 0.22139546\n",
      "Iteration 1375, loss = 0.22128131\n",
      "Iteration 1376, loss = 0.22118066\n",
      "Iteration 1377, loss = 0.22102704\n",
      "Iteration 1378, loss = 0.22074793\n",
      "Iteration 1379, loss = 0.22068462\n",
      "Iteration 1380, loss = 0.22051596\n",
      "Iteration 1381, loss = 0.22024176\n",
      "Iteration 1382, loss = 0.22009007\n",
      "Iteration 1383, loss = 0.21995121\n",
      "Iteration 1384, loss = 0.21979994\n",
      "Iteration 1385, loss = 0.21960308\n",
      "Iteration 1386, loss = 0.21959568\n",
      "Iteration 1387, loss = 0.21939067\n",
      "Iteration 1388, loss = 0.21913761\n",
      "Iteration 1389, loss = 0.21894957\n",
      "Iteration 1390, loss = 0.21880966\n",
      "Iteration 1391, loss = 0.21870546\n",
      "Iteration 1392, loss = 0.21854715\n",
      "Iteration 1393, loss = 0.21841454\n",
      "Iteration 1394, loss = 0.21825621\n",
      "Iteration 1395, loss = 0.21806739\n",
      "Iteration 1396, loss = 0.21786612\n",
      "Iteration 1397, loss = 0.21769289\n",
      "Iteration 1398, loss = 0.21752149\n",
      "Iteration 1399, loss = 0.21743776\n",
      "Iteration 1400, loss = 0.21736724\n",
      "Iteration 1401, loss = 0.21699094\n",
      "Iteration 1402, loss = 0.21682518\n",
      "Iteration 1403, loss = 0.21672094\n",
      "Iteration 1404, loss = 0.21671116\n",
      "Iteration 1405, loss = 0.21647061\n",
      "Iteration 1406, loss = 0.21627953\n",
      "Iteration 1407, loss = 0.21612495\n",
      "Iteration 1408, loss = 0.21597067\n",
      "Iteration 1409, loss = 0.21589729\n",
      "Iteration 1410, loss = 0.21563635\n",
      "Iteration 1411, loss = 0.21563058\n",
      "Iteration 1412, loss = 0.21528724\n",
      "Iteration 1413, loss = 0.21517603\n",
      "Iteration 1414, loss = 0.21492916\n",
      "Iteration 1415, loss = 0.21477974\n",
      "Iteration 1416, loss = 0.21472650\n",
      "Iteration 1417, loss = 0.21444016\n",
      "Iteration 1418, loss = 0.21440889\n",
      "Iteration 1419, loss = 0.21413579\n",
      "Iteration 1420, loss = 0.21394205\n",
      "Iteration 1421, loss = 0.21386853\n",
      "Iteration 1422, loss = 0.21367847\n",
      "Iteration 1423, loss = 0.21356017\n",
      "Iteration 1424, loss = 0.21336316\n",
      "Iteration 1425, loss = 0.21339109\n",
      "Iteration 1426, loss = 0.21301207\n",
      "Iteration 1427, loss = 0.21293790\n",
      "Iteration 1428, loss = 0.21288665\n",
      "Iteration 1429, loss = 0.21257618\n",
      "Iteration 1430, loss = 0.21234786\n",
      "Iteration 1431, loss = 0.21217505\n",
      "Iteration 1432, loss = 0.21202523\n",
      "Iteration 1433, loss = 0.21204931\n",
      "Iteration 1434, loss = 0.21178381\n",
      "Iteration 1435, loss = 0.21163029\n",
      "Iteration 1436, loss = 0.21171751\n",
      "Iteration 1437, loss = 0.21124929\n",
      "Iteration 1438, loss = 0.21113065\n",
      "Iteration 1439, loss = 0.21121710\n",
      "Iteration 1440, loss = 0.21085884\n",
      "Iteration 1441, loss = 0.21066123\n",
      "Iteration 1442, loss = 0.21051153\n",
      "Iteration 1443, loss = 0.21042277\n",
      "Iteration 1444, loss = 0.21021939\n",
      "Iteration 1445, loss = 0.21047221\n",
      "Iteration 1446, loss = 0.20996088\n",
      "Iteration 1447, loss = 0.20973052\n",
      "Iteration 1448, loss = 0.20974882\n",
      "Iteration 1449, loss = 0.20951353\n",
      "Iteration 1450, loss = 0.20917099\n",
      "Iteration 1451, loss = 0.20906090\n",
      "Iteration 1452, loss = 0.20893097\n",
      "Iteration 1453, loss = 0.20879853\n",
      "Iteration 1454, loss = 0.20871887\n",
      "Iteration 1455, loss = 0.20842392\n",
      "Iteration 1456, loss = 0.20832066\n",
      "Iteration 1457, loss = 0.20813072\n",
      "Iteration 1458, loss = 0.20790955\n",
      "Iteration 1459, loss = 0.20784617\n",
      "Iteration 1460, loss = 0.20763024\n",
      "Iteration 1461, loss = 0.20744194\n",
      "Iteration 1462, loss = 0.20726852\n",
      "Iteration 1463, loss = 0.20714810\n",
      "Iteration 1464, loss = 0.20698808\n",
      "Iteration 1465, loss = 0.20714159\n",
      "Iteration 1466, loss = 0.20668039\n",
      "Iteration 1467, loss = 0.20667806\n",
      "Iteration 1468, loss = 0.20646193\n",
      "Iteration 1469, loss = 0.20622158\n",
      "Iteration 1470, loss = 0.20616717\n",
      "Iteration 1471, loss = 0.20595803\n",
      "Iteration 1472, loss = 0.20576769\n",
      "Iteration 1473, loss = 0.20563566\n",
      "Iteration 1474, loss = 0.20548625\n",
      "Iteration 1475, loss = 0.20535044\n",
      "Iteration 1476, loss = 0.20523488\n",
      "Iteration 1477, loss = 0.20500536\n",
      "Iteration 1478, loss = 0.20486294\n",
      "Iteration 1479, loss = 0.20472517\n",
      "Iteration 1480, loss = 0.20469991\n",
      "Iteration 1481, loss = 0.20441902\n",
      "Iteration 1482, loss = 0.20433043\n",
      "Iteration 1483, loss = 0.20459631\n",
      "Iteration 1484, loss = 0.20407632\n",
      "Iteration 1485, loss = 0.20392984\n",
      "Iteration 1486, loss = 0.20373233\n",
      "Iteration 1487, loss = 0.20373958\n",
      "Iteration 1488, loss = 0.20331339\n",
      "Iteration 1489, loss = 0.20331660\n",
      "Iteration 1490, loss = 0.20305620\n",
      "Iteration 1491, loss = 0.20289218\n",
      "Iteration 1492, loss = 0.20271381\n",
      "Iteration 1493, loss = 0.20260180\n",
      "Iteration 1494, loss = 0.20263994\n",
      "Iteration 1495, loss = 0.20227873\n",
      "Iteration 1496, loss = 0.20218185\n",
      "Iteration 1497, loss = 0.20207361\n",
      "Iteration 1498, loss = 0.20177818\n",
      "Iteration 1499, loss = 0.20167407\n",
      "Iteration 1500, loss = 0.20156157\n",
      "Iteration 1501, loss = 0.20134116\n",
      "Iteration 1502, loss = 0.20125676\n",
      "Iteration 1503, loss = 0.20100609\n",
      "Iteration 1504, loss = 0.20098538\n",
      "Iteration 1505, loss = 0.20071490\n",
      "Iteration 1506, loss = 0.20063858\n",
      "Iteration 1507, loss = 0.20040491\n",
      "Iteration 1508, loss = 0.20027277\n",
      "Iteration 1509, loss = 0.20016945\n",
      "Iteration 1510, loss = 0.20004006\n",
      "Iteration 1511, loss = 0.19991150\n",
      "Iteration 1512, loss = 0.19976725\n",
      "Iteration 1513, loss = 0.19951927\n",
      "Iteration 1514, loss = 0.19941881\n",
      "Iteration 1515, loss = 0.19927231\n",
      "Iteration 1516, loss = 0.19918069\n",
      "Iteration 1517, loss = 0.19922951\n",
      "Iteration 1518, loss = 0.19876722\n",
      "Iteration 1519, loss = 0.19871554\n",
      "Iteration 1520, loss = 0.19848213\n",
      "Iteration 1521, loss = 0.19841040\n",
      "Iteration 1522, loss = 0.19826576\n",
      "Iteration 1523, loss = 0.19805406\n",
      "Iteration 1524, loss = 0.19796111\n",
      "Iteration 1525, loss = 0.19775075\n",
      "Iteration 1526, loss = 0.19762228\n",
      "Iteration 1527, loss = 0.19747915\n",
      "Iteration 1528, loss = 0.19726818\n",
      "Iteration 1529, loss = 0.19734857\n",
      "Iteration 1530, loss = 0.19696401\n",
      "Iteration 1531, loss = 0.19679329\n",
      "Iteration 1532, loss = 0.19674327\n",
      "Iteration 1533, loss = 0.19653329\n",
      "Iteration 1534, loss = 0.19636951\n",
      "Iteration 1535, loss = 0.19628114\n",
      "Iteration 1536, loss = 0.19609228\n",
      "Iteration 1537, loss = 0.19592810\n",
      "Iteration 1538, loss = 0.19582017\n",
      "Iteration 1539, loss = 0.19562436\n",
      "Iteration 1540, loss = 0.19553702\n",
      "Iteration 1541, loss = 0.19535436\n",
      "Iteration 1542, loss = 0.19524166\n",
      "Iteration 1543, loss = 0.19510154\n",
      "Iteration 1544, loss = 0.19492653\n",
      "Iteration 1545, loss = 0.19483594\n",
      "Iteration 1546, loss = 0.19463706\n",
      "Iteration 1547, loss = 0.19453605\n",
      "Iteration 1548, loss = 0.19430322\n",
      "Iteration 1549, loss = 0.19422817\n",
      "Iteration 1550, loss = 0.19410920\n",
      "Iteration 1551, loss = 0.19393781\n",
      "Iteration 1552, loss = 0.19379215\n",
      "Iteration 1553, loss = 0.19363441\n",
      "Iteration 1554, loss = 0.19387899\n",
      "Iteration 1555, loss = 0.19329436\n",
      "Iteration 1556, loss = 0.19321099\n",
      "Iteration 1557, loss = 0.19315365\n",
      "Iteration 1558, loss = 0.19302990\n",
      "Iteration 1559, loss = 0.19277613\n",
      "Iteration 1560, loss = 0.19263961\n",
      "Iteration 1561, loss = 0.19251883\n",
      "Iteration 1562, loss = 0.19236109\n",
      "Iteration 1563, loss = 0.19230160\n",
      "Iteration 1564, loss = 0.19207744\n",
      "Iteration 1565, loss = 0.19189471\n",
      "Iteration 1566, loss = 0.19179708\n",
      "Iteration 1567, loss = 0.19158634\n",
      "Iteration 1568, loss = 0.19149853\n",
      "Iteration 1569, loss = 0.19131270\n",
      "Iteration 1570, loss = 0.19117700\n",
      "Iteration 1571, loss = 0.19106809\n",
      "Iteration 1572, loss = 0.19093781\n",
      "Iteration 1573, loss = 0.19081301\n",
      "Iteration 1574, loss = 0.19074507\n",
      "Iteration 1575, loss = 0.19052414\n",
      "Iteration 1576, loss = 0.19042407\n",
      "Iteration 1577, loss = 0.19021084\n",
      "Iteration 1578, loss = 0.19022529\n",
      "Iteration 1579, loss = 0.18995959\n",
      "Iteration 1580, loss = 0.18979884\n",
      "Iteration 1581, loss = 0.18963278\n",
      "Iteration 1582, loss = 0.18947962\n",
      "Iteration 1583, loss = 0.18945941\n",
      "Iteration 1584, loss = 0.18922112\n",
      "Iteration 1585, loss = 0.18916405\n",
      "Iteration 1586, loss = 0.18892840\n",
      "Iteration 1587, loss = 0.18906078\n",
      "Iteration 1588, loss = 0.18876468\n",
      "Iteration 1589, loss = 0.18856411\n",
      "Iteration 1590, loss = 0.18839555\n",
      "Iteration 1591, loss = 0.18823182\n",
      "Iteration 1592, loss = 0.18811321\n",
      "Iteration 1593, loss = 0.18801129\n",
      "Iteration 1594, loss = 0.18800286\n",
      "Iteration 1595, loss = 0.18793839\n",
      "Iteration 1596, loss = 0.18754912\n",
      "Iteration 1597, loss = 0.18749536\n",
      "Iteration 1598, loss = 0.18749665\n",
      "Iteration 1599, loss = 0.18724359\n",
      "Iteration 1600, loss = 0.18704421\n",
      "Iteration 1601, loss = 0.18687388\n",
      "Iteration 1602, loss = 0.18677976\n",
      "Iteration 1603, loss = 0.18663142\n",
      "Iteration 1604, loss = 0.18672134\n",
      "Iteration 1605, loss = 0.18647458\n",
      "Iteration 1606, loss = 0.18627476\n",
      "Iteration 1607, loss = 0.18612612\n",
      "Iteration 1608, loss = 0.18594052\n",
      "Iteration 1609, loss = 0.18585662\n",
      "Iteration 1610, loss = 0.18570130\n",
      "Iteration 1611, loss = 0.18565194\n",
      "Iteration 1612, loss = 0.18551635\n",
      "Iteration 1613, loss = 0.18538652\n",
      "Iteration 1614, loss = 0.18517747\n",
      "Iteration 1615, loss = 0.18504347\n",
      "Iteration 1616, loss = 0.18494432\n",
      "Iteration 1617, loss = 0.18494908\n",
      "Iteration 1618, loss = 0.18477768\n",
      "Iteration 1619, loss = 0.18461414\n",
      "Iteration 1620, loss = 0.18438286\n",
      "Iteration 1621, loss = 0.18432231\n",
      "Iteration 1622, loss = 0.18415072\n",
      "Iteration 1623, loss = 0.18409197\n",
      "Iteration 1624, loss = 0.18389378\n",
      "Iteration 1625, loss = 0.18378125\n",
      "Iteration 1626, loss = 0.18371746\n",
      "Iteration 1627, loss = 0.18346738\n",
      "Iteration 1628, loss = 0.18363442\n",
      "Iteration 1629, loss = 0.18332029\n",
      "Iteration 1630, loss = 0.18311041\n",
      "Iteration 1631, loss = 0.18310521\n",
      "Iteration 1632, loss = 0.18284162\n",
      "Iteration 1633, loss = 0.18282672\n",
      "Iteration 1634, loss = 0.18271883\n",
      "Iteration 1635, loss = 0.18254403\n",
      "Iteration 1636, loss = 0.18241446\n",
      "Iteration 1637, loss = 0.18226671\n",
      "Iteration 1638, loss = 0.18219256\n",
      "Iteration 1639, loss = 0.18204259\n",
      "Iteration 1640, loss = 0.18196671\n",
      "Iteration 1641, loss = 0.18178797\n",
      "Iteration 1642, loss = 0.18162729\n",
      "Iteration 1643, loss = 0.18159218\n",
      "Iteration 1644, loss = 0.18135623\n",
      "Iteration 1645, loss = 0.18143184\n",
      "Iteration 1646, loss = 0.18110633\n",
      "Iteration 1647, loss = 0.18103025\n",
      "Iteration 1648, loss = 0.18118188\n",
      "Iteration 1649, loss = 0.18090437\n",
      "Iteration 1650, loss = 0.18070220\n",
      "Iteration 1651, loss = 0.18066448\n",
      "Iteration 1652, loss = 0.18040256\n",
      "Iteration 1653, loss = 0.18033233\n",
      "Iteration 1654, loss = 0.18022905\n",
      "Iteration 1655, loss = 0.18011555\n",
      "Iteration 1656, loss = 0.18009880\n",
      "Iteration 1657, loss = 0.17980825\n",
      "Iteration 1658, loss = 0.17965802\n",
      "Iteration 1659, loss = 0.17954848\n",
      "Iteration 1660, loss = 0.17962385\n",
      "Iteration 1661, loss = 0.17938418\n",
      "Iteration 1662, loss = 0.17916686\n",
      "Iteration 1663, loss = 0.17935821\n",
      "Iteration 1664, loss = 0.17912220\n",
      "Iteration 1665, loss = 0.17886164\n",
      "Iteration 1666, loss = 0.17869650\n",
      "Iteration 1667, loss = 0.17879628\n",
      "Iteration 1668, loss = 0.17847462\n",
      "Iteration 1669, loss = 0.17843404\n",
      "Iteration 1670, loss = 0.17838810\n",
      "Iteration 1671, loss = 0.17810231\n",
      "Iteration 1672, loss = 0.17800007\n",
      "Iteration 1673, loss = 0.17794721\n",
      "Iteration 1674, loss = 0.17797694\n",
      "Iteration 1675, loss = 0.17764294\n",
      "Iteration 1676, loss = 0.17756591\n",
      "Iteration 1677, loss = 0.17745932\n",
      "Iteration 1678, loss = 0.17749477\n",
      "Iteration 1679, loss = 0.17721239\n",
      "Iteration 1680, loss = 0.17704258\n",
      "Iteration 1681, loss = 0.17698968\n",
      "Iteration 1682, loss = 0.17681589\n",
      "Iteration 1683, loss = 0.17672635\n",
      "Iteration 1684, loss = 0.17663609\n",
      "Iteration 1685, loss = 0.17652271\n",
      "Iteration 1686, loss = 0.17636604\n",
      "Iteration 1687, loss = 0.17633418\n",
      "Iteration 1688, loss = 0.17611936\n",
      "Iteration 1689, loss = 0.17605113\n",
      "Iteration 1690, loss = 0.17597158\n",
      "Iteration 1691, loss = 0.17584936\n",
      "Iteration 1692, loss = 0.17577739\n",
      "Iteration 1693, loss = 0.17562493\n",
      "Iteration 1694, loss = 0.17559449\n",
      "Iteration 1695, loss = 0.17533687\n",
      "Iteration 1696, loss = 0.17539616\n",
      "Iteration 1697, loss = 0.17521332\n",
      "Iteration 1698, loss = 0.17502522\n",
      "Iteration 1699, loss = 0.17487971\n",
      "Iteration 1700, loss = 0.17492044\n",
      "Iteration 1701, loss = 0.17483902\n",
      "Iteration 1702, loss = 0.17457959\n",
      "Iteration 1703, loss = 0.17458829\n",
      "Iteration 1704, loss = 0.17446913\n",
      "Iteration 1705, loss = 0.17420264\n",
      "Iteration 1706, loss = 0.17413513\n",
      "Iteration 1707, loss = 0.17407361\n",
      "Iteration 1708, loss = 0.17407490\n",
      "Iteration 1709, loss = 0.17384765\n",
      "Iteration 1710, loss = 0.17373270\n",
      "Iteration 1711, loss = 0.17365555\n",
      "Iteration 1712, loss = 0.17359341\n",
      "Iteration 1713, loss = 0.17337499\n",
      "Iteration 1714, loss = 0.17330547\n",
      "Iteration 1715, loss = 0.17333402\n",
      "Iteration 1716, loss = 0.17310260\n",
      "Iteration 1717, loss = 0.17296279\n",
      "Iteration 1718, loss = 0.17283197\n",
      "Iteration 1719, loss = 0.17275774\n",
      "Iteration 1720, loss = 0.17263948\n",
      "Iteration 1721, loss = 0.17248039\n",
      "Iteration 1722, loss = 0.17241639\n",
      "Iteration 1723, loss = 0.17247320\n",
      "Iteration 1724, loss = 0.17220081\n",
      "Iteration 1725, loss = 0.17217509\n",
      "Iteration 1726, loss = 0.17199694\n",
      "Iteration 1727, loss = 0.17185346\n",
      "Iteration 1728, loss = 0.17182207\n",
      "Iteration 1729, loss = 0.17172473\n",
      "Iteration 1730, loss = 0.17156255\n",
      "Iteration 1731, loss = 0.17149996\n",
      "Iteration 1732, loss = 0.17138233\n",
      "Iteration 1733, loss = 0.17122660\n",
      "Iteration 1734, loss = 0.17119761\n",
      "Iteration 1735, loss = 0.17107326\n",
      "Iteration 1736, loss = 0.17094973\n",
      "Iteration 1737, loss = 0.17086747\n",
      "Iteration 1738, loss = 0.17079050\n",
      "Iteration 1739, loss = 0.17061993\n",
      "Iteration 1740, loss = 0.17053923\n",
      "Iteration 1741, loss = 0.17042136\n",
      "Iteration 1742, loss = 0.17034186\n",
      "Iteration 1743, loss = 0.17027457\n",
      "Iteration 1744, loss = 0.17016024\n",
      "Iteration 1745, loss = 0.17006275\n",
      "Iteration 1746, loss = 0.16991378\n",
      "Iteration 1747, loss = 0.16988809\n",
      "Iteration 1748, loss = 0.16976767\n",
      "Iteration 1749, loss = 0.16960786\n",
      "Iteration 1750, loss = 0.16953727\n",
      "Iteration 1751, loss = 0.16948739\n",
      "Iteration 1752, loss = 0.16942147\n",
      "Iteration 1753, loss = 0.16922863\n",
      "Iteration 1754, loss = 0.16919009\n",
      "Iteration 1755, loss = 0.16905896\n",
      "Iteration 1756, loss = 0.16907352\n",
      "Iteration 1757, loss = 0.16883281\n",
      "Iteration 1758, loss = 0.16873858\n",
      "Iteration 1759, loss = 0.16876518\n",
      "Iteration 1760, loss = 0.16862983\n",
      "Iteration 1761, loss = 0.16849672\n",
      "Iteration 1762, loss = 0.16845665\n",
      "Iteration 1763, loss = 0.16836854\n",
      "Iteration 1764, loss = 0.16820507\n",
      "Iteration 1765, loss = 0.16816850\n",
      "Iteration 1766, loss = 0.16797755\n",
      "Iteration 1767, loss = 0.16793044\n",
      "Iteration 1768, loss = 0.16779401\n",
      "Iteration 1769, loss = 0.16782484\n",
      "Iteration 1770, loss = 0.16770129\n",
      "Iteration 1771, loss = 0.16771914\n",
      "Iteration 1772, loss = 0.16751589\n",
      "Iteration 1773, loss = 0.16740658\n",
      "Iteration 1774, loss = 0.16726412\n",
      "Iteration 1775, loss = 0.16717293\n",
      "Iteration 1776, loss = 0.16705909\n",
      "Iteration 1777, loss = 0.16694734\n",
      "Iteration 1778, loss = 0.16687678\n",
      "Iteration 1779, loss = 0.16678118\n",
      "Iteration 1780, loss = 0.16690470\n",
      "Iteration 1781, loss = 0.16653566\n",
      "Iteration 1782, loss = 0.16653820\n",
      "Iteration 1783, loss = 0.16643435\n",
      "Iteration 1784, loss = 0.16641490\n",
      "Iteration 1785, loss = 0.16620163\n",
      "Iteration 1786, loss = 0.16621910\n",
      "Iteration 1787, loss = 0.16610411\n",
      "Iteration 1788, loss = 0.16604648\n",
      "Iteration 1789, loss = 0.16584832\n",
      "Iteration 1790, loss = 0.16580230\n",
      "Iteration 1791, loss = 0.16566248\n",
      "Iteration 1792, loss = 0.16564988\n",
      "Iteration 1793, loss = 0.16547420\n",
      "Iteration 1794, loss = 0.16560617\n",
      "Iteration 1795, loss = 0.16539544\n",
      "Iteration 1796, loss = 0.16529959\n",
      "Iteration 1797, loss = 0.16513033\n",
      "Iteration 1798, loss = 0.16509994\n",
      "Iteration 1799, loss = 0.16494543\n",
      "Iteration 1800, loss = 0.16493116\n",
      "Iteration 1801, loss = 0.16496293\n",
      "Iteration 1802, loss = 0.16475826\n",
      "Iteration 1803, loss = 0.16474870\n",
      "Iteration 1804, loss = 0.16458509\n",
      "Iteration 1805, loss = 0.16454227\n",
      "Iteration 1806, loss = 0.16433907\n",
      "Iteration 1807, loss = 0.16425083\n",
      "Iteration 1808, loss = 0.16427529\n",
      "Iteration 1809, loss = 0.16417302\n",
      "Iteration 1810, loss = 0.16401949\n",
      "Iteration 1811, loss = 0.16391835\n",
      "Iteration 1812, loss = 0.16386067\n",
      "Iteration 1813, loss = 0.16370037\n",
      "Iteration 1814, loss = 0.16363975\n",
      "Iteration 1815, loss = 0.16358777\n",
      "Iteration 1816, loss = 0.16347902\n",
      "Iteration 1817, loss = 0.16345760\n",
      "Iteration 1818, loss = 0.16337794\n",
      "Iteration 1819, loss = 0.16336383\n",
      "Iteration 1820, loss = 0.16315252\n",
      "Iteration 1821, loss = 0.16308316\n",
      "Iteration 1822, loss = 0.16297954\n",
      "Iteration 1823, loss = 0.16290523\n",
      "Iteration 1824, loss = 0.16280773\n",
      "Iteration 1825, loss = 0.16285313\n",
      "Iteration 1826, loss = 0.16274118\n",
      "Iteration 1827, loss = 0.16256320\n",
      "Iteration 1828, loss = 0.16248988\n",
      "Iteration 1829, loss = 0.16250064\n",
      "Iteration 1830, loss = 0.16233682\n",
      "Iteration 1831, loss = 0.16230440\n",
      "Iteration 1832, loss = 0.16215955\n",
      "Iteration 1833, loss = 0.16203581\n",
      "Iteration 1834, loss = 0.16203514\n",
      "Iteration 1835, loss = 0.16190395\n",
      "Iteration 1836, loss = 0.16191565\n",
      "Iteration 1837, loss = 0.16183346\n",
      "Iteration 1838, loss = 0.16169897\n",
      "Iteration 1839, loss = 0.16161855\n",
      "Iteration 1840, loss = 0.16151133\n",
      "Iteration 1841, loss = 0.16146110\n",
      "Iteration 1842, loss = 0.16140163\n",
      "Iteration 1843, loss = 0.16135704\n",
      "Iteration 1844, loss = 0.16130323\n",
      "Iteration 1845, loss = 0.16128400\n",
      "Iteration 1846, loss = 0.16110946\n",
      "Iteration 1847, loss = 0.16094923\n",
      "Iteration 1848, loss = 0.16097312\n",
      "Iteration 1849, loss = 0.16090362\n",
      "Iteration 1850, loss = 0.16078456\n",
      "Iteration 1851, loss = 0.16069100\n",
      "Iteration 1852, loss = 0.16063233\n",
      "Iteration 1853, loss = 0.16051273\n",
      "Iteration 1854, loss = 0.16044344\n",
      "Iteration 1855, loss = 0.16035246\n",
      "Iteration 1856, loss = 0.16026284\n",
      "Iteration 1857, loss = 0.16027971\n",
      "Iteration 1858, loss = 0.16010894\n",
      "Iteration 1859, loss = 0.16004410\n",
      "Iteration 1860, loss = 0.16013959\n",
      "Iteration 1861, loss = 0.15986758\n",
      "Iteration 1862, loss = 0.15991069\n",
      "Iteration 1863, loss = 0.15976213\n",
      "Iteration 1864, loss = 0.15979398\n",
      "Iteration 1865, loss = 0.15954240\n",
      "Iteration 1866, loss = 0.15952686\n",
      "Iteration 1867, loss = 0.15950770\n",
      "Iteration 1868, loss = 0.15936652\n",
      "Iteration 1869, loss = 0.15933520\n",
      "Iteration 1870, loss = 0.15920853\n",
      "Iteration 1871, loss = 0.15915100\n",
      "Iteration 1872, loss = 0.15914176\n",
      "Iteration 1873, loss = 0.15898798\n",
      "Iteration 1874, loss = 0.15888776\n",
      "Iteration 1875, loss = 0.15896979\n",
      "Iteration 1876, loss = 0.15875131\n",
      "Iteration 1877, loss = 0.15868635\n",
      "Iteration 1878, loss = 0.15867183\n",
      "Iteration 1879, loss = 0.15875413\n",
      "Iteration 1880, loss = 0.15849862\n",
      "Iteration 1881, loss = 0.15846780\n",
      "Iteration 1882, loss = 0.15839543\n",
      "Iteration 1883, loss = 0.15828129\n",
      "Iteration 1884, loss = 0.15816420\n",
      "Iteration 1885, loss = 0.15813473\n",
      "Iteration 1886, loss = 0.15805556\n",
      "Iteration 1887, loss = 0.15794598\n",
      "Iteration 1888, loss = 0.15794886\n",
      "Iteration 1889, loss = 0.15794621\n",
      "Iteration 1890, loss = 0.15779904\n",
      "Iteration 1891, loss = 0.15777816\n",
      "Iteration 1892, loss = 0.15760595\n",
      "Iteration 1893, loss = 0.15755915\n",
      "Iteration 1894, loss = 0.15760977\n",
      "Iteration 1895, loss = 0.15746930\n",
      "Iteration 1896, loss = 0.15734935\n",
      "Iteration 1897, loss = 0.15726072\n",
      "Iteration 1898, loss = 0.15736848\n",
      "Iteration 1899, loss = 0.15714368\n",
      "Iteration 1900, loss = 0.15704872\n",
      "Iteration 1901, loss = 0.15701741\n",
      "Iteration 1902, loss = 0.15693605\n",
      "Iteration 1903, loss = 0.15685528\n",
      "Iteration 1904, loss = 0.15678527\n",
      "Iteration 1905, loss = 0.15678679\n",
      "Iteration 1906, loss = 0.15694157\n",
      "Iteration 1907, loss = 0.15656386\n",
      "Iteration 1908, loss = 0.15650289\n",
      "Iteration 1909, loss = 0.15644639\n",
      "Iteration 1910, loss = 0.15662934\n",
      "Iteration 1911, loss = 0.15636255\n",
      "Iteration 1912, loss = 0.15623313\n",
      "Iteration 1913, loss = 0.15619724\n",
      "Iteration 1914, loss = 0.15610034\n",
      "Iteration 1915, loss = 0.15611320\n",
      "Iteration 1916, loss = 0.15606178\n",
      "Iteration 1917, loss = 0.15597866\n",
      "Iteration 1918, loss = 0.15592019\n",
      "Iteration 1919, loss = 0.15578510\n",
      "Iteration 1920, loss = 0.15578329\n",
      "Iteration 1921, loss = 0.15569435\n",
      "Iteration 1922, loss = 0.15575856\n",
      "Iteration 1923, loss = 0.15552585\n",
      "Iteration 1924, loss = 0.15547861\n",
      "Iteration 1925, loss = 0.15554985\n",
      "Iteration 1926, loss = 0.15534563\n",
      "Iteration 1927, loss = 0.15529334\n",
      "Iteration 1928, loss = 0.15524522\n",
      "Iteration 1929, loss = 0.15519661\n",
      "Iteration 1930, loss = 0.15512242\n",
      "Iteration 1931, loss = 0.15528781\n",
      "Iteration 1932, loss = 0.15514759\n",
      "Iteration 1933, loss = 0.15494054\n",
      "Iteration 1934, loss = 0.15490571\n",
      "Iteration 1935, loss = 0.15480413\n",
      "Iteration 1936, loss = 0.15473045\n",
      "Iteration 1937, loss = 0.15464328\n",
      "Iteration 1938, loss = 0.15464210\n",
      "Iteration 1939, loss = 0.15461615\n",
      "Iteration 1940, loss = 0.15445940\n",
      "Iteration 1941, loss = 0.15448546\n",
      "Iteration 1942, loss = 0.15438550\n",
      "Iteration 1943, loss = 0.15427202\n",
      "Iteration 1944, loss = 0.15423751\n",
      "Iteration 1945, loss = 0.15442395\n",
      "Iteration 1946, loss = 0.15413763\n",
      "Iteration 1947, loss = 0.15408029\n",
      "Iteration 1948, loss = 0.15397957\n",
      "Iteration 1949, loss = 0.15396489\n",
      "Iteration 1950, loss = 0.15386007\n",
      "Iteration 1951, loss = 0.15378994\n",
      "Iteration 1952, loss = 0.15373085\n",
      "Iteration 1953, loss = 0.15366979\n",
      "Iteration 1954, loss = 0.15360996\n",
      "Iteration 1955, loss = 0.15362827\n",
      "Iteration 1956, loss = 0.15351052\n",
      "Iteration 1957, loss = 0.15346275\n",
      "Iteration 1958, loss = 0.15340166\n",
      "Iteration 1959, loss = 0.15334400\n",
      "Iteration 1960, loss = 0.15330777\n",
      "Iteration 1961, loss = 0.15323424\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the parameter range for hidden_layer_sizes\n",
    "hidden_layer_sizes_range = [\n",
    "    (50,), (100,), (150,), \n",
    "    (50, 50), (100, 50), (100, 100), (150, 100), \n",
    "    (50, 50, 50), (100, 100, 100), (150, 150, 150), \n",
    "    (50, 50, 50, 50), (100, 100, 100, 100), (150, 150, 150, 150)\n",
    "]\n",
    "hidden_layer_sizes_labels = [str(hls) for hls in hidden_layer_sizes_range]\n",
    "\n",
    "# Create a scorer for F1 Score\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# Validation curve for hidden_layer_sizes\n",
    "train_scores, valid_scores = validation_curve(\n",
    "    MLPClassifier(solver='sgd', max_iter=4000, learning_rate='constant', alpha=0.1, activation='relu', random_state=42, verbose=True),\n",
    "    X_train, y_train,\n",
    "    param_name='hidden_layer_sizes',\n",
    "    param_range=hidden_layer_sizes_range,\n",
    "    cv=5,\n",
    "    scoring=f1_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate mean and standard deviation for training and validation scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "valid_mean = np.mean(valid_scores, axis=1)\n",
    "valid_std = np.std(valid_scores, axis=1)\n",
    "\n",
    "# Plot the validation curve\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(range(len(hidden_layer_sizes_range)), train_mean, label='Training score', color='blue')\n",
    "plt.plot(range(len(hidden_layer_sizes_range)), valid_mean, label='Cross-validation score', color='green')\n",
    "plt.fill_between(range(len(hidden_layer_sizes_range)), train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n",
    "plt.fill_between(range(len(hidden_layer_sizes_range)), valid_mean - valid_std, valid_mean + valid_std, alpha=0.2, color='green')\n",
    "plt.xticks(range(len(hidden_layer_sizes_labels)), hidden_layer_sizes_labels, rotation=45, ha='right')\n",
    "plt.title('Validation Curve for MLPClassifier (hidden_layer_sizes)')\n",
    "plt.xlabel('Hidden Layer Sizes')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c38ec5ad-2af4-4e6b-bfa6-6b51ec5c8686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70212145\n",
      "Iteration 2, loss = 0.70168309\n",
      "Iteration 1, loss = 0.80326313\n",
      "Iteration 1, loss = 0.70226065\n",
      "Iteration 2, loss = 0.79412193\n",
      "Iteration 1, loss = 0.70135888\n",
      "Iteration 3, loss = 0.70125375\n",
      "Iteration 2, loss = 0.70076614\n",
      "Iteration 3, loss = 0.77940272\n",
      "Iteration 4, loss = 0.76328239\n",
      "Iteration 4, loss = 0.70068299\n",
      "Iteration 2, loss = 0.70101440\n",
      "Iteration 3, loss = 0.69829746\n",
      "Iteration 1, loss = 0.80218168\n",
      "Iteration 5, loss = 0.74529459\n",
      "Iteration 4, loss = 0.69552805\n",
      "Iteration 5, loss = 0.70016098\n",
      "Iteration 6, loss = 0.72645972\n",
      "Iteration 1, loss = 0.70334932Iteration 2, loss = 0.79244744\n",
      "\n",
      "Iteration 3, loss = 0.70058977\n",
      "Iteration 7, loss = 0.70893151\n",
      "Iteration 6, loss = 0.69971853\n",
      "Iteration 5, loss = 0.69229822\n",
      "Iteration 2, loss = 0.70177640\n",
      "Iteration 8, loss = 0.69103236\n",
      "Iteration 6, loss = 0.68871664\n",
      "Iteration 3, loss = 0.77707858\n",
      "Iteration 7, loss = 0.68515194\n",
      "Iteration 3, loss = 0.69918269\n",
      "Iteration 7, loss = 0.69913563\n",
      "Iteration 4, loss = 0.75980770\n",
      "Iteration 9, loss = 0.67382765\n",
      "Iteration 4, loss = 0.70007647\n",
      "Iteration 8, loss = 0.68130667\n",
      "Iteration 4, loss = 0.69617384\n",
      "Iteration 10, loss = 0.65836616\n",
      "Iteration 8, loss = 0.69865990\n",
      "Iteration 9, loss = 0.67741708\n",
      "Iteration 5, loss = 0.74117384\n",
      "Iteration 11, loss = 0.64353893\n",
      "Iteration 5, loss = 0.69279074\n",
      "Iteration 6, loss = 0.72144259\n",
      "Iteration 9, loss = 0.69843940\n",
      "Iteration 5, loss = 0.69952239\n",
      "Iteration 10, loss = 0.67360048\n",
      "Iteration 12, loss = 0.63048938\n",
      "Iteration 6, loss = 0.68899440\n",
      "Iteration 11, loss = 0.66970962\n",
      "Iteration 10, loss = 0.69804434\n",
      "Iteration 7, loss = 0.70274669\n",
      "Iteration 12, loss = 0.66594582\n",
      "Iteration 13, loss = 0.61766566\n",
      "Iteration 8, loss = 0.68397889\n",
      "Iteration 6, loss = 0.69905752\n",
      "Iteration 7, loss = 0.68515050\n",
      "Iteration 9, loss = 0.66648548\n",
      "Iteration 11, loss = 0.69793701\n",
      "Iteration 8, loss = 0.68104711\n",
      "Iteration 13, loss = 0.66210621\n",
      "Iteration 10, loss = 0.65035697\n",
      "Iteration 1, loss = 0.70132079\n",
      "Iteration 14, loss = 0.60618584\n",
      "Iteration 9, loss = 0.67696323\n",
      "Iteration 11, loss = 0.63514047\n",
      "Iteration 14, loss = 0.65836699\n",
      "Iteration 15, loss = 0.59531726\n",
      "Iteration 12, loss = 0.69757415\n",
      "Iteration 2, loss = 0.70100520\n",
      "Iteration 10, loss = 0.67288354\n",
      "Iteration 7, loss = 0.69861434\n",
      "Iteration 15, loss = 0.65460670\n",
      "Iteration 3, loss = 0.70058105\n",
      "Iteration 12, loss = 0.62156096\n",
      "Iteration 16, loss = 0.65089594\n",
      "Iteration 16, loss = 0.58524905\n",
      "Iteration 4, loss = 0.70011799\n",
      "Iteration 13, loss = 0.60850692\n",
      "Iteration 17, loss = 0.64731806\n",
      "Iteration 11, loss = 0.66880269\n",
      "Iteration 5, loss = 0.69948284\n",
      "Iteration 13, loss = 0.69748601\n",
      "Iteration 18, loss = 0.64369028\n",
      "Iteration 17, loss = 0.57622395\n",
      "Iteration 14, loss = 0.59676182\n",
      "Iteration 8, loss = 0.69819041Iteration 12, loss = 0.66478294\n",
      "\n",
      "Iteration 18, loss = 0.56747296\n",
      "Iteration 19, loss = 0.64017152\n",
      "Iteration 15, loss = 0.58596408\n",
      "Iteration 14, loss = 0.69734657\n",
      "Iteration 6, loss = 0.69897303\n",
      "Iteration 19, loss = 0.55970632\n",
      "Iteration 13, loss = 0.66070415\n",
      "Iteration 16, loss = 0.57577505\n",
      "Iteration 9, loss = 0.69794448\n",
      "Iteration 20, loss = 0.63657422\n",
      "Iteration 14, loss = 0.65673899\n",
      "Iteration 20, loss = 0.55203825\n",
      "Iteration 15, loss = 0.69726326\n",
      "Iteration 17, loss = 0.56665364\n",
      "Iteration 7, loss = 0.69856912\n",
      "Iteration 15, loss = 0.65284646\n",
      "Iteration 21, loss = 0.54486011\n",
      "Iteration 21, loss = 0.63293993\n",
      "Iteration 10, loss = 0.69758187\n",
      "Iteration 22, loss = 0.53879728\n",
      "Iteration 18, loss = 0.55779919\n",
      "Iteration 16, loss = 0.64891265\n",
      "Iteration 16, loss = 0.69719682\n",
      "Iteration 22, loss = 0.62972489\n",
      "Iteration 1, loss = 0.79779362\n",
      "Iteration 23, loss = 0.53233007\n",
      "Iteration 8, loss = 0.69814655\n",
      "Iteration 23, loss = 0.62617560\n",
      "Iteration 17, loss = 0.64512735\n",
      "Iteration 24, loss = 0.52675355\n",
      "Iteration 11, loss = 0.69753865\n",
      "Iteration 2, loss = 0.78794049\n",
      "Iteration 25, loss = 0.52137147\n",
      "Iteration 9, loss = 0.69793343\n",
      "Iteration 24, loss = 0.62280858\n",
      "Iteration 12, loss = 0.69714387\n",
      "Iteration 18, loss = 0.64127097\n",
      "Iteration 17, loss = 0.69711019\n",
      "Iteration 3, loss = 0.77289655\n",
      "Iteration 26, loss = 0.51622743\n",
      "Iteration 13, loss = 0.69705473\n",
      "Iteration 19, loss = 0.63753730\n",
      "Iteration 19, loss = 0.54988651\n",
      "Iteration 25, loss = 0.61950796\n",
      "Iteration 27, loss = 0.51137576\n",
      "Iteration 18, loss = 0.69716521\n",
      "Iteration 20, loss = 0.63372626\n",
      "Iteration 28, loss = 0.50687816\n",
      "Iteration 14, loss = 0.69694138\n",
      "Iteration 20, loss = 0.54227994\n",
      "Iteration 10, loss = 0.69757008\n",
      "Iteration 26, loss = 0.61613277\n",
      "Iteration 4, loss = 0.75532166\n",
      "Iteration 21, loss = 0.62994121\n",
      "Iteration 21, loss = 0.53507200\n",
      "Iteration 19, loss = 0.69703303\n",
      "Iteration 11, loss = 0.69739937\n",
      "Iteration 22, loss = 0.62651237\n",
      "Iteration 20, loss = 0.69702660\n",
      "Iteration 29, loss = 0.50269836\n",
      "Iteration 12, loss = 0.69710912\n",
      "Iteration 15, loss = 0.69686382\n",
      "Iteration 22, loss = 0.52892026\n",
      "Iteration 30, loss = 0.49859942\n",
      "Iteration 21, loss = 0.69698239\n",
      "Iteration 27, loss = 0.61281216\n",
      "Iteration 23, loss = 0.62288788\n",
      "Iteration 16, loss = 0.69676408\n",
      "Iteration 22, loss = 0.69697424\n",
      "Iteration 31, loss = 0.49459015\n",
      "Iteration 5, loss = 0.73692246\n",
      "Iteration 28, loss = 0.60952492\n",
      "Iteration 23, loss = 0.69693899\n",
      "Iteration 32, loss = 0.49110510\n",
      "Iteration 24, loss = 0.61929911\n",
      "Iteration 6, loss = 0.71742477\n",
      "Iteration 23, loss = 0.52272577\n",
      "Iteration 33, loss = 0.48772958\n",
      "Iteration 25, loss = 0.61580611\n",
      "Iteration 24, loss = 0.51706481\n",
      "Iteration 34, loss = 0.48434833\n",
      "Iteration 7, loss = 0.69817193\n",
      "Iteration 29, loss = 0.60631930\n",
      "Iteration 17, loss = 0.69670757\n",
      "Iteration 24, loss = 0.69692616\n",
      "Iteration 26, loss = 0.61222647\n",
      "Iteration 13, loss = 0.69700336\n",
      "Iteration 35, loss = 0.48122456\n",
      "Iteration 27, loss = 0.60875457\n",
      "Iteration 30, loss = 0.60302850\n",
      "Iteration 14, loss = 0.69694042\n",
      "Iteration 8, loss = 0.67947980\n",
      "Iteration 28, loss = 0.60529370\n",
      "Iteration 18, loss = 0.69668444\n",
      "Iteration 9, loss = 0.66187783\n",
      "Iteration 31, loss = 0.59977154\n",
      "Iteration 25, loss = 0.69690900\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 25, loss = 0.51166115\n",
      "Iteration 32, loss = 0.59665457\n",
      "Iteration 36, loss = 0.47821116\n",
      "Iteration 10, loss = 0.64566881\n",
      "Iteration 26, loss = 0.50654612\n",
      "Iteration 15, loss = 0.69680504\n",
      "Iteration 19, loss = 0.69662582\n",
      "Iteration 37, loss = 0.47532588\n",
      "Iteration 11, loss = 0.63047454\n",
      "Iteration 29, loss = 0.60190877\n",
      "Iteration 27, loss = 0.50170473\n",
      "Iteration 16, loss = 0.69673675\n",
      "Iteration 12, loss = 0.61656328\n",
      "Iteration 28, loss = 0.49724439\n",
      "Iteration 38, loss = 0.47262750\n",
      "Iteration 30, loss = 0.59847406\n",
      "Iteration 29, loss = 0.49309416\n",
      "Iteration 20, loss = 0.69662173\n",
      "Iteration 13, loss = 0.60332427\n",
      "Iteration 39, loss = 0.47006185\n",
      "Iteration 17, loss = 0.69668325\n",
      "Iteration 33, loss = 0.59357304\n",
      "Iteration 31, loss = 0.59505365\n",
      "Iteration 21, loss = 0.69658927\n",
      "Iteration 14, loss = 0.59123508\n",
      "Iteration 40, loss = 0.46750216\n",
      "Iteration 30, loss = 0.48899881\n",
      "Iteration 34, loss = 0.59038240\n",
      "Iteration 18, loss = 0.69666677\n",
      "Iteration 32, loss = 0.59177706\n",
      "Iteration 22, loss = 0.69655954\n",
      "Iteration 31, loss = 0.48505754\n",
      "Iteration 41, loss = 0.46518858\n",
      "Iteration 15, loss = 0.58044966\n",
      "Iteration 23, loss = 0.69653470\n",
      "Iteration 42, loss = 0.46296358\n",
      "Iteration 16, loss = 0.57007728\n",
      "Iteration 35, loss = 0.58727969\n",
      "Iteration 24, loss = 0.69651348\n",
      "Iteration 43, loss = 0.46065642\n",
      "Iteration 17, loss = 0.56064414\n",
      "Iteration 19, loss = 0.69662484\n",
      "Iteration 33, loss = 0.58852272\n",
      "Iteration 32, loss = 0.48154875\n",
      "Iteration 25, loss = 0.69653324\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 18, loss = 0.55154130\n",
      "Iteration 33, loss = 0.47816176\n",
      "Iteration 20, loss = 0.69656631\n",
      "Iteration 36, loss = 0.58422936\n",
      "Iteration 34, loss = 0.58516192\n",
      "Iteration 19, loss = 0.54329824\n",
      "Iteration 34, loss = 0.47481425\n",
      "Iteration 21, loss = 0.69653849\n",
      "Iteration 20, loss = 0.53557667\n",
      "Iteration 37, loss = 0.58112976\n",
      "Iteration 35, loss = 0.47164916\n",
      "Iteration 22, loss = 0.69652442\n",
      "Iteration 35, loss = 0.58188788\n",
      "Iteration 21, loss = 0.52831675\n",
      "Iteration 38, loss = 0.57811451\n",
      "Iteration 36, loss = 0.46865625\n",
      "Iteration 23, loss = 0.69650300\n",
      "Iteration 1, loss = 0.70176382\n",
      "Iteration 22, loss = 0.52179732\n",
      "Iteration 44, loss = 0.45852834\n",
      "Iteration 37, loss = 0.46580397\n",
      "Iteration 24, loss = 0.69649471\n",
      "Iteration 36, loss = 0.57867759\n",
      "Iteration 39, loss = 0.57511936\n",
      "Iteration 45, loss = 0.45649746\n",
      "Iteration 1, loss = 0.70141021\n",
      "Iteration 37, loss = 0.57541869\n",
      "Iteration 2, loss = 0.70014913\n",
      "Iteration 25, loss = 0.69652693\n",
      "Iteration 2, loss = 0.70107469\n",
      "Iteration 23, loss = 0.51544506\n",
      "Iteration 3, loss = 0.69759393Iteration 40, loss = 0.57206637\n",
      "\n",
      "Iteration 38, loss = 0.46309876\n",
      "Iteration 38, loss = 0.57229926\n",
      "Iteration 46, loss = 0.45454264\n",
      "Iteration 24, loss = 0.50958323\n",
      "Iteration 39, loss = 0.46049278\n",
      "Iteration 26, loss = 0.69644177\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 0.70061078\n",
      "Iteration 25, loss = 0.50402431\n",
      "Iteration 41, loss = 0.56911736\n",
      "Iteration 39, loss = 0.56915262\n",
      "Iteration 47, loss = 0.45277782\n",
      "Iteration 4, loss = 0.69447812\n",
      "Iteration 4, loss = 0.70029568\n",
      "Iteration 40, loss = 0.45793446\n",
      "Iteration 48, loss = 0.45086992\n",
      "Iteration 42, loss = 0.56618609\n",
      "Iteration 26, loss = 0.49858857\n",
      "Iteration 49, loss = 0.44922871\n",
      "Iteration 5, loss = 0.69106821\n",
      "Iteration 41, loss = 0.45559636\n",
      "Iteration 40, loss = 0.56597304\n",
      "Iteration 5, loss = 0.69955556\n",
      "Iteration 43, loss = 0.56315790\n",
      "Iteration 42, loss = 0.45329202\n",
      "Iteration 50, loss = 0.44751785\n",
      "Iteration 27, loss = 0.49370860\n",
      "Iteration 6, loss = 0.68727267\n",
      "Iteration 41, loss = 0.56293563\n",
      "Iteration 7, loss = 0.68324488\n",
      "Iteration 28, loss = 0.48895946\n",
      "Iteration 44, loss = 0.56024062\n",
      "Iteration 42, loss = 0.55988960\n",
      "Iteration 6, loss = 0.69905753\n",
      "Iteration 45, loss = 0.55735053\n",
      "Iteration 29, loss = 0.48466403\n",
      "Iteration 51, loss = 0.44587677\n",
      "Iteration 43, loss = 0.45104686\n",
      "Iteration 46, loss = 0.55450078\n",
      "Iteration 52, loss = 0.44428400\n",
      "Iteration 44, loss = 0.44892368\n",
      "Iteration 47, loss = 0.55171584\n",
      "Iteration 53, loss = 0.44288045\n",
      "Iteration 8, loss = 0.67909142\n",
      "Iteration 30, loss = 0.48041435\n",
      "Iteration 45, loss = 0.44681358\n",
      "Iteration 1, loss = 0.80376041\n",
      "Iteration 7, loss = 0.69863862\n",
      "Iteration 54, loss = 0.44140295\n",
      "Iteration 43, loss = 0.55679168\n",
      "Iteration 46, loss = 0.44483432\n",
      "Iteration 48, loss = 0.54877873\n",
      "Iteration 55, loss = 0.43993182\n",
      "Iteration 31, loss = 0.47630564\n",
      "Iteration 2, loss = 0.79328789\n",
      "Iteration 47, loss = 0.44308024\n",
      "Iteration 9, loss = 0.67491943\n",
      "Iteration 48, loss = 0.44111060\n",
      "Iteration 44, loss = 0.55373717\n",
      "Iteration 8, loss = 0.69820515\n",
      "Iteration 45, loss = 0.55077189\n",
      "Iteration 10, loss = 0.67076524\n",
      "Iteration 49, loss = 0.54606862\n",
      "Iteration 9, loss = 0.69794369\n",
      "Iteration 11, loss = 0.66658590\n",
      "Iteration 56, loss = 0.43857702\n",
      "Iteration 12, loss = 0.66245578\n",
      "Iteration 49, loss = 0.43941527\n",
      "Iteration 50, loss = 0.43770529\n",
      "Iteration 13, loss = 0.65826485\n",
      "Iteration 57, loss = 0.43728030\n",
      "Iteration 58, loss = 0.43603492\n",
      "Iteration 3, loss = 0.77782963\n",
      "Iteration 32, loss = 0.47263989\n",
      "Iteration 51, loss = 0.43598295\n",
      "Iteration 59, loss = 0.43477053\n",
      "Iteration 33, loss = 0.46898017\n",
      "Iteration 52, loss = 0.43438084\n",
      "Iteration 53, loss = 0.43294651\n",
      "Iteration 60, loss = 0.43356077\n",
      "Iteration 46, loss = 0.54781454\n",
      "Iteration 34, loss = 0.46559817\n",
      "Iteration 14, loss = 0.65417335\n",
      "Iteration 50, loss = 0.54326475\n",
      "Iteration 47, loss = 0.54495213\n",
      "Iteration 15, loss = 0.65022350\n",
      "Iteration 51, loss = 0.54051643\n",
      "Iteration 10, loss = 0.69770703\n",
      "Iteration 4, loss = 0.75871211\n",
      "Iteration 11, loss = 0.69743050\n",
      "Iteration 48, loss = 0.54194424\n",
      "Iteration 54, loss = 0.43138126\n",
      "Iteration 16, loss = 0.64618842\n",
      "Iteration 35, loss = 0.46226295\n",
      "Iteration 52, loss = 0.53779899\n",
      "Iteration 49, loss = 0.53911111\n",
      "Iteration 17, loss = 0.64229432\n",
      "Iteration 12, loss = 0.69721688\n",
      "Iteration 53, loss = 0.53518375\n",
      "Iteration 36, loss = 0.45912334\n",
      "Iteration 61, loss = 0.43241692\n",
      "Iteration 50, loss = 0.53628901\n",
      "Iteration 5, loss = 0.73944231\n",
      "Iteration 55, loss = 0.42990747\n",
      "Iteration 13, loss = 0.69708247\n",
      "Iteration 56, loss = 0.42853826\n",
      "Iteration 51, loss = 0.53336663\n",
      "Iteration 62, loss = 0.43127321\n",
      "Iteration 54, loss = 0.53258121\n",
      "Iteration 57, loss = 0.42718027\n",
      "Iteration 52, loss = 0.53061777\n",
      "Iteration 6, loss = 0.71858159\n",
      "Iteration 63, loss = 0.43022859\n",
      "Iteration 14, loss = 0.69699136\n",
      "Iteration 58, loss = 0.42587475\n",
      "Iteration 7, loss = 0.69828204\n",
      "Iteration 55, loss = 0.52995758\n",
      "Iteration 53, loss = 0.52795527\n",
      "Iteration 64, loss = 0.42918887\n",
      "Iteration 37, loss = 0.45615539\n",
      "Iteration 56, loss = 0.52734207\n",
      "Iteration 65, loss = 0.42810637\n",
      "Iteration 54, loss = 0.52520210\n",
      "Iteration 15, loss = 0.69689333\n",
      "Iteration 66, loss = 0.42708660\n",
      "Iteration 59, loss = 0.42457841\n",
      "Iteration 57, loss = 0.52481482\n",
      "Iteration 55, loss = 0.52254145\n",
      "Iteration 67, loss = 0.42607005\n",
      "Iteration 60, loss = 0.42331336\n",
      "Iteration 58, loss = 0.52234503\n",
      "Iteration 56, loss = 0.51983083\n",
      "Iteration 68, loss = 0.42518788\n",
      "Iteration 61, loss = 0.42214370\n",
      "Iteration 59, loss = 0.51981406\n",
      "Iteration 69, loss = 0.42422516\n",
      "Iteration 57, loss = 0.51725247\n",
      "Iteration 60, loss = 0.51735221\n",
      "Iteration 18, loss = 0.63829860\n",
      "Iteration 62, loss = 0.42093751\n",
      "Iteration 70, loss = 0.42342045\n",
      "Iteration 16, loss = 0.69679178\n",
      "Iteration 61, loss = 0.51499413\n",
      "Iteration 71, loss = 0.42246424\n",
      "Iteration 8, loss = 0.67875332\n",
      "Iteration 38, loss = 0.45333902\n",
      "Iteration 72, loss = 0.42160530\n",
      "Iteration 17, loss = 0.69675406\n",
      "Iteration 73, loss = 0.42076931\n",
      "Iteration 74, loss = 0.41993206\n",
      "Iteration 63, loss = 0.41984660\n",
      "Iteration 75, loss = 0.41916864\n",
      "Iteration 62, loss = 0.51252491\n",
      "Iteration 58, loss = 0.51466720\n",
      "Iteration 18, loss = 0.69670475\n",
      "Iteration 63, loss = 0.51027232\n",
      "Iteration 76, loss = 0.41843158\n",
      "Iteration 19, loss = 0.63444804\n",
      "Iteration 77, loss = 0.41765210\n",
      "Iteration 64, loss = 0.50802536\n",
      "Iteration 19, loss = 0.69672086\n",
      "Iteration 64, loss = 0.41873033\n",
      "Iteration 9, loss = 0.66033963\n",
      "Iteration 65, loss = 0.50567666\n",
      "Iteration 20, loss = 0.69662241\n",
      "Iteration 66, loss = 0.50335929\n",
      "Iteration 39, loss = 0.45051851\n",
      "Iteration 20, loss = 0.63055706\n",
      "Iteration 10, loss = 0.64282414\n",
      "Iteration 78, loss = 0.41689871\n",
      "Iteration 59, loss = 0.51205267\n",
      "Iteration 40, loss = 0.44800307\n",
      "Iteration 21, loss = 0.62673074\n",
      "Iteration 79, loss = 0.41626226\n",
      "Iteration 21, loss = 0.69661502\n",
      "Iteration 41, loss = 0.44537798\n",
      "Iteration 22, loss = 0.62312397\n",
      "Iteration 80, loss = 0.41549758\n",
      "Iteration 42, loss = 0.44304299\n",
      "Iteration 23, loss = 0.61941482\n",
      "Iteration 60, loss = 0.50949416\n",
      "Iteration 81, loss = 0.41482424\n",
      "Iteration 11, loss = 0.62706309\n",
      "Iteration 65, loss = 0.41762573\n",
      "Iteration 61, loss = 0.50707353\n",
      "Iteration 24, loss = 0.61571716\n",
      "Iteration 82, loss = 0.41411560\n",
      "Iteration 43, loss = 0.44066489\n",
      "Iteration 12, loss = 0.61218019\n",
      "Iteration 62, loss = 0.50448779\n",
      "Iteration 13, loss = 0.59854131\n",
      "Iteration 44, loss = 0.43850537\n",
      "Iteration 25, loss = 0.61214028\n",
      "Iteration 66, loss = 0.41657821\n",
      "Iteration 14, loss = 0.58570261\n",
      "Iteration 22, loss = 0.69659424\n",
      "Iteration 63, loss = 0.50217314\n",
      "Iteration 83, loss = 0.41348353\n",
      "Iteration 67, loss = 0.41552642\n",
      "Iteration 45, loss = 0.43624625\n",
      "Iteration 84, loss = 0.41283267\n",
      "Iteration 26, loss = 0.60842858\n",
      "Iteration 67, loss = 0.50111477\n",
      "Iteration 68, loss = 0.41459632\n",
      "Iteration 85, loss = 0.41221694\n",
      "Iteration 15, loss = 0.57442660\n",
      "Iteration 27, loss = 0.60489045\n",
      "Iteration 69, loss = 0.41358003\n",
      "Iteration 86, loss = 0.41156975\n",
      "Iteration 28, loss = 0.60128241\n",
      "Iteration 64, loss = 0.49979501\n",
      "Iteration 46, loss = 0.43418856\n",
      "Iteration 68, loss = 0.49900144\n",
      "Iteration 87, loss = 0.41099866\n",
      "Iteration 29, loss = 0.59779940\n",
      "Iteration 65, loss = 0.49744430\n",
      "Iteration 23, loss = 0.69655693\n",
      "Iteration 88, loss = 0.41040562\n",
      "Iteration 16, loss = 0.56362264\n",
      "Iteration 30, loss = 0.59429931\n",
      "Iteration 31, loss = 0.59075926\n",
      "Iteration 24, loss = 0.69654299\n",
      "Iteration 47, loss = 0.43235588\n",
      "Iteration 69, loss = 0.49678003\n",
      "Iteration 32, loss = 0.58741478\n",
      "Iteration 89, loss = 0.40984249\n",
      "Iteration 48, loss = 0.43032906\n",
      "Iteration 25, loss = 0.69661775\n",
      "Iteration 33, loss = 0.58398617\n",
      "Iteration 49, loss = 0.42850277\n",
      "Iteration 17, loss = 0.55376991\n",
      "Iteration 66, loss = 0.49504544\n",
      "Iteration 26, loss = 0.69651840\n",
      "Iteration 70, loss = 0.49481492\n",
      "Iteration 70, loss = 0.41274470\n",
      "Iteration 34, loss = 0.58059437\n",
      "Iteration 71, loss = 0.49260387\n",
      "Iteration 18, loss = 0.54427354\n",
      "Iteration 90, loss = 0.40925905\n",
      "Iteration 35, loss = 0.57720278\n",
      "Iteration 72, loss = 0.49054437\n",
      "Iteration 71, loss = 0.41168181\n",
      "Iteration 67, loss = 0.49275015\n",
      "Iteration 73, loss = 0.48854553\n",
      "Iteration 36, loss = 0.57388449\n",
      "Iteration 72, loss = 0.41079251\n",
      "Iteration 27, loss = 0.69648319\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 91, loss = 0.40869480\n",
      "Iteration 50, loss = 0.42672401\n",
      "Iteration 73, loss = 0.40993004\n",
      "Iteration 68, loss = 0.49054350\n",
      "Iteration 92, loss = 0.40817240\n",
      "Iteration 51, loss = 0.42492879\n",
      "Iteration 37, loss = 0.57049903\n",
      "Iteration 74, loss = 0.40906084\n",
      "Iteration 93, loss = 0.40766142\n",
      "Iteration 52, loss = 0.42328854\n",
      "Iteration 75, loss = 0.40823246\n",
      "Iteration 69, loss = 0.48827130\n",
      "Iteration 74, loss = 0.48653871\n",
      "Iteration 94, loss = 0.40712640\n",
      "Iteration 53, loss = 0.42176470\n",
      "Iteration 38, loss = 0.56727417\n",
      "Iteration 70, loss = 0.48623156\n",
      "Iteration 95, loss = 0.40661769\n",
      "Iteration 75, loss = 0.48460029\n",
      "Iteration 39, loss = 0.56393602\n",
      "Iteration 76, loss = 0.40745990\n",
      "Iteration 54, loss = 0.42016184\n",
      "Iteration 96, loss = 0.40612174\n",
      "Iteration 76, loss = 0.48274001\n",
      "Iteration 40, loss = 0.56069927\n",
      "Iteration 55, loss = 0.41856097\n",
      "Iteration 19, loss = 0.53555217\n",
      "Iteration 97, loss = 0.40561369\n",
      "Iteration 77, loss = 0.48079501\n",
      "Iteration 41, loss = 0.55743737\n",
      "Iteration 71, loss = 0.48394878\n",
      "Iteration 98, loss = 0.40513564\n",
      "Iteration 20, loss = 0.52754246\n",
      "Iteration 56, loss = 0.41717207\n",
      "Iteration 77, loss = 0.40662799\n",
      "Iteration 99, loss = 0.40467561\n",
      "Iteration 42, loss = 0.55429698\n",
      "Iteration 78, loss = 0.47897752\n",
      "Iteration 72, loss = 0.48183441\n",
      "Iteration 21, loss = 0.52009042\n",
      "Iteration 73, loss = 0.47979257\n",
      "Iteration 57, loss = 0.41575491\n",
      "Iteration 78, loss = 0.40584421\n",
      "Iteration 100, loss = 0.40420938\n",
      "Iteration 22, loss = 0.51318946\n",
      "Iteration 58, loss = 0.41438529\n",
      "Iteration 43, loss = 0.55099866\n",
      "Iteration 79, loss = 0.40513728\n",
      "Iteration 101, loss = 0.40375341\n",
      "Iteration 59, loss = 0.41307553\n",
      "Iteration 23, loss = 0.50676179\n",
      "Iteration 44, loss = 0.54786634\n",
      "Iteration 79, loss = 0.47722266\n",
      "Iteration 74, loss = 0.47775259\n",
      "Iteration 24, loss = 0.50061381\n",
      "Iteration 60, loss = 0.41173056\n",
      "Iteration 45, loss = 0.54469948\n",
      "Iteration 25, loss = 0.49485111\n",
      "Iteration 1, loss = 0.69930646\n",
      "Iteration 80, loss = 0.40433627\n",
      "Iteration 26, loss = 0.48938798\n",
      "Iteration 102, loss = 0.40331537\n",
      "Iteration 75, loss = 0.47573120\n",
      "Iteration 80, loss = 0.47536772\n",
      "Iteration 61, loss = 0.41054648\n",
      "Iteration 103, loss = 0.40287348\n",
      "Iteration 81, loss = 0.40363805\n",
      "Iteration 2, loss = 0.69757414\n",
      "Iteration 62, loss = 0.40932814\n",
      "Iteration 81, loss = 0.47361699\n",
      "Iteration 104, loss = 0.40241355\n",
      "Iteration 27, loss = 0.48427764\n",
      "Iteration 3, loss = 0.69492295\n",
      "Iteration 63, loss = 0.40814430\n",
      "Iteration 82, loss = 0.47190368\n",
      "Iteration 105, loss = 0.40200735\n",
      "Iteration 4, loss = 0.69152695\n",
      "Iteration 64, loss = 0.40701874\n",
      "Iteration 106, loss = 0.40159512\n",
      "Iteration 82, loss = 0.40288492\n",
      "Iteration 46, loss = 0.54160666\n",
      "Iteration 28, loss = 0.47944684\n",
      "Iteration 76, loss = 0.47382796\n",
      "Iteration 83, loss = 0.47021186\n",
      "Iteration 107, loss = 0.40116581\n",
      "Iteration 83, loss = 0.40219694\n",
      "Iteration 47, loss = 0.53858785\n",
      "Iteration 108, loss = 0.40078618\n",
      "Iteration 29, loss = 0.47502574\n",
      "Iteration 84, loss = 0.40152856\n",
      "Iteration 77, loss = 0.47185879\n",
      "Iteration 48, loss = 0.53547663\n",
      "Iteration 30, loss = 0.47058748\n",
      "Iteration 84, loss = 0.46855141\n",
      "Iteration 109, loss = 0.40043494\n",
      "Iteration 78, loss = 0.46996283\n",
      "Iteration 65, loss = 0.40584391\n",
      "Iteration 31, loss = 0.46648111\n",
      "Iteration 85, loss = 0.46697534\n",
      "Iteration 110, loss = 0.40000475\n",
      "Iteration 79, loss = 0.46820790\n",
      "Iteration 49, loss = 0.53248122\n",
      "Iteration 111, loss = 0.39963012\n",
      "Iteration 32, loss = 0.46267344\n",
      "Iteration 50, loss = 0.52957542\n",
      "Iteration 86, loss = 0.46534037\n",
      "Iteration 112, loss = 0.39923579\n",
      "Iteration 66, loss = 0.40479189\n",
      "Iteration 5, loss = 0.68789964\n",
      "Iteration 85, loss = 0.40086872\n",
      "Iteration 51, loss = 0.52649600\n",
      "Iteration 113, loss = 0.39887578\n",
      "Iteration 67, loss = 0.40369857\n",
      "Iteration 6, loss = 0.68378430\n",
      "Iteration 52, loss = 0.52360046\n",
      "Iteration 114, loss = 0.39854854\n",
      "Iteration 68, loss = 0.40269856\n",
      "Iteration 7, loss = 0.67953222\n",
      "Iteration 80, loss = 0.46628813\n",
      "Iteration 115, loss = 0.39814860\n",
      "Iteration 69, loss = 0.40171824\n",
      "Iteration 87, loss = 0.46387148\n",
      "Iteration 81, loss = 0.46451873\n",
      "Iteration 33, loss = 0.45891651\n",
      "Iteration 86, loss = 0.40018459\n",
      "Iteration 70, loss = 0.40079603\n",
      "Iteration 53, loss = 0.52080730\n",
      "Iteration 88, loss = 0.46230087\n",
      "Iteration 8, loss = 0.67515676\n",
      "Iteration 87, loss = 0.39957837\n",
      "Iteration 71, loss = 0.39976600\n",
      "Iteration 89, loss = 0.46081091\n",
      "Iteration 34, loss = 0.45554575\n",
      "Iteration 72, loss = 0.39884026\n",
      "Iteration 88, loss = 0.39892350\n",
      "Iteration 90, loss = 0.45932976\n",
      "Iteration 82, loss = 0.46276085\n",
      "Iteration 35, loss = 0.45214027\n",
      "Iteration 9, loss = 0.67073893\n",
      "Iteration 54, loss = 0.51791896\n",
      "Iteration 89, loss = 0.39835166\n",
      "Iteration 116, loss = 0.39780439\n",
      "Iteration 55, loss = 0.51504521\n",
      "Iteration 90, loss = 0.39774441\n",
      "Iteration 83, loss = 0.46099961\n",
      "Iteration 56, loss = 0.51229990\n",
      "Iteration 117, loss = 0.39749862\n",
      "Iteration 36, loss = 0.44894513\n",
      "Iteration 73, loss = 0.39794831\n",
      "Iteration 10, loss = 0.66621860\n",
      "Iteration 57, loss = 0.50954361\n",
      "Iteration 118, loss = 0.39711097\n",
      "Iteration 91, loss = 0.45784648\n",
      "Iteration 74, loss = 0.39709752\n",
      "Iteration 84, loss = 0.45931080Iteration 119, loss = 0.39679814\n",
      "\n",
      "Iteration 92, loss = 0.45648597\n",
      "Iteration 75, loss = 0.39619892\n",
      "Iteration 37, loss = 0.44595706\n",
      "Iteration 120, loss = 0.39649469\n",
      "Iteration 85, loss = 0.45768657\n",
      "Iteration 11, loss = 0.66183211\n",
      "Iteration 93, loss = 0.45511651\n",
      "Iteration 76, loss = 0.39540671\n",
      "Iteration 91, loss = 0.39712627\n",
      "Iteration 94, loss = 0.45374124\n",
      "Iteration 38, loss = 0.44302830\n",
      "Iteration 121, loss = 0.39613288\n",
      "Iteration 86, loss = 0.45599200\n",
      "Iteration 77, loss = 0.39458829\n",
      "Iteration 95, loss = 0.45238665\n",
      "Iteration 39, loss = 0.44025815\n",
      "Iteration 122, loss = 0.39584111\n",
      "Iteration 78, loss = 0.39378528\n",
      "Iteration 58, loss = 0.50681761\n",
      "Iteration 96, loss = 0.45112063\n",
      "Iteration 40, loss = 0.43777233\n",
      "Iteration 123, loss = 0.39552302\n",
      "Iteration 79, loss = 0.39302041\n",
      "Iteration 92, loss = 0.39656446\n",
      "Iteration 97, loss = 0.44980646\n",
      "Iteration 41, loss = 0.43507536\n",
      "Iteration 124, loss = 0.39522218\n",
      "Iteration 80, loss = 0.39224849\n",
      "Iteration 42, loss = 0.43267427\n",
      "Iteration 87, loss = 0.45446672\n",
      "Iteration 59, loss = 0.50413332\n",
      "Iteration 43, loss = 0.43034241\n",
      "Iteration 12, loss = 0.65739885\n",
      "Iteration 125, loss = 0.39491741\n",
      "Iteration 44, loss = 0.42818584\n",
      "Iteration 88, loss = 0.45284997\n",
      "Iteration 13, loss = 0.65302314\n",
      "Iteration 98, loss = 0.44853453\n",
      "Iteration 45, loss = 0.42592127\n",
      "Iteration 89, loss = 0.45134621\n",
      "Iteration 81, loss = 0.39153797\n",
      "Iteration 14, loss = 0.64866311\n",
      "Iteration 60, loss = 0.50145996\n",
      "Iteration 126, loss = 0.39459064\n",
      "Iteration 82, loss = 0.39075360\n",
      "Iteration 93, loss = 0.39601095\n",
      "Iteration 90, loss = 0.44978357\n",
      "Iteration 94, loss = 0.39546166\n",
      "Iteration 127, loss = 0.39429912\n",
      "Iteration 83, loss = 0.39006583\n",
      "Iteration 99, loss = 0.44733256\n",
      "Iteration 95, loss = 0.39490663\n",
      "Iteration 15, loss = 0.64450018\n",
      "Iteration 84, loss = 0.38939687\n",
      "Iteration 91, loss = 0.44826342\n",
      "Iteration 100, loss = 0.44612552\n",
      "Iteration 61, loss = 0.49892551\n",
      "Iteration 85, loss = 0.38872356\n",
      "Iteration 46, loss = 0.42387662\n",
      "Iteration 101, loss = 0.44494587\n",
      "Iteration 96, loss = 0.39437805\n",
      "Iteration 92, loss = 0.44682633\n",
      "Iteration 62, loss = 0.49629981\n",
      "Iteration 86, loss = 0.38804256\n",
      "Iteration 16, loss = 0.64024003\n",
      "Iteration 102, loss = 0.44379024\n",
      "Iteration 93, loss = 0.44541669\n",
      "Iteration 63, loss = 0.49378243\n",
      "Iteration 87, loss = 0.38740382\n",
      "Iteration 103, loss = 0.44262181\n",
      "Iteration 128, loss = 0.39401206\n",
      "Iteration 94, loss = 0.44399425\n",
      "Iteration 64, loss = 0.49130277\n",
      "Iteration 88, loss = 0.38676329\n",
      "Iteration 104, loss = 0.44147168\n",
      "Iteration 47, loss = 0.42200781\n",
      "Iteration 97, loss = 0.39384730\n",
      "Iteration 65, loss = 0.48885982\n",
      "Iteration 17, loss = 0.63610592\n",
      "Iteration 95, loss = 0.44258431\n",
      "Iteration 129, loss = 0.39374207\n",
      "Iteration 66, loss = 0.48635143\n",
      "Iteration 96, loss = 0.44125085\n",
      "Iteration 89, loss = 0.38617994\n",
      "Iteration 67, loss = 0.48398420\n",
      "Iteration 18, loss = 0.63190457\n",
      "Iteration 48, loss = 0.42004566\n",
      "Iteration 130, loss = 0.39346902\n",
      "Iteration 98, loss = 0.39335667\n",
      "Iteration 105, loss = 0.44038660\n",
      "Iteration 68, loss = 0.48164118\n",
      "Iteration 69, loss = 0.47935397\n",
      "Iteration 19, loss = 0.62777849\n",
      "Iteration 97, loss = 0.43991009\n",
      "Iteration 90, loss = 0.38556642\n",
      "Iteration 49, loss = 0.41820833\n",
      "Iteration 70, loss = 0.47714703\n",
      "Iteration 106, loss = 0.43930187\n",
      "Iteration 131, loss = 0.39316600\n",
      "Iteration 98, loss = 0.43859505\n",
      "Iteration 99, loss = 0.39283413\n",
      "Iteration 71, loss = 0.47484172\n",
      "Iteration 91, loss = 0.38498312\n",
      "Iteration 50, loss = 0.41637546\n",
      "Iteration 107, loss = 0.43824984\n",
      "Iteration 72, loss = 0.47265749\n",
      "Iteration 99, loss = 0.43732225\n",
      "Iteration 132, loss = 0.39291670\n",
      "Iteration 100, loss = 0.39234730\n",
      "Iteration 73, loss = 0.47049898\n",
      "Iteration 108, loss = 0.43721177\n",
      "Iteration 74, loss = 0.46848066\n",
      "Iteration 101, loss = 0.39187629\n",
      "Iteration 75, loss = 0.46632211\n",
      "Iteration 133, loss = 0.39262768\n",
      "Iteration 92, loss = 0.38437583\n",
      "Iteration 100, loss = 0.43605388\n",
      "Iteration 102, loss = 0.39142634\n",
      "Iteration 76, loss = 0.46434538\n",
      "Iteration 77, loss = 0.46231166\n",
      "Iteration 103, loss = 0.39092097\n",
      "Iteration 101, loss = 0.43484947\n",
      "Iteration 20, loss = 0.62371008\n",
      "Iteration 78, loss = 0.46035787\n",
      "Iteration 51, loss = 0.41466629Iteration 109, loss = 0.43622571\n",
      "Iteration 79, loss = 0.45846941\n",
      "Iteration 93, loss = 0.38380954\n",
      "Iteration 80, loss = 0.45653002\n",
      "Iteration 81, loss = 0.45469145\n",
      "Iteration 82, loss = 0.45285764\n",
      "Iteration 83, loss = 0.45101844\n",
      "Iteration 134, loss = 0.39235631\n",
      "Iteration 21, loss = 0.61973740\n",
      "Iteration 84, loss = 0.44931796\n",
      "Iteration 85, loss = 0.44761760\n",
      "\n",
      "Iteration 102, loss = 0.43365003\n",
      "Iteration 86, loss = 0.44588332\n",
      "Iteration 103, loss = 0.43239680\n",
      "Iteration 104, loss = 0.39045419\n",
      "Iteration 87, loss = 0.44428645\n",
      "Iteration 135, loss = 0.39210199\n",
      "Iteration 104, loss = 0.43121080\n",
      "Iteration 88, loss = 0.44259520\n",
      "Iteration 105, loss = 0.39002845\n",
      "Iteration 22, loss = 0.61579642\n",
      "Iteration 89, loss = 0.44106910\n",
      "Iteration 105, loss = 0.43009301\n",
      "Iteration 136, loss = 0.39183914\n",
      "Iteration 90, loss = 0.43948691\n",
      "Iteration 137, loss = 0.39161413\n",
      "Iteration 23, loss = 0.61192355\n",
      "Iteration 91, loss = 0.43792055\n",
      "Iteration 106, loss = 0.38958440\n",
      "Iteration 138, loss = 0.39136536\n",
      "Iteration 92, loss = 0.43641753\n",
      "Iteration 24, loss = 0.60801842\n",
      "Iteration 93, loss = 0.43496358\n",
      "Iteration 52, loss = 0.41303302\n",
      "Iteration 25, loss = 0.60418212\n",
      "Iteration 94, loss = 0.43348348\n",
      "Iteration 94, loss = 0.38326353\n",
      "Iteration 139, loss = 0.39108998\n",
      "Iteration 110, loss = 0.43519471\n",
      "Iteration 95, loss = 0.43204439\n",
      "Iteration 96, loss = 0.43070559\n",
      "Iteration 26, loss = 0.60031540\n",
      "Iteration 97, loss = 0.42932855\n",
      "Iteration 98, loss = 0.42795293\n",
      "Iteration 140, loss = 0.39087506\n",
      "Iteration 27, loss = 0.59653118\n",
      "Iteration 99, loss = 0.42665383\n",
      "Iteration 141, loss = 0.39060833\n",
      "Iteration 100, loss = 0.42538657\n",
      "Iteration 101, loss = 0.42408581\n",
      "Iteration 28, loss = 0.59271107\n",
      "Iteration 102, loss = 0.42290346\n",
      "Iteration 142, loss = 0.39039418\n",
      "Iteration 103, loss = 0.42160652\n",
      "Iteration 29, loss = 0.58903448\n",
      "Iteration 143, loss = 0.39012766\n",
      "Iteration 104, loss = 0.42043424\n",
      "Iteration 105, loss = 0.41926363\n",
      "Iteration 30, loss = 0.58530606\n",
      "Iteration 95, loss = 0.38270569\n",
      "Iteration 144, loss = 0.38990840\n",
      "Iteration 106, loss = 0.41812034\n",
      "Iteration 31, loss = 0.58156308\n",
      "Iteration 53, loss = 0.41153431\n",
      "Iteration 145, loss = 0.38966581\n",
      "Iteration 107, loss = 0.41696778\n",
      "Iteration 111, loss = 0.43425441\n",
      "Iteration 108, loss = 0.41589312\n",
      "Iteration 32, loss = 0.57799564\n",
      "Iteration 109, loss = 0.41481986\n",
      "Iteration 33, loss = 0.57433798\n",
      "Iteration 54, loss = 0.40996005\n",
      "Iteration 110, loss = 0.41373473\n",
      "Iteration 34, loss = 0.57079154\n",
      "Iteration 112, loss = 0.43326843\n",
      "Iteration 96, loss = 0.38218603\n",
      "Iteration 111, loss = 0.41268374\n",
      "Iteration 112, loss = 0.41167382\n",
      "Iteration 35, loss = 0.56717357\n",
      "Iteration 113, loss = 0.41066506\n",
      "Iteration 114, loss = 0.40965937\n",
      "Iteration 115, loss = 0.40869851\n",
      "Iteration 36, loss = 0.56363825\n",
      "Iteration 116, loss = 0.40773275\n",
      "Iteration 117, loss = 0.40683775\n",
      "Iteration 37, loss = 0.56012212\n",
      "Iteration 118, loss = 0.40583425\n",
      "Iteration 55, loss = 0.40837364\n",
      "Iteration 113, loss = 0.43230645\n",
      "Iteration 38, loss = 0.55662945\n",
      "Iteration 119, loss = 0.40501076\n",
      "Iteration 97, loss = 0.38166062\n",
      "Iteration 120, loss = 0.40409744\n",
      "Iteration 39, loss = 0.55320165\n",
      "Iteration 114, loss = 0.43138652\n",
      "Iteration 121, loss = 0.40324198\n",
      "Iteration 56, loss = 0.40699384\n",
      "Iteration 40, loss = 0.54980511\n",
      "Iteration 98, loss = 0.38113585\n",
      "Iteration 122, loss = 0.40233044\n",
      "Iteration 41, loss = 0.54634307\n",
      "Iteration 115, loss = 0.43047404\n",
      "Iteration 57, loss = 0.40562975\n",
      "Iteration 42, loss = 0.54302349\n",
      "Iteration 99, loss = 0.38064728\n",
      "Iteration 107, loss = 0.38913190\n",
      "Iteration 123, loss = 0.40153856\n",
      "Iteration 43, loss = 0.53966492\n",
      "Iteration 116, loss = 0.42954812\n",
      "Iteration 58, loss = 0.40424945\n",
      "Iteration 44, loss = 0.53637580\n",
      "Iteration 124, loss = 0.40073208\n",
      "Iteration 100, loss = 0.38017783\n",
      "Iteration 125, loss = 0.39992046\n",
      "Iteration 45, loss = 0.53309943\n",
      "Iteration 59, loss = 0.40299229\n",
      "Iteration 101, loss = 0.37966116\n",
      "Iteration 117, loss = 0.42869242\n",
      "Iteration 106, loss = 0.42895407\n",
      "Iteration 108, loss = 0.38872655\n",
      "Iteration 126, loss = 0.39910429\n",
      "Iteration 102, loss = 0.37922905\n",
      "Iteration 109, loss = 0.38832174\n",
      "Iteration 107, loss = 0.42783060\n",
      "Iteration 127, loss = 0.39830387\n",
      "Iteration 60, loss = 0.40167221\n",
      "Iteration 103, loss = 0.37872824\n",
      "Iteration 110, loss = 0.38790393\n",
      "Iteration 128, loss = 0.39750090\n",
      "Iteration 108, loss = 0.42674992\n",
      "Iteration 146, loss = 0.38943636\n",
      "Iteration 129, loss = 0.39680437\n",
      "Iteration 104, loss = 0.37828350\n",
      "Iteration 61, loss = 0.40047794\n",
      "Iteration 109, loss = 0.42570398\n",
      "Iteration 118, loss = 0.42776911\n",
      "Iteration 130, loss = 0.39606896\n",
      "Iteration 147, loss = 0.38929990\n",
      "Iteration 119, loss = 0.42693550\n",
      "Iteration 62, loss = 0.39934757\n",
      "Iteration 148, loss = 0.38899943\n",
      "Iteration 131, loss = 0.39531495\n",
      "Iteration 120, loss = 0.42609825\n",
      "Iteration 63, loss = 0.39817116\n",
      "Iteration 111, loss = 0.38748498\n",
      "Iteration 110, loss = 0.42463883\n",
      "Iteration 149, loss = 0.38879340\n",
      "Iteration 64, loss = 0.39710505\n",
      "Iteration 121, loss = 0.42526690\n",
      "Iteration 132, loss = 0.39461574\n",
      "Iteration 111, loss = 0.42360814\n",
      "Iteration 122, loss = 0.42447741\n",
      "Iteration 133, loss = 0.39390522\n",
      "Iteration 65, loss = 0.39595467\n",
      "Iteration 112, loss = 0.38711043\n",
      "Iteration 150, loss = 0.38857657Iteration 123, loss = 0.42363417\n",
      "Iteration 134, loss = 0.39317926\n",
      "\n",
      "Iteration 112, loss = 0.42261286\n",
      "Iteration 105, loss = 0.37784645\n",
      "Iteration 113, loss = 0.38671597\n",
      "Iteration 113, loss = 0.42160202\n",
      "Iteration 135, loss = 0.39255528\n",
      "Iteration 151, loss = 0.38836017\n",
      "Iteration 66, loss = 0.39492663\n",
      "Iteration 136, loss = 0.39183732\n",
      "Iteration 152, loss = 0.38815480\n",
      "Iteration 114, loss = 0.42061026\n",
      "Iteration 106, loss = 0.37738151\n",
      "Iteration 114, loss = 0.38635314\n",
      "Iteration 107, loss = 0.37693663\n",
      "Iteration 115, loss = 0.38595302\n",
      "Iteration 137, loss = 0.39122855\n",
      "Iteration 116, loss = 0.38558142\n",
      "Iteration 124, loss = 0.42290021\n",
      "Iteration 117, loss = 0.38526032\n",
      "Iteration 67, loss = 0.39387925\n",
      "Iteration 138, loss = 0.39053830\n",
      "Iteration 108, loss = 0.37656702\n",
      "Iteration 118, loss = 0.38484805\n",
      "Iteration 153, loss = 0.38797496\n",
      "Iteration 109, loss = 0.37614040\n",
      "Iteration 139, loss = 0.38990685\n",
      "Iteration 119, loss = 0.38452774\n",
      "Iteration 68, loss = 0.39290021\n",
      "Iteration 125, loss = 0.42211648\n",
      "Iteration 154, loss = 0.38776759\n",
      "Iteration 140, loss = 0.38926857\n",
      "Iteration 120, loss = 0.38421687\n",
      "Iteration 115, loss = 0.41966897\n",
      "Iteration 69, loss = 0.39196487\n",
      "Iteration 141, loss = 0.38868920\n",
      "Iteration 126, loss = 0.42132429\n",
      "Iteration 110, loss = 0.37571758\n",
      "Iteration 116, loss = 0.41870662\n",
      "Iteration 121, loss = 0.38384053\n",
      "Iteration 142, loss = 0.38804893\n",
      "Iteration 117, loss = 0.41782170\n",
      "Iteration 111, loss = 0.37530372\n",
      "Iteration 127, loss = 0.42058731\n",
      "Iteration 112, loss = 0.37493676\n",
      "Iteration 143, loss = 0.38742196\n",
      "Iteration 155, loss = 0.38753213\n",
      "Iteration 118, loss = 0.41683737\n",
      "Iteration 113, loss = 0.37454099\n",
      "Iteration 144, loss = 0.38691209\n",
      "Iteration 46, loss = 0.52991582\n",
      "Iteration 114, loss = 0.37418859\n",
      "Iteration 145, loss = 0.38627179\n",
      "Iteration 70, loss = 0.39103389\n",
      "Iteration 156, loss = 0.38739689\n",
      "Iteration 115, loss = 0.37379775\n",
      "Iteration 47, loss = 0.52675625\n",
      "Iteration 146, loss = 0.38566500\n",
      "Iteration 116, loss = 0.37342128\n",
      "Iteration 128, loss = 0.41982103\n",
      "Iteration 122, loss = 0.38350090\n",
      "Iteration 117, loss = 0.37310955\n",
      "Iteration 147, loss = 0.38521404\n",
      "Iteration 119, loss = 0.41597816\n",
      "Iteration 157, loss = 0.38715678\n",
      "Iteration 48, loss = 0.52357957\n",
      "Iteration 118, loss = 0.37269058\n",
      "Iteration 71, loss = 0.39006553\n",
      "Iteration 148, loss = 0.38461319\n",
      "Iteration 119, loss = 0.37239143\n",
      "Iteration 129, loss = 0.41912603\n",
      "Iteration 120, loss = 0.41511241\n",
      "Iteration 149, loss = 0.38408198\n",
      "Iteration 120, loss = 0.37206054\n",
      "Iteration 121, loss = 0.41424070Iteration 49, loss = 0.52045657\n",
      "\n",
      "Iteration 72, loss = 0.38916631\n",
      "Iteration 130, loss = 0.41843559\n",
      "Iteration 150, loss = 0.38351046\n",
      "Iteration 123, loss = 0.38317723\n",
      "Iteration 121, loss = 0.37170420\n",
      "Iteration 122, loss = 0.41337702\n",
      "Iteration 50, loss = 0.51737729\n",
      "Iteration 73, loss = 0.38832948\n",
      "Iteration 151, loss = 0.38296217\n",
      "Iteration 158, loss = 0.38694355\n",
      "Iteration 131, loss = 0.41771701\n",
      "Iteration 152, loss = 0.38245295\n",
      "Iteration 51, loss = 0.51422942\n",
      "Iteration 153, loss = 0.38195302\n",
      "Iteration 74, loss = 0.38756113\n",
      "Iteration 122, loss = 0.37135731\n",
      "Iteration 123, loss = 0.41252857\n",
      "Iteration 159, loss = 0.38676206\n",
      "Iteration 132, loss = 0.41703745\n",
      "Iteration 124, loss = 0.38287190\n",
      "Iteration 154, loss = 0.38142850\n",
      "Iteration 133, loss = 0.41634377\n",
      "Iteration 160, loss = 0.38658176\n",
      "Iteration 155, loss = 0.38095291\n",
      "Iteration 123, loss = 0.37106890\n",
      "Iteration 134, loss = 0.41565803\n",
      "Iteration 75, loss = 0.38667075\n",
      "Iteration 156, loss = 0.38048355\n",
      "Iteration 124, loss = 0.41176674\n",
      "Iteration 125, loss = 0.38255206\n",
      "Iteration 135, loss = 0.41503993\n",
      "Iteration 76, loss = 0.38590606\n",
      "Iteration 125, loss = 0.41093286\n",
      "Iteration 124, loss = 0.37073223\n",
      "Iteration 161, loss = 0.38640826\n",
      "Iteration 77, loss = 0.38512583\n",
      "Iteration 136, loss = 0.41435290\n",
      "Iteration 125, loss = 0.37044391\n",
      "Iteration 126, loss = 0.38222359\n",
      "Iteration 126, loss = 0.41013991\n",
      "Iteration 137, loss = 0.41378273\n",
      "Iteration 126, loss = 0.37010486\n",
      "Iteration 52, loss = 0.51123325\n",
      "Iteration 157, loss = 0.37996409\n",
      "Iteration 138, loss = 0.41312121\n",
      "Iteration 127, loss = 0.38192193\n",
      "Iteration 127, loss = 0.36980411\n",
      "Iteration 53, loss = 0.50834613\n",
      "Iteration 78, loss = 0.38435396\n",
      "Iteration 162, loss = 0.38618820\n",
      "Iteration 128, loss = 0.36950144\n",
      "Iteration 139, loss = 0.41252352Iteration 127, loss = 0.40935143\n",
      "\n",
      "Iteration 128, loss = 0.38163330\n",
      "Iteration 54, loss = 0.50539908\n",
      "Iteration 129, loss = 0.36924211\n",
      "Iteration 129, loss = 0.38133544\n",
      "Iteration 140, loss = 0.41189304\n",
      "Iteration 158, loss = 0.37949509\n",
      "Iteration 163, loss = 0.38604949\n",
      "Iteration 130, loss = 0.36893972\n",
      "Iteration 130, loss = 0.38103677\n",
      "Iteration 141, loss = 0.41130362\n",
      "Iteration 79, loss = 0.38361200\n",
      "Iteration 128, loss = 0.40858560\n",
      "Iteration 55, loss = 0.50240714\n",
      "Iteration 131, loss = 0.36865072\n",
      "Iteration 164, loss = 0.38584689\n",
      "Iteration 159, loss = 0.37902926\n",
      "Iteration 131, loss = 0.38074778\n",
      "Iteration 56, loss = 0.49958936\n",
      "Iteration 142, loss = 0.41071065\n",
      "Iteration 80, loss = 0.38289714\n",
      "Iteration 129, loss = 0.40783021\n",
      "Iteration 81, loss = 0.38222226\n",
      "Iteration 130, loss = 0.40708978\n",
      "Iteration 57, loss = 0.49676290\n",
      "Iteration 165, loss = 0.38565234\n",
      "Iteration 143, loss = 0.41010428\n",
      "Iteration 131, loss = 0.40637787\n",
      "Iteration 82, loss = 0.38149777\n",
      "Iteration 58, loss = 0.49391688\n",
      "Iteration 132, loss = 0.38050353\n",
      "Iteration 160, loss = 0.37855259\n",
      "Iteration 132, loss = 0.40567889\n",
      "Iteration 83, loss = 0.38085090\n",
      "Iteration 59, loss = 0.49127092\n",
      "Iteration 144, loss = 0.40958336\n",
      "Iteration 133, loss = 0.40497429\n",
      "Iteration 84, loss = 0.38017839\n",
      "Iteration 133, loss = 0.38021070\n",
      "Iteration 145, loss = 0.40898934\n",
      "Iteration 134, loss = 0.40421552\n",
      "Iteration 85, loss = 0.37958345\n",
      "Iteration 135, loss = 0.40358947\n",
      "Iteration 146, loss = 0.40838966\n",
      "Iteration 134, loss = 0.37990002\n",
      "Iteration 86, loss = 0.37893313\n",
      "Iteration 136, loss = 0.40288453\n",
      "Iteration 135, loss = 0.37965476\n",
      "Iteration 166, loss = 0.38547829\n",
      "Iteration 132, loss = 0.36838328\n",
      "Iteration 147, loss = 0.40796317\n",
      "Iteration 60, loss = 0.48853182\n",
      "Iteration 136, loss = 0.37939025\n",
      "Iteration 167, loss = 0.38533537\n",
      "Iteration 137, loss = 0.37916523\n",
      "Iteration 161, loss = 0.37806312\n",
      "Iteration 87, loss = 0.37829475\n",
      "Iteration 137, loss = 0.40229249\n",
      "Iteration 168, loss = 0.38513454\n",
      "Iteration 133, loss = 0.36811851\n",
      "Iteration 138, loss = 0.37888791\n",
      "Iteration 148, loss = 0.40734420\n",
      "Iteration 88, loss = 0.37773484\n",
      "Iteration 162, loss = 0.37761268\n",
      "Iteration 169, loss = 0.38496177\n",
      "Iteration 139, loss = 0.37862350\n",
      "Iteration 61, loss = 0.48586328\n",
      "Iteration 170, loss = 0.38480073\n",
      "Iteration 89, loss = 0.37721367\n",
      "Iteration 138, loss = 0.40160135\n",
      "Iteration 140, loss = 0.37838139\n",
      "Iteration 149, loss = 0.40682861\n",
      "Iteration 163, loss = 0.37719250\n",
      "Iteration 134, loss = 0.36782743\n",
      "Iteration 171, loss = 0.38464211\n",
      "Iteration 90, loss = 0.37659054\n",
      "Iteration 141, loss = 0.37813758\n",
      "Iteration 150, loss = 0.40628654\n",
      "Iteration 62, loss = 0.48331137\n",
      "Iteration 142, loss = 0.37789977\n",
      "Iteration 91, loss = 0.37604405\n",
      "Iteration 172, loss = 0.38445653\n",
      "Iteration 139, loss = 0.40095826\n",
      "Iteration 151, loss = 0.40576920\n",
      "Iteration 143, loss = 0.37764289\n",
      "Iteration 164, loss = 0.37671682\n",
      "Iteration 92, loss = 0.37549986\n",
      "Iteration 135, loss = 0.36757903\n",
      "Iteration 63, loss = 0.48069086\n",
      "Iteration 144, loss = 0.37742066\n",
      "Iteration 173, loss = 0.38433267\n",
      "Iteration 165, loss = 0.37628212\n",
      "Iteration 93, loss = 0.37494720\n",
      "Iteration 64, loss = 0.47821305\n",
      "Iteration 145, loss = 0.37717161\n",
      "Iteration 152, loss = 0.40524350\n",
      "Iteration 94, loss = 0.37442256\n",
      "Iteration 166, loss = 0.37583519\n",
      "Iteration 140, loss = 0.40031061\n",
      "Iteration 146, loss = 0.37694528\n",
      "Iteration 65, loss = 0.47570203\n",
      "Iteration 141, loss = 0.39971158\n",
      "Iteration 167, loss = 0.37542814\n",
      "Iteration 147, loss = 0.37678202\n",
      "Iteration 66, loss = 0.47319756\n",
      "Iteration 136, loss = 0.36730262\n",
      "Iteration 153, loss = 0.40478327\n",
      "Iteration 148, loss = 0.37651664\n",
      "Iteration 67, loss = 0.47079326\n",
      "Iteration 174, loss = 0.38415268\n",
      "Iteration 95, loss = 0.37393628\n",
      "Iteration 137, loss = 0.36708130\n",
      "Iteration 149, loss = 0.37630520\n",
      "Iteration 154, loss = 0.40426966\n",
      "Iteration 68, loss = 0.46843509\n",
      "Iteration 142, loss = 0.39908506\n",
      "Iteration 168, loss = 0.37497805\n",
      "Iteration 138, loss = 0.36681750\n",
      "Iteration 150, loss = 0.37607898\n",
      "Iteration 155, loss = 0.40373661\n",
      "Iteration 69, loss = 0.46613365\n",
      "Iteration 139, loss = 0.36656177\n",
      "Iteration 156, loss = 0.40332925\n",
      "Iteration 151, loss = 0.37586224\n",
      "Iteration 70, loss = 0.46386050\n",
      "Iteration 175, loss = 0.38401660\n",
      "Iteration 169, loss = 0.37455996\n",
      "Iteration 96, loss = 0.37342166\n",
      "Iteration 143, loss = 0.39846367\n",
      "Iteration 157, loss = 0.40280333\n",
      "Iteration 170, loss = 0.37417149\n",
      "Iteration 144, loss = 0.39792332\n",
      "Iteration 97, loss = 0.37297071\n",
      "Iteration 158, loss = 0.40231126\n",
      "Iteration 71, loss = 0.46158402\n",
      "Iteration 140, loss = 0.36632457\n",
      "Iteration 171, loss = 0.37377660\n",
      "Iteration 152, loss = 0.37565070\n",
      "Iteration 145, loss = 0.39731044\n",
      "Iteration 159, loss = 0.40186645\n",
      "Iteration 176, loss = 0.38382515\n",
      "Iteration 172, loss = 0.37335209\n",
      "Iteration 72, loss = 0.45934635\n",
      "Iteration 146, loss = 0.39671072\n",
      "Iteration 160, loss = 0.40139014\n",
      "Iteration 173, loss = 0.37294853\n",
      "Iteration 73, loss = 0.45718470\n",
      "Iteration 153, loss = 0.37546841\n",
      "Iteration 147, loss = 0.39623217\n",
      "Iteration 174, loss = 0.37257612\n",
      "Iteration 141, loss = 0.36609008\n",
      "Iteration 74, loss = 0.45519192\n",
      "Iteration 154, loss = 0.37525587\n",
      "Iteration 148, loss = 0.39562616\n",
      "Iteration 175, loss = 0.37222678\n",
      "Iteration 161, loss = 0.40095708\n",
      "Iteration 75, loss = 0.45298500\n",
      "Iteration 177, loss = 0.38366056\n",
      "Iteration 149, loss = 0.39508129\n",
      "Iteration 176, loss = 0.37177863\n",
      "Iteration 76, loss = 0.45095641\n",
      "Iteration 177, loss = 0.37140876\n",
      "Iteration 150, loss = 0.39452737\n",
      "Iteration 77, loss = 0.44897058\n",
      "Iteration 162, loss = 0.40047436\n",
      "Iteration 155, loss = 0.37506819\n",
      "Iteration 142, loss = 0.36584126\n",
      "Iteration 178, loss = 0.37106651\n",
      "Iteration 151, loss = 0.39397477\n",
      "Iteration 78, loss = 0.44698677\n",
      "Iteration 178, loss = 0.38352482\n",
      "Iteration 179, loss = 0.37065246\n",
      "Iteration 152, loss = 0.39344772\n",
      "Iteration 143, loss = 0.36558982\n",
      "Iteration 153, loss = 0.39296943\n",
      "Iteration 180, loss = 0.37031194\n",
      "Iteration 79, loss = 0.44504306\n",
      "Iteration 156, loss = 0.37488507\n",
      "Iteration 163, loss = 0.40009550\n",
      "Iteration 154, loss = 0.39242916\n",
      "Iteration 164, loss = 0.39962142\n",
      "Iteration 157, loss = 0.37468152\n",
      "Iteration 179, loss = 0.38338651\n",
      "Iteration 155, loss = 0.39192230\n",
      "Iteration 80, loss = 0.44312787\n",
      "Iteration 165, loss = 0.39918950\n",
      "Iteration 181, loss = 0.36993251\n",
      "Iteration 158, loss = 0.37447744\n",
      "Iteration 156, loss = 0.39145551\n",
      "Iteration 144, loss = 0.36539933\n",
      "Iteration 166, loss = 0.39875221\n",
      "Iteration 81, loss = 0.44126995\n",
      "Iteration 159, loss = 0.37428483\n",
      "Iteration 182, loss = 0.36960903\n",
      "Iteration 180, loss = 0.38323264\n",
      "Iteration 157, loss = 0.39094302\n",
      "Iteration 145, loss = 0.36511806\n",
      "Iteration 181, loss = 0.38306508\n",
      "Iteration 158, loss = 0.39044678\n",
      "Iteration 146, loss = 0.36489394\n",
      "Iteration 167, loss = 0.39835817\n",
      "Iteration 182, loss = 0.38293375\n",
      "Iteration 159, loss = 0.38997662\n",
      "Iteration 147, loss = 0.36476472\n",
      "Iteration 183, loss = 0.36925094\n",
      "Iteration 82, loss = 0.43942154\n",
      "Iteration 183, loss = 0.38277793\n",
      "Iteration 160, loss = 0.38948491\n",
      "Iteration 168, loss = 0.39791440\n",
      "Iteration 160, loss = 0.37411832\n",
      "Iteration 83, loss = 0.43765309\n",
      "Iteration 148, loss = 0.36448901\n",
      "Iteration 184, loss = 0.38263424\n",
      "Iteration 161, loss = 0.38902900\n",
      "Iteration 84, loss = 0.43588824\n",
      "Iteration 149, loss = 0.36430192\n",
      "Iteration 184, loss = 0.36888819\n",
      "Iteration 161, loss = 0.37393873\n",
      "Iteration 162, loss = 0.38853819\n",
      "Iteration 85, loss = 0.43420261\n",
      "Iteration 169, loss = 0.39749822\n",
      "Iteration 150, loss = 0.36405573\n",
      "Iteration 185, loss = 0.36853742\n",
      "Iteration 163, loss = 0.38812584\n",
      "Iteration 86, loss = 0.43247952\n",
      "Iteration 162, loss = 0.37372048\n",
      "Iteration 170, loss = 0.39710119\n",
      "Iteration 186, loss = 0.36821349\n",
      "Iteration 164, loss = 0.38767040\n",
      "Iteration 185, loss = 0.38248063\n",
      "Iteration 163, loss = 0.37357295\n",
      "Iteration 165, loss = 0.38723748\n",
      "Iteration 187, loss = 0.36787453\n",
      "Iteration 164, loss = 0.37339060\n",
      "Iteration 87, loss = 0.43080461\n",
      "Iteration 151, loss = 0.36383154\n",
      "Iteration 166, loss = 0.38678363\n",
      "Iteration 188, loss = 0.36753307\n",
      "Iteration 171, loss = 0.39671054\n",
      "Iteration 186, loss = 0.38235774\n",
      "Iteration 167, loss = 0.38638696\n",
      "Iteration 165, loss = 0.37322883\n",
      "Iteration 172, loss = 0.39627673\n",
      "Iteration 189, loss = 0.36722350\n",
      "Iteration 187, loss = 0.38220403\n",
      "Iteration 152, loss = 0.36362500\n",
      "Iteration 173, loss = 0.39592644\n",
      "Iteration 88, loss = 0.42923757\n",
      "Iteration 188, loss = 0.38207872\n",
      "Iteration 166, loss = 0.37304420\n",
      "Iteration 174, loss = 0.39550807\n",
      "Iteration 168, loss = 0.38589626\n",
      "Iteration 189, loss = 0.38193861\n",
      "Iteration 190, loss = 0.36689098\n",
      "Iteration 167, loss = 0.37292089\n",
      "Iteration 89, loss = 0.42765835\n",
      "Iteration 175, loss = 0.39514397\n",
      "Iteration 153, loss = 0.36344063\n",
      "Iteration 168, loss = 0.37269089\n",
      "Iteration 176, loss = 0.39472715\n",
      "Iteration 90, loss = 0.42605788\n",
      "Iteration 190, loss = 0.38180318\n",
      "Iteration 169, loss = 0.38549828\n",
      "Iteration 177, loss = 0.39434191\n",
      "Iteration 91, loss = 0.42451530\n",
      "Iteration 191, loss = 0.36656566\n",
      "Iteration 191, loss = 0.38168685\n",
      "Iteration 178, loss = 0.39398349\n",
      "Iteration 92, loss = 0.42304327\n",
      "Iteration 192, loss = 0.38153540\n",
      "Iteration 154, loss = 0.36324222\n",
      "Iteration 170, loss = 0.38508540\n",
      "Iteration 93, loss = 0.42157638\n",
      "Iteration 193, loss = 0.38138384\n",
      "Iteration 169, loss = 0.37254286\n",
      "Iteration 192, loss = 0.36624317\n",
      "Iteration 179, loss = 0.39359523\n",
      "Iteration 155, loss = 0.36306261\n",
      "Iteration 171, loss = 0.38468210\n",
      "Iteration 94, loss = 0.42007730\n",
      "Iteration 180, loss = 0.39326019\n",
      "Iteration 170, loss = 0.37238104\n",
      "Iteration 156, loss = 0.36287733\n",
      "Iteration 172, loss = 0.38426888\n",
      "Iteration 194, loss = 0.38128201\n",
      "Iteration 95, loss = 0.41872597\n",
      "Iteration 193, loss = 0.36593898\n",
      "Iteration 157, loss = 0.36266251\n",
      "Iteration 171, loss = 0.37222155\n",
      "Iteration 173, loss = 0.38387325\n",
      "Iteration 96, loss = 0.41735381\n",
      "Iteration 98, loss = 0.37248580\n",
      "Iteration 158, loss = 0.36248044\n",
      "Iteration 172, loss = 0.37205651\n",
      "Iteration 181, loss = 0.39286864\n",
      "Iteration 194, loss = 0.36561189\n",
      "Iteration 174, loss = 0.38347046\n",
      "Iteration 97, loss = 0.41604150\n",
      "Iteration 195, loss = 0.38113433\n",
      "Iteration 98, loss = 0.41469890\n",
      "Iteration 182, loss = 0.39253053\n",
      "Iteration 175, loss = 0.38312306\n",
      "Iteration 195, loss = 0.36531483\n",
      "Iteration 159, loss = 0.36230480\n",
      "Iteration 173, loss = 0.37191983\n",
      "Iteration 196, loss = 0.38101928\n",
      "Iteration 196, loss = 0.36501445\n",
      "Iteration 176, loss = 0.38267178\n",
      "Iteration 99, loss = 0.41338135\n",
      "Iteration 160, loss = 0.36213012\n",
      "Iteration 183, loss = 0.39214324\n",
      "Iteration 99, loss = 0.37200943\n",
      "Iteration 174, loss = 0.37176423\n",
      "Iteration 197, loss = 0.38088490\n",
      "Iteration 177, loss = 0.38229610\n",
      "Iteration 161, loss = 0.36192691\n",
      "Iteration 100, loss = 0.41216402\n",
      "Iteration 197, loss = 0.36469759\n",
      "Iteration 175, loss = 0.37165967\n",
      "Iteration 178, loss = 0.38193033\n",
      "Iteration 184, loss = 0.39180530\n",
      "Iteration 101, loss = 0.41086642\n",
      "Iteration 176, loss = 0.37144484\n",
      "Iteration 198, loss = 0.38077296\n",
      "Iteration 179, loss = 0.38152474\n",
      "Iteration 162, loss = 0.36174624\n",
      "Iteration 198, loss = 0.36441224\n",
      "Iteration 177, loss = 0.37130372\n",
      "Iteration 199, loss = 0.38063839\n",
      "Iteration 180, loss = 0.38117934\n",
      "Iteration 102, loss = 0.40972956\n",
      "Iteration 199, loss = 0.36411823\n",
      "Iteration 185, loss = 0.39144130\n",
      "Iteration 163, loss = 0.36159397\n",
      "Iteration 200, loss = 0.38051435\n",
      "Iteration 200, loss = 0.36383735\n",
      "Iteration 178, loss = 0.37117886\n",
      "Iteration 181, loss = 0.38078350\n",
      "Iteration 164, loss = 0.36139068\n",
      "Iteration 186, loss = 0.39111819\n",
      "Iteration 100, loss = 0.37157694\n",
      "Iteration 103, loss = 0.40847914\n",
      "Iteration 201, loss = 0.36352009\n",
      "Iteration 182, loss = 0.38047177\n",
      "Iteration 104, loss = 0.40733827\n",
      "Iteration 202, loss = 0.36324273\n",
      "Iteration 179, loss = 0.37101165\n",
      "Iteration 201, loss = 0.38037857\n",
      "Iteration 165, loss = 0.36122783\n",
      "Iteration 187, loss = 0.39074910\n",
      "Iteration 101, loss = 0.37110411\n",
      "Iteration 202, loss = 0.38029981\n",
      "Iteration 166, loss = 0.36104638\n",
      "Iteration 188, loss = 0.39042937\n",
      "Iteration 105, loss = 0.40619343\n",
      "Iteration 183, loss = 0.38005803\n",
      "Iteration 203, loss = 0.38016758\n",
      "Iteration 167, loss = 0.36090956\n",
      "Iteration 180, loss = 0.37087580\n",
      "Iteration 106, loss = 0.40505455\n",
      "Iteration 203, loss = 0.36295984\n",
      "Iteration 189, loss = 0.39008412\n",
      "Iteration 204, loss = 0.38003215\n",
      "Iteration 181, loss = 0.37072899\n",
      "Iteration 204, loss = 0.36266086\n",
      "Iteration 102, loss = 0.37070251\n",
      "Iteration 205, loss = 0.37991643\n",
      "Iteration 107, loss = 0.40398948\n",
      "Iteration 168, loss = 0.36071998\n",
      "Iteration 205, loss = 0.36238979\n",
      "Iteration 184, loss = 0.37971112\n",
      "Iteration 206, loss = 0.37979426\n",
      "Iteration 108, loss = 0.40287149\n",
      "Iteration 169, loss = 0.36055338\n",
      "Iteration 190, loss = 0.38974819\n",
      "Iteration 182, loss = 0.37064050\n",
      "Iteration 206, loss = 0.36212712\n",
      "Iteration 191, loss = 0.38943765\n",
      "Iteration 183, loss = 0.37046504\n",
      "Iteration 185, loss = 0.37933660\n",
      "Iteration 170, loss = 0.36041371\n",
      "Iteration 207, loss = 0.37966537\n",
      "Iteration 207, loss = 0.36184352\n",
      "Iteration 109, loss = 0.40183844\n",
      "Iteration 186, loss = 0.37900893\n",
      "Iteration 208, loss = 0.37957317\n",
      "Iteration 171, loss = 0.36027178\n",
      "Iteration 110, loss = 0.40082182\n",
      "Iteration 184, loss = 0.37032851\n",
      "Iteration 192, loss = 0.38908210\n",
      "Iteration 209, loss = 0.37946567\n",
      "Iteration 103, loss = 0.37024280\n",
      "Iteration 172, loss = 0.36009079\n",
      "Iteration 185, loss = 0.37017859\n",
      "Iteration 208, loss = 0.36155204\n",
      "Iteration 187, loss = 0.37865965\n",
      "Iteration 210, loss = 0.37934244\n",
      "Iteration 104, loss = 0.36983177\n",
      "Iteration 186, loss = 0.37005899\n",
      "Iteration 111, loss = 0.39980651\n",
      "Iteration 173, loss = 0.35993699\n",
      "Iteration 193, loss = 0.38874655\n",
      "Iteration 209, loss = 0.36129476\n",
      "Iteration 188, loss = 0.37830918\n",
      "Iteration 187, loss = 0.36993719\n",
      "Iteration 210, loss = 0.36102723\n",
      "Iteration 211, loss = 0.37923863\n",
      "Iteration 189, loss = 0.37796826\n",
      "Iteration 194, loss = 0.38844120\n",
      "Iteration 188, loss = 0.36979325\n",
      "Iteration 174, loss = 0.35980854\n",
      "Iteration 112, loss = 0.39880855\n",
      "Iteration 212, loss = 0.37912596\n",
      "Iteration 189, loss = 0.36967443\n",
      "Iteration 175, loss = 0.35971686\n",
      "Iteration 190, loss = 0.37764098\n",
      "Iteration 195, loss = 0.38811798\n",
      "Iteration 213, loss = 0.37903282\n",
      "Iteration 176, loss = 0.35948161\n",
      "Iteration 211, loss = 0.36074550\n",
      "Iteration 191, loss = 0.37729832\n",
      "Iteration 105, loss = 0.36944026Iteration 113, loss = 0.39784915Iteration 190, loss = 0.36956080\n",
      "\n",
      "\n",
      "Iteration 214, loss = 0.37890141\n",
      "Iteration 177, loss = 0.35933868\n",
      "Iteration 196, loss = 0.38780801\n",
      "Iteration 215, loss = 0.37882055\n",
      "Iteration 178, loss = 0.35923725Iteration 197, loss = 0.38749198Iteration 114, loss = 0.39685977\n",
      "\n",
      "\n",
      "Iteration 212, loss = 0.36049308\n",
      "Iteration 192, loss = 0.37696808\n",
      "Iteration 115, loss = 0.39593991\n",
      "Iteration 216, loss = 0.37868826\n",
      "Iteration 198, loss = 0.38719403\n",
      "Iteration 179, loss = 0.35904740\n",
      "Iteration 193, loss = 0.37663577\n",
      "Iteration 191, loss = 0.36941764\n",
      "Iteration 116, loss = 0.39502642\n",
      "Iteration 217, loss = 0.37857983Iteration 194, loss = 0.37631445\n",
      "\n",
      "Iteration 199, loss = 0.38687530\n",
      "Iteration 106, loss = 0.36901371\n",
      "Iteration 117, loss = 0.39416319\n",
      "Iteration 213, loss = 0.36024311\n",
      "Iteration 195, loss = 0.37600764\n",
      "Iteration 200, loss = 0.38656970\n",
      "Iteration 107, loss = 0.36863025\n",
      "Iteration 180, loss = 0.35890899\n",
      "Iteration 118, loss = 0.39323996\n",
      "Iteration 196, loss = 0.37569284\n",
      "Iteration 201, loss = 0.38625947\n",
      "Iteration 192, loss = 0.36930811\n",
      "Iteration 108, loss = 0.36825271\n",
      "Iteration 119, loss = 0.39247231\n",
      "Iteration 197, loss = 0.37537206\n",
      "Iteration 218, loss = 0.37848332\n",
      "Iteration 202, loss = 0.38599583\n",
      "Iteration 214, loss = 0.35999031\n",
      "Iteration 181, loss = 0.35877472\n",
      "Iteration 203, loss = 0.38569148\n",
      "Iteration 120, loss = 0.39155755\n",
      "Iteration 109, loss = 0.36786068\n",
      "Iteration 198, loss = 0.37508476\n",
      "Iteration 182, loss = 0.35865642\n",
      "Iteration 219, loss = 0.37840120\n",
      "Iteration 193, loss = 0.36917182Iteration 204, loss = 0.38538656\n",
      "\n",
      "Iteration 121, loss = 0.39072975\n",
      "Iteration 199, loss = 0.37475993\n",
      "Iteration 220, loss = 0.37828318\n",
      "Iteration 215, loss = 0.35974599\n",
      "Iteration 122, loss = 0.38983530\n",
      "Iteration 205, loss = 0.38509042\n",
      "Iteration 194, loss = 0.36905319\n",
      "Iteration 200, loss = 0.37448272\n",
      "Iteration 183, loss = 0.35853562\n",
      "Iteration 201, loss = 0.37415802\n",
      "Iteration 123, loss = 0.38909993\n",
      "Iteration 195, loss = 0.36894064\n",
      "Iteration 216, loss = 0.35947787\n",
      "Iteration 206, loss = 0.38480411\n",
      "Iteration 221, loss = 0.37819172\n",
      "Iteration 110, loss = 0.36747958\n",
      "Iteration 124, loss = 0.38832189\n",
      "Iteration 217, loss = 0.35921623\n",
      "Iteration 184, loss = 0.35837033\n",
      "Iteration 196, loss = 0.36883669\n",
      "Iteration 202, loss = 0.37388142\n",
      "Iteration 207, loss = 0.38450904\n",
      "Iteration 218, loss = 0.35898141\n",
      "Iteration 222, loss = 0.37807311\n",
      "Iteration 185, loss = 0.35824361\n",
      "Iteration 125, loss = 0.38756112\n",
      "Iteration 197, loss = 0.36870032\n",
      "Iteration 203, loss = 0.37360358\n",
      "Iteration 208, loss = 0.38424167\n",
      "Iteration 219, loss = 0.35875113\n",
      "Iteration 126, loss = 0.38680930\n",
      "Iteration 223, loss = 0.37797095\n",
      "Iteration 186, loss = 0.35811491\n",
      "Iteration 209, loss = 0.38395841\n",
      "Iteration 198, loss = 0.36860599\n",
      "Iteration 187, loss = 0.35798559\n",
      "Iteration 111, loss = 0.36711899Iteration 127, loss = 0.38601681\n",
      "\n",
      "Iteration 199, loss = 0.36847678Iteration 210, loss = 0.38367805\n",
      "\n",
      "Iteration 224, loss = 0.37789765\n",
      "Iteration 204, loss = 0.37327906\n",
      "Iteration 188, loss = 0.35784439\n",
      "Iteration 220, loss = 0.35846597\n",
      "Iteration 112, loss = 0.36675692\n",
      "Iteration 225, loss = 0.37777946\n",
      "Iteration 221, loss = 0.35824210\n",
      "Iteration 128, loss = 0.38525487\n",
      "Iteration 205, loss = 0.37301176\n",
      "Iteration 211, loss = 0.38339640\n",
      "Iteration 226, loss = 0.37767668\n",
      "Iteration 200, loss = 0.36837562\n",
      "Iteration 129, loss = 0.38458358\n",
      "Iteration 212, loss = 0.38312907\n",
      "Iteration 189, loss = 0.35774197\n",
      "Iteration 201, loss = 0.36824697\n",
      "Iteration 130, loss = 0.38390376\n",
      "Iteration 227, loss = 0.37760545Iteration 213, loss = 0.38287676\n",
      "\n",
      "Iteration 206, loss = 0.37273589\n",
      "Iteration 222, loss = 0.35801128\n",
      "Iteration 113, loss = 0.36642361\n",
      "Iteration 214, loss = 0.38258128\n",
      "Iteration 228, loss = 0.37750067\n",
      "Iteration 223, loss = 0.35775305\n",
      "Iteration 190, loss = 0.35761090\n",
      "Iteration 215, loss = 0.38233384\n",
      "Iteration 229, loss = 0.37740282\n",
      "Iteration 191, loss = 0.35748659\n",
      "Iteration 114, loss = 0.36606873\n",
      "Iteration 131, loss = 0.38320357\n",
      "Iteration 202, loss = 0.36816037\n",
      "Iteration 216, loss = 0.38204996\n",
      "Iteration 230, loss = 0.37730584\n",
      "Iteration 224, loss = 0.35752659\n",
      "Iteration 192, loss = 0.35736225\n",
      "Iteration 207, loss = 0.37245488\n",
      "Iteration 132, loss = 0.38254990\n",
      "Iteration 217, loss = 0.38177987\n",
      "Iteration 225, loss = 0.35727788\n",
      "Iteration 133, loss = 0.38185875\n",
      "Iteration 218, loss = 0.38152200\n",
      "Iteration 231, loss = 0.37719620\n",
      "Iteration 226, loss = 0.35704055\n",
      "Iteration 208, loss = 0.37216546\n",
      "Iteration 219, loss = 0.38128067\n",
      "Iteration 203, loss = 0.36806261\n",
      "Iteration 134, loss = 0.38118857\n",
      "Iteration 193, loss = 0.35724403\n",
      "Iteration 220, loss = 0.38101033\n",
      "Iteration 209, loss = 0.37189475\n",
      "Iteration 135, loss = 0.38061801\n",
      "Iteration 204, loss = 0.36792780\n",
      "Iteration 115, loss = 0.36573165\n",
      "Iteration 227, loss = 0.35682497\n",
      "Iteration 232, loss = 0.37711652\n",
      "Iteration 194, loss = 0.35711771\n",
      "Iteration 136, loss = 0.37992402\n",
      "Iteration 221, loss = 0.38076609\n",
      "Iteration 228, loss = 0.35658426\n",
      "Iteration 137, loss = 0.37930854\n",
      "Iteration 233, loss = 0.37702791\n",
      "Iteration 210, loss = 0.37162720Iteration 229, loss = 0.35637509\n",
      "\n",
      "Iteration 138, loss = 0.37870371\n",
      "Iteration 234, loss = 0.37694387\n",
      "Iteration 195, loss = 0.35700653\n",
      "Iteration 205, loss = 0.36782882\n",
      "Iteration 222, loss = 0.38050171\n",
      "Iteration 230, loss = 0.35613217\n",
      "Iteration 116, loss = 0.36539184\n",
      "Iteration 139, loss = 0.37810269\n",
      "Iteration 235, loss = 0.37687002\n",
      "Iteration 223, loss = 0.38024358\n",
      "Iteration 206, loss = 0.36773755\n",
      "Iteration 140, loss = 0.37750961\n",
      "Iteration 196, loss = 0.35689629\n",
      "Iteration 236, loss = 0.37677332\n",
      "Iteration 231, loss = 0.35590595Iteration 211, loss = 0.37134199\n",
      "\n",
      "Iteration 141, loss = 0.37695116\n",
      "Iteration 117, loss = 0.36509294\n",
      "Iteration 237, loss = 0.37665945\n",
      "Iteration 212, loss = 0.37108817\n",
      "Iteration 142, loss = 0.37637078\n",
      "Iteration 207, loss = 0.36762278\n",
      "Iteration 224, loss = 0.38000470\n",
      "Iteration 238, loss = 0.37658430\n",
      "Iteration 143, loss = 0.37575932\n",
      "Iteration 232, loss = 0.35568023\n",
      "Iteration 197, loss = 0.35676748\n",
      "Iteration 213, loss = 0.37083190\n",
      "Iteration 118, loss = 0.36474829\n",
      "Iteration 208, loss = 0.36751796\n",
      "Iteration 225, loss = 0.37974482\n",
      "Iteration 144, loss = 0.37527385\n",
      "Iteration 209, loss = 0.36742583\n",
      "Iteration 226, loss = 0.37948957\n",
      "Iteration 198, loss = 0.35666277\n",
      "Iteration 214, loss = 0.37055974\n",
      "Iteration 239, loss = 0.37649363\n",
      "Iteration 227, loss = 0.37927506\n",
      "Iteration 233, loss = 0.35547017\n",
      "Iteration 199, loss = 0.35654546\n",
      "Iteration 210, loss = 0.36732492\n",
      "Iteration 119, loss = 0.36450383\n",
      "Iteration 145, loss = 0.37468725\n",
      "Iteration 215, loss = 0.37030819\n",
      "Iteration 228, loss = 0.37902659\n",
      "Iteration 200, loss = 0.35644641\n",
      "Iteration 240, loss = 0.37639463\n",
      "Iteration 234, loss = 0.35526734\n",
      "Iteration 229, loss = 0.37876643\n",
      "Iteration 201, loss = 0.35632044\n",
      "Iteration 216, loss = 0.37004646\n",
      "Iteration 211, loss = 0.36722023\n",
      "Iteration 230, loss = 0.37852336\n",
      "Iteration 217, loss = 0.36978119\n",
      "Iteration 241, loss = 0.37631759\n",
      "Iteration 146, loss = 0.37412300\n",
      "Iteration 212, loss = 0.36712307\n",
      "Iteration 202, loss = 0.35622359\n",
      "Iteration 231, loss = 0.37827114\n",
      "Iteration 235, loss = 0.35503484\n",
      "Iteration 120, loss = 0.36417208\n",
      "Iteration 218, loss = 0.36955457\n",
      "Iteration 242, loss = 0.37623813\n",
      "Iteration 232, loss = 0.37804485\n",
      "Iteration 203, loss = 0.35611825\n",
      "Iteration 219, loss = 0.36929559\n",
      "Iteration 213, loss = 0.36703161\n",
      "Iteration 204, loss = 0.35599446\n",
      "Iteration 236, loss = 0.35481431\n",
      "Iteration 147, loss = 0.37367905\n",
      "Iteration 220, loss = 0.36902066\n",
      "Iteration 243, loss = 0.37614433\n",
      "Iteration 214, loss = 0.36692002\n",
      "Iteration 121, loss = 0.36386035\n",
      "Iteration 221, loss = 0.36879498\n",
      "Iteration 233, loss = 0.37780694\n",
      "Iteration 215, loss = 0.36684080\n",
      "Iteration 237, loss = 0.35459030\n",
      "Iteration 222, loss = 0.36855311\n",
      "Iteration 244, loss = 0.37604878\n",
      "Iteration 122, loss = 0.36352355\n",
      "Iteration 148, loss = 0.37315108\n",
      "Iteration 205, loss = 0.35589680\n",
      "Iteration 238, loss = 0.35438781\n",
      "Iteration 223, loss = 0.36830634\n",
      "Iteration 123, loss = 0.36326819\n",
      "Iteration 245, loss = 0.37598762\n",
      "Iteration 239, loss = 0.35418842\n",
      "Iteration 216, loss = 0.36674579\n",
      "Iteration 234, loss = 0.37758202\n",
      "Iteration 149, loss = 0.37262987\n",
      "Iteration 224, loss = 0.36805441\n",
      "Iteration 240, loss = 0.35396649\n",
      "Iteration 217, loss = 0.36663739\n",
      "Iteration 124, loss = 0.36298497\n",
      "Iteration 241, loss = 0.35379695\n",
      "Iteration 246, loss = 0.37590207\n",
      "Iteration 235, loss = 0.37736692\n",
      "Iteration 206, loss = 0.35580391\n",
      "Iteration 225, loss = 0.36781869\n",
      "Iteration 150, loss = 0.37214747\n",
      "Iteration 207, loss = 0.35569453\n",
      "Iteration 125, loss = 0.36272490Iteration 226, loss = 0.36758500\n",
      "\n",
      "Iteration 242, loss = 0.35357362\n",
      "Iteration 218, loss = 0.36656913\n",
      "Iteration 236, loss = 0.37713510\n",
      "Iteration 247, loss = 0.37580443\n",
      "Iteration 208, loss = 0.35557793\n",
      "Iteration 151, loss = 0.37161801\n",
      "Iteration 237, loss = 0.37686706\n",
      "Iteration 126, loss = 0.36242841\n",
      "Iteration 152, loss = 0.37115560\n",
      "Iteration 219, loss = 0.36647363\n",
      "Iteration 127, loss = 0.36213175\n",
      "Iteration 227, loss = 0.36737219\n",
      "Iteration 209, loss = 0.35548862\n",
      "Iteration 248, loss = 0.37572015\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 243, loss = 0.35338067\n",
      "Iteration 238, loss = 0.37665808\n",
      "Iteration 128, loss = 0.36185017\n",
      "Iteration 153, loss = 0.37070346\n",
      "Iteration 239, loss = 0.37643271\n",
      "Iteration 129, loss = 0.36164018\n",
      "Iteration 228, loss = 0.36711371\n",
      "Iteration 220, loss = 0.36636385\n",
      "Iteration 240, loss = 0.37619013\n",
      "Iteration 130, loss = 0.36137703\n",
      "Iteration 244, loss = 0.35314438\n",
      "Iteration 154, loss = 0.37019562\n",
      "Iteration 221, loss = 0.36628909\n",
      "Iteration 210, loss = 0.35539313\n",
      "Iteration 229, loss = 0.36689859\n",
      "Iteration 241, loss = 0.37598400\n",
      "Iteration 155, loss = 0.36976399\n",
      "Iteration 245, loss = 0.35293473\n",
      "Iteration 230, loss = 0.36665934\n",
      "Iteration 222, loss = 0.36620013\n",
      "Iteration 242, loss = 0.37576079\n",
      "Iteration 211, loss = 0.35529062\n",
      "Iteration 131, loss = 0.36111928\n",
      "Iteration 223, loss = 0.36611016\n",
      "Iteration 243, loss = 0.37553223\n",
      "Iteration 231, loss = 0.36642269\n",
      "Iteration 156, loss = 0.36928671\n",
      "Iteration 246, loss = 0.35275260\n",
      "Iteration 1, loss = 0.70182636Iteration 224, loss = 0.36602919\n",
      "\n",
      "Iteration 212, loss = 0.35518895\n",
      "Iteration 157, loss = 0.36883700\n",
      "Iteration 232, loss = 0.36619812\n",
      "Iteration 2, loss = 0.70147420\n",
      "Iteration 213, loss = 0.35509934\n",
      "Iteration 158, loss = 0.36841938\n",
      "Iteration 244, loss = 0.37530484\n",
      "Iteration 132, loss = 0.36089878\n",
      "Iteration 247, loss = 0.35255080\n",
      "Iteration 159, loss = 0.36798590\n",
      "Iteration 245, loss = 0.37510319\n",
      "Iteration 3, loss = 0.70105907\n",
      "Iteration 225, loss = 0.36593275\n",
      "Iteration 233, loss = 0.36599719\n",
      "Iteration 246, loss = 0.37489034\n",
      "Iteration 214, loss = 0.35500541\n",
      "Iteration 248, loss = 0.35236657\n",
      "Iteration 160, loss = 0.36754909\n",
      "Iteration 226, loss = 0.36585019\n",
      "Iteration 247, loss = 0.37465737\n",
      "Iteration 234, loss = 0.36578013\n",
      "Iteration 4, loss = 0.70048664\n",
      "Iteration 161, loss = 0.36711478\n",
      "Iteration 133, loss = 0.36063681\n",
      "Iteration 227, loss = 0.36578180\n",
      "Iteration 249, loss = 0.35217858\n",
      "Iteration 215, loss = 0.35492413\n",
      "Iteration 235, loss = 0.36556304\n",
      "Iteration 162, loss = 0.36670151\n",
      "Iteration 228, loss = 0.36568249\n",
      "Iteration 250, loss = 0.35197713\n",
      "Iteration 248, loss = 0.37445406\n",
      "Iteration 216, loss = 0.35482333\n",
      "Iteration 229, loss = 0.36561335\n",
      "Iteration 163, loss = 0.36630677\n",
      "Iteration 217, loss = 0.35471330\n",
      "Iteration 236, loss = 0.36533938\n",
      "Iteration 5, loss = 0.69994127\n",
      "Iteration 164, loss = 0.36590441\n",
      "Iteration 230, loss = 0.36552238\n",
      "Iteration 134, loss = 0.36038196\n",
      "Iteration 165, loss = 0.36547507\n",
      "Iteration 251, loss = 0.35178494\n",
      "Iteration 6, loss = 0.69940748\n",
      "Iteration 218, loss = 0.35463568\n",
      "Iteration 237, loss = 0.36509876\n",
      "Iteration 249, loss = 0.37424196\n",
      "Iteration 219, loss = 0.35456437\n",
      "Iteration 238, loss = 0.36490251\n",
      "Iteration 231, loss = 0.36542754\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 166, loss = 0.36509118\n",
      "Iteration 220, loss = 0.35443649\n",
      "Iteration 239, loss = 0.36469617\n",
      "Iteration 252, loss = 0.35159089\n",
      "Iteration 135, loss = 0.36018164\n",
      "Iteration 7, loss = 0.69904511\n",
      "Iteration 240, loss = 0.36447346\n",
      "Iteration 221, loss = 0.35436789\n",
      "Iteration 250, loss = 0.37403654\n",
      "Iteration 167, loss = 0.36470686\n",
      "Iteration 241, loss = 0.36428088\n",
      "Iteration 136, loss = 0.35991941\n",
      "Iteration 8, loss = 0.69861551\n",
      "Iteration 168, loss = 0.36433523\n",
      "Iteration 253, loss = 0.35140371\n",
      "Iteration 222, loss = 0.35427805\n",
      "Iteration 251, loss = 0.37382916\n",
      "Iteration 254, loss = 0.35121900\n",
      "Iteration 242, loss = 0.36409758\n",
      "Iteration 1, loss = 0.80743053\n",
      "Iteration 223, loss = 0.35418259\n",
      "Iteration 252, loss = 0.37361884\n",
      "Iteration 9, loss = 0.69817959\n",
      "Iteration 137, loss = 0.35969603\n",
      "Iteration 243, loss = 0.36387436\n",
      "Iteration 2, loss = 0.79658341\n",
      "Iteration 255, loss = 0.35101742\n",
      "Iteration 10, loss = 0.69789153\n",
      "Iteration 244, loss = 0.36366201\n",
      "Iteration 3, loss = 0.78008379\n",
      "Iteration 224, loss = 0.35411094\n",
      "Iteration 253, loss = 0.37340073\n",
      "Iteration 245, loss = 0.36345964\n",
      "Iteration 4, loss = 0.76030947\n",
      "Iteration 169, loss = 0.36392779\n",
      "Iteration 11, loss = 0.69781608\n",
      "Iteration 256, loss = 0.35083183\n",
      "Iteration 5, loss = 0.73960111\n",
      "Iteration 246, loss = 0.36326655\n",
      "Iteration 138, loss = 0.35949586\n",
      "Iteration 254, loss = 0.37319768\n",
      "Iteration 257, loss = 0.35065635\n",
      "Iteration 170, loss = 0.36355602\n",
      "Iteration 12, loss = 0.69751423\n",
      "Iteration 258, loss = 0.35047193\n",
      "Iteration 255, loss = 0.37300015\n",
      "Iteration 6, loss = 0.71814745\n",
      "Iteration 225, loss = 0.35401612\n",
      "Iteration 259, loss = 0.35029149\n",
      "Iteration 13, loss = 0.69733383\n",
      "Iteration 247, loss = 0.36305462\n",
      "Iteration 139, loss = 0.35927366\n",
      "Iteration 226, loss = 0.35391512\n",
      "Iteration 260, loss = 0.35012321\n",
      "Iteration 14, loss = 0.69717912\n",
      "Iteration 256, loss = 0.37280315\n",
      "Iteration 171, loss = 0.36322592\n",
      "Iteration 227, loss = 0.35385058\n",
      "Iteration 140, loss = 0.35904955\n",
      "Iteration 228, loss = 0.35375971\n",
      "Iteration 257, loss = 0.37258880\n",
      "Iteration 248, loss = 0.36287777\n",
      "Iteration 172, loss = 0.36282041\n",
      "Iteration 141, loss = 0.35885251\n",
      "Iteration 15, loss = 0.69711082\n",
      "Iteration 7, loss = 0.69718784\n",
      "Iteration 229, loss = 0.35369900\n",
      "Iteration 249, loss = 0.36267966\n",
      "Iteration 261, loss = 0.34993953\n",
      "Iteration 142, loss = 0.35863123\n",
      "Iteration 173, loss = 0.36247913\n",
      "Iteration 258, loss = 0.37240685\n",
      "Iteration 262, loss = 0.34975815\n",
      "Iteration 16, loss = 0.69701830\n",
      "Iteration 143, loss = 0.35841458\n",
      "Iteration 230, loss = 0.35360379\n",
      "Iteration 250, loss = 0.36247401\n",
      "Iteration 174, loss = 0.36213390\n",
      "Iteration 231, loss = 0.35351512\n",
      "Iteration 144, loss = 0.35823864\n",
      "Iteration 251, loss = 0.36229163\n",
      "Iteration 232, loss = 0.35343058\n",
      "Iteration 145, loss = 0.35799458\n",
      "Iteration 8, loss = 0.67657743\n",
      "Iteration 259, loss = 0.37221279\n",
      "Iteration 263, loss = 0.34958639\n",
      "Iteration 17, loss = 0.69707701\n",
      "Iteration 252, loss = 0.36211888\n",
      "Iteration 260, loss = 0.37201478\n",
      "Iteration 264, loss = 0.34943533\n",
      "Iteration 175, loss = 0.36176681\n",
      "Iteration 233, loss = 0.35335831\n",
      "Iteration 9, loss = 0.65766241\n",
      "Iteration 265, loss = 0.34924727\n",
      "Iteration 176, loss = 0.36141367\n",
      "Iteration 10, loss = 0.63980065\n",
      "Iteration 18, loss = 0.69695576\n",
      "Iteration 234, loss = 0.35330297\n",
      "Iteration 253, loss = 0.36190936\n",
      "Iteration 11, loss = 0.62302184\n",
      "Iteration 19, loss = 0.69688783\n",
      "Iteration 177, loss = 0.36108853\n",
      "Iteration 261, loss = 0.37181112\n",
      "Iteration 266, loss = 0.34907333\n",
      "Iteration 20, loss = 0.69686763\n",
      "Iteration 267, loss = 0.34891298\n",
      "Iteration 262, loss = 0.37162033\n",
      "Iteration 146, loss = 0.35780486\n",
      "Iteration 12, loss = 0.60768387\n",
      "Iteration 235, loss = 0.35321159\n",
      "Iteration 178, loss = 0.36076567\n",
      "Iteration 268, loss = 0.34873957\n",
      "Iteration 263, loss = 0.37141933\n",
      "Iteration 13, loss = 0.59360539\n",
      "Iteration 21, loss = 0.69682324\n",
      "Iteration 269, loss = 0.34855214\n",
      "Iteration 147, loss = 0.35766309\n",
      "Iteration 254, loss = 0.36173032\n",
      "Iteration 264, loss = 0.37125430\n",
      "Iteration 179, loss = 0.36039435\n",
      "Iteration 14, loss = 0.58077785\n",
      "Iteration 236, loss = 0.35313396\n",
      "Iteration 270, loss = 0.34837910\n",
      "Iteration 265, loss = 0.37103579\n",
      "Iteration 255, loss = 0.36153637\n",
      "Iteration 180, loss = 0.36009386\n",
      "Iteration 271, loss = 0.34822358\n",
      "Iteration 266, loss = 0.37084892\n",
      "Iteration 22, loss = 0.69679110\n",
      "Iteration 237, loss = 0.35303824\n",
      "Iteration 181, loss = 0.35972898Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "Iteration 15, loss = 0.56854459\n",
      "Iteration 23, loss = 0.69680501\n",
      "Iteration 182, loss = 0.35944481\n",
      "Iteration 148, loss = 0.35744498\n",
      "Iteration 256, loss = 0.36136142\n",
      "Iteration 16, loss = 0.55769188\n",
      "Iteration 272, loss = 0.34803566\n",
      "Iteration 267, loss = 0.37066091\n",
      "Iteration 24, loss = 0.69676395\n",
      "Iteration 149, loss = 0.35727907\n",
      "Iteration 17, loss = 0.54681170\n",
      "Iteration 25, loss = 0.69672497\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 183, loss = 0.35912655\n",
      "Iteration 257, loss = 0.36115898\n",
      "Iteration 1, loss = 0.70126928\n",
      "Iteration 184, loss = 0.35879872\n",
      "Iteration 18, loss = 0.53728671\n",
      "Iteration 268, loss = 0.37048296\n",
      "Iteration 258, loss = 0.36098577\n",
      "Iteration 19, loss = 0.52877191\n",
      "Iteration 20, loss = 0.52046652\n",
      "Iteration 259, loss = 0.36081689\n",
      "Iteration 185, loss = 0.35849596\n",
      "Iteration 21, loss = 0.51297449\n",
      "Iteration 150, loss = 0.35708620\n",
      "Iteration 273, loss = 0.34787892Iteration 22, loss = 0.50552486\n",
      "\n",
      "Iteration 186, loss = 0.35820177\n",
      "Iteration 23, loss = 0.49903927\n",
      "Iteration 2, loss = 0.69950285\n",
      "Iteration 151, loss = 0.35687141\n",
      "Iteration 24, loss = 0.49258589\n",
      "Iteration 260, loss = 0.36064558\n",
      "Iteration 187, loss = 0.35787962\n",
      "Iteration 25, loss = 0.48670599\n",
      "Iteration 3, loss = 0.69671957\n",
      "Iteration 269, loss = 0.37026526\n",
      "Iteration 152, loss = 0.35671618\n",
      "Iteration 4, loss = 0.69324308\n",
      "Iteration 274, loss = 0.34775773\n",
      "Iteration 5, loss = 0.68942440\n",
      "Iteration 275, loss = 0.34756364\n",
      "Iteration 261, loss = 0.36043950\n",
      "Iteration 270, loss = 0.37009248\n",
      "Iteration 188, loss = 0.35758236\n",
      "Iteration 6, loss = 0.68524952\n",
      "Iteration 276, loss = 0.34738835\n",
      "Iteration 26, loss = 0.48111120\n",
      "Iteration 153, loss = 0.35656537\n",
      "Iteration 262, loss = 0.36028585\n",
      "Iteration 7, loss = 0.68099545\n",
      "Iteration 277, loss = 0.34723060\n",
      "Iteration 27, loss = 0.47602214\n",
      "Iteration 278, loss = 0.34715392\n",
      "Iteration 189, loss = 0.35728486\n",
      "Iteration 271, loss = 0.36991684\n",
      "Iteration 263, loss = 0.36010170\n",
      "Iteration 154, loss = 0.35636153\n",
      "Iteration 190, loss = 0.35698670\n",
      "Iteration 279, loss = 0.34692584\n",
      "Iteration 8, loss = 0.67643989\n",
      "Iteration 28, loss = 0.47091079\n",
      "Iteration 272, loss = 0.36971059\n",
      "Iteration 155, loss = 0.35621508\n",
      "Iteration 264, loss = 0.35994292\n",
      "Iteration 29, loss = 0.46633944\n",
      "Iteration 280, loss = 0.34676543\n",
      "Iteration 9, loss = 0.67199422\n",
      "Iteration 30, loss = 0.46189896\n",
      "Iteration 281, loss = 0.34659565\n",
      "Iteration 273, loss = 0.36953420\n",
      "Iteration 156, loss = 0.35602388\n",
      "Iteration 265, loss = 0.35976866\n",
      "Iteration 282, loss = 0.34646901\n",
      "Iteration 191, loss = 0.35670596\n",
      "Iteration 274, loss = 0.36936405\n",
      "Iteration 31, loss = 0.45786428\n",
      "Iteration 266, loss = 0.35960307\n",
      "Iteration 283, loss = 0.34630722\n",
      "Iteration 10, loss = 0.66746453\n",
      "Iteration 32, loss = 0.45375903\n",
      "Iteration 192, loss = 0.35641914\n",
      "Iteration 267, loss = 0.35942531\n",
      "Iteration 275, loss = 0.36917153\n",
      "Iteration 284, loss = 0.34618578\n",
      "Iteration 193, loss = 0.35614839\n",
      "Iteration 157, loss = 0.35585941\n",
      "Iteration 276, loss = 0.36899035\n",
      "Iteration 33, loss = 0.44994139\n",
      "Iteration 11, loss = 0.66294245\n",
      "Iteration 268, loss = 0.35925575\n",
      "Iteration 194, loss = 0.35584856\n",
      "Iteration 277, loss = 0.36879784\n",
      "Iteration 285, loss = 0.34600547\n",
      "Iteration 269, loss = 0.35907807\n",
      "Iteration 34, loss = 0.44638554\n",
      "Iteration 12, loss = 0.65835670\n",
      "Iteration 158, loss = 0.35571779\n",
      "Iteration 286, loss = 0.34582283\n",
      "Iteration 270, loss = 0.35890041\n",
      "Iteration 35, loss = 0.44294877\n",
      "Iteration 278, loss = 0.36865161\n",
      "Iteration 195, loss = 0.35560091\n",
      "Iteration 271, loss = 0.35875164\n",
      "Iteration 287, loss = 0.34567897\n",
      "Iteration 36, loss = 0.43965338\n",
      "Iteration 13, loss = 0.65395971\n",
      "Iteration 196, loss = 0.35532333\n",
      "Iteration 288, loss = 0.34551462\n",
      "Iteration 279, loss = 0.36846205\n",
      "Iteration 272, loss = 0.35857183\n",
      "Iteration 159, loss = 0.35555542\n",
      "Iteration 37, loss = 0.43656919\n",
      "Iteration 197, loss = 0.35503613\n",
      "Iteration 289, loss = 0.34538195\n",
      "Iteration 14, loss = 0.64956144\n",
      "Iteration 198, loss = 0.35476630\n",
      "Iteration 290, loss = 0.34524632\n",
      "Iteration 280, loss = 0.36828491\n",
      "Iteration 160, loss = 0.35540473\n",
      "Iteration 15, loss = 0.64510969\n",
      "Iteration 199, loss = 0.35452001\n",
      "Iteration 291, loss = 0.34508618\n",
      "Iteration 281, loss = 0.36811416\n",
      "Iteration 273, loss = 0.35840337\n",
      "Iteration 16, loss = 0.64085510\n",
      "Iteration 38, loss = 0.43363835\n",
      "Iteration 292, loss = 0.34492614\n",
      "Iteration 274, loss = 0.35826765\n",
      "Iteration 17, loss = 0.63638354\n",
      "Iteration 293, loss = 0.34478526\n",
      "Iteration 161, loss = 0.35522712\n",
      "Iteration 200, loss = 0.35426993\n",
      "Iteration 282, loss = 0.36792216\n",
      "Iteration 18, loss = 0.63207822\n",
      "Iteration 39, loss = 0.43068288\n",
      "Iteration 294, loss = 0.34465071\n",
      "Iteration 283, loss = 0.36773874\n",
      "Iteration 201, loss = 0.35398212\n",
      "Iteration 295, loss = 0.34449697\n",
      "Iteration 19, loss = 0.62800973\n",
      "Iteration 275, loss = 0.35809024\n",
      "Iteration 284, loss = 0.36759230\n",
      "Iteration 162, loss = 0.35507899\n",
      "Iteration 20, loss = 0.62386615\n",
      "Iteration 296, loss = 0.34434948\n",
      "Iteration 40, loss = 0.42806085\n",
      "Iteration 285, loss = 0.36742194\n",
      "Iteration 276, loss = 0.35791832\n",
      "Iteration 21, loss = 0.61975883\n",
      "Iteration 297, loss = 0.34422514\n",
      "Iteration 286, loss = 0.36721447\n",
      "Iteration 202, loss = 0.35373442\n",
      "Iteration 22, loss = 0.61562970\n",
      "Iteration 298, loss = 0.34406957\n",
      "Iteration 163, loss = 0.35494896\n",
      "Iteration 287, loss = 0.36705108\n",
      "Iteration 277, loss = 0.35775099\n",
      "Iteration 23, loss = 0.61164606\n",
      "Iteration 41, loss = 0.42543749\n",
      "Iteration 203, loss = 0.35348450\n",
      "Iteration 299, loss = 0.34393854\n",
      "Iteration 164, loss = 0.35479380\n",
      "Iteration 278, loss = 0.35769054\n",
      "Iteration 288, loss = 0.36686214\n",
      "Iteration 300, loss = 0.34381671\n",
      "Iteration 42, loss = 0.42290818\n",
      "Iteration 165, loss = 0.35462604\n",
      "Iteration 24, loss = 0.60757658\n",
      "Iteration 204, loss = 0.35322888\n",
      "Iteration 301, loss = 0.34365952\n",
      "Iteration 43, loss = 0.42052704\n",
      "Iteration 279, loss = 0.35745143\n",
      "Iteration 166, loss = 0.35449410\n",
      "Iteration 302, loss = 0.34350969\n",
      "Iteration 205, loss = 0.35299456\n",
      "Iteration 289, loss = 0.36671252\n",
      "Iteration 25, loss = 0.60358840\n",
      "Iteration 303, loss = 0.34338714\n",
      "Iteration 167, loss = 0.35435771\n",
      "Iteration 280, loss = 0.35729839\n",
      "Iteration 206, loss = 0.35273742\n",
      "Iteration 44, loss = 0.41826550\n",
      "Iteration 304, loss = 0.34325050\n",
      "Iteration 168, loss = 0.35423126\n",
      "Iteration 281, loss = 0.35714301\n",
      "Iteration 305, loss = 0.34312068\n",
      "Iteration 207, loss = 0.35250024\n",
      "Iteration 290, loss = 0.36655958\n",
      "Iteration 26, loss = 0.59968387\n",
      "Iteration 282, loss = 0.35699495\n",
      "Iteration 45, loss = 0.41597527\n",
      "Iteration 169, loss = 0.35406775\n",
      "Iteration 208, loss = 0.35226420\n",
      "Iteration 306, loss = 0.34296835\n",
      "Iteration 283, loss = 0.35683632\n",
      "Iteration 209, loss = 0.35203368\n",
      "Iteration 307, loss = 0.34282966\n",
      "Iteration 27, loss = 0.59576750\n",
      "Iteration 46, loss = 0.41384904\n",
      "Iteration 284, loss = 0.35669678\n",
      "Iteration 308, loss = 0.34270242\n",
      "Iteration 210, loss = 0.35178066\n",
      "Iteration 170, loss = 0.35393356\n",
      "Iteration 291, loss = 0.36637856\n",
      "Iteration 309, loss = 0.34256825\n",
      "Iteration 285, loss = 0.35653360\n",
      "Iteration 211, loss = 0.35154976\n",
      "Iteration 171, loss = 0.35382523\n",
      "Iteration 310, loss = 0.34242943\n",
      "Iteration 28, loss = 0.59184657\n",
      "Iteration 47, loss = 0.41177122\n",
      "Iteration 212, loss = 0.35132240\n",
      "Iteration 172, loss = 0.35366415\n",
      "Iteration 292, loss = 0.36619353\n",
      "Iteration 311, loss = 0.34234272\n",
      "Iteration 286, loss = 0.35637687\n",
      "Iteration 29, loss = 0.58804091\n",
      "Iteration 48, loss = 0.40981340\n",
      "Iteration 312, loss = 0.34217879\n",
      "Iteration 213, loss = 0.35111363\n",
      "Iteration 173, loss = 0.35354658\n",
      "Iteration 30, loss = 0.58426300\n",
      "Iteration 287, loss = 0.35622834\n",
      "Iteration 214, loss = 0.35088374\n",
      "Iteration 293, loss = 0.36604392\n",
      "Iteration 313, loss = 0.34206350\n",
      "Iteration 174, loss = 0.35344098\n",
      "Iteration 49, loss = 0.40789999\n",
      "Iteration 294, loss = 0.36586996\n",
      "Iteration 175, loss = 0.35330581\n",
      "Iteration 314, loss = 0.34192834\n",
      "Iteration 31, loss = 0.58050596\n",
      "Iteration 288, loss = 0.35606638\n",
      "Iteration 295, loss = 0.36569266\n",
      "Iteration 215, loss = 0.35064543\n",
      "Iteration 176, loss = 0.35317191\n",
      "Iteration 315, loss = 0.34177538\n",
      "Iteration 296, loss = 0.36554087\n",
      "Iteration 32, loss = 0.57668441\n",
      "Iteration 177, loss = 0.35305284\n",
      "Iteration 289, loss = 0.35594003\n",
      "Iteration 50, loss = 0.40616611\n",
      "Iteration 216, loss = 0.35045695\n",
      "Iteration 178, loss = 0.35294697\n",
      "Iteration 316, loss = 0.34164930\n",
      "Iteration 33, loss = 0.57291773\n",
      "Iteration 297, loss = 0.36537276\n",
      "Iteration 179, loss = 0.35279794\n",
      "Iteration 290, loss = 0.35580328\n",
      "Iteration 34, loss = 0.56924269\n",
      "Iteration 317, loss = 0.34154592\n",
      "Iteration 180, loss = 0.35269588\n",
      "Iteration 217, loss = 0.35021027\n",
      "Iteration 291, loss = 0.35564677\n",
      "Iteration 298, loss = 0.36520208\n",
      "Iteration 35, loss = 0.56552843\n",
      "Iteration 51, loss = 0.40434197\n",
      "Iteration 318, loss = 0.34142801\n",
      "Iteration 181, loss = 0.35256369\n",
      "Iteration 36, loss = 0.56180709\n",
      "Iteration 292, loss = 0.35549320\n",
      "Iteration 218, loss = 0.34999907\n",
      "Iteration 299, loss = 0.36506207\n",
      "Iteration 37, loss = 0.55820897\n",
      "Iteration 52, loss = 0.40260464\n",
      "Iteration 182, loss = 0.35248114\n",
      "Iteration 319, loss = 0.34128459\n",
      "Iteration 293, loss = 0.35535015\n",
      "Iteration 300, loss = 0.36490551\n",
      "Iteration 38, loss = 0.55466960\n",
      "Iteration 294, loss = 0.35521818\n",
      "Iteration 320, loss = 0.34113829\n",
      "Iteration 183, loss = 0.35237303\n",
      "Iteration 301, loss = 0.36473561\n",
      "Iteration 219, loss = 0.34980219\n",
      "Iteration 53, loss = 0.40092579\n",
      "Iteration 184, loss = 0.35223293\n",
      "Iteration 302, loss = 0.36456628\n",
      "Iteration 295, loss = 0.35505013\n",
      "Iteration 39, loss = 0.55101232\n",
      "Iteration 220, loss = 0.34958623\n",
      "Iteration 185, loss = 0.35213898\n",
      "Iteration 321, loss = 0.34101608\n",
      "Iteration 296, loss = 0.35490998\n",
      "Iteration 54, loss = 0.39931585\n",
      "Iteration 186, loss = 0.35203372\n",
      "Iteration 322, loss = 0.34090337\n",
      "Iteration 297, loss = 0.35478918\n",
      "Iteration 221, loss = 0.34936763\n",
      "Iteration 187, loss = 0.35191166\n",
      "Iteration 323, loss = 0.34077705\n",
      "Iteration 303, loss = 0.36440904\n",
      "Iteration 298, loss = 0.35463062\n",
      "Iteration 40, loss = 0.54752395\n",
      "Iteration 55, loss = 0.39779325\n",
      "Iteration 299, loss = 0.35451349\n",
      "Iteration 304, loss = 0.36424728\n",
      "Iteration 222, loss = 0.34915380\n",
      "Iteration 324, loss = 0.34065260\n",
      "Iteration 300, loss = 0.35436443\n",
      "Iteration 188, loss = 0.35180676\n",
      "Iteration 41, loss = 0.54399051\n",
      "Iteration 223, loss = 0.34896120\n",
      "Iteration 325, loss = 0.34052513\n",
      "Iteration 301, loss = 0.35422986\n",
      "Iteration 56, loss = 0.39632739\n",
      "Iteration 305, loss = 0.36409215\n",
      "Iteration 224, loss = 0.34873846\n",
      "Iteration 302, loss = 0.35407784\n",
      "Iteration 42, loss = 0.54043788\n",
      "Iteration 189, loss = 0.35171033\n",
      "Iteration 303, loss = 0.35395075\n",
      "Iteration 326, loss = 0.34039743\n",
      "Iteration 57, loss = 0.39486682\n",
      "Iteration 306, loss = 0.36393306\n",
      "Iteration 225, loss = 0.34854123Iteration 43, loss = 0.53699050\n",
      "\n",
      "Iteration 304, loss = 0.35380566\n",
      "Iteration 190, loss = 0.35159954\n",
      "Iteration 327, loss = 0.34028177\n",
      "Iteration 44, loss = 0.53361210\n",
      "Iteration 305, loss = 0.35367052\n",
      "Iteration 58, loss = 0.39339581\n",
      "Iteration 191, loss = 0.35150715\n",
      "Iteration 226, loss = 0.34833618\n",
      "Iteration 307, loss = 0.36378305\n",
      "Iteration 328, loss = 0.34022082\n",
      "Iteration 45, loss = 0.53013430\n",
      "Iteration 227, loss = 0.34813983\n",
      "Iteration 192, loss = 0.35139869\n",
      "Iteration 306, loss = 0.35353552\n",
      "Iteration 308, loss = 0.36360340\n",
      "Iteration 228, loss = 0.34794484\n",
      "Iteration 193, loss = 0.35131350\n",
      "Iteration 46, loss = 0.52677690\n",
      "Iteration 329, loss = 0.34002970\n",
      "Iteration 309, loss = 0.36348795\n",
      "Iteration 194, loss = 0.35119977\n",
      "Iteration 47, loss = 0.52349384\n",
      "Iteration 59, loss = 0.39209091\n",
      "Iteration 307, loss = 0.35339574\n",
      "Iteration 330, loss = 0.33992079\n",
      "Iteration 195, loss = 0.35112688\n",
      "Iteration 310, loss = 0.36330012\n",
      "Iteration 48, loss = 0.52019899\n",
      "Iteration 60, loss = 0.39074836\n",
      "Iteration 196, loss = 0.35103119\n",
      "Iteration 229, loss = 0.34774314\n",
      "Iteration 308, loss = 0.35324985\n",
      "Iteration 331, loss = 0.33980107\n",
      "Iteration 49, loss = 0.51689167\n",
      "Iteration 311, loss = 0.36316769\n",
      "Iteration 197, loss = 0.35091489\n",
      "Iteration 50, loss = 0.51378013\n",
      "Iteration 309, loss = 0.35313547\n",
      "Iteration 198, loss = 0.35082301\n",
      "Iteration 61, loss = 0.38951458\n",
      "Iteration 51, loss = 0.51055312\n",
      "Iteration 332, loss = 0.33969264\n",
      "Iteration 312, loss = 0.36302525\n",
      "Iteration 230, loss = 0.34756654\n",
      "Iteration 62, loss = 0.38820016\n",
      "Iteration 52, loss = 0.50735105\n",
      "Iteration 333, loss = 0.33955558\n",
      "Iteration 199, loss = 0.35073661\n",
      "Iteration 310, loss = 0.35299215\n",
      "Iteration 53, loss = 0.50421680\n",
      "Iteration 334, loss = 0.33945025\n",
      "Iteration 311, loss = 0.35287821\n",
      "Iteration 63, loss = 0.38699042\n",
      "Iteration 200, loss = 0.35066587\n",
      "Iteration 54, loss = 0.50114840\n",
      "Iteration 313, loss = 0.36285113\n",
      "Iteration 335, loss = 0.33932378\n",
      "Iteration 231, loss = 0.34735749\n",
      "Iteration 64, loss = 0.38581947\n",
      "Iteration 312, loss = 0.35276899\n",
      "Iteration 336, loss = 0.33922146\n",
      "Iteration 201, loss = 0.35055477\n",
      "Iteration 65, loss = 0.38473130\n",
      "Iteration 314, loss = 0.36269459\n",
      "Iteration 337, loss = 0.33910532\n",
      "Iteration 313, loss = 0.35261484\n",
      "Iteration 55, loss = 0.49814873\n",
      "Iteration 66, loss = 0.38360979\n",
      "Iteration 338, loss = 0.33900529\n",
      "Iteration 314, loss = 0.35248285\n",
      "Iteration 232, loss = 0.34718388\n",
      "Iteration 202, loss = 0.35047251\n",
      "Iteration 56, loss = 0.49512596\n",
      "Iteration 67, loss = 0.38256715\n",
      "Iteration 233, loss = 0.34699141\n",
      "Iteration 315, loss = 0.35234084\n",
      "Iteration 57, loss = 0.49213738\n",
      "Iteration 315, loss = 0.36253207\n",
      "Iteration 339, loss = 0.33886095\n",
      "Iteration 68, loss = 0.38146174\n",
      "Iteration 234, loss = 0.34680886\n",
      "Iteration 203, loss = 0.35038269\n",
      "Iteration 58, loss = 0.48918851\n",
      "Iteration 316, loss = 0.35221209\n",
      "Iteration 316, loss = 0.36238838\n",
      "Iteration 235, loss = 0.34660739\n",
      "Iteration 204, loss = 0.35029386\n",
      "Iteration 69, loss = 0.38047714\n",
      "Iteration 340, loss = 0.33877116\n",
      "Iteration 317, loss = 0.35211780\n",
      "Iteration 59, loss = 0.48638698\n",
      "Iteration 205, loss = 0.35021841\n",
      "Iteration 317, loss = 0.36226873\n",
      "Iteration 341, loss = 0.33866055\n",
      "Iteration 236, loss = 0.34643403\n",
      "Iteration 60, loss = 0.48347874\n",
      "Iteration 70, loss = 0.37946509\n",
      "Iteration 318, loss = 0.35196506\n",
      "Iteration 206, loss = 0.35012238\n",
      "Iteration 71, loss = 0.37845601\n",
      "Iteration 61, loss = 0.48075766\n",
      "Iteration 319, loss = 0.35185698\n",
      "Iteration 342, loss = 0.33852448\n",
      "Iteration 207, loss = 0.35004748\n",
      "Iteration 72, loss = 0.37753996\n",
      "Iteration 62, loss = 0.47792755\n",
      "Iteration 320, loss = 0.35172451\n",
      "Iteration 318, loss = 0.36208818\n",
      "Iteration 237, loss = 0.34624660\n",
      "Iteration 73, loss = 0.37659007\n",
      "Iteration 208, loss = 0.34996176\n",
      "Iteration 343, loss = 0.33842694\n",
      "Iteration 319, loss = 0.36193361\n",
      "Iteration 63, loss = 0.47514730\n",
      "Iteration 238, loss = 0.34607502\n",
      "Iteration 74, loss = 0.37567940\n",
      "Iteration 321, loss = 0.35158999\n",
      "Iteration 344, loss = 0.33832532\n",
      "Iteration 209, loss = 0.34989036\n",
      "Iteration 75, loss = 0.37487037\n",
      "Iteration 320, loss = 0.36178836\n",
      "Iteration 239, loss = 0.34590088\n",
      "Iteration 64, loss = 0.47253262\n",
      "Iteration 322, loss = 0.35145603\n",
      "Iteration 345, loss = 0.33821423\n",
      "Iteration 210, loss = 0.34978877\n",
      "Iteration 76, loss = 0.37398825\n",
      "Iteration 321, loss = 0.36164794\n",
      "Iteration 323, loss = 0.35133248\n",
      "Iteration 65, loss = 0.46986967\n",
      "Iteration 211, loss = 0.34971957\n",
      "Iteration 240, loss = 0.34572053\n",
      "Iteration 346, loss = 0.33808938Iteration 324, loss = 0.35123837\n",
      "\n",
      "Iteration 212, loss = 0.34964277\n",
      "Iteration 322, loss = 0.36148350\n",
      "Iteration 66, loss = 0.46730425\n",
      "Iteration 241, loss = 0.34555982\n",
      "Iteration 325, loss = 0.35110288\n",
      "Iteration 77, loss = 0.37312049\n",
      "Iteration 347, loss = 0.33797137\n",
      "Iteration 213, loss = 0.34957323\n",
      "Iteration 67, loss = 0.46482320\n",
      "Iteration 323, loss = 0.36133759\n",
      "Iteration 214, loss = 0.34949843\n",
      "Iteration 78, loss = 0.37228141\n",
      "Iteration 326, loss = 0.35097135\n",
      "Iteration 348, loss = 0.33785076\n",
      "Iteration 242, loss = 0.34538099\n",
      "Iteration 68, loss = 0.46228985\n",
      "Iteration 215, loss = 0.34941225\n",
      "Iteration 324, loss = 0.36119552\n",
      "Iteration 243, loss = 0.34520983\n",
      "Iteration 69, loss = 0.45985526\n",
      "Iteration 79, loss = 0.37151017\n",
      "Iteration 327, loss = 0.35085266\n",
      "Iteration 216, loss = 0.34936482\n",
      "Iteration 349, loss = 0.33774346\n",
      "Iteration 244, loss = 0.34503350\n",
      "Iteration 328, loss = 0.35078042\n",
      "Iteration 80, loss = 0.37075123\n",
      "Iteration 325, loss = 0.36107930\n",
      "Iteration 217, loss = 0.34926518\n",
      "Iteration 245, loss = 0.34483738\n",
      "Iteration 70, loss = 0.45738848\n",
      "Iteration 350, loss = 0.33763778\n",
      "Iteration 81, loss = 0.37000332\n",
      "Iteration 326, loss = 0.36089638\n",
      "Iteration 329, loss = 0.35060870\n",
      "Iteration 246, loss = 0.34468415\n",
      "Iteration 218, loss = 0.34920187\n",
      "Iteration 351, loss = 0.33753794\n",
      "Iteration 71, loss = 0.45506946\n",
      "Iteration 82, loss = 0.36924492\n",
      "Iteration 327, loss = 0.36076465\n",
      "Iteration 219, loss = 0.34914268\n",
      "Iteration 330, loss = 0.35050357\n",
      "Iteration 247, loss = 0.34452669\n",
      "Iteration 352, loss = 0.33744106\n",
      "Iteration 328, loss = 0.36064412\n",
      "Iteration 331, loss = 0.35037117\n",
      "Iteration 220, loss = 0.34906604\n",
      "Iteration 248, loss = 0.34437141\n",
      "Iteration 72, loss = 0.45275011\n",
      "Iteration 353, loss = 0.33732535\n",
      "Iteration 332, loss = 0.35026218\n",
      "Iteration 83, loss = 0.36853777\n",
      "Iteration 329, loss = 0.36047170\n",
      "Iteration 221, loss = 0.34898617\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 354, loss = 0.33720837\n",
      "Iteration 333, loss = 0.35013744\n",
      "Iteration 330, loss = 0.36033789\n",
      "Iteration 249, loss = 0.34418609\n",
      "Iteration 334, loss = 0.35003321\n",
      "Iteration 355, loss = 0.33711167\n",
      "Iteration 73, loss = 0.45047421\n",
      "Iteration 250, loss = 0.34403764\n",
      "Iteration 331, loss = 0.36020923\n",
      "Iteration 335, loss = 0.34989840\n",
      "Iteration 84, loss = 0.36782649\n",
      "Iteration 356, loss = 0.33700735\n",
      "Iteration 74, loss = 0.44819820\n",
      "Iteration 332, loss = 0.36004363\n",
      "Iteration 85, loss = 0.36709436\n",
      "Iteration 251, loss = 0.34386446\n",
      "Iteration 336, loss = 0.34979598\n",
      "Iteration 333, loss = 0.35993119\n",
      "Iteration 75, loss = 0.44606210\n",
      "Iteration 337, loss = 0.34970623\n",
      "Iteration 86, loss = 0.36639055\n",
      "Iteration 334, loss = 0.35978697\n",
      "Iteration 357, loss = 0.33688779\n",
      "Iteration 338, loss = 0.34956545\n",
      "Iteration 335, loss = 0.35962447\n",
      "Iteration 87, loss = 0.36577602\n",
      "Iteration 358, loss = 0.33678670\n",
      "Iteration 76, loss = 0.44387906\n",
      "Iteration 339, loss = 0.34944307\n",
      "Iteration 252, loss = 0.34368783\n",
      "Iteration 336, loss = 0.35948487\n",
      "Iteration 88, loss = 0.36512187\n",
      "Iteration 340, loss = 0.34934933\n",
      "Iteration 359, loss = 0.33667902\n",
      "Iteration 77, loss = 0.44174237\n",
      "Iteration 89, loss = 0.36448744\n",
      "Iteration 253, loss = 0.34354084\n",
      "Iteration 337, loss = 0.35936215\n",
      "Iteration 360, loss = 0.33657348\n",
      "Iteration 341, loss = 0.34923645\n",
      "Iteration 78, loss = 0.43964186\n",
      "Iteration 254, loss = 0.34340308\n",
      "Iteration 90, loss = 0.36385513\n",
      "Iteration 338, loss = 0.35921586\n",
      "Iteration 255, loss = 0.34322818\n",
      "Iteration 91, loss = 0.36325466\n",
      "Iteration 342, loss = 0.34910159\n",
      "Iteration 339, loss = 0.35907473\n",
      "Iteration 79, loss = 0.43761496\n",
      "Iteration 256, loss = 0.34304600\n",
      "Iteration 361, loss = 0.33648321\n",
      "Iteration 257, loss = 0.34290472\n",
      "Iteration 340, loss = 0.35892052\n",
      "Iteration 343, loss = 0.34900033\n",
      "Iteration 362, loss = 0.33636518\n",
      "Iteration 258, loss = 0.34275760\n",
      "Iteration 92, loss = 0.36262396\n",
      "Iteration 80, loss = 0.43561524\n",
      "Iteration 341, loss = 0.35881356\n",
      "Iteration 259, loss = 0.34260268\n",
      "Iteration 363, loss = 0.33627792\n",
      "Iteration 344, loss = 0.34889777\n",
      "Iteration 342, loss = 0.35866581\n",
      "Iteration 260, loss = 0.34245231\n",
      "Iteration 81, loss = 0.43367701\n",
      "Iteration 93, loss = 0.36205960\n",
      "Iteration 364, loss = 0.33617198\n",
      "Iteration 343, loss = 0.35854061\n",
      "Iteration 82, loss = 0.43173311\n",
      "Iteration 261, loss = 0.34229841\n",
      "Iteration 365, loss = 0.33605853\n",
      "Iteration 344, loss = 0.35840008\n",
      "Iteration 345, loss = 0.34878318\n",
      "Iteration 83, loss = 0.42986634\n",
      "Iteration 262, loss = 0.34214833\n",
      "Iteration 366, loss = 0.33595232\n",
      "Iteration 345, loss = 0.35826684\n",
      "Iteration 84, loss = 0.42801672\n",
      "Iteration 94, loss = 0.36146795\n",
      "Iteration 367, loss = 0.33585884\n",
      "Iteration 346, loss = 0.34866223\n",
      "Iteration 263, loss = 0.34199796\n",
      "Iteration 347, loss = 0.34855935\n",
      "Iteration 85, loss = 0.42617377\n",
      "Iteration 95, loss = 0.36095422\n",
      "Iteration 264, loss = 0.34186014\n",
      "Iteration 346, loss = 0.35811597\n",
      "Iteration 368, loss = 0.33582102\n",
      "Iteration 348, loss = 0.34843723\n",
      "Iteration 96, loss = 0.36037564\n",
      "Iteration 265, loss = 0.34170969\n",
      "Iteration 349, loss = 0.34832158\n",
      "Iteration 86, loss = 0.42434458\n",
      "Iteration 97, loss = 0.35983429\n",
      "Iteration 347, loss = 0.35798139\n",
      "Iteration 266, loss = 0.34158447\n",
      "Iteration 350, loss = 0.34822194\n",
      "Iteration 87, loss = 0.42267747\n",
      "Iteration 98, loss = 0.35927052\n",
      "Iteration 351, loss = 0.34811933\n",
      "Iteration 267, loss = 0.34143282\n",
      "Iteration 88, loss = 0.42090324\n",
      "Iteration 99, loss = 0.35877252\n",
      "Iteration 352, loss = 0.34803926\n",
      "Iteration 268, loss = 0.34128971\n",
      "Iteration 89, loss = 0.41925152\n",
      "Iteration 100, loss = 0.35823796\n",
      "Iteration 369, loss = 0.33566000\n",
      "Iteration 348, loss = 0.35784542\n",
      "Iteration 269, loss = 0.34114334\n",
      "Iteration 353, loss = 0.34790409\n",
      "Iteration 90, loss = 0.41762563\n",
      "Iteration 349, loss = 0.35771419\n",
      "Iteration 370, loss = 0.33557773\n",
      "Iteration 354, loss = 0.34778317\n",
      "Iteration 350, loss = 0.35758183\n",
      "Iteration 101, loss = 0.35774246\n",
      "Iteration 371, loss = 0.33545194\n",
      "Iteration 270, loss = 0.34099717\n",
      "Iteration 355, loss = 0.34768487\n",
      "Iteration 91, loss = 0.41600816\n",
      "Iteration 356, loss = 0.34758786\n",
      "Iteration 102, loss = 0.35730668\n",
      "Iteration 271, loss = 0.34086133\n",
      "Iteration 372, loss = 0.33536638\n",
      "Iteration 92, loss = 0.41441552\n",
      "Iteration 351, loss = 0.35746230\n",
      "Iteration 357, loss = 0.34746439\n",
      "Iteration 272, loss = 0.34071477\n",
      "Iteration 103, loss = 0.35678885\n",
      "Iteration 352, loss = 0.35737464\n",
      "Iteration 93, loss = 0.41289123\n",
      "Iteration 373, loss = 0.33527298\n",
      "Iteration 358, loss = 0.34738607\n",
      "Iteration 273, loss = 0.34058432\n",
      "Iteration 94, loss = 0.41132534\n",
      "Iteration 353, loss = 0.35718537\n",
      "Iteration 359, loss = 0.34725521\n",
      "Iteration 104, loss = 0.35632596\n",
      "Iteration 274, loss = 0.34046802\n",
      "Iteration 374, loss = 0.33515852\n",
      "Iteration 360, loss = 0.34716385\n",
      "Iteration 95, loss = 0.40991768\n",
      "Iteration 275, loss = 0.34032143\n",
      "Iteration 354, loss = 0.35706391\n",
      "Iteration 375, loss = 0.33507451\n",
      "Iteration 96, loss = 0.40843454\n",
      "Iteration 276, loss = 0.34017356\n",
      "Iteration 361, loss = 0.34704249\n",
      "Iteration 105, loss = 0.35584364\n",
      "Iteration 355, loss = 0.35692661\n",
      "Iteration 376, loss = 0.33498701\n",
      "Iteration 97, loss = 0.40696699\n",
      "Iteration 362, loss = 0.34694736\n",
      "Iteration 106, loss = 0.35538702\n",
      "Iteration 377, loss = 0.33486951\n",
      "Iteration 277, loss = 0.34004443\n",
      "Iteration 356, loss = 0.35682030\n",
      "Iteration 98, loss = 0.40558215\n",
      "Iteration 107, loss = 0.35495798\n",
      "Iteration 363, loss = 0.34684680\n",
      "Iteration 278, loss = 0.33994614\n",
      "Iteration 357, loss = 0.35667386\n",
      "Iteration 378, loss = 0.33477194\n",
      "Iteration 364, loss = 0.34675784\n",
      "Iteration 99, loss = 0.40424838\n",
      "Iteration 108, loss = 0.35447653\n",
      "Iteration 279, loss = 0.33977741Iteration 358, loss = 0.35656334\n",
      "\n",
      "Iteration 100, loss = 0.40285212\n",
      "Iteration 109, loss = 0.35406095\n",
      "Iteration 359, loss = 0.35641799\n",
      "Iteration 280, loss = 0.33964434\n",
      "Iteration 365, loss = 0.34663602\n",
      "Iteration 101, loss = 0.40153787\n",
      "Iteration 360, loss = 0.35628432\n",
      "Iteration 379, loss = 0.33466594\n",
      "Iteration 110, loss = 0.35363648\n",
      "Iteration 281, loss = 0.33952092\n",
      "Iteration 366, loss = 0.34651400\n",
      "Iteration 102, loss = 0.40031248\n",
      "Iteration 111, loss = 0.35324307\n",
      "Iteration 361, loss = 0.35615642\n",
      "Iteration 380, loss = 0.33457236\n",
      "Iteration 367, loss = 0.34642209\n",
      "Iteration 282, loss = 0.33938107\n",
      "Iteration 103, loss = 0.39900338\n",
      "Iteration 362, loss = 0.35602713\n",
      "Iteration 368, loss = 0.34634939\n",
      "Iteration 283, loss = 0.33925831\n",
      "Iteration 104, loss = 0.39776397\n",
      "Iteration 112, loss = 0.35284510\n",
      "Iteration 381, loss = 0.33447301\n",
      "Iteration 363, loss = 0.35591199\n",
      "Iteration 105, loss = 0.39654227\n",
      "Iteration 369, loss = 0.34622888\n",
      "Iteration 284, loss = 0.33916236\n",
      "Iteration 382, loss = 0.33437899\n",
      "Iteration 106, loss = 0.39537753\n",
      "Iteration 370, loss = 0.34612531\n",
      "Iteration 364, loss = 0.35578204\n",
      "Iteration 285, loss = 0.33900806\n",
      "Iteration 113, loss = 0.35242120\n",
      "Iteration 383, loss = 0.33428490\n",
      "Iteration 107, loss = 0.39419892\n",
      "Iteration 365, loss = 0.35564883\n",
      "Iteration 371, loss = 0.34602414\n",
      "Iteration 286, loss = 0.33886625\n",
      "Iteration 114, loss = 0.35202658\n",
      "Iteration 384, loss = 0.33421785\n",
      "Iteration 372, loss = 0.34592563\n",
      "Iteration 287, loss = 0.33874313\n",
      "Iteration 366, loss = 0.35550250\n",
      "Iteration 108, loss = 0.39301821\n",
      "Iteration 115, loss = 0.35161359\n",
      "Iteration 288, loss = 0.33861268\n",
      "Iteration 367, loss = 0.35538641\n",
      "Iteration 385, loss = 0.33408947\n",
      "Iteration 373, loss = 0.34582040\n",
      "Iteration 116, loss = 0.35127261\n",
      "Iteration 109, loss = 0.39192524\n",
      "Iteration 289, loss = 0.33850279\n",
      "Iteration 368, loss = 0.35527802\n",
      "Iteration 374, loss = 0.34571552\n",
      "Iteration 117, loss = 0.35084496\n",
      "Iteration 386, loss = 0.33399259\n",
      "Iteration 110, loss = 0.39079712\n",
      "Iteration 290, loss = 0.33837262\n",
      "Iteration 375, loss = 0.34562921\n",
      "Iteration 118, loss = 0.35053729\n",
      "Iteration 369, loss = 0.35515382\n",
      "Iteration 387, loss = 0.33389695\n",
      "Iteration 111, loss = 0.38977827\n",
      "Iteration 291, loss = 0.33825650\n",
      "Iteration 119, loss = 0.35015982\n",
      "Iteration 388, loss = 0.33380801\n",
      "Iteration 370, loss = 0.35501526\n",
      "Iteration 292, loss = 0.33811922\n",
      "Iteration 376, loss = 0.34551234\n",
      "Iteration 389, loss = 0.33372052\n",
      "Iteration 120, loss = 0.34978380\n",
      "Iteration 377, loss = 0.34542462\n",
      "Iteration 293, loss = 0.33800180\n",
      "Iteration 112, loss = 0.38870818\n",
      "Iteration 371, loss = 0.35490054\n",
      "Iteration 390, loss = 0.33362885\n",
      "Iteration 378, loss = 0.34532121\n",
      "Iteration 113, loss = 0.38766224\n",
      "Iteration 121, loss = 0.34943485\n",
      "Iteration 372, loss = 0.35477986\n",
      "Iteration 294, loss = 0.33792668\n",
      "Iteration 391, loss = 0.33351286\n",
      "Iteration 373, loss = 0.35465175\n",
      "Iteration 114, loss = 0.38664672\n",
      "Iteration 295, loss = 0.33776457\n",
      "Iteration 379, loss = 0.34521283\n",
      "Iteration 122, loss = 0.34907559\n",
      "Iteration 392, loss = 0.33347169\n",
      "Iteration 115, loss = 0.38562870\n",
      "Iteration 296, loss = 0.33764324\n",
      "Iteration 374, loss = 0.35451847\n",
      "Iteration 380, loss = 0.34512673\n",
      "Iteration 123, loss = 0.34873618\n",
      "Iteration 116, loss = 0.38466863\n",
      "Iteration 393, loss = 0.33333943\n",
      "Iteration 375, loss = 0.35439806\n",
      "Iteration 297, loss = 0.33753920\n",
      "Iteration 381, loss = 0.34503219\n",
      "Iteration 117, loss = 0.38368565\n",
      "Iteration 124, loss = 0.34842860\n",
      "Iteration 394, loss = 0.33323937\n",
      "Iteration 376, loss = 0.35427582\n",
      "Iteration 298, loss = 0.33741060\n",
      "Iteration 382, loss = 0.34492338\n",
      "Iteration 118, loss = 0.38282921\n",
      "Iteration 377, loss = 0.35418002\n",
      "Iteration 125, loss = 0.34811269\n",
      "Iteration 299, loss = 0.33729377\n",
      "Iteration 395, loss = 0.33314361\n",
      "Iteration 383, loss = 0.34483530\n",
      "Iteration 378, loss = 0.35403857\n",
      "Iteration 119, loss = 0.38184903\n",
      "Iteration 126, loss = 0.34777519\n",
      "Iteration 300, loss = 0.33717639\n",
      "Iteration 379, loss = 0.35390384\n",
      "Iteration 396, loss = 0.33305442\n",
      "Iteration 384, loss = 0.34475301\n",
      "Iteration 120, loss = 0.38093477\n",
      "Iteration 301, loss = 0.33706567\n",
      "Iteration 380, loss = 0.35378976\n",
      "Iteration 127, loss = 0.34742124\n",
      "Iteration 397, loss = 0.33295858\n",
      "Iteration 121, loss = 0.38005350Iteration 381, loss = 0.35367925\n",
      "\n",
      "Iteration 302, loss = 0.33694797\n",
      "Iteration 385, loss = 0.34465690\n",
      "Iteration 398, loss = 0.33287756\n",
      "Iteration 382, loss = 0.35355438\n",
      "Iteration 128, loss = 0.34711453\n",
      "Iteration 386, loss = 0.34453690\n",
      "Iteration 122, loss = 0.37917590\n",
      "Iteration 303, loss = 0.33683354\n",
      "Iteration 399, loss = 0.33279172\n",
      "Iteration 129, loss = 0.34681410\n",
      "Iteration 383, loss = 0.35343298\n",
      "Iteration 123, loss = 0.37830184\n",
      "Iteration 387, loss = 0.34445149\n",
      "Iteration 304, loss = 0.33671982\n",
      "Iteration 130, loss = 0.34654164\n",
      "Iteration 384, loss = 0.35331860\n",
      "Iteration 305, loss = 0.33659895\n",
      "Iteration 400, loss = 0.33269946\n",
      "Iteration 388, loss = 0.34438348\n",
      "Iteration 124, loss = 0.37752170\n",
      "Iteration 385, loss = 0.35317771\n",
      "Iteration 131, loss = 0.34619123\n",
      "Iteration 306, loss = 0.33648758\n",
      "Iteration 401, loss = 0.33259254\n",
      "Iteration 389, loss = 0.34425187\n",
      "Iteration 386, loss = 0.35308453\n",
      "Iteration 125, loss = 0.37666775\n",
      "Iteration 132, loss = 0.34592602\n",
      "Iteration 307, loss = 0.33637879\n",
      "Iteration 387, loss = 0.35295189\n",
      "Iteration 390, loss = 0.34415857\n",
      "Iteration 126, loss = 0.37589779\n",
      "Iteration 133, loss = 0.34563612\n",
      "Iteration 308, loss = 0.33625390\n",
      "Iteration 402, loss = 0.33251525\n",
      "Iteration 388, loss = 0.35284489\n",
      "Iteration 391, loss = 0.34406174\n",
      "Iteration 309, loss = 0.33615161\n",
      "Iteration 127, loss = 0.37503906\n",
      "Iteration 389, loss = 0.35269969\n",
      "Iteration 403, loss = 0.33242286\n",
      "Iteration 134, loss = 0.34533882\n",
      "Iteration 310, loss = 0.33603123\n",
      "Iteration 392, loss = 0.34404449\n",
      "Iteration 404, loss = 0.33232864\n",
      "Iteration 128, loss = 0.37428158\n",
      "Iteration 390, loss = 0.35258438\n",
      "Iteration 311, loss = 0.33600094\n",
      "Iteration 135, loss = 0.34506802\n",
      "Iteration 405, loss = 0.33225633\n",
      "Iteration 129, loss = 0.37349752\n",
      "Iteration 393, loss = 0.34387359\n",
      "Iteration 391, loss = 0.35248065\n",
      "Iteration 312, loss = 0.33581136\n",
      "Iteration 406, loss = 0.33215892\n",
      "Iteration 136, loss = 0.34478390\n",
      "Iteration 313, loss = 0.33571546\n",
      "Iteration 394, loss = 0.34378360\n",
      "Iteration 392, loss = 0.35243563\n",
      "Iteration 130, loss = 0.37277935\n",
      "Iteration 407, loss = 0.33206855\n",
      "Iteration 314, loss = 0.33560055\n",
      "Iteration 137, loss = 0.34449993\n",
      "Iteration 131, loss = 0.37199655\n",
      "Iteration 393, loss = 0.35223541\n",
      "Iteration 315, loss = 0.33548749\n",
      "Iteration 408, loss = 0.33197002\n",
      "Iteration 395, loss = 0.34368872\n",
      "Iteration 138, loss = 0.34423847\n",
      "Iteration 132, loss = 0.37128883\n",
      "Iteration 316, loss = 0.33537898\n",
      "Iteration 396, loss = 0.34361040\n",
      "Iteration 394, loss = 0.35212315\n",
      "Iteration 139, loss = 0.34396468\n",
      "Iteration 409, loss = 0.33187912\n",
      "Iteration 133, loss = 0.37056721\n",
      "Iteration 140, loss = 0.34372433\n",
      "Iteration 397, loss = 0.34349531\n",
      "Iteration 134, loss = 0.36986533\n",
      "Iteration 317, loss = 0.33528170\n",
      "Iteration 395, loss = 0.35199435\n",
      "Iteration 410, loss = 0.33179859\n",
      "Iteration 135, loss = 0.36919183\n",
      "Iteration 318, loss = 0.33517692\n",
      "Iteration 396, loss = 0.35188929\n",
      "Iteration 411, loss = 0.33171596\n",
      "Iteration 141, loss = 0.34346338\n",
      "Iteration 398, loss = 0.34341492\n",
      "Iteration 319, loss = 0.33508439\n",
      "Iteration 397, loss = 0.35176793\n",
      "Iteration 399, loss = 0.34332348\n",
      "Iteration 412, loss = 0.33161738\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 136, loss = 0.36847671\n",
      "Iteration 398, loss = 0.35165198\n",
      "Iteration 137, loss = 0.36780401\n",
      "Iteration 400, loss = 0.34322795\n",
      "Iteration 320, loss = 0.33496750\n",
      "Iteration 142, loss = 0.34319952\n",
      "Iteration 401, loss = 0.34313503\n",
      "Iteration 399, loss = 0.35154668\n",
      "Iteration 138, loss = 0.36714975\n",
      "Iteration 321, loss = 0.33487183\n",
      "Iteration 322, loss = 0.33475457Iteration 139, loss = 0.36648355\n",
      "\n",
      "Iteration 400, loss = 0.35142841\n",
      "Iteration 402, loss = 0.34304087\n",
      "Iteration 143, loss = 0.34294438\n",
      "Iteration 323, loss = 0.33464069\n",
      "Iteration 140, loss = 0.36586157\n",
      "Iteration 401, loss = 0.35130975\n",
      "Iteration 144, loss = 0.34269053\n",
      "Iteration 141, loss = 0.36525500\n",
      "Iteration 403, loss = 0.34295362\n",
      "Iteration 324, loss = 0.33456110\n",
      "Iteration 142, loss = 0.36458164\n",
      "Iteration 402, loss = 0.35120974\n",
      "Iteration 404, loss = 0.34287185\n",
      "Iteration 145, loss = 0.34244805\n",
      "Iteration 325, loss = 0.33442691\n",
      "Iteration 143, loss = 0.36399077\n",
      "Iteration 405, loss = 0.34276491\n",
      "Iteration 403, loss = 0.35108175\n",
      "Iteration 326, loss = 0.33433180\n",
      "Iteration 144, loss = 0.36337405\n",
      "Iteration 404, loss = 0.35098351\n",
      "Iteration 327, loss = 0.33421933\n",
      "Iteration 146, loss = 0.34222003\n",
      "Iteration 406, loss = 0.34269358\n",
      "Iteration 328, loss = 0.33412802\n",
      "Iteration 405, loss = 0.35085737\n",
      "Iteration 147, loss = 0.34198116\n",
      "Iteration 329, loss = 0.33400897\n",
      "Iteration 407, loss = 0.34259419\n",
      "Iteration 145, loss = 0.36278896\n",
      "Iteration 406, loss = 0.35073587\n",
      "Iteration 148, loss = 0.34172722\n",
      "Iteration 146, loss = 0.36222064\n",
      "Iteration 408, loss = 0.34250673\n",
      "Iteration 330, loss = 0.33392412\n",
      "Iteration 407, loss = 0.35065466\n",
      "Iteration 149, loss = 0.34151854\n",
      "Iteration 147, loss = 0.36164036\n",
      "Iteration 409, loss = 0.34240365\n",
      "Iteration 408, loss = 0.35055473\n",
      "Iteration 148, loss = 0.36104696\n",
      "Iteration 331, loss = 0.33381291\n",
      "Iteration 150, loss = 0.34128831\n",
      "Iteration 410, loss = 0.34232147\n",
      "Iteration 149, loss = 0.36051965\n",
      "Iteration 332, loss = 0.33372229\n",
      "Iteration 409, loss = 0.35039794\n",
      "Iteration 150, loss = 0.35996517\n",
      "Iteration 411, loss = 0.34222937\n",
      "Iteration 151, loss = 0.35941101\n",
      "Iteration 410, loss = 0.35029794\n",
      "Iteration 151, loss = 0.34105409\n",
      "Iteration 333, loss = 0.33360947\n",
      "Iteration 412, loss = 0.34213628\n",
      "Iteration 152, loss = 0.35889629\n",
      "Iteration 334, loss = 0.33350626\n",
      "Iteration 413, loss = 0.34204444\n",
      "Iteration 152, loss = 0.34085571\n",
      "Iteration 411, loss = 0.35020338\n",
      "Iteration 335, loss = 0.33340296\n",
      "Iteration 414, loss = 0.34196520\n",
      "Iteration 336, loss = 0.33330853\n",
      "Iteration 153, loss = 0.35835153\n",
      "Iteration 153, loss = 0.34063202\n",
      "Iteration 412, loss = 0.35008723\n",
      "Iteration 337, loss = 0.33320443\n",
      "Iteration 415, loss = 0.34187135\n",
      "Iteration 154, loss = 0.35782451\n",
      "Iteration 413, loss = 0.34996096\n",
      "Iteration 338, loss = 0.33311415\n",
      "Iteration 154, loss = 0.34042031\n",
      "Iteration 155, loss = 0.35737587\n",
      "Iteration 416, loss = 0.34180741\n",
      "Iteration 414, loss = 0.34985113\n",
      "Iteration 339, loss = 0.33300024\n",
      "Iteration 417, loss = 0.34169366\n",
      "Iteration 415, loss = 0.34974726\n",
      "Iteration 155, loss = 0.34022630\n",
      "Iteration 156, loss = 0.35681124\n",
      "Iteration 416, loss = 0.34963246\n",
      "Iteration 418, loss = 0.34164104\n",
      "Iteration 340, loss = 0.33294344\n",
      "Iteration 156, loss = 0.33998684\n",
      "Iteration 417, loss = 0.34951854\n",
      "Iteration 419, loss = 0.34152041\n",
      "Iteration 157, loss = 0.35631506\n",
      "Iteration 157, loss = 0.33979120\n",
      "Iteration 341, loss = 0.33282585\n",
      "Iteration 420, loss = 0.34143309\n",
      "Iteration 418, loss = 0.34944248\n",
      "Iteration 158, loss = 0.33959226\n",
      "Iteration 158, loss = 0.35584072\n",
      "Iteration 342, loss = 0.33272362\n",
      "Iteration 159, loss = 0.33939890\n",
      "Iteration 421, loss = 0.34133937\n",
      "Iteration 419, loss = 0.34930168\n",
      "Iteration 343, loss = 0.33260628\n",
      "Iteration 159, loss = 0.35536639\n",
      "Iteration 420, loss = 0.34920732\n",
      "Iteration 160, loss = 0.33924621\n",
      "Iteration 422, loss = 0.34126095\n",
      "Iteration 344, loss = 0.33253017\n",
      "Iteration 160, loss = 0.35493387\n",
      "Iteration 161, loss = 0.33900034\n",
      "Iteration 421, loss = 0.34910077\n",
      "Iteration 345, loss = 0.33244139\n",
      "Iteration 161, loss = 0.35440332\n",
      "Iteration 423, loss = 0.34116787\n",
      "Iteration 422, loss = 0.34897896\n",
      "Iteration 162, loss = 0.33882030\n",
      "Iteration 346, loss = 0.33232783\n",
      "Iteration 162, loss = 0.35395671\n",
      "Iteration 424, loss = 0.34108595\n",
      "Iteration 423, loss = 0.34886798\n",
      "Iteration 163, loss = 0.35346759\n",
      "Iteration 163, loss = 0.33860919\n",
      "Iteration 425, loss = 0.34099492\n",
      "Iteration 347, loss = 0.33224090\n",
      "Iteration 164, loss = 0.33844311\n",
      "Iteration 424, loss = 0.34877478\n",
      "Iteration 164, loss = 0.35302452\n",
      "Iteration 426, loss = 0.34090689\n",
      "Iteration 165, loss = 0.33826583\n",
      "Iteration 425, loss = 0.34866458\n",
      "Iteration 165, loss = 0.35258481\n",
      "Iteration 348, loss = 0.33213248\n",
      "Iteration 427, loss = 0.34083999\n",
      "Iteration 426, loss = 0.34854224\n",
      "Iteration 166, loss = 0.35212244\n",
      "Iteration 349, loss = 0.33205028\n",
      "Iteration 428, loss = 0.34074223\n",
      "Iteration 427, loss = 0.34844257\n",
      "Iteration 167, loss = 0.35168591\n",
      "Iteration 350, loss = 0.33194421\n",
      "Iteration 429, loss = 0.34070545\n",
      "Iteration 166, loss = 0.33805536\n",
      "Iteration 428, loss = 0.34832613\n",
      "Iteration 351, loss = 0.33185033\n",
      "Iteration 430, loss = 0.34057949\n",
      "Iteration 167, loss = 0.33787319\n",
      "Iteration 168, loss = 0.35126448\n",
      "Iteration 429, loss = 0.34825647\n",
      "Iteration 431, loss = 0.34049113\n",
      "Iteration 168, loss = 0.33771763\n",
      "Iteration 169, loss = 0.35082528\n",
      "Iteration 430, loss = 0.34812569\n",
      "Iteration 352, loss = 0.33176496\n",
      "Iteration 432, loss = 0.34038521\n",
      "Iteration 169, loss = 0.33752217\n",
      "Iteration 431, loss = 0.34800552\n",
      "Iteration 170, loss = 0.35044648\n",
      "Iteration 353, loss = 0.33167293\n",
      "Iteration 433, loss = 0.34030710\n",
      "Iteration 432, loss = 0.34791074\n",
      "Iteration 170, loss = 0.33738971\n",
      "Iteration 354, loss = 0.33158682\n",
      "Iteration 434, loss = 0.34022610\n",
      "Iteration 171, loss = 0.34999723\n",
      "Iteration 433, loss = 0.34778878\n",
      "Iteration 171, loss = 0.33718887\n",
      "Iteration 434, loss = 0.34769532\n",
      "Iteration 172, loss = 0.34962065\n",
      "Iteration 435, loss = 0.34013603\n",
      "Iteration 355, loss = 0.33148619\n",
      "Iteration 172, loss = 0.33704855\n",
      "Iteration 173, loss = 0.34917640\n",
      "Iteration 435, loss = 0.34757058\n",
      "Iteration 436, loss = 0.34006941\n",
      "Iteration 173, loss = 0.33685108\n",
      "Iteration 356, loss = 0.33139181\n",
      "Iteration 437, loss = 0.33997735\n",
      "Iteration 357, loss = 0.33128835Iteration 174, loss = 0.33669654\n",
      "\n",
      "Iteration 436, loss = 0.34748259\n",
      "Iteration 438, loss = 0.33990300\n",
      "Iteration 174, loss = 0.34877461\n",
      "Iteration 358, loss = 0.33120529\n",
      "Iteration 437, loss = 0.34737403\n",
      "Iteration 175, loss = 0.33652318\n",
      "Iteration 175, loss = 0.34838500\n",
      "Iteration 439, loss = 0.33979285\n",
      "Iteration 359, loss = 0.33110890\n",
      "Iteration 438, loss = 0.34725565\n",
      "Iteration 176, loss = 0.33635584\n",
      "Iteration 176, loss = 0.34799405\n",
      "Iteration 360, loss = 0.33101905\n",
      "Iteration 440, loss = 0.33970915\n",
      "Iteration 177, loss = 0.33620773\n",
      "Iteration 361, loss = 0.33093630\n",
      "Iteration 441, loss = 0.33964993\n",
      "Iteration 439, loss = 0.34714564\n",
      "Iteration 178, loss = 0.33604856\n",
      "Iteration 177, loss = 0.34762056\n",
      "Iteration 440, loss = 0.34704944\n",
      "Iteration 362, loss = 0.33084561\n",
      "Iteration 442, loss = 0.33956746\n",
      "Iteration 178, loss = 0.34721601\n",
      "Iteration 179, loss = 0.33589882\n",
      "Iteration 441, loss = 0.34696751\n",
      "Iteration 363, loss = 0.33075177\n",
      "Iteration 179, loss = 0.34685982\n",
      "Iteration 443, loss = 0.33945554\n",
      "Iteration 180, loss = 0.33573828\n",
      "Iteration 442, loss = 0.34684558\n",
      "Iteration 180, loss = 0.34647613\n",
      "Iteration 444, loss = 0.33937640\n",
      "Iteration 181, loss = 0.33561066\n",
      "Iteration 443, loss = 0.34672812\n",
      "Iteration 364, loss = 0.33067138\n",
      "Iteration 182, loss = 0.33544386\n",
      "Iteration 444, loss = 0.34661795\n",
      "Iteration 181, loss = 0.34614314\n",
      "Iteration 445, loss = 0.33930351\n",
      "Iteration 365, loss = 0.33058037\n",
      "Iteration 182, loss = 0.34575359\n",
      "Iteration 445, loss = 0.34652156\n",
      "Iteration 446, loss = 0.33921014\n",
      "Iteration 183, loss = 0.33528231\n",
      "Iteration 183, loss = 0.34536863\n",
      "Iteration 446, loss = 0.34640728\n",
      "Iteration 366, loss = 0.33049014\n",
      "Iteration 447, loss = 0.33914533\n",
      "Iteration 184, loss = 0.34503881\n",
      "Iteration 184, loss = 0.33514582\n",
      "Iteration 367, loss = 0.33039958\n",
      "Iteration 447, loss = 0.34632331\n",
      "Iteration 185, loss = 0.33499759\n",
      "Iteration 448, loss = 0.33903259\n",
      "Iteration 185, loss = 0.34467697\n",
      "Iteration 368, loss = 0.33035419\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 448, loss = 0.34618594\n",
      "Iteration 186, loss = 0.33485635\n",
      "Iteration 186, loss = 0.34432139\n",
      "Iteration 449, loss = 0.34610277\n",
      "Iteration 449, loss = 0.33897165\n",
      "Iteration 187, loss = 0.33473745\n",
      "Iteration 187, loss = 0.34399564\n",
      "Iteration 450, loss = 0.34602658\n",
      "Iteration 450, loss = 0.33890381\n",
      "Iteration 188, loss = 0.33456515\n",
      "Iteration 188, loss = 0.34362992\n",
      "Iteration 451, loss = 0.34590329\n",
      "Iteration 451, loss = 0.33880703\n",
      "Iteration 189, loss = 0.34330462\n",
      "Iteration 189, loss = 0.33444033\n",
      "Iteration 452, loss = 0.33872695\n",
      "Iteration 452, loss = 0.34579476\n",
      "Iteration 190, loss = 0.34296747\n",
      "Iteration 190, loss = 0.33431166\n",
      "Iteration 453, loss = 0.33863720\n",
      "Iteration 453, loss = 0.34567995\n",
      "Iteration 191, loss = 0.34263604\n",
      "Iteration 191, loss = 0.33418160\n",
      "Iteration 454, loss = 0.33855514\n",
      "Iteration 192, loss = 0.34231899\n",
      "Iteration 454, loss = 0.34557876\n",
      "Iteration 192, loss = 0.33404485\n",
      "Iteration 193, loss = 0.34200666\n",
      "Iteration 455, loss = 0.33846557\n",
      "Iteration 193, loss = 0.33391836\n",
      "Iteration 455, loss = 0.34546453\n",
      "Iteration 194, loss = 0.34167728\n",
      "Iteration 456, loss = 0.33840073\n",
      "Iteration 194, loss = 0.33378472\n",
      "Iteration 456, loss = 0.34538428\n",
      "Iteration 195, loss = 0.34135484\n",
      "Iteration 457, loss = 0.33830337\n",
      "Iteration 195, loss = 0.33365885\n",
      "Iteration 457, loss = 0.34526467\n",
      "Iteration 196, loss = 0.34102423\n",
      "Iteration 458, loss = 0.33822674\n",
      "Iteration 196, loss = 0.33351230\n",
      "Iteration 458, loss = 0.34515746\n",
      "Iteration 197, loss = 0.34072560\n",
      "Iteration 459, loss = 0.33815486\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 197, loss = 0.33338994\n",
      "Iteration 459, loss = 0.34505859\n",
      "Iteration 198, loss = 0.34042036\n",
      "Iteration 460, loss = 0.34495730\n",
      "Iteration 198, loss = 0.33327782\n",
      "Iteration 199, loss = 0.34012476\n",
      "Iteration 461, loss = 0.34485548\n",
      "Iteration 199, loss = 0.33315257\n",
      "Iteration 200, loss = 0.33981957\n",
      "Iteration 462, loss = 0.34475905\n",
      "Iteration 200, loss = 0.33302058\n",
      "Iteration 201, loss = 0.33951540\n",
      "Iteration 463, loss = 0.34464606\n",
      "Iteration 201, loss = 0.33290118\n",
      "Iteration 202, loss = 0.33925773\n",
      "Iteration 464, loss = 0.34455294\n",
      "Iteration 202, loss = 0.33281548\n",
      "Iteration 203, loss = 0.33894465\n",
      "Iteration 465, loss = 0.34444350\n",
      "Iteration 203, loss = 0.33267645\n",
      "Iteration 466, loss = 0.34433817\n",
      "Iteration 204, loss = 0.33867560\n",
      "Iteration 204, loss = 0.33257687\n",
      "Iteration 467, loss = 0.34423373\n",
      "Iteration 205, loss = 0.33836576\n",
      "Iteration 205, loss = 0.33244379\n",
      "Iteration 468, loss = 0.34413001\n",
      "Iteration 206, loss = 0.33811001\n",
      "Iteration 206, loss = 0.33234667\n",
      "Iteration 469, loss = 0.34403804\n",
      "Iteration 207, loss = 0.33782237\n",
      "Iteration 207, loss = 0.33223380\n",
      "Iteration 470, loss = 0.34397440\n",
      "Iteration 208, loss = 0.33754043\n",
      "Iteration 208, loss = 0.33211132\n",
      "Iteration 471, loss = 0.34382408\n",
      "Iteration 209, loss = 0.33726855\n",
      "Iteration 209, loss = 0.33199671\n",
      "Iteration 472, loss = 0.34373785\n",
      "Iteration 210, loss = 0.33698610\n",
      "Iteration 210, loss = 0.33187964\n",
      "Iteration 473, loss = 0.34362185\n",
      "Iteration 211, loss = 0.33672694\n",
      "Iteration 211, loss = 0.33178377\n",
      "Iteration 474, loss = 0.34352255\n",
      "Iteration 212, loss = 0.33645532\n",
      "Iteration 212, loss = 0.33167517\n",
      "Iteration 475, loss = 0.34343391\n",
      "Iteration 213, loss = 0.33619638\n",
      "Iteration 213, loss = 0.33156205\n",
      "Iteration 476, loss = 0.34333377\n",
      "Iteration 214, loss = 0.33593667\n",
      "Iteration 214, loss = 0.33146201\n",
      "Iteration 477, loss = 0.34324342\n",
      "Iteration 215, loss = 0.33568568\n",
      "Iteration 215, loss = 0.33135733\n",
      "Iteration 478, loss = 0.34312229\n",
      "Iteration 216, loss = 0.33542544\n",
      "Iteration 216, loss = 0.33125721\n",
      "Iteration 479, loss = 0.34303168\n",
      "Iteration 217, loss = 0.33517787\n",
      "Iteration 217, loss = 0.33115776\n",
      "Iteration 480, loss = 0.34293173\n",
      "Iteration 218, loss = 0.33493920\n",
      "Iteration 218, loss = 0.33106086\n",
      "Iteration 481, loss = 0.34284376\n",
      "Iteration 219, loss = 0.33465378\n",
      "Iteration 219, loss = 0.33093797\n",
      "Iteration 482, loss = 0.34272634\n",
      "Iteration 220, loss = 0.33442434\n",
      "Iteration 220, loss = 0.33085353\n",
      "Iteration 483, loss = 0.34263183\n",
      "Iteration 221, loss = 0.33418556\n",
      "Iteration 221, loss = 0.33076030\n",
      "Iteration 484, loss = 0.34253467\n",
      "Iteration 222, loss = 0.33394247\n",
      "Iteration 222, loss = 0.33065680\n",
      "Iteration 485, loss = 0.34247069\n",
      "Iteration 223, loss = 0.33369019\n",
      "Iteration 223, loss = 0.33055970\n",
      "Iteration 486, loss = 0.34235999\n",
      "Iteration 224, loss = 0.33345638\n",
      "Iteration 224, loss = 0.33046582\n",
      "Iteration 487, loss = 0.34223975\n",
      "Iteration 225, loss = 0.33320685\n",
      "Iteration 225, loss = 0.33036970\n",
      "Iteration 488, loss = 0.34218313\n",
      "Iteration 226, loss = 0.33298311\n",
      "Iteration 226, loss = 0.33028453\n",
      "Iteration 489, loss = 0.34205452\n",
      "Iteration 227, loss = 0.33275701\n",
      "Iteration 227, loss = 0.33019456\n",
      "Iteration 490, loss = 0.34194356\n",
      "Iteration 228, loss = 0.33254209\n",
      "Iteration 228, loss = 0.33012046\n",
      "Iteration 491, loss = 0.34186166\n",
      "Iteration 229, loss = 0.33230257\n",
      "Iteration 229, loss = 0.33002080\n",
      "Iteration 492, loss = 0.34175031\n",
      "Iteration 230, loss = 0.33209075\n",
      "Iteration 230, loss = 0.32995092\n",
      "Iteration 493, loss = 0.34165415\n",
      "Iteration 231, loss = 0.33182377\n",
      "Iteration 231, loss = 0.32982925\n",
      "Iteration 494, loss = 0.34155806\n",
      "Iteration 232, loss = 0.33162194\n",
      "Iteration 232, loss = 0.32975898\n",
      "Iteration 495, loss = 0.34145619\n",
      "Iteration 233, loss = 0.33138354\n",
      "Iteration 233, loss = 0.32966913\n",
      "Iteration 496, loss = 0.34139199\n",
      "Iteration 234, loss = 0.33115628\n",
      "Iteration 234, loss = 0.32958022\n",
      "Iteration 497, loss = 0.34128177\n",
      "Iteration 235, loss = 0.33093218\n",
      "Iteration 235, loss = 0.32948678\n",
      "Iteration 498, loss = 0.34119983\n",
      "Iteration 236, loss = 0.33071950\n",
      "Iteration 236, loss = 0.32940177\n",
      "Iteration 499, loss = 0.34108884\n",
      "Iteration 237, loss = 0.33050244\n",
      "Iteration 237, loss = 0.32932638\n",
      "Iteration 500, loss = 0.34098593\n",
      "Iteration 238, loss = 0.33028793\n",
      "Iteration 238, loss = 0.32924608\n",
      "Iteration 501, loss = 0.34088429\n",
      "Iteration 239, loss = 0.33004890\n",
      "Iteration 239, loss = 0.32914325\n",
      "Iteration 502, loss = 0.34079271\n",
      "Iteration 240, loss = 0.32986982\n",
      "Iteration 240, loss = 0.32908312\n",
      "Iteration 503, loss = 0.34068867\n",
      "Iteration 241, loss = 0.32965972\n",
      "Iteration 241, loss = 0.32900692\n",
      "Iteration 504, loss = 0.34059867\n",
      "Iteration 242, loss = 0.32945198\n",
      "Iteration 242, loss = 0.32892195\n",
      "Iteration 505, loss = 0.34051065\n",
      "Iteration 243, loss = 0.32923082\n",
      "Iteration 506, loss = 0.34040468\n",
      "Iteration 243, loss = 0.32882925\n",
      "Iteration 244, loss = 0.32904597\n",
      "Iteration 507, loss = 0.34032054\n",
      "Iteration 244, loss = 0.32877341\n",
      "Iteration 245, loss = 0.32882219\n",
      "Iteration 508, loss = 0.34024458\n",
      "Iteration 245, loss = 0.32867740\n",
      "Iteration 246, loss = 0.32860826\n",
      "Iteration 509, loss = 0.34012860\n",
      "Iteration 246, loss = 0.32859094\n",
      "Iteration 247, loss = 0.32842216\n",
      "Iteration 510, loss = 0.34004475\n",
      "Iteration 247, loss = 0.32852789\n",
      "Iteration 248, loss = 0.32820564\n",
      "Iteration 511, loss = 0.33994642\n",
      "Iteration 248, loss = 0.32844380\n",
      "Iteration 249, loss = 0.32801841\n",
      "Iteration 512, loss = 0.33986379\n",
      "Iteration 249, loss = 0.32837228\n",
      "Iteration 250, loss = 0.32781210\n",
      "Iteration 513, loss = 0.33974960\n",
      "Iteration 250, loss = 0.32829400\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 251, loss = 0.32763110\n",
      "Iteration 514, loss = 0.33965824\n",
      "Iteration 252, loss = 0.32743937\n",
      "Iteration 515, loss = 0.33956794\n",
      "Iteration 253, loss = 0.32724769\n",
      "Iteration 516, loss = 0.33947173\n",
      "Iteration 254, loss = 0.32702529\n",
      "Iteration 517, loss = 0.33941036\n",
      "Iteration 255, loss = 0.32684695\n",
      "Iteration 518, loss = 0.33929192\n",
      "Iteration 256, loss = 0.32665979\n",
      "Iteration 519, loss = 0.33918279\n",
      "Iteration 257, loss = 0.32646854\n",
      "Iteration 520, loss = 0.33909347\n",
      "Iteration 258, loss = 0.32627309\n",
      "Iteration 521, loss = 0.33904564\n",
      "Iteration 259, loss = 0.32608544\n",
      "Iteration 522, loss = 0.33891610\n",
      "Iteration 260, loss = 0.32590562\n",
      "Iteration 523, loss = 0.33881692\n",
      "Iteration 261, loss = 0.32573692\n",
      "Iteration 524, loss = 0.33875660\n",
      "Iteration 262, loss = 0.32553895\n",
      "Iteration 525, loss = 0.33863035\n",
      "Iteration 263, loss = 0.32534954\n",
      "Iteration 526, loss = 0.33854247\n",
      "Iteration 264, loss = 0.32515962\n",
      "Iteration 527, loss = 0.33845765\n",
      "Iteration 265, loss = 0.32500669\n",
      "Iteration 528, loss = 0.33836762\n",
      "Iteration 266, loss = 0.32481387\n",
      "Iteration 529, loss = 0.33828080\n",
      "Iteration 267, loss = 0.32464785\n",
      "Iteration 530, loss = 0.33817818\n",
      "Iteration 268, loss = 0.32445539\n",
      "Iteration 531, loss = 0.33809343\n",
      "Iteration 269, loss = 0.32429782\n",
      "Iteration 532, loss = 0.33799156\n",
      "Iteration 270, loss = 0.32412876\n",
      "Iteration 533, loss = 0.33790915\n",
      "Iteration 271, loss = 0.32395751\n",
      "Iteration 534, loss = 0.33781134\n",
      "Iteration 272, loss = 0.32379529\n",
      "Iteration 535, loss = 0.33772616\n",
      "Iteration 273, loss = 0.32360370\n",
      "Iteration 536, loss = 0.33765322\n",
      "Iteration 274, loss = 0.32343981\n",
      "Iteration 537, loss = 0.33754096\n",
      "Iteration 275, loss = 0.32327128\n",
      "Iteration 538, loss = 0.33744399\n",
      "Iteration 276, loss = 0.32309900\n",
      "Iteration 539, loss = 0.33737599\n",
      "Iteration 277, loss = 0.32293648\n",
      "Iteration 540, loss = 0.33727261\n",
      "Iteration 278, loss = 0.32277380\n",
      "Iteration 541, loss = 0.33718680\n",
      "Iteration 279, loss = 0.32262005\n",
      "Iteration 542, loss = 0.33710049\n",
      "Iteration 280, loss = 0.32244062\n",
      "Iteration 543, loss = 0.33699946\n",
      "Iteration 281, loss = 0.32229473\n",
      "Iteration 544, loss = 0.33690908\n",
      "Iteration 282, loss = 0.32213124\n",
      "Iteration 545, loss = 0.33683960\n",
      "Iteration 283, loss = 0.32196136\n",
      "Iteration 546, loss = 0.33673509\n",
      "Iteration 284, loss = 0.32179691\n",
      "Iteration 547, loss = 0.33664260\n",
      "Iteration 285, loss = 0.32165467\n",
      "Iteration 548, loss = 0.33657061\n",
      "Iteration 286, loss = 0.32149717\n",
      "Iteration 549, loss = 0.33647459\n",
      "Iteration 287, loss = 0.32133830\n",
      "Iteration 550, loss = 0.33638166\n",
      "Iteration 288, loss = 0.32118163\n",
      "Iteration 551, loss = 0.33629866\n",
      "Iteration 289, loss = 0.32102533\n",
      "Iteration 552, loss = 0.33621711\n",
      "Iteration 290, loss = 0.32088933\n",
      "Iteration 553, loss = 0.33611215\n",
      "Iteration 291, loss = 0.32071738\n",
      "Iteration 554, loss = 0.33602520\n",
      "Iteration 292, loss = 0.32058637\n",
      "Iteration 555, loss = 0.33594159\n",
      "Iteration 293, loss = 0.32042498\n",
      "Iteration 556, loss = 0.33586201\n",
      "Iteration 294, loss = 0.32028356\n",
      "Iteration 557, loss = 0.33575943\n",
      "Iteration 295, loss = 0.32012462\n",
      "Iteration 558, loss = 0.33567787\n",
      "Iteration 296, loss = 0.32001896\n",
      "Iteration 559, loss = 0.33558402\n",
      "Iteration 297, loss = 0.31984474\n",
      "Iteration 560, loss = 0.33551378\n",
      "Iteration 298, loss = 0.31968325\n",
      "Iteration 561, loss = 0.33542702\n",
      "Iteration 299, loss = 0.31954374\n",
      "Iteration 562, loss = 0.33532476\n",
      "Iteration 300, loss = 0.31941190\n",
      "Iteration 563, loss = 0.33523551\n",
      "Iteration 301, loss = 0.31926601\n",
      "Iteration 564, loss = 0.33514973\n",
      "Iteration 302, loss = 0.31913455\n",
      "Iteration 565, loss = 0.33507364\n",
      "Iteration 303, loss = 0.31897974\n",
      "Iteration 566, loss = 0.33498886\n",
      "Iteration 304, loss = 0.31884325\n",
      "Iteration 567, loss = 0.33490763\n",
      "Iteration 305, loss = 0.31870690\n",
      "Iteration 568, loss = 0.33480608\n",
      "Iteration 306, loss = 0.31856372\n",
      "Iteration 569, loss = 0.33472565\n",
      "Iteration 307, loss = 0.31843420\n",
      "Iteration 570, loss = 0.33465461\n",
      "Iteration 308, loss = 0.31828313\n",
      "Iteration 571, loss = 0.33455375\n",
      "Iteration 309, loss = 0.31817133\n",
      "Iteration 572, loss = 0.33447282\n",
      "Iteration 310, loss = 0.31802402\n",
      "Iteration 573, loss = 0.33438123\n",
      "Iteration 311, loss = 0.31790750\n",
      "Iteration 574, loss = 0.33431590\n",
      "Iteration 312, loss = 0.31776856\n",
      "Iteration 575, loss = 0.33420698\n",
      "Iteration 313, loss = 0.31762420\n",
      "Iteration 576, loss = 0.33412915\n",
      "Iteration 314, loss = 0.31748586\n",
      "Iteration 577, loss = 0.33402635\n",
      "Iteration 315, loss = 0.31734516\n",
      "Iteration 578, loss = 0.33395244\n",
      "Iteration 316, loss = 0.31722742\n",
      "Iteration 579, loss = 0.33387232\n",
      "Iteration 317, loss = 0.31709471\n",
      "Iteration 580, loss = 0.33377097\n",
      "Iteration 318, loss = 0.31697990\n",
      "Iteration 581, loss = 0.33367873\n",
      "Iteration 319, loss = 0.31684746\n",
      "Iteration 582, loss = 0.33358623\n",
      "Iteration 320, loss = 0.31673904\n",
      "Iteration 583, loss = 0.33350731\n",
      "Iteration 321, loss = 0.31659391\n",
      "Iteration 584, loss = 0.33341787\n",
      "Iteration 322, loss = 0.31646153\n",
      "Iteration 585, loss = 0.33334987\n",
      "Iteration 323, loss = 0.31632861\n",
      "Iteration 586, loss = 0.33324448\n",
      "Iteration 324, loss = 0.31619829\n",
      "Iteration 587, loss = 0.33316761\n",
      "Iteration 325, loss = 0.31609802\n",
      "Iteration 588, loss = 0.33307464\n",
      "Iteration 326, loss = 0.31594729\n",
      "Iteration 589, loss = 0.33298902\n",
      "Iteration 327, loss = 0.31583608\n",
      "Iteration 590, loss = 0.33290677\n",
      "Iteration 328, loss = 0.31569428\n",
      "Iteration 591, loss = 0.33282333\n",
      "Iteration 329, loss = 0.31556760\n",
      "Iteration 592, loss = 0.33272789\n",
      "Iteration 330, loss = 0.31546225\n",
      "Iteration 593, loss = 0.33264558\n",
      "Iteration 331, loss = 0.31533816\n",
      "Iteration 594, loss = 0.33255008\n",
      "Iteration 332, loss = 0.31520935\n",
      "Iteration 595, loss = 0.33250016\n",
      "Iteration 333, loss = 0.31509161\n",
      "Iteration 596, loss = 0.33239056\n",
      "Iteration 334, loss = 0.31496940\n",
      "Iteration 597, loss = 0.33229321\n",
      "Iteration 335, loss = 0.31485393\n",
      "Iteration 598, loss = 0.33225581\n",
      "Iteration 336, loss = 0.31473471\n",
      "Iteration 599, loss = 0.33212691\n",
      "Iteration 337, loss = 0.31461647\n",
      "Iteration 600, loss = 0.33203799\n",
      "Iteration 338, loss = 0.31450421\n",
      "Iteration 601, loss = 0.33195698\n",
      "Iteration 339, loss = 0.31439949\n",
      "Iteration 602, loss = 0.33186858\n",
      "Iteration 340, loss = 0.31426493\n",
      "Iteration 603, loss = 0.33178890\n",
      "Iteration 341, loss = 0.31414478\n",
      "Iteration 604, loss = 0.33170743\n",
      "Iteration 342, loss = 0.31403568\n",
      "Iteration 605, loss = 0.33163617\n",
      "Iteration 343, loss = 0.31392363\n",
      "Iteration 606, loss = 0.33154455\n",
      "Iteration 344, loss = 0.31379662\n",
      "Iteration 607, loss = 0.33145394\n",
      "Iteration 345, loss = 0.31369550\n",
      "Iteration 608, loss = 0.33135695\n",
      "Iteration 346, loss = 0.31358695\n",
      "Iteration 609, loss = 0.33127803\n",
      "Iteration 347, loss = 0.31346186\n",
      "Iteration 610, loss = 0.33121096\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 348, loss = 0.31337842\n",
      "Iteration 349, loss = 0.31324415\n",
      "Iteration 350, loss = 0.31314400\n",
      "Iteration 351, loss = 0.31301853\n",
      "Iteration 352, loss = 0.31291510\n",
      "Iteration 353, loss = 0.31280600\n",
      "Iteration 354, loss = 0.31270269\n",
      "Iteration 355, loss = 0.31258369\n",
      "Iteration 356, loss = 0.31248873\n",
      "Iteration 357, loss = 0.31237697\n",
      "Iteration 358, loss = 0.31227267\n",
      "Iteration 359, loss = 0.31216963\n",
      "Iteration 360, loss = 0.31206263\n",
      "Iteration 361, loss = 0.31195700\n",
      "Iteration 362, loss = 0.31185120\n",
      "Iteration 363, loss = 0.31175090\n",
      "Iteration 364, loss = 0.31165816\n",
      "Iteration 365, loss = 0.31153338\n",
      "Iteration 366, loss = 0.31143524\n",
      "Iteration 367, loss = 0.31133247\n",
      "Iteration 368, loss = 0.31124533\n",
      "Iteration 369, loss = 0.31112870\n",
      "Iteration 370, loss = 0.31101449\n",
      "Iteration 371, loss = 0.31093656\n",
      "Iteration 372, loss = 0.31082156\n",
      "Iteration 373, loss = 0.31072083\n",
      "Iteration 374, loss = 0.31062428\n",
      "Iteration 375, loss = 0.31050675\n",
      "Iteration 376, loss = 0.31041818\n",
      "Iteration 377, loss = 0.31032247\n",
      "Iteration 378, loss = 0.31022397\n",
      "Iteration 379, loss = 0.31012658\n",
      "Iteration 380, loss = 0.31002749\n",
      "Iteration 381, loss = 0.30991782\n",
      "Iteration 382, loss = 0.30983431\n",
      "Iteration 383, loss = 0.30974165\n",
      "Iteration 384, loss = 0.30963103\n",
      "Iteration 385, loss = 0.30954418\n",
      "Iteration 386, loss = 0.30945229\n",
      "Iteration 387, loss = 0.30935202\n",
      "Iteration 388, loss = 0.30925315\n",
      "Iteration 389, loss = 0.30917142\n",
      "Iteration 390, loss = 0.30907350\n",
      "Iteration 391, loss = 0.30897481\n",
      "Iteration 392, loss = 0.30888540\n",
      "Iteration 393, loss = 0.30878453\n",
      "Iteration 394, loss = 0.30870319\n",
      "Iteration 395, loss = 0.30861541\n",
      "Iteration 396, loss = 0.30853074\n",
      "Iteration 397, loss = 0.30842576\n",
      "Iteration 398, loss = 0.30832767\n",
      "Iteration 399, loss = 0.30824036\n",
      "Iteration 400, loss = 0.30816251\n",
      "Iteration 401, loss = 0.30805037\n",
      "Iteration 402, loss = 0.30796027\n",
      "Iteration 403, loss = 0.30788099\n",
      "Iteration 404, loss = 0.30781463\n",
      "Iteration 405, loss = 0.30770089\n",
      "Iteration 406, loss = 0.30761708\n",
      "Iteration 407, loss = 0.30752432\n",
      "Iteration 408, loss = 0.30743780\n",
      "Iteration 409, loss = 0.30734499\n",
      "Iteration 410, loss = 0.30725141\n",
      "Iteration 411, loss = 0.30716498\n",
      "Iteration 412, loss = 0.30709788\n",
      "Iteration 413, loss = 0.30698936\n",
      "Iteration 414, loss = 0.30690145\n",
      "Iteration 415, loss = 0.30681390\n",
      "Iteration 416, loss = 0.30672599\n",
      "Iteration 417, loss = 0.30667762\n",
      "Iteration 418, loss = 0.30655228\n",
      "Iteration 419, loss = 0.30646868\n",
      "Iteration 420, loss = 0.30636755\n",
      "Iteration 421, loss = 0.30628766\n",
      "Iteration 422, loss = 0.30620174\n",
      "Iteration 423, loss = 0.30612470\n",
      "Iteration 424, loss = 0.30604257\n",
      "Iteration 425, loss = 0.30595980\n",
      "Iteration 426, loss = 0.30585243\n",
      "Iteration 427, loss = 0.30578142\n",
      "Iteration 428, loss = 0.30569645\n",
      "Iteration 429, loss = 0.30561206\n",
      "Iteration 430, loss = 0.30551800\n",
      "Iteration 431, loss = 0.30545950\n",
      "Iteration 432, loss = 0.30536263\n",
      "Iteration 433, loss = 0.30527356\n",
      "Iteration 434, loss = 0.30519490\n",
      "Iteration 435, loss = 0.30511025\n",
      "Iteration 436, loss = 0.30502075\n",
      "Iteration 437, loss = 0.30492790\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the parameter range for activation\n",
    "activation_range = ['logistic', 'tanh', 'relu']\n",
    "\n",
    "# Create a scorer for F1 Score\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# Validation curve for activation\n",
    "train_scores, valid_scores = validation_curve(\n",
    "    MLPClassifier(solver='sgd', max_iter=4000, learning_rate='constant', alpha=0.1, hidden_layer_sizes=(100, 50), random_state=42, verbose=True),\n",
    "    X_train, y_train,\n",
    "    param_name='activation',\n",
    "    param_range=activation_range,\n",
    "    cv=5,\n",
    "    scoring=f1_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate mean and standard deviation for training and validation scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "valid_mean = np.mean(valid_scores, axis=1)\n",
    "valid_std = np.std(valid_scores, axis=1)\n",
    "\n",
    "# Plot the validation curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(activation_range, train_mean, label='Training score', color='blue')\n",
    "plt.plot(activation_range, valid_mean, label='Cross-validation score', color='green')\n",
    "plt.fill_between(activation_range, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n",
    "plt.fill_between(activation_range, valid_mean - valid_std, valid_mean + valid_std, alpha=0.2, color='green')\n",
    "plt.title('Validation Curve for MLPClassifier (activation)')\n",
    "plt.xlabel('Activation Function')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04a90dc-d119-46ce-a12f-2bc9a2874160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'momentum': [0.9, 0.95, 0.99],\n",
    "    'early_stopping': [True, False],\n",
    "    'max_iter': [1000, 2000, 4000]\n",
    "}\n",
    "\n",
    "# Create the MLPClassifier with the tuned hyperparameters\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    activation='relu',\n",
    "    alpha=0.1,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Create a scorer for F1 Score\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# Set up the GridSearchCV\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring=f1_scorer, n_jobs=-1)\n",
    "\n",
    "# Fit the model with GridSearchCV to find the best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters found\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Train the model with the best parameters on the full training data\n",
    "best_mlp = grid_search.best_estimator_\n",
    "best_mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "predictions = best_mlp.predict(X_val)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
