import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
import matplotlib.pyplot as plt

# Load the dataset
heart_df = pd.read_csv('heart.csv')

# Define the categorical and numerical columns
categorical_features = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']
numerical_features = ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak']

# Define the preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(), categorical_features)
    ])

# Custom Transformer to ensure DataFrame structure is maintained
class DataFrameSelector(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self
    def transform(self, X):
        return X



# Separate features and target
X = heart_df.drop('HeartDisease', axis=1)
y = heart_df['HeartDisease']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


# Define the pipeline with preprocessing and SVC (using polynomial kernel)
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('svc', SVC(kernel='sigmoid'))
])

# Define the range of parameter values to test for C
param_range = np.logspace(-4, 2, 7)

# Calculate the validation curve
train_scores, test_scores = validation_curve(
    pipeline, X_train, y_train, param_name="svc__C", param_range=param_range,
    scoring="f1", n_jobs=-1, cv=5
)

# Calculate mean and standard deviation of the training and test scores
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

# Plot the validation curve
plt.figure()
plt.title("Validation Curve with SVM (Sigmoid Kernel)")
plt.xlabel("C")
plt.ylabel("F1 Score")
plt.ylim(0.0, 1.1)
plt.semilogx(param_range, train_scores_mean, label="Training score", color="b")
plt.semilogx(param_range, test_scores_mean, label="Cross-validation score", color="g")
plt.legend(loc="best")
plt.grid(True)
plt.show()


import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import validation_curve
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer

# Assuming DataFrameSelector and preprocessor are already defined as in previous context

# Define the pipeline with preprocessing and SVC
pipeline = Pipeline([
    ('selector', DataFrameSelector()),  # Select columns (if needed, otherwise remove this step)
    ('preprocessor', preprocessor),
    ('svc', SVC(kernel='poly', C=0.1))
])

# Separate features and target
X = heart_df.drop('HeartDisease', axis=1)
y = heart_df['HeartDisease']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Define the range of parameter values to test for degree
param_range = np.arange(1, 10)  # Testing polynomial degrees from 1 to 9

# Calculate the validation curve
train_scores, test_scores = validation_curve(
    pipeline, X_train, y_train, param_name="svc__degree", param_range=param_range,
    scoring="f1", n_jobs=-1, cv=5
)

# Calculate mean and standard deviation of the training and test scores
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

# Plot the validation curve
plt.figure()
plt.title("Validation Curve with SVM (Polynomial Kernel, C=0.1)")
plt.xlabel("Degree of polynomial")
plt.ylabel("F1 Score")
plt.ylim(0.0, 1.1)
plt.plot(param_range, train_scores_mean, label="Training score", color="b")
plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color="b")
plt.plot(param_range, test_scores_mean, label="Cross-validation score", color="g")
plt.fill_between(param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color="g")
plt.legend(loc="best")
plt.grid(True)
plt.show()



import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import validation_curve
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer

# Assuming DataFrameSelector and preprocessor are already defined as in previous context

# Define the pipeline with preprocessing and SVC
pipeline = Pipeline([
    ('selector', DataFrameSelector()),  # Select columns (if needed, otherwise remove this step)
    ('preprocessor', preprocessor),
    ('svc', SVC(kernel='poly', degree=3, C=0.1))
])

# Separate features and target
X = heart_df.drop('HeartDisease', axis=1)
y = heart_df['HeartDisease']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Define the range of parameter values to test for gamma
param_range = np.logspace(-4, 2, 7)

# Calculate the validation curve
train_scores, test_scores = validation_curve(
    pipeline, X_train, y_train, param_name="svc__gamma", param_range=param_range,
    scoring="f1", n_jobs=-1, cv=5
)

# Calculate mean and standard deviation of the training and test scores
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

# Plot the validation curve
plt.figure()
plt.title("Validation Curve with SVM (Polynomial Kernel, Degree=3, C=0.1)")
plt.xlabel("gamma")
plt.ylabel("F1 Score")
plt.ylim(0.0, 1.1)
plt.semilogx(param_range, train_scores_mean, label="Training score", color="b")
plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color="b")
plt.semilogx(param_range, test_scores_mean, label="Cross-validation score", color="g")
plt.fill_between(param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color="g")
plt.legend(loc="best")
plt.grid(True)
plt.show()



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC

# Load the dataset
heart_df = pd.read_csv('heart.csv')

# Define the categorical and numerical columns
categorical_features = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']
numerical_features = ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak']

# Define the preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(), categorical_features)
    ])

# Define the SVM model with optimal parameters
svm_model = SVC(kernel='poly', degree=3, gamma=0.1, C=0.1)

# Create the full pipeline
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('svc', svm_model)
])

# Separate features and target
X = heart_df.drop('HeartDisease', axis=1)
y = heart_df['HeartDisease']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Calculate the learning curve
train_sizes, train_scores, test_scores = learning_curve(
    pipeline, X_train, y_train, cv=5, scoring='f1', n_jobs=-1,
    train_sizes=np.linspace(0.1, 1.0, 10), random_state=42
)

# Calculate mean and standard deviation of the training and test scores
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)

# Plot the learning curve
plt.figure()
plt.title("Learning Curve for SVM (Polynomial Kernel, Degree=3, Gamma=0.1, C=0.1)")
plt.xlabel("Number of Training Samples")
plt.ylabel("F1 Score")
plt.ylim(0.0, 1.1)
plt.plot(train_sizes, train_scores_mean, color="b", label="Training score")
plt.plot(train_sizes, test_scores_mean, color="g", label="Cross-validation score")
plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color="g")
plt.legend(loc="best")
plt.grid(True)
plt.show()



from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.svm import SVC
# Define the SVM model with optimal parameters

svm = SVC(kernel='poly', degree=3, gamma=0.1, C=0.1)

# Define the pipeline with preprocessing and SVM
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('svc', svm)
])

# Train the model on the training data
pipeline.fit(X_train, y_train)

# Make predictions on the test data
y_pred = pipeline.predict(X_test)

# Calculate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Display the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix for SVM (Polynomial Kernel, Degree=3, Gamma=0.1, C=0.1)')
plt.show()



