import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import classification_report
from sklearn.neural_network import MLPClassifier
import matplotlib.pyplot as plt

from sklearn.model_selection import RandomizedSearchCV
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report


# Load the dataset
heart_df = pd.read_csv('heart.csv')

# Display the first few rows of the dataset
heart_df.head()



# Check for missing values
missing_values = heart_df.isnull().sum()
print("Missing values in each column:\n", missing_values)



# Define the categorical and numerical columns
categorical_features = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']
numerical_features = ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak']

# Define the preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(), categorical_features)
    ])

# Separate features and target
X = heart_df.drop('HeartDisease', axis=1)
y = heart_df['HeartDisease']

# Apply the preprocessing pipeline to the features
X_preprocessed = preprocessor.fit_transform(X)



# Apply the preprocessing pipeline to the features and convert to DataFrame for better visualization
X_preprocessed_df = pd.DataFrame(X_preprocessed, columns=preprocessor.get_feature_names_out())

# Display the first few rows of the preprocessed data
print("First few rows of the preprocessed data:\n", X_preprocessed_df.head())



# Check the class distribution of the target variable
class_distribution = y.value_counts(normalize=True) * 100
print("Class distribution (%):\n", class_distribution)

# Visualize the class distribution
plt.bar(class_distribution.index, class_distribution.values, tick_label=['No Disease', 'Heart Disease'])
plt.xlabel('Class')
plt.ylabel('Percentage')
plt.title('Class Distribution in Heart Disease Dataset')
plt.show()



# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42, stratify=y)



# Define the hyperparameter space for RandomizedSearchCV
param_dist = {
    'hidden_layer_sizes': [(50,), (100,), (150,), (50, 50), (100, 50), (100, 100)],
    'activation': ['tanh', 'relu'],
    'solver': ['sgd', 'adam'],
    'alpha': [0.0001, 0.001, 0.01, 0.1],
    'learning_rate': ['constant', 'adaptive'],
    'max_iter': [3000],
}

# Initialize the MLPClassifier
mlp = MLPClassifier(random_state=42)

# Set up the RandomizedSearchCV
random_search = RandomizedSearchCV(
    mlp, param_distributions=param_dist, n_iter=100, cv=5, random_state=42, n_jobs=-1, verbose=2
)



# Fit the random search on the training data
random_search.fit(X_train, y_train)

# Print the best parameters found by RandomizedSearchCV
print("Best parameters found: ", random_search.best_params_)



import matplotlib.pyplot as plt
from sklearn.model_selection import validation_curve
from sklearn.metrics import make_scorer, f1_score

# Define the parameter range for alpha
alpha_range = np.logspace(-3, 1, 30)

# Create a scorer for F1 Score
f1_scorer = make_scorer(f1_score, average='weighted')

# Validation curve for alpha
train_scores, valid_scores = validation_curve(
    MLPClassifier(solver='sgd', max_iter=2500, learning_rate='constant', hidden_layer_sizes=(100, 50), activation='relu', random_state=42),
    X_train, y_train,
    param_name='alpha',
    param_range=alpha_range,
    cv=5,
    scoring=f1_scorer,
    n_jobs=-1
)

# Calculate mean and standard deviation for training and validation scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
valid_mean = np.mean(valid_scores, axis=1)
valid_std = np.std(valid_scores, axis=1)

# Plot the validation curve
plt.figure(figsize=(10, 6))
plt.plot(alpha_range, train_mean, label='Training score', color='blue')
plt.plot(alpha_range, valid_mean, label='Cross-validation score', color='green')
plt.fill_between(alpha_range, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')
plt.fill_between(alpha_range, valid_mean - valid_std, valid_mean + valid_std, alpha=0.2, color='green')
plt.xscale('log')
plt.title('Validation Curve for MLPClassifier (alpha)')
plt.xlabel('Alpha')
plt.ylabel('F1 Score')
plt.legend(loc='best')
plt.grid(True)
plt.show()



# Define the parameter range for hidden_layer_sizes
hidden_layer_sizes_range = [
    (50,), (100,), (150,), 
    (50, 50), (100, 50), (100, 100), (150, 100), 
    (50, 50, 50), (100, 100, 100), (150, 150, 150), 
    (50, 50, 50, 50), (100, 100, 100, 100), (150, 150, 150, 150)
]
hidden_layer_sizes_labels = [str(hls) for hls in hidden_layer_sizes_range]

# Create a scorer for F1 Score
f1_scorer = make_scorer(f1_score, average='weighted')

# Validation curve for hidden_layer_sizes
train_scores, valid_scores = validation_curve(
    MLPClassifier(solver='sgd', max_iter=4000, learning_rate='constant', alpha=0.1, activation='relu', random_state=42, verbose=True),
    X_train, y_train,
    param_name='hidden_layer_sizes',
    param_range=hidden_layer_sizes_range,
    cv=5,
    scoring=f1_scorer,
    n_jobs=-1
)

# Calculate mean and standard deviation for training and validation scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
valid_mean = np.mean(valid_scores, axis=1)
valid_std = np.std(valid_scores, axis=1)

# Plot the validation curve
plt.figure(figsize=(12, 8))
plt.plot(range(len(hidden_layer_sizes_range)), train_mean, label='Training score', color='blue')
plt.plot(range(len(hidden_layer_sizes_range)), valid_mean, label='Cross-validation score', color='green')
plt.fill_between(range(len(hidden_layer_sizes_range)), train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')
plt.fill_between(range(len(hidden_layer_sizes_range)), valid_mean - valid_std, valid_mean + valid_std, alpha=0.2, color='green')
plt.xticks(range(len(hidden_layer_sizes_labels)), hidden_layer_sizes_labels, rotation=45, ha='right')
plt.title('Validation Curve for MLPClassifier (hidden_layer_sizes)')
plt.xlabel('Hidden Layer Sizes')
plt.ylabel('F1 Score')
plt.legend(loc='best')
plt.grid(True)
plt.tight_layout()
plt.show()



# Define the parameter range for activation
activation_range = ['logistic', 'tanh', 'relu']

# Create a scorer for F1 Score
f1_scorer = make_scorer(f1_score, average='weighted')

# Validation curve for activation
train_scores, valid_scores = validation_curve(
    MLPClassifier(solver='sgd', max_iter=4000, learning_rate='constant', alpha=0.1, hidden_layer_sizes=(100, 50), random_state=42, verbose=True),
    X_train, y_train,
    param_name='activation',
    param_range=activation_range,
    cv=5,
    scoring=f1_scorer,
    n_jobs=-1
)

# Calculate mean and standard deviation for training and validation scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
valid_mean = np.mean(valid_scores, axis=1)
valid_std = np.std(valid_scores, axis=1)

# Plot the validation curve
plt.figure(figsize=(10, 6))
plt.plot(activation_range, train_mean, label='Training score', color='blue')
plt.plot(activation_range, valid_mean, label='Cross-validation score', color='green')
plt.fill_between(activation_range, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')
plt.fill_between(activation_range, valid_mean - valid_std, valid_mean + valid_std, alpha=0.2, color='green')
plt.title('Validation Curve for MLPClassifier (activation)')
plt.xlabel('Activation Function')
plt.ylabel('F1 Score')
plt.legend(loc='best')
plt.grid(True)
plt.show()



from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split, cross_val_score

# Define the model with the tuned hyperparameters
mlp = MLPClassifier(solver='sgd', max_iter=1, learning_rate='constant', alpha=0.1,
                    hidden_layer_sizes=(100, 50), activation='relu', warm_start=True, random_state=42)

# Initialize lists to store scores
train_scores = []
cv_scores = []

# Define the number of epochs
epochs = np.arange(1, 101)

# Cross-validation setup
cv = StratifiedKFold(n_splits=5)

# Loop over the number of epochs
for epoch in epochs:
    mlp.max_iter = epoch  # Set the current number of epochs
    mlp.fit(X_train, y_train)  # Fit the model on the training data
    
    # Calculate training score
    train_score = f1_score(y_train, mlp.predict(X_train), average='weighted')
    train_scores.append(train_score)
    
    # Calculate cross-validation score
    cv_score = cross_val_score(mlp, X_train, y_train, cv=cv, scoring=f1_scorer).mean()
    cv_scores.append(cv_score)

# Plot the learning curve
plt.figure(figsize=(10, 6))
plt.plot(epochs, train_scores, label='Training score', color='blue')
plt.plot(epochs, cv_scores, label='Cross-validation score', color='green')
plt.xlabel('Epochs')
plt.ylabel('F1 Score')
plt.title('Learning Curve for MLPClassifier')
plt.legend(loc='best')
plt.grid(True)
plt.show()



# Define the model with the tuned hyperparameters
mlp = MLPClassifier(solver='sgd', max_iter=2000, learning_rate='constant', alpha=0.1,
                    hidden_layer_sizes=(100, 50), activation='relu', random_state=42)

# Initialize lists to store scores
train_scores = []
cv_scores = []

# Define the number of epochs
epochs = np.arange(1, 401, 20)

# Cross-validation setup
cv = StratifiedKFold(n_splits=5)

# Create a scorer for F1 Score
f1_scorer = make_scorer(f1_score, average='weighted')

# Loop over the number of epochs
for epoch in epochs:
    mlp.set_params(max_iter=epoch)  # Set the current number of epochs
    mlp.fit(X_train, y_train)  # Fit the model on the training data
    
    # Calculate training score
    train_score = f1_score(y_train, mlp.predict(X_train), average='weighted')
    train_scores.append(train_score)
    
    # Calculate cross-validation score
    cv_score = cross_val_score(mlp, X_train, y_train, cv=cv, scoring=f1_scorer).mean()
    cv_scores.append(cv_score)

# Plot the learning curve
plt.figure(figsize=(10, 6))
plt.plot(epochs, train_scores, label='Training score', color='blue')
plt.plot(epochs, cv_scores, label='Cross-validation score', color='green')
plt.xlabel('Epochs')
plt.ylabel('F1 Score')
plt.title('Learning Curve for MLPClassifier')
plt.legend(loc='best')
plt.grid(True)
plt.show()



from sklearn.model_selection import learning_curve

# Define the model with the tuned hyperparameters
mlp = MLPClassifier(solver='sgd', max_iter=4000, learning_rate='constant', alpha=0.1,
                    hidden_layer_sizes=(100, 50), activation='relu', random_state=42)

# Create a scorer for F1 Score
f1_scorer = make_scorer(f1_score, average='weighted')

# Calculate learning curve
train_sizes, train_scores, cv_scores = learning_curve(
    mlp, X_train, y_train, cv=5, scoring=f1_scorer,
    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1)

# Calculate mean and standard deviation for training and validation scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
cv_mean = np.mean(cv_scores, axis=1)
cv_std = np.std(cv_scores, axis=1)

# Plot the learning curve
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_mean, label='Training score', color='blue')
plt.plot(train_sizes, cv_mean, label='Cross-validation score', color='green')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')
plt.fill_between(train_sizes, cv_mean - cv_std, cv_mean + cv_std, alpha=0.2, color='green')
plt.xlabel('Number of Training Samples')
plt.ylabel('F1 Score')
plt.title('Learning Curve for MLPClassifier')
plt.legend(loc='best')
plt.grid(True)
plt.show()



# Define the model with the tuned hyperparameters
mlp = MLPClassifier(solver='sgd', max_iter=2000, learning_rate='constant', alpha=0.1,
                    hidden_layer_sizes=(100, 50), activation='relu', random_state=42, verbose=True)

# Fit the model on the training data
mlp.fit(X_train, y_train)

# Plot the loss curve
plt.figure(figsize=(10, 6))
plt.plot(mlp.loss_curve_, label='Training loss', color='blue')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss Curve for MLPClassifier')
plt.legend(loc='best')
plt.grid(True)
plt.show()



# Define the model with the tuned hyperparameters
mlp = MLPClassifier(solver='sgd', learning_rate='constant', alpha=0.1,
                    hidden_layer_sizes=(100, 50), activation='relu', random_state=42, warm_start=True)

# Initialize lists to store scores and loss values
train_losses = []
cv_scores = []

# Define the number of epochs
epochs = 100

# Cross-validation setup
cv = StratifiedKFold(n_splits=5)
f1_scorer = make_scorer(f1_score, average='weighted')

# Loop over the number of epochs
for epoch in range(1, epochs + 1):
    mlp.set_params(max_iter=epoch)  # Increment the number of epochs
    mlp.fit(X_train, y_train)  # Fit the model on the training data
    
    # Record the training loss
    train_losses.append(mlp.loss_)
    
    # Calculate the cross-validation score
    cv_score = cross_val_score(mlp, X_train, y_train, cv=cv, scoring=f1_scorer).mean()
    cv_scores.append(cv_score)

# Plot the training loss and cross-validation score
plt.figure(figsize=(10, 6))
plt.plot(range(1, epochs + 1), train_losses, label='Training loss', color='blue')
plt.plot(range(1, epochs + 1), cv_scores, label='Cross-validation score', color='green')
plt.xlabel('Epochs')
plt.ylabel('Loss / F1 Score')
plt.title('Training Loss and Cross-Validation Score Curve for MLPClassifier')
plt.legend(loc='best')
plt.grid(True)
plt.show()




