{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "036cdeb2-77f9-4e51-a55b-b2aad51aecc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   baseline value  accelerations  fetal_movement  uterine_contractions  \\\n",
      "0             132          0.006             0.0                 0.006   \n",
      "1             133          0.003             0.0                 0.008   \n",
      "2             134          0.003             0.0                 0.008   \n",
      "3             132          0.007             0.0                 0.008   \n",
      "4             134          0.001             0.0                 0.010   \n",
      "\n",
      "   light_decelerations  severe_decelerations  prolongued_decelerations  \\\n",
      "0                0.003                   0.0                     0.000   \n",
      "1                0.003                   0.0                     0.000   \n",
      "2                0.003                   0.0                     0.000   \n",
      "3                0.000                   0.0                     0.000   \n",
      "4                0.009                   0.0                     0.002   \n",
      "\n",
      "   abnormal_short_term_variability  mean_value_of_short_term_variability  \\\n",
      "0                               17                                   2.1   \n",
      "1                               16                                   2.1   \n",
      "2                               16                                   2.4   \n",
      "3                               16                                   2.4   \n",
      "4                               26                                   5.9   \n",
      "\n",
      "   percentage_of_time_with_abnormal_long_term_variability  ...  histogram_min  \\\n",
      "0                                                  0       ...             68   \n",
      "1                                                  0       ...             68   \n",
      "2                                                  0       ...             53   \n",
      "3                                                  0       ...             53   \n",
      "4                                                  0       ...             50   \n",
      "\n",
      "   histogram_max  histogram_number_of_peaks  histogram_number_of_zeroes  \\\n",
      "0            198                          6                           1   \n",
      "1            198                          5                           1   \n",
      "2            170                         11                           0   \n",
      "3            170                          9                           0   \n",
      "4            200                          5                           3   \n",
      "\n",
      "   histogram_mode  histogram_mean  histogram_median  histogram_variance  \\\n",
      "0             141             136               140                  12   \n",
      "1             141             135               138                  13   \n",
      "2             137             134               137                  13   \n",
      "3             137             136               138                  11   \n",
      "4              76             107               107                 170   \n",
      "\n",
      "   histogram_tendency  fetal_health  \n",
      "0                   0             0  \n",
      "1                   0             0  \n",
      "2                   1             0  \n",
      "3                   1             0  \n",
      "4                   0             1  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "Missing values in each column:\n",
      " baseline value                                            0\n",
      "accelerations                                             0\n",
      "fetal_movement                                            0\n",
      "uterine_contractions                                      0\n",
      "light_decelerations                                       0\n",
      "severe_decelerations                                      0\n",
      "prolongued_decelerations                                  0\n",
      "abnormal_short_term_variability                           0\n",
      "mean_value_of_short_term_variability                      0\n",
      "percentage_of_time_with_abnormal_long_term_variability    0\n",
      "mean_value_of_long_term_variability                       0\n",
      "histogram_width                                           0\n",
      "histogram_min                                             0\n",
      "histogram_max                                             0\n",
      "histogram_number_of_peaks                                 0\n",
      "histogram_number_of_zeroes                                0\n",
      "histogram_mode                                            0\n",
      "histogram_mean                                            0\n",
      "histogram_median                                          0\n",
      "histogram_variance                                        0\n",
      "histogram_tendency                                        0\n",
      "fetal_health                                              0\n",
      "dtype: int64\n",
      "Class distribution (%):\n",
      " fetal_health\n",
      "0    90.387766\n",
      "1     9.612234\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Load the dataset\n",
    "fetal_health_df = pd.read_csv('fetal_health.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(fetal_health_df.head())\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = fetal_health_df.isnull().sum()\n",
    "print(\"Missing values in each column:\\n\", missing_values)\n",
    "\n",
    "# Define the numerical columns (all columns except 'fetal_health')\n",
    "numerical_features = fetal_health_df.columns.difference(['fetal_health']).tolist()\n",
    "\n",
    "# Define the preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features)\n",
    "    ])\n",
    "\n",
    "# Separate features and target\n",
    "X = fetal_health_df.drop('fetal_health', axis=1)\n",
    "y = fetal_health_df['fetal_health']\n",
    "\n",
    "# Apply the preprocessing pipeline to the features\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Check the class distribution of the target variable\n",
    "class_distribution = y.value_counts(normalize=True) * 100\n",
    "print(\"Class distribution (%):\\n\", class_distribution)\n",
    "\n",
    "# Split the data into training and validation sets (80% training, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aac2e7-5113-464f-83d8-486d7e5cd14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "\n",
    "# Define the parameter range for alpha\n",
    "alpha_range = np.logspace(-3, 1, 30)\n",
    "\n",
    "# Create a scorer for Recall\n",
    "recall_scorer = make_scorer(recall_score, average='weighted')\n",
    "\n",
    "# Validation curve for alpha\n",
    "train_scores, valid_scores = validation_curve(\n",
    "    MLPClassifier(solver='sgd', max_iter=2500, learning_rate='constant', hidden_layer_sizes=(100, 50), activation='relu', random_state=42),\n",
    "    X_train, y_train,\n",
    "    param_name='alpha',\n",
    "    param_range=alpha_range,\n",
    "    cv=5,\n",
    "    scoring=recall_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate mean and standard deviation for training and validation scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "valid_mean = np.mean(valid_scores, axis=1)\n",
    "valid_std = np.std(valid_scores, axis=1)\n",
    "\n",
    "# Plot the validation curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alpha_range, train_mean, label='Training score', color='blue')\n",
    "plt.plot(alpha_range, valid_mean, label='Cross-validation score', color='green')\n",
    "plt.fill_between(alpha_range, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n",
    "plt.fill_between(alpha_range, valid_mean - valid_std, valid_mean + valid_std, alpha=0.2, color='green')\n",
    "plt.xscale('log')\n",
    "plt.title('Validation Curve for MLPClassifier (alpha)')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Recall Score')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4887e173-3a05-4385-874e-1421e2f500b0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.07751010\n",
      "Iteration 2, loss = 1.00905011\n",
      "Iteration 3, loss = 0.91837724\n",
      "Iteration 4, loss = 0.82660543\n",
      "Iteration 5, loss = 0.74206471\n",
      "Iteration 6, loss = 0.67034712\n",
      "Iteration 7, loss = 0.60902833\n",
      "Iteration 8, loss = 0.55935622\n",
      "Iteration 9, loss = 0.51759295\n",
      "Iteration 1, loss = 0.69421570\n",
      "Iteration 10, loss = 0.48330039\n",
      "Iteration 11, loss = 0.45374023\n",
      "Iteration 12, loss = 0.42876468\n",
      "Iteration 2, loss = 0.66197493\n",
      "Iteration 13, loss = 0.40761606\n",
      "Iteration 3, loss = 0.62001671\n",
      "Iteration 14, loss = 0.38894909\n",
      "Iteration 1, loss = 0.69510285\n",
      "Iteration 15, loss = 0.37288236\n",
      "Iteration 4, loss = 0.57670824\n",
      "Iteration 16, loss = 0.35812439\n",
      "Iteration 17, loss = 0.34540290\n",
      "Iteration 5, loss = 0.53656689\n",
      "Iteration 2, loss = 0.65974571\n",
      "Iteration 18, loss = 0.33369889\n",
      "Iteration 6, loss = 0.50101753\n",
      "Iteration 19, loss = 0.32302970\n",
      "Iteration 20, loss = 0.31325803\n",
      "Iteration 3, loss = 0.61423391\n",
      "Iteration 7, loss = 0.47066013\n",
      "Iteration 1, loss = 0.71258382\n",
      "Iteration 21, loss = 0.30468798\n",
      "Iteration 22, loss = 0.29635855\n",
      "Iteration 2, loss = 0.68061867\n",
      "Iteration 8, loss = 0.44463309\n",
      "Iteration 23, loss = 0.28880631\n",
      "Iteration 4, loss = 0.56792573\n",
      "Iteration 9, loss = 0.42200015\n",
      "Iteration 24, loss = 0.28181626\n",
      "Iteration 1, loss = 0.57450853\n",
      "Iteration 25, loss = 0.27526712\n",
      "Iteration 10, loss = 0.40244728\n",
      "Iteration 3, loss = 0.63864001\n",
      "Iteration 26, loss = 0.26928314\n",
      "Iteration 5, loss = 0.52617510\n",
      "Iteration 2, loss = 0.56034914\n",
      "Iteration 27, loss = 0.26360535\n",
      "Iteration 11, loss = 0.38548378\n",
      "Iteration 4, loss = 0.59605260\n",
      "Iteration 6, loss = 0.49012066\n",
      "Iteration 12, loss = 0.37041386\n",
      "Iteration 3, loss = 0.54102938\n",
      "Iteration 13, loss = 0.35696445\n",
      "Iteration 5, loss = 0.55697656\n",
      "Iteration 7, loss = 0.45998899\n",
      "Iteration 6, loss = 0.52292024\n",
      "Iteration 4, loss = 0.52016416\n",
      "Iteration 14, loss = 0.34492482\n",
      "Iteration 7, loss = 0.49393273\n",
      "Iteration 28, loss = 0.25820325\n",
      "Iteration 8, loss = 0.43428158\n",
      "Iteration 8, loss = 0.46888519\n",
      "Iteration 29, loss = 0.25333990\n",
      "Iteration 5, loss = 0.49959193\n",
      "Iteration 15, loss = 0.33398834\n",
      "Iteration 30, loss = 0.24860863\n",
      "Iteration 9, loss = 0.41232409\n",
      "Iteration 9, loss = 0.44775648\n",
      "Iteration 31, loss = 0.24411402\n",
      "Iteration 16, loss = 0.32406968\n",
      "Iteration 32, loss = 0.24000665\n",
      "Iteration 1, loss = 0.74542181\n",
      "Iteration 10, loss = 0.39390664\n",
      "Iteration 33, loss = 0.23597057\n",
      "Iteration 10, loss = 0.42933880\n",
      "Iteration 17, loss = 0.31487780\n",
      "Iteration 11, loss = 0.41323467\n",
      "Iteration 34, loss = 0.23222161\n",
      "Iteration 1, loss = 0.68695197\n",
      "Iteration 18, loss = 0.30652746\n",
      "Iteration 11, loss = 0.37781269\n",
      "Iteration 35, loss = 0.22867467\n",
      "Iteration 12, loss = 0.39911918\n",
      "Iteration 19, loss = 0.29869876\n",
      "Iteration 6, loss = 0.48020760\n",
      "Iteration 36, loss = 0.22528950\n",
      "Iteration 37, loss = 0.22205363\n",
      "Iteration 20, loss = 0.29149546\n",
      "Iteration 38, loss = 0.21899285\n",
      "Iteration 2, loss = 0.70934717\n",
      "Iteration 12, loss = 0.36373689\n",
      "Iteration 21, loss = 0.28462103\n",
      "Iteration 13, loss = 0.38629719\n",
      "Iteration 22, loss = 0.27826506\n",
      "Iteration 39, loss = 0.21602444Iteration 7, loss = 0.46233047\n",
      "\n",
      "Iteration 13, loss = 0.35116790\n",
      "Iteration 2, loss = 0.66576037\n",
      "Iteration 14, loss = 0.37508846\n",
      "Iteration 1, loss = 0.79552870\n",
      "Iteration 3, loss = 0.63684416\n",
      "Iteration 3, loss = 0.66226148\n",
      "Iteration 4, loss = 0.60702408\n",
      "Iteration 8, loss = 0.44590441\n",
      "Iteration 40, loss = 0.21319195\n",
      "Iteration 14, loss = 0.33986390\n",
      "Iteration 41, loss = 0.21051269Iteration 5, loss = 0.57873114\n",
      "Iteration 9, loss = 0.43055715\n",
      "Iteration 2, loss = 0.75656183\n",
      "Iteration 6, loss = 0.55375483\n",
      "Iteration 23, loss = 0.27232139\n",
      "\n",
      "Iteration 4, loss = 0.61486900\n",
      "Iteration 15, loss = 0.36459441\n",
      "Iteration 10, loss = 0.41677848\n",
      "Iteration 42, loss = 0.20789537Iteration 16, loss = 0.35489301\n",
      "\n",
      "Iteration 7, loss = 0.53118475\n",
      "Iteration 17, loss = 0.34604624\n",
      "Iteration 43, loss = 0.20541608\n",
      "Iteration 44, loss = 0.20306527\n",
      "Iteration 18, loss = 0.33791192\n",
      "Iteration 11, loss = 0.40358145\n",
      "Iteration 8, loss = 0.51122070\n",
      "Iteration 5, loss = 0.57196514\n",
      "Iteration 3, loss = 0.70591236\n",
      "Iteration 9, loss = 0.49361840\n",
      "Iteration 19, loss = 0.32997439\n",
      "Iteration 10, loss = 0.47772654\n",
      "Iteration 12, loss = 0.39138760\n",
      "Iteration 15, loss = 0.32966615\n",
      "Iteration 20, loss = 0.32260288\n",
      "Iteration 24, loss = 0.26668968\n",
      "Iteration 6, loss = 0.53441945\n",
      "Iteration 21, loss = 0.31592227\n",
      "Iteration 25, loss = 0.26146215\n",
      "Iteration 13, loss = 0.38014554\n",
      "Iteration 4, loss = 0.65477094\n",
      "Iteration 26, loss = 0.25641609\n",
      "Iteration 11, loss = 0.46413090\n",
      "Iteration 27, loss = 0.25160564\n",
      "Iteration 22, loss = 0.30932251\n",
      "Iteration 28, loss = 0.24719812\n",
      "Iteration 23, loss = 0.30291407\n",
      "Iteration 16, loss = 0.32032732\n",
      "Iteration 14, loss = 0.36934058\n",
      "Iteration 7, loss = 0.50302399\n",
      "Iteration 45, loss = 0.20074537\n",
      "Iteration 15, loss = 0.35925653\n",
      "Iteration 12, loss = 0.45149521\n",
      "Iteration 16, loss = 0.34952539\n",
      "Iteration 24, loss = 0.29707822\n",
      "Iteration 8, loss = 0.47597323\n",
      "Iteration 17, loss = 0.34056947\n",
      "Iteration 13, loss = 0.44039844\n",
      "Iteration 17, loss = 0.31161782\n",
      "Iteration 5, loss = 0.60853918\n",
      "Iteration 18, loss = 0.30372985\n",
      "Iteration 19, loss = 0.29617633\n",
      "Iteration 46, loss = 0.19855525\n",
      "Iteration 14, loss = 0.43007530\n",
      "Iteration 47, loss = 0.19642334\n",
      "Iteration 48, loss = 0.19437568\n",
      "Iteration 29, loss = 0.24292717Iteration 49, loss = 0.19242982\n",
      "Iteration 50, loss = 0.19049997\n",
      "Iteration 15, loss = 0.42063146\n",
      "Iteration 51, loss = 0.18864607\n",
      "Iteration 6, loss = 0.56895940\n",
      "Iteration 52, loss = 0.18688195\n",
      "\n",
      "Iteration 16, loss = 0.41208221\n",
      "Iteration 20, loss = 0.28924305\n",
      "Iteration 53, loss = 0.18514627\n",
      "Iteration 25, loss = 0.29119729\n",
      "Iteration 17, loss = 0.40401824\n",
      "Iteration 7, loss = 0.53465464\n",
      "Iteration 21, loss = 0.28274060\n",
      "Iteration 18, loss = 0.39652624\n",
      "Iteration 9, loss = 0.45323477\n",
      "Iteration 26, loss = 0.28582097\n",
      "Iteration 54, loss = 0.18347210\n",
      "Iteration 19, loss = 0.38952314\n",
      "Iteration 30, loss = 0.23884607\n",
      "Iteration 55, loss = 0.18183084\n",
      "Iteration 56, loss = 0.18026621\n",
      "Iteration 22, loss = 0.27661106\n",
      "Iteration 18, loss = 0.33175092\n",
      "Iteration 23, loss = 0.27083354\n",
      "Iteration 20, loss = 0.38295833\n",
      "Iteration 24, loss = 0.26536038\n",
      "Iteration 25, loss = 0.26013109\n",
      "Iteration 21, loss = 0.37670803\n",
      "Iteration 8, loss = 0.50533542\n",
      "Iteration 57, loss = 0.17875110\n",
      "Iteration 58, loss = 0.17724277\n",
      "Iteration 59, loss = 0.17579099\n",
      "Iteration 31, loss = 0.23497688\n",
      "Iteration 26, loss = 0.25518976\n",
      "Iteration 27, loss = 0.25046523\n",
      "Iteration 27, loss = 0.28048540\n",
      "Iteration 9, loss = 0.48048855\n",
      "Iteration 10, loss = 0.43369076\n",
      "Iteration 22, loss = 0.37082352\n",
      "Iteration 28, loss = 0.27539043\n",
      "Iteration 60, loss = 0.17441711\n",
      "Iteration 29, loss = 0.27055841\n",
      "Iteration 32, loss = 0.23130020\n",
      "Iteration 19, loss = 0.32338269\n",
      "Iteration 30, loss = 0.26586114\n",
      "Iteration 33, loss = 0.22771298\n",
      "Iteration 11, loss = 0.41672008\n",
      "Iteration 31, loss = 0.26119960\n",
      "Iteration 34, loss = 0.22431264\n",
      "Iteration 32, loss = 0.25673605\n",
      "Iteration 35, loss = 0.22115273\n",
      "Iteration 36, loss = 0.21802816\n",
      "Iteration 37, loss = 0.21502417\n",
      "Iteration 20, loss = 0.31537317\n",
      "Iteration 38, loss = 0.21222787\n",
      "Iteration 39, loss = 0.20941617\n",
      "Iteration 40, loss = 0.20680797\n",
      "Iteration 21, loss = 0.30786810\n",
      "Iteration 41, loss = 0.20428406\n",
      "Iteration 61, loss = 0.17304988\n",
      "Iteration 62, loss = 0.17170651\n",
      "Iteration 63, loss = 0.17043800\n",
      "Iteration 64, loss = 0.16916680\n",
      "Iteration 28, loss = 0.24608258\n",
      "Iteration 65, loss = 0.16793037\n",
      "Iteration 66, loss = 0.16676733\n",
      "Iteration 42, loss = 0.20179013\n",
      "Iteration 67, loss = 0.16557708\n",
      "Iteration 68, loss = 0.16444218\n",
      "Iteration 69, loss = 0.16334659\n",
      "Iteration 29, loss = 0.24173807\n",
      "Iteration 70, loss = 0.16226574\n",
      "Iteration 71, loss = 0.16120865\n",
      "Iteration 43, loss = 0.19948591\n",
      "Iteration 72, loss = 0.16017289\n",
      "Iteration 33, loss = 0.25258597\n",
      "Iteration 12, loss = 0.40182189\n",
      "Iteration 30, loss = 0.23765890\n",
      "Iteration 34, loss = 0.24827951\n",
      "Iteration 44, loss = 0.19714628\n",
      "Iteration 23, loss = 0.36501129\n",
      "Iteration 45, loss = 0.19498673\n",
      "Iteration 31, loss = 0.23374009\n",
      "Iteration 46, loss = 0.19283261\n",
      "Iteration 10, loss = 0.45882896\n",
      "Iteration 35, loss = 0.24438108\n",
      "Iteration 47, loss = 0.19073762\n",
      "Iteration 73, loss = 0.15916418Iteration 48, loss = 0.18873765\n",
      "Iteration 13, loss = 0.38854144\n",
      "\n",
      "Iteration 49, loss = 0.18680748\n",
      "Iteration 50, loss = 0.18492468\n",
      "Iteration 22, loss = 0.30060835\n",
      "Iteration 74, loss = 0.15815526\n",
      "Iteration 75, loss = 0.15719493\n",
      "Iteration 76, loss = 0.15624164\n",
      "Iteration 51, loss = 0.18308157\n",
      "Iteration 52, loss = 0.18130206\n",
      "Iteration 53, loss = 0.17959332\n",
      "Iteration 23, loss = 0.29360063\n",
      "Iteration 54, loss = 0.17790427\n",
      "Iteration 11, loss = 0.43936250\n",
      "Iteration 77, loss = 0.15530486\n",
      "Iteration 78, loss = 0.15439123\n",
      "Iteration 79, loss = 0.15350897\n",
      "Iteration 24, loss = 0.28688648\n",
      "Iteration 36, loss = 0.24051583\n",
      "Iteration 14, loss = 0.37661944\n",
      "Iteration 24, loss = 0.35957050\n",
      "Iteration 55, loss = 0.17625159\n",
      "Iteration 25, loss = 0.35435539\n",
      "Iteration 32, loss = 0.23002478\n",
      "Iteration 15, loss = 0.36557745\n",
      "Iteration 37, loss = 0.23678331\n",
      "Iteration 26, loss = 0.34920147\n",
      "Iteration 33, loss = 0.22644704\n",
      "Iteration 25, loss = 0.28042330\n",
      "Iteration 27, loss = 0.34447311\n",
      "Iteration 38, loss = 0.23318618\n",
      "Iteration 80, loss = 0.15261725\n",
      "Iteration 34, loss = 0.22293748\n",
      "Iteration 12, loss = 0.42219253\n",
      "Iteration 16, loss = 0.35557805\n",
      "Iteration 35, loss = 0.21962080\n",
      "Iteration 36, loss = 0.21643667\n",
      "Iteration 81, loss = 0.15178464\n",
      "Iteration 17, loss = 0.34642091\n",
      "Iteration 37, loss = 0.21339973\n",
      "Iteration 26, loss = 0.27435859\n",
      "Iteration 82, loss = 0.15093561\n",
      "Iteration 39, loss = 0.22950770\n",
      "Iteration 38, loss = 0.21046197\n",
      "Iteration 18, loss = 0.33770123\n",
      "Iteration 83, loss = 0.15009637\n",
      "Iteration 84, loss = 0.14928262\n",
      "Iteration 27, loss = 0.26852342\n",
      "Iteration 85, loss = 0.14849706\n",
      "Iteration 19, loss = 0.32957083\n",
      "Iteration 40, loss = 0.22613183\n",
      "Iteration 28, loss = 0.26282265\n",
      "Iteration 28, loss = 0.33965989\n",
      "Iteration 56, loss = 0.17469642\n",
      "Iteration 13, loss = 0.40671102\n",
      "Iteration 57, loss = 0.17311413\n",
      "Iteration 86, loss = 0.14772681\n",
      "Iteration 58, loss = 0.17160183\n",
      "Iteration 41, loss = 0.22284216\n",
      "Iteration 59, loss = 0.17016181\n",
      "Iteration 87, loss = 0.14693155\n",
      "Iteration 29, loss = 0.33514409\n",
      "Iteration 39, loss = 0.20752511Iteration 88, loss = 0.14619129\n",
      "\n",
      "Iteration 89, loss = 0.14543243\n",
      "Iteration 60, loss = 0.16872073\n",
      "Iteration 90, loss = 0.14473615\n",
      "Iteration 29, loss = 0.25750986\n",
      "Iteration 42, loss = 0.21960166\n",
      "Iteration 91, loss = 0.14401757\n",
      "Iteration 61, loss = 0.16733212\n",
      "Iteration 92, loss = 0.14329866\n",
      "Iteration 30, loss = 0.33056875\n",
      "Iteration 30, loss = 0.25235570\n",
      "Iteration 43, loss = 0.21640247\n",
      "Iteration 44, loss = 0.21352219\n",
      "Iteration 62, loss = 0.16595796\n",
      "Iteration 93, loss = 0.14261512\n",
      "Iteration 31, loss = 0.32629626\n",
      "Iteration 31, loss = 0.24738801\n",
      "Iteration 40, loss = 0.20477640\n",
      "Iteration 32, loss = 0.32182089\n",
      "Iteration 63, loss = 0.16465380\n",
      "Iteration 64, loss = 0.16335271\n",
      "Iteration 94, loss = 0.14192301\n",
      "Iteration 33, loss = 0.31761533\n",
      "Iteration 41, loss = 0.20211122\n",
      "Iteration 32, loss = 0.24271660\n",
      "Iteration 34, loss = 0.31348291\n",
      "Iteration 95, loss = 0.14125938\n",
      "Iteration 65, loss = 0.16211000\n",
      "Iteration 42, loss = 0.19950910\n",
      "Iteration 33, loss = 0.23814662\n",
      "Iteration 66, loss = 0.16085636\n",
      "Iteration 67, loss = 0.15965700\n",
      "Iteration 96, loss = 0.14059975\n",
      "Iteration 68, loss = 0.15847985\n",
      "Iteration 69, loss = 0.15734851\n",
      "Iteration 20, loss = 0.32193009\n",
      "Iteration 34, loss = 0.23377192\n",
      "Iteration 70, loss = 0.15621664\n",
      "Iteration 71, loss = 0.15512659\n",
      "Iteration 72, loss = 0.15406068\n",
      "Iteration 73, loss = 0.15299246\n",
      "Iteration 45, loss = 0.21049461\n",
      "Iteration 35, loss = 0.30933109\n",
      "Iteration 74, loss = 0.15197661\n",
      "Iteration 46, loss = 0.20753573\n",
      "Iteration 75, loss = 0.15096954Iteration 97, loss = 0.13994568\n",
      "Iteration 43, loss = 0.19703436\n",
      "Iteration 21, loss = 0.31466941\n",
      "Iteration 44, loss = 0.19459351\n",
      "Iteration 98, loss = 0.13931170\n",
      "Iteration 14, loss = 0.39244052\n",
      "\n",
      "Iteration 36, loss = 0.30527241\n",
      "Iteration 47, loss = 0.20483992\n",
      "Iteration 99, loss = 0.13868017\n",
      "Iteration 100, loss = 0.13805890\n",
      "Iteration 101, loss = 0.13743761\n",
      "Iteration 45, loss = 0.19233169\n",
      "Iteration 35, loss = 0.22951952\n",
      "Iteration 37, loss = 0.30112422\n",
      "Iteration 36, loss = 0.22561714\n",
      "Iteration 48, loss = 0.20222704\n",
      "Iteration 22, loss = 0.30780650\n",
      "Iteration 37, loss = 0.22167103\n",
      "Iteration 49, loss = 0.19951320\n",
      "Iteration 102, loss = 0.13683956\n",
      "Iteration 15, loss = 0.37941805\n",
      "Iteration 38, loss = 0.21789178\n",
      "Iteration 50, loss = 0.19697100\n",
      "Iteration 46, loss = 0.19003607\n",
      "Iteration 103, loss = 0.13625954\n",
      "Iteration 51, loss = 0.19453806\n",
      "Iteration 23, loss = 0.30127315\n",
      "Iteration 76, loss = 0.14998614\n",
      "Iteration 47, loss = 0.18786441\n",
      "Iteration 52, loss = 0.19214357\n",
      "Iteration 77, loss = 0.14901928\n",
      "Iteration 48, loss = 0.18571650\n",
      "Iteration 78, loss = 0.14808248\n",
      "Iteration 49, loss = 0.18367589\n",
      "Iteration 50, loss = 0.18172453\n",
      "Iteration 104, loss = 0.13567073\n",
      "Iteration 24, loss = 0.29490830\n",
      "Iteration 105, loss = 0.13508900\n",
      "Iteration 38, loss = 0.29724683\n",
      "Iteration 106, loss = 0.13451515\n",
      "Iteration 16, loss = 0.36727836\n",
      "Iteration 107, loss = 0.13395496\n",
      "Iteration 79, loss = 0.14714652\n",
      "Iteration 108, loss = 0.13340376\n",
      "Iteration 39, loss = 0.21442642\n",
      "Iteration 109, loss = 0.13284929\n",
      "Iteration 110, loss = 0.13232249\n",
      "Iteration 80, loss = 0.14625839\n",
      "Iteration 81, loss = 0.14534486\n",
      "Iteration 51, loss = 0.17976652\n",
      "Iteration 111, loss = 0.13179095\n",
      "Iteration 17, loss = 0.35588198\n",
      "Iteration 82, loss = 0.14447511\n",
      "Iteration 53, loss = 0.18984373\n",
      "Iteration 83, loss = 0.14364080\n",
      "Iteration 84, loss = 0.14278752\n",
      "Iteration 85, loss = 0.14196796\n",
      "Iteration 54, loss = 0.18758910\n",
      "Iteration 25, loss = 0.28883915\n",
      "Iteration 86, loss = 0.14115764\n",
      "Iteration 55, loss = 0.18541867\n",
      "Iteration 87, loss = 0.14034903\n",
      "Iteration 88, loss = 0.13959211\n",
      "Iteration 56, loss = 0.18322148\n",
      "Iteration 39, loss = 0.29322297\n",
      "Iteration 89, loss = 0.13880742\n",
      "Iteration 57, loss = 0.18118585\n",
      "Iteration 58, loss = 0.17915184\n",
      "Iteration 90, loss = 0.13804322\n",
      "Iteration 26, loss = 0.28310716\n",
      "Iteration 52, loss = 0.17788311\n",
      "Iteration 59, loss = 0.17720393\n",
      "Iteration 40, loss = 0.21098864\n",
      "Iteration 53, loss = 0.17605488\n",
      "Iteration 60, loss = 0.17522573\n",
      "Iteration 61, loss = 0.17338094\n",
      "Iteration 112, loss = 0.13127604\n",
      "Iteration 40, loss = 0.28937945\n",
      "Iteration 41, loss = 0.20766596\n",
      "Iteration 113, loss = 0.13074259\n",
      "Iteration 114, loss = 0.13023503\n",
      "Iteration 91, loss = 0.13730486\n",
      "Iteration 54, loss = 0.17424554Iteration 42, loss = 0.20451379\n",
      "\n",
      "Iteration 115, loss = 0.12972786\n",
      "Iteration 116, loss = 0.12923157\n",
      "Iteration 41, loss = 0.28543050\n",
      "Iteration 43, loss = 0.20149639\n",
      "Iteration 18, loss = 0.34525544\n",
      "Iteration 42, loss = 0.28167508\n",
      "Iteration 27, loss = 0.27754672\n",
      "Iteration 44, loss = 0.19861409\n",
      "Iteration 43, loss = 0.27786098\n",
      "Iteration 55, loss = 0.17254792\n",
      "Iteration 28, loss = 0.27215671\n",
      "Iteration 117, loss = 0.12875566\n",
      "Iteration 118, loss = 0.12826249Iteration 44, loss = 0.27422472\n",
      "\n",
      "Iteration 119, loss = 0.12779107\n",
      "Iteration 120, loss = 0.12732856\n",
      "Iteration 56, loss = 0.17087175\n",
      "Iteration 29, loss = 0.26697206\n",
      "Iteration 45, loss = 0.27054908\n",
      "Iteration 92, loss = 0.13658048\n",
      "Iteration 121, loss = 0.12684707\n",
      "Iteration 122, loss = 0.12639675\n",
      "Iteration 62, loss = 0.17156596\n",
      "Iteration 123, loss = 0.12595572\n",
      "Iteration 124, loss = 0.12549445\n",
      "Iteration 45, loss = 0.19582512\n",
      "Iteration 19, loss = 0.33525602\n",
      "Iteration 63, loss = 0.16983121\n",
      "Iteration 93, loss = 0.13586487\n",
      "Iteration 46, loss = 0.26686428\n",
      "Iteration 57, loss = 0.16924742\n",
      "Iteration 47, loss = 0.26339311\n",
      "Iteration 58, loss = 0.16764268\n",
      "Iteration 46, loss = 0.19306459\n",
      "Iteration 59, loss = 0.16610220\n",
      "Iteration 48, loss = 0.25979983\n",
      "Iteration 20, loss = 0.32579643\n",
      "Iteration 125, loss = 0.12504757\n",
      "Iteration 126, loss = 0.12460800\n",
      "Iteration 127, loss = 0.12418540\n",
      "Iteration 128, loss = 0.12374311\n",
      "Iteration 129, loss = 0.12333767\n",
      "Iteration 64, loss = 0.16808778\n",
      "Iteration 94, loss = 0.13516383\n",
      "Iteration 130, loss = 0.12290895\n",
      "Iteration 131, loss = 0.12250210\n",
      "Iteration 95, loss = 0.13448169\n",
      "Iteration 132, loss = 0.12207891\n",
      "Iteration 96, loss = 0.13378964\n",
      "Iteration 133, loss = 0.12167274\n",
      "Iteration 30, loss = 0.26191451\n",
      "Iteration 134, loss = 0.12127849\n",
      "Iteration 21, loss = 0.31693878\n",
      "Iteration 97, loss = 0.13312019\n",
      "Iteration 135, loss = 0.12087466\n",
      "Iteration 65, loss = 0.16637830\n",
      "Iteration 136, loss = 0.12049792\n",
      "Iteration 66, loss = 0.16477931\n",
      "Iteration 47, loss = 0.19050835\n",
      "Iteration 137, loss = 0.12008678\n",
      "Iteration 67, loss = 0.16319262\n",
      "Iteration 49, loss = 0.25618644\n",
      "Iteration 138, loss = 0.11971593\n",
      "Iteration 48, loss = 0.18797977\n",
      "Iteration 68, loss = 0.16160532\n",
      "Iteration 139, loss = 0.11931942\n",
      "Iteration 140, loss = 0.11895385\n",
      "Iteration 69, loss = 0.16015199\n",
      "Iteration 141, loss = 0.11857191\n",
      "Iteration 49, loss = 0.18567084\n",
      "Iteration 142, loss = 0.11819999\n",
      "Iteration 60, loss = 0.16461314\n",
      "Iteration 50, loss = 0.25272633\n",
      "Iteration 61, loss = 0.16309631\n",
      "Iteration 98, loss = 0.13246637\n",
      "Iteration 51, loss = 0.24950107\n",
      "Iteration 62, loss = 0.16167458\n",
      "Iteration 50, loss = 0.18330503\n",
      "Iteration 52, loss = 0.24606665\n",
      "Iteration 70, loss = 0.15863594\n",
      "Iteration 71, loss = 0.15718804\n",
      "Iteration 53, loss = 0.24269251\n",
      "Iteration 51, loss = 0.18105377\n",
      "Iteration 31, loss = 0.25716675\n",
      "Iteration 143, loss = 0.11783541\n",
      "Iteration 72, loss = 0.15578552\n",
      "Iteration 22, loss = 0.30851996\n",
      "Iteration 63, loss = 0.16028221\n",
      "Iteration 99, loss = 0.13181064\n",
      "Iteration 144, loss = 0.11746993\n",
      "Iteration 100, loss = 0.13119274\n",
      "Iteration 145, loss = 0.11713615\n",
      "Iteration 23, loss = 0.30060550\n",
      "Iteration 64, loss = 0.15891276\n",
      "Iteration 32, loss = 0.25258641\n",
      "Iteration 146, loss = 0.11675922\n",
      "Iteration 65, loss = 0.15756759\n",
      "Iteration 54, loss = 0.23941982\n",
      "Iteration 52, loss = 0.17892677Iteration 73, loss = 0.15446058\n",
      "Iteration 147, loss = 0.11640442\n",
      "\n",
      "Iteration 101, loss = 0.13054251\n",
      "Iteration 74, loss = 0.15307414\n",
      "Iteration 66, loss = 0.15629286\n",
      "Iteration 75, loss = 0.15176144\n",
      "Iteration 53, loss = 0.17684917\n",
      "Iteration 55, loss = 0.23612761\n",
      "Iteration 76, loss = 0.15049850\n",
      "Iteration 56, loss = 0.23294886\n",
      "Iteration 148, loss = 0.11605376\n",
      "Iteration 54, loss = 0.17482554\n",
      "Iteration 77, loss = 0.14923270\n",
      "Iteration 102, loss = 0.12993579\n",
      "Iteration 78, loss = 0.14804052Iteration 33, loss = 0.24799863\n",
      "Iteration 67, loss = 0.15500297\n",
      "Iteration 103, loss = 0.12932480\n",
      "Iteration 104, loss = 0.12872189\n",
      "Iteration 68, loss = 0.15374706\n",
      "\n",
      "Iteration 149, loss = 0.11572059\n",
      "Iteration 150, loss = 0.11537835\n",
      "Iteration 105, loss = 0.12814354\n",
      "Iteration 34, loss = 0.24372035\n",
      "Iteration 69, loss = 0.15253635\n",
      "Iteration 106, loss = 0.12754743\n",
      "Iteration 24, loss = 0.29293242\n",
      "Iteration 151, loss = 0.11504691\n",
      "Iteration 107, loss = 0.12697201\n",
      "Iteration 152, loss = 0.11471364\n",
      "Iteration 108, loss = 0.12642099\n",
      "Iteration 153, loss = 0.11439410\n",
      "Iteration 70, loss = 0.15135635\n",
      "Iteration 154, loss = 0.11405107\n",
      "Iteration 71, loss = 0.15018453\n",
      "Iteration 109, loss = 0.12585559\n",
      "Iteration 155, loss = 0.11373273\n",
      "Iteration 156, loss = 0.11341187\n",
      "Iteration 157, loss = 0.11310525\n",
      "Iteration 79, loss = 0.14684108\n",
      "Iteration 158, loss = 0.11278319\n",
      "Iteration 159, loss = 0.11246952\n",
      "Iteration 160, loss = 0.11216037\n",
      "Iteration 161, loss = 0.11186870\n",
      "Iteration 162, loss = 0.11156552\n",
      "Iteration 25, loss = 0.28571740\n",
      "Iteration 57, loss = 0.22984147\n",
      "Iteration 80, loss = 0.14563250\n",
      "Iteration 26, loss = 0.27896742\n",
      "Iteration 35, loss = 0.23954761Iteration 55, loss = 0.17286812\n",
      "\n",
      "Iteration 56, loss = 0.17100286\n",
      "Iteration 57, loss = 0.16920498\n",
      "Iteration 163, loss = 0.11124801\n",
      "Iteration 27, loss = 0.27242573\n",
      "Iteration 36, loss = 0.23562291\n",
      "Iteration 164, loss = 0.11096290\n",
      "Iteration 58, loss = 0.16742497\n",
      "Iteration 165, loss = 0.11065871\n",
      "Iteration 166, loss = 0.11036941\n",
      "Iteration 167, loss = 0.11007376\n",
      "Iteration 59, loss = 0.16568085\n",
      "Iteration 168, loss = 0.10979011\n",
      "Iteration 169, loss = 0.10951907\n",
      "Iteration 72, loss = 0.14904947\n",
      "Iteration 170, loss = 0.10922158\n",
      "Iteration 60, loss = 0.16401896\n",
      "Iteration 171, loss = 0.10893695\n",
      "Iteration 61, loss = 0.16237200\n",
      "Iteration 172, loss = 0.10865481\n",
      "Iteration 173, loss = 0.10837836\n",
      "Iteration 174, loss = 0.10811472\n",
      "Iteration 175, loss = 0.10782866\n",
      "Iteration 81, loss = 0.14453530\n",
      "Iteration 176, loss = 0.10756804\n",
      "Iteration 177, loss = 0.10729882\n",
      "Iteration 37, loss = 0.23170614\n",
      "Iteration 178, loss = 0.10703737\n",
      "Iteration 58, loss = 0.22675084\n",
      "Iteration 179, loss = 0.10677321\n",
      "Iteration 110, loss = 0.12530783\n",
      "Iteration 73, loss = 0.14793601\n",
      "Iteration 82, loss = 0.14338109\n",
      "Iteration 74, loss = 0.14684293\n",
      "Iteration 111, loss = 0.12477463\n",
      "Iteration 38, loss = 0.22806677\n",
      "Iteration 75, loss = 0.14579993\n",
      "Iteration 112, loss = 0.12425073\n",
      "Iteration 39, loss = 0.22433131\n",
      "Iteration 113, loss = 0.12369893\n",
      "Iteration 59, loss = 0.22368718\n",
      "Iteration 83, loss = 0.14229118\n",
      "Iteration 76, loss = 0.14476009\n",
      "Iteration 28, loss = 0.26617753\n",
      "Iteration 84, loss = 0.14121464\n",
      "Iteration 62, loss = 0.16081488\n",
      "Iteration 85, loss = 0.14019614\n",
      "Iteration 60, loss = 0.22080356\n",
      "Iteration 114, loss = 0.12318734\n",
      "Iteration 40, loss = 0.22087821\n",
      "Iteration 77, loss = 0.14372799\n",
      "Iteration 61, loss = 0.21789527\n",
      "Iteration 115, loss = 0.12267673\n",
      "Iteration 78, loss = 0.14273110\n",
      "Iteration 116, loss = 0.12217545\n",
      "Iteration 63, loss = 0.15931380\n",
      "Iteration 180, loss = 0.10651319\n",
      "Iteration 117, loss = 0.12167968\n",
      "Iteration 79, loss = 0.14173904\n",
      "Iteration 118, loss = 0.12118986\n",
      "Iteration 41, loss = 0.21756057\n",
      "Iteration 80, loss = 0.14079139\n",
      "Iteration 62, loss = 0.21498031\n",
      "Iteration 86, loss = 0.13915450Iteration 181, loss = 0.10625033\n",
      "Iteration 119, loss = 0.12068553\n",
      "Iteration 182, loss = 0.10600110\n",
      "\n",
      "Iteration 29, loss = 0.26029596\n",
      "Iteration 63, loss = 0.21221296\n",
      "Iteration 183, loss = 0.10575255\n",
      "Iteration 64, loss = 0.15782213\n",
      "Iteration 87, loss = 0.13816448\n",
      "Iteration 184, loss = 0.10548576\n",
      "Iteration 88, loss = 0.13720174\n",
      "Iteration 185, loss = 0.10524573\n",
      "Iteration 89, loss = 0.13621211\n",
      "Iteration 30, loss = 0.25470822\n",
      "Iteration 65, loss = 0.15639815\n",
      "Iteration 90, loss = 0.13528262\n",
      "Iteration 81, loss = 0.13988074\n",
      "Iteration 120, loss = 0.12021395\n",
      "Iteration 82, loss = 0.13894465\n",
      "Iteration 186, loss = 0.10498947\n",
      "Iteration 121, loss = 0.11973267\n",
      "Iteration 122, loss = 0.11926954\n",
      "Iteration 187, loss = 0.10474110\n",
      "Iteration 83, loss = 0.13805339\n",
      "Iteration 64, loss = 0.20949517\n",
      "Iteration 188, loss = 0.10449395\n",
      "Iteration 42, loss = 0.21423975\n",
      "Iteration 189, loss = 0.10425989\n",
      "Iteration 84, loss = 0.13716339\n",
      "Iteration 190, loss = 0.10401130\n",
      "Iteration 191, loss = 0.10377481\n",
      "Iteration 192, loss = 0.10353406\n",
      "Iteration 65, loss = 0.20674401\n",
      "Iteration 193, loss = 0.10329598\n",
      "Iteration 85, loss = 0.13629146\n",
      "Iteration 194, loss = 0.10306460\n",
      "Iteration 91, loss = 0.13437686\n",
      "Iteration 66, loss = 0.15500154\n",
      "Iteration 86, loss = 0.13545119\n",
      "Iteration 67, loss = 0.15363522\n",
      "Iteration 123, loss = 0.11881165\n",
      "Iteration 43, loss = 0.21116284\n",
      "Iteration 92, loss = 0.13344295\n",
      "Iteration 87, loss = 0.13463649\n",
      "Iteration 93, loss = 0.13255644\n",
      "Iteration 68, loss = 0.15234905\n",
      "Iteration 88, loss = 0.13379804\n",
      "Iteration 31, loss = 0.24944885\n",
      "Iteration 89, loss = 0.13300064\n",
      "Iteration 94, loss = 0.13167691\n",
      "Iteration 124, loss = 0.11834002\n",
      "Iteration 195, loss = 0.10282854\n",
      "Iteration 44, loss = 0.20814119\n",
      "Iteration 90, loss = 0.13222645\n",
      "Iteration 196, loss = 0.10260460\n",
      "Iteration 95, loss = 0.13081692\n",
      "Iteration 66, loss = 0.20423451\n",
      "Iteration 197, loss = 0.10237498\n",
      "Iteration 198, loss = 0.10214113\n",
      "Iteration 91, loss = 0.13144591\n",
      "Iteration 96, loss = 0.12999249\n",
      "Iteration 92, loss = 0.13069589\n",
      "Iteration 199, loss = 0.10191660\n",
      "Iteration 93, loss = 0.12993614\n",
      "Iteration 67, loss = 0.20163636\n",
      "Iteration 200, loss = 0.10169061\n",
      "Iteration 94, loss = 0.12921681\n",
      "Iteration 125, loss = 0.11790100\n",
      "Iteration 95, loss = 0.12850616\n",
      "Iteration 126, loss = 0.11745337\n",
      "Iteration 127, loss = 0.11700958\n",
      "Iteration 97, loss = 0.12916870\n",
      "Iteration 128, loss = 0.11657463\n",
      "Iteration 129, loss = 0.11614344\n",
      "Iteration 68, loss = 0.19903257\n",
      "Iteration 69, loss = 0.15104957\n",
      "Iteration 130, loss = 0.11571695\n",
      "Iteration 131, loss = 0.11529957\n",
      "Iteration 70, loss = 0.14982687\n",
      "Iteration 132, loss = 0.11487872\n",
      "Iteration 69, loss = 0.19662417\n",
      "Iteration 70, loss = 0.19420153\n",
      "Iteration 201, loss = 0.10147017\n",
      "Iteration 45, loss = 0.20522258\n",
      "Iteration 133, loss = 0.11447312\n",
      "Iteration 202, loss = 0.10124262\n",
      "Iteration 203, loss = 0.10101912\n",
      "Iteration 32, loss = 0.24422781\n",
      "Iteration 71, loss = 0.19188906\n",
      "Iteration 134, loss = 0.11404761\n",
      "Iteration 72, loss = 0.18965705\n",
      "Iteration 204, loss = 0.10080278\n",
      "Iteration 135, loss = 0.11364058\n",
      "Iteration 205, loss = 0.10058475\n",
      "Iteration 206, loss = 0.10036845\n",
      "Iteration 136, loss = 0.11323771\n",
      "Iteration 207, loss = 0.10014354\n",
      "Iteration 137, loss = 0.11284911\n",
      "Iteration 138, loss = 0.11245423\n",
      "Iteration 208, loss = 0.09992804\n",
      "Iteration 46, loss = 0.20230835\n",
      "Iteration 209, loss = 0.09972452\n",
      "Iteration 71, loss = 0.14862815\n",
      "Iteration 210, loss = 0.09950984\n",
      "Iteration 98, loss = 0.12837238\n",
      "Iteration 47, loss = 0.19969358\n",
      "Iteration 99, loss = 0.12754813\n",
      "Iteration 72, loss = 0.14745314\n",
      "Iteration 100, loss = 0.12678958\n",
      "Iteration 48, loss = 0.19701742\n",
      "Iteration 211, loss = 0.09929390\n",
      "Iteration 212, loss = 0.09908166\n",
      "Iteration 101, loss = 0.12602838\n",
      "Iteration 102, loss = 0.12526952\n",
      "Iteration 96, loss = 0.12779424\n",
      "Iteration 139, loss = 0.11205679\n",
      "Iteration 97, loss = 0.12711140\n",
      "Iteration 73, loss = 0.14626376\n",
      "Iteration 140, loss = 0.11167748\n",
      "Iteration 73, loss = 0.18747182\n",
      "Iteration 141, loss = 0.11129184\n",
      "Iteration 33, loss = 0.23932478\n",
      "Iteration 142, loss = 0.11093192\n",
      "Iteration 74, loss = 0.18530365\n",
      "Iteration 98, loss = 0.12641365\n",
      "Iteration 143, loss = 0.11054280\n",
      "Iteration 144, loss = 0.11017734\n",
      "Iteration 75, loss = 0.18326180\n",
      "Iteration 145, loss = 0.10981735\n",
      "Iteration 146, loss = 0.10945077\n",
      "Iteration 74, loss = 0.14518380\n",
      "Iteration 99, loss = 0.12575106\n",
      "Iteration 76, loss = 0.18122003\n",
      "Iteration 75, loss = 0.14410218\n",
      "Iteration 77, loss = 0.17923367\n",
      "Iteration 147, loss = 0.10910328\n",
      "Iteration 100, loss = 0.12509996\n",
      "Iteration 78, loss = 0.17733003\n",
      "Iteration 103, loss = 0.12453633\n",
      "Iteration 213, loss = 0.09887367\n",
      "Iteration 101, loss = 0.12446439\n",
      "Iteration 76, loss = 0.14303811\n",
      "Iteration 102, loss = 0.12381645\n",
      "Iteration 79, loss = 0.17539082\n",
      "Iteration 103, loss = 0.12318922\n",
      "Iteration 214, loss = 0.09866723\n",
      "Iteration 215, loss = 0.09845530\n",
      "Iteration 77, loss = 0.14199780\n",
      "Iteration 216, loss = 0.09826011\n",
      "Iteration 80, loss = 0.17354467\n",
      "Iteration 104, loss = 0.12381040\n",
      "Iteration 34, loss = 0.23465795\n",
      "Iteration 217, loss = 0.09804606\n",
      "Iteration 81, loss = 0.17174548\n",
      "Iteration 49, loss = 0.19446194\n",
      "Iteration 218, loss = 0.09784934\n",
      "Iteration 219, loss = 0.09765229\n",
      "Iteration 78, loss = 0.14100774\n",
      "Iteration 220, loss = 0.09744512\n",
      "Iteration 221, loss = 0.09725115\n",
      "Iteration 222, loss = 0.09706590\n",
      "Iteration 148, loss = 0.10876226\n",
      "Iteration 35, loss = 0.23024441\n",
      "Iteration 105, loss = 0.12309541\n",
      "Iteration 149, loss = 0.10839667\n",
      "Iteration 50, loss = 0.19199022\n",
      "Iteration 36, loss = 0.22597735\n",
      "Iteration 223, loss = 0.09685469\n",
      "Iteration 106, loss = 0.12240091\n",
      "Iteration 224, loss = 0.09666230\n",
      "Iteration 107, loss = 0.12170638\n",
      "Iteration 150, loss = 0.10805622\n",
      "Iteration 225, loss = 0.09645964\n",
      "Iteration 226, loss = 0.09627326\n",
      "Iteration 108, loss = 0.12100742\n",
      "Iteration 51, loss = 0.18964130\n",
      "Iteration 227, loss = 0.09607631\n",
      "Iteration 228, loss = 0.09588847\n",
      "Iteration 151, loss = 0.10770965\n",
      "Iteration 229, loss = 0.09570472\n",
      "Iteration 109, loss = 0.12035733\n",
      "Iteration 230, loss = 0.09551355\n",
      "Iteration 231, loss = 0.09532993\n",
      "Iteration 232, loss = 0.09513371\n",
      "Iteration 104, loss = 0.12258236\n",
      "Iteration 233, loss = 0.09495215\n",
      "Iteration 234, loss = 0.09476284\n",
      "Iteration 235, loss = 0.09458006\n",
      "Iteration 236, loss = 0.09439382\n",
      "Iteration 237, loss = 0.09421222\n",
      "Iteration 52, loss = 0.18732292\n",
      "Iteration 110, loss = 0.11967700\n",
      "Iteration 152, loss = 0.10735939\n",
      "Iteration 111, loss = 0.11904858\n",
      "Iteration 105, loss = 0.12198026\n",
      "Iteration 37, loss = 0.22185258\n",
      "Iteration 79, loss = 0.14000377\n",
      "Iteration 112, loss = 0.11840842\n",
      "Iteration 82, loss = 0.16998936\n",
      "Iteration 153, loss = 0.10703286\n",
      "Iteration 238, loss = 0.09401973\n",
      "Iteration 154, loss = 0.10668662\n",
      "Iteration 106, loss = 0.12138407\n",
      "Iteration 113, loss = 0.11776798\n",
      "Iteration 83, loss = 0.16831967\n",
      "Iteration 239, loss = 0.09384475\n",
      "Iteration 240, loss = 0.09366119\n",
      "Iteration 241, loss = 0.09347629\n",
      "Iteration 84, loss = 0.16665425\n",
      "Iteration 107, loss = 0.12079531\n",
      "Iteration 242, loss = 0.09330193\n",
      "Iteration 80, loss = 0.13905764\n",
      "Iteration 114, loss = 0.11714021\n",
      "Iteration 243, loss = 0.09312152\n",
      "Iteration 108, loss = 0.12021376\n",
      "Iteration 115, loss = 0.11651975\n",
      "Iteration 116, loss = 0.11594380\n",
      "Iteration 81, loss = 0.13809668\n",
      "Iteration 244, loss = 0.09293396\n",
      "Iteration 109, loss = 0.11965741\n",
      "Iteration 85, loss = 0.16499168\n",
      "Iteration 155, loss = 0.10635651Iteration 245, loss = 0.09276124\n",
      "Iteration 82, loss = 0.13717921\n",
      "Iteration 53, loss = 0.18510863\n",
      "Iteration 110, loss = 0.11909047\n",
      "Iteration 38, loss = 0.21792941\n",
      "Iteration 83, loss = 0.13631868\n",
      "Iteration 111, loss = 0.11854878\n",
      "Iteration 86, loss = 0.16341922\n",
      "Iteration 84, loss = 0.13542845\n",
      "\n",
      "Iteration 117, loss = 0.11533230\n",
      "Iteration 246, loss = 0.09258787\n",
      "Iteration 247, loss = 0.09241159\n",
      "Iteration 248, loss = 0.09223379\n",
      "Iteration 249, loss = 0.09205848\n",
      "Iteration 118, loss = 0.11472966\n",
      "Iteration 112, loss = 0.11799266\n",
      "Iteration 156, loss = 0.10603467\n",
      "Iteration 250, loss = 0.09188218\n",
      "Iteration 85, loss = 0.13456754\n",
      "Iteration 251, loss = 0.09171049\n",
      "Iteration 157, loss = 0.10571380\n",
      "Iteration 54, loss = 0.18297232\n",
      "Iteration 158, loss = 0.10537700\n",
      "Iteration 252, loss = 0.09153892\n",
      "Iteration 159, loss = 0.10505681\n",
      "Iteration 253, loss = 0.09136700\n",
      "Iteration 254, loss = 0.09120118\n",
      "Iteration 255, loss = 0.09103099\n",
      "Iteration 256, loss = 0.09086179\n",
      "Iteration 160, loss = 0.10473898\n",
      "Iteration 257, loss = 0.09070086\n",
      "Iteration 258, loss = 0.09053079\n",
      "Iteration 113, loss = 0.11747208\n",
      "Iteration 161, loss = 0.10443220\n",
      "Iteration 259, loss = 0.09036382\n",
      "Iteration 55, loss = 0.18085654\n",
      "Iteration 260, loss = 0.09019545\n",
      "Iteration 119, loss = 0.11415282\n",
      "Iteration 162, loss = 0.10411388\n",
      "Iteration 261, loss = 0.09003367\n",
      "Iteration 262, loss = 0.08986845\n",
      "Iteration 86, loss = 0.13372687\n",
      "Iteration 114, loss = 0.11693718\n",
      "Iteration 87, loss = 0.16185298\n",
      "Iteration 39, loss = 0.21421477\n",
      "Iteration 263, loss = 0.08970927\n",
      "Iteration 87, loss = 0.13292598\n",
      "Iteration 163, loss = 0.10380253\n",
      "Iteration 120, loss = 0.11359332\n",
      "Iteration 121, loss = 0.11300934\n",
      "Iteration 56, loss = 0.17884571\n",
      "Iteration 40, loss = 0.21056159\n",
      "Iteration 164, loss = 0.10349326\n",
      "Iteration 88, loss = 0.16036875\n",
      "Iteration 165, loss = 0.10318944\n",
      "Iteration 89, loss = 0.15889317\n",
      "Iteration 166, loss = 0.10290504\n",
      "Iteration 115, loss = 0.11642159\n",
      "Iteration 167, loss = 0.10258535\n",
      "Iteration 264, loss = 0.08954388\n",
      "Iteration 88, loss = 0.13211476\n",
      "Iteration 57, loss = 0.17692047\n",
      "Iteration 116, loss = 0.11591263\n",
      "Iteration 122, loss = 0.11246908\n",
      "Iteration 117, loss = 0.11540584\n",
      "Iteration 123, loss = 0.11192351\n",
      "Iteration 90, loss = 0.15740537\n",
      "Iteration 168, loss = 0.10229353\n",
      "Iteration 118, loss = 0.11489854\n",
      "Iteration 89, loss = 0.13131740\n",
      "Iteration 90, loss = 0.13057551\n",
      "Iteration 124, loss = 0.11136842\n",
      "Iteration 91, loss = 0.15601485\n",
      "Iteration 58, loss = 0.17502645\n",
      "Iteration 91, loss = 0.12978954\n",
      "Iteration 125, loss = 0.11085410\n",
      "Iteration 92, loss = 0.12908320\n",
      "Iteration 93, loss = 0.12833674\n",
      "Iteration 92, loss = 0.15463434\n",
      "Iteration 93, loss = 0.15332098\n",
      "Iteration 94, loss = 0.12762550\n",
      "Iteration 95, loss = 0.12693827\n",
      "Iteration 94, loss = 0.15196057\n",
      "Iteration 96, loss = 0.12625601\n",
      "Iteration 169, loss = 0.10200498\n",
      "Iteration 265, loss = 0.08937898\n",
      "Iteration 41, loss = 0.20716334\n",
      "Iteration 95, loss = 0.15066047\n",
      "Iteration 97, loss = 0.12559822\n",
      "Iteration 266, loss = 0.08922084\n",
      "Iteration 119, loss = 0.11441589\n",
      "Iteration 267, loss = 0.08905900\n",
      "Iteration 96, loss = 0.14938948\n",
      "Iteration 120, loss = 0.11393830\n",
      "Iteration 268, loss = 0.08890544\n",
      "Iteration 97, loss = 0.14821715\n",
      "Iteration 269, loss = 0.08874592\n",
      "Iteration 121, loss = 0.11345620\n",
      "Iteration 126, loss = 0.11033686\n",
      "Iteration 270, loss = 0.08858838\n",
      "Iteration 42, loss = 0.20388313\n",
      "Iteration 271, loss = 0.08843915\n",
      "Iteration 272, loss = 0.08828287\n",
      "Iteration 122, loss = 0.11297980\n",
      "Iteration 273, loss = 0.08812444\n",
      "Iteration 274, loss = 0.08797143\n",
      "Iteration 127, loss = 0.10979644\n",
      "Iteration 275, loss = 0.08782712\n",
      "Iteration 170, loss = 0.10170311\n",
      "Iteration 128, loss = 0.10928885\n",
      "Iteration 276, loss = 0.08766499\n",
      "Iteration 277, loss = 0.08751091\n",
      "Iteration 171, loss = 0.10141502\n",
      "Iteration 278, loss = 0.08736373\n",
      "Iteration 129, loss = 0.10878058\n",
      "Iteration 59, loss = 0.17316010\n",
      "Iteration 43, loss = 0.20065330\n",
      "Iteration 172, loss = 0.10112385\n",
      "Iteration 130, loss = 0.10828959\n",
      "Iteration 173, loss = 0.10084394\n",
      "Iteration 131, loss = 0.10779598\n",
      "Iteration 174, loss = 0.10055369\n",
      "Iteration 175, loss = 0.10026929\n",
      "Iteration 60, loss = 0.17140604\n",
      "Iteration 132, loss = 0.10729905\n",
      "Iteration 44, loss = 0.19765423\n",
      "Iteration 176, loss = 0.09999531\n",
      "Iteration 61, loss = 0.16969486\n",
      "Iteration 177, loss = 0.09971947\n",
      "Iteration 178, loss = 0.09943669\n",
      "Iteration 179, loss = 0.09916190\n",
      "Iteration 123, loss = 0.11251612\n",
      "Iteration 98, loss = 0.14701849\n",
      "Iteration 124, loss = 0.11205965\n",
      "Iteration 99, loss = 0.14579736\n",
      "Iteration 125, loss = 0.11160638\n",
      "Iteration 98, loss = 0.12493724\n",
      "Iteration 99, loss = 0.12431430\n",
      "Iteration 126, loss = 0.11115882\n",
      "Iteration 279, loss = 0.08721487\n",
      "Iteration 280, loss = 0.08706442\n",
      "Iteration 180, loss = 0.09890850\n",
      "Iteration 100, loss = 0.12365808\n",
      "Iteration 133, loss = 0.10684959\n",
      "Iteration 281, loss = 0.08691752\n",
      "Iteration 127, loss = 0.11073654\n",
      "Iteration 100, loss = 0.14466629\n",
      "Iteration 282, loss = 0.08676637\n",
      "Iteration 181, loss = 0.09863628\n",
      "Iteration 101, loss = 0.12307505\n",
      "Iteration 283, loss = 0.08661981\n",
      "Iteration 128, loss = 0.11027834\n",
      "Iteration 182, loss = 0.09836938\n",
      "Iteration 102, loss = 0.12243822\n",
      "Iteration 284, loss = 0.08646802\n",
      "Iteration 183, loss = 0.09810264\n",
      "Iteration 129, loss = 0.10985846\n",
      "Iteration 285, loss = 0.08632177\n",
      "Iteration 286, loss = 0.08618112\n",
      "Iteration 62, loss = 0.16800755\n",
      "Iteration 103, loss = 0.12182711\n",
      "Iteration 287, loss = 0.08602705\n",
      "Iteration 184, loss = 0.09784784\n",
      "Iteration 288, loss = 0.08588344\n",
      "Iteration 130, loss = 0.10942868\n",
      "Iteration 289, loss = 0.08573535\n",
      "Iteration 185, loss = 0.09757340\n",
      "Iteration 63, loss = 0.16636642\n",
      "Iteration 186, loss = 0.09732707\n",
      "Iteration 187, loss = 0.09706177\n",
      "Iteration 134, loss = 0.10635335\n",
      "Iteration 188, loss = 0.09680663\n",
      "Iteration 64, loss = 0.16478207\n",
      "Iteration 189, loss = 0.09656587\n",
      "Iteration 135, loss = 0.10588256\n",
      "Iteration 190, loss = 0.09631627\n",
      "Iteration 101, loss = 0.14355164\n",
      "Iteration 136, loss = 0.10543178\n",
      "Iteration 191, loss = 0.09606041\n",
      "Iteration 137, loss = 0.10499230\n",
      "Iteration 192, loss = 0.09581625\n",
      "Iteration 104, loss = 0.12127165\n",
      "Iteration 193, loss = 0.09556719\n",
      "Iteration 138, loss = 0.10453684\n",
      "Iteration 194, loss = 0.09533804\n",
      "Iteration 102, loss = 0.14242282\n",
      "Iteration 195, loss = 0.09507903\n",
      "Iteration 196, loss = 0.09484942\n",
      "Iteration 139, loss = 0.10408612\n",
      "Iteration 197, loss = 0.09460629\n",
      "Iteration 140, loss = 0.10368053\n",
      "Iteration 198, loss = 0.09438115\n",
      "Iteration 141, loss = 0.10322451\n",
      "Iteration 199, loss = 0.09413348\n",
      "Iteration 142, loss = 0.10280860\n",
      "Iteration 200, loss = 0.09390304\n",
      "Iteration 131, loss = 0.10901212\n",
      "Iteration 143, loss = 0.10238845\n",
      "Iteration 45, loss = 0.19475009\n",
      "Iteration 290, loss = 0.08559364\n",
      "Iteration 201, loss = 0.09367200\n",
      "Iteration 202, loss = 0.09343983\n",
      "Iteration 291, loss = 0.08543990\n",
      "Iteration 203, loss = 0.09322228\n",
      "Iteration 132, loss = 0.10860054\n",
      "Iteration 292, loss = 0.08530216\n",
      "Iteration 204, loss = 0.09299406\n",
      "Iteration 205, loss = 0.09276386\n",
      "Iteration 46, loss = 0.19191193\n",
      "Iteration 103, loss = 0.14138623\n",
      "Iteration 293, loss = 0.08515825\n",
      "Iteration 206, loss = 0.09254206\n",
      "Iteration 294, loss = 0.08501365\n",
      "Iteration 65, loss = 0.16329074\n",
      "Iteration 133, loss = 0.10819876\n",
      "Iteration 295, loss = 0.08486703\n",
      "Iteration 105, loss = 0.12068093\n",
      "Iteration 134, loss = 0.10778945\n",
      "Iteration 296, loss = 0.08473195\n",
      "Iteration 66, loss = 0.16178552\n",
      "Iteration 135, loss = 0.10740093\n",
      "Iteration 136, loss = 0.10699949\n",
      "Iteration 67, loss = 0.16037245\n",
      "Iteration 137, loss = 0.10661068\n",
      "Iteration 138, loss = 0.10622888\n",
      "Iteration 104, loss = 0.14031649\n",
      "Iteration 68, loss = 0.15896284\n",
      "Iteration 47, loss = 0.18918458\n",
      "Iteration 105, loss = 0.13928122\n",
      "Iteration 106, loss = 0.12011765\n",
      "Iteration 144, loss = 0.10198529\n",
      "Iteration 207, loss = 0.09232431\n",
      "Iteration 107, loss = 0.11956445\n",
      "Iteration 145, loss = 0.10156326\n",
      "Iteration 146, loss = 0.10114236\n",
      "Iteration 106, loss = 0.13827093\n",
      "Iteration 147, loss = 0.10077130\n",
      "Iteration 108, loss = 0.11900999\n",
      "Iteration 107, loss = 0.13728491\n",
      "Iteration 148, loss = 0.10034818\n",
      "Iteration 69, loss = 0.15759404\n",
      "Iteration 149, loss = 0.09996695\n",
      "Iteration 150, loss = 0.09958572\n",
      "Iteration 108, loss = 0.13633356\n",
      "Iteration 297, loss = 0.08459464\n",
      "Iteration 151, loss = 0.09921515\n",
      "Iteration 152, loss = 0.09881834\n",
      "Iteration 298, loss = 0.08445007\n",
      "Iteration 299, loss = 0.08431943\n",
      "Iteration 300, loss = 0.08418345\n",
      "Iteration 301, loss = 0.08404022\n",
      "Iteration 48, loss = 0.18658086\n",
      "Iteration 153, loss = 0.09847870\n",
      "Iteration 70, loss = 0.15624351\n",
      "Iteration 302, loss = 0.08390366\n",
      "Iteration 208, loss = 0.09210471\n",
      "Iteration 303, loss = 0.08376555\n",
      "Iteration 304, loss = 0.08363662\n",
      "Iteration 154, loss = 0.09808409\n",
      "Iteration 305, loss = 0.08350233\n",
      "Iteration 306, loss = 0.08336256\n",
      "Iteration 307, loss = 0.08323088\n",
      "Iteration 139, loss = 0.10586244\n",
      "Iteration 109, loss = 0.13538788\n",
      "Iteration 109, loss = 0.11848089\n",
      "Iteration 209, loss = 0.09188873\n",
      "Iteration 110, loss = 0.11795449\n",
      "Iteration 110, loss = 0.13445272\n",
      "Iteration 210, loss = 0.09166496\n",
      "Iteration 211, loss = 0.09145961\n",
      "Iteration 111, loss = 0.13356728\n",
      "Iteration 140, loss = 0.10548271\n",
      "Iteration 155, loss = 0.09772147\n",
      "Iteration 212, loss = 0.09123664\n",
      "Iteration 308, loss = 0.08309683\n",
      "Iteration 309, loss = 0.08296888\n",
      "Iteration 310, loss = 0.08283235\n",
      "Iteration 49, loss = 0.18409101\n",
      "Iteration 141, loss = 0.10510890\n",
      "Iteration 111, loss = 0.11743551\n",
      "Iteration 311, loss = 0.08270713\n",
      "Iteration 312, loss = 0.08257380\n",
      "Iteration 142, loss = 0.10473933\n",
      "Iteration 313, loss = 0.08244924\n",
      "Iteration 314, loss = 0.08231554\n",
      "Iteration 315, loss = 0.08219103\n",
      "Iteration 143, loss = 0.10439446\n",
      "Iteration 156, loss = 0.09734948\n",
      "Iteration 316, loss = 0.08205823\n",
      "Iteration 71, loss = 0.15497589\n",
      "Iteration 112, loss = 0.11691597\n",
      "Iteration 50, loss = 0.18167775\n",
      "Iteration 157, loss = 0.09700757\n",
      "Iteration 158, loss = 0.09663566\n",
      "Iteration 144, loss = 0.10402866\n",
      "Iteration 159, loss = 0.09631022\n",
      "Iteration 72, loss = 0.15372137\n",
      "Iteration 51, loss = 0.17937073\n",
      "Iteration 112, loss = 0.13264567\n",
      "Iteration 160, loss = 0.09596131\n",
      "Iteration 161, loss = 0.09562087\n",
      "Iteration 317, loss = 0.08193407\n",
      "Iteration 113, loss = 0.13178781\n",
      "Iteration 318, loss = 0.08180184\n",
      "Iteration 145, loss = 0.10368391\n",
      "Iteration 213, loss = 0.09102970\n",
      "Iteration 113, loss = 0.11641079\n",
      "Iteration 319, loss = 0.08167615\n",
      "Iteration 146, loss = 0.10332382\n",
      "Iteration 214, loss = 0.09082365\n",
      "Iteration 114, loss = 0.11592409\n",
      "Iteration 320, loss = 0.08155283\n",
      "Iteration 147, loss = 0.10297792\n",
      "Iteration 321, loss = 0.08142584\n",
      "Iteration 215, loss = 0.09060600\n",
      "Iteration 216, loss = 0.09040576\n",
      "Iteration 115, loss = 0.11543023\n",
      "Iteration 217, loss = 0.09019247\n",
      "Iteration 322, loss = 0.08130279\n",
      "Iteration 116, loss = 0.11495884\n",
      "Iteration 323, loss = 0.08117880\n",
      "Iteration 148, loss = 0.10264081\n",
      "Iteration 324, loss = 0.08105159\n",
      "Iteration 325, loss = 0.08092443Iteration 149, loss = 0.10230186\n",
      "\n",
      "Iteration 162, loss = 0.09528537\n",
      "Iteration 73, loss = 0.15248449\n",
      "Iteration 52, loss = 0.17715621\n",
      "Iteration 326, loss = 0.08080730\n",
      "Iteration 163, loss = 0.09495089\n",
      "Iteration 74, loss = 0.15132874\n",
      "Iteration 327, loss = 0.08068193\n",
      "Iteration 218, loss = 0.08999804\n",
      "Iteration 219, loss = 0.08979032\n",
      "Iteration 328, loss = 0.08056025\n",
      "Iteration 220, loss = 0.08958452\n",
      "Iteration 114, loss = 0.13092471\n",
      "Iteration 221, loss = 0.08938962\n",
      "Iteration 164, loss = 0.09463553\n",
      "Iteration 222, loss = 0.08919095\n",
      "Iteration 150, loss = 0.10196584\n",
      "Iteration 115, loss = 0.13010856\n",
      "Iteration 151, loss = 0.10163701\n",
      "Iteration 165, loss = 0.09430130\n",
      "Iteration 223, loss = 0.08899248\n",
      "Iteration 329, loss = 0.08043987\n",
      "Iteration 224, loss = 0.08879425\n",
      "Iteration 53, loss = 0.17500968\n",
      "Iteration 330, loss = 0.08030951\n",
      "Iteration 117, loss = 0.11446937\n",
      "Iteration 331, loss = 0.08019260\n",
      "Iteration 116, loss = 0.12925151\n",
      "Iteration 332, loss = 0.08007386\n",
      "Iteration 333, loss = 0.07994938\n",
      "Iteration 118, loss = 0.11400790\n",
      "Iteration 117, loss = 0.12846328\n",
      "Iteration 334, loss = 0.07983085\n",
      "Iteration 54, loss = 0.17290119\n",
      "Iteration 166, loss = 0.09399098\n",
      "Iteration 335, loss = 0.07971068\n",
      "Iteration 119, loss = 0.11354612\n",
      "Iteration 152, loss = 0.10131451\n",
      "Iteration 118, loss = 0.12764583\n",
      "Iteration 225, loss = 0.08859937\n",
      "Iteration 226, loss = 0.08840712\n",
      "Iteration 167, loss = 0.09366318\n",
      "Iteration 55, loss = 0.17088872\n",
      "Iteration 227, loss = 0.08821389\n",
      "Iteration 119, loss = 0.12688246\n",
      "Iteration 228, loss = 0.08802809\n",
      "Iteration 120, loss = 0.11309528\n",
      "Iteration 120, loss = 0.12611879\n",
      "Iteration 229, loss = 0.08784037\n",
      "Iteration 121, loss = 0.11263141\n",
      "Iteration 230, loss = 0.08765220\n",
      "Iteration 56, loss = 0.16895479\n",
      "Iteration 121, loss = 0.12534594\n",
      "Iteration 231, loss = 0.08746098\n",
      "Iteration 122, loss = 0.11219933\n",
      "Iteration 232, loss = 0.08727016\n",
      "Iteration 122, loss = 0.12461722\n",
      "Iteration 233, loss = 0.08709385\n",
      "Iteration 123, loss = 0.11175306\n",
      "Iteration 234, loss = 0.08690596\n",
      "Iteration 57, loss = 0.16708154\n",
      "Iteration 123, loss = 0.12390114\n",
      "Iteration 235, loss = 0.08672227\n",
      "Iteration 236, loss = 0.08653375\n",
      "Iteration 168, loss = 0.09335591\n",
      "Iteration 75, loss = 0.15015487\n",
      "Iteration 169, loss = 0.09304072\n",
      "Iteration 237, loss = 0.08636731\n",
      "Iteration 238, loss = 0.08618437\n",
      "Iteration 170, loss = 0.09272237\n",
      "Iteration 239, loss = 0.08600595\n",
      "Iteration 171, loss = 0.09241561\n",
      "Iteration 336, loss = 0.07958496\n",
      "Iteration 153, loss = 0.10098578\n",
      "Iteration 76, loss = 0.14903277\n",
      "Iteration 337, loss = 0.07946796\n",
      "Iteration 338, loss = 0.07935095\n",
      "Iteration 154, loss = 0.10067367\n",
      "Iteration 339, loss = 0.07922525\n",
      "Iteration 155, loss = 0.10035369\n",
      "Iteration 340, loss = 0.07911412\n",
      "Iteration 341, loss = 0.07899593\n",
      "Iteration 156, loss = 0.10003202\n",
      "Iteration 342, loss = 0.07888027\n",
      "Iteration 157, loss = 0.09972496\n",
      "Iteration 343, loss = 0.07876127\n",
      "Iteration 77, loss = 0.14790773\n",
      "Iteration 158, loss = 0.09942110\n",
      "Iteration 124, loss = 0.12316195\n",
      "Iteration 78, loss = 0.14683812\n",
      "Iteration 172, loss = 0.09212193\n",
      "Iteration 159, loss = 0.09911600\n",
      "Iteration 344, loss = 0.07864866\n",
      "Iteration 345, loss = 0.07853155\n",
      "Iteration 346, loss = 0.07841297\n",
      "Iteration 173, loss = 0.09181853\n",
      "Iteration 125, loss = 0.12249522\n",
      "Iteration 347, loss = 0.07830122\n",
      "Iteration 348, loss = 0.07819063\n",
      "Iteration 160, loss = 0.09881263\n",
      "Iteration 174, loss = 0.09152595\n",
      "Iteration 349, loss = 0.07807327\n",
      "Iteration 126, loss = 0.12179226\n",
      "Iteration 350, loss = 0.07795694\n",
      "Iteration 175, loss = 0.09123749\n",
      "Iteration 79, loss = 0.14577902\n",
      "Iteration 351, loss = 0.07784745\n",
      "Iteration 352, loss = 0.07773328\n",
      "Iteration 176, loss = 0.09093338\n",
      "Iteration 177, loss = 0.09065135Iteration 161, loss = 0.09851448\n",
      "Iteration 80, loss = 0.14476512\n",
      "Iteration 127, loss = 0.12113299\n",
      "\n",
      "Iteration 162, loss = 0.09823044\n",
      "Iteration 353, loss = 0.07761759\n",
      "Iteration 163, loss = 0.09792497\n",
      "Iteration 240, loss = 0.08582076\n",
      "Iteration 124, loss = 0.11132947\n",
      "Iteration 354, loss = 0.07750724\n",
      "Iteration 178, loss = 0.09036955\n",
      "Iteration 179, loss = 0.09007662\n",
      "Iteration 241, loss = 0.08564953\n",
      "Iteration 58, loss = 0.16531303\n",
      "Iteration 180, loss = 0.08980040\n",
      "Iteration 242, loss = 0.08547070\n",
      "Iteration 181, loss = 0.08952572\n",
      "Iteration 125, loss = 0.11089031\n",
      "Iteration 81, loss = 0.14379580\n",
      "Iteration 355, loss = 0.07740244\n",
      "Iteration 182, loss = 0.08924816\n",
      "Iteration 356, loss = 0.07728817\n",
      "Iteration 357, loss = 0.07717517\n",
      "Iteration 183, loss = 0.08898731\n",
      "Iteration 358, loss = 0.07706279\n",
      "Iteration 184, loss = 0.08871745\n",
      "Iteration 126, loss = 0.11046669\n",
      "Iteration 128, loss = 0.12047027\n",
      "Iteration 243, loss = 0.08529784\n",
      "Iteration 185, loss = 0.08845167\n",
      "Iteration 359, loss = 0.07695710\n",
      "Iteration 186, loss = 0.08819361\n",
      "Iteration 187, loss = 0.08791047\n",
      "Iteration 59, loss = 0.16356444\n",
      "Iteration 188, loss = 0.08766559\n",
      "Iteration 164, loss = 0.09763921\n",
      "Iteration 129, loss = 0.11979926\n",
      "Iteration 189, loss = 0.08741650\n",
      "Iteration 130, loss = 0.11917162\n",
      "Iteration 127, loss = 0.11005057\n",
      "Iteration 165, loss = 0.09734979\n",
      "Iteration 82, loss = 0.14280683\n",
      "Iteration 244, loss = 0.08512441\n",
      "Iteration 166, loss = 0.09706439\n",
      "Iteration 360, loss = 0.07684736\n",
      "Iteration 361, loss = 0.07673378\n",
      "Iteration 167, loss = 0.09678856\n",
      "Iteration 128, loss = 0.10963976\n",
      "Iteration 362, loss = 0.07662854\n",
      "Iteration 245, loss = 0.08495450\n",
      "Iteration 129, loss = 0.10922164\n",
      "Iteration 363, loss = 0.07651453\n",
      "Iteration 364, loss = 0.07641388\n",
      "Iteration 246, loss = 0.08478426\n",
      "Iteration 60, loss = 0.16189656\n",
      "Iteration 247, loss = 0.08461650\n",
      "Iteration 130, loss = 0.10881306\n",
      "Iteration 248, loss = 0.08445089\n",
      "Iteration 249, loss = 0.08427636\n",
      "Iteration 168, loss = 0.09651174\n",
      "Iteration 131, loss = 0.10842111\n",
      "Iteration 131, loss = 0.11853570\n",
      "Iteration 132, loss = 0.11790357\n",
      "Iteration 365, loss = 0.07630745\n",
      "Iteration 169, loss = 0.09622846\n",
      "Iteration 83, loss = 0.14184630\n",
      "Iteration 61, loss = 0.16027444\n",
      "Iteration 250, loss = 0.08411916\n",
      "Iteration 170, loss = 0.09595679\n",
      "Iteration 251, loss = 0.08395031\n",
      "Iteration 366, loss = 0.07619149\n",
      "Iteration 132, loss = 0.10801935Iteration 367, loss = 0.07608893\n",
      "Iteration 171, loss = 0.09568817\n",
      "Iteration 252, loss = 0.08378034\n",
      "Iteration 172, loss = 0.09541497\n",
      "Iteration 62, loss = 0.15867425\n",
      "Iteration 253, loss = 0.08361934\n",
      "Iteration 368, loss = 0.07597634\n",
      "\n",
      "Iteration 173, loss = 0.09515215\n",
      "Iteration 254, loss = 0.08345521\n",
      "Iteration 133, loss = 0.11729287\n",
      "Iteration 369, loss = 0.07587022\n",
      "Iteration 255, loss = 0.08329271\n",
      "Iteration 190, loss = 0.08715248\n",
      "Iteration 174, loss = 0.09489176\n",
      "Iteration 63, loss = 0.15716120\n",
      "Iteration 191, loss = 0.08690989\n",
      "Iteration 256, loss = 0.08312793\n",
      "Iteration 84, loss = 0.14092757\n",
      "Iteration 175, loss = 0.09462779\n",
      "Iteration 192, loss = 0.08664547\n",
      "Iteration 257, loss = 0.08296764\n",
      "Iteration 370, loss = 0.07576160\n",
      "Iteration 193, loss = 0.08639321\n",
      "Iteration 134, loss = 0.11674405\n",
      "Iteration 371, loss = 0.07566314\n",
      "Iteration 194, loss = 0.08614566\n",
      "Iteration 372, loss = 0.07554904\n",
      "Iteration 373, loss = 0.07544087\n",
      "Iteration 374, loss = 0.07533457\n",
      "Iteration 133, loss = 0.10763699\n",
      "Iteration 375, loss = 0.07523603\n",
      "Iteration 376, loss = 0.07513028\n",
      "Iteration 135, loss = 0.11611572\n",
      "Iteration 85, loss = 0.14000535\n",
      "Iteration 136, loss = 0.11554679Iteration 195, loss = 0.08590448\n",
      "Iteration 134, loss = 0.10726137\n",
      "\n",
      "Iteration 176, loss = 0.09436746\n",
      "Iteration 196, loss = 0.08567267\n",
      "Iteration 197, loss = 0.08541863\n",
      "Iteration 86, loss = 0.13912786\n",
      "Iteration 198, loss = 0.08517680\n",
      "Iteration 135, loss = 0.10686494\n",
      "Iteration 377, loss = 0.07501965\n",
      "Iteration 378, loss = 0.07491855\n",
      "Iteration 177, loss = 0.09411068\n",
      "Iteration 379, loss = 0.07481397\n",
      "Iteration 87, loss = 0.13826774\n",
      "Iteration 258, loss = 0.08280890Iteration 136, loss = 0.10650946\n",
      "\n",
      "Iteration 380, loss = 0.07470885\n",
      "Iteration 178, loss = 0.09385835\n",
      "Iteration 381, loss = 0.07460257\n",
      "Iteration 382, loss = 0.07449784\n",
      "Iteration 88, loss = 0.13740824\n",
      "Iteration 259, loss = 0.08264731\n",
      "Iteration 383, loss = 0.07439950\n",
      "Iteration 260, loss = 0.08248352\n",
      "Iteration 137, loss = 0.11497649\n",
      "Iteration 261, loss = 0.08233374\n",
      "Iteration 262, loss = 0.08217446\n",
      "Iteration 64, loss = 0.15570546\n",
      "Iteration 138, loss = 0.11440153\n",
      "Iteration 199, loss = 0.08495115\n",
      "Iteration 179, loss = 0.09360310\n",
      "Iteration 200, loss = 0.08470030\n",
      "Iteration 65, loss = 0.15426163\n",
      "Iteration 89, loss = 0.13657356\n",
      "Iteration 201, loss = 0.08447937\n",
      "Iteration 384, loss = 0.07429250\n",
      "Iteration 90, loss = 0.13576676\n",
      "Iteration 385, loss = 0.07418967\n",
      "Iteration 139, loss = 0.11385943Iteration 386, loss = 0.07408836\n",
      "\n",
      "Iteration 91, loss = 0.13494704\n",
      "Iteration 180, loss = 0.09335849\n",
      "Iteration 137, loss = 0.10613318\n",
      "Iteration 181, loss = 0.09310149\n",
      "Iteration 263, loss = 0.08201563\n",
      "Iteration 182, loss = 0.09286061\n",
      "Iteration 264, loss = 0.08185772\n",
      "Iteration 387, loss = 0.07398470\n",
      "Iteration 138, loss = 0.10575875\n",
      "Iteration 140, loss = 0.11330808\n",
      "Iteration 388, loss = 0.07387881\n",
      "Iteration 265, loss = 0.08169746\n",
      "Iteration 389, loss = 0.07377771\n",
      "Iteration 202, loss = 0.08425032\n",
      "Iteration 266, loss = 0.08154673\n",
      "Iteration 390, loss = 0.07367567\n",
      "Iteration 267, loss = 0.08140035\n",
      "Iteration 141, loss = 0.11276415\n",
      "Iteration 268, loss = 0.08123796\n",
      "Iteration 391, loss = 0.07357342\n",
      "Iteration 203, loss = 0.08400916\n",
      "Iteration 269, loss = 0.08108284\n",
      "Iteration 392, loss = 0.07347274\n",
      "Iteration 393, loss = 0.07336980\n",
      "Iteration 270, loss = 0.08093949\n",
      "Iteration 394, loss = 0.07326575\n",
      "Iteration 204, loss = 0.08379015\n",
      "Iteration 142, loss = 0.11222566\n",
      "Iteration 139, loss = 0.10538191\n",
      "Iteration 205, loss = 0.08356692\n",
      "Iteration 66, loss = 0.15288327\n",
      "Iteration 183, loss = 0.09262514\n",
      "Iteration 143, loss = 0.11170360\n",
      "Iteration 271, loss = 0.08078294\n",
      "Iteration 140, loss = 0.10503709\n",
      "Iteration 395, loss = 0.07316661\n",
      "Iteration 396, loss = 0.07306633\n",
      "Iteration 397, loss = 0.07296730\n",
      "Iteration 272, loss = 0.08063429\n",
      "Iteration 273, loss = 0.08048529\n",
      "Iteration 206, loss = 0.08333481\n",
      "Iteration 274, loss = 0.08033444\n",
      "Iteration 398, loss = 0.07286563\n",
      "Iteration 207, loss = 0.08311557\n",
      "Iteration 399, loss = 0.07276772\n",
      "Iteration 400, loss = 0.07266733\n",
      "Iteration 208, loss = 0.08290348\n",
      "Iteration 141, loss = 0.10468372\n",
      "Iteration 401, loss = 0.07257703\n",
      "Iteration 275, loss = 0.08018830\n",
      "Iteration 209, loss = 0.08269868\n",
      "Iteration 402, loss = 0.07247119\n",
      "Iteration 403, loss = 0.07237695\n",
      "Iteration 142, loss = 0.10433715\n",
      "Iteration 92, loss = 0.13417968\n",
      "Iteration 184, loss = 0.09237326\n",
      "Iteration 144, loss = 0.11121035\n",
      "Iteration 185, loss = 0.09213729\n",
      "Iteration 67, loss = 0.15155547\n",
      "Iteration 186, loss = 0.09190515\n",
      "Iteration 145, loss = 0.11068610\n",
      "Iteration 404, loss = 0.07227440\n",
      "Iteration 210, loss = 0.08245745\n",
      "Iteration 405, loss = 0.07218102\n",
      "Iteration 406, loss = 0.07208320\n",
      "Iteration 146, loss = 0.11019725\n",
      "Iteration 407, loss = 0.07198390\n",
      "Iteration 187, loss = 0.09167065\n",
      "Iteration 408, loss = 0.07189213\n",
      "Iteration 143, loss = 0.10398285\n",
      "Iteration 409, loss = 0.07179469\n",
      "Iteration 188, loss = 0.09143652\n",
      "Iteration 410, loss = 0.07169802\n",
      "Iteration 276, loss = 0.08004412\n",
      "Iteration 211, loss = 0.08225350\n",
      "Iteration 93, loss = 0.13340128\n",
      "Iteration 277, loss = 0.07989135\n",
      "Iteration 94, loss = 0.13264915\n",
      "Iteration 147, loss = 0.10968692\n",
      "Iteration 68, loss = 0.15022615\n",
      "Iteration 95, loss = 0.13192524\n",
      "Iteration 96, loss = 0.13120121\n",
      "Iteration 144, loss = 0.10363354\n",
      "Iteration 411, loss = 0.07160073\n",
      "Iteration 97, loss = 0.13048230\n",
      "Iteration 412, loss = 0.07150166\n",
      "Iteration 413, loss = 0.07141092\n",
      "Iteration 98, loss = 0.12978268\n",
      "Iteration 414, loss = 0.07131573\n",
      "Iteration 145, loss = 0.10329232\n",
      "Iteration 415, loss = 0.07122651\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 189, loss = 0.09121004\n",
      "Iteration 278, loss = 0.07975092\n",
      "Iteration 212, loss = 0.08204626\n",
      "Iteration 148, loss = 0.10920955\n",
      "Iteration 279, loss = 0.07960718Iteration 190, loss = 0.09098217\n",
      "Iteration 149, loss = 0.10871659\n",
      "\n",
      "Iteration 280, loss = 0.07946500\n",
      "Iteration 281, loss = 0.07932128\n",
      "Iteration 150, loss = 0.10822408\n",
      "Iteration 191, loss = 0.09075258\n",
      "Iteration 282, loss = 0.07917337\n",
      "Iteration 69, loss = 0.14900301\n",
      "Iteration 283, loss = 0.07903817\n",
      "Iteration 192, loss = 0.09052983\n",
      "Iteration 193, loss = 0.09030860\n",
      "Iteration 194, loss = 0.09009069\n",
      "Iteration 213, loss = 0.08183477\n",
      "Iteration 195, loss = 0.08986836\n",
      "Iteration 70, loss = 0.14774626\n",
      "Iteration 214, loss = 0.08162572\n",
      "Iteration 146, loss = 0.10295098\n",
      "Iteration 284, loss = 0.07889123\n",
      "Iteration 215, loss = 0.08142227\n",
      "Iteration 216, loss = 0.08121757\n",
      "Iteration 71, loss = 0.14654952\n",
      "Iteration 151, loss = 0.10776450\n",
      "Iteration 196, loss = 0.08965140\n",
      "Iteration 197, loss = 0.08943814\n",
      "Iteration 285, loss = 0.07875125\n",
      "Iteration 152, loss = 0.10729462\n",
      "Iteration 147, loss = 0.10262735\n",
      "Iteration 217, loss = 0.08101794\n",
      "Iteration 286, loss = 0.07861391\n",
      "Iteration 198, loss = 0.08921612\n",
      "Iteration 287, loss = 0.07847165\n",
      "Iteration 218, loss = 0.08082015\n",
      "Iteration 219, loss = 0.08060172\n",
      "Iteration 220, loss = 0.08040864\n",
      "Iteration 288, loss = 0.07833825\n",
      "Iteration 289, loss = 0.07820327\n",
      "Iteration 72, loss = 0.14537688\n",
      "Iteration 290, loss = 0.07806239\n",
      "Iteration 221, loss = 0.08020274\n",
      "Iteration 222, loss = 0.08001327\n",
      "Iteration 291, loss = 0.07792827\n",
      "Iteration 223, loss = 0.07981387\n",
      "Iteration 148, loss = 0.10230447\n",
      "Iteration 199, loss = 0.08901022\n",
      "Iteration 99, loss = 0.12909928\n",
      "Iteration 153, loss = 0.10687183\n",
      "Iteration 200, loss = 0.08879829\n",
      "Iteration 149, loss = 0.10197469\n",
      "Iteration 154, loss = 0.10639738\n",
      "Iteration 150, loss = 0.10164901\n",
      "Iteration 1, loss = 0.86746895\n",
      "Iteration 201, loss = 0.08859279\n",
      "Iteration 151, loss = 0.10132739\n",
      "Iteration 155, loss = 0.10596166\n",
      "Iteration 152, loss = 0.10101615\n",
      "Iteration 100, loss = 0.12845352\n",
      "Iteration 153, loss = 0.10069499\n",
      "Iteration 202, loss = 0.08838133\n",
      "Iteration 224, loss = 0.07962730\n",
      "Iteration 101, loss = 0.12780863\n",
      "Iteration 2, loss = 0.80110982\n",
      "Iteration 203, loss = 0.08817336\n",
      "Iteration 225, loss = 0.07942164\n",
      "Iteration 226, loss = 0.07923686\n",
      "Iteration 292, loss = 0.07779211\n",
      "Iteration 102, loss = 0.12717066\n",
      "Iteration 154, loss = 0.10038784\n",
      "Iteration 204, loss = 0.08797277\n",
      "Iteration 103, loss = 0.12649679\n",
      "Iteration 155, loss = 0.10008213\n",
      "Iteration 293, loss = 0.07765915\n",
      "Iteration 294, loss = 0.07751837\n",
      "Iteration 295, loss = 0.07738831\n",
      "Iteration 104, loss = 0.12589318Iteration 296, loss = 0.07725845\n",
      "Iteration 156, loss = 0.09977027\n",
      "Iteration 297, loss = 0.07712620\n",
      "\n",
      "Iteration 298, loss = 0.07699171\n",
      "Iteration 299, loss = 0.07685997\n",
      "Iteration 156, loss = 0.10550688\n",
      "Iteration 300, loss = 0.07673235\n",
      "Iteration 3, loss = 0.72088241\n",
      "Iteration 105, loss = 0.12526956\n",
      "Iteration 301, loss = 0.07660138\n",
      "Iteration 227, loss = 0.07905129\n",
      "Iteration 302, loss = 0.07647384\n",
      "Iteration 228, loss = 0.07886033\n",
      "Iteration 303, loss = 0.07634560\n",
      "Iteration 106, loss = 0.12467481\n",
      "Iteration 304, loss = 0.07621271\n",
      "Iteration 305, loss = 0.07609853\n",
      "Iteration 73, loss = 0.14431215\n",
      "Iteration 306, loss = 0.07595860\n",
      "Iteration 4, loss = 0.64536698\n",
      "Iteration 157, loss = 0.10506517\n",
      "Iteration 205, loss = 0.08776423\n",
      "Iteration 229, loss = 0.07867476\n",
      "Iteration 230, loss = 0.07849792\n",
      "Iteration 158, loss = 0.10464630\n",
      "Iteration 74, loss = 0.14314552\n",
      "Iteration 206, loss = 0.08757103\n",
      "Iteration 231, loss = 0.07830461\n",
      "Iteration 159, loss = 0.10420873\n",
      "Iteration 207, loss = 0.08736498\n",
      "Iteration 307, loss = 0.07583245\n",
      "Iteration 232, loss = 0.07812157\n",
      "Iteration 160, loss = 0.10378138\n",
      "Iteration 208, loss = 0.08717387\n",
      "Iteration 75, loss = 0.14208635Iteration 233, loss = 0.07793724\n",
      "\n",
      "Iteration 209, loss = 0.08697067\n",
      "Iteration 161, loss = 0.10337270\n",
      "Iteration 234, loss = 0.07775715\n",
      "Iteration 210, loss = 0.08677802\n",
      "Iteration 107, loss = 0.12406964\n",
      "Iteration 162, loss = 0.10297121\n",
      "Iteration 235, loss = 0.07756898\n",
      "Iteration 211, loss = 0.08658703\n",
      "Iteration 212, loss = 0.08639064\n",
      "Iteration 157, loss = 0.09947546\n",
      "Iteration 213, loss = 0.08619176\n",
      "Iteration 76, loss = 0.14104059\n",
      "Iteration 214, loss = 0.08600500\n",
      "Iteration 163, loss = 0.10254977\n",
      "Iteration 164, loss = 0.10215226\n",
      "Iteration 5, loss = 0.58523300\n",
      "Iteration 308, loss = 0.07571350\n",
      "Iteration 108, loss = 0.12351117\n",
      "Iteration 309, loss = 0.07558465\n",
      "Iteration 310, loss = 0.07545590\n",
      "Iteration 311, loss = 0.07533208\n",
      "Iteration 236, loss = 0.07740789\n",
      "Iteration 312, loss = 0.07521014\n",
      "Iteration 313, loss = 0.07508625\n",
      "Iteration 165, loss = 0.10175720\n",
      "Iteration 109, loss = 0.12293424\n",
      "Iteration 158, loss = 0.09917299\n",
      "Iteration 314, loss = 0.07496471\n",
      "Iteration 166, loss = 0.10137852\n",
      "Iteration 315, loss = 0.07484555\n",
      "Iteration 316, loss = 0.07472429\n",
      "Iteration 167, loss = 0.10098768\n",
      "Iteration 317, loss = 0.07459765\n",
      "Iteration 215, loss = 0.08581471\n",
      "Iteration 110, loss = 0.12237102\n",
      "Iteration 168, loss = 0.10061823\n",
      "Iteration 318, loss = 0.07447868\n",
      "Iteration 159, loss = 0.09887581\n",
      "Iteration 111, loss = 0.12182970\n",
      "Iteration 160, loss = 0.09859264\n",
      "Iteration 216, loss = 0.08562581\n",
      "Iteration 237, loss = 0.07722912\n",
      "Iteration 217, loss = 0.08543625\n",
      "Iteration 77, loss = 0.14003659\n",
      "Iteration 218, loss = 0.08525877\n",
      "Iteration 169, loss = 0.10022910\n",
      "Iteration 319, loss = 0.07436026\n",
      "Iteration 219, loss = 0.08507518\n",
      "Iteration 238, loss = 0.07705371\n",
      "Iteration 320, loss = 0.07424051\n",
      "Iteration 112, loss = 0.12127728\n",
      "Iteration 239, loss = 0.07688141\n",
      "Iteration 170, loss = 0.09983054\n",
      "Iteration 321, loss = 0.07412479\n",
      "Iteration 240, loss = 0.07670856\n",
      "Iteration 220, loss = 0.08489498\n",
      "Iteration 322, loss = 0.07400268\n",
      "Iteration 241, loss = 0.07653617\n",
      "Iteration 171, loss = 0.09946250\n",
      "Iteration 242, loss = 0.07637136\n",
      "Iteration 78, loss = 0.13907463\n",
      "Iteration 323, loss = 0.07389092\n",
      "Iteration 172, loss = 0.09909780\n",
      "Iteration 324, loss = 0.07378047\n",
      "Iteration 173, loss = 0.09871838\n",
      "Iteration 161, loss = 0.09829993\n",
      "Iteration 174, loss = 0.09834811\n",
      "Iteration 6, loss = 0.53779141\n",
      "Iteration 325, loss = 0.07365153\n",
      "Iteration 79, loss = 0.13812465\n",
      "Iteration 162, loss = 0.09800643\n",
      "Iteration 221, loss = 0.08471446\n",
      "Iteration 7, loss = 0.50218757\n",
      "Iteration 222, loss = 0.08452942\n",
      "Iteration 163, loss = 0.09772714\n",
      "Iteration 223, loss = 0.08435000\n",
      "Iteration 164, loss = 0.09744643\n",
      "Iteration 224, loss = 0.08417699\n",
      "Iteration 165, loss = 0.09716260\n",
      "Iteration 225, loss = 0.08399464\n",
      "Iteration 243, loss = 0.07623649\n",
      "Iteration 113, loss = 0.12073632\n",
      "Iteration 226, loss = 0.08382238\n",
      "Iteration 244, loss = 0.07602730\n",
      "Iteration 326, loss = 0.07354034\n",
      "Iteration 327, loss = 0.07342504\n",
      "Iteration 245, loss = 0.07586632\n",
      "Iteration 114, loss = 0.12021321\n",
      "Iteration 246, loss = 0.07570331\n",
      "Iteration 8, loss = 0.47430601\n",
      "Iteration 328, loss = 0.07331111\n",
      "Iteration 166, loss = 0.09688787\n",
      "Iteration 80, loss = 0.13715622\n",
      "Iteration 329, loss = 0.07319928\n",
      "Iteration 227, loss = 0.08364690\n",
      "Iteration 175, loss = 0.09798987\n",
      "Iteration 228, loss = 0.08347727\n",
      "Iteration 247, loss = 0.07555945\n",
      "Iteration 167, loss = 0.09661770\n",
      "Iteration 229, loss = 0.08330169\n",
      "Iteration 115, loss = 0.11969944\n",
      "Iteration 230, loss = 0.08313085\n",
      "Iteration 9, loss = 0.45212321\n",
      "Iteration 248, loss = 0.07536822\n",
      "Iteration 168, loss = 0.09634853\n",
      "Iteration 231, loss = 0.08295992\n",
      "Iteration 249, loss = 0.07520811\n",
      "Iteration 232, loss = 0.08279606\n",
      "Iteration 233, loss = 0.08262600\n",
      "Iteration 169, loss = 0.09607639\n",
      "Iteration 234, loss = 0.08246738\n",
      "Iteration 116, loss = 0.11919083\n",
      "Iteration 330, loss = 0.07307880\n",
      "Iteration 170, loss = 0.09580439\n",
      "Iteration 10, loss = 0.43425985\n",
      "Iteration 250, loss = 0.07505507\n",
      "Iteration 171, loss = 0.09555194\n",
      "Iteration 172, loss = 0.09528605\n",
      "Iteration 251, loss = 0.07488130\n",
      "Iteration 81, loss = 0.13626468\n",
      "Iteration 173, loss = 0.09501147\n",
      "Iteration 331, loss = 0.07297175\n",
      "Iteration 332, loss = 0.07286313\n",
      "Iteration 252, loss = 0.07472528\n",
      "Iteration 333, loss = 0.07275512\n",
      "Iteration 334, loss = 0.07263711\n",
      "Iteration 176, loss = 0.09763845\n",
      "Iteration 335, loss = 0.07252559\n",
      "Iteration 253, loss = 0.07456371\n",
      "Iteration 336, loss = 0.07241753\n",
      "Iteration 337, loss = 0.07230645\n",
      "Iteration 254, loss = 0.07441194\n",
      "Iteration 177, loss = 0.09728219\n",
      "Iteration 117, loss = 0.11866055\n",
      "Iteration 255, loss = 0.07425287\n",
      "Iteration 11, loss = 0.41967126\n",
      "Iteration 178, loss = 0.09695557\n",
      "Iteration 118, loss = 0.11817123\n",
      "Iteration 179, loss = 0.09657751\n",
      "Iteration 82, loss = 0.13537904\n",
      "Iteration 338, loss = 0.07219499\n",
      "Iteration 119, loss = 0.11768733\n",
      "Iteration 339, loss = 0.07208555\n",
      "Iteration 180, loss = 0.09623093\n",
      "Iteration 340, loss = 0.07197657\n",
      "Iteration 120, loss = 0.11719064\n",
      "Iteration 341, loss = 0.07187233\n",
      "Iteration 181, loss = 0.09587932\n",
      "Iteration 256, loss = 0.07408661\n",
      "Iteration 342, loss = 0.07176173\n",
      "Iteration 235, loss = 0.08229354\n",
      "Iteration 343, loss = 0.07165117\n",
      "Iteration 182, loss = 0.09556406\n",
      "Iteration 344, loss = 0.07154600\n",
      "Iteration 174, loss = 0.09476097\n",
      "Iteration 257, loss = 0.07394025\n",
      "Iteration 183, loss = 0.09521142\n",
      "Iteration 83, loss = 0.13450273\n",
      "Iteration 236, loss = 0.08213084\n",
      "Iteration 237, loss = 0.08196843\n",
      "Iteration 175, loss = 0.09448725\n",
      "Iteration 258, loss = 0.07378163\n",
      "Iteration 12, loss = 0.40669751\n",
      "Iteration 176, loss = 0.09424604\n",
      "Iteration 259, loss = 0.07363670\n",
      "Iteration 238, loss = 0.08180115\n",
      "Iteration 260, loss = 0.07348272\n",
      "Iteration 13, loss = 0.39529301\n",
      "Iteration 121, loss = 0.11671606\n",
      "Iteration 345, loss = 0.07143901\n",
      "Iteration 184, loss = 0.09485763\n",
      "Iteration 346, loss = 0.07133125\n",
      "Iteration 177, loss = 0.09398382\n",
      "Iteration 347, loss = 0.07122906\n",
      "Iteration 185, loss = 0.09452268\n",
      "Iteration 348, loss = 0.07112335\n",
      "Iteration 178, loss = 0.09373538\n",
      "Iteration 186, loss = 0.09418588\n",
      "Iteration 84, loss = 0.13367495\n",
      "Iteration 349, loss = 0.07101918\n",
      "Iteration 179, loss = 0.09349157\n",
      "Iteration 122, loss = 0.11624152\n",
      "Iteration 239, loss = 0.08164180\n",
      "Iteration 350, loss = 0.07091317\n",
      "Iteration 180, loss = 0.09323278\n",
      "Iteration 240, loss = 0.08148442\n",
      "Iteration 261, loss = 0.07333285\n",
      "Iteration 241, loss = 0.08132224\n",
      "Iteration 123, loss = 0.11579239\n",
      "Iteration 242, loss = 0.08116697\n",
      "Iteration 262, loss = 0.07316825\n",
      "Iteration 263, loss = 0.07302978\n",
      "Iteration 187, loss = 0.09385837\n",
      "Iteration 181, loss = 0.09298594\n",
      "Iteration 188, loss = 0.09352681\n",
      "Iteration 85, loss = 0.13282817\n",
      "Iteration 351, loss = 0.07081334\n",
      "Iteration 264, loss = 0.07287980\n",
      "Iteration 189, loss = 0.09320308\n",
      "Iteration 352, loss = 0.07070339\n",
      "Iteration 14, loss = 0.38495291\n",
      "Iteration 190, loss = 0.09288763\n",
      "Iteration 182, loss = 0.09274059\n",
      "Iteration 243, loss = 0.08100141\n",
      "Iteration 265, loss = 0.07272286\n",
      "Iteration 191, loss = 0.09259522\n",
      "Iteration 353, loss = 0.07059997\n",
      "Iteration 266, loss = 0.07257809\n",
      "Iteration 244, loss = 0.08084690\n",
      "Iteration 267, loss = 0.07243579\n",
      "Iteration 354, loss = 0.07049919\n",
      "Iteration 15, loss = 0.37549937\n",
      "Iteration 355, loss = 0.07039505\n",
      "Iteration 86, loss = 0.13204979\n",
      "Iteration 245, loss = 0.08069080\n",
      "Iteration 192, loss = 0.09227715\n",
      "Iteration 183, loss = 0.09249811\n",
      "Iteration 124, loss = 0.11532016\n",
      "Iteration 184, loss = 0.09226416\n",
      "Iteration 87, loss = 0.13127037\n",
      "Iteration 125, loss = 0.11487377\n",
      "Iteration 268, loss = 0.07229383\n",
      "Iteration 356, loss = 0.07029587\n",
      "Iteration 269, loss = 0.07213870\n",
      "Iteration 270, loss = 0.07199727\n",
      "Iteration 193, loss = 0.09196662\n",
      "Iteration 357, loss = 0.07019050\n",
      "Iteration 271, loss = 0.07186207\n",
      "Iteration 194, loss = 0.09166704\n",
      "Iteration 358, loss = 0.07010166\n",
      "Iteration 16, loss = 0.36649721\n",
      "Iteration 185, loss = 0.09201954\n",
      "Iteration 195, loss = 0.09134767\n",
      "Iteration 246, loss = 0.08053672\n",
      "Iteration 247, loss = 0.08038332\n",
      "Iteration 126, loss = 0.11443675\n",
      "Iteration 196, loss = 0.09105840\n",
      "Iteration 248, loss = 0.08022944\n",
      "Iteration 88, loss = 0.13050338\n",
      "Iteration 359, loss = 0.06999518\n",
      "Iteration 249, loss = 0.08007510\n",
      "Iteration 272, loss = 0.07171621\n",
      "Iteration 186, loss = 0.09179776\n",
      "Iteration 273, loss = 0.07157442\n",
      "Iteration 274, loss = 0.07142890\n",
      "Iteration 360, loss = 0.06989753\n",
      "Iteration 361, loss = 0.06979672\n",
      "Iteration 362, loss = 0.06969932\n",
      "Iteration 275, loss = 0.07130331\n",
      "Iteration 197, loss = 0.09075977\n",
      "Iteration 127, loss = 0.11399343\n",
      "Iteration 363, loss = 0.06960215\n",
      "Iteration 250, loss = 0.07992650\n",
      "Iteration 276, loss = 0.07116292\n",
      "Iteration 89, loss = 0.12975413\n",
      "Iteration 364, loss = 0.06950183\n",
      "Iteration 128, loss = 0.11355749\n",
      "Iteration 365, loss = 0.06941230\n",
      "Iteration 277, loss = 0.07101754\n",
      "Iteration 366, loss = 0.06930704\n",
      "Iteration 367, loss = 0.06922022\n",
      "Iteration 17, loss = 0.35793658\n",
      "Iteration 198, loss = 0.09048955\n",
      "Iteration 368, loss = 0.06912027\n",
      "Iteration 187, loss = 0.09155636\n",
      "Iteration 90, loss = 0.12902743\n",
      "Iteration 278, loss = 0.07088522\n",
      "Iteration 199, loss = 0.09016703\n",
      "Iteration 279, loss = 0.07075395\n",
      "Iteration 200, loss = 0.08989949\n",
      "Iteration 188, loss = 0.09131955\n",
      "Iteration 251, loss = 0.07977231\n",
      "Iteration 280, loss = 0.07061846\n",
      "Iteration 91, loss = 0.12829920\n",
      "Iteration 201, loss = 0.08960337\n",
      "Iteration 369, loss = 0.06902255\n",
      "Iteration 252, loss = 0.07962229\n",
      "Iteration 281, loss = 0.07048813\n",
      "Iteration 202, loss = 0.08935535\n",
      "Iteration 282, loss = 0.07034109\n",
      "Iteration 283, loss = 0.07021450\n",
      "Iteration 253, loss = 0.07947720\n",
      "Iteration 189, loss = 0.09109973\n",
      "Iteration 284, loss = 0.07007749\n",
      "Iteration 254, loss = 0.07933116\n",
      "Iteration 129, loss = 0.11313738\n",
      "Iteration 190, loss = 0.09087570\n",
      "Iteration 255, loss = 0.07917941\n",
      "Iteration 191, loss = 0.09063870\n",
      "Iteration 192, loss = 0.09044424\n",
      "Iteration 256, loss = 0.07903370Iteration 18, loss = 0.34980670\n",
      "\n",
      "Iteration 370, loss = 0.06892889\n",
      "Iteration 130, loss = 0.11272875\n",
      "Iteration 193, loss = 0.09020156\n",
      "Iteration 257, loss = 0.07889046\n",
      "Iteration 371, loss = 0.06883077\n",
      "Iteration 372, loss = 0.06873613\n",
      "Iteration 131, loss = 0.11229267\n",
      "Iteration 258, loss = 0.07874253\n",
      "Iteration 373, loss = 0.06864478\n",
      "Iteration 374, loss = 0.06855510\n",
      "Iteration 132, loss = 0.11188797\n",
      "Iteration 194, loss = 0.08998260\n",
      "Iteration 19, loss = 0.34184654\n",
      "Iteration 92, loss = 0.12760281\n",
      "Iteration 259, loss = 0.07860202\n",
      "Iteration 285, loss = 0.06995905\n",
      "Iteration 203, loss = 0.08904858\n",
      "Iteration 195, loss = 0.08977006\n",
      "Iteration 286, loss = 0.06983419\n",
      "Iteration 375, loss = 0.06845829\n",
      "Iteration 204, loss = 0.08876297\n",
      "Iteration 376, loss = 0.06836519\n",
      "Iteration 377, loss = 0.06827594\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 205, loss = 0.08848748\n",
      "Iteration 287, loss = 0.06969245\n",
      "Iteration 133, loss = 0.11149136\n",
      "Iteration 260, loss = 0.07846569\n",
      "Iteration 261, loss = 0.07831778Iteration 134, loss = 0.11109026\n",
      "Iteration 20, loss = 0.33440741\n",
      "\n",
      "Iteration 262, loss = 0.07818374\n",
      "Iteration 21, loss = 0.32719292\n",
      "Iteration 93, loss = 0.12690531\n",
      "Iteration 135, loss = 0.11069835\n",
      "Iteration 206, loss = 0.08822668\n",
      "Iteration 94, loss = 0.12624864\n",
      "Iteration 196, loss = 0.08955111\n",
      "Iteration 288, loss = 0.06956838\n",
      "Iteration 136, loss = 0.11032055\n",
      "Iteration 197, loss = 0.08933131\n",
      "Iteration 289, loss = 0.06942879\n",
      "Iteration 263, loss = 0.07803544\n",
      "Iteration 290, loss = 0.06930559\n",
      "Iteration 264, loss = 0.07789164\n",
      "Iteration 265, loss = 0.07775216\n",
      "Iteration 291, loss = 0.06918561\n",
      "Iteration 198, loss = 0.08912699\n",
      "Iteration 266, loss = 0.07762474\n",
      "Iteration 292, loss = 0.06906895\n",
      "Iteration 293, loss = 0.06894263\n",
      "Iteration 267, loss = 0.07748305\n",
      "Iteration 199, loss = 0.08890701\n",
      "Iteration 294, loss = 0.06880891\n",
      "Iteration 295, loss = 0.06869519\n",
      "Iteration 200, loss = 0.08871288\n",
      "Iteration 268, loss = 0.07734161\n",
      "Iteration 296, loss = 0.06857408\n",
      "Iteration 201, loss = 0.08849557\n",
      "Iteration 297, loss = 0.06844206\n",
      "Iteration 137, loss = 0.10992224\n",
      "Iteration 298, loss = 0.06832795\n",
      "Iteration 202, loss = 0.08830861\n",
      "Iteration 299, loss = 0.06821852\n",
      "Iteration 207, loss = 0.08796437\n",
      "Iteration 95, loss = 0.12558487\n",
      "Iteration 300, loss = 0.06808943\n",
      "Iteration 22, loss = 0.31989839\n",
      "Iteration 301, loss = 0.06797079\n",
      "Iteration 1, loss = 0.86326201\n",
      "Iteration 208, loss = 0.08770241\n",
      "Iteration 302, loss = 0.06784659\n",
      "Iteration 303, loss = 0.06772643\n",
      "Iteration 138, loss = 0.10955394\n",
      "Iteration 304, loss = 0.06760744\n",
      "Iteration 209, loss = 0.08744279\n",
      "Iteration 305, loss = 0.06751925\n",
      "Iteration 306, loss = 0.06738362\n",
      "Iteration 96, loss = 0.12494478\n",
      "Iteration 210, loss = 0.08717914\n",
      "Iteration 211, loss = 0.08692261\n",
      "Iteration 269, loss = 0.07720547\n",
      "Iteration 270, loss = 0.07707604\n",
      "Iteration 271, loss = 0.07693516\n",
      "Iteration 139, loss = 0.10916855\n",
      "Iteration 272, loss = 0.07680887\n",
      "Iteration 273, loss = 0.07666779\n",
      "Iteration 274, loss = 0.07653867\n",
      "Iteration 140, loss = 0.10881343\n",
      "Iteration 275, loss = 0.07640628\n",
      "Iteration 23, loss = 0.31303745\n",
      "Iteration 212, loss = 0.08667311\n",
      "Iteration 276, loss = 0.07627341\n",
      "Iteration 277, loss = 0.07614721\n",
      "Iteration 278, loss = 0.07601648\n",
      "Iteration 97, loss = 0.12432394\n",
      "Iteration 307, loss = 0.06727138\n",
      "Iteration 203, loss = 0.08807843Iteration 213, loss = 0.08640925\n",
      "Iteration 98, loss = 0.12368933\n",
      "Iteration 308, loss = 0.06715536\n",
      "\n",
      "Iteration 214, loss = 0.08616119\n",
      "Iteration 24, loss = 0.30648300\n",
      "Iteration 215, loss = 0.08590932\n",
      "Iteration 141, loss = 0.10843974\n",
      "Iteration 25, loss = 0.29999471\n",
      "Iteration 204, loss = 0.08787891\n",
      "Iteration 2, loss = 0.81453467\n",
      "Iteration 309, loss = 0.06703166\n",
      "Iteration 279, loss = 0.07588976\n",
      "Iteration 205, loss = 0.08768442\n",
      "Iteration 310, loss = 0.06692310\n",
      "Iteration 206, loss = 0.08747544\n",
      "Iteration 142, loss = 0.10807974\n",
      "Iteration 207, loss = 0.08727969\n",
      "Iteration 208, loss = 0.08707726\n",
      "Iteration 311, loss = 0.06680596\n",
      "Iteration 280, loss = 0.07575934\n",
      "Iteration 99, loss = 0.12308510\n",
      "Iteration 312, loss = 0.06670492\n",
      "Iteration 313, loss = 0.06658633Iteration 281, loss = 0.07562953\n",
      "Iteration 216, loss = 0.08567682\n",
      "Iteration 282, loss = 0.07550296\n",
      "Iteration 26, loss = 0.29382713\n",
      "\n",
      "Iteration 100, loss = 0.12249644\n",
      "Iteration 283, loss = 0.07538148\n",
      "Iteration 209, loss = 0.08689657\n",
      "Iteration 217, loss = 0.08543092\n",
      "Iteration 314, loss = 0.06648708\n",
      "Iteration 315, loss = 0.06637571\n",
      "Iteration 3, loss = 0.75293055\n",
      "Iteration 101, loss = 0.12192657\n",
      "Iteration 316, loss = 0.06625766\n",
      "Iteration 143, loss = 0.10774940\n",
      "Iteration 317, loss = 0.06614753\n",
      "Iteration 218, loss = 0.08519578\n",
      "Iteration 27, loss = 0.28781869\n",
      "Iteration 210, loss = 0.08668687\n",
      "Iteration 219, loss = 0.08494157\n",
      "Iteration 144, loss = 0.10738031\n",
      "Iteration 102, loss = 0.12134426\n",
      "Iteration 145, loss = 0.10702414\n",
      "Iteration 318, loss = 0.06603896\n",
      "Iteration 211, loss = 0.08649439\n",
      "Iteration 284, loss = 0.07525587\n",
      "Iteration 220, loss = 0.08474823\n",
      "Iteration 319, loss = 0.06592893\n",
      "Iteration 146, loss = 0.10668080\n",
      "Iteration 285, loss = 0.07513022\n",
      "Iteration 221, loss = 0.08448392\n",
      "Iteration 320, loss = 0.06582611\n",
      "Iteration 147, loss = 0.10635811\n",
      "Iteration 212, loss = 0.08630815\n",
      "Iteration 321, loss = 0.06572125\n",
      "Iteration 148, loss = 0.10599492\n",
      "Iteration 28, loss = 0.28202131\n",
      "Iteration 103, loss = 0.12080932\n",
      "Iteration 213, loss = 0.08611744\n",
      "Iteration 286, loss = 0.07500205\n",
      "Iteration 29, loss = 0.27636314\n",
      "Iteration 287, loss = 0.07488319\n",
      "Iteration 104, loss = 0.12022521\n",
      "Iteration 322, loss = 0.06560968\n",
      "Iteration 288, loss = 0.07475597\n",
      "Iteration 214, loss = 0.08592425\n",
      "Iteration 149, loss = 0.10567502\n",
      "Iteration 30, loss = 0.27100569\n",
      "Iteration 222, loss = 0.08425350\n",
      "Iteration 289, loss = 0.07463462\n",
      "Iteration 150, loss = 0.10535646\n",
      "Iteration 4, loss = 0.69482308\n",
      "Iteration 223, loss = 0.08401683\n",
      "Iteration 224, loss = 0.08380114\n",
      "Iteration 290, loss = 0.07451662\n",
      "Iteration 323, loss = 0.06552658\n",
      "Iteration 291, loss = 0.07438769\n",
      "Iteration 324, loss = 0.06540007\n",
      "Iteration 215, loss = 0.08575111\n",
      "Iteration 292, loss = 0.07426814\n",
      "Iteration 225, loss = 0.08356500\n",
      "Iteration 293, loss = 0.07414392\n",
      "Iteration 216, loss = 0.08555340\n",
      "Iteration 325, loss = 0.06529262\n",
      "Iteration 226, loss = 0.08334073\n",
      "Iteration 151, loss = 0.10501559\n",
      "Iteration 217, loss = 0.08536586\n",
      "Iteration 326, loss = 0.06519078\n",
      "Iteration 227, loss = 0.08312472\n",
      "Iteration 327, loss = 0.06508590\n",
      "Iteration 218, loss = 0.08517824\n",
      "Iteration 31, loss = 0.26572850\n",
      "Iteration 328, loss = 0.06499038\n",
      "Iteration 329, loss = 0.06488717\n",
      "Iteration 330, loss = 0.06478139\n",
      "Iteration 331, loss = 0.06467502\n",
      "Iteration 105, loss = 0.11968948\n",
      "Iteration 332, loss = 0.06457341\n",
      "Iteration 333, loss = 0.06447094\n",
      "Iteration 334, loss = 0.06436852\n",
      "Iteration 335, loss = 0.06427498\n",
      "Iteration 228, loss = 0.08289109\n",
      "Iteration 152, loss = 0.10470383\n",
      "Iteration 336, loss = 0.06417289\n",
      "Iteration 32, loss = 0.26074363\n",
      "Iteration 294, loss = 0.07403321\n",
      "Iteration 5, loss = 0.64339913\n",
      "Iteration 106, loss = 0.11916295\n",
      "Iteration 295, loss = 0.07391277\n",
      "Iteration 337, loss = 0.06406766\n",
      "Iteration 219, loss = 0.08499951\n",
      "Iteration 296, loss = 0.07379088\n",
      "Iteration 297, loss = 0.07367516\n",
      "Iteration 229, loss = 0.08268715\n",
      "Iteration 107, loss = 0.11863846\n",
      "Iteration 298, loss = 0.07355452\n",
      "Iteration 299, loss = 0.07344489\n",
      "Iteration 230, loss = 0.08248357\n",
      "Iteration 108, loss = 0.11812807\n",
      "Iteration 231, loss = 0.08224554\n",
      "Iteration 220, loss = 0.08481600\n",
      "Iteration 33, loss = 0.25587011\n",
      "Iteration 300, loss = 0.07332205\n",
      "Iteration 221, loss = 0.08463585\n",
      "Iteration 153, loss = 0.10438600\n",
      "Iteration 34, loss = 0.25111497\n",
      "Iteration 301, loss = 0.07320949\n",
      "Iteration 302, loss = 0.07308996\n",
      "Iteration 154, loss = 0.10405783\n",
      "Iteration 303, loss = 0.07297620\n",
      "Iteration 222, loss = 0.08445431\n",
      "Iteration 223, loss = 0.08428325\n",
      "Iteration 155, loss = 0.10374147\n",
      "Iteration 224, loss = 0.08411307\n",
      "Iteration 232, loss = 0.08203298\n",
      "Iteration 225, loss = 0.08392562\n",
      "Iteration 338, loss = 0.06397242\n",
      "Iteration 156, loss = 0.10344104\n",
      "Iteration 6, loss = 0.60199233\n",
      "Iteration 109, loss = 0.11763002\n",
      "Iteration 233, loss = 0.08183804\n",
      "Iteration 339, loss = 0.06388423\n",
      "Iteration 226, loss = 0.08375731\n",
      "Iteration 340, loss = 0.06378462\n",
      "Iteration 157, loss = 0.10313737\n",
      "Iteration 341, loss = 0.06368560\n",
      "Iteration 304, loss = 0.07286020\n",
      "Iteration 158, loss = 0.10285605\n",
      "Iteration 227, loss = 0.08357713\n",
      "Iteration 305, loss = 0.07274833\n",
      "Iteration 306, loss = 0.07264091\n",
      "Iteration 234, loss = 0.08162361\n",
      "Iteration 307, loss = 0.07252580\n",
      "Iteration 308, loss = 0.07241677\n",
      "Iteration 235, loss = 0.08140714\n",
      "Iteration 342, loss = 0.06358808\n",
      "Iteration 110, loss = 0.11712798\n",
      "Iteration 309, loss = 0.07230706\n",
      "Iteration 236, loss = 0.08120019\n",
      "Iteration 35, loss = 0.24655344\n",
      "Iteration 310, loss = 0.07219186\n",
      "Iteration 343, loss = 0.06348814\n",
      "Iteration 237, loss = 0.08100720\n",
      "Iteration 111, loss = 0.11663626\n",
      "Iteration 344, loss = 0.06339000\n",
      "Iteration 228, loss = 0.08340722\n",
      "Iteration 345, loss = 0.06329718\n",
      "Iteration 238, loss = 0.08078774\n",
      "Iteration 229, loss = 0.08324147\n",
      "Iteration 346, loss = 0.06320549\n",
      "Iteration 347, loss = 0.06312538\n",
      "Iteration 230, loss = 0.08306799\n",
      "Iteration 159, loss = 0.10253243\n",
      "Iteration 112, loss = 0.11616505\n",
      "Iteration 160, loss = 0.10224381\n",
      "Iteration 7, loss = 0.56616263\n",
      "Iteration 231, loss = 0.08292667\n",
      "Iteration 161, loss = 0.10194959\n",
      "Iteration 239, loss = 0.08061322\n",
      "Iteration 232, loss = 0.08274471\n",
      "Iteration 233, loss = 0.08257879\n",
      "Iteration 162, loss = 0.10167175\n",
      "Iteration 348, loss = 0.06301062\n",
      "Iteration 349, loss = 0.06292302\n",
      "Iteration 311, loss = 0.07208660\n",
      "Iteration 350, loss = 0.06284910\n",
      "Iteration 351, loss = 0.06273592\n",
      "Iteration 36, loss = 0.24226107\n",
      "Iteration 352, loss = 0.06264451\n",
      "Iteration 312, loss = 0.07197630\n",
      "Iteration 163, loss = 0.10136480\n",
      "Iteration 353, loss = 0.06254965\n",
      "Iteration 354, loss = 0.06245732\n",
      "Iteration 8, loss = 0.53862643\n",
      "Iteration 240, loss = 0.08041733\n",
      "Iteration 355, loss = 0.06236465\n",
      "Iteration 164, loss = 0.10108579\n",
      "Iteration 313, loss = 0.07186139\n",
      "Iteration 234, loss = 0.08241002\n",
      "Iteration 241, loss = 0.08020431\n",
      "Iteration 242, loss = 0.08000858\n",
      "Iteration 37, loss = 0.23798336\n",
      "Iteration 243, loss = 0.07981484\n",
      "Iteration 314, loss = 0.07176156\n",
      "Iteration 235, loss = 0.08223411\n",
      "Iteration 113, loss = 0.11569154\n",
      "Iteration 38, loss = 0.23393785\n",
      "Iteration 315, loss = 0.07164726\n",
      "Iteration 244, loss = 0.07962100\n",
      "Iteration 316, loss = 0.07154222\n",
      "Iteration 236, loss = 0.08208433\n",
      "Iteration 245, loss = 0.07946376\n",
      "Iteration 317, loss = 0.07143179\n",
      "Iteration 165, loss = 0.10081412\n",
      "Iteration 356, loss = 0.06227086\n",
      "Iteration 237, loss = 0.08193024\n",
      "Iteration 39, loss = 0.23006225\n",
      "Iteration 9, loss = 0.51536108\n",
      "Iteration 318, loss = 0.07133047\n",
      "Iteration 357, loss = 0.06218808\n",
      "Iteration 166, loss = 0.10052773\n",
      "Iteration 358, loss = 0.06209080\n",
      "Iteration 359, loss = 0.06199886\n",
      "Iteration 238, loss = 0.08175094\n",
      "Iteration 114, loss = 0.11522904\n",
      "Iteration 246, loss = 0.07925366\n",
      "Iteration 319, loss = 0.07123149\n",
      "Iteration 167, loss = 0.10024622\n",
      "Iteration 360, loss = 0.06190087\n",
      "Iteration 320, loss = 0.07111819\n",
      "Iteration 115, loss = 0.11477298\n",
      "Iteration 361, loss = 0.06182534\n",
      "Iteration 247, loss = 0.07904909\n",
      "Iteration 168, loss = 0.09998926\n",
      "Iteration 248, loss = 0.07886278\n",
      "Iteration 362, loss = 0.06172984\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 239, loss = 0.08159881\n",
      "Iteration 321, loss = 0.07101763\n",
      "Iteration 116, loss = 0.11433674\n",
      "Iteration 322, loss = 0.07091150\n",
      "Iteration 40, loss = 0.22631830\n",
      "Iteration 240, loss = 0.08143450\n",
      "Iteration 241, loss = 0.08126298\n",
      "Iteration 10, loss = 0.49490150\n",
      "Iteration 323, loss = 0.07080784\n",
      "Iteration 242, loss = 0.08111612\n",
      "Iteration 41, loss = 0.22275249\n",
      "Iteration 324, loss = 0.07070323\n",
      "Iteration 243, loss = 0.08097039\n",
      "Iteration 325, loss = 0.07060330\n",
      "Iteration 244, loss = 0.08079571\n",
      "Iteration 1, loss = 0.81553961\n",
      "Iteration 245, loss = 0.08064646\n",
      "Iteration 42, loss = 0.21915242\n",
      "Iteration 326, loss = 0.07049512Iteration 2, loss = 0.78746539\n",
      "\n",
      "Iteration 246, loss = 0.08048852\n",
      "Iteration 3, loss = 0.75045835\n",
      "Iteration 247, loss = 0.08033372\n",
      "Iteration 169, loss = 0.09971594\n",
      "Iteration 170, loss = 0.09944333\n",
      "Iteration 4, loss = 0.71211435\n",
      "Iteration 249, loss = 0.07868028\n",
      "Iteration 117, loss = 0.11388980\n",
      "Iteration 250, loss = 0.07848961\n",
      "Iteration 171, loss = 0.09919389\n",
      "Iteration 327, loss = 0.07039349\n",
      "Iteration 328, loss = 0.07029738\n",
      "Iteration 172, loss = 0.09892145\n",
      "Iteration 329, loss = 0.07019363\n",
      "Iteration 330, loss = 0.07009248\n",
      "Iteration 173, loss = 0.09867049\n",
      "Iteration 251, loss = 0.07830573\n",
      "Iteration 118, loss = 0.11344185\n",
      "Iteration 331, loss = 0.06999330\n",
      "Iteration 11, loss = 0.47779223\n",
      "Iteration 252, loss = 0.07813646\n",
      "Iteration 174, loss = 0.09840046\n",
      "Iteration 119, loss = 0.11302356\n",
      "Iteration 253, loss = 0.07796506\n",
      "Iteration 254, loss = 0.07779472\n",
      "Iteration 120, loss = 0.11259608\n",
      "Iteration 255, loss = 0.07760402\n",
      "Iteration 175, loss = 0.09815756\n",
      "Iteration 256, loss = 0.07744220\n",
      "Iteration 176, loss = 0.09789065\n",
      "Iteration 121, loss = 0.11216972\n",
      "Iteration 248, loss = 0.08019409\n",
      "Iteration 177, loss = 0.09763815\n",
      "Iteration 332, loss = 0.06989069\n",
      "Iteration 122, loss = 0.11176523\n",
      "Iteration 43, loss = 0.21596539\n",
      "Iteration 333, loss = 0.06979402\n",
      "Iteration 5, loss = 0.67555934\n",
      "Iteration 249, loss = 0.08004425\n",
      "Iteration 123, loss = 0.11134790\n",
      "Iteration 257, loss = 0.07725612\n",
      "Iteration 124, loss = 0.11096863\n",
      "Iteration 258, loss = 0.07708999\n",
      "Iteration 12, loss = 0.46258498\n",
      "Iteration 6, loss = 0.64279205\n",
      "Iteration 178, loss = 0.09739650\n",
      "Iteration 259, loss = 0.07692705\n",
      "Iteration 125, loss = 0.11057159\n",
      "Iteration 334, loss = 0.06969716\n",
      "Iteration 260, loss = 0.07675519\n",
      "Iteration 7, loss = 0.61370352\n",
      "Iteration 261, loss = 0.07659749\n",
      "Iteration 44, loss = 0.21263175\n",
      "Iteration 262, loss = 0.07641540\n",
      "Iteration 335, loss = 0.06960480\n",
      "Iteration 126, loss = 0.11018430\n",
      "Iteration 250, loss = 0.07987596\n",
      "Iteration 336, loss = 0.06949954\n",
      "Iteration 263, loss = 0.07625260\n",
      "Iteration 127, loss = 0.10979921\n",
      "Iteration 179, loss = 0.09715256\n",
      "Iteration 337, loss = 0.06939876\n",
      "Iteration 251, loss = 0.07972796\n",
      "Iteration 8, loss = 0.58768099\n",
      "Iteration 128, loss = 0.10942908\n",
      "Iteration 45, loss = 0.20952277\n",
      "Iteration 252, loss = 0.07957333\n",
      "Iteration 338, loss = 0.06930581\n",
      "Iteration 9, loss = 0.56447432\n",
      "Iteration 129, loss = 0.10906450\n",
      "Iteration 253, loss = 0.07943643\n",
      "Iteration 339, loss = 0.06920625\n",
      "Iteration 10, loss = 0.54358606\n",
      "Iteration 180, loss = 0.09691064\n",
      "Iteration 130, loss = 0.10870745\n",
      "Iteration 11, loss = 0.52504376\n",
      "Iteration 254, loss = 0.07927926\n",
      "Iteration 46, loss = 0.20653464\n",
      "Iteration 264, loss = 0.07608132\n",
      "Iteration 131, loss = 0.10834936\n",
      "Iteration 265, loss = 0.07593100\n",
      "Iteration 255, loss = 0.07913663\n",
      "Iteration 132, loss = 0.10798155\n",
      "Iteration 256, loss = 0.07898111\n",
      "Iteration 340, loss = 0.06910951\n",
      "Iteration 133, loss = 0.10764105\n",
      "Iteration 257, loss = 0.07883824\n",
      "Iteration 341, loss = 0.06901484\n",
      "Iteration 342, loss = 0.06891961\n",
      "Iteration 13, loss = 0.44893469\n",
      "Iteration 343, loss = 0.06882458\n",
      "Iteration 344, loss = 0.06873191\n",
      "Iteration 266, loss = 0.07577761\n",
      "Iteration 345, loss = 0.06864115\n",
      "Iteration 181, loss = 0.09668052\n",
      "Iteration 346, loss = 0.06853842\n",
      "Iteration 258, loss = 0.07869646\n",
      "Iteration 267, loss = 0.07562518\n",
      "Iteration 47, loss = 0.20373103\n",
      "Iteration 347, loss = 0.06844782\n",
      "Iteration 182, loss = 0.09643006\n",
      "Iteration 12, loss = 0.50768977\n",
      "Iteration 48, loss = 0.20096385\n",
      "Iteration 268, loss = 0.07546269\n",
      "Iteration 348, loss = 0.06835808\n",
      "Iteration 183, loss = 0.09619395\n",
      "Iteration 349, loss = 0.06826631\n",
      "Iteration 269, loss = 0.07529873\n",
      "Iteration 259, loss = 0.07854332\n",
      "Iteration 350, loss = 0.06816691\n",
      "Iteration 13, loss = 0.49278619\n",
      "Iteration 260, loss = 0.07840303\n",
      "Iteration 184, loss = 0.09596119\n",
      "Iteration 14, loss = 0.47866497\n",
      "Iteration 270, loss = 0.07514733\n",
      "Iteration 351, loss = 0.06807951\n",
      "Iteration 352, loss = 0.06798137\n",
      "Iteration 49, loss = 0.19828228\n",
      "Iteration 353, loss = 0.06789130\n",
      "Iteration 134, loss = 0.10730113\n",
      "Iteration 14, loss = 0.43682195\n",
      "Iteration 354, loss = 0.06780328\n",
      "Iteration 355, loss = 0.06770820\n",
      "Iteration 261, loss = 0.07826786\n",
      "Iteration 356, loss = 0.06762657\n",
      "Iteration 185, loss = 0.09573922\n",
      "Iteration 357, loss = 0.06753221\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 271, loss = 0.07500432\n",
      "Iteration 262, loss = 0.07811714\n",
      "Iteration 272, loss = 0.07483320\n",
      "Iteration 15, loss = 0.46628257\n",
      "Iteration 273, loss = 0.07468016\n",
      "Iteration 50, loss = 0.19576419\n",
      "Iteration 16, loss = 0.45479862\n",
      "Iteration 186, loss = 0.09549622\n",
      "Iteration 263, loss = 0.07798247\n",
      "Iteration 274, loss = 0.07452527\n",
      "Iteration 187, loss = 0.09527188\n",
      "Iteration 51, loss = 0.19333545\n",
      "Iteration 275, loss = 0.07438381\n",
      "Iteration 17, loss = 0.44425627\n",
      "Iteration 264, loss = 0.07784427\n",
      "Iteration 276, loss = 0.07423827\n",
      "Iteration 18, loss = 0.43491003\n",
      "Iteration 265, loss = 0.07770403\n",
      "Iteration 19, loss = 0.42606476\n",
      "Iteration 52, loss = 0.19090782\n",
      "Iteration 188, loss = 0.09504507\n",
      "Iteration 277, loss = 0.07408683\n",
      "Iteration 266, loss = 0.07757829\n",
      "Iteration 189, loss = 0.09481488\n",
      "Iteration 15, loss = 0.42570120\n",
      "Iteration 267, loss = 0.07744059\n",
      "Iteration 278, loss = 0.07395405\n",
      "Iteration 53, loss = 0.18868235\n",
      "Iteration 268, loss = 0.07729432\n",
      "Iteration 269, loss = 0.07716575\n",
      "Iteration 279, loss = 0.07378720\n",
      "Iteration 190, loss = 0.09461762\n",
      "Iteration 54, loss = 0.18651661\n",
      "Iteration 280, loss = 0.07365401\n",
      "Iteration 281, loss = 0.07349958\n",
      "Iteration 20, loss = 0.41830966\n",
      "Iteration 282, loss = 0.07335410\n",
      "Iteration 55, loss = 0.18434177\n",
      "Iteration 1, loss = 0.79337282\n",
      "Iteration 283, loss = 0.07320848\n",
      "Iteration 284, loss = 0.07306913\n",
      "Iteration 285, loss = 0.07292849\n",
      "Iteration 135, loss = 0.10695603\n",
      "Iteration 270, loss = 0.07702696\n",
      "Iteration 21, loss = 0.41097448\n",
      "Iteration 16, loss = 0.41514689\n",
      "Iteration 56, loss = 0.18232372\n",
      "Iteration 136, loss = 0.10662217\n",
      "Iteration 271, loss = 0.07689211\n",
      "Iteration 286, loss = 0.07278688\n",
      "Iteration 22, loss = 0.40444423\n",
      "Iteration 137, loss = 0.10631506\n",
      "Iteration 57, loss = 0.18038958\n",
      "Iteration 191, loss = 0.09439503\n",
      "Iteration 2, loss = 0.75405366\n",
      "Iteration 272, loss = 0.07676848\n",
      "Iteration 192, loss = 0.09416454\n",
      "Iteration 273, loss = 0.07663282\n",
      "Iteration 193, loss = 0.09394369\n",
      "Iteration 23, loss = 0.39794272\n",
      "Iteration 274, loss = 0.07651175\n",
      "Iteration 24, loss = 0.39217381\n",
      "Iteration 194, loss = 0.09373403\n",
      "Iteration 275, loss = 0.07637766\n",
      "Iteration 138, loss = 0.10597926\n",
      "Iteration 25, loss = 0.38666140\n",
      "Iteration 3, loss = 0.70344529\n",
      "Iteration 195, loss = 0.09351862\n",
      "Iteration 276, loss = 0.07624248\n",
      "Iteration 139, loss = 0.10565583\n",
      "Iteration 196, loss = 0.09332004\n",
      "Iteration 287, loss = 0.07265021\n",
      "Iteration 277, loss = 0.07611414\n",
      "Iteration 140, loss = 0.10533693\n",
      "Iteration 26, loss = 0.38142888\n",
      "Iteration 288, loss = 0.07251241\n",
      "Iteration 278, loss = 0.07599469\n",
      "Iteration 289, loss = 0.07237989\n",
      "Iteration 27, loss = 0.37650374\n",
      "Iteration 279, loss = 0.07585719\n",
      "Iteration 290, loss = 0.07225675\n",
      "Iteration 280, loss = 0.07573621\n",
      "Iteration 197, loss = 0.09310580\n",
      "Iteration 17, loss = 0.40538964\n",
      "Iteration 291, loss = 0.07211560\n",
      "Iteration 58, loss = 0.17851290\n",
      "Iteration 28, loss = 0.37164452\n",
      "Iteration 4, loss = 0.65416697\n",
      "Iteration 292, loss = 0.07198256\n",
      "Iteration 198, loss = 0.09290129\n",
      "Iteration 141, loss = 0.10502763\n",
      "Iteration 29, loss = 0.36709693\n",
      "Iteration 59, loss = 0.17675660\n",
      "Iteration 293, loss = 0.07184920\n",
      "Iteration 199, loss = 0.09269205\n",
      "Iteration 30, loss = 0.36268916\n",
      "Iteration 294, loss = 0.07171761\n",
      "Iteration 60, loss = 0.17498707\n",
      "Iteration 5, loss = 0.61018258\n",
      "Iteration 31, loss = 0.35831673\n",
      "Iteration 142, loss = 0.10472600\n",
      "Iteration 295, loss = 0.07159048\n",
      "Iteration 32, loss = 0.35421517\n",
      "Iteration 33, loss = 0.35003343\n",
      "Iteration 143, loss = 0.10441427\n",
      "Iteration 281, loss = 0.07560852\n",
      "Iteration 34, loss = 0.34606640\n",
      "Iteration 282, loss = 0.07549222\n",
      "Iteration 35, loss = 0.34214167\n",
      "Iteration 283, loss = 0.07536410\n",
      "Iteration 200, loss = 0.09248133\n",
      "Iteration 18, loss = 0.39633937\n",
      "Iteration 284, loss = 0.07524145\n",
      "Iteration 296, loss = 0.07145272\n",
      "Iteration 201, loss = 0.09228224\n",
      "Iteration 297, loss = 0.07132751\n",
      "Iteration 61, loss = 0.17327943\n",
      "Iteration 298, loss = 0.07119667\n",
      "Iteration 144, loss = 0.10411228\n",
      "Iteration 6, loss = 0.57371811\n",
      "Iteration 285, loss = 0.07511870\n",
      "Iteration 202, loss = 0.09208707\n",
      "Iteration 286, loss = 0.07500720\n",
      "Iteration 299, loss = 0.07106611\n",
      "Iteration 19, loss = 0.38739172\n",
      "Iteration 36, loss = 0.33829241\n",
      "Iteration 287, loss = 0.07488937\n",
      "Iteration 288, loss = 0.07475028\n",
      "Iteration 300, loss = 0.07093999\n",
      "Iteration 37, loss = 0.33438112\n",
      "Iteration 301, loss = 0.07081765\n",
      "Iteration 302, loss = 0.07068890\n",
      "Iteration 62, loss = 0.17166782\n",
      "Iteration 38, loss = 0.33060504\n",
      "Iteration 145, loss = 0.10381970\n",
      "Iteration 203, loss = 0.09188452\n",
      "Iteration 289, loss = 0.07463193\n",
      "Iteration 146, loss = 0.10351796\n",
      "Iteration 303, loss = 0.07056823\n",
      "Iteration 290, loss = 0.07452276\n",
      "Iteration 39, loss = 0.32681690\n",
      "Iteration 63, loss = 0.17010552\n",
      "Iteration 304, loss = 0.07042244\n",
      "Iteration 40, loss = 0.32307450\n",
      "Iteration 41, loss = 0.31943525\n",
      "Iteration 291, loss = 0.07439927\n",
      "Iteration 204, loss = 0.09168196\n",
      "Iteration 292, loss = 0.07427885\n",
      "Iteration 64, loss = 0.16862225\n",
      "Iteration 293, loss = 0.07416191\n",
      "Iteration 305, loss = 0.07030988\n",
      "Iteration 42, loss = 0.31567055\n",
      "Iteration 43, loss = 0.31211264\n",
      "Iteration 20, loss = 0.37917608\n",
      "Iteration 294, loss = 0.07405144\n",
      "Iteration 295, loss = 0.07392982\n",
      "Iteration 296, loss = 0.07381455\n",
      "Iteration 44, loss = 0.30828072\n",
      "Iteration 297, loss = 0.07371397\n",
      "Iteration 147, loss = 0.10322050\n",
      "Iteration 205, loss = 0.09148698\n",
      "Iteration 7, loss = 0.54317770\n",
      "Iteration 65, loss = 0.16717281\n",
      "Iteration 206, loss = 0.09129945\n",
      "Iteration 148, loss = 0.10293216\n",
      "Iteration 207, loss = 0.09108956\n",
      "Iteration 208, loss = 0.09090437\n",
      "Iteration 306, loss = 0.07018139\n",
      "Iteration 298, loss = 0.07359075\n",
      "Iteration 45, loss = 0.30471164\n",
      "Iteration 66, loss = 0.16577968\n",
      "Iteration 149, loss = 0.10266182\n",
      "Iteration 8, loss = 0.51773547\n",
      "Iteration 307, loss = 0.07007181\n",
      "Iteration 21, loss = 0.37100466\n",
      "Iteration 308, loss = 0.06993442\n",
      "Iteration 309, loss = 0.06980854\n",
      "Iteration 9, loss = 0.49702714\n",
      "Iteration 209, loss = 0.09071259\n",
      "Iteration 150, loss = 0.10237030\n",
      "Iteration 46, loss = 0.30108964\n",
      "Iteration 47, loss = 0.29748336\n",
      "Iteration 210, loss = 0.09052859\n",
      "Iteration 67, loss = 0.16446884\n",
      "Iteration 299, loss = 0.07348661\n",
      "Iteration 151, loss = 0.10208513\n",
      "Iteration 310, loss = 0.06968811\n",
      "Iteration 211, loss = 0.09033116\n",
      "Iteration 300, loss = 0.07337251\n",
      "Iteration 10, loss = 0.48011764\n",
      "Iteration 311, loss = 0.06959055\n",
      "Iteration 301, loss = 0.07325401\n",
      "Iteration 48, loss = 0.29400376\n",
      "Iteration 22, loss = 0.36339609\n",
      "Iteration 312, loss = 0.06945640\n",
      "Iteration 302, loss = 0.07314092\n",
      "Iteration 68, loss = 0.16313707\n",
      "Iteration 212, loss = 0.09015827\n",
      "Iteration 303, loss = 0.07304031\n",
      "Iteration 313, loss = 0.06932850\n",
      "Iteration 304, loss = 0.07291765\n",
      "Iteration 49, loss = 0.29022639\n",
      "Iteration 152, loss = 0.10181403\n",
      "Iteration 50, loss = 0.28666997\n",
      "Iteration 314, loss = 0.06921277\n",
      "Iteration 153, loss = 0.10153706\n",
      "Iteration 213, loss = 0.08995666\n",
      "Iteration 11, loss = 0.46496389\n",
      "Iteration 305, loss = 0.07280598\n",
      "Iteration 69, loss = 0.16190414\n",
      "Iteration 315, loss = 0.06909046\n",
      "Iteration 23, loss = 0.35625396\n",
      "Iteration 306, loss = 0.07270769\n",
      "Iteration 307, loss = 0.07259021\n",
      "Iteration 154, loss = 0.10127487\n",
      "Iteration 316, loss = 0.06897849\n",
      "Iteration 51, loss = 0.28317516\n",
      "Iteration 308, loss = 0.07247576\n",
      "Iteration 214, loss = 0.08977917\n",
      "Iteration 317, loss = 0.06886657\n",
      "Iteration 309, loss = 0.07237553\n",
      "Iteration 52, loss = 0.27968562\n",
      "Iteration 310, loss = 0.07226538\n",
      "Iteration 155, loss = 0.10100261\n",
      "Iteration 318, loss = 0.06873922\n",
      "Iteration 12, loss = 0.45248185\n",
      "Iteration 53, loss = 0.27600038\n",
      "Iteration 70, loss = 0.16071376\n",
      "Iteration 215, loss = 0.08958653\n",
      "Iteration 54, loss = 0.27247570\n",
      "Iteration 71, loss = 0.15953437\n",
      "Iteration 24, loss = 0.34921738\n",
      "Iteration 156, loss = 0.10073238\n",
      "Iteration 319, loss = 0.06864670\n",
      "Iteration 311, loss = 0.07214411\n",
      "Iteration 216, loss = 0.08940607\n",
      "Iteration 13, loss = 0.44117535\n",
      "Iteration 157, loss = 0.10046597\n",
      "Iteration 55, loss = 0.26901212\n",
      "Iteration 56, loss = 0.26565022\n",
      "Iteration 217, loss = 0.08924122\n",
      "Iteration 72, loss = 0.15841956\n",
      "Iteration 312, loss = 0.07204288\n",
      "Iteration 320, loss = 0.06850796\n",
      "Iteration 218, loss = 0.08904758\n",
      "Iteration 313, loss = 0.07194517\n",
      "Iteration 158, loss = 0.10021022\n",
      "Iteration 57, loss = 0.26213213\n",
      "Iteration 219, loss = 0.08887195\n",
      "Iteration 220, loss = 0.08869460\n",
      "Iteration 321, loss = 0.06838958\n",
      "Iteration 73, loss = 0.15732041\n",
      "Iteration 322, loss = 0.06828518\n",
      "Iteration 14, loss = 0.43159734\n",
      "Iteration 314, loss = 0.07183047\n",
      "Iteration 323, loss = 0.06817955\n",
      "Iteration 58, loss = 0.25876499\n",
      "Iteration 221, loss = 0.08852548\n",
      "Iteration 159, loss = 0.09995934\n",
      "Iteration 324, loss = 0.06808616\n",
      "Iteration 59, loss = 0.25544620\n",
      "Iteration 315, loss = 0.07172477\n",
      "Iteration 325, loss = 0.06794943\n",
      "Iteration 316, loss = 0.07161229\n",
      "Iteration 160, loss = 0.09970212\n",
      "Iteration 25, loss = 0.34240707\n",
      "Iteration 15, loss = 0.42284102\n",
      "Iteration 60, loss = 0.25213504\n",
      "Iteration 317, loss = 0.07150833\n",
      "Iteration 326, loss = 0.06783833\n",
      "Iteration 74, loss = 0.15632544\n",
      "Iteration 161, loss = 0.09944217\n",
      "Iteration 318, loss = 0.07140255\n",
      "Iteration 61, loss = 0.24872442\n",
      "Iteration 319, loss = 0.07130198\n",
      "Iteration 222, loss = 0.08837728\n",
      "Iteration 320, loss = 0.07118884\n",
      "Iteration 321, loss = 0.07107957\n",
      "Iteration 327, loss = 0.06772256\n",
      "Iteration 26, loss = 0.33583006\n",
      "Iteration 223, loss = 0.08817770\n",
      "Iteration 322, loss = 0.07097813\n",
      "Iteration 328, loss = 0.06762243\n",
      "Iteration 323, loss = 0.07087718\n",
      "Iteration 224, loss = 0.08801504\n",
      "Iteration 329, loss = 0.06750558\n",
      "Iteration 75, loss = 0.15525761\n",
      "Iteration 62, loss = 0.24552628\n",
      "Iteration 330, loss = 0.06739603\n",
      "Iteration 225, loss = 0.08784886\n",
      "Iteration 63, loss = 0.24213201\n",
      "Iteration 226, loss = 0.08767087\n",
      "Iteration 27, loss = 0.32963440\n",
      "Iteration 64, loss = 0.23908357\n",
      "Iteration 65, loss = 0.23575998\n",
      "Iteration 227, loss = 0.08750613\n",
      "Iteration 66, loss = 0.23265776\n",
      "Iteration 228, loss = 0.08734393\n",
      "Iteration 67, loss = 0.22954953\n",
      "Iteration 162, loss = 0.09920014\n",
      "Iteration 16, loss = 0.41484462\n",
      "Iteration 331, loss = 0.06729121\n",
      "Iteration 332, loss = 0.06720055\n",
      "Iteration 324, loss = 0.07077884\n",
      "Iteration 163, loss = 0.09895998\n",
      "Iteration 325, loss = 0.07066570\n",
      "Iteration 333, loss = 0.06708119\n",
      "Iteration 76, loss = 0.15429051\n",
      "Iteration 17, loss = 0.40708437\n",
      "Iteration 164, loss = 0.09871183\n",
      "Iteration 326, loss = 0.07058099\n",
      "Iteration 327, loss = 0.07046101\n",
      "Iteration 334, loss = 0.06698292\n",
      "Iteration 335, loss = 0.06687483\n",
      "Iteration 229, loss = 0.08718814\n",
      "Iteration 165, loss = 0.09846431\n",
      "Iteration 336, loss = 0.06677630\n",
      "Iteration 18, loss = 0.40006147\n",
      "Iteration 337, loss = 0.06666649\n",
      "Iteration 166, loss = 0.09823422\n",
      "Iteration 328, loss = 0.07035664\n",
      "Iteration 167, loss = 0.09800651\n",
      "Iteration 338, loss = 0.06656869\n",
      "Iteration 68, loss = 0.22644201\n",
      "Iteration 230, loss = 0.08702418\n",
      "Iteration 329, loss = 0.07026585\n",
      "Iteration 28, loss = 0.32347712\n",
      "Iteration 69, loss = 0.22339914\n",
      "Iteration 231, loss = 0.08685908\n",
      "Iteration 77, loss = 0.15331552\n",
      "Iteration 339, loss = 0.06646595\n",
      "Iteration 232, loss = 0.08670601\n",
      "Iteration 19, loss = 0.39340287\n",
      "Iteration 70, loss = 0.22036696\n",
      "Iteration 233, loss = 0.08654708\n",
      "Iteration 78, loss = 0.15239727\n",
      "Iteration 234, loss = 0.08638335\n",
      "Iteration 71, loss = 0.21740073\n",
      "Iteration 340, loss = 0.06636756\n",
      "Iteration 330, loss = 0.07015631\n",
      "Iteration 235, loss = 0.08622766\n",
      "Iteration 72, loss = 0.21445078\n",
      "Iteration 341, loss = 0.06627607\n",
      "Iteration 236, loss = 0.08607164\n",
      "Iteration 20, loss = 0.38672888\n",
      "Iteration 342, loss = 0.06617539\n",
      "Iteration 343, loss = 0.06606677\n",
      "Iteration 237, loss = 0.08591409\n",
      "Iteration 331, loss = 0.07005203\n",
      "Iteration 344, loss = 0.06596326\n",
      "Iteration 29, loss = 0.31776274\n",
      "Iteration 332, loss = 0.06995191\n",
      "Iteration 238, loss = 0.08575039\n",
      "Iteration 333, loss = 0.06984852\n",
      "Iteration 79, loss = 0.15148518\n",
      "Iteration 168, loss = 0.09778154\n",
      "Iteration 73, loss = 0.21166199\n",
      "Iteration 74, loss = 0.20864054\n",
      "Iteration 239, loss = 0.08559753\n",
      "Iteration 80, loss = 0.15060626\n",
      "Iteration 345, loss = 0.06587001\n",
      "Iteration 240, loss = 0.08543686\n",
      "Iteration 30, loss = 0.31223625\n",
      "Iteration 81, loss = 0.14974996\n",
      "Iteration 346, loss = 0.06578104\n",
      "Iteration 334, loss = 0.06976004\n",
      "Iteration 347, loss = 0.06567308\n",
      "Iteration 169, loss = 0.09754651\n",
      "Iteration 335, loss = 0.06967115\n",
      "Iteration 75, loss = 0.20579571\n",
      "Iteration 170, loss = 0.09730351\n",
      "Iteration 171, loss = 0.09708066\n",
      "Iteration 76, loss = 0.20308727\n",
      "Iteration 348, loss = 0.06557411\n",
      "Iteration 82, loss = 0.14891258\n",
      "Iteration 77, loss = 0.20025876\n",
      "Iteration 349, loss = 0.06548815\n",
      "Iteration 21, loss = 0.38053742\n",
      "Iteration 336, loss = 0.06956476\n",
      "Iteration 31, loss = 0.30677490\n",
      "Iteration 22, loss = 0.37434942\n",
      "Iteration 78, loss = 0.19753392\n",
      "Iteration 350, loss = 0.06538806\n",
      "Iteration 32, loss = 0.30162395\n",
      "Iteration 351, loss = 0.06527992\n",
      "Iteration 241, loss = 0.08528709\n",
      "Iteration 172, loss = 0.09686342\n",
      "Iteration 79, loss = 0.19491917\n",
      "Iteration 337, loss = 0.06948480\n",
      "Iteration 338, loss = 0.06937105\n",
      "Iteration 242, loss = 0.08513910\n",
      "Iteration 339, loss = 0.06926981\n",
      "Iteration 243, loss = 0.08499288\n",
      "Iteration 340, loss = 0.06919365\n",
      "Iteration 173, loss = 0.09662973\n",
      "Iteration 341, loss = 0.06908974\n",
      "Iteration 352, loss = 0.06518587\n",
      "Iteration 83, loss = 0.14811010\n",
      "Iteration 80, loss = 0.19230308\n",
      "Iteration 353, loss = 0.06509271\n",
      "Iteration 174, loss = 0.09642182\n",
      "Iteration 81, loss = 0.18977261\n",
      "Iteration 244, loss = 0.08483938\n",
      "Iteration 82, loss = 0.18738030\n",
      "Iteration 23, loss = 0.36825819\n",
      "Iteration 83, loss = 0.18492513\n",
      "Iteration 84, loss = 0.14730090\n",
      "Iteration 175, loss = 0.09620248\n",
      "Iteration 354, loss = 0.06499823\n",
      "Iteration 342, loss = 0.06899789\n",
      "Iteration 245, loss = 0.08468201\n",
      "Iteration 355, loss = 0.06491273\n",
      "Iteration 246, loss = 0.08454606\n",
      "Iteration 356, loss = 0.06482713\n",
      "Iteration 343, loss = 0.06889837\n",
      "Iteration 344, loss = 0.06880619\n",
      "Iteration 85, loss = 0.14655198\n",
      "Iteration 247, loss = 0.08439572\n",
      "Iteration 357, loss = 0.06475068\n",
      "Iteration 176, loss = 0.09598437\n",
      "Iteration 24, loss = 0.36234856\n",
      "Iteration 358, loss = 0.06463235\n",
      "Iteration 84, loss = 0.18250203\n",
      "Iteration 33, loss = 0.29663984\n",
      "Iteration 248, loss = 0.08424684\n",
      "Iteration 85, loss = 0.18026449\n",
      "Iteration 86, loss = 0.14579693\n",
      "Iteration 249, loss = 0.08409787\n",
      "Iteration 86, loss = 0.17800763\n",
      "Iteration 359, loss = 0.06453751\n",
      "Iteration 250, loss = 0.08395115\n",
      "Iteration 87, loss = 0.17570528\n",
      "Iteration 87, loss = 0.14507887\n",
      "Iteration 345, loss = 0.06871399\n",
      "Iteration 177, loss = 0.09577147\n",
      "Iteration 346, loss = 0.06862597\n",
      "Iteration 347, loss = 0.06853070\n",
      "Iteration 360, loss = 0.06443788\n",
      "Iteration 25, loss = 0.35670212\n",
      "Iteration 348, loss = 0.06844558\n",
      "Iteration 361, loss = 0.06437405\n",
      "Iteration 349, loss = 0.06835151\n",
      "Iteration 178, loss = 0.09555972\n",
      "Iteration 251, loss = 0.08381285\n",
      "Iteration 350, loss = 0.06825661\n",
      "Iteration 88, loss = 0.17360404Iteration 362, loss = 0.06425006\n",
      "\n",
      "Iteration 363, loss = 0.06416366\n",
      "Iteration 34, loss = 0.29170813\n",
      "Iteration 89, loss = 0.17148639\n",
      "Iteration 351, loss = 0.06816731\n",
      "Iteration 88, loss = 0.14438538\n",
      "Iteration 90, loss = 0.16952626\n",
      "Iteration 352, loss = 0.06807682\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 252, loss = 0.08366169\n",
      "Iteration 179, loss = 0.09534852\n",
      "Iteration 26, loss = 0.35087559\n",
      "Iteration 180, loss = 0.09514269\n",
      "Iteration 364, loss = 0.06407219\n",
      "Iteration 253, loss = 0.08351812\n",
      "Iteration 365, loss = 0.06398793\n",
      "Iteration 89, loss = 0.14368137\n",
      "Iteration 366, loss = 0.06389222\n",
      "Iteration 254, loss = 0.08336600\n",
      "Iteration 90, loss = 0.14295814\n",
      "Iteration 255, loss = 0.08323461\n",
      "Iteration 91, loss = 0.16753746\n",
      "Iteration 367, loss = 0.06380994\n",
      "Iteration 181, loss = 0.09495532\n",
      "Iteration 368, loss = 0.06372200\n",
      "Iteration 91, loss = 0.14231475\n",
      "Iteration 369, loss = 0.06366136\n",
      "Iteration 370, loss = 0.06355101\n",
      "Iteration 27, loss = 0.34531129\n",
      "Iteration 92, loss = 0.16566015\n",
      "Iteration 182, loss = 0.09473694\n",
      "Iteration 371, loss = 0.06345504\n",
      "Iteration 93, loss = 0.16375773\n",
      "Iteration 35, loss = 0.28720390\n",
      "Iteration 183, loss = 0.09454011\n",
      "Iteration 256, loss = 0.08308987\n",
      "Iteration 94, loss = 0.16197878\n",
      "Iteration 95, loss = 0.16021888\n",
      "Iteration 257, loss = 0.08294810\n",
      "Iteration 92, loss = 0.14166917\n",
      "Iteration 96, loss = 0.15851097\n",
      "Iteration 28, loss = 0.33976389\n",
      "Iteration 184, loss = 0.09433143\n",
      "Iteration 97, loss = 0.15687592\n",
      "Iteration 258, loss = 0.08280787\n",
      "Iteration 1, loss = 0.87744424\n",
      "Iteration 372, loss = 0.06337867\n",
      "Iteration 373, loss = 0.06330187\n",
      "Iteration 259, loss = 0.08267107\n",
      "Iteration 98, loss = 0.15522934\n",
      "Iteration 374, loss = 0.06320071\n",
      "Iteration 29, loss = 0.33438801\n",
      "Iteration 375, loss = 0.06310629\n",
      "Iteration 36, loss = 0.28260646\n",
      "Iteration 93, loss = 0.14103821\n",
      "Iteration 185, loss = 0.09415115\n",
      "Iteration 99, loss = 0.15365761\n",
      "Iteration 376, loss = 0.06302664\n",
      "Iteration 186, loss = 0.09393507\n",
      "Iteration 377, loss = 0.06293571\n",
      "Iteration 378, loss = 0.06285469\n",
      "Iteration 100, loss = 0.15218941\n",
      "Iteration 260, loss = 0.08253282\n",
      "Iteration 101, loss = 0.15073130\n",
      "Iteration 261, loss = 0.08238681\n",
      "Iteration 94, loss = 0.14043498\n",
      "Iteration 102, loss = 0.14926478\n",
      "Iteration 187, loss = 0.09374201\n",
      "Iteration 2, loss = 0.83617590\n",
      "Iteration 103, loss = 0.14784517\n",
      "Iteration 262, loss = 0.08225979\n",
      "Iteration 95, loss = 0.13981785\n",
      "Iteration 379, loss = 0.06276654\n",
      "Iteration 104, loss = 0.14643816\n",
      "Iteration 30, loss = 0.32905495\n",
      "Iteration 37, loss = 0.27841880\n",
      "Iteration 380, loss = 0.06268411\n",
      "Iteration 105, loss = 0.14508671\n",
      "Iteration 96, loss = 0.13925750\n",
      "Iteration 381, loss = 0.06260378\n",
      "Iteration 263, loss = 0.08211446\n",
      "Iteration 188, loss = 0.09355514\n",
      "Iteration 38, loss = 0.27416585\n",
      "Iteration 264, loss = 0.08197432\n",
      "Iteration 382, loss = 0.06254133\n",
      "Iteration 189, loss = 0.09336974\n",
      "Iteration 31, loss = 0.32367202\n",
      "Iteration 383, loss = 0.06243361\n",
      "Iteration 190, loss = 0.09317202\n",
      "Iteration 384, loss = 0.06234896\n",
      "Iteration 265, loss = 0.08184307\n",
      "Iteration 385, loss = 0.06226839\n",
      "Iteration 191, loss = 0.09299669\n",
      "Iteration 97, loss = 0.13864841\n",
      "Iteration 3, loss = 0.78323023\n",
      "Iteration 39, loss = 0.27015745\n",
      "Iteration 386, loss = 0.06222296\n",
      "Iteration 32, loss = 0.31840445\n",
      "Iteration 266, loss = 0.08170952\n",
      "Iteration 106, loss = 0.14378263\n",
      "Iteration 192, loss = 0.09281182\n",
      "Iteration 267, loss = 0.08158069\n",
      "Iteration 387, loss = 0.06209892\n",
      "Iteration 98, loss = 0.13808376\n",
      "Iteration 268, loss = 0.08143479\n",
      "Iteration 269, loss = 0.08131330\n",
      "Iteration 107, loss = 0.14248919\n",
      "Iteration 270, loss = 0.08118273\n",
      "Iteration 33, loss = 0.31327202\n",
      "Iteration 108, loss = 0.14126013\n",
      "Iteration 388, loss = 0.06201444\n",
      "Iteration 4, loss = 0.73332021\n",
      "Iteration 109, loss = 0.14001444\n",
      "Iteration 389, loss = 0.06194894\n",
      "Iteration 110, loss = 0.13886490\n",
      "Iteration 390, loss = 0.06186839\n",
      "Iteration 111, loss = 0.13768151\n",
      "Iteration 99, loss = 0.13751640\n",
      "Iteration 112, loss = 0.13653788\n",
      "Iteration 40, loss = 0.26635720\n",
      "Iteration 391, loss = 0.06177671\n",
      "Iteration 113, loss = 0.13543510\n",
      "Iteration 114, loss = 0.13436554\n",
      "Iteration 193, loss = 0.09261582\n",
      "Iteration 115, loss = 0.13330013\n",
      "Iteration 271, loss = 0.08104062\n",
      "Iteration 392, loss = 0.06170276\n",
      "Iteration 194, loss = 0.09243981\n",
      "Iteration 34, loss = 0.30824806\n",
      "Iteration 272, loss = 0.08092851\n",
      "Iteration 100, loss = 0.13698844\n",
      "Iteration 273, loss = 0.08079377\n",
      "Iteration 393, loss = 0.06161602\n",
      "Iteration 5, loss = 0.68887414\n",
      "Iteration 41, loss = 0.26259157\n",
      "Iteration 35, loss = 0.30324231\n",
      "Iteration 116, loss = 0.13229527\n",
      "Iteration 195, loss = 0.09225486\n",
      "Iteration 394, loss = 0.06153532\n",
      "Iteration 196, loss = 0.09207201\n",
      "Iteration 395, loss = 0.06146427\n",
      "Iteration 101, loss = 0.13644727\n",
      "Iteration 274, loss = 0.08065474\n",
      "Iteration 42, loss = 0.25899456\n",
      "Iteration 102, loss = 0.13593068\n",
      "Iteration 275, loss = 0.08052909\n",
      "Iteration 276, loss = 0.08040158\n",
      "Iteration 117, loss = 0.13124779\n",
      "Iteration 277, loss = 0.08027414\n",
      "Iteration 118, loss = 0.13023873\n",
      "Iteration 396, loss = 0.06139448\n",
      "Iteration 397, loss = 0.06131784\n",
      "Iteration 197, loss = 0.09189869\n",
      "Iteration 6, loss = 0.65225119\n",
      "Iteration 36, loss = 0.29834607\n",
      "Iteration 119, loss = 0.12931481\n",
      "Iteration 278, loss = 0.08014244\n",
      "Iteration 43, loss = 0.25559545\n",
      "Iteration 279, loss = 0.08002494\n",
      "Iteration 120, loss = 0.12835556\n",
      "Iteration 37, loss = 0.29357060\n",
      "Iteration 121, loss = 0.12742207\n",
      "Iteration 280, loss = 0.07989438\n",
      "Iteration 198, loss = 0.09172466\n",
      "Iteration 122, loss = 0.12658083\n",
      "Iteration 281, loss = 0.07977533\n",
      "Iteration 44, loss = 0.25221735\n",
      "Iteration 103, loss = 0.13541557\n",
      "Iteration 199, loss = 0.09154510\n",
      "Iteration 104, loss = 0.13489711\n",
      "Iteration 38, loss = 0.28877619\n",
      "Iteration 105, loss = 0.13439329\n",
      "Iteration 7, loss = 0.62370376\n",
      "Iteration 200, loss = 0.09136933\n",
      "Iteration 123, loss = 0.12569998\n",
      "Iteration 124, loss = 0.12479606\n",
      "Iteration 398, loss = 0.06122155\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 106, loss = 0.13390360\n",
      "Iteration 107, loss = 0.13341154\n",
      "Iteration 125, loss = 0.12400072\n",
      "Iteration 201, loss = 0.09119778\n",
      "Iteration 1, loss = 0.63205275\n",
      "Iteration 282, loss = 0.07964284\n",
      "Iteration 108, loss = 0.13295880\n",
      "Iteration 2, loss = 0.61440174\n",
      "Iteration 283, loss = 0.07952827\n",
      "Iteration 284, loss = 0.07940819\n",
      "Iteration 3, loss = 0.59021604\n",
      "Iteration 285, loss = 0.07929219\n",
      "Iteration 202, loss = 0.09103460\n",
      "Iteration 286, loss = 0.07916424\n",
      "Iteration 4, loss = 0.56396307\n",
      "Iteration 287, loss = 0.07906548\n",
      "Iteration 109, loss = 0.13247783\n",
      "Iteration 203, loss = 0.09087334\n",
      "Iteration 5, loss = 0.53815500\n",
      "Iteration 126, loss = 0.12317746\n",
      "Iteration 204, loss = 0.09070047\n",
      "Iteration 127, loss = 0.12239240\n",
      "Iteration 45, loss = 0.24899289\n",
      "Iteration 39, loss = 0.28407671\n",
      "Iteration 6, loss = 0.51411277\n",
      "Iteration 128, loss = 0.12159028\n",
      "Iteration 7, loss = 0.49222548\n",
      "Iteration 8, loss = 0.59710250\n",
      "Iteration 8, loss = 0.47374038\n",
      "Iteration 288, loss = 0.07893964\n",
      "Iteration 129, loss = 0.12085894\n",
      "Iteration 40, loss = 0.27968543\n",
      "Iteration 205, loss = 0.09053191\n",
      "Iteration 289, loss = 0.07880571\n",
      "Iteration 46, loss = 0.24588544\n",
      "Iteration 130, loss = 0.12012103\n",
      "Iteration 110, loss = 0.13201852\n",
      "Iteration 206, loss = 0.09036498\n",
      "Iteration 131, loss = 0.11936302\n",
      "Iteration 132, loss = 0.11867634\n",
      "Iteration 290, loss = 0.07869433\n",
      "Iteration 9, loss = 0.45658332\n",
      "Iteration 207, loss = 0.09020315\n",
      "Iteration 111, loss = 0.13157864\n",
      "Iteration 9, loss = 0.57741968\n",
      "Iteration 10, loss = 0.44210742\n",
      "Iteration 41, loss = 0.27507934\n",
      "Iteration 133, loss = 0.11800148\n",
      "Iteration 291, loss = 0.07857141\n",
      "Iteration 11, loss = 0.42989522\n",
      "Iteration 208, loss = 0.09004134\n",
      "Iteration 134, loss = 0.11730395\n",
      "Iteration 12, loss = 0.41897491\n",
      "Iteration 112, loss = 0.13111237\n",
      "Iteration 47, loss = 0.24285723\n",
      "Iteration 209, loss = 0.08988227\n",
      "Iteration 42, loss = 0.27062189\n",
      "Iteration 13, loss = 0.40997673\n",
      "Iteration 292, loss = 0.07846178\n",
      "Iteration 10, loss = 0.56020735\n",
      "Iteration 135, loss = 0.11661230\n",
      "Iteration 113, loss = 0.13069168\n",
      "Iteration 136, loss = 0.11597754\n",
      "Iteration 293, loss = 0.07833447\n",
      "Iteration 137, loss = 0.11534686\n",
      "Iteration 210, loss = 0.08973283\n",
      "Iteration 48, loss = 0.23994972\n",
      "Iteration 138, loss = 0.11468755\n",
      "Iteration 114, loss = 0.13025105\n",
      "Iteration 139, loss = 0.11408093\n",
      "Iteration 140, loss = 0.11347252\n",
      "Iteration 211, loss = 0.08955694\n",
      "Iteration 14, loss = 0.40196252\n",
      "Iteration 141, loss = 0.11288360\n",
      "Iteration 43, loss = 0.26640660\n",
      "Iteration 49, loss = 0.23720138\n",
      "Iteration 15, loss = 0.39502028\n",
      "Iteration 294, loss = 0.07822604\n",
      "Iteration 142, loss = 0.11230416\n",
      "Iteration 143, loss = 0.11171650\n",
      "Iteration 16, loss = 0.38913924\n",
      "Iteration 144, loss = 0.11115730\n",
      "Iteration 17, loss = 0.38342384\n",
      "Iteration 145, loss = 0.11060600\n",
      "Iteration 50, loss = 0.23447299\n",
      "Iteration 18, loss = 0.37850723\n",
      "Iteration 295, loss = 0.07812943\n",
      "Iteration 11, loss = 0.54529191\n",
      "Iteration 146, loss = 0.11007971\n",
      "Iteration 147, loss = 0.10954541\n",
      "Iteration 51, loss = 0.23184534\n",
      "Iteration 44, loss = 0.26224037\n",
      "Iteration 12, loss = 0.53266664\n",
      "Iteration 212, loss = 0.08940411\n",
      "Iteration 296, loss = 0.07799439\n",
      "Iteration 115, loss = 0.12981431\n",
      "Iteration 297, loss = 0.07789276\n",
      "Iteration 19, loss = 0.37381684\n",
      "Iteration 213, loss = 0.08925728\n",
      "Iteration 116, loss = 0.12939445\n",
      "Iteration 148, loss = 0.10904252\n",
      "Iteration 214, loss = 0.08909786\n",
      "Iteration 298, loss = 0.07777666\n",
      "Iteration 45, loss = 0.25799883\n",
      "Iteration 20, loss = 0.36953961\n",
      "Iteration 52, loss = 0.22922989\n",
      "Iteration 299, loss = 0.07766756\n",
      "Iteration 21, loss = 0.36535689\n",
      "Iteration 117, loss = 0.12897774\n",
      "Iteration 46, loss = 0.25394557\n",
      "Iteration 215, loss = 0.08893938\n",
      "Iteration 300, loss = 0.07755628\n",
      "Iteration 216, loss = 0.08879269\n",
      "Iteration 22, loss = 0.36129308\n",
      "Iteration 118, loss = 0.12858092\n",
      "Iteration 301, loss = 0.07744043\n",
      "Iteration 47, loss = 0.25011991\n",
      "Iteration 13, loss = 0.52146458\n",
      "Iteration 302, loss = 0.07733575\n",
      "Iteration 303, loss = 0.07723098\n",
      "Iteration 23, loss = 0.35744280\n",
      "Iteration 217, loss = 0.08863595\n",
      "Iteration 149, loss = 0.10849577\n",
      "Iteration 304, loss = 0.07712230\n",
      "Iteration 24, loss = 0.35364094\n",
      "Iteration 150, loss = 0.10799828\n",
      "Iteration 218, loss = 0.08849111\n",
      "Iteration 25, loss = 0.34983907\n",
      "Iteration 119, loss = 0.12817155\n",
      "Iteration 305, loss = 0.07701814\n",
      "Iteration 26, loss = 0.34616115\n",
      "Iteration 306, loss = 0.07689906\n",
      "Iteration 120, loss = 0.12778021\n",
      "Iteration 151, loss = 0.10751638\n",
      "Iteration 121, loss = 0.12741569\n",
      "Iteration 48, loss = 0.24632751\n",
      "Iteration 152, loss = 0.10701420\n",
      "Iteration 219, loss = 0.08833467\n",
      "Iteration 307, loss = 0.07679373\n",
      "Iteration 27, loss = 0.34249479\n",
      "Iteration 153, loss = 0.10659733\n",
      "Iteration 220, loss = 0.08819074\n",
      "Iteration 122, loss = 0.12699420\n",
      "Iteration 28, loss = 0.33876722\n",
      "Iteration 49, loss = 0.24260671\n",
      "Iteration 154, loss = 0.10605634\n",
      "Iteration 53, loss = 0.22682276\n",
      "Iteration 308, loss = 0.07668811\n",
      "Iteration 155, loss = 0.10560493\n",
      "Iteration 156, loss = 0.10512625\n",
      "Iteration 29, loss = 0.33514684\n",
      "Iteration 54, loss = 0.22445226\n",
      "Iteration 221, loss = 0.08803976\n",
      "Iteration 30, loss = 0.33153607\n",
      "Iteration 14, loss = 0.51145422\n",
      "Iteration 50, loss = 0.23902052Iteration 309, loss = 0.07658531\n",
      "\n",
      "Iteration 31, loss = 0.32787778\n",
      "Iteration 123, loss = 0.12662901\n",
      "Iteration 222, loss = 0.08788954\n",
      "Iteration 157, loss = 0.10468376\n",
      "Iteration 310, loss = 0.07648474\n",
      "Iteration 55, loss = 0.22216077\n",
      "Iteration 51, loss = 0.23562177\n",
      "Iteration 223, loss = 0.08775760\n",
      "Iteration 124, loss = 0.12625138\n",
      "Iteration 52, loss = 0.23226792\n",
      "Iteration 56, loss = 0.21993849\n",
      "Iteration 32, loss = 0.32418377\n",
      "Iteration 158, loss = 0.10431801\n",
      "Iteration 159, loss = 0.10379950\n",
      "Iteration 53, loss = 0.22902810\n",
      "Iteration 33, loss = 0.32059273\n",
      "Iteration 311, loss = 0.07638052\n",
      "Iteration 57, loss = 0.21780486\n",
      "Iteration 224, loss = 0.08759787\n",
      "Iteration 312, loss = 0.07627268\n",
      "Iteration 160, loss = 0.10338567\n",
      "Iteration 161, loss = 0.10297833\n",
      "Iteration 15, loss = 0.50192838\n",
      "Iteration 225, loss = 0.08746220\n",
      "Iteration 58, loss = 0.21575824\n",
      "Iteration 162, loss = 0.10255344\n",
      "Iteration 34, loss = 0.31697334\n",
      "Iteration 163, loss = 0.10212956\n",
      "Iteration 313, loss = 0.07617799\n",
      "Iteration 125, loss = 0.12588522\n",
      "Iteration 59, loss = 0.21374757\n",
      "Iteration 126, loss = 0.12552922\n",
      "Iteration 35, loss = 0.31334330\n",
      "Iteration 36, loss = 0.30968266\n",
      "Iteration 60, loss = 0.21183122\n",
      "Iteration 314, loss = 0.07607028\n",
      "Iteration 164, loss = 0.10175176\n",
      "Iteration 127, loss = 0.12515018\n",
      "Iteration 226, loss = 0.08732015\n",
      "Iteration 37, loss = 0.30610609\n",
      "Iteration 315, loss = 0.07597232\n",
      "Iteration 61, loss = 0.20990860\n",
      "Iteration 165, loss = 0.10132654\n",
      "Iteration 316, loss = 0.07586739\n",
      "Iteration 38, loss = 0.30246233\n",
      "Iteration 16, loss = 0.49339377\n",
      "Iteration 227, loss = 0.08716807\n",
      "Iteration 39, loss = 0.29892822\n",
      "Iteration 54, loss = 0.22610178\n",
      "Iteration 128, loss = 0.12480542\n",
      "Iteration 166, loss = 0.10094088\n",
      "Iteration 167, loss = 0.10056458\n",
      "Iteration 228, loss = 0.08704443\n",
      "Iteration 129, loss = 0.12445407\n",
      "Iteration 55, loss = 0.22294630\n",
      "Iteration 317, loss = 0.07576804\n",
      "Iteration 40, loss = 0.29536943\n",
      "Iteration 229, loss = 0.08688730\n",
      "Iteration 168, loss = 0.10020169\n",
      "Iteration 56, loss = 0.22009387\n",
      "Iteration 62, loss = 0.20813600\n",
      "Iteration 318, loss = 0.07565770\n",
      "Iteration 57, loss = 0.21740005\n",
      "Iteration 230, loss = 0.08675126\n",
      "Iteration 319, loss = 0.07557308\n",
      "Iteration 41, loss = 0.29184083\n",
      "Iteration 130, loss = 0.12409789\n",
      "Iteration 17, loss = 0.48530904\n",
      "Iteration 320, loss = 0.07546767\n",
      "Iteration 42, loss = 0.28842023\n",
      "Iteration 321, loss = 0.07536996\n",
      "Iteration 169, loss = 0.09980027\n",
      "Iteration 43, loss = 0.28487714\n",
      "Iteration 131, loss = 0.12373570\n",
      "Iteration 231, loss = 0.08662512\n",
      "Iteration 170, loss = 0.09945172\n",
      "Iteration 171, loss = 0.09908767\n",
      "Iteration 322, loss = 0.07526988\n",
      "Iteration 172, loss = 0.09874203\n",
      "Iteration 173, loss = 0.09839620\n",
      "Iteration 58, loss = 0.21468810\n",
      "Iteration 44, loss = 0.28154317\n",
      "Iteration 323, loss = 0.07516590\n",
      "Iteration 232, loss = 0.08647938\n",
      "Iteration 132, loss = 0.12340099\n",
      "Iteration 324, loss = 0.07507633\n",
      "Iteration 133, loss = 0.12306563\n",
      "Iteration 325, loss = 0.07498032\n",
      "Iteration 45, loss = 0.27820869\n",
      "Iteration 233, loss = 0.08634962\n",
      "Iteration 174, loss = 0.09804488\n",
      "Iteration 18, loss = 0.47764503\n",
      "Iteration 175, loss = 0.09770637\n",
      "Iteration 46, loss = 0.27504022\n",
      "Iteration 326, loss = 0.07488004\n",
      "Iteration 234, loss = 0.08620742\n",
      "Iteration 134, loss = 0.12272358\n",
      "Iteration 235, loss = 0.08608566\n",
      "Iteration 176, loss = 0.09738672\n",
      "Iteration 47, loss = 0.27181427\n",
      "Iteration 236, loss = 0.08594790\n",
      "Iteration 59, loss = 0.21219552\n",
      "Iteration 327, loss = 0.07478324\n",
      "Iteration 48, loss = 0.26872806\n",
      "Iteration 135, loss = 0.12240352\n",
      "Iteration 237, loss = 0.08581487\n",
      "Iteration 63, loss = 0.20641831\n",
      "Iteration 136, loss = 0.12207769\n",
      "Iteration 238, loss = 0.08567962\n",
      "Iteration 19, loss = 0.47028054\n",
      "Iteration 177, loss = 0.09708573\n",
      "Iteration 49, loss = 0.26569840\n",
      "Iteration 328, loss = 0.07468753\n",
      "Iteration 50, loss = 0.26272462\n",
      "Iteration 178, loss = 0.09673396\n",
      "Iteration 329, loss = 0.07460214\n",
      "Iteration 330, loss = 0.07450481\n",
      "Iteration 137, loss = 0.12177090\n",
      "Iteration 239, loss = 0.08555230\n",
      "Iteration 51, loss = 0.25980810\n",
      "Iteration 179, loss = 0.09642573\n",
      "Iteration 331, loss = 0.07441498\n",
      "Iteration 332, loss = 0.07432023\n",
      "Iteration 20, loss = 0.46310668\n",
      "Iteration 180, loss = 0.09612256\n",
      "Iteration 240, loss = 0.08543632\n",
      "Iteration 60, loss = 0.20972347\n",
      "Iteration 181, loss = 0.09580352\n",
      "Iteration 52, loss = 0.25696777\n",
      "Iteration 64, loss = 0.20467222\n",
      "Iteration 333, loss = 0.07422719\n",
      "Iteration 182, loss = 0.09549943\n",
      "Iteration 138, loss = 0.12144298\n",
      "Iteration 241, loss = 0.08529879\n",
      "Iteration 61, loss = 0.20737868\n",
      "Iteration 183, loss = 0.09523806\n",
      "Iteration 53, loss = 0.25405268\n",
      "Iteration 184, loss = 0.09494951\n",
      "Iteration 62, loss = 0.20506391\n",
      "Iteration 185, loss = 0.09463685\n",
      "Iteration 242, loss = 0.08517417\n",
      "Iteration 334, loss = 0.07412461\n",
      "Iteration 54, loss = 0.25132968\n",
      "Iteration 335, loss = 0.07403784\n",
      "Iteration 336, loss = 0.07395203\n",
      "Iteration 55, loss = 0.24861432\n",
      "Iteration 65, loss = 0.20299860\n",
      "Iteration 56, loss = 0.24595406\n",
      "Iteration 139, loss = 0.12112768\n",
      "Iteration 57, loss = 0.24336986\n",
      "Iteration 140, loss = 0.12081117\n",
      "Iteration 141, loss = 0.12050655\n",
      "Iteration 58, loss = 0.24080955\n",
      "Iteration 243, loss = 0.08504687\n",
      "Iteration 337, loss = 0.07385835\n",
      "Iteration 21, loss = 0.45613225\n",
      "Iteration 186, loss = 0.09433844\n",
      "Iteration 142, loss = 0.12020273\n",
      "Iteration 66, loss = 0.20143657\n",
      "Iteration 244, loss = 0.08492908\n",
      "Iteration 63, loss = 0.20296844\n",
      "Iteration 338, loss = 0.07376533\n",
      "Iteration 187, loss = 0.09410266\n",
      "Iteration 339, loss = 0.07367734\n",
      "Iteration 64, loss = 0.20094211\n",
      "Iteration 188, loss = 0.09377449\n",
      "Iteration 340, loss = 0.07358727\n",
      "Iteration 189, loss = 0.09350196\n",
      "Iteration 341, loss = 0.07349802\n",
      "Iteration 245, loss = 0.08480103\n",
      "Iteration 190, loss = 0.09323225\n",
      "Iteration 342, loss = 0.07341159\n",
      "Iteration 59, loss = 0.23818932\n",
      "Iteration 191, loss = 0.09294839\n",
      "Iteration 60, loss = 0.23570048\n",
      "Iteration 143, loss = 0.11989966\n",
      "Iteration 246, loss = 0.08467746\n",
      "Iteration 61, loss = 0.23314734\n",
      "Iteration 22, loss = 0.44938235\n",
      "Iteration 144, loss = 0.11960231\n",
      "Iteration 247, loss = 0.08455687\n",
      "Iteration 192, loss = 0.09271859\n",
      "Iteration 193, loss = 0.09243897\n",
      "Iteration 248, loss = 0.08443677\n",
      "Iteration 194, loss = 0.09219594\n",
      "Iteration 249, loss = 0.08431434\n",
      "Iteration 67, loss = 0.19992652\n",
      "Iteration 343, loss = 0.07332223\n",
      "Iteration 65, loss = 0.19883907\n",
      "Iteration 195, loss = 0.09191934\n",
      "Iteration 66, loss = 0.19698136\n",
      "Iteration 196, loss = 0.09166593\n",
      "Iteration 62, loss = 0.23072924\n",
      "Iteration 145, loss = 0.11930384\n",
      "Iteration 344, loss = 0.07324189\n",
      "Iteration 197, loss = 0.09142789\n",
      "Iteration 198, loss = 0.09118718\n",
      "Iteration 199, loss = 0.09094182\n",
      "Iteration 345, loss = 0.07314613\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 63, loss = 0.22830730\n",
      "Iteration 250, loss = 0.08419371\n",
      "Iteration 23, loss = 0.44289202\n",
      "Iteration 68, loss = 0.19838596\n",
      "Iteration 146, loss = 0.11901923\n",
      "Iteration 64, loss = 0.22585055\n",
      "Iteration 147, loss = 0.11871062\n",
      "Iteration 200, loss = 0.09068359\n",
      "Iteration 67, loss = 0.19505334\n",
      "Iteration 201, loss = 0.09045928\n",
      "Iteration 251, loss = 0.08407742\n",
      "Iteration 202, loss = 0.09023591\n",
      "Iteration 65, loss = 0.22343931\n",
      "Iteration 69, loss = 0.19696390\n",
      "Iteration 66, loss = 0.22112134\n",
      "Iteration 68, loss = 0.19332006\n",
      "Iteration 203, loss = 0.09000387\n",
      "Iteration 252, loss = 0.08395905\n",
      "Iteration 1, loss = 0.82998719\n",
      "Iteration 253, loss = 0.08384527\n",
      "Iteration 148, loss = 0.11846697\n",
      "Iteration 254, loss = 0.08373655\n",
      "Iteration 204, loss = 0.08978317\n",
      "Iteration 67, loss = 0.21878873\n",
      "Iteration 70, loss = 0.19551692\n",
      "Iteration 24, loss = 0.43638190\n",
      "Iteration 205, loss = 0.08963177\n",
      "Iteration 149, loss = 0.11814749\n",
      "Iteration 68, loss = 0.21661130\n",
      "Iteration 69, loss = 0.19157537\n",
      "Iteration 206, loss = 0.08930371\n",
      "Iteration 150, loss = 0.11786368\n",
      "Iteration 69, loss = 0.21434698\n",
      "Iteration 151, loss = 0.11759735\n",
      "Iteration 2, loss = 0.79093710\n",
      "Iteration 255, loss = 0.08360197\n",
      "Iteration 152, loss = 0.11731883\n",
      "Iteration 207, loss = 0.08910209\n",
      "Iteration 70, loss = 0.18992841\n",
      "Iteration 256, loss = 0.08350311\n",
      "Iteration 70, loss = 0.21222347\n",
      "Iteration 71, loss = 0.19416904\n",
      "Iteration 208, loss = 0.08888137\n",
      "Iteration 25, loss = 0.43006212\n",
      "Iteration 153, loss = 0.11704047\n",
      "Iteration 209, loss = 0.08866629\n",
      "Iteration 3, loss = 0.74006309\n",
      "Iteration 257, loss = 0.08338331\n",
      "Iteration 71, loss = 0.21011674\n",
      "Iteration 71, loss = 0.18835435\n",
      "Iteration 210, loss = 0.08843134\n",
      "Iteration 72, loss = 0.19281836\n",
      "Iteration 72, loss = 0.20808977\n",
      "Iteration 154, loss = 0.11677732\n",
      "Iteration 4, loss = 0.68932664\n",
      "Iteration 211, loss = 0.08825169\n",
      "Iteration 72, loss = 0.18675741\n",
      "Iteration 73, loss = 0.20599745\n",
      "Iteration 258, loss = 0.08326442\n",
      "Iteration 212, loss = 0.08802598\n",
      "Iteration 74, loss = 0.20406237\n",
      "Iteration 73, loss = 0.18531712\n",
      "Iteration 155, loss = 0.11651994\n",
      "Iteration 75, loss = 0.20205745\n",
      "Iteration 213, loss = 0.08781126\n",
      "Iteration 214, loss = 0.08760015\n",
      "Iteration 259, loss = 0.08315438\n",
      "Iteration 156, loss = 0.11625559\n",
      "Iteration 76, loss = 0.20009178\n",
      "Iteration 73, loss = 0.19156093\n",
      "Iteration 157, loss = 0.11598915\n",
      "Iteration 5, loss = 0.64291959\n",
      "Iteration 158, loss = 0.11573035\n",
      "Iteration 260, loss = 0.08303686\n",
      "Iteration 74, loss = 0.18399111\n",
      "Iteration 77, loss = 0.19827013\n",
      "Iteration 215, loss = 0.08741815\n",
      "Iteration 159, loss = 0.11546624\n",
      "Iteration 26, loss = 0.42379562\n",
      "Iteration 216, loss = 0.08720868\n",
      "Iteration 160, loss = 0.11521649\n",
      "Iteration 78, loss = 0.19640356\n",
      "Iteration 6, loss = 0.60311200\n",
      "Iteration 79, loss = 0.19454595\n",
      "Iteration 217, loss = 0.08701025\n",
      "Iteration 74, loss = 0.19028568\n",
      "Iteration 218, loss = 0.08681135\n",
      "Iteration 80, loss = 0.19267680\n",
      "Iteration 261, loss = 0.08292901\n",
      "Iteration 75, loss = 0.18252831\n",
      "Iteration 27, loss = 0.41773084\n",
      "Iteration 262, loss = 0.08281421\n",
      "Iteration 219, loss = 0.08663937\n",
      "Iteration 220, loss = 0.08641217\n",
      "Iteration 161, loss = 0.11496671\n",
      "Iteration 263, loss = 0.08270793\n",
      "Iteration 7, loss = 0.57048130\n",
      "Iteration 221, loss = 0.08623062\n",
      "Iteration 76, loss = 0.18118808\n",
      "Iteration 162, loss = 0.11472192\n",
      "Iteration 222, loss = 0.08604114\n",
      "Iteration 81, loss = 0.19092852\n",
      "Iteration 75, loss = 0.18909293\n",
      "Iteration 77, loss = 0.17994293\n",
      "Iteration 82, loss = 0.18907012\n",
      "Iteration 83, loss = 0.18740306\n",
      "Iteration 76, loss = 0.18787371\n",
      "Iteration 28, loss = 0.41172141\n",
      "Iteration 163, loss = 0.11446985\n",
      "Iteration 78, loss = 0.17869402\n",
      "Iteration 264, loss = 0.08260067\n",
      "Iteration 164, loss = 0.11420819\n",
      "Iteration 8, loss = 0.54268998\n",
      "Iteration 223, loss = 0.08586957\n",
      "Iteration 224, loss = 0.08564205\n",
      "Iteration 84, loss = 0.18558041\n",
      "Iteration 165, loss = 0.11397940\n",
      "Iteration 265, loss = 0.08249146\n",
      "Iteration 225, loss = 0.08550022\n",
      "Iteration 226, loss = 0.08528695\n",
      "Iteration 85, loss = 0.18392598\n",
      "Iteration 227, loss = 0.08509152\n",
      "Iteration 266, loss = 0.08237524\n",
      "Iteration 79, loss = 0.17753022\n",
      "Iteration 9, loss = 0.52015415\n",
      "Iteration 228, loss = 0.08494418\n",
      "Iteration 267, loss = 0.08228293\n",
      "Iteration 86, loss = 0.18227074\n",
      "Iteration 77, loss = 0.18680746\n",
      "Iteration 229, loss = 0.08474552\n",
      "Iteration 268, loss = 0.08215834\n",
      "Iteration 166, loss = 0.11373631\n",
      "Iteration 29, loss = 0.40576212\n",
      "Iteration 87, loss = 0.18060198\n",
      "Iteration 78, loss = 0.18561998\n",
      "Iteration 230, loss = 0.08456615\n",
      "Iteration 269, loss = 0.08205300\n",
      "Iteration 231, loss = 0.08440177\n",
      "Iteration 80, loss = 0.17641199\n",
      "Iteration 88, loss = 0.17889856\n",
      "Iteration 270, loss = 0.08195315\n",
      "Iteration 89, loss = 0.17731269\n",
      "Iteration 10, loss = 0.50158649\n",
      "Iteration 90, loss = 0.17568961\n",
      "Iteration 232, loss = 0.08422239\n",
      "Iteration 167, loss = 0.11348586\n",
      "Iteration 271, loss = 0.08185223\n",
      "Iteration 81, loss = 0.17526361\n",
      "Iteration 79, loss = 0.18444544\n",
      "Iteration 233, loss = 0.08403415\n",
      "Iteration 272, loss = 0.08173809\n",
      "Iteration 234, loss = 0.08389033\n",
      "Iteration 168, loss = 0.11325614\n",
      "Iteration 91, loss = 0.17412565\n",
      "Iteration 273, loss = 0.08163861\n",
      "Iteration 11, loss = 0.48600180\n",
      "Iteration 235, loss = 0.08370346\n",
      "Iteration 82, loss = 0.17422401\n",
      "Iteration 30, loss = 0.39986682\n",
      "Iteration 80, loss = 0.18346042\n",
      "Iteration 92, loss = 0.17249468\n",
      "Iteration 169, loss = 0.11303225\n",
      "Iteration 83, loss = 0.17319625\n",
      "Iteration 236, loss = 0.08354848\n",
      "Iteration 12, loss = 0.47304318\n",
      "Iteration 237, loss = 0.08335552\n",
      "Iteration 274, loss = 0.08153586\n",
      "Iteration 170, loss = 0.11277717\n",
      "Iteration 84, loss = 0.17221591\n",
      "Iteration 93, loss = 0.17100194\n",
      "Iteration 275, loss = 0.08143450\n",
      "Iteration 31, loss = 0.39394028\n",
      "Iteration 94, loss = 0.16944468\n",
      "Iteration 238, loss = 0.08322594\n",
      "Iteration 95, loss = 0.16791254\n",
      "Iteration 171, loss = 0.11254952\n",
      "Iteration 276, loss = 0.08132780\n",
      "Iteration 81, loss = 0.18236428\n",
      "Iteration 239, loss = 0.08304639\n",
      "Iteration 277, loss = 0.08124376\n",
      "Iteration 172, loss = 0.11231017\n",
      "Iteration 240, loss = 0.08287511\n",
      "Iteration 13, loss = 0.46198738\n",
      "Iteration 173, loss = 0.11211933\n",
      "Iteration 85, loss = 0.17121638\n",
      "Iteration 241, loss = 0.08273517\n",
      "Iteration 96, loss = 0.16637785\n",
      "Iteration 82, loss = 0.18139555\n",
      "Iteration 278, loss = 0.08113405\n",
      "Iteration 97, loss = 0.16485193\n",
      "Iteration 174, loss = 0.11188096\n",
      "Iteration 86, loss = 0.17036977\n",
      "Iteration 98, loss = 0.16339687\n",
      "Iteration 99, loss = 0.16193871\n",
      "Iteration 83, loss = 0.18036493\n",
      "Iteration 279, loss = 0.08103094\n",
      "Iteration 175, loss = 0.11165109\n",
      "Iteration 100, loss = 0.16047783\n",
      "Iteration 101, loss = 0.15901551\n",
      "Iteration 242, loss = 0.08256178\n",
      "Iteration 280, loss = 0.08093114\n",
      "Iteration 14, loss = 0.45289399\n",
      "Iteration 32, loss = 0.38836386\n",
      "Iteration 87, loss = 0.16941935\n",
      "Iteration 102, loss = 0.15769354\n",
      "Iteration 281, loss = 0.08083181\n",
      "Iteration 243, loss = 0.08239093\n",
      "Iteration 244, loss = 0.08226119\n",
      "Iteration 84, loss = 0.17940803\n",
      "Iteration 245, loss = 0.08210428\n",
      "Iteration 88, loss = 0.16853343\n",
      "Iteration 176, loss = 0.11142247\n",
      "Iteration 282, loss = 0.08073617\n",
      "Iteration 246, loss = 0.08195230\n",
      "Iteration 15, loss = 0.44431904\n",
      "Iteration 247, loss = 0.08177598\n",
      "Iteration 248, loss = 0.08162935\n",
      "Iteration 33, loss = 0.38249517\n",
      "Iteration 283, loss = 0.08063840\n",
      "Iteration 103, loss = 0.15627049\n",
      "Iteration 89, loss = 0.16767582\n",
      "Iteration 177, loss = 0.11122003\n",
      "Iteration 249, loss = 0.08145489\n",
      "Iteration 250, loss = 0.08131278\n",
      "Iteration 104, loss = 0.15489672\n",
      "Iteration 16, loss = 0.43697400\n",
      "Iteration 178, loss = 0.11097953\n",
      "Iteration 284, loss = 0.08052992\n",
      "Iteration 251, loss = 0.08116188\n",
      "Iteration 252, loss = 0.08100220\n",
      "Iteration 85, loss = 0.17846292\n",
      "Iteration 105, loss = 0.15362190\n",
      "Iteration 253, loss = 0.08087449\n",
      "Iteration 90, loss = 0.16685250\n",
      "Iteration 285, loss = 0.08043914\n",
      "Iteration 179, loss = 0.11076619\n",
      "Iteration 286, loss = 0.08033835\n",
      "Iteration 180, loss = 0.11055364\n",
      "Iteration 86, loss = 0.17755898\n",
      "Iteration 34, loss = 0.37683264\n",
      "Iteration 17, loss = 0.43004537\n",
      "Iteration 287, loss = 0.08024242\n",
      "Iteration 91, loss = 0.16604585\n",
      "Iteration 181, loss = 0.11032986\n",
      "Iteration 288, loss = 0.08014625\n",
      "Iteration 106, loss = 0.15226690\n",
      "Iteration 254, loss = 0.08070315\n",
      "Iteration 107, loss = 0.15099295\n",
      "Iteration 92, loss = 0.16527396\n",
      "Iteration 18, loss = 0.42380053\n",
      "Iteration 255, loss = 0.08056615\n",
      "Iteration 256, loss = 0.08040393\n",
      "Iteration 289, loss = 0.08004857\n",
      "Iteration 108, loss = 0.14971856\n",
      "Iteration 93, loss = 0.16452600\n",
      "Iteration 257, loss = 0.08026413\n",
      "Iteration 109, loss = 0.14851606\n",
      "Iteration 258, loss = 0.08010341\n",
      "Iteration 35, loss = 0.37132660\n",
      "Iteration 182, loss = 0.11010987\n",
      "Iteration 259, loss = 0.08000624\n",
      "Iteration 87, loss = 0.17666059\n",
      "Iteration 260, loss = 0.07981855\n",
      "Iteration 290, loss = 0.07996461\n",
      "Iteration 110, loss = 0.14732231\n",
      "Iteration 261, loss = 0.07967420\n",
      "Iteration 94, loss = 0.16377037\n",
      "Iteration 19, loss = 0.41791013\n",
      "Iteration 111, loss = 0.14607298\n",
      "Iteration 183, loss = 0.10990973\n",
      "Iteration 262, loss = 0.07955028\n",
      "Iteration 291, loss = 0.07987125\n",
      "Iteration 184, loss = 0.10970134\n",
      "Iteration 112, loss = 0.14495245\n",
      "Iteration 113, loss = 0.14380706\n",
      "Iteration 263, loss = 0.07939467\n",
      "Iteration 88, loss = 0.17579184\n",
      "Iteration 264, loss = 0.07927785Iteration 292, loss = 0.07978032\n",
      "Iteration 95, loss = 0.16305988\n",
      "\n",
      "Iteration 114, loss = 0.14265613\n",
      "Iteration 293, loss = 0.07968517\n",
      "Iteration 185, loss = 0.10950386\n",
      "Iteration 36, loss = 0.36573687\n",
      "Iteration 265, loss = 0.07914355\n",
      "Iteration 20, loss = 0.41217097\n",
      "Iteration 115, loss = 0.14156680\n",
      "Iteration 96, loss = 0.16236931\n",
      "Iteration 116, loss = 0.14050932\n",
      "Iteration 21, loss = 0.40655377\n",
      "Iteration 266, loss = 0.07899430\n",
      "Iteration 186, loss = 0.10928418\n",
      "Iteration 267, loss = 0.07886587\n",
      "Iteration 97, loss = 0.16165487\n",
      "Iteration 117, loss = 0.13946291\n",
      "Iteration 294, loss = 0.07959509\n",
      "Iteration 187, loss = 0.10909125\n",
      "Iteration 118, loss = 0.13840846\n",
      "Iteration 89, loss = 0.17493170\n",
      "Iteration 295, loss = 0.07950361\n",
      "Iteration 268, loss = 0.07872615\n",
      "Iteration 296, loss = 0.07940301\n",
      "Iteration 188, loss = 0.10886441\n",
      "Iteration 269, loss = 0.07858743\n",
      "Iteration 98, loss = 0.16096204\n",
      "Iteration 119, loss = 0.13742105\n",
      "Iteration 297, loss = 0.07931435\n",
      "Iteration 90, loss = 0.17409884\n",
      "Iteration 37, loss = 0.36025800\n",
      "Iteration 22, loss = 0.40114207\n",
      "Iteration 120, loss = 0.13638519\n",
      "Iteration 270, loss = 0.07846795\n",
      "Iteration 121, loss = 0.13542043\n",
      "Iteration 271, loss = 0.07832720\n",
      "Iteration 272, loss = 0.07818995\n",
      "Iteration 298, loss = 0.07922185\n",
      "Iteration 122, loss = 0.13444840\n",
      "Iteration 23, loss = 0.39587343\n",
      "Iteration 189, loss = 0.10866862\n",
      "Iteration 123, loss = 0.13352792\n",
      "Iteration 273, loss = 0.07806601\n",
      "Iteration 299, loss = 0.07914121\n",
      "Iteration 124, loss = 0.13260887\n",
      "Iteration 91, loss = 0.17327221\n",
      "Iteration 300, loss = 0.07904324\n",
      "Iteration 99, loss = 0.16031405\n",
      "Iteration 190, loss = 0.10848181\n",
      "Iteration 274, loss = 0.07794208\n",
      "Iteration 125, loss = 0.13172714\n",
      "Iteration 38, loss = 0.35472810\n",
      "Iteration 275, loss = 0.07782012\n",
      "Iteration 301, loss = 0.07895393\n",
      "Iteration 126, loss = 0.13084379\n",
      "Iteration 100, loss = 0.15965667\n",
      "Iteration 276, loss = 0.07767552\n",
      "Iteration 191, loss = 0.10829388\n",
      "Iteration 277, loss = 0.07756177\n",
      "Iteration 127, loss = 0.12995244\n",
      "Iteration 92, loss = 0.17249640\n",
      "Iteration 128, loss = 0.12913745\n",
      "Iteration 302, loss = 0.07887166\n",
      "Iteration 129, loss = 0.12829323\n",
      "Iteration 130, loss = 0.12752159\n",
      "Iteration 192, loss = 0.10805713\n",
      "Iteration 278, loss = 0.07741969Iteration 39, loss = 0.34940343\n",
      "\n",
      "Iteration 24, loss = 0.39059057\n",
      "Iteration 101, loss = 0.15903333\n",
      "Iteration 303, loss = 0.07877498\n",
      "Iteration 279, loss = 0.07730858\n",
      "Iteration 25, loss = 0.38547125\n",
      "Iteration 193, loss = 0.10788622\n",
      "Iteration 93, loss = 0.17171677\n",
      "Iteration 194, loss = 0.10767465\n",
      "Iteration 280, loss = 0.07716700\n",
      "Iteration 304, loss = 0.07869172\n",
      "Iteration 40, loss = 0.34415350\n",
      "Iteration 131, loss = 0.12670464\n",
      "Iteration 281, loss = 0.07704045\n",
      "Iteration 102, loss = 0.15839795\n",
      "Iteration 132, loss = 0.12593105\n",
      "Iteration 282, loss = 0.07693983\n",
      "Iteration 305, loss = 0.07860400\n",
      "Iteration 195, loss = 0.10751469\n",
      "Iteration 133, loss = 0.12516236\n",
      "Iteration 94, loss = 0.17093761\n",
      "Iteration 283, loss = 0.07679702\n",
      "Iteration 306, loss = 0.07851924\n",
      "Iteration 134, loss = 0.12443368\n",
      "Iteration 284, loss = 0.07666372\n",
      "Iteration 285, loss = 0.07656461\n",
      "Iteration 26, loss = 0.38025854\n",
      "Iteration 196, loss = 0.10728293\n",
      "Iteration 135, loss = 0.12368420\n",
      "Iteration 307, loss = 0.07844250\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 286, loss = 0.07643008\n",
      "Iteration 136, loss = 0.12297835\n",
      "Iteration 103, loss = 0.15776698\n",
      "Iteration 41, loss = 0.33912212\n",
      "Iteration 27, loss = 0.37511689\n",
      "Iteration 287, loss = 0.07631771\n",
      "Iteration 137, loss = 0.12229238\n",
      "Iteration 95, loss = 0.17017055\n",
      "Iteration 288, loss = 0.07620669\n",
      "Iteration 289, loss = 0.07607871\n",
      "Iteration 138, loss = 0.12157347\n",
      "Iteration 290, loss = 0.07595704\n",
      "Iteration 197, loss = 0.10709770\n",
      "Iteration 291, loss = 0.07584623\n",
      "Iteration 104, loss = 0.15718680\n",
      "Iteration 198, loss = 0.10690720\n",
      "Iteration 292, loss = 0.07571923\n",
      "Iteration 293, loss = 0.07561631\n",
      "Iteration 199, loss = 0.10673104\n",
      "Iteration 105, loss = 0.15656358\n",
      "Iteration 294, loss = 0.07548212\n",
      "Iteration 295, loss = 0.07539189\n",
      "Iteration 200, loss = 0.10653660\n",
      "Iteration 139, loss = 0.12089890\n",
      "Iteration 201, loss = 0.10635521\n",
      "Iteration 28, loss = 0.37008688\n",
      "Iteration 29, loss = 0.36509469\n",
      "Iteration 140, loss = 0.12027377\n",
      "Iteration 106, loss = 0.15598803\n",
      "Iteration 42, loss = 0.33388765\n",
      "Iteration 1, loss = 0.93272198\n",
      "Iteration 96, loss = 0.16945291\n",
      "Iteration 141, loss = 0.11958659\n",
      "Iteration 296, loss = 0.07528724\n",
      "Iteration 297, loss = 0.07516219\n",
      "Iteration 298, loss = 0.07504874\n",
      "Iteration 202, loss = 0.10616385\n",
      "Iteration 30, loss = 0.36009121\n",
      "Iteration 142, loss = 0.11895986\n",
      "Iteration 31, loss = 0.35519133\n",
      "Iteration 143, loss = 0.11833104\n",
      "Iteration 299, loss = 0.07492704\n",
      "Iteration 107, loss = 0.15541365\n",
      "Iteration 300, loss = 0.07482572\n",
      "Iteration 144, loss = 0.11769406\n",
      "Iteration 301, loss = 0.07473375\n",
      "Iteration 302, loss = 0.07460145\n",
      "Iteration 97, loss = 0.16873008\n",
      "Iteration 43, loss = 0.32907470\n",
      "Iteration 203, loss = 0.10598697\n",
      "Iteration 108, loss = 0.15485940\n",
      "Iteration 303, loss = 0.07451963\n",
      "Iteration 145, loss = 0.11711928\n",
      "Iteration 2, loss = 0.88848296\n",
      "Iteration 204, loss = 0.10582432\n",
      "Iteration 146, loss = 0.11652381\n",
      "Iteration 98, loss = 0.16805974\n",
      "Iteration 32, loss = 0.35033300\n",
      "Iteration 304, loss = 0.07440695\n",
      "Iteration 147, loss = 0.11593706\n",
      "Iteration 148, loss = 0.11533952\n",
      "Iteration 305, loss = 0.07429383\n",
      "Iteration 205, loss = 0.10564198\n",
      "Iteration 44, loss = 0.32401929\n",
      "Iteration 306, loss = 0.07418891\n",
      "Iteration 206, loss = 0.10543535\n",
      "Iteration 307, loss = 0.07409095\n",
      "Iteration 33, loss = 0.34552474\n",
      "Iteration 109, loss = 0.15427898\n",
      "Iteration 149, loss = 0.11481335\n",
      "Iteration 99, loss = 0.16733461\n",
      "Iteration 150, loss = 0.11421739\n",
      "Iteration 308, loss = 0.07397611\n",
      "Iteration 110, loss = 0.15374658\n",
      "Iteration 207, loss = 0.10525434\n",
      "Iteration 3, loss = 0.83053294\n",
      "Iteration 151, loss = 0.11367191\n",
      "Iteration 208, loss = 0.10509309\n",
      "Iteration 100, loss = 0.16665954\n",
      "Iteration 111, loss = 0.15318731\n",
      "Iteration 309, loss = 0.07386442\n",
      "Iteration 152, loss = 0.11315391\n",
      "Iteration 45, loss = 0.31913792\n",
      "Iteration 310, loss = 0.07378046\n",
      "Iteration 311, loss = 0.07368539\n",
      "Iteration 153, loss = 0.11260885\n",
      "Iteration 34, loss = 0.34089401\n",
      "Iteration 112, loss = 0.15267778\n",
      "Iteration 154, loss = 0.11208280\n",
      "Iteration 209, loss = 0.10491097\n",
      "Iteration 155, loss = 0.11156910\n",
      "Iteration 312, loss = 0.07357202\n",
      "Iteration 101, loss = 0.16602382\n",
      "Iteration 210, loss = 0.10474508\n",
      "Iteration 4, loss = 0.77552579\n",
      "Iteration 156, loss = 0.11104985\n",
      "Iteration 46, loss = 0.31461579\n",
      "Iteration 157, loss = 0.11054383\n",
      "Iteration 313, loss = 0.07346745\n",
      "Iteration 113, loss = 0.15216247\n",
      "Iteration 35, loss = 0.33618638\n",
      "Iteration 211, loss = 0.10457400\n",
      "Iteration 314, loss = 0.07336709\n",
      "Iteration 158, loss = 0.11007343\n",
      "Iteration 315, loss = 0.07329820\n",
      "Iteration 159, loss = 0.10957090\n",
      "Iteration 212, loss = 0.10439836\n",
      "Iteration 102, loss = 0.16534036\n",
      "Iteration 114, loss = 0.15164392\n",
      "Iteration 47, loss = 0.31000215\n",
      "Iteration 316, loss = 0.07316182\n",
      "Iteration 115, loss = 0.15115101\n",
      "Iteration 317, loss = 0.07308703\n",
      "Iteration 103, loss = 0.16472612\n",
      "Iteration 318, loss = 0.07299158\n",
      "Iteration 116, loss = 0.15061938\n",
      "Iteration 160, loss = 0.10908199\n",
      "Iteration 36, loss = 0.33142220\n",
      "Iteration 213, loss = 0.10422064\n",
      "Iteration 161, loss = 0.10862572\n",
      "Iteration 5, loss = 0.72416040\n",
      "Iteration 319, loss = 0.07288115\n",
      "Iteration 162, loss = 0.10818270\n",
      "Iteration 104, loss = 0.16410342\n",
      "Iteration 163, loss = 0.10769983\n",
      "Iteration 320, loss = 0.07279607\n",
      "Iteration 37, loss = 0.32679346\n",
      "Iteration 214, loss = 0.10405095\n",
      "Iteration 321, loss = 0.07272045\n",
      "Iteration 322, loss = 0.07262200\n",
      "Iteration 215, loss = 0.10389015\n",
      "Iteration 164, loss = 0.10728836\n",
      "Iteration 105, loss = 0.16350647\n",
      "Iteration 323, loss = 0.07252374\n",
      "Iteration 48, loss = 0.30562783\n",
      "Iteration 117, loss = 0.15014003\n",
      "Iteration 216, loss = 0.10370864\n",
      "Iteration 106, loss = 0.16288483\n",
      "Iteration 118, loss = 0.14965934\n",
      "Iteration 324, loss = 0.07241886\n",
      "Iteration 325, loss = 0.07234176\n",
      "Iteration 38, loss = 0.32233879\n",
      "Iteration 326, loss = 0.07226376\n",
      "Iteration 6, loss = 0.68274278\n",
      "Iteration 327, loss = 0.07215507\n",
      "Iteration 217, loss = 0.10355035\n",
      "Iteration 328, loss = 0.07207972\n",
      "Iteration 39, loss = 0.31778838\n",
      "Iteration 165, loss = 0.10684046\n",
      "Iteration 218, loss = 0.10338642\n",
      "Iteration 49, loss = 0.30136038\n",
      "Iteration 119, loss = 0.14917852\n",
      "Iteration 166, loss = 0.10640774\n",
      "Iteration 329, loss = 0.07196204\n",
      "Iteration 120, loss = 0.14869369\n",
      "Iteration 167, loss = 0.10598750\n",
      "Iteration 330, loss = 0.07191156\n",
      "Iteration 219, loss = 0.10322117\n",
      "Iteration 168, loss = 0.10556120\n",
      "Iteration 331, loss = 0.07180475\n",
      "Iteration 169, loss = 0.10516203\n",
      "Iteration 40, loss = 0.31341928\n",
      "Iteration 170, loss = 0.10477533\n",
      "Iteration 7, loss = 0.64775250\n",
      "Iteration 107, loss = 0.16227340\n",
      "Iteration 332, loss = 0.07170945\n",
      "Iteration 171, loss = 0.10437304\n",
      "Iteration 172, loss = 0.10398198\n",
      "Iteration 220, loss = 0.10307406\n",
      "Iteration 108, loss = 0.16171144\n",
      "Iteration 221, loss = 0.10290393Iteration 333, loss = 0.07162088\n",
      "Iteration 121, loss = 0.14824988\n",
      "\n",
      "Iteration 41, loss = 0.30880926\n",
      "Iteration 50, loss = 0.29710225\n",
      "Iteration 173, loss = 0.10365508\n",
      "Iteration 174, loss = 0.10321535\n",
      "Iteration 334, loss = 0.07154053\n",
      "Iteration 8, loss = 0.61937476\n",
      "Iteration 335, loss = 0.07147810\n",
      "Iteration 122, loss = 0.14777047\n",
      "Iteration 336, loss = 0.07136765\n",
      "Iteration 222, loss = 0.10273861\n",
      "Iteration 337, loss = 0.07128052\n",
      "Iteration 338, loss = 0.07119075\n",
      "Iteration 223, loss = 0.10258570\n",
      "Iteration 42, loss = 0.30464110\n",
      "Iteration 109, loss = 0.16110549\n",
      "Iteration 175, loss = 0.10284138\n",
      "Iteration 339, loss = 0.07111062\n",
      "Iteration 43, loss = 0.30028940\n",
      "Iteration 176, loss = 0.10244297\n",
      "Iteration 224, loss = 0.10241808\n",
      "Iteration 340, loss = 0.07104696\n",
      "Iteration 110, loss = 0.16055826\n",
      "Iteration 177, loss = 0.10209522\n",
      "Iteration 123, loss = 0.14732626\n",
      "Iteration 341, loss = 0.07093740\n",
      "Iteration 51, loss = 0.29300561\n",
      "Iteration 225, loss = 0.10226438\n",
      "Iteration 178, loss = 0.10174195\n",
      "Iteration 179, loss = 0.10138648\n",
      "Iteration 124, loss = 0.14685270\n",
      "Iteration 180, loss = 0.10104156\n",
      "Iteration 226, loss = 0.10210975\n",
      "Iteration 342, loss = 0.07086827\n",
      "Iteration 9, loss = 0.59601610\n",
      "Iteration 44, loss = 0.29629109\n",
      "Iteration 181, loss = 0.10069798\n",
      "Iteration 111, loss = 0.15998890\n",
      "Iteration 343, loss = 0.07080498\n",
      "Iteration 182, loss = 0.10037782\n",
      "Iteration 344, loss = 0.07070890\n",
      "Iteration 227, loss = 0.10194891\n",
      "Iteration 45, loss = 0.29229745\n",
      "Iteration 345, loss = 0.07062769\n",
      "Iteration 183, loss = 0.10003109\n",
      "Iteration 125, loss = 0.14640128\n",
      "Iteration 52, loss = 0.28917063\n",
      "Iteration 346, loss = 0.07056004\n",
      "Iteration 228, loss = 0.10179446\n",
      "Iteration 184, loss = 0.09971220\n",
      "Iteration 126, loss = 0.14596416\n",
      "Iteration 347, loss = 0.07046675\n",
      "Iteration 112, loss = 0.15945799\n",
      "Iteration 46, loss = 0.28827977\n",
      "Iteration 10, loss = 0.57663403\n",
      "Iteration 229, loss = 0.10166272\n",
      "Iteration 185, loss = 0.09940437\n",
      "Iteration 348, loss = 0.07041499\n",
      "Iteration 230, loss = 0.10147995\n",
      "Iteration 349, loss = 0.07030376\n",
      "Iteration 113, loss = 0.15891085\n",
      "Iteration 186, loss = 0.09905234\n",
      "Iteration 53, loss = 0.28538325\n",
      "Iteration 127, loss = 0.14551574\n",
      "Iteration 350, loss = 0.07023025\n",
      "Iteration 187, loss = 0.09875297\n",
      "Iteration 231, loss = 0.10135102\n",
      "Iteration 188, loss = 0.09843514\n",
      "Iteration 128, loss = 0.14511997\n",
      "Iteration 351, loss = 0.07015216\n",
      "Iteration 114, loss = 0.15836772\n",
      "Iteration 352, loss = 0.07007684\n",
      "Iteration 47, loss = 0.28457674\n",
      "Iteration 353, loss = 0.07000342\n",
      "Iteration 129, loss = 0.14465680\n",
      "Iteration 232, loss = 0.10118077\n",
      "Iteration 354, loss = 0.06992164\n",
      "Iteration 233, loss = 0.10103351\n",
      "Iteration 189, loss = 0.09815675\n",
      "Iteration 355, loss = 0.06986350\n",
      "Iteration 54, loss = 0.28164392\n",
      "Iteration 190, loss = 0.09785255\n",
      "Iteration 234, loss = 0.10091389\n",
      "Iteration 235, loss = 0.10075270\n",
      "Iteration 356, loss = 0.06976832\n",
      "Iteration 130, loss = 0.14423463\n",
      "Iteration 191, loss = 0.09758462\n",
      "Iteration 11, loss = 0.56011790\n",
      "Iteration 357, loss = 0.06971335\n",
      "Iteration 115, loss = 0.15786336\n",
      "Iteration 358, loss = 0.06963271\n",
      "Iteration 236, loss = 0.10058672\n",
      "Iteration 359, loss = 0.06958977\n",
      "Iteration 360, loss = 0.06948755\n",
      "Iteration 237, loss = 0.10046619\n",
      "Iteration 361, loss = 0.06941186\n",
      "Iteration 116, loss = 0.15735622\n",
      "Iteration 362, loss = 0.06932033\n",
      "Iteration 192, loss = 0.09727199Iteration 48, loss = 0.28070127\n",
      "Iteration 238, loss = 0.10028670\n",
      "\n",
      "Iteration 55, loss = 0.27803902\n",
      "Iteration 131, loss = 0.14385886\n",
      "Iteration 49, loss = 0.27711804\n",
      "Iteration 193, loss = 0.09697934\n",
      "Iteration 194, loss = 0.09670860\n",
      "Iteration 12, loss = 0.54614486\n",
      "Iteration 132, loss = 0.14340714\n",
      "Iteration 363, loss = 0.06929322\n",
      "Iteration 195, loss = 0.09641393\n",
      "Iteration 364, loss = 0.06919874\n",
      "Iteration 239, loss = 0.10015617\n",
      "Iteration 117, loss = 0.15682590\n",
      "Iteration 196, loss = 0.09615235\n",
      "Iteration 365, loss = 0.06912464\n",
      "Iteration 50, loss = 0.27360204\n",
      "Iteration 56, loss = 0.27467089\n",
      "Iteration 366, loss = 0.06906791\n",
      "Iteration 240, loss = 0.10000388\n",
      "Iteration 367, loss = 0.06897922\n",
      "Iteration 241, loss = 0.09986947\n",
      "Iteration 133, loss = 0.14300373\n",
      "Iteration 197, loss = 0.09587333\n",
      "Iteration 368, loss = 0.06893707\n",
      "Iteration 369, loss = 0.06884107\n",
      "Iteration 198, loss = 0.09559666\n",
      "Iteration 370, loss = 0.06877595\n",
      "Iteration 134, loss = 0.14259986\n",
      "Iteration 57, loss = 0.27142685\n",
      "Iteration 51, loss = 0.27008703\n",
      "Iteration 242, loss = 0.09971115\n",
      "Iteration 118, loss = 0.15632271\n",
      "Iteration 371, loss = 0.06871712\n",
      "Iteration 13, loss = 0.53355186\n",
      "Iteration 243, loss = 0.09956161\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 199, loss = 0.09532235\n",
      "Iteration 119, loss = 0.15584214\n",
      "Iteration 135, loss = 0.14218216\n",
      "Iteration 200, loss = 0.09510552\n",
      "Iteration 244, loss = 0.09944269\n",
      "Iteration 58, loss = 0.26811369\n",
      "Iteration 1, loss = 0.74159203\n",
      "Iteration 2, loss = 0.72055996\n",
      "Iteration 201, loss = 0.09487450\n",
      "Iteration 3, loss = 0.69255611\n",
      "Iteration 52, loss = 0.26673283\n",
      "Iteration 202, loss = 0.09459734\n",
      "Iteration 203, loss = 0.09434921\n",
      "Iteration 245, loss = 0.09929081\n",
      "Iteration 136, loss = 0.14180985\n",
      "Iteration 4, loss = 0.66301956\n",
      "Iteration 204, loss = 0.09411929\n",
      "Iteration 120, loss = 0.15535606\n",
      "Iteration 5, loss = 0.63547986\n",
      "Iteration 137, loss = 0.14142703\n",
      "Iteration 14, loss = 0.52222925\n",
      "Iteration 53, loss = 0.26345871\n",
      "Iteration 246, loss = 0.09913632\n",
      "Iteration 59, loss = 0.26507206\n",
      "Iteration 205, loss = 0.09384549\n",
      "Iteration 247, loss = 0.09901117\n",
      "Iteration 6, loss = 0.61016642\n",
      "Iteration 206, loss = 0.09362275\n",
      "Iteration 138, loss = 0.14100675\n",
      "Iteration 248, loss = 0.09885067\n",
      "Iteration 207, loss = 0.09339655\n",
      "Iteration 54, loss = 0.26017387\n",
      "Iteration 7, loss = 0.58757014\n",
      "Iteration 121, loss = 0.15487164\n",
      "Iteration 8, loss = 0.56810117\n",
      "Iteration 249, loss = 0.09872221\n",
      "Iteration 208, loss = 0.09316671\n",
      "Iteration 9, loss = 0.54998545\n",
      "Iteration 250, loss = 0.09857553\n",
      "Iteration 122, loss = 0.15439741\n",
      "Iteration 10, loss = 0.53461590\n",
      "Iteration 60, loss = 0.26207031Iteration 209, loss = 0.09294316\n",
      "\n",
      "Iteration 11, loss = 0.52017911\n",
      "Iteration 139, loss = 0.14065582Iteration 61, loss = 0.25922074\n",
      "\n",
      "Iteration 210, loss = 0.09271500\n",
      "Iteration 251, loss = 0.09845097\n",
      "Iteration 15, loss = 0.51169524\n",
      "Iteration 211, loss = 0.09248680\n",
      "Iteration 123, loss = 0.15394112\n",
      "Iteration 55, loss = 0.25707732\n",
      "Iteration 140, loss = 0.14026958\n",
      "Iteration 12, loss = 0.50783941\n",
      "Iteration 56, loss = 0.25405912\n",
      "Iteration 252, loss = 0.09831186\n",
      "Iteration 13, loss = 0.49642830\n",
      "Iteration 14, loss = 0.48636422\n",
      "Iteration 212, loss = 0.09229782\n",
      "Iteration 253, loss = 0.09818499\n",
      "Iteration 141, loss = 0.13989405\n",
      "Iteration 213, loss = 0.09204235\n",
      "Iteration 16, loss = 0.50193688\n",
      "Iteration 254, loss = 0.09802166\n",
      "Iteration 57, loss = 0.25119714\n",
      "Iteration 62, loss = 0.25647452\n",
      "Iteration 214, loss = 0.09189999\n",
      "Iteration 15, loss = 0.47678213\n",
      "Iteration 124, loss = 0.15349648\n",
      "Iteration 215, loss = 0.09161191\n",
      "Iteration 255, loss = 0.09790652\n",
      "Iteration 16, loss = 0.46841859\n",
      "Iteration 216, loss = 0.09142976\n",
      "Iteration 17, loss = 0.46076551\n",
      "Iteration 142, loss = 0.13953163\n",
      "Iteration 18, loss = 0.45367142\n",
      "Iteration 58, loss = 0.24827364\n",
      "Iteration 217, loss = 0.09119290\n",
      "Iteration 19, loss = 0.44715741\n",
      "Iteration 218, loss = 0.09100428\n",
      "Iteration 20, loss = 0.44118539\n",
      "Iteration 21, loss = 0.43585680\n",
      "Iteration 256, loss = 0.09776187\n",
      "Iteration 125, loss = 0.15302619\n",
      "Iteration 143, loss = 0.13915729\n",
      "Iteration 59, loss = 0.24546445\n",
      "Iteration 22, loss = 0.43084163\n",
      "Iteration 17, loss = 0.49256973\n",
      "Iteration 219, loss = 0.09080473\n",
      "Iteration 23, loss = 0.42640132\n",
      "Iteration 257, loss = 0.09763932\n",
      "Iteration 258, loss = 0.09750926\n",
      "Iteration 24, loss = 0.42229444\n",
      "Iteration 126, loss = 0.15260206\n",
      "Iteration 63, loss = 0.25377428\n",
      "Iteration 25, loss = 0.41863637\n",
      "Iteration 220, loss = 0.09058172\n",
      "Iteration 26, loss = 0.41530651\n",
      "Iteration 221, loss = 0.09036342\n",
      "Iteration 27, loss = 0.41214221\n",
      "Iteration 144, loss = 0.13880022\n",
      "Iteration 28, loss = 0.40942280\n",
      "Iteration 222, loss = 0.09019749\n",
      "Iteration 29, loss = 0.40679872\n",
      "Iteration 259, loss = 0.09737736\n",
      "Iteration 60, loss = 0.24272743\n",
      "Iteration 223, loss = 0.08998129\n",
      "Iteration 145, loss = 0.13842890\n",
      "Iteration 30, loss = 0.40433474\n",
      "Iteration 224, loss = 0.08975697\n",
      "Iteration 31, loss = 0.40208041\n",
      "Iteration 32, loss = 0.40011134\n",
      "Iteration 64, loss = 0.25121981\n",
      "Iteration 18, loss = 0.48340686\n",
      "Iteration 225, loss = 0.08958834\n",
      "Iteration 127, loss = 0.15214733\n",
      "Iteration 226, loss = 0.08939875\n",
      "Iteration 260, loss = 0.09723813\n",
      "Iteration 61, loss = 0.24011298\n",
      "Iteration 146, loss = 0.13809897\n",
      "Iteration 261, loss = 0.09709974\n",
      "Iteration 227, loss = 0.08920128\n",
      "Iteration 128, loss = 0.15170695\n",
      "Iteration 147, loss = 0.13773176\n",
      "Iteration 228, loss = 0.08900954\n",
      "Iteration 33, loss = 0.39806008\n",
      "Iteration 65, loss = 0.24880371\n",
      "Iteration 148, loss = 0.13738047\n",
      "Iteration 262, loss = 0.09698180\n",
      "Iteration 62, loss = 0.23767509\n",
      "Iteration 229, loss = 0.08881862\n",
      "Iteration 19, loss = 0.47488988\n",
      "Iteration 129, loss = 0.15128953\n",
      "Iteration 149, loss = 0.13701671\n",
      "Iteration 230, loss = 0.08866542\n",
      "Iteration 231, loss = 0.08849120\n",
      "Iteration 263, loss = 0.09685101\n",
      "Iteration 150, loss = 0.13667272\n",
      "Iteration 66, loss = 0.24638038\n",
      "Iteration 130, loss = 0.15087410\n",
      "Iteration 232, loss = 0.08829078\n",
      "Iteration 264, loss = 0.09672875\n",
      "Iteration 63, loss = 0.23513600\n",
      "Iteration 20, loss = 0.46648807\n",
      "Iteration 34, loss = 0.39625518\n",
      "Iteration 64, loss = 0.23274073\n",
      "Iteration 151, loss = 0.13634471\n",
      "Iteration 233, loss = 0.08808686\n",
      "Iteration 265, loss = 0.09658733\n",
      "Iteration 152, loss = 0.13599684\n",
      "Iteration 35, loss = 0.39451128\n",
      "Iteration 234, loss = 0.08793039\n",
      "Iteration 235, loss = 0.08776949\n",
      "Iteration 36, loss = 0.39284650\n",
      "Iteration 131, loss = 0.15047920\n",
      "Iteration 266, loss = 0.09647645\n",
      "Iteration 236, loss = 0.08758859\n",
      "Iteration 65, loss = 0.23044227\n",
      "Iteration 67, loss = 0.24427607\n",
      "Iteration 37, loss = 0.39128268\n",
      "Iteration 21, loss = 0.45827028\n",
      "Iteration 237, loss = 0.08740602\n",
      "Iteration 238, loss = 0.08724671\n",
      "Iteration 153, loss = 0.13566118\n",
      "Iteration 132, loss = 0.15005930\n",
      "Iteration 38, loss = 0.38971254\n",
      "Iteration 267, loss = 0.09634019\n",
      "Iteration 239, loss = 0.08708121\n",
      "Iteration 268, loss = 0.09621142\n",
      "Iteration 240, loss = 0.08693320\n",
      "Iteration 39, loss = 0.38821275\n",
      "Iteration 154, loss = 0.13535915\n",
      "Iteration 241, loss = 0.08674717\n",
      "Iteration 66, loss = 0.22819553\n",
      "Iteration 40, loss = 0.38675435\n",
      "Iteration 242, loss = 0.08657753\n",
      "Iteration 243, loss = 0.08642520\n",
      "Iteration 68, loss = 0.24201760\n",
      "Iteration 269, loss = 0.09609284\n",
      "Iteration 41, loss = 0.38526098\n",
      "Iteration 42, loss = 0.38382071\n",
      "Iteration 244, loss = 0.08625186\n",
      "Iteration 270, loss = 0.09596054\n",
      "Iteration 43, loss = 0.38238734\n",
      "Iteration 245, loss = 0.08612803\n",
      "Iteration 67, loss = 0.22601651\n",
      "Iteration 133, loss = 0.14965218\n",
      "Iteration 155, loss = 0.13503800\n",
      "Iteration 22, loss = 0.45024899\n",
      "Iteration 271, loss = 0.09585096\n",
      "Iteration 44, loss = 0.38093337\n",
      "Iteration 69, loss = 0.23990782\n",
      "Iteration 134, loss = 0.14926620\n",
      "Iteration 246, loss = 0.08594241\n",
      "Iteration 68, loss = 0.22401298\n",
      "Iteration 45, loss = 0.37950658\n",
      "Iteration 156, loss = 0.13467150\n",
      "Iteration 247, loss = 0.08578722\n",
      "Iteration 70, loss = 0.23788592\n",
      "Iteration 135, loss = 0.14888429\n",
      "Iteration 46, loss = 0.37808843\n",
      "Iteration 272, loss = 0.09571187\n",
      "Iteration 69, loss = 0.22193622\n",
      "Iteration 248, loss = 0.08564140\n",
      "Iteration 157, loss = 0.13434764\n",
      "Iteration 23, loss = 0.44233256\n",
      "Iteration 136, loss = 0.14849572\n",
      "Iteration 47, loss = 0.37659274\n",
      "Iteration 70, loss = 0.22001883\n",
      "Iteration 273, loss = 0.09559422\n",
      "Iteration 249, loss = 0.08550008\n",
      "Iteration 250, loss = 0.08535718\n",
      "Iteration 158, loss = 0.13405968\n",
      "Iteration 71, loss = 0.23593154\n",
      "Iteration 251, loss = 0.08518908\n",
      "Iteration 137, loss = 0.14812634\n",
      "Iteration 252, loss = 0.08507621\n",
      "Iteration 274, loss = 0.09547643Iteration 48, loss = 0.37515013\n",
      "\n",
      "Iteration 275, loss = 0.09535030\n",
      "Iteration 49, loss = 0.37369090\n",
      "Iteration 24, loss = 0.43455434\n",
      "Iteration 159, loss = 0.13372498\n",
      "Iteration 50, loss = 0.37221926\n",
      "Iteration 253, loss = 0.08491454\n",
      "Iteration 71, loss = 0.21815456\n",
      "Iteration 138, loss = 0.14774916\n",
      "Iteration 276, loss = 0.09522904\n",
      "Iteration 51, loss = 0.37073540\n",
      "Iteration 72, loss = 0.21637735\n",
      "Iteration 254, loss = 0.08481573\n",
      "Iteration 52, loss = 0.36916241\n",
      "Iteration 277, loss = 0.09512079\n",
      "Iteration 255, loss = 0.08459793\n",
      "Iteration 53, loss = 0.36761437\n",
      "Iteration 160, loss = 0.13340445\n",
      "Iteration 278, loss = 0.09500207\n",
      "Iteration 139, loss = 0.14737889\n",
      "Iteration 72, loss = 0.23412300\n",
      "Iteration 256, loss = 0.08446811\n",
      "Iteration 279, loss = 0.09487765\n",
      "Iteration 257, loss = 0.08430887\n",
      "Iteration 161, loss = 0.13308490\n",
      "Iteration 258, loss = 0.08416594\n",
      "Iteration 73, loss = 0.21457676\n",
      "Iteration 54, loss = 0.36595829\n",
      "Iteration 259, loss = 0.08401778\n",
      "Iteration 55, loss = 0.36430777\n",
      "Iteration 74, loss = 0.21291114\n",
      "Iteration 25, loss = 0.42697659\n",
      "Iteration 280, loss = 0.09475872\n",
      "Iteration 140, loss = 0.14705928\n",
      "Iteration 56, loss = 0.36266376\n",
      "Iteration 162, loss = 0.13278849\n",
      "Iteration 260, loss = 0.08388899\n",
      "Iteration 281, loss = 0.09464299\n",
      "Iteration 57, loss = 0.36082932\n",
      "Iteration 75, loss = 0.21128662\n",
      "Iteration 282, loss = 0.09452231\n",
      "Iteration 76, loss = 0.20975011\n",
      "Iteration 141, loss = 0.14668330\n",
      "Iteration 163, loss = 0.13247744\n",
      "Iteration 283, loss = 0.09442258\n",
      "Iteration 284, loss = 0.09431029\n",
      "Iteration 142, loss = 0.14630168\n",
      "Iteration 285, loss = 0.09417597\n",
      "Iteration 73, loss = 0.23231896\n",
      "Iteration 261, loss = 0.08377856\n",
      "Iteration 58, loss = 0.35910672\n",
      "Iteration 262, loss = 0.08362279\n",
      "Iteration 263, loss = 0.08352001\n",
      "Iteration 164, loss = 0.13218755\n",
      "Iteration 59, loss = 0.35735261\n",
      "Iteration 264, loss = 0.08335475\n",
      "Iteration 60, loss = 0.35538447\n",
      "Iteration 265, loss = 0.08324836\n",
      "Iteration 26, loss = 0.41939523\n",
      "Iteration 266, loss = 0.08311677\n",
      "Iteration 74, loss = 0.23061742\n",
      "Iteration 61, loss = 0.35351986\n",
      "Iteration 77, loss = 0.20818172\n",
      "Iteration 165, loss = 0.13187286\n",
      "Iteration 62, loss = 0.35159276\n",
      "Iteration 286, loss = 0.09407084\n",
      "Iteration 267, loss = 0.08296759\n",
      "Iteration 143, loss = 0.14593752\n",
      "Iteration 287, loss = 0.09396620\n",
      "Iteration 268, loss = 0.08286467\n",
      "Iteration 288, loss = 0.09384706\n",
      "Iteration 166, loss = 0.13155051\n",
      "Iteration 27, loss = 0.41169426\n",
      "Iteration 78, loss = 0.20675567\n",
      "Iteration 269, loss = 0.08271737\n",
      "Iteration 289, loss = 0.09374730\n",
      "Iteration 270, loss = 0.08267244Iteration 63, loss = 0.34961588\n",
      "\n",
      "Iteration 290, loss = 0.09363054\n",
      "Iteration 271, loss = 0.08245023\n",
      "Iteration 75, loss = 0.22889135\n",
      "Iteration 64, loss = 0.34768947\n",
      "Iteration 144, loss = 0.14562770\n",
      "Iteration 291, loss = 0.09352973\n",
      "Iteration 167, loss = 0.13128092\n",
      "Iteration 65, loss = 0.34572213\n",
      "Iteration 272, loss = 0.08233168\n",
      "Iteration 79, loss = 0.20532981\n",
      "Iteration 145, loss = 0.14526747\n",
      "Iteration 66, loss = 0.34372137\n",
      "Iteration 28, loss = 0.40433737\n",
      "Iteration 67, loss = 0.34161334\n",
      "Iteration 273, loss = 0.08222069\n",
      "Iteration 80, loss = 0.20393617\n",
      "Iteration 274, loss = 0.08210994\n",
      "Iteration 76, loss = 0.22732372\n",
      "Iteration 275, loss = 0.08197587\n",
      "Iteration 276, loss = 0.08188816\n",
      "Iteration 68, loss = 0.33959297\n",
      "Iteration 81, loss = 0.20261920\n",
      "Iteration 69, loss = 0.33741242Iteration 29, loss = 0.39714988\n",
      "\n",
      "Iteration 77, loss = 0.22578788\n",
      "Iteration 168, loss = 0.13097637\n",
      "Iteration 292, loss = 0.09343977\n",
      "Iteration 146, loss = 0.14491391\n",
      "Iteration 169, loss = 0.13070629\n",
      "Iteration 293, loss = 0.09331988\n",
      "Iteration 277, loss = 0.08173346\n",
      "Iteration 294, loss = 0.09320024\n",
      "Iteration 70, loss = 0.33523913\n",
      "Iteration 71, loss = 0.33304290\n",
      "Iteration 82, loss = 0.20132543\n",
      "Iteration 170, loss = 0.13039538\n",
      "Iteration 147, loss = 0.14459333\n",
      "Iteration 278, loss = 0.08163676\n",
      "Iteration 72, loss = 0.33084051\n",
      "Iteration 295, loss = 0.09308877\n",
      "Iteration 78, loss = 0.22424525\n",
      "Iteration 73, loss = 0.32867537\n",
      "Iteration 171, loss = 0.13009525\n",
      "Iteration 74, loss = 0.32646259\n",
      "Iteration 148, loss = 0.14424209\n",
      "Iteration 296, loss = 0.09298746\n",
      "Iteration 279, loss = 0.08150952\n",
      "Iteration 280, loss = 0.08138941\n",
      "Iteration 83, loss = 0.20007946\n",
      "Iteration 75, loss = 0.32414470\n",
      "Iteration 281, loss = 0.08126241\n",
      "Iteration 297, loss = 0.09287461\n",
      "Iteration 282, loss = 0.08116722\n",
      "Iteration 283, loss = 0.08107992\n",
      "Iteration 30, loss = 0.39047395\n",
      "Iteration 284, loss = 0.08093003\n",
      "Iteration 298, loss = 0.09276290\n",
      "Iteration 172, loss = 0.12985741\n",
      "Iteration 76, loss = 0.32192709\n",
      "Iteration 149, loss = 0.14393491\n",
      "Iteration 84, loss = 0.19884520\n",
      "Iteration 77, loss = 0.31970442\n",
      "Iteration 173, loss = 0.12953196\n",
      "Iteration 299, loss = 0.09266895\n",
      "Iteration 79, loss = 0.22284403\n",
      "Iteration 78, loss = 0.31730868\n",
      "Iteration 285, loss = 0.08083800\n",
      "Iteration 286, loss = 0.08071890\n",
      "Iteration 174, loss = 0.12924497\n",
      "Iteration 287, loss = 0.08065268\n",
      "Iteration 79, loss = 0.31499743\n",
      "Iteration 85, loss = 0.19767349\n",
      "Iteration 80, loss = 0.31258132\n",
      "Iteration 80, loss = 0.22141363\n",
      "Iteration 150, loss = 0.14361069\n",
      "Iteration 81, loss = 0.31018148\n",
      "Iteration 300, loss = 0.09255274\n",
      "Iteration 151, loss = 0.14327966\n",
      "Iteration 31, loss = 0.38349088\n",
      "Iteration 175, loss = 0.12895817\n",
      "Iteration 301, loss = 0.09244175\n",
      "Iteration 288, loss = 0.08050560\n",
      "Iteration 289, loss = 0.08041307\n",
      "Iteration 82, loss = 0.30768287\n",
      "Iteration 152, loss = 0.14297835\n",
      "Iteration 302, loss = 0.09234120\n",
      "Iteration 290, loss = 0.08028606\n",
      "Iteration 176, loss = 0.12868633\n",
      "Iteration 291, loss = 0.08021766\n",
      "Iteration 86, loss = 0.19652944\n",
      "Iteration 153, loss = 0.14267065\n",
      "Iteration 83, loss = 0.30515537\n",
      "Iteration 154, loss = 0.14236424\n",
      "Iteration 84, loss = 0.30257264\n",
      "Iteration 292, loss = 0.08007463\n",
      "Iteration 81, loss = 0.22007835\n",
      "Iteration 293, loss = 0.07998791\n",
      "Iteration 155, loss = 0.14206795\n",
      "Iteration 177, loss = 0.12844727\n",
      "Iteration 32, loss = 0.37698358\n",
      "Iteration 85, loss = 0.30000230\n",
      "Iteration 86, loss = 0.29742141\n",
      "Iteration 156, loss = 0.14174853\n",
      "Iteration 303, loss = 0.09222803\n",
      "Iteration 294, loss = 0.07987077\n",
      "Iteration 304, loss = 0.09212571\n",
      "Iteration 87, loss = 0.19537238\n",
      "Iteration 178, loss = 0.12814029\n",
      "Iteration 157, loss = 0.14146294\n",
      "Iteration 305, loss = 0.09201760\n",
      "Iteration 179, loss = 0.12788754\n",
      "Iteration 306, loss = 0.09192254\n",
      "Iteration 88, loss = 0.19434108\n",
      "Iteration 295, loss = 0.07978061\n",
      "Iteration 87, loss = 0.29483192\n",
      "Iteration 82, loss = 0.21877037\n",
      "Iteration 296, loss = 0.07967282\n",
      "Iteration 88, loss = 0.29222076\n",
      "Iteration 297, loss = 0.07958569\n",
      "Iteration 307, loss = 0.09181531\n",
      "Iteration 298, loss = 0.07949444\n",
      "Iteration 33, loss = 0.37066404\n",
      "Iteration 180, loss = 0.12759637\n",
      "Iteration 89, loss = 0.19323787\n",
      "Iteration 299, loss = 0.07937409\n",
      "Iteration 308, loss = 0.09170778\n",
      "Iteration 89, loss = 0.28955635\n",
      "Iteration 309, loss = 0.09161630\n",
      "Iteration 181, loss = 0.12733191\n",
      "Iteration 158, loss = 0.14116707\n",
      "Iteration 300, loss = 0.07927883\n",
      "Iteration 310, loss = 0.09150257\n",
      "Iteration 90, loss = 0.28691917\n",
      "Iteration 301, loss = 0.07918874\n",
      "Iteration 90, loss = 0.19220155\n",
      "Iteration 182, loss = 0.12710756\n",
      "Iteration 91, loss = 0.28420452\n",
      "Iteration 83, loss = 0.21750781\n",
      "Iteration 311, loss = 0.09140931\n",
      "Iteration 302, loss = 0.07910265\n",
      "Iteration 303, loss = 0.07899479\n",
      "Iteration 92, loss = 0.28146269\n",
      "Iteration 183, loss = 0.12684516\n",
      "Iteration 312, loss = 0.09133011\n",
      "Iteration 93, loss = 0.27867982\n",
      "Iteration 304, loss = 0.07888934\n",
      "Iteration 34, loss = 0.36443776\n",
      "Iteration 305, loss = 0.07881777\n",
      "Iteration 313, loss = 0.09119744\n",
      "Iteration 184, loss = 0.12658432\n",
      "Iteration 84, loss = 0.21628280\n",
      "Iteration 91, loss = 0.19122540\n",
      "Iteration 306, loss = 0.07871052\n",
      "Iteration 94, loss = 0.27577764\n",
      "Iteration 307, loss = 0.07863914\n",
      "Iteration 95, loss = 0.27293303\n",
      "Iteration 314, loss = 0.09109235\n",
      "Iteration 159, loss = 0.14087377\n",
      "Iteration 308, loss = 0.07854899\n",
      "Iteration 92, loss = 0.19018405\n",
      "Iteration 96, loss = 0.27002837\n",
      "Iteration 185, loss = 0.12631271\n",
      "Iteration 93, loss = 0.18924574\n",
      "Iteration 315, loss = 0.09099229\n",
      "Iteration 160, loss = 0.14059580\n",
      "Iteration 186, loss = 0.12604874\n",
      "Iteration 309, loss = 0.07842517\n",
      "Iteration 316, loss = 0.09088715\n",
      "Iteration 85, loss = 0.21508712\n",
      "Iteration 35, loss = 0.35860867\n",
      "Iteration 310, loss = 0.07835872\n",
      "Iteration 97, loss = 0.26709420\n",
      "Iteration 317, loss = 0.09077984\n",
      "Iteration 98, loss = 0.26406678\n",
      "Iteration 94, loss = 0.18828629\n",
      "Iteration 161, loss = 0.14030794\n",
      "Iteration 187, loss = 0.12582509\n",
      "Iteration 311, loss = 0.07825822\n",
      "Iteration 99, loss = 0.26110008\n",
      "Iteration 318, loss = 0.09069207\n",
      "Iteration 312, loss = 0.07816850\n",
      "Iteration 86, loss = 0.21396799\n",
      "Iteration 319, loss = 0.09060645\n",
      "Iteration 95, loss = 0.18735032\n",
      "Iteration 100, loss = 0.25818678\n",
      "Iteration 313, loss = 0.07806310\n",
      "Iteration 188, loss = 0.12557844\n",
      "Iteration 189, loss = 0.12532537\n",
      "Iteration 162, loss = 0.14003431\n",
      "Iteration 320, loss = 0.09050613\n",
      "Iteration 314, loss = 0.07801313\n",
      "Iteration 101, loss = 0.25516537\n",
      "Iteration 315, loss = 0.07790762\n",
      "Iteration 316, loss = 0.07781219\n",
      "Iteration 317, loss = 0.07773739\n",
      "Iteration 96, loss = 0.18647580\n",
      "Iteration 318, loss = 0.07765103\n",
      "Iteration 102, loss = 0.25208530\n",
      "Iteration 319, loss = 0.07756346\n",
      "Iteration 321, loss = 0.09038534\n",
      "Iteration 36, loss = 0.35285871\n",
      "Iteration 87, loss = 0.21282854\n",
      "Iteration 190, loss = 0.12509196\n",
      "Iteration 103, loss = 0.24909772\n",
      "Iteration 322, loss = 0.09028865\n",
      "Iteration 104, loss = 0.24624137\n",
      "Iteration 88, loss = 0.21180419\n",
      "Iteration 97, loss = 0.18561573\n",
      "Iteration 163, loss = 0.13976444\n",
      "Iteration 105, loss = 0.24307677\n",
      "Iteration 323, loss = 0.09020773\n",
      "Iteration 320, loss = 0.07749856\n",
      "Iteration 191, loss = 0.12488017\n",
      "Iteration 324, loss = 0.09011649\n",
      "Iteration 164, loss = 0.13950516\n",
      "Iteration 321, loss = 0.07739504\n",
      "Iteration 192, loss = 0.12461669\n",
      "Iteration 325, loss = 0.09001051\n",
      "Iteration 37, loss = 0.34734523\n",
      "Iteration 106, loss = 0.24017634\n",
      "Iteration 98, loss = 0.18473176\n",
      "Iteration 322, loss = 0.07732711Iteration 89, loss = 0.21071725\n",
      "\n",
      "Iteration 326, loss = 0.08990306\n",
      "Iteration 99, loss = 0.18393271\n",
      "Iteration 323, loss = 0.07722138\n",
      "Iteration 107, loss = 0.23712133\n",
      "Iteration 324, loss = 0.07714591\n",
      "Iteration 193, loss = 0.12440815\n",
      "Iteration 327, loss = 0.08983077\n",
      "Iteration 165, loss = 0.13920780\n",
      "Iteration 108, loss = 0.23423713\n",
      "Iteration 38, loss = 0.34193450\n",
      "Iteration 109, loss = 0.23117480\n",
      "Iteration 325, loss = 0.07706316\n",
      "Iteration 328, loss = 0.08973395\n",
      "Iteration 100, loss = 0.18305971\n",
      "Iteration 166, loss = 0.13891097\n",
      "Iteration 90, loss = 0.20978109\n",
      "Iteration 194, loss = 0.12416388\n",
      "Iteration 326, loss = 0.07698443\n",
      "Iteration 110, loss = 0.22835222\n",
      "Iteration 327, loss = 0.07691444\n",
      "Iteration 329, loss = 0.08963314\n",
      "Iteration 195, loss = 0.12393081\n",
      "Iteration 330, loss = 0.08955855\n",
      "Iteration 167, loss = 0.13866080\n",
      "Iteration 328, loss = 0.07682951\n",
      "Iteration 111, loss = 0.22536567\n",
      "Iteration 329, loss = 0.07674482\n",
      "Iteration 101, loss = 0.18225960\n",
      "Iteration 91, loss = 0.20872032\n",
      "Iteration 112, loss = 0.22246433\n",
      "Iteration 330, loss = 0.07669624\n",
      "Iteration 331, loss = 0.08945475\n",
      "Iteration 196, loss = 0.12370979\n",
      "Iteration 331, loss = 0.07658691\n",
      "Iteration 332, loss = 0.07650426\n",
      "Iteration 332, loss = 0.08936316\n",
      "Iteration 333, loss = 0.07644300\n",
      "Iteration 197, loss = 0.12345629\n",
      "Iteration 113, loss = 0.21964906\n",
      "Iteration 92, loss = 0.20779196\n",
      "Iteration 114, loss = 0.21707022\n",
      "Iteration 39, loss = 0.33667715\n",
      "Iteration 334, loss = 0.07635350\n",
      "Iteration 115, loss = 0.21427335\n",
      "Iteration 102, loss = 0.18145306\n",
      "Iteration 198, loss = 0.12322580\n",
      "Iteration 333, loss = 0.08933135\n",
      "Iteration 103, loss = 0.18070703\n",
      "Iteration 335, loss = 0.07629439\n",
      "Iteration 168, loss = 0.13838891\n",
      "Iteration 116, loss = 0.21150452\n",
      "Iteration 334, loss = 0.08918055\n",
      "Iteration 199, loss = 0.12299809\n",
      "Iteration 117, loss = 0.20892349\n",
      "Iteration 169, loss = 0.13814441\n",
      "Iteration 118, loss = 0.20628595\n",
      "Iteration 119, loss = 0.20381920\n",
      "Iteration 40, loss = 0.33156452\n",
      "Iteration 200, loss = 0.12278864\n",
      "Iteration 335, loss = 0.08909298\n",
      "Iteration 336, loss = 0.07620974\n",
      "Iteration 337, loss = 0.07613206\n",
      "Iteration 338, loss = 0.07608744\n",
      "Iteration 93, loss = 0.20683444\n",
      "Iteration 170, loss = 0.13787390\n",
      "Iteration 120, loss = 0.20123007\n",
      "Iteration 339, loss = 0.07599273\n",
      "Iteration 104, loss = 0.17989059\n",
      "Iteration 121, loss = 0.19891329\n",
      "Iteration 336, loss = 0.08900168\n",
      "Iteration 201, loss = 0.12256981\n",
      "Iteration 337, loss = 0.08890793\n",
      "Iteration 340, loss = 0.07592367\n",
      "Iteration 338, loss = 0.08883263\n",
      "Iteration 105, loss = 0.17919977\n",
      "Iteration 341, loss = 0.07585219\n",
      "Iteration 122, loss = 0.19644124\n",
      "Iteration 171, loss = 0.13760178\n",
      "Iteration 106, loss = 0.17843673\n",
      "Iteration 202, loss = 0.12235332\n",
      "Iteration 339, loss = 0.08876127\n",
      "Iteration 342, loss = 0.07581090\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 94, loss = 0.20596196\n",
      "Iteration 123, loss = 0.19409584\n",
      "Iteration 41, loss = 0.32679459\n",
      "Iteration 172, loss = 0.13736998\n",
      "Iteration 124, loss = 0.19187198\n",
      "Iteration 125, loss = 0.18973791\n",
      "Iteration 340, loss = 0.08864093\n",
      "Iteration 203, loss = 0.12214182\n",
      "Iteration 204, loss = 0.12194172\n",
      "Iteration 173, loss = 0.13709198\n",
      "Iteration 341, loss = 0.08855657\n",
      "Iteration 1, loss = 0.79639151\n",
      "Iteration 342, loss = 0.08848229\n",
      "Iteration 107, loss = 0.17768792\n",
      "Iteration 95, loss = 0.20506442\n",
      "Iteration 126, loss = 0.18755920\n",
      "Iteration 343, loss = 0.08839616\n",
      "Iteration 127, loss = 0.18546005\n",
      "Iteration 42, loss = 0.32217212\n",
      "Iteration 128, loss = 0.18351610\n",
      "Iteration 129, loss = 0.18151729\n",
      "Iteration 205, loss = 0.12170941\n",
      "Iteration 130, loss = 0.17960431\n",
      "Iteration 344, loss = 0.08830085\n",
      "Iteration 206, loss = 0.12152242\n",
      "Iteration 108, loss = 0.17700924\n",
      "Iteration 131, loss = 0.17775777\n",
      "Iteration 96, loss = 0.20419002\n",
      "Iteration 132, loss = 0.17599597\n",
      "Iteration 109, loss = 0.17632020\n",
      "Iteration 345, loss = 0.08821018\n",
      "Iteration 110, loss = 0.17565087\n",
      "Iteration 2, loss = 0.77265971\n",
      "Iteration 43, loss = 0.31766952\n",
      "Iteration 133, loss = 0.17427014\n",
      "Iteration 207, loss = 0.12129954\n",
      "Iteration 346, loss = 0.08812615\n",
      "Iteration 97, loss = 0.20335319\n",
      "Iteration 3, loss = 0.74059889\n",
      "Iteration 347, loss = 0.08805523\n",
      "Iteration 174, loss = 0.13682486\n",
      "Iteration 208, loss = 0.12109736\n",
      "Iteration 134, loss = 0.17266517\n",
      "Iteration 111, loss = 0.17496929\n",
      "Iteration 175, loss = 0.13657552\n",
      "Iteration 209, loss = 0.12089208\n",
      "Iteration 135, loss = 0.17092272\n",
      "Iteration 348, loss = 0.08794495\n",
      "Iteration 176, loss = 0.13632121\n",
      "Iteration 98, loss = 0.20253328\n",
      "Iteration 136, loss = 0.16945809\n",
      "Iteration 112, loss = 0.17431184\n",
      "Iteration 349, loss = 0.08785998\n",
      "Iteration 210, loss = 0.12070788\n",
      "Iteration 44, loss = 0.31351325\n",
      "Iteration 137, loss = 0.16788511\n",
      "Iteration 4, loss = 0.70702333\n",
      "Iteration 350, loss = 0.08777903\n",
      "Iteration 177, loss = 0.13608516\n",
      "Iteration 211, loss = 0.12050733\n",
      "Iteration 351, loss = 0.08770606\n",
      "Iteration 212, loss = 0.12031337\n",
      "Iteration 178, loss = 0.13584049\n",
      "Iteration 138, loss = 0.16640508\n",
      "Iteration 5, loss = 0.67519366\n",
      "Iteration 139, loss = 0.16500727\n",
      "Iteration 113, loss = 0.17370263\n",
      "Iteration 352, loss = 0.08761488\n",
      "Iteration 99, loss = 0.20173440\n",
      "Iteration 179, loss = 0.13558474\n",
      "Iteration 140, loss = 0.16362864\n",
      "Iteration 141, loss = 0.16228382\n",
      "Iteration 180, loss = 0.13535835\n",
      "Iteration 142, loss = 0.16106572\n",
      "Iteration 45, loss = 0.30934188\n",
      "Iteration 143, loss = 0.15977864\n",
      "Iteration 6, loss = 0.64606045\n",
      "Iteration 144, loss = 0.15859521\n",
      "Iteration 353, loss = 0.08754194\n",
      "Iteration 213, loss = 0.12012164\n",
      "Iteration 114, loss = 0.17307056\n",
      "Iteration 354, loss = 0.08745601\n",
      "Iteration 100, loss = 0.20097793\n",
      "Iteration 214, loss = 0.11994890\n",
      "Iteration 355, loss = 0.08736774\n",
      "Iteration 115, loss = 0.17242580\n",
      "Iteration 145, loss = 0.15731178\n",
      "Iteration 215, loss = 0.11972653\n",
      "Iteration 7, loss = 0.62023340\n",
      "Iteration 46, loss = 0.30552155\n",
      "Iteration 181, loss = 0.13511446\n",
      "Iteration 101, loss = 0.20020190\n",
      "Iteration 356, loss = 0.08729972\n",
      "Iteration 8, loss = 0.59704865\n",
      "Iteration 146, loss = 0.15620060\n",
      "Iteration 147, loss = 0.15504543\n",
      "Iteration 102, loss = 0.19946428\n",
      "Iteration 357, loss = 0.08721570\n",
      "Iteration 148, loss = 0.15397447\n",
      "Iteration 116, loss = 0.17182080\n",
      "Iteration 216, loss = 0.11955936\n",
      "Iteration 358, loss = 0.08713276\n",
      "Iteration 182, loss = 0.13487783\n",
      "Iteration 217, loss = 0.11936249\n",
      "Iteration 359, loss = 0.08704646\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 149, loss = 0.15289929\n",
      "Iteration 47, loss = 0.30176273\n",
      "Iteration 117, loss = 0.17120397\n",
      "Iteration 218, loss = 0.11918439\n",
      "Iteration 9, loss = 0.57713915\n",
      "Iteration 150, loss = 0.15185736\n",
      "Iteration 151, loss = 0.15083377\n",
      "Iteration 183, loss = 0.13463988\n",
      "Iteration 152, loss = 0.14991239\n",
      "Iteration 219, loss = 0.11897633\n",
      "Iteration 153, loss = 0.14886066\n",
      "Iteration 103, loss = 0.19875380\n",
      "Iteration 184, loss = 0.13441949\n",
      "Iteration 118, loss = 0.17068987Iteration 10, loss = 0.55891988\n",
      "Iteration 220, loss = 0.11880657\n",
      "\n",
      "Iteration 48, loss = 0.29823132\n",
      "Iteration 1, loss = 0.97395494\n",
      "Iteration 154, loss = 0.14793055\n",
      "Iteration 119, loss = 0.17008577\n",
      "Iteration 155, loss = 0.14701016\n",
      "Iteration 156, loss = 0.14610534\n",
      "Iteration 221, loss = 0.11862850\n",
      "Iteration 185, loss = 0.13418385\n",
      "Iteration 11, loss = 0.54347932\n",
      "Iteration 104, loss = 0.19803880\n",
      "Iteration 120, loss = 0.16947622\n",
      "Iteration 222, loss = 0.11844168\n",
      "Iteration 186, loss = 0.13394657\n",
      "Iteration 157, loss = 0.14525650\n",
      "Iteration 223, loss = 0.11827762\n",
      "Iteration 158, loss = 0.14439029\n",
      "Iteration 121, loss = 0.16892358\n",
      "Iteration 159, loss = 0.14353188\n",
      "Iteration 49, loss = 0.29478874\n",
      "Iteration 12, loss = 0.52958647\n",
      "Iteration 105, loss = 0.19733878\n",
      "Iteration 224, loss = 0.11810741\n",
      "Iteration 160, loss = 0.14271167\n",
      "Iteration 187, loss = 0.13372734\n",
      "Iteration 13, loss = 0.51676932\n",
      "Iteration 122, loss = 0.16841415\n",
      "Iteration 225, loss = 0.11792722\n",
      "Iteration 161, loss = 0.14193033\n",
      "Iteration 14, loss = 0.50650037\n",
      "Iteration 188, loss = 0.13350713Iteration 123, loss = 0.16784382\n",
      "Iteration 162, loss = 0.14118660\n",
      "Iteration 163, loss = 0.14039125\n",
      "\n",
      "Iteration 50, loss = 0.29164937\n",
      "Iteration 106, loss = 0.19668498\n",
      "Iteration 2, loss = 0.93422202\n",
      "Iteration 164, loss = 0.13965237\n",
      "Iteration 226, loss = 0.11774856\n",
      "Iteration 165, loss = 0.13890528\n",
      "Iteration 166, loss = 0.13819520\n",
      "Iteration 51, loss = 0.28863267\n",
      "Iteration 189, loss = 0.13329987\n",
      "Iteration 167, loss = 0.13750663\n",
      "Iteration 124, loss = 0.16733127\n",
      "Iteration 15, loss = 0.49659198\n",
      "Iteration 227, loss = 0.11756780\n",
      "Iteration 168, loss = 0.13689568\n",
      "Iteration 125, loss = 0.16680546\n",
      "Iteration 228, loss = 0.11743756\n",
      "Iteration 107, loss = 0.19603618\n",
      "Iteration 190, loss = 0.13306754\n",
      "Iteration 169, loss = 0.13618679\n",
      "Iteration 170, loss = 0.13551158\n",
      "Iteration 16, loss = 0.48770286\n",
      "Iteration 171, loss = 0.13490251\n",
      "Iteration 3, loss = 0.88332455\n",
      "Iteration 229, loss = 0.11725440\n",
      "Iteration 191, loss = 0.13284600\n",
      "Iteration 126, loss = 0.16624517\n",
      "Iteration 52, loss = 0.28569508\n",
      "Iteration 17, loss = 0.47995215\n",
      "Iteration 192, loss = 0.13264755\n",
      "Iteration 127, loss = 0.16580822\n",
      "Iteration 18, loss = 0.47323773\n",
      "Iteration 172, loss = 0.13425516\n",
      "Iteration 108, loss = 0.19540902\n",
      "Iteration 230, loss = 0.11709168\n",
      "Iteration 173, loss = 0.13364077\n",
      "Iteration 128, loss = 0.16528330\n",
      "Iteration 231, loss = 0.11691765\n",
      "Iteration 174, loss = 0.13307201\n",
      "Iteration 193, loss = 0.13241759\n",
      "Iteration 175, loss = 0.13246817\n",
      "Iteration 232, loss = 0.11676216\n",
      "Iteration 53, loss = 0.28295290Iteration 109, loss = 0.19475575\n",
      "\n",
      "Iteration 4, loss = 0.83464585\n",
      "Iteration 194, loss = 0.13220996\n",
      "Iteration 176, loss = 0.13186745\n",
      "Iteration 177, loss = 0.13128400\n",
      "Iteration 19, loss = 0.46667149\n",
      "Iteration 20, loss = 0.46090559\n",
      "Iteration 233, loss = 0.11661831\n",
      "Iteration 110, loss = 0.19421318\n",
      "Iteration 129, loss = 0.16476931\n",
      "Iteration 234, loss = 0.11645556\n",
      "Iteration 178, loss = 0.13073309\n",
      "Iteration 179, loss = 0.13019617\n",
      "Iteration 235, loss = 0.11627758\n",
      "Iteration 195, loss = 0.13200096\n",
      "Iteration 180, loss = 0.12964306\n",
      "Iteration 181, loss = 0.12910923\n",
      "Iteration 54, loss = 0.28030357\n",
      "Iteration 130, loss = 0.16430173\n",
      "Iteration 182, loss = 0.12854593\n",
      "Iteration 5, loss = 0.79045130\n",
      "Iteration 21, loss = 0.45573955\n",
      "Iteration 236, loss = 0.11612696\n",
      "Iteration 183, loss = 0.12804452\n",
      "Iteration 131, loss = 0.16380796\n",
      "Iteration 184, loss = 0.12752012\n",
      "Iteration 237, loss = 0.11596685\n",
      "Iteration 238, loss = 0.11583080\n",
      "Iteration 185, loss = 0.12704398\n",
      "Iteration 196, loss = 0.13177443\n",
      "Iteration 22, loss = 0.45056199\n",
      "Iteration 239, loss = 0.11566196\n",
      "Iteration 111, loss = 0.19357216\n",
      "Iteration 240, loss = 0.11552728\n",
      "Iteration 132, loss = 0.16335904\n",
      "Iteration 186, loss = 0.12658944\n",
      "Iteration 197, loss = 0.13158906\n",
      "Iteration 6, loss = 0.75285468\n",
      "Iteration 187, loss = 0.12604830\n",
      "Iteration 55, loss = 0.27772522\n",
      "Iteration 198, loss = 0.13138973\n",
      "Iteration 23, loss = 0.44595636\n",
      "Iteration 241, loss = 0.11538283\n",
      "Iteration 188, loss = 0.12558193\n",
      "Iteration 133, loss = 0.16289621\n",
      "Iteration 189, loss = 0.12514243\n",
      "Iteration 112, loss = 0.19296047\n",
      "Iteration 134, loss = 0.16245140\n",
      "Iteration 242, loss = 0.11522407\n",
      "Iteration 190, loss = 0.12471027\n",
      "Iteration 191, loss = 0.12426279\n",
      "Iteration 199, loss = 0.13119025\n",
      "Iteration 135, loss = 0.16201054\n",
      "Iteration 24, loss = 0.44168303\n",
      "Iteration 56, loss = 0.27546757\n",
      "Iteration 7, loss = 0.71932556\n",
      "Iteration 243, loss = 0.11508097\n",
      "Iteration 113, loss = 0.19238799\n",
      "Iteration 192, loss = 0.12378298\n",
      "Iteration 136, loss = 0.16156366\n",
      "Iteration 25, loss = 0.43734380\n",
      "Iteration 200, loss = 0.13097108\n",
      "Iteration 193, loss = 0.12333151\n",
      "Iteration 244, loss = 0.11492992\n",
      "Iteration 201, loss = 0.13077542\n",
      "Iteration 194, loss = 0.12291034\n",
      "Iteration 8, loss = 0.69085379\n",
      "Iteration 57, loss = 0.27312602\n",
      "Iteration 137, loss = 0.16115175\n",
      "Iteration 114, loss = 0.19183917\n",
      "Iteration 245, loss = 0.11479467\n",
      "Iteration 195, loss = 0.12247298\n",
      "Iteration 202, loss = 0.13058278\n",
      "Iteration 138, loss = 0.16073022\n",
      "Iteration 196, loss = 0.12211296\n",
      "Iteration 197, loss = 0.12164908\n",
      "Iteration 246, loss = 0.11464951\n",
      "Iteration 26, loss = 0.43325516\n",
      "Iteration 198, loss = 0.12123770\n",
      "Iteration 115, loss = 0.19127281\n",
      "Iteration 199, loss = 0.12086275\n",
      "Iteration 203, loss = 0.13037566\n",
      "Iteration 247, loss = 0.11449898\n",
      "Iteration 139, loss = 0.16028509\n",
      "Iteration 27, loss = 0.42931203\n",
      "Iteration 248, loss = 0.11437622\n",
      "Iteration 58, loss = 0.27093901\n",
      "Iteration 249, loss = 0.11422783\n",
      "Iteration 9, loss = 0.66701055\n",
      "Iteration 200, loss = 0.12045757\n",
      "Iteration 201, loss = 0.12011363\n",
      "Iteration 204, loss = 0.13017757\n",
      "Iteration 116, loss = 0.19071957\n",
      "Iteration 202, loss = 0.11970848\n",
      "Iteration 203, loss = 0.11938944\n",
      "Iteration 28, loss = 0.42547701\n",
      "Iteration 250, loss = 0.11408539\n",
      "Iteration 205, loss = 0.12998406\n",
      "Iteration 140, loss = 0.15989861\n",
      "Iteration 204, loss = 0.11893136\n",
      "Iteration 251, loss = 0.11395438\n",
      "Iteration 141, loss = 0.15950522\n",
      "Iteration 59, loss = 0.26886649\n",
      "Iteration 206, loss = 0.12979359\n",
      "Iteration 205, loss = 0.11856943\n",
      "Iteration 10, loss = 0.64530473\n",
      "Iteration 117, loss = 0.19019452\n",
      "Iteration 29, loss = 0.42167274\n",
      "Iteration 252, loss = 0.11382294\n",
      "Iteration 206, loss = 0.11818056\n",
      "Iteration 207, loss = 0.12958746\n",
      "Iteration 142, loss = 0.15907461\n",
      "Iteration 207, loss = 0.11784693\n",
      "Iteration 208, loss = 0.11750015\n",
      "Iteration 143, loss = 0.15867696\n",
      "Iteration 208, loss = 0.12938993\n",
      "Iteration 253, loss = 0.11368285\n",
      "Iteration 209, loss = 0.11714227\n",
      "Iteration 254, loss = 0.11356482\n",
      "Iteration 210, loss = 0.11676372\n",
      "Iteration 60, loss = 0.26691895\n",
      "Iteration 211, loss = 0.11648342\n",
      "Iteration 144, loss = 0.15827621\n",
      "Iteration 11, loss = 0.62717548\n",
      "Iteration 118, loss = 0.18966560\n",
      "Iteration 209, loss = 0.12921204\n",
      "Iteration 255, loss = 0.11344170\n",
      "Iteration 212, loss = 0.11609653\n",
      "Iteration 145, loss = 0.15789148\n",
      "Iteration 213, loss = 0.11576503\n",
      "Iteration 119, loss = 0.18915148\n",
      "Iteration 256, loss = 0.11328268\n",
      "Iteration 214, loss = 0.11542282\n",
      "Iteration 61, loss = 0.26497514\n",
      "Iteration 210, loss = 0.12902492\n",
      "Iteration 257, loss = 0.11317576\n",
      "Iteration 215, loss = 0.11513393\n",
      "Iteration 216, loss = 0.11480721\n",
      "Iteration 30, loss = 0.41800289\n",
      "Iteration 217, loss = 0.11447635\n",
      "Iteration 31, loss = 0.41436038\n",
      "Iteration 12, loss = 0.61133738\n",
      "Iteration 258, loss = 0.11303001\n",
      "Iteration 120, loss = 0.18863215\n",
      "Iteration 146, loss = 0.15751467\n",
      "Iteration 211, loss = 0.12884983\n",
      "Iteration 32, loss = 0.41078215\n",
      "Iteration 218, loss = 0.11415192\n",
      "Iteration 147, loss = 0.15715371\n",
      "Iteration 62, loss = 0.26315520\n",
      "Iteration 219, loss = 0.11387677\n",
      "Iteration 259, loss = 0.11289970\n",
      "Iteration 121, loss = 0.18816003\n",
      "Iteration 33, loss = 0.40727808\n",
      "Iteration 212, loss = 0.12864485\n",
      "Iteration 220, loss = 0.11358909\n",
      "Iteration 148, loss = 0.15677207\n",
      "Iteration 260, loss = 0.11277579\n",
      "Iteration 221, loss = 0.11325111\n",
      "Iteration 213, loss = 0.12845704\n",
      "Iteration 13, loss = 0.59756996\n",
      "Iteration 34, loss = 0.40380649\n",
      "Iteration 222, loss = 0.11297378\n",
      "Iteration 261, loss = 0.11264592\n",
      "Iteration 223, loss = 0.11268656\n",
      "Iteration 224, loss = 0.11239971\n",
      "Iteration 262, loss = 0.11252880\n",
      "Iteration 149, loss = 0.15637813\n",
      "Iteration 63, loss = 0.26141757\n",
      "Iteration 122, loss = 0.18766772\n",
      "Iteration 214, loss = 0.12829937\n",
      "Iteration 225, loss = 0.11210659\n",
      "Iteration 35, loss = 0.40043853\n",
      "Iteration 150, loss = 0.15604511\n",
      "Iteration 226, loss = 0.11182910\n",
      "Iteration 227, loss = 0.11154932\n",
      "Iteration 263, loss = 0.11239060\n",
      "Iteration 228, loss = 0.11127040\n",
      "Iteration 64, loss = 0.25971860\n",
      "Iteration 264, loss = 0.11229273\n",
      "Iteration 36, loss = 0.39701362\n",
      "Iteration 215, loss = 0.12809999\n",
      "Iteration 14, loss = 0.58468424\n",
      "Iteration 123, loss = 0.18713736\n",
      "Iteration 151, loss = 0.15567706\n",
      "Iteration 265, loss = 0.11215525\n",
      "Iteration 229, loss = 0.11101371\n",
      "Iteration 230, loss = 0.11072888\n",
      "Iteration 231, loss = 0.11047056\n",
      "Iteration 266, loss = 0.11204079\n",
      "Iteration 152, loss = 0.15530762\n",
      "Iteration 216, loss = 0.12791584\n",
      "Iteration 267, loss = 0.11189697\n",
      "Iteration 232, loss = 0.11022746\n",
      "Iteration 124, loss = 0.18667319\n",
      "Iteration 217, loss = 0.12774238\n",
      "Iteration 268, loss = 0.11178296\n",
      "Iteration 153, loss = 0.15498720\n",
      "Iteration 37, loss = 0.39364359\n",
      "Iteration 125, loss = 0.18620789\n",
      "Iteration 218, loss = 0.12756283\n",
      "Iteration 15, loss = 0.57409392\n",
      "Iteration 269, loss = 0.11165846\n",
      "Iteration 65, loss = 0.25809053\n",
      "Iteration 233, loss = 0.10995963\n",
      "Iteration 234, loss = 0.10964746\n",
      "Iteration 38, loss = 0.39034323\n",
      "Iteration 235, loss = 0.10943831\n",
      "Iteration 154, loss = 0.15463981\n",
      "Iteration 219, loss = 0.12741230\n",
      "Iteration 155, loss = 0.15429787\n",
      "Iteration 220, loss = 0.12723905\n",
      "Iteration 236, loss = 0.10917973\n",
      "Iteration 270, loss = 0.11157371\n",
      "Iteration 39, loss = 0.38711343\n",
      "Iteration 126, loss = 0.18573424\n",
      "Iteration 271, loss = 0.11142779\n",
      "Iteration 237, loss = 0.10890157\n",
      "Iteration 156, loss = 0.15397577\n",
      "Iteration 66, loss = 0.25660774\n",
      "Iteration 238, loss = 0.10862964\n",
      "Iteration 239, loss = 0.10841674\n",
      "Iteration 16, loss = 0.56427887\n",
      "Iteration 272, loss = 0.11129857\n",
      "Iteration 40, loss = 0.38381091\n",
      "Iteration 221, loss = 0.12705059\n",
      "Iteration 127, loss = 0.18531920\n",
      "Iteration 67, loss = 0.25505666\n",
      "Iteration 222, loss = 0.12688604\n",
      "Iteration 240, loss = 0.10823350\n",
      "Iteration 273, loss = 0.11118735\n",
      "Iteration 157, loss = 0.15359718\n",
      "Iteration 241, loss = 0.10792138\n",
      "Iteration 41, loss = 0.38058449\n",
      "Iteration 242, loss = 0.10766813\n",
      "Iteration 158, loss = 0.15328692\n",
      "Iteration 274, loss = 0.11108524\n",
      "Iteration 243, loss = 0.10741936\n",
      "Iteration 42, loss = 0.37737241\n",
      "Iteration 17, loss = 0.55519675\n",
      "Iteration 128, loss = 0.18482652\n",
      "Iteration 159, loss = 0.15295434\n",
      "Iteration 275, loss = 0.11098570\n",
      "Iteration 244, loss = 0.10719699\n",
      "Iteration 129, loss = 0.18438120\n",
      "Iteration 68, loss = 0.25361902\n",
      "Iteration 223, loss = 0.12672071\n",
      "Iteration 245, loss = 0.10703262\n",
      "Iteration 276, loss = 0.11085625\n",
      "Iteration 160, loss = 0.15262540\n",
      "Iteration 224, loss = 0.12656497\n",
      "Iteration 43, loss = 0.37409161\n",
      "Iteration 246, loss = 0.10671625\n",
      "Iteration 277, loss = 0.11072554\n",
      "Iteration 130, loss = 0.18394864\n",
      "Iteration 225, loss = 0.12638038\n",
      "Iteration 18, loss = 0.54692614\n",
      "Iteration 161, loss = 0.15229411\n",
      "Iteration 278, loss = 0.11063015\n",
      "Iteration 247, loss = 0.10649709\n",
      "Iteration 69, loss = 0.25223977\n",
      "Iteration 44, loss = 0.37090692\n",
      "Iteration 248, loss = 0.10627966\n",
      "Iteration 162, loss = 0.15197663\n",
      "Iteration 249, loss = 0.10604528\n",
      "Iteration 226, loss = 0.12621936\n",
      "Iteration 45, loss = 0.36769977\n",
      "Iteration 250, loss = 0.10585030\n",
      "Iteration 131, loss = 0.18354010\n",
      "Iteration 227, loss = 0.12604303\n",
      "Iteration 279, loss = 0.11051593\n",
      "Iteration 70, loss = 0.25088028\n",
      "Iteration 251, loss = 0.10561361\n",
      "Iteration 163, loss = 0.15164268\n",
      "Iteration 280, loss = 0.11038709\n",
      "Iteration 19, loss = 0.53931446\n",
      "Iteration 252, loss = 0.10538047\n",
      "Iteration 132, loss = 0.18309860\n",
      "Iteration 46, loss = 0.36460675\n",
      "Iteration 281, loss = 0.11030198\n",
      "Iteration 164, loss = 0.15135971\n",
      "Iteration 253, loss = 0.10518477\n",
      "Iteration 228, loss = 0.12589387\n",
      "Iteration 229, loss = 0.12573285\n",
      "Iteration 47, loss = 0.36134919\n",
      "Iteration 282, loss = 0.11017222\n",
      "Iteration 254, loss = 0.10494408\n",
      "Iteration 255, loss = 0.10477867Iteration 71, loss = 0.24961618\n",
      "\n",
      "Iteration 133, loss = 0.18266885\n",
      "Iteration 283, loss = 0.11006895\n",
      "Iteration 256, loss = 0.10452023\n",
      "Iteration 20, loss = 0.53199639\n",
      "Iteration 257, loss = 0.10429860\n",
      "Iteration 48, loss = 0.35827554\n",
      "Iteration 165, loss = 0.15103897\n",
      "Iteration 258, loss = 0.10412057\n",
      "Iteration 230, loss = 0.12556186\n",
      "Iteration 259, loss = 0.10391440\n",
      "Iteration 284, loss = 0.10995533\n",
      "Iteration 166, loss = 0.15070913\n",
      "Iteration 134, loss = 0.18225925\n",
      "Iteration 72, loss = 0.24832254\n",
      "Iteration 260, loss = 0.10371942\n",
      "Iteration 261, loss = 0.10352284\n",
      "Iteration 167, loss = 0.15042058\n",
      "Iteration 231, loss = 0.12541879\n",
      "Iteration 262, loss = 0.10328755\n",
      "Iteration 49, loss = 0.35518546\n",
      "Iteration 285, loss = 0.10986234\n",
      "Iteration 135, loss = 0.18184825\n",
      "Iteration 168, loss = 0.15014154\n",
      "Iteration 21, loss = 0.52506247\n",
      "Iteration 232, loss = 0.12525805\n",
      "Iteration 73, loss = 0.24712458\n",
      "Iteration 263, loss = 0.10310865\n",
      "Iteration 286, loss = 0.10974368\n",
      "Iteration 50, loss = 0.35212611\n",
      "Iteration 264, loss = 0.10292838\n",
      "Iteration 169, loss = 0.14983864\n",
      "Iteration 265, loss = 0.10275248\n",
      "Iteration 136, loss = 0.18144759\n",
      "Iteration 51, loss = 0.34910624\n",
      "Iteration 266, loss = 0.10251479\n",
      "Iteration 267, loss = 0.10234371\n",
      "Iteration 287, loss = 0.10964159\n",
      "Iteration 268, loss = 0.10214988\n",
      "Iteration 269, loss = 0.10206130\n",
      "Iteration 270, loss = 0.10175999\n",
      "Iteration 170, loss = 0.14950023\n",
      "Iteration 233, loss = 0.12510448\n",
      "Iteration 288, loss = 0.10953893\n",
      "Iteration 74, loss = 0.24596379\n",
      "Iteration 52, loss = 0.34603435\n",
      "Iteration 171, loss = 0.14919453\n",
      "Iteration 137, loss = 0.18104244\n",
      "Iteration 289, loss = 0.10943493\n",
      "Iteration 22, loss = 0.51812268\n",
      "Iteration 53, loss = 0.34309591\n",
      "Iteration 290, loss = 0.10935366\n",
      "Iteration 271, loss = 0.10158603\n",
      "Iteration 234, loss = 0.12494191\n",
      "Iteration 272, loss = 0.10139661Iteration 172, loss = 0.14897333\n",
      "Iteration 291, loss = 0.10923939\n",
      "\n",
      "Iteration 54, loss = 0.34022890\n",
      "Iteration 273, loss = 0.10121221\n",
      "Iteration 235, loss = 0.12478410\n",
      "Iteration 75, loss = 0.24483333\n",
      "Iteration 274, loss = 0.10103889\n",
      "Iteration 173, loss = 0.14859846\n",
      "Iteration 138, loss = 0.18065454\n",
      "Iteration 292, loss = 0.10912788\n",
      "Iteration 275, loss = 0.10086755\n",
      "Iteration 23, loss = 0.51143695\n",
      "Iteration 236, loss = 0.12463179\n",
      "Iteration 276, loss = 0.10069798\n",
      "Iteration 55, loss = 0.33727465\n",
      "Iteration 293, loss = 0.10903455\n",
      "Iteration 277, loss = 0.10054966\n",
      "Iteration 237, loss = 0.12448452\n",
      "Iteration 174, loss = 0.14835177\n",
      "Iteration 294, loss = 0.10893488\n",
      "Iteration 139, loss = 0.18028362\n",
      "Iteration 24, loss = 0.50484969\n",
      "Iteration 56, loss = 0.33445409\n",
      "Iteration 76, loss = 0.24370865\n",
      "Iteration 295, loss = 0.10884681\n",
      "Iteration 57, loss = 0.33170922\n",
      "Iteration 278, loss = 0.10037908\n",
      "Iteration 175, loss = 0.14805359\n",
      "Iteration 296, loss = 0.10874015\n",
      "Iteration 140, loss = 0.17988472\n",
      "Iteration 238, loss = 0.12433432\n",
      "Iteration 279, loss = 0.10016210\n",
      "Iteration 280, loss = 0.10004894\n",
      "Iteration 281, loss = 0.09993862\n",
      "Iteration 239, loss = 0.12417920\n",
      "Iteration 176, loss = 0.14776638\n",
      "Iteration 58, loss = 0.32893430\n",
      "Iteration 282, loss = 0.09967179\n",
      "Iteration 283, loss = 0.09951936\n",
      "Iteration 25, loss = 0.49837944\n",
      "Iteration 77, loss = 0.24263425\n",
      "Iteration 297, loss = 0.10866179\n",
      "Iteration 177, loss = 0.14750496\n",
      "Iteration 141, loss = 0.17950486\n",
      "Iteration 284, loss = 0.09937238\n",
      "Iteration 240, loss = 0.12403303\n",
      "Iteration 178, loss = 0.14721532\n",
      "Iteration 298, loss = 0.10855436\n",
      "Iteration 285, loss = 0.09921837\n",
      "Iteration 59, loss = 0.32614040\n",
      "Iteration 286, loss = 0.09906067\n",
      "Iteration 299, loss = 0.10844956\n",
      "Iteration 241, loss = 0.12387669\n",
      "Iteration 287, loss = 0.09886406\n",
      "Iteration 78, loss = 0.24160805\n",
      "Iteration 300, loss = 0.10834442\n",
      "Iteration 60, loss = 0.32345248\n",
      "Iteration 288, loss = 0.09879379\n",
      "Iteration 142, loss = 0.17912056\n",
      "Iteration 301, loss = 0.10829021\n",
      "Iteration 79, loss = 0.24059661\n",
      "Iteration 179, loss = 0.14694435\n",
      "Iteration 289, loss = 0.09859549\n",
      "Iteration 26, loss = 0.49187092\n",
      "Iteration 180, loss = 0.14668774\n",
      "Iteration 61, loss = 0.32080315\n",
      "Iteration 290, loss = 0.09841638\n",
      "Iteration 242, loss = 0.12374801\n",
      "Iteration 143, loss = 0.17874123Iteration 302, loss = 0.10817935\n",
      "\n",
      "Iteration 291, loss = 0.09825380\n",
      "Iteration 243, loss = 0.12359641\n",
      "Iteration 181, loss = 0.14641535\n",
      "Iteration 303, loss = 0.10806136\n",
      "Iteration 304, loss = 0.10797949\n",
      "Iteration 62, loss = 0.31816641\n",
      "Iteration 27, loss = 0.48535724\n",
      "Iteration 292, loss = 0.09809768\n",
      "Iteration 144, loss = 0.17838573\n",
      "Iteration 182, loss = 0.14610356\n",
      "Iteration 244, loss = 0.12346112\n",
      "Iteration 293, loss = 0.09794720\n",
      "Iteration 183, loss = 0.14586187\n",
      "Iteration 63, loss = 0.31555973\n",
      "Iteration 294, loss = 0.09781875\n",
      "Iteration 305, loss = 0.10789456\n",
      "Iteration 80, loss = 0.23959681\n",
      "Iteration 295, loss = 0.09766270\n",
      "Iteration 296, loss = 0.09751822\n",
      "Iteration 245, loss = 0.12331869\n",
      "Iteration 184, loss = 0.14560501\n",
      "Iteration 306, loss = 0.10780870\n",
      "Iteration 307, loss = 0.10770897\n",
      "Iteration 64, loss = 0.31299661\n",
      "Iteration 297, loss = 0.09734945\n",
      "Iteration 185, loss = 0.14530210\n",
      "Iteration 246, loss = 0.12315592\n",
      "Iteration 145, loss = 0.17802375\n",
      "Iteration 81, loss = 0.23864320\n",
      "Iteration 298, loss = 0.09723681\n",
      "Iteration 65, loss = 0.31050996\n",
      "Iteration 247, loss = 0.12302890\n",
      "Iteration 308, loss = 0.10761126\n",
      "Iteration 299, loss = 0.09706882\n",
      "Iteration 186, loss = 0.14510595\n",
      "Iteration 28, loss = 0.47916925\n",
      "Iteration 300, loss = 0.09691268\n",
      "Iteration 309, loss = 0.10754074\n",
      "Iteration 301, loss = 0.09677751\n",
      "Iteration 302, loss = 0.09661405\n",
      "Iteration 248, loss = 0.12288914\n",
      "Iteration 146, loss = 0.17766603\n",
      "Iteration 66, loss = 0.30797670\n",
      "Iteration 187, loss = 0.14479254\n",
      "Iteration 310, loss = 0.10745156\n",
      "Iteration 29, loss = 0.47253155\n",
      "Iteration 303, loss = 0.09648276\n",
      "Iteration 82, loss = 0.23771392\n",
      "Iteration 311, loss = 0.10736187\n",
      "Iteration 249, loss = 0.12273344\n",
      "Iteration 188, loss = 0.14456191\n",
      "Iteration 67, loss = 0.30560639\n",
      "Iteration 304, loss = 0.09636274\n",
      "Iteration 147, loss = 0.17729440\n",
      "Iteration 305, loss = 0.09619773\n",
      "Iteration 312, loss = 0.10727836\n",
      "Iteration 306, loss = 0.09611204\n",
      "Iteration 83, loss = 0.23684010\n",
      "Iteration 189, loss = 0.14430014\n",
      "Iteration 313, loss = 0.10719519\n",
      "Iteration 250, loss = 0.12259991\n",
      "Iteration 148, loss = 0.17697042\n",
      "Iteration 251, loss = 0.12246085\n",
      "Iteration 307, loss = 0.09593045\n",
      "Iteration 190, loss = 0.14407035\n",
      "Iteration 308, loss = 0.09577917\n",
      "Iteration 68, loss = 0.30313007\n",
      "Iteration 252, loss = 0.12232554\n",
      "Iteration 149, loss = 0.17659633\n",
      "Iteration 309, loss = 0.09565147\n",
      "Iteration 314, loss = 0.10710150\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 69, loss = 0.30078182\n",
      "Iteration 191, loss = 0.14382666\n",
      "Iteration 1, loss = 1.07411600\n",
      "Iteration 2, loss = 1.00530064\n",
      "Iteration 3, loss = 0.91425174\n",
      "Iteration 4, loss = 0.82147517\n",
      "Iteration 5, loss = 0.73737214\n",
      "Iteration 6, loss = 0.66586080\n",
      "Iteration 7, loss = 0.60477139\n",
      "Iteration 8, loss = 0.55564776\n",
      "Iteration 9, loss = 0.51393460\n",
      "Iteration 10, loss = 0.48011434\n",
      "Iteration 11, loss = 0.45118087\n",
      "Iteration 12, loss = 0.42668645\n",
      "Iteration 70, loss = 0.29838452\n",
      "Iteration 30, loss = 0.46622941\n",
      "Iteration 13, loss = 0.40612954\n",
      "Iteration 84, loss = 0.23593338\n",
      "Iteration 310, loss = 0.09552476\n",
      "Iteration 14, loss = 0.38784766\n",
      "Iteration 15, loss = 0.37226354\n",
      "Iteration 16, loss = 0.35797824\n",
      "Iteration 17, loss = 0.34563777\n",
      "Iteration 18, loss = 0.33425915\n",
      "Iteration 19, loss = 0.32402926\n",
      "Iteration 20, loss = 0.31460627\n",
      "Iteration 192, loss = 0.14360788\n",
      "Iteration 150, loss = 0.17628945\n",
      "Iteration 253, loss = 0.12219700\n",
      "Iteration 311, loss = 0.09542832\n",
      "Iteration 312, loss = 0.09531215\n",
      "Iteration 313, loss = 0.09514436\n",
      "Iteration 21, loss = 0.30611680\n",
      "Iteration 71, loss = 0.29601384\n",
      "Iteration 22, loss = 0.29808970\n",
      "Iteration 193, loss = 0.14336139\n",
      "Iteration 23, loss = 0.29082274\n",
      "Iteration 24, loss = 0.28399175\n",
      "Iteration 25, loss = 0.27767071\n",
      "Iteration 26, loss = 0.27182023\n",
      "Iteration 151, loss = 0.17595003\n",
      "Iteration 27, loss = 0.26625632\n",
      "Iteration 28, loss = 0.26100550\n",
      "Iteration 29, loss = 0.25624752\n",
      "Iteration 30, loss = 0.25161092\n",
      "Iteration 31, loss = 0.24723971\n",
      "Iteration 32, loss = 0.24321680\n",
      "Iteration 33, loss = 0.23927016\n",
      "Iteration 314, loss = 0.09500423\n",
      "Iteration 31, loss = 0.45992814\n",
      "Iteration 254, loss = 0.12205611\n",
      "Iteration 194, loss = 0.14311974Iteration 85, loss = 0.23507266\n",
      "Iteration 72, loss = 0.29380003\n",
      "\n",
      "Iteration 34, loss = 0.23557274\n",
      "Iteration 315, loss = 0.09487697\n",
      "Iteration 35, loss = 0.23208088\n",
      "Iteration 36, loss = 0.22874904\n",
      "Iteration 316, loss = 0.09478152\n",
      "Iteration 37, loss = 0.22557152\n",
      "Iteration 38, loss = 0.22256270\n",
      "Iteration 39, loss = 0.21966594\n",
      "Iteration 40, loss = 0.21687347\n",
      "Iteration 41, loss = 0.21424045\n",
      "Iteration 317, loss = 0.09463926\n",
      "Iteration 42, loss = 0.21168307\n",
      "Iteration 73, loss = 0.29150638\n",
      "Iteration 195, loss = 0.14290305\n",
      "Iteration 255, loss = 0.12192694\n",
      "Iteration 43, loss = 0.20920920\n",
      "Iteration 152, loss = 0.17558109\n",
      "Iteration 318, loss = 0.09447271\n",
      "Iteration 44, loss = 0.20691073\n",
      "Iteration 86, loss = 0.23421229\n",
      "Iteration 45, loss = 0.20458679\n",
      "Iteration 319, loss = 0.09436417\n",
      "Iteration 46, loss = 0.20243829\n",
      "Iteration 256, loss = 0.12180050\n",
      "Iteration 47, loss = 0.20030374\n",
      "Iteration 48, loss = 0.19827577\n",
      "Iteration 320, loss = 0.09424671\n",
      "Iteration 49, loss = 0.19636078\n",
      "Iteration 32, loss = 0.45370686\n",
      "Iteration 74, loss = 0.28918840\n",
      "Iteration 50, loss = 0.19443308\n",
      "Iteration 51, loss = 0.19261290\n",
      "Iteration 52, loss = 0.19084125\n",
      "Iteration 196, loss = 0.14265674\n",
      "Iteration 53, loss = 0.18912219\n",
      "Iteration 54, loss = 0.18745866\n",
      "Iteration 55, loss = 0.18582654\n",
      "Iteration 56, loss = 0.18427467\n",
      "Iteration 57, loss = 0.18273798\n",
      "Iteration 153, loss = 0.17527444\n",
      "Iteration 58, loss = 0.18126213\n",
      "Iteration 59, loss = 0.17981394\n",
      "Iteration 60, loss = 0.17842912\n",
      "Iteration 61, loss = 0.17707361\n",
      "Iteration 62, loss = 0.17572942\n",
      "Iteration 63, loss = 0.17447974\n",
      "Iteration 64, loss = 0.17320681\n",
      "Iteration 65, loss = 0.17197174\n",
      "Iteration 66, loss = 0.17079464\n",
      "Iteration 67, loss = 0.16962517\n",
      "Iteration 68, loss = 0.16849680\n",
      "Iteration 69, loss = 0.16738800\n",
      "Iteration 321, loss = 0.09414773\n",
      "Iteration 75, loss = 0.28707253\n",
      "Iteration 70, loss = 0.16630259\n",
      "Iteration 257, loss = 0.12167413\n",
      "Iteration 71, loss = 0.16524560\n",
      "Iteration 72, loss = 0.16422905\n",
      "Iteration 73, loss = 0.16321472\n",
      "Iteration 74, loss = 0.16219844\n",
      "Iteration 75, loss = 0.16124071\n",
      "Iteration 76, loss = 0.16028425\n",
      "Iteration 322, loss = 0.09397524\n",
      "Iteration 77, loss = 0.15934599\n",
      "Iteration 78, loss = 0.15845149\n",
      "Iteration 79, loss = 0.15755538\n",
      "Iteration 80, loss = 0.15667768\n",
      "Iteration 81, loss = 0.15583903\n",
      "Iteration 82, loss = 0.15499326\n",
      "Iteration 154, loss = 0.17494741\n",
      "Iteration 197, loss = 0.14243884\n",
      "Iteration 83, loss = 0.15415386\n",
      "Iteration 87, loss = 0.23343654\n",
      "Iteration 258, loss = 0.12152737\n",
      "Iteration 323, loss = 0.09386356\n",
      "Iteration 324, loss = 0.09377310\n",
      "Iteration 84, loss = 0.15334669\n",
      "Iteration 85, loss = 0.15257560\n",
      "Iteration 86, loss = 0.15179584\n",
      "Iteration 87, loss = 0.15101974\n",
      "Iteration 88, loss = 0.15028901Iteration 198, loss = 0.14225748\n",
      "Iteration 325, loss = 0.09363526\n",
      "Iteration 76, loss = 0.28481041\n",
      "\n",
      "Iteration 89, loss = 0.14953857\n",
      "Iteration 90, loss = 0.14882785\n",
      "Iteration 33, loss = 0.44742004\n",
      "Iteration 91, loss = 0.14810996\n",
      "Iteration 92, loss = 0.14740293\n",
      "Iteration 259, loss = 0.12139815\n",
      "Iteration 93, loss = 0.14671423\n",
      "Iteration 94, loss = 0.14604054\n",
      "Iteration 95, loss = 0.14537871\n",
      "Iteration 96, loss = 0.14472739\n",
      "Iteration 97, loss = 0.14408503\n",
      "Iteration 98, loss = 0.14344634\n",
      "Iteration 99, loss = 0.14282131\n",
      "Iteration 100, loss = 0.14220535\n",
      "Iteration 326, loss = 0.09350614\n",
      "Iteration 101, loss = 0.14158888\n",
      "Iteration 102, loss = 0.14099349\n",
      "Iteration 103, loss = 0.14040535\n",
      "Iteration 155, loss = 0.17461591\n",
      "Iteration 104, loss = 0.13982661\n",
      "Iteration 105, loss = 0.13925232\n",
      "Iteration 106, loss = 0.13868445\n",
      "Iteration 107, loss = 0.13813011\n",
      "Iteration 199, loss = 0.14204346\n",
      "Iteration 108, loss = 0.13757336\n",
      "Iteration 109, loss = 0.13702820\n",
      "Iteration 110, loss = 0.13649777\n",
      "Iteration 111, loss = 0.13597392\n",
      "Iteration 88, loss = 0.23258276\n",
      "Iteration 112, loss = 0.13544146\n",
      "Iteration 113, loss = 0.13492240\n",
      "Iteration 114, loss = 0.13443587\n",
      "Iteration 115, loss = 0.13391608\n",
      "Iteration 116, loss = 0.13342645\n",
      "Iteration 327, loss = 0.09339687\n",
      "Iteration 117, loss = 0.13294120\n",
      "Iteration 118, loss = 0.13246483\n",
      "Iteration 119, loss = 0.13198309\n",
      "Iteration 120, loss = 0.13151853\n",
      "Iteration 121, loss = 0.13104484\n",
      "Iteration 77, loss = 0.28265646\n",
      "Iteration 122, loss = 0.13058872\n",
      "Iteration 260, loss = 0.12128093\n",
      "Iteration 123, loss = 0.13014256\n",
      "Iteration 200, loss = 0.14178082\n",
      "Iteration 124, loss = 0.12969595\n",
      "Iteration 328, loss = 0.09329742\n",
      "Iteration 125, loss = 0.12924687\n",
      "Iteration 126, loss = 0.12882141\n",
      "Iteration 329, loss = 0.09318772\n",
      "Iteration 156, loss = 0.17429462\n",
      "Iteration 330, loss = 0.09307030Iteration 127, loss = 0.12839926\n",
      "Iteration 128, loss = 0.12795532\n",
      "Iteration 129, loss = 0.12754295\n",
      "Iteration 130, loss = 0.12711209\n",
      "Iteration 131, loss = 0.12671375\n",
      "Iteration 132, loss = 0.12629860\n",
      "Iteration 133, loss = 0.12589566\n",
      "\n",
      "Iteration 201, loss = 0.14156897\n",
      "Iteration 134, loss = 0.12548515\n",
      "Iteration 135, loss = 0.12508739\n",
      "Iteration 136, loss = 0.12469798\n",
      "Iteration 137, loss = 0.12430532\n",
      "Iteration 34, loss = 0.44131614\n",
      "Iteration 78, loss = 0.28045936\n",
      "Iteration 138, loss = 0.12393086\n",
      "Iteration 139, loss = 0.12352911\n",
      "Iteration 331, loss = 0.09297143\n",
      "Iteration 140, loss = 0.12317125\n",
      "Iteration 141, loss = 0.12278309\n",
      "Iteration 142, loss = 0.12241213\n",
      "Iteration 143, loss = 0.12204970\n",
      "Iteration 202, loss = 0.14139049\n",
      "Iteration 144, loss = 0.12168980\n",
      "Iteration 145, loss = 0.12133809\n",
      "Iteration 146, loss = 0.12096481\n",
      "Iteration 332, loss = 0.09283530\n",
      "Iteration 147, loss = 0.12060275\n",
      "Iteration 148, loss = 0.12025670\n",
      "Iteration 261, loss = 0.12114520\n",
      "Iteration 157, loss = 0.17401557\n",
      "Iteration 89, loss = 0.23185100\n",
      "Iteration 333, loss = 0.09273015\n",
      "Iteration 79, loss = 0.27832646\n",
      "Iteration 262, loss = 0.12102517\n",
      "Iteration 149, loss = 0.11990565\n",
      "Iteration 150, loss = 0.11956113\n",
      "Iteration 151, loss = 0.11921786\n",
      "Iteration 334, loss = 0.09264420\n",
      "Iteration 203, loss = 0.14114229\n",
      "Iteration 152, loss = 0.11887253\n",
      "Iteration 263, loss = 0.12089086\n",
      "Iteration 153, loss = 0.11854874\n",
      "Iteration 154, loss = 0.11819864\n",
      "Iteration 155, loss = 0.11787553\n",
      "Iteration 204, loss = 0.14094011\n",
      "Iteration 158, loss = 0.17368049\n",
      "Iteration 156, loss = 0.11754552\n",
      "Iteration 157, loss = 0.11722450\n",
      "Iteration 158, loss = 0.11689610\n",
      "Iteration 335, loss = 0.09251680\n",
      "Iteration 159, loss = 0.11657980\n",
      "Iteration 35, loss = 0.43509930\n",
      "Iteration 160, loss = 0.11626477\n",
      "Iteration 161, loss = 0.11596027\n",
      "Iteration 162, loss = 0.11564395\n",
      "Iteration 163, loss = 0.11533118\n",
      "Iteration 90, loss = 0.23107628\n",
      "Iteration 164, loss = 0.11503190\n",
      "Iteration 80, loss = 0.27622547\n",
      "Iteration 336, loss = 0.09243819\n",
      "Iteration 165, loss = 0.11472281\n",
      "Iteration 166, loss = 0.11442797\n",
      "Iteration 167, loss = 0.11412652\n",
      "Iteration 168, loss = 0.11383411\n",
      "Iteration 169, loss = 0.11354974\n",
      "Iteration 170, loss = 0.11324822\n",
      "Iteration 171, loss = 0.11295789\n",
      "Iteration 172, loss = 0.11267896\n",
      "Iteration 264, loss = 0.12075577\n",
      "Iteration 173, loss = 0.11239365\n",
      "Iteration 205, loss = 0.14074608\n",
      "Iteration 174, loss = 0.11211824\n",
      "Iteration 175, loss = 0.11183698\n",
      "Iteration 337, loss = 0.09233486\n",
      "Iteration 81, loss = 0.27414092\n",
      "Iteration 176, loss = 0.11155957\n",
      "Iteration 177, loss = 0.11128352\n",
      "Iteration 159, loss = 0.17337127\n",
      "Iteration 338, loss = 0.09223697\n",
      "Iteration 91, loss = 0.23035803\n",
      "Iteration 339, loss = 0.09212399\n",
      "Iteration 265, loss = 0.12064324\n",
      "Iteration 36, loss = 0.42934263\n",
      "Iteration 178, loss = 0.11102204\n",
      "Iteration 179, loss = 0.11074170\n",
      "Iteration 180, loss = 0.11047701\n",
      "Iteration 181, loss = 0.11020016\n",
      "Iteration 182, loss = 0.10994701\n",
      "Iteration 183, loss = 0.10968001\n",
      "Iteration 206, loss = 0.14053574\n",
      "Iteration 184, loss = 0.10941372\n",
      "Iteration 340, loss = 0.09200890\n",
      "Iteration 185, loss = 0.10916406\n",
      "Iteration 82, loss = 0.27202412\n",
      "Iteration 341, loss = 0.09191090\n",
      "Iteration 186, loss = 0.10890081\n",
      "Iteration 187, loss = 0.10863971\n",
      "Iteration 188, loss = 0.10838398\n",
      "Iteration 189, loss = 0.10813197\n",
      "Iteration 190, loss = 0.10787620\n",
      "Iteration 191, loss = 0.10762962\n",
      "Iteration 160, loss = 0.17306275\n",
      "Iteration 192, loss = 0.10738178\n",
      "Iteration 193, loss = 0.10712722\n",
      "Iteration 266, loss = 0.12051205\n",
      "Iteration 342, loss = 0.09180613\n",
      "Iteration 194, loss = 0.10688246\n",
      "Iteration 343, loss = 0.09169787\n",
      "Iteration 207, loss = 0.14033973\n",
      "Iteration 195, loss = 0.10663214\n",
      "Iteration 267, loss = 0.12039643\n",
      "Iteration 196, loss = 0.10639333\n",
      "Iteration 197, loss = 0.10615474\n",
      "Iteration 198, loss = 0.10590657\n",
      "Iteration 199, loss = 0.10567278\n",
      "Iteration 200, loss = 0.10542992\n",
      "Iteration 92, loss = 0.22965803\n",
      "Iteration 201, loss = 0.10519798\n",
      "Iteration 202, loss = 0.10495482\n",
      "Iteration 203, loss = 0.10471869\n",
      "Iteration 204, loss = 0.10448219\n",
      "Iteration 161, loss = 0.17277786\n",
      "Iteration 208, loss = 0.14013434\n",
      "Iteration 344, loss = 0.09162711\n",
      "Iteration 345, loss = 0.09153570\n",
      "Iteration 83, loss = 0.27006434\n",
      "Iteration 37, loss = 0.42327028\n",
      "Iteration 205, loss = 0.10425870\n",
      "Iteration 206, loss = 0.10402435\n",
      "Iteration 207, loss = 0.10379079\n",
      "Iteration 208, loss = 0.10356391\n",
      "Iteration 346, loss = 0.09143914\n",
      "Iteration 209, loss = 0.10333799\n",
      "Iteration 210, loss = 0.10311321\n",
      "Iteration 268, loss = 0.12025568\n",
      "Iteration 211, loss = 0.10288605\n",
      "Iteration 212, loss = 0.10265922\n",
      "Iteration 213, loss = 0.10243891\n",
      "Iteration 214, loss = 0.10221106\n",
      "Iteration 215, loss = 0.10198860\n",
      "Iteration 216, loss = 0.10178197\n",
      "Iteration 217, loss = 0.10154398\n",
      "Iteration 218, loss = 0.10132832\n",
      "Iteration 84, loss = 0.26788841\n",
      "Iteration 347, loss = 0.09133705\n",
      "Iteration 219, loss = 0.10111780\n",
      "Iteration 209, loss = 0.14001209\n",
      "Iteration 93, loss = 0.22885502\n",
      "Iteration 162, loss = 0.17246863\n",
      "Iteration 220, loss = 0.10089752\n",
      "Iteration 221, loss = 0.10068751\n",
      "Iteration 222, loss = 0.10047567\n",
      "Iteration 223, loss = 0.10026461\n",
      "Iteration 348, loss = 0.09120877\n",
      "Iteration 269, loss = 0.12014293\n",
      "Iteration 224, loss = 0.10005191\n",
      "Iteration 225, loss = 0.09983734\n",
      "Iteration 226, loss = 0.09962923\n",
      "Iteration 227, loss = 0.09941995\n",
      "Iteration 228, loss = 0.09921576\n",
      "Iteration 229, loss = 0.09900877\n",
      "Iteration 210, loss = 0.13974501\n",
      "Iteration 230, loss = 0.09880278\n",
      "Iteration 85, loss = 0.26593569\n",
      "Iteration 231, loss = 0.09860803\n",
      "Iteration 232, loss = 0.09838904\n",
      "Iteration 349, loss = 0.09112953\n",
      "Iteration 233, loss = 0.09819414\n",
      "Iteration 234, loss = 0.09798828\n",
      "Iteration 235, loss = 0.09778690\n",
      "Iteration 236, loss = 0.09758847\n",
      "Iteration 350, loss = 0.09104180\n",
      "Iteration 237, loss = 0.09739398\n",
      "Iteration 238, loss = 0.09718493\n",
      "Iteration 38, loss = 0.41731878\n",
      "Iteration 351, loss = 0.09095833\n",
      "Iteration 239, loss = 0.09699813\n",
      "Iteration 240, loss = 0.09679715\n",
      "Iteration 211, loss = 0.13958796\n",
      "Iteration 241, loss = 0.09660308\n",
      "Iteration 242, loss = 0.09641690\n",
      "Iteration 243, loss = 0.09622142\n",
      "Iteration 244, loss = 0.09602152\n",
      "Iteration 245, loss = 0.09583454\n",
      "Iteration 352, loss = 0.09084953\n",
      "Iteration 246, loss = 0.09564057\n",
      "Iteration 270, loss = 0.12002838\n",
      "Iteration 94, loss = 0.22819079\n",
      "Iteration 86, loss = 0.26390007\n",
      "Iteration 271, loss = 0.11990267\n",
      "Iteration 247, loss = 0.09545041\n",
      "Iteration 248, loss = 0.09526302\n",
      "Iteration 249, loss = 0.09507548\n",
      "Iteration 163, loss = 0.17220183\n",
      "Iteration 250, loss = 0.09488616\n",
      "Iteration 251, loss = 0.09469891\n",
      "Iteration 252, loss = 0.09451536\n",
      "Iteration 253, loss = 0.09432892\n",
      "Iteration 254, loss = 0.09414681\n",
      "Iteration 255, loss = 0.09396982\n",
      "Iteration 256, loss = 0.09378507\n",
      "Iteration 257, loss = 0.09361059\n",
      "Iteration 212, loss = 0.13937487\n",
      "Iteration 258, loss = 0.09342760\n",
      "Iteration 353, loss = 0.09078883\n",
      "Iteration 259, loss = 0.09324255\n",
      "Iteration 260, loss = 0.09307230\n",
      "Iteration 261, loss = 0.09289255\n",
      "Iteration 262, loss = 0.09271681\n",
      "Iteration 263, loss = 0.09254731\n",
      "Iteration 264, loss = 0.09237349\n",
      "Iteration 265, loss = 0.09219158\n",
      "Iteration 266, loss = 0.09202662\n",
      "Iteration 267, loss = 0.09184921\n",
      "Iteration 268, loss = 0.09168489\n",
      "Iteration 269, loss = 0.09151373\n",
      "Iteration 95, loss = 0.22747670\n",
      "Iteration 270, loss = 0.09134024\n",
      "Iteration 271, loss = 0.09117927\n",
      "Iteration 354, loss = 0.09067079\n",
      "Iteration 272, loss = 0.09101383\n",
      "Iteration 273, loss = 0.09084080\n",
      "Iteration 164, loss = 0.17189393\n",
      "Iteration 272, loss = 0.11979225\n",
      "Iteration 87, loss = 0.26195338\n",
      "Iteration 274, loss = 0.09067626\n",
      "Iteration 213, loss = 0.13917636\n",
      "Iteration 355, loss = 0.09060204\n",
      "Iteration 39, loss = 0.41157426\n",
      "Iteration 273, loss = 0.11967555\n",
      "Iteration 275, loss = 0.09051444\n",
      "Iteration 214, loss = 0.13896035\n",
      "Iteration 276, loss = 0.09033938\n",
      "Iteration 356, loss = 0.09049917\n",
      "Iteration 277, loss = 0.09017406\n",
      "Iteration 278, loss = 0.09001621\n",
      "Iteration 357, loss = 0.09041283\n",
      "Iteration 279, loss = 0.08985099\n",
      "Iteration 280, loss = 0.08968830\n",
      "Iteration 281, loss = 0.08952344\n",
      "Iteration 282, loss = 0.08936528\n",
      "Iteration 274, loss = 0.11955106\n",
      "Iteration 283, loss = 0.08920286\n",
      "Iteration 284, loss = 0.08904268\n",
      "Iteration 285, loss = 0.08888600\n",
      "Iteration 286, loss = 0.08874121\n",
      "Iteration 287, loss = 0.08856709\n",
      "Iteration 288, loss = 0.08841578\n",
      "Iteration 289, loss = 0.08825661\n",
      "Iteration 290, loss = 0.08810279\n",
      "Iteration 291, loss = 0.08794145\n",
      "Iteration 292, loss = 0.08778663\n",
      "Iteration 358, loss = 0.09029424\n",
      "Iteration 88, loss = 0.25996500\n",
      "Iteration 165, loss = 0.17158662\n",
      "Iteration 96, loss = 0.22677884\n",
      "Iteration 293, loss = 0.08763299\n",
      "Iteration 294, loss = 0.08747479\n",
      "Iteration 295, loss = 0.08732663\n",
      "Iteration 296, loss = 0.08717108\n",
      "Iteration 275, loss = 0.11943828\n",
      "Iteration 89, loss = 0.25798514\n",
      "Iteration 297, loss = 0.08702282\n",
      "Iteration 359, loss = 0.09022302\n",
      "Iteration 215, loss = 0.13877564\n",
      "Iteration 298, loss = 0.08686753\n",
      "Iteration 299, loss = 0.08672194\n",
      "Iteration 300, loss = 0.08656810\n",
      "Iteration 301, loss = 0.08642001\n",
      "Iteration 302, loss = 0.08627386\n",
      "Iteration 360, loss = 0.09012871\n",
      "Iteration 303, loss = 0.08612208\n",
      "Iteration 304, loss = 0.08597760\n",
      "Iteration 40, loss = 0.40605476\n",
      "Iteration 305, loss = 0.08582898\n",
      "Iteration 306, loss = 0.08567883\n",
      "Iteration 307, loss = 0.08553470\n",
      "Iteration 308, loss = 0.08538490\n",
      "Iteration 216, loss = 0.13864878\n",
      "Iteration 90, loss = 0.25603171\n",
      "Iteration 97, loss = 0.22613543\n",
      "Iteration 309, loss = 0.08524185\n",
      "Iteration 166, loss = 0.17130655\n",
      "Iteration 276, loss = 0.11932707\n",
      "Iteration 361, loss = 0.09006369\n",
      "Iteration 310, loss = 0.08509345\n",
      "Iteration 362, loss = 0.08995169\n",
      "Iteration 311, loss = 0.08495126\n",
      "Iteration 312, loss = 0.08480626\n",
      "Iteration 313, loss = 0.08466850\n",
      "Iteration 314, loss = 0.08452086\n",
      "Iteration 315, loss = 0.08437396\n",
      "Iteration 316, loss = 0.08423456\n",
      "Iteration 217, loss = 0.13842029\n",
      "Iteration 317, loss = 0.08409000\n",
      "Iteration 363, loss = 0.08987096\n",
      "Iteration 318, loss = 0.08394827\n",
      "Iteration 277, loss = 0.11920814\n",
      "Iteration 319, loss = 0.08380655\n",
      "Iteration 320, loss = 0.08367057\n",
      "Iteration 321, loss = 0.08353143\n",
      "Iteration 322, loss = 0.08339816\n",
      "Iteration 323, loss = 0.08325094\n",
      "Iteration 324, loss = 0.08311329\n",
      "Iteration 325, loss = 0.08297177\n",
      "Iteration 326, loss = 0.08283940\n",
      "Iteration 364, loss = 0.08980001\n",
      "Iteration 98, loss = 0.22548271\n",
      "Iteration 167, loss = 0.17103233\n",
      "Iteration 327, loss = 0.08269793\n",
      "Iteration 365, loss = 0.08972035\n",
      "Iteration 328, loss = 0.08256460\n",
      "Iteration 91, loss = 0.25417739\n",
      "Iteration 329, loss = 0.08243338\n",
      "Iteration 330, loss = 0.08229036\n",
      "Iteration 218, loss = 0.13824964\n",
      "Iteration 331, loss = 0.08215440\n",
      "Iteration 41, loss = 0.40046157\n",
      "Iteration 278, loss = 0.11909426\n",
      "Iteration 332, loss = 0.08202056\n",
      "Iteration 333, loss = 0.08188568\n",
      "Iteration 334, loss = 0.08175287\n",
      "Iteration 168, loss = 0.17072640\n",
      "Iteration 366, loss = 0.08961293\n",
      "Iteration 335, loss = 0.08162176\n",
      "Iteration 336, loss = 0.08148694\n",
      "Iteration 337, loss = 0.08135211\n",
      "Iteration 367, loss = 0.08953716\n",
      "Iteration 338, loss = 0.08122400\n",
      "Iteration 92, loss = 0.25226671\n",
      "Iteration 339, loss = 0.08109118\n",
      "Iteration 219, loss = 0.13804351\n",
      "Iteration 340, loss = 0.08096098\n",
      "Iteration 341, loss = 0.08083265\n",
      "Iteration 342, loss = 0.08071387\n",
      "Iteration 343, loss = 0.08057534\n",
      "Iteration 344, loss = 0.08044378\n",
      "Iteration 345, loss = 0.08032192\n",
      "Iteration 279, loss = 0.11898237\n",
      "Iteration 368, loss = 0.08946271\n",
      "Iteration 369, loss = 0.08939395\n",
      "Iteration 346, loss = 0.08019094\n",
      "Iteration 220, loss = 0.13788166\n",
      "Iteration 347, loss = 0.08006879\n",
      "Iteration 370, loss = 0.08929573\n",
      "Iteration 348, loss = 0.07994358\n",
      "Iteration 349, loss = 0.07981442\n",
      "Iteration 350, loss = 0.07968545\n",
      "Iteration 221, loss = 0.13773530\n",
      "Iteration 351, loss = 0.07956068\n",
      "Iteration 93, loss = 0.25046780\n",
      "Iteration 352, loss = 0.07943587\n",
      "Iteration 353, loss = 0.07931190\n",
      "Iteration 280, loss = 0.11886079\n",
      "Iteration 354, loss = 0.07919123\n",
      "Iteration 355, loss = 0.07907310\n",
      "Iteration 356, loss = 0.07894614\n",
      "Iteration 371, loss = 0.08925332\n",
      "Iteration 357, loss = 0.07882227\n",
      "Iteration 99, loss = 0.22486256\n",
      "Iteration 358, loss = 0.07870179\n",
      "Iteration 359, loss = 0.07858341\n",
      "Iteration 169, loss = 0.17045092\n",
      "Iteration 360, loss = 0.07846393\n",
      "Iteration 361, loss = 0.07834253\n",
      "Iteration 362, loss = 0.07822607\n",
      "Iteration 363, loss = 0.07810276\n",
      "Iteration 364, loss = 0.07799042\n",
      "Iteration 365, loss = 0.07786854\n",
      "Iteration 366, loss = 0.07774610\n",
      "Iteration 367, loss = 0.07762934\n",
      "Iteration 368, loss = 0.07750837\n",
      "Iteration 42, loss = 0.39497044\n",
      "Iteration 94, loss = 0.24859914\n",
      "Iteration 372, loss = 0.08914310\n",
      "Iteration 222, loss = 0.13754416\n",
      "Iteration 373, loss = 0.08905065\n",
      "Iteration 374, loss = 0.08898083\n",
      "Iteration 223, loss = 0.13738017\n",
      "Iteration 281, loss = 0.11875672\n",
      "Iteration 375, loss = 0.08889965\n",
      "Iteration 170, loss = 0.17016631\n",
      "Iteration 369, loss = 0.07738731\n",
      "Iteration 370, loss = 0.07727376\n",
      "Iteration 371, loss = 0.07715192\n",
      "Iteration 372, loss = 0.07704132\n",
      "Iteration 171, loss = 0.16992665\n",
      "Iteration 373, loss = 0.07691860\n",
      "Iteration 100, loss = 0.22423406Iteration 374, loss = 0.07680201\n",
      "\n",
      "Iteration 95, loss = 0.24675634\n",
      "Iteration 375, loss = 0.07669018\n",
      "Iteration 376, loss = 0.07657887\n",
      "Iteration 377, loss = 0.07645676\n",
      "Iteration 282, loss = 0.11863958\n",
      "Iteration 378, loss = 0.07634644\n",
      "Iteration 376, loss = 0.08883883\n",
      "Iteration 379, loss = 0.07623666\n",
      "Iteration 380, loss = 0.07611757\n",
      "Iteration 381, loss = 0.07600569Iteration 224, loss = 0.13718167\n",
      "\n",
      "Iteration 382, loss = 0.07589057\n",
      "Iteration 96, loss = 0.24508561\n",
      "Iteration 383, loss = 0.07578598\n",
      "Iteration 377, loss = 0.08872233\n",
      "Iteration 384, loss = 0.07566849\n",
      "Iteration 385, loss = 0.07555883\n",
      "Iteration 386, loss = 0.07545341\n",
      "Iteration 387, loss = 0.07534178\n",
      "Iteration 388, loss = 0.07523243\n",
      "Iteration 389, loss = 0.07512263\n",
      "Iteration 390, loss = 0.07501831\n",
      "Iteration 283, loss = 0.11852365\n",
      "Iteration 391, loss = 0.07491076\n",
      "Iteration 392, loss = 0.07480098\n",
      "Iteration 393, loss = 0.07469347\n",
      "Iteration 394, loss = 0.07458382\n",
      "Iteration 225, loss = 0.13700047\n",
      "Iteration 395, loss = 0.07448136\n",
      "Iteration 396, loss = 0.07437576\n",
      "Iteration 397, loss = 0.07426671\n",
      "Iteration 378, loss = 0.08868858\n",
      "Iteration 43, loss = 0.38978556\n",
      "Iteration 379, loss = 0.08859786\n",
      "Iteration 398, loss = 0.07416613\n",
      "Iteration 284, loss = 0.11841511\n",
      "Iteration 399, loss = 0.07405595\n",
      "Iteration 380, loss = 0.08851428\n",
      "Iteration 400, loss = 0.07395004\n",
      "Iteration 401, loss = 0.07384822\n",
      "Iteration 97, loss = 0.24324295\n",
      "Iteration 402, loss = 0.07374200\n",
      "Iteration 172, loss = 0.16961915\n",
      "Iteration 101, loss = 0.22364599\n",
      "Iteration 381, loss = 0.08846244\n",
      "Iteration 403, loss = 0.07364337\n",
      "Iteration 404, loss = 0.07353782\n",
      "Iteration 405, loss = 0.07344255\n",
      "Iteration 406, loss = 0.07333814\n",
      "Iteration 407, loss = 0.07323521\n",
      "Iteration 382, loss = 0.08836174\n",
      "Iteration 408, loss = 0.07313541\n",
      "Iteration 285, loss = 0.11830398\n",
      "Iteration 226, loss = 0.13682954\n",
      "Iteration 409, loss = 0.07303873\n",
      "Iteration 410, loss = 0.07294065\n",
      "Iteration 411, loss = 0.07283792\n",
      "Iteration 173, loss = 0.16935750Iteration 98, loss = 0.24155739\n",
      "\n",
      "Iteration 412, loss = 0.07274030\n",
      "Iteration 413, loss = 0.07264210\n",
      "Iteration 414, loss = 0.07254511\n",
      "Iteration 415, loss = 0.07246093\n",
      "Iteration 383, loss = 0.08827755\n",
      "Iteration 227, loss = 0.13666900\n",
      "Iteration 384, loss = 0.08822708\n",
      "Iteration 416, loss = 0.07235959\n",
      "Iteration 286, loss = 0.11819509\n",
      "Iteration 417, loss = 0.07226480\n",
      "Iteration 418, loss = 0.07217382\n",
      "Iteration 419, loss = 0.07207924\n",
      "Iteration 420, loss = 0.07198613\n",
      "Iteration 102, loss = 0.22302892\n",
      "Iteration 421, loss = 0.07189448\n",
      "Iteration 422, loss = 0.07180445\n",
      "Iteration 423, loss = 0.07170794\n",
      "Iteration 424, loss = 0.07162074\n",
      "Iteration 425, loss = 0.07153381\n",
      "Iteration 385, loss = 0.08816357\n",
      "Iteration 174, loss = 0.16910061\n",
      "Iteration 386, loss = 0.08807600\n",
      "Iteration 426, loss = 0.07144011\n",
      "Iteration 44, loss = 0.38461320\n",
      "Iteration 99, loss = 0.23986550\n",
      "Iteration 427, loss = 0.07135277\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 287, loss = 0.11810548\n",
      "Iteration 1, loss = 0.69386060\n",
      "Iteration 2, loss = 0.66172610\n",
      "Iteration 228, loss = 0.13649413\n",
      "Iteration 3, loss = 0.61965168\n",
      "Iteration 4, loss = 0.57641425\n",
      "Iteration 5, loss = 0.53665478\n",
      "Iteration 6, loss = 0.50136125\n",
      "Iteration 7, loss = 0.47130594\n",
      "Iteration 387, loss = 0.08800614\n",
      "Iteration 8, loss = 0.44564990\n",
      "Iteration 9, loss = 0.42360613\n",
      "Iteration 10, loss = 0.40433790\n",
      "Iteration 11, loss = 0.38754270\n",
      "Iteration 12, loss = 0.37290316\n",
      "Iteration 100, loss = 0.23819381\n",
      "Iteration 13, loss = 0.35980508\n",
      "Iteration 229, loss = 0.13633116\n",
      "Iteration 14, loss = 0.34797165\n",
      "Iteration 15, loss = 0.33721067\n",
      "Iteration 16, loss = 0.32753653\n",
      "Iteration 17, loss = 0.31841837\n",
      "Iteration 18, loss = 0.31027617\n",
      "Iteration 19, loss = 0.30256207\n",
      "Iteration 20, loss = 0.29542043\n",
      "Iteration 21, loss = 0.28860372\n",
      "Iteration 103, loss = 0.22247029\n",
      "Iteration 101, loss = 0.23659009\n",
      "Iteration 230, loss = 0.13617624\n",
      "Iteration 175, loss = 0.16884297\n",
      "Iteration 288, loss = 0.11798150\n",
      "Iteration 388, loss = 0.08795632\n",
      "Iteration 22, loss = 0.28238623\n",
      "Iteration 23, loss = 0.27645117\n",
      "Iteration 24, loss = 0.27087058\n",
      "Iteration 389, loss = 0.08788960\n",
      "Iteration 25, loss = 0.26566175\n",
      "Iteration 26, loss = 0.26062816\n",
      "Iteration 289, loss = 0.11790767\n",
      "Iteration 27, loss = 0.25580052\n",
      "Iteration 28, loss = 0.25139700\n",
      "Iteration 29, loss = 0.24714686\n",
      "Iteration 30, loss = 0.24303535\n",
      "Iteration 31, loss = 0.23914225\n",
      "Iteration 390, loss = 0.08778088\n",
      "Iteration 45, loss = 0.37964165\n",
      "Iteration 32, loss = 0.23545511\n",
      "Iteration 391, loss = 0.08773023\n",
      "Iteration 231, loss = 0.13599940\n",
      "Iteration 290, loss = 0.11777673\n",
      "Iteration 392, loss = 0.08764799\n",
      "Iteration 102, loss = 0.23501232\n",
      "Iteration 33, loss = 0.23186751\n",
      "Iteration 34, loss = 0.22847073\n",
      "Iteration 35, loss = 0.22526968\n",
      "Iteration 36, loss = 0.22213528\n",
      "Iteration 104, loss = 0.22185685\n",
      "Iteration 291, loss = 0.11767577\n",
      "Iteration 176, loss = 0.16856950\n",
      "Iteration 232, loss = 0.13588411\n",
      "Iteration 292, loss = 0.11757272\n",
      "Iteration 393, loss = 0.08756618\n",
      "Iteration 37, loss = 0.21909305\n",
      "Iteration 38, loss = 0.21629436\n",
      "Iteration 39, loss = 0.21343426\n",
      "Iteration 105, loss = 0.22128379\n",
      "Iteration 103, loss = 0.23350051\n",
      "Iteration 40, loss = 0.21077670\n",
      "Iteration 41, loss = 0.20822650\n",
      "Iteration 42, loss = 0.20570850\n",
      "Iteration 43, loss = 0.20331987\n",
      "Iteration 44, loss = 0.20098039\n",
      "Iteration 45, loss = 0.19876538\n",
      "Iteration 46, loss = 0.19660591\n",
      "Iteration 47, loss = 0.19446016\n",
      "Iteration 394, loss = 0.08749708\n",
      "Iteration 293, loss = 0.11746681\n",
      "Iteration 48, loss = 0.19241081\n",
      "Iteration 49, loss = 0.19046886\n",
      "Iteration 104, loss = 0.23199887\n",
      "Iteration 46, loss = 0.37478206\n",
      "Iteration 50, loss = 0.18851476\n",
      "Iteration 233, loss = 0.13568699\n",
      "Iteration 177, loss = 0.16834878\n",
      "Iteration 395, loss = 0.08742899\n",
      "Iteration 51, loss = 0.18666209\n",
      "Iteration 52, loss = 0.18484330\n",
      "Iteration 53, loss = 0.18309075\n",
      "Iteration 54, loss = 0.18136897\n",
      "Iteration 55, loss = 0.17968703\n",
      "Iteration 396, loss = 0.08739272\n",
      "Iteration 56, loss = 0.17808738\n",
      "Iteration 105, loss = 0.23048635\n",
      "Iteration 397, loss = 0.08729918\n",
      "Iteration 57, loss = 0.17648027\n",
      "Iteration 234, loss = 0.13553765\n",
      "Iteration 58, loss = 0.17493013\n",
      "Iteration 178, loss = 0.16808118\n",
      "Iteration 398, loss = 0.08721858\n",
      "Iteration 294, loss = 0.11736079\n",
      "Iteration 235, loss = 0.13536383\n",
      "Iteration 59, loss = 0.17345106\n",
      "Iteration 60, loss = 0.17198019\n",
      "Iteration 61, loss = 0.17055468\n",
      "Iteration 62, loss = 0.16916670\n",
      "Iteration 63, loss = 0.16782778\n",
      "Iteration 106, loss = 0.22072237\n",
      "Iteration 64, loss = 0.16647722\n",
      "Iteration 399, loss = 0.08716128\n",
      "Iteration 65, loss = 0.16521416\n",
      "Iteration 66, loss = 0.16393291\n",
      "Iteration 400, loss = 0.08711936\n",
      "Iteration 401, loss = 0.08702668\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 236, loss = 0.13524128\n",
      "Iteration 67, loss = 0.16272679\n",
      "Iteration 106, loss = 0.22905351\n",
      "Iteration 179, loss = 0.16781817\n",
      "Iteration 295, loss = 0.11725556\n",
      "Iteration 68, loss = 0.16149822\n",
      "Iteration 69, loss = 0.16033591\n",
      "Iteration 70, loss = 0.15917571\n",
      "Iteration 237, loss = 0.13506057\n",
      "Iteration 1, loss = 0.70107942\n",
      "Iteration 2, loss = 0.66508923\n",
      "Iteration 3, loss = 0.61873470\n",
      "Iteration 71, loss = 0.15806745\n",
      "Iteration 47, loss = 0.36978953\n",
      "Iteration 72, loss = 0.15696519\n",
      "Iteration 73, loss = 0.15588708\n",
      "Iteration 74, loss = 0.15483505\n",
      "Iteration 238, loss = 0.13490767\n",
      "Iteration 4, loss = 0.57141395\n",
      "Iteration 75, loss = 0.15380398\n",
      "Iteration 76, loss = 0.15278994\n",
      "Iteration 77, loss = 0.15179576\n",
      "Iteration 107, loss = 0.22761047\n",
      "Iteration 78, loss = 0.15081991\n",
      "Iteration 296, loss = 0.11716811\n",
      "Iteration 79, loss = 0.14986004Iteration 107, loss = 0.22018527\n",
      "\n",
      "Iteration 80, loss = 0.14893771\n",
      "Iteration 5, loss = 0.52946323\n",
      "Iteration 297, loss = 0.11704995\n",
      "Iteration 81, loss = 0.14800799\n",
      "Iteration 108, loss = 0.22624961\n",
      "Iteration 6, loss = 0.49293611\n",
      "Iteration 7, loss = 0.46264826\n",
      "Iteration 8, loss = 0.43712581\n",
      "Iteration 9, loss = 0.41517483\n",
      "Iteration 10, loss = 0.39699694\n",
      "Iteration 180, loss = 0.16755268\n",
      "Iteration 11, loss = 0.38102488\n",
      "Iteration 82, loss = 0.14708211\n",
      "Iteration 83, loss = 0.14623260\n",
      "Iteration 84, loss = 0.14535047\n",
      "Iteration 85, loss = 0.14450980\n",
      "Iteration 86, loss = 0.14365277\n",
      "Iteration 87, loss = 0.14282226\n",
      "Iteration 109, loss = 0.22489307\n",
      "Iteration 88, loss = 0.14203136\n",
      "Iteration 89, loss = 0.14121529\n",
      "Iteration 90, loss = 0.14043738\n",
      "Iteration 12, loss = 0.36719748\n",
      "Iteration 13, loss = 0.35480350\n",
      "Iteration 14, loss = 0.34380702\n",
      "Iteration 91, loss = 0.13966383\n",
      "Iteration 15, loss = 0.33379500\n",
      "Iteration 239, loss = 0.13474308\n",
      "Iteration 16, loss = 0.32464615\n",
      "Iteration 17, loss = 0.31621092\n",
      "Iteration 18, loss = 0.30839137\n",
      "Iteration 298, loss = 0.11695585\n",
      "Iteration 19, loss = 0.30102870\n",
      "Iteration 92, loss = 0.13892365\n",
      "Iteration 108, loss = 0.21964051\n",
      "Iteration 20, loss = 0.29420648\n",
      "Iteration 93, loss = 0.13816947\n",
      "Iteration 94, loss = 0.13744130\n",
      "Iteration 95, loss = 0.13672602\n",
      "Iteration 96, loss = 0.13600740\n",
      "Iteration 97, loss = 0.13530464\n",
      "Iteration 98, loss = 0.13462075\n",
      "Iteration 48, loss = 0.36524971\n",
      "Iteration 99, loss = 0.13393495\n",
      "Iteration 100, loss = 0.13330017\n",
      "Iteration 101, loss = 0.13261071\n",
      "Iteration 102, loss = 0.13198193\n",
      "Iteration 181, loss = 0.16730047\n",
      "Iteration 103, loss = 0.13135063\n",
      "Iteration 240, loss = 0.13460714\n",
      "Iteration 104, loss = 0.13071737\n",
      "Iteration 21, loss = 0.28784179\n",
      "Iteration 22, loss = 0.28180152\n",
      "Iteration 105, loss = 0.13011998\n",
      "Iteration 110, loss = 0.22354794\n",
      "Iteration 23, loss = 0.27605531\n",
      "Iteration 299, loss = 0.11686364\n",
      "Iteration 24, loss = 0.27073527\n",
      "Iteration 241, loss = 0.13445964\n",
      "Iteration 25, loss = 0.26547982\n",
      "Iteration 182, loss = 0.16705829\n",
      "Iteration 26, loss = 0.26064185\n",
      "Iteration 27, loss = 0.25590956\n",
      "Iteration 28, loss = 0.25152341\n",
      "Iteration 106, loss = 0.12949131\n",
      "Iteration 29, loss = 0.24723204\n",
      "Iteration 107, loss = 0.12889382\n",
      "Iteration 108, loss = 0.12831583\n",
      "Iteration 109, loss = 0.12772299\n",
      "Iteration 110, loss = 0.12715101\n",
      "Iteration 109, loss = 0.21909059\n",
      "Iteration 30, loss = 0.24311758\n",
      "Iteration 111, loss = 0.12660383Iteration 300, loss = 0.11675936\n",
      "Iteration 31, loss = 0.23920325\n",
      "\n",
      "Iteration 32, loss = 0.23547592\n",
      "Iteration 111, loss = 0.22224356\n",
      "Iteration 33, loss = 0.23186592\n",
      "Iteration 34, loss = 0.22833072\n",
      "Iteration 301, loss = 0.11665083\n",
      "Iteration 242, loss = 0.13429699\n",
      "Iteration 183, loss = 0.16679753\n",
      "Iteration 112, loss = 0.12605212\n",
      "Iteration 35, loss = 0.22497656\n",
      "Iteration 49, loss = 0.36069962\n",
      "Iteration 113, loss = 0.12548716\n",
      "Iteration 36, loss = 0.22176535\n",
      "Iteration 114, loss = 0.12494291\n",
      "Iteration 37, loss = 0.21868910\n",
      "Iteration 115, loss = 0.12441424\n",
      "Iteration 116, loss = 0.12389686\n",
      "Iteration 117, loss = 0.12337115\n",
      "Iteration 118, loss = 0.12286487\n",
      "Iteration 119, loss = 0.12234352\n",
      "Iteration 112, loss = 0.22097265\n",
      "Iteration 120, loss = 0.12184882\n",
      "Iteration 38, loss = 0.21572997\n",
      "Iteration 243, loss = 0.13415242\n",
      "Iteration 121, loss = 0.12134204\n",
      "Iteration 184, loss = 0.16658950\n",
      "Iteration 122, loss = 0.12085928\n",
      "Iteration 39, loss = 0.21275785\n",
      "Iteration 40, loss = 0.20997159\n",
      "Iteration 123, loss = 0.12038975\n",
      "Iteration 41, loss = 0.20729346\n",
      "Iteration 124, loss = 0.11988636\n",
      "Iteration 244, loss = 0.13398998Iteration 125, loss = 0.11942522\n",
      "\n",
      "Iteration 126, loss = 0.11895342\n",
      "Iteration 42, loss = 0.20464322\n",
      "Iteration 127, loss = 0.11849832\n",
      "Iteration 110, loss = 0.21858602\n",
      "Iteration 113, loss = 0.21972165\n",
      "Iteration 50, loss = 0.35638397\n",
      "Iteration 128, loss = 0.11803735\n",
      "Iteration 302, loss = 0.11656708\n",
      "Iteration 43, loss = 0.20212021\n",
      "Iteration 44, loss = 0.19965053\n",
      "Iteration 129, loss = 0.11758696Iteration 45, loss = 0.19732931\n",
      "Iteration 46, loss = 0.19498945\n",
      "Iteration 185, loss = 0.16632277\n",
      "\n",
      "Iteration 47, loss = 0.19278263\n",
      "Iteration 130, loss = 0.11713323\n",
      "Iteration 131, loss = 0.11670422\n",
      "Iteration 48, loss = 0.19058039\n",
      "Iteration 132, loss = 0.11625612\n",
      "Iteration 133, loss = 0.11583774\n",
      "Iteration 134, loss = 0.11540161\n",
      "Iteration 245, loss = 0.13386441\n",
      "Iteration 49, loss = 0.18852275\n",
      "Iteration 50, loss = 0.18652006\n",
      "Iteration 51, loss = 0.18450376\n",
      "Iteration 52, loss = 0.18259947\n",
      "Iteration 135, loss = 0.11498041\n",
      "Iteration 136, loss = 0.11456755\n",
      "Iteration 53, loss = 0.18072429\n",
      "Iteration 137, loss = 0.11416882\n",
      "Iteration 138, loss = 0.11375588\n",
      "Iteration 139, loss = 0.11335518Iteration 303, loss = 0.11646166\n",
      "\n",
      "Iteration 54, loss = 0.17886057\n",
      "Iteration 55, loss = 0.17713094\n",
      "Iteration 114, loss = 0.21849477\n",
      "Iteration 140, loss = 0.11296149\n",
      "Iteration 141, loss = 0.11255387\n",
      "Iteration 246, loss = 0.13371834\n",
      "Iteration 56, loss = 0.17539210\n",
      "Iteration 111, loss = 0.21808295\n",
      "Iteration 57, loss = 0.17375454\n",
      "Iteration 58, loss = 0.17209867\n",
      "Iteration 59, loss = 0.17052225\n",
      "Iteration 142, loss = 0.11218222\n",
      "Iteration 60, loss = 0.16897738\n",
      "Iteration 304, loss = 0.11637338\n",
      "Iteration 186, loss = 0.16610297\n",
      "Iteration 305, loss = 0.11626503\n",
      "Iteration 51, loss = 0.35196729\n",
      "Iteration 143, loss = 0.11178432\n",
      "Iteration 247, loss = 0.13356517\n",
      "Iteration 144, loss = 0.11140993\n",
      "Iteration 145, loss = 0.11102446\n",
      "Iteration 146, loss = 0.11064679\n",
      "Iteration 147, loss = 0.11028361\n",
      "Iteration 306, loss = 0.11617538\n",
      "Iteration 148, loss = 0.10993429\n",
      "Iteration 149, loss = 0.10955697\n",
      "Iteration 112, loss = 0.21756499\n",
      "Iteration 150, loss = 0.10920230\n",
      "Iteration 187, loss = 0.16583906\n",
      "Iteration 151, loss = 0.10883508\n",
      "Iteration 61, loss = 0.16744302\n",
      "Iteration 152, loss = 0.10848625\n",
      "Iteration 153, loss = 0.10814249\n",
      "Iteration 154, loss = 0.10779160\n",
      "Iteration 155, loss = 0.10744947\n",
      "Iteration 62, loss = 0.16598953\n",
      "Iteration 63, loss = 0.16453340\n",
      "Iteration 115, loss = 0.21729428\n",
      "Iteration 156, loss = 0.10711548Iteration 64, loss = 0.16314889\n",
      "\n",
      "Iteration 65, loss = 0.16177197\n",
      "Iteration 66, loss = 0.16043080\n",
      "Iteration 157, loss = 0.10678371\n",
      "Iteration 67, loss = 0.15911807\n",
      "Iteration 248, loss = 0.13344088\n",
      "Iteration 68, loss = 0.15782448\n",
      "Iteration 158, loss = 0.10643839\n",
      "Iteration 159, loss = 0.10611269\n",
      "Iteration 188, loss = 0.16562519\n",
      "Iteration 69, loss = 0.15660145\n",
      "Iteration 160, loss = 0.10578539\n",
      "Iteration 70, loss = 0.15536557\n",
      "Iteration 71, loss = 0.15417227\n",
      "Iteration 72, loss = 0.15300475\n",
      "Iteration 73, loss = 0.15184883\n",
      "Iteration 161, loss = 0.10546683\n",
      "Iteration 162, loss = 0.10513719\n",
      "Iteration 307, loss = 0.11609158\n",
      "Iteration 52, loss = 0.34784647\n",
      "Iteration 74, loss = 0.15073342\n",
      "Iteration 249, loss = 0.13328096\n",
      "Iteration 163, loss = 0.10481648\n",
      "Iteration 164, loss = 0.10450072\n",
      "Iteration 165, loss = 0.10418511\n",
      "Iteration 166, loss = 0.10388886\n",
      "Iteration 167, loss = 0.10356584\n",
      "Iteration 116, loss = 0.21609372\n",
      "Iteration 168, loss = 0.10326594\n",
      "Iteration 169, loss = 0.10296435\n",
      "Iteration 170, loss = 0.10267058\n",
      "Iteration 171, loss = 0.10236160\n",
      "Iteration 172, loss = 0.10207102\n",
      "Iteration 189, loss = 0.16537039\n",
      "Iteration 113, loss = 0.21705303\n",
      "Iteration 75, loss = 0.14965249\n",
      "Iteration 76, loss = 0.14857517\n",
      "Iteration 77, loss = 0.14752843\n",
      "Iteration 117, loss = 0.21497314\n",
      "Iteration 78, loss = 0.14648228\n",
      "Iteration 173, loss = 0.10177688\n",
      "Iteration 79, loss = 0.14546031\n",
      "Iteration 80, loss = 0.14448743\n",
      "Iteration 174, loss = 0.10148184\n",
      "Iteration 175, loss = 0.10119060\n",
      "Iteration 176, loss = 0.10091405\n",
      "Iteration 177, loss = 0.10063445\n",
      "Iteration 250, loss = 0.13315853\n",
      "Iteration 308, loss = 0.11598744\n",
      "Iteration 178, loss = 0.10033834\n",
      "Iteration 179, loss = 0.10006893\n",
      "Iteration 81, loss = 0.14354147\n",
      "Iteration 180, loss = 0.09979516\n",
      "Iteration 181, loss = 0.09951599\n",
      "Iteration 82, loss = 0.14256303\n",
      "Iteration 83, loss = 0.14165179\n",
      "Iteration 182, loss = 0.09923986\n",
      "Iteration 183, loss = 0.09897078\n",
      "Iteration 251, loss = 0.13301392\n",
      "Iteration 190, loss = 0.16516544\n",
      "Iteration 309, loss = 0.11589916\n",
      "Iteration 114, loss = 0.21655314\n",
      "Iteration 53, loss = 0.34390377\n",
      "Iteration 118, loss = 0.21383149\n",
      "Iteration 84, loss = 0.14073641\n",
      "Iteration 85, loss = 0.13982815\n",
      "Iteration 86, loss = 0.13895741\n",
      "Iteration 119, loss = 0.21275400\n",
      "Iteration 87, loss = 0.13812965\n",
      "Iteration 88, loss = 0.13725059\n",
      "Iteration 310, loss = 0.11580873\n",
      "Iteration 184, loss = 0.09870878\n",
      "Iteration 252, loss = 0.13291284\n",
      "Iteration 185, loss = 0.09843293\n",
      "Iteration 186, loss = 0.09816963\n",
      "Iteration 187, loss = 0.09790968\n",
      "Iteration 188, loss = 0.09764393\n",
      "Iteration 189, loss = 0.09738526\n",
      "Iteration 89, loss = 0.13641529\n",
      "Iteration 120, loss = 0.21162536\n",
      "Iteration 190, loss = 0.09714199\n",
      "Iteration 90, loss = 0.13561842\n",
      "Iteration 191, loss = 0.09687791\n",
      "Iteration 192, loss = 0.09662801\n",
      "Iteration 91, loss = 0.13480013\n",
      "Iteration 193, loss = 0.09636954\n",
      "Iteration 194, loss = 0.09612589\n",
      "Iteration 253, loss = 0.13275402\n",
      "Iteration 191, loss = 0.16494856\n",
      "Iteration 92, loss = 0.13402623\n",
      "Iteration 195, loss = 0.09586993\n",
      "Iteration 93, loss = 0.13324460\n",
      "Iteration 94, loss = 0.13248870\n",
      "Iteration 95, loss = 0.13175736\n",
      "Iteration 96, loss = 0.13101646\n",
      "Iteration 97, loss = 0.13030129\n",
      "Iteration 98, loss = 0.12958924\n",
      "Iteration 99, loss = 0.12888806\n",
      "Iteration 196, loss = 0.09562720\n",
      "Iteration 311, loss = 0.11571853\n",
      "Iteration 197, loss = 0.09537898\n",
      "Iteration 198, loss = 0.09514275\n",
      "Iteration 199, loss = 0.09489988\n",
      "Iteration 100, loss = 0.12821011\n",
      "Iteration 101, loss = 0.12754817\n",
      "Iteration 115, loss = 0.21609441\n",
      "Iteration 200, loss = 0.09465367\n",
      "Iteration 102, loss = 0.12687918\n",
      "Iteration 54, loss = 0.34006412\n",
      "Iteration 103, loss = 0.12622514\n",
      "Iteration 201, loss = 0.09442201Iteration 312, loss = 0.11561345\n",
      "Iteration 254, loss = 0.13261766\n",
      "\n",
      "Iteration 202, loss = 0.09418574\n",
      "Iteration 203, loss = 0.09395646\n",
      "Iteration 204, loss = 0.09371907\n",
      "Iteration 205, loss = 0.09348626\n",
      "Iteration 206, loss = 0.09326032\n",
      "Iteration 207, loss = 0.09303992\n",
      "Iteration 104, loss = 0.12559520\n",
      "Iteration 121, loss = 0.21062961\n",
      "Iteration 208, loss = 0.09280953\n",
      "Iteration 209, loss = 0.09258539\n",
      "Iteration 210, loss = 0.09235879\n",
      "Iteration 211, loss = 0.09214303\n",
      "Iteration 105, loss = 0.12496031\n",
      "Iteration 192, loss = 0.16468597\n",
      "Iteration 212, loss = 0.09192115\n",
      "Iteration 122, loss = 0.20953258\n",
      "Iteration 106, loss = 0.12433312\n",
      "Iteration 107, loss = 0.12372316\n",
      "Iteration 213, loss = 0.09169841\n",
      "Iteration 255, loss = 0.13251028\n",
      "Iteration 108, loss = 0.12312995\n",
      "Iteration 109, loss = 0.12253830\n",
      "Iteration 110, loss = 0.12194409\n",
      "Iteration 111, loss = 0.12137646\n",
      "Iteration 313, loss = 0.11553384\n",
      "Iteration 256, loss = 0.13240495\n",
      "Iteration 116, loss = 0.21557734\n",
      "Iteration 214, loss = 0.09148462\n",
      "Iteration 112, loss = 0.12080437\n",
      "Iteration 55, loss = 0.33637947\n",
      "Iteration 215, loss = 0.09125712\n",
      "Iteration 216, loss = 0.09104836\n",
      "Iteration 217, loss = 0.09083542\n",
      "Iteration 218, loss = 0.09062842\n",
      "Iteration 219, loss = 0.09041237\n",
      "Iteration 220, loss = 0.09019493\n",
      "Iteration 193, loss = 0.16445530Iteration 221, loss = 0.09001404\n",
      "Iteration 113, loss = 0.12025630\n",
      "Iteration 222, loss = 0.08978389\n",
      "Iteration 223, loss = 0.08957995\n",
      "Iteration 114, loss = 0.11970461\n",
      "\n",
      "Iteration 224, loss = 0.08937534\n",
      "Iteration 225, loss = 0.08916894\n",
      "Iteration 226, loss = 0.08896399\n",
      "Iteration 227, loss = 0.08877247\n",
      "Iteration 123, loss = 0.20854856\n",
      "Iteration 228, loss = 0.08856825\n",
      "Iteration 115, loss = 0.11917037\n",
      "Iteration 229, loss = 0.08837148\n",
      "Iteration 116, loss = 0.11862464\n",
      "Iteration 314, loss = 0.11544526\n",
      "Iteration 230, loss = 0.08817109\n",
      "Iteration 117, loss = 0.11810825\n",
      "Iteration 231, loss = 0.08797552\n",
      "Iteration 232, loss = 0.08778936\n",
      "Iteration 118, loss = 0.11758719\n",
      "Iteration 233, loss = 0.08759756\n",
      "Iteration 119, loss = 0.11708177\n",
      "Iteration 117, loss = 0.21510980\n",
      "Iteration 257, loss = 0.13222204\n",
      "Iteration 234, loss = 0.08739616\n",
      "Iteration 120, loss = 0.11658819\n",
      "Iteration 121, loss = 0.11607169Iteration 235, loss = 0.08720806\n",
      "\n",
      "Iteration 236, loss = 0.08701633\n",
      "Iteration 122, loss = 0.11558546\n",
      "Iteration 123, loss = 0.11510812\n",
      "Iteration 124, loss = 0.11462364\n",
      "Iteration 194, loss = 0.16422747\n",
      "Iteration 237, loss = 0.08682935\n",
      "Iteration 125, loss = 0.11414738\n",
      "Iteration 126, loss = 0.11368524\n",
      "Iteration 238, loss = 0.08663779\n",
      "Iteration 127, loss = 0.11323814\n",
      "Iteration 128, loss = 0.11276735\n",
      "Iteration 315, loss = 0.11535773\n",
      "Iteration 129, loss = 0.11231762\n",
      "Iteration 130, loss = 0.11187228\n",
      "Iteration 131, loss = 0.11143775\n",
      "Iteration 132, loss = 0.11100621\n",
      "Iteration 239, loss = 0.08645356\n",
      "Iteration 133, loss = 0.11058474\n",
      "Iteration 134, loss = 0.11015431\n",
      "Iteration 118, loss = 0.21464798\n",
      "Iteration 135, loss = 0.10974638\n",
      "Iteration 240, loss = 0.08626592\n",
      "Iteration 136, loss = 0.10932687\n",
      "Iteration 241, loss = 0.08608581\n",
      "Iteration 258, loss = 0.13210562\n",
      "Iteration 242, loss = 0.08589782\n",
      "Iteration 243, loss = 0.08571464\n",
      "Iteration 244, loss = 0.08554221\n",
      "Iteration 124, loss = 0.20756157\n",
      "Iteration 56, loss = 0.33277667\n",
      "Iteration 245, loss = 0.08536235\n",
      "Iteration 316, loss = 0.11527158\n",
      "Iteration 246, loss = 0.08518842\n",
      "Iteration 247, loss = 0.08500729\n",
      "Iteration 137, loss = 0.10891933\n",
      "Iteration 248, loss = 0.08483223\n",
      "Iteration 195, loss = 0.16401541\n",
      "Iteration 249, loss = 0.08465468\n",
      "Iteration 317, loss = 0.11517039\n",
      "Iteration 138, loss = 0.10851335\n",
      "Iteration 139, loss = 0.10812005\n",
      "Iteration 250, loss = 0.08448906\n",
      "Iteration 140, loss = 0.10772856\n",
      "Iteration 251, loss = 0.08431235\n",
      "Iteration 125, loss = 0.20656049\n",
      "Iteration 141, loss = 0.10733844\n",
      "Iteration 252, loss = 0.08413967\n",
      "Iteration 142, loss = 0.10694684\n",
      "Iteration 253, loss = 0.08397015\n",
      "Iteration 143, loss = 0.10658098\n",
      "Iteration 144, loss = 0.10619363\n",
      "Iteration 254, loss = 0.08380211\n",
      "Iteration 259, loss = 0.13201713\n",
      "Iteration 255, loss = 0.08363457\n",
      "Iteration 256, loss = 0.08346295\n",
      "Iteration 257, loss = 0.08330079\n",
      "Iteration 258, loss = 0.08313363\n",
      "Iteration 259, loss = 0.08296659\n",
      "Iteration 260, loss = 0.08281037\n",
      "Iteration 261, loss = 0.08264543\n",
      "Iteration 262, loss = 0.08248587Iteration 145, loss = 0.10583252\n",
      "Iteration 146, loss = 0.10545929\n",
      "Iteration 318, loss = 0.11508435\n",
      "Iteration 147, loss = 0.10509395\n",
      "Iteration 260, loss = 0.13186477\n",
      "Iteration 126, loss = 0.20559259\n",
      "Iteration 196, loss = 0.16378643\n",
      "Iteration 148, loss = 0.10473663\n",
      "\n",
      "Iteration 119, loss = 0.21415860\n",
      "Iteration 149, loss = 0.10438348\n",
      "Iteration 150, loss = 0.10403465\n",
      "Iteration 319, loss = 0.11499935\n",
      "Iteration 57, loss = 0.32951850\n",
      "Iteration 151, loss = 0.10368674\n",
      "Iteration 263, loss = 0.08232490\n",
      "Iteration 264, loss = 0.08216385\n",
      "Iteration 265, loss = 0.08200901\n",
      "Iteration 266, loss = 0.08185052\n",
      "Iteration 197, loss = 0.16357719\n",
      "Iteration 267, loss = 0.08169563\n",
      "Iteration 127, loss = 0.20467619\n",
      "Iteration 268, loss = 0.08153749\n",
      "Iteration 152, loss = 0.10333862\n",
      "Iteration 153, loss = 0.10300296\n",
      "Iteration 269, loss = 0.08138260\n",
      "Iteration 270, loss = 0.08122852\n",
      "Iteration 154, loss = 0.10266615\n",
      "Iteration 261, loss = 0.13176036\n",
      "Iteration 155, loss = 0.10232842\n",
      "Iteration 156, loss = 0.10198875\n",
      "Iteration 157, loss = 0.10166319\n",
      "Iteration 158, loss = 0.10134630\n",
      "Iteration 128, loss = 0.20379201\n",
      "Iteration 159, loss = 0.10101865\n",
      "Iteration 160, loss = 0.10070162\n",
      "Iteration 320, loss = 0.11493393\n",
      "Iteration 271, loss = 0.08107135\n",
      "Iteration 161, loss = 0.10038326\n",
      "Iteration 120, loss = 0.21374559\n",
      "Iteration 162, loss = 0.10008195\n",
      "Iteration 272, loss = 0.08092616\n",
      "Iteration 273, loss = 0.08077435\n",
      "Iteration 274, loss = 0.08061567\n",
      "Iteration 275, loss = 0.08046958\n",
      "Iteration 321, loss = 0.11483617\n",
      "Iteration 276, loss = 0.08031910\n",
      "Iteration 198, loss = 0.16337434\n",
      "Iteration 277, loss = 0.08016735\n",
      "Iteration 278, loss = 0.08002304\n",
      "Iteration 279, loss = 0.07987713\n",
      "Iteration 262, loss = 0.13159757\n",
      "Iteration 280, loss = 0.07973660\n",
      "Iteration 281, loss = 0.07958895\n",
      "Iteration 129, loss = 0.20288295\n",
      "Iteration 163, loss = 0.09976286\n",
      "Iteration 282, loss = 0.07943717\n",
      "Iteration 58, loss = 0.32614708\n",
      "Iteration 164, loss = 0.09946268\n",
      "Iteration 322, loss = 0.11475146\n",
      "Iteration 165, loss = 0.09915043\n",
      "Iteration 283, loss = 0.07930013\n",
      "Iteration 284, loss = 0.07915346\n",
      "Iteration 166, loss = 0.09886115\n",
      "Iteration 167, loss = 0.09855962\n",
      "Iteration 168, loss = 0.09826222\n",
      "Iteration 130, loss = 0.20200099\n",
      "Iteration 285, loss = 0.07901031Iteration 169, loss = 0.09797102\n",
      "Iteration 170, loss = 0.09768358\n",
      "\n",
      "Iteration 171, loss = 0.09739925\n",
      "Iteration 263, loss = 0.13148304\n",
      "Iteration 172, loss = 0.09711418\n",
      "Iteration 173, loss = 0.09683464\n",
      "Iteration 174, loss = 0.09655372\n",
      "Iteration 286, loss = 0.07887341\n",
      "Iteration 175, loss = 0.09627592\n",
      "Iteration 264, loss = 0.13135614\n",
      "Iteration 176, loss = 0.09600694\n",
      "Iteration 287, loss = 0.07873229\n",
      "Iteration 288, loss = 0.07859607\n",
      "Iteration 289, loss = 0.07846161\n",
      "Iteration 177, loss = 0.09573844\n",
      "Iteration 178, loss = 0.09546696\n",
      "Iteration 290, loss = 0.07832373\n",
      "Iteration 323, loss = 0.11466279\n",
      "Iteration 179, loss = 0.09520012\n",
      "Iteration 121, loss = 0.21329092\n",
      "Iteration 199, loss = 0.16313721\n",
      "Iteration 180, loss = 0.09494543\n",
      "Iteration 291, loss = 0.07818041\n",
      "Iteration 292, loss = 0.07805167\n",
      "Iteration 293, loss = 0.07791666\n",
      "Iteration 294, loss = 0.07776952\n",
      "Iteration 295, loss = 0.07763845\n",
      "Iteration 296, loss = 0.07750999\n",
      "Iteration 181, loss = 0.09467973\n",
      "Iteration 182, loss = 0.09441711\n",
      "Iteration 183, loss = 0.09418003\n",
      "Iteration 184, loss = 0.09390600\n",
      "Iteration 185, loss = 0.09365611\n",
      "Iteration 186, loss = 0.09340777\n",
      "Iteration 187, loss = 0.09315956\n",
      "Iteration 188, loss = 0.09292254\n",
      "Iteration 297, loss = 0.07737860\n",
      "Iteration 189, loss = 0.09267843\n",
      "Iteration 324, loss = 0.11458790\n",
      "Iteration 190, loss = 0.09243096\n",
      "Iteration 191, loss = 0.09218766\n",
      "Iteration 265, loss = 0.13125643\n",
      "Iteration 192, loss = 0.09195379\n",
      "Iteration 298, loss = 0.07724037\n",
      "Iteration 193, loss = 0.09171517\n",
      "Iteration 194, loss = 0.09148693\n",
      "Iteration 299, loss = 0.07710854\n",
      "Iteration 195, loss = 0.09125438\n",
      "Iteration 300, loss = 0.07698434\n",
      "Iteration 196, loss = 0.09102286\n",
      "Iteration 301, loss = 0.07685198\n",
      "Iteration 197, loss = 0.09079138\n",
      "Iteration 198, loss = 0.09056621\n",
      "Iteration 199, loss = 0.09033867\n",
      "Iteration 200, loss = 0.09012039\n",
      "Iteration 201, loss = 0.08989970\n",
      "Iteration 302, loss = 0.07672164\n",
      "Iteration 202, loss = 0.08967519\n",
      "Iteration 131, loss = 0.20116020\n",
      "Iteration 203, loss = 0.08945624\n",
      "Iteration 59, loss = 0.32306013\n",
      "Iteration 303, loss = 0.07659455\n",
      "Iteration 304, loss = 0.07646180\n",
      "Iteration 200, loss = 0.16290338\n",
      "Iteration 204, loss = 0.08924660\n",
      "Iteration 305, loss = 0.07634286\n",
      "Iteration 306, loss = 0.07620546\n",
      "Iteration 307, loss = 0.07608457\n",
      "Iteration 122, loss = 0.21284369\n",
      "Iteration 308, loss = 0.07595817\n",
      "Iteration 309, loss = 0.07583119\n",
      "Iteration 325, loss = 0.11452111\n",
      "Iteration 266, loss = 0.13114481\n",
      "Iteration 310, loss = 0.07570701\n",
      "Iteration 311, loss = 0.07557949\n",
      "Iteration 312, loss = 0.07545725\n",
      "Iteration 201, loss = 0.16269210\n",
      "Iteration 205, loss = 0.08902759\n",
      "Iteration 132, loss = 0.20031678\n",
      "Iteration 267, loss = 0.13101306\n",
      "Iteration 313, loss = 0.07533390\n",
      "Iteration 326, loss = 0.11440592\n",
      "Iteration 314, loss = 0.07521585\n",
      "Iteration 315, loss = 0.07509001\n",
      "Iteration 316, loss = 0.07497623\n",
      "Iteration 317, loss = 0.07485001\n",
      "Iteration 318, loss = 0.07472920\n",
      "Iteration 206, loss = 0.08881690\n",
      "Iteration 319, loss = 0.07460627\n",
      "Iteration 207, loss = 0.08860281\n",
      "Iteration 320, loss = 0.07449187\n",
      "Iteration 268, loss = 0.13088201\n",
      "Iteration 208, loss = 0.08839523\n",
      "Iteration 321, loss = 0.07437394\n",
      "Iteration 60, loss = 0.32008685\n",
      "Iteration 322, loss = 0.07425364\n",
      "Iteration 323, loss = 0.07414660\n",
      "Iteration 324, loss = 0.07403090\n",
      "Iteration 325, loss = 0.07390367\n",
      "Iteration 326, loss = 0.07379226\n",
      "Iteration 327, loss = 0.07367338\n",
      "Iteration 202, loss = 0.16248854\n",
      "Iteration 328, loss = 0.07355752\n",
      "Iteration 327, loss = 0.11433485\n",
      "Iteration 133, loss = 0.19957955\n",
      "Iteration 329, loss = 0.07344927\n",
      "Iteration 269, loss = 0.13077818\n",
      "Iteration 330, loss = 0.07333132\n",
      "Iteration 123, loss = 0.21246809\n",
      "Iteration 331, loss = 0.07322046\n",
      "Iteration 209, loss = 0.08818569\n",
      "Iteration 210, loss = 0.08798322\n",
      "Iteration 211, loss = 0.08777807\n",
      "Iteration 332, loss = 0.07311313\n",
      "Iteration 333, loss = 0.07300479\n",
      "Iteration 334, loss = 0.07288361\n",
      "Iteration 335, loss = 0.07277181\n",
      "Iteration 336, loss = 0.07266388\n",
      "Iteration 212, loss = 0.08756924\n",
      "Iteration 134, loss = 0.19874028\n",
      "Iteration 213, loss = 0.08736689\n",
      "Iteration 270, loss = 0.13068553\n",
      "Iteration 214, loss = 0.08716655\n",
      "Iteration 328, loss = 0.11425110\n",
      "Iteration 215, loss = 0.08697037\n",
      "Iteration 203, loss = 0.16229705\n",
      "Iteration 216, loss = 0.08677470\n",
      "Iteration 217, loss = 0.08657152\n",
      "Iteration 61, loss = 0.31726178\n",
      "Iteration 124, loss = 0.21196515\n",
      "Iteration 218, loss = 0.08638253\n",
      "Iteration 337, loss = 0.07254818\n",
      "Iteration 338, loss = 0.07243653\n",
      "Iteration 339, loss = 0.07232828\n",
      "Iteration 329, loss = 0.11417260\n",
      "Iteration 340, loss = 0.07221918\n",
      "Iteration 341, loss = 0.07211450\n",
      "Iteration 342, loss = 0.07200138\n",
      "Iteration 343, loss = 0.07189420\n",
      "Iteration 344, loss = 0.07178696\n",
      "Iteration 345, loss = 0.07167833\n",
      "Iteration 271, loss = 0.13054930\n",
      "Iteration 346, loss = 0.07157164\n",
      "Iteration 347, loss = 0.07146817\n",
      "Iteration 219, loss = 0.08618983\n",
      "Iteration 348, loss = 0.07136209\n",
      "Iteration 204, loss = 0.16204918\n",
      "Iteration 349, loss = 0.07126013\n",
      "Iteration 125, loss = 0.21152361Iteration 220, loss = 0.08599750\n",
      "\n",
      "Iteration 221, loss = 0.08580040\n",
      "Iteration 350, loss = 0.07115122\n",
      "Iteration 330, loss = 0.11408884\n",
      "Iteration 222, loss = 0.08561489\n",
      "Iteration 223, loss = 0.08542440\n",
      "Iteration 351, loss = 0.07104861\n",
      "Iteration 224, loss = 0.08524319\n",
      "Iteration 135, loss = 0.19795596\n",
      "Iteration 225, loss = 0.08504985\n",
      "Iteration 352, loss = 0.07095046\n",
      "Iteration 226, loss = 0.08486878\n",
      "Iteration 353, loss = 0.07083764\n",
      "Iteration 354, loss = 0.07073943\n",
      "Iteration 227, loss = 0.08468408\n",
      "Iteration 355, loss = 0.07063344\n",
      "Iteration 228, loss = 0.08450685\n",
      "Iteration 356, loss = 0.07053366\n",
      "Iteration 357, loss = 0.07042715\n",
      "Iteration 229, loss = 0.08432048\n",
      "Iteration 358, loss = 0.07032930\n",
      "Iteration 230, loss = 0.08414140\n",
      "Iteration 359, loss = 0.07022733\n",
      "Iteration 360, loss = 0.07012962\n",
      "Iteration 361, loss = 0.07002773\n",
      "Iteration 362, loss = 0.06993097\n",
      "Iteration 136, loss = 0.19720484\n",
      "Iteration 231, loss = 0.08396130\n",
      "Iteration 272, loss = 0.13043161\n",
      "Iteration 232, loss = 0.08378965\n",
      "Iteration 363, loss = 0.06982675\n",
      "Iteration 364, loss = 0.06972794\n",
      "Iteration 331, loss = 0.11400374\n",
      "Iteration 233, loss = 0.08360827\n",
      "Iteration 365, loss = 0.06963707\n",
      "Iteration 234, loss = 0.08344248\n",
      "Iteration 273, loss = 0.13035027\n",
      "Iteration 235, loss = 0.08325854\n",
      "Iteration 205, loss = 0.16187055\n",
      "Iteration 236, loss = 0.08309268\n",
      "Iteration 237, loss = 0.08291668\n",
      "Iteration 62, loss = 0.31460862\n",
      "Iteration 366, loss = 0.06952966\n",
      "Iteration 367, loss = 0.06943865\n",
      "Iteration 368, loss = 0.06934599\n",
      "Iteration 238, loss = 0.08274579\n",
      "Iteration 239, loss = 0.08257615\n",
      "Iteration 137, loss = 0.19644720\n",
      "Iteration 240, loss = 0.08240346\n",
      "Iteration 369, loss = 0.06923925\n",
      "Iteration 241, loss = 0.08223813\n",
      "Iteration 242, loss = 0.08206666\n",
      "Iteration 126, loss = 0.21110614\n",
      "Iteration 332, loss = 0.11392339\n",
      "Iteration 370, loss = 0.06914348\n",
      "Iteration 371, loss = 0.06905051\n",
      "Iteration 372, loss = 0.06895507Iteration 243, loss = 0.08190354\n",
      "\n",
      "Iteration 244, loss = 0.08173907\n",
      "Iteration 206, loss = 0.16165366\n",
      "Iteration 245, loss = 0.08157524\n",
      "Iteration 246, loss = 0.08141528\n",
      "Iteration 274, loss = 0.13021883\n",
      "Iteration 373, loss = 0.06886219\n",
      "Iteration 374, loss = 0.06876335\n",
      "Iteration 247, loss = 0.08125646\n",
      "Iteration 138, loss = 0.19571333\n",
      "Iteration 375, loss = 0.06867064\n",
      "Iteration 376, loss = 0.06857975\n",
      "Iteration 248, loss = 0.08108573\n",
      "Iteration 249, loss = 0.08093000\n",
      "Iteration 250, loss = 0.08076985\n",
      "Iteration 377, loss = 0.06849210\n",
      "Iteration 251, loss = 0.08061139\n",
      "Iteration 333, loss = 0.11384704\n",
      "Iteration 378, loss = 0.06839294\n",
      "Iteration 379, loss = 0.06830111\n",
      "Iteration 252, loss = 0.08045102\n",
      "Iteration 380, loss = 0.06820862\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 253, loss = 0.08030306\n",
      "Iteration 254, loss = 0.08014251\n",
      "Iteration 63, loss = 0.31195405\n",
      "Iteration 275, loss = 0.13010388\n",
      "Iteration 207, loss = 0.16142340\n",
      "Iteration 255, loss = 0.07998995\n",
      "Iteration 1, loss = 0.70883202\n",
      "Iteration 139, loss = 0.19500390\n",
      "Iteration 127, loss = 0.21075828\n",
      "Iteration 2, loss = 0.67632231\n",
      "Iteration 3, loss = 0.63396357\n",
      "Iteration 256, loss = 0.07983306\n",
      "Iteration 4, loss = 0.59076410\n",
      "Iteration 257, loss = 0.07967884\n",
      "Iteration 258, loss = 0.07952334\n",
      "Iteration 5, loss = 0.55152704\n",
      "Iteration 6, loss = 0.51785763\n",
      "Iteration 334, loss = 0.11377544\n",
      "Iteration 259, loss = 0.07937369\n",
      "Iteration 260, loss = 0.07922277\n",
      "Iteration 261, loss = 0.07908148\n",
      "Iteration 7, loss = 0.48889628\n",
      "Iteration 8, loss = 0.46407453\n",
      "Iteration 276, loss = 0.13002042\n",
      "Iteration 262, loss = 0.07892684\n",
      "Iteration 9, loss = 0.44370476\n",
      "Iteration 263, loss = 0.07878321\n",
      "Iteration 10, loss = 0.42556268\n",
      "Iteration 264, loss = 0.07863162\n",
      "Iteration 208, loss = 0.16124660\n",
      "Iteration 11, loss = 0.41003205\n",
      "Iteration 140, loss = 0.19433468\n",
      "Iteration 265, loss = 0.07848168\n",
      "Iteration 12, loss = 0.39651392\n",
      "Iteration 266, loss = 0.07835698\n",
      "Iteration 13, loss = 0.38419460\n",
      "Iteration 267, loss = 0.07819497\n",
      "Iteration 14, loss = 0.37352545\n",
      "Iteration 268, loss = 0.07805180\n",
      "Iteration 335, loss = 0.11368306\n",
      "Iteration 15, loss = 0.36347816\n",
      "Iteration 269, loss = 0.07791307\n",
      "Iteration 16, loss = 0.35432392\n",
      "Iteration 17, loss = 0.34593105\n",
      "Iteration 128, loss = 0.21031180\n",
      "Iteration 18, loss = 0.33817567\n",
      "Iteration 270, loss = 0.07776948\n",
      "Iteration 19, loss = 0.33079202\n",
      "Iteration 20, loss = 0.32381179\n",
      "Iteration 21, loss = 0.31743053\n",
      "Iteration 271, loss = 0.07763047\n",
      "Iteration 272, loss = 0.07748823\n",
      "Iteration 64, loss = 0.30956446\n",
      "Iteration 277, loss = 0.12990359\n",
      "Iteration 336, loss = 0.11361077\n",
      "Iteration 273, loss = 0.07734555\n",
      "Iteration 274, loss = 0.07720752\n",
      "Iteration 141, loss = 0.19357908\n",
      "Iteration 278, loss = 0.12980686\n",
      "Iteration 22, loss = 0.31119586\n",
      "Iteration 275, loss = 0.07707205\n",
      "Iteration 209, loss = 0.16103205\n",
      "Iteration 23, loss = 0.30518929\n",
      "Iteration 276, loss = 0.07693531\n",
      "Iteration 277, loss = 0.07680866\n",
      "Iteration 278, loss = 0.07665655\n",
      "Iteration 24, loss = 0.29959956\n",
      "Iteration 279, loss = 0.07652646\n",
      "Iteration 280, loss = 0.07639693\n",
      "Iteration 25, loss = 0.29414159\n",
      "Iteration 281, loss = 0.07625363\n",
      "Iteration 279, loss = 0.12969758\n",
      "Iteration 129, loss = 0.20990141\n",
      "Iteration 142, loss = 0.19293481\n",
      "Iteration 26, loss = 0.28892463\n",
      "Iteration 27, loss = 0.28377764\n",
      "Iteration 282, loss = 0.07612813\n",
      "Iteration 283, loss = 0.07599168\n",
      "Iteration 28, loss = 0.27895526\n",
      "Iteration 337, loss = 0.11353764\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 29, loss = 0.27427168\n",
      "Iteration 30, loss = 0.26983284\n",
      "Iteration 31, loss = 0.26530451\n",
      "Iteration 32, loss = 0.26102997\n",
      "Iteration 1, loss = 0.57086739\n",
      "Iteration 284, loss = 0.07586100\n",
      "Iteration 33, loss = 0.25700034\n",
      "Iteration 34, loss = 0.25292761\n",
      "Iteration 35, loss = 0.24908435\n",
      "Iteration 285, loss = 0.07573187\n",
      "Iteration 286, loss = 0.07560563\n",
      "Iteration 210, loss = 0.16083945\n",
      "Iteration 65, loss = 0.30717036\n",
      "Iteration 2, loss = 0.55720142\n",
      "Iteration 36, loss = 0.24536046\n",
      "Iteration 287, loss = 0.07548006\n",
      "Iteration 3, loss = 0.53865805\n",
      "Iteration 37, loss = 0.24164206\n",
      "Iteration 288, loss = 0.07534202\n",
      "Iteration 289, loss = 0.07522200\n",
      "Iteration 290, loss = 0.07509458\n",
      "Iteration 143, loss = 0.19223085\n",
      "Iteration 291, loss = 0.07496215\n",
      "Iteration 280, loss = 0.12958858\n",
      "Iteration 38, loss = 0.23815024\n",
      "Iteration 211, loss = 0.16062200\n",
      "Iteration 292, loss = 0.07483761\n",
      "Iteration 4, loss = 0.51862404\n",
      "Iteration 39, loss = 0.23458789\n",
      "Iteration 5, loss = 0.49879899\n",
      "Iteration 40, loss = 0.23131177\n",
      "Iteration 130, loss = 0.20950603\n",
      "Iteration 6, loss = 0.48038404\n",
      "Iteration 281, loss = 0.12949554\n",
      "Iteration 41, loss = 0.22807052\n",
      "Iteration 144, loss = 0.19159869\n",
      "Iteration 293, loss = 0.07470895\n",
      "Iteration 294, loss = 0.07459002\n",
      "Iteration 295, loss = 0.07446308\n",
      "Iteration 296, loss = 0.07434217\n",
      "Iteration 42, loss = 0.22488909\n",
      "Iteration 297, loss = 0.07422194\n",
      "Iteration 7, loss = 0.46320790\n",
      "Iteration 43, loss = 0.22173801\n",
      "Iteration 44, loss = 0.21884149\n",
      "Iteration 8, loss = 0.44762478\n",
      "Iteration 298, loss = 0.07409727\n",
      "Iteration 45, loss = 0.21585526\n",
      "Iteration 282, loss = 0.12941277\n",
      "Iteration 299, loss = 0.07397988\n",
      "Iteration 46, loss = 0.21295610\n",
      "Iteration 300, loss = 0.07385183\n",
      "Iteration 301, loss = 0.07373451\n",
      "Iteration 9, loss = 0.43307216\n",
      "Iteration 66, loss = 0.30495976\n",
      "Iteration 302, loss = 0.07361077\n",
      "Iteration 47, loss = 0.21020543\n",
      "Iteration 10, loss = 0.42013607\n",
      "Iteration 48, loss = 0.20760189\n",
      "Iteration 11, loss = 0.40771035\n",
      "Iteration 283, loss = 0.12930061\n",
      "Iteration 49, loss = 0.20485193\n",
      "Iteration 212, loss = 0.16045693\n",
      "Iteration 50, loss = 0.20232075\n",
      "Iteration 303, loss = 0.07349623\n",
      "Iteration 131, loss = 0.20909970\n",
      "Iteration 304, loss = 0.07337840\n",
      "Iteration 305, loss = 0.07325807\n",
      "Iteration 145, loss = 0.19096909\n",
      "Iteration 12, loss = 0.39618214\n",
      "Iteration 13, loss = 0.38568490\n",
      "Iteration 51, loss = 0.19982140\n",
      "Iteration 14, loss = 0.37560994\n",
      "Iteration 284, loss = 0.12917946\n",
      "Iteration 15, loss = 0.36604565\n",
      "Iteration 52, loss = 0.19738011\n",
      "Iteration 306, loss = 0.07314202\n",
      "Iteration 16, loss = 0.35698546\n",
      "Iteration 307, loss = 0.07302679\n",
      "Iteration 53, loss = 0.19507069\n",
      "Iteration 146, loss = 0.19029561\n",
      "Iteration 308, loss = 0.07291311\n",
      "Iteration 54, loss = 0.19274785\n",
      "Iteration 309, loss = 0.07280144\n",
      "Iteration 55, loss = 0.19051483\n",
      "Iteration 56, loss = 0.18824259\n",
      "Iteration 57, loss = 0.18612096\n",
      "Iteration 310, loss = 0.07267677\n",
      "Iteration 17, loss = 0.34844649\n",
      "Iteration 213, loss = 0.16022751\n",
      "Iteration 18, loss = 0.34024507\n",
      "Iteration 19, loss = 0.33222592\n",
      "Iteration 311, loss = 0.07256998Iteration 58, loss = 0.18407478\n",
      "Iteration 59, loss = 0.18199683\n",
      "Iteration 285, loss = 0.12912619\n",
      "\n",
      "Iteration 60, loss = 0.17997818\n",
      "Iteration 312, loss = 0.07245340\n",
      "Iteration 61, loss = 0.17807270\n",
      "Iteration 313, loss = 0.07233698\n",
      "Iteration 132, loss = 0.20872402\n",
      "Iteration 62, loss = 0.17622231\n",
      "Iteration 20, loss = 0.32461837\n",
      "Iteration 63, loss = 0.17439540\n",
      "Iteration 314, loss = 0.07222828\n",
      "Iteration 64, loss = 0.17260960\n",
      "Iteration 315, loss = 0.07211102\n",
      "Iteration 316, loss = 0.07200409\n",
      "Iteration 21, loss = 0.31742401\n",
      "Iteration 67, loss = 0.30280855\n",
      "Iteration 65, loss = 0.17082019\n",
      "Iteration 214, loss = 0.16002808\n",
      "Iteration 22, loss = 0.31041457\n",
      "Iteration 317, loss = 0.07188999\n",
      "Iteration 23, loss = 0.30354644\n",
      "Iteration 147, loss = 0.18967790\n",
      "Iteration 318, loss = 0.07178337\n",
      "Iteration 66, loss = 0.16915355\n",
      "Iteration 286, loss = 0.12897548\n",
      "Iteration 67, loss = 0.16749614\n",
      "Iteration 68, loss = 0.16585875\n",
      "Iteration 69, loss = 0.16433453\n",
      "Iteration 319, loss = 0.07168006\n",
      "Iteration 133, loss = 0.20835015\n",
      "Iteration 24, loss = 0.29709748\n",
      "Iteration 320, loss = 0.07156217\n",
      "Iteration 321, loss = 0.07145850\n",
      "Iteration 70, loss = 0.16274497\n",
      "Iteration 71, loss = 0.16124275\n",
      "Iteration 287, loss = 0.12893166\n",
      "Iteration 25, loss = 0.29076351\n",
      "Iteration 322, loss = 0.07134859\n",
      "Iteration 72, loss = 0.15976607\n",
      "Iteration 323, loss = 0.07124041\n",
      "Iteration 73, loss = 0.15836786\n",
      "Iteration 324, loss = 0.07113572\n",
      "Iteration 26, loss = 0.28478677\n",
      "Iteration 74, loss = 0.15690483\n",
      "Iteration 68, loss = 0.30082867\n",
      "Iteration 75, loss = 0.15553221\n",
      "Iteration 27, loss = 0.27896752Iteration 215, loss = 0.15983564\n",
      "Iteration 148, loss = 0.18903271\n",
      "\n",
      "Iteration 76, loss = 0.15416665\n",
      "Iteration 325, loss = 0.07103330\n",
      "Iteration 77, loss = 0.15286623\n",
      "Iteration 326, loss = 0.07091857\n",
      "Iteration 78, loss = 0.15159909\n",
      "Iteration 288, loss = 0.12881450\n",
      "Iteration 28, loss = 0.27334953\n",
      "Iteration 79, loss = 0.15035470\n",
      "Iteration 80, loss = 0.14908622\n",
      "Iteration 289, loss = 0.12872390\n",
      "Iteration 81, loss = 0.14791115\n",
      "Iteration 327, loss = 0.07081658\n",
      "Iteration 328, loss = 0.07071397\n",
      "Iteration 329, loss = 0.07060708\n",
      "Iteration 29, loss = 0.26804917\n",
      "Iteration 330, loss = 0.07050625\n",
      "Iteration 30, loss = 0.26276557\n",
      "Iteration 134, loss = 0.20797314\n",
      "Iteration 331, loss = 0.07040030\n",
      "Iteration 31, loss = 0.25778943\n",
      "Iteration 32, loss = 0.25299815\n",
      "Iteration 33, loss = 0.24835794\n",
      "Iteration 149, loss = 0.18846748\n",
      "Iteration 34, loss = 0.24397541\n",
      "Iteration 82, loss = 0.14672640\n",
      "Iteration 35, loss = 0.23950251\n",
      "Iteration 332, loss = 0.07029832\n",
      "Iteration 216, loss = 0.15965600\n",
      "Iteration 333, loss = 0.07019655\n",
      "Iteration 36, loss = 0.23550948\n",
      "Iteration 83, loss = 0.14557471\n",
      "Iteration 334, loss = 0.07009628\n",
      "Iteration 335, loss = 0.06999874\n",
      "Iteration 84, loss = 0.14446030\n",
      "Iteration 336, loss = 0.06990144\n",
      "Iteration 337, loss = 0.06979018\n",
      "Iteration 338, loss = 0.06969605\n",
      "Iteration 85, loss = 0.14337251\n",
      "Iteration 37, loss = 0.23145554\n",
      "Iteration 86, loss = 0.14226605\n",
      "Iteration 87, loss = 0.14122628\n",
      "Iteration 88, loss = 0.14019331\n",
      "Iteration 290, loss = 0.12862197\n",
      "Iteration 38, loss = 0.22753016\n",
      "Iteration 89, loss = 0.13915151\n",
      "Iteration 69, loss = 0.29885330\n",
      "Iteration 90, loss = 0.13817073\n",
      "Iteration 339, loss = 0.06959512\n",
      "Iteration 91, loss = 0.13722585\n",
      "Iteration 92, loss = 0.13624856\n",
      "Iteration 340, loss = 0.06949637\n",
      "Iteration 150, loss = 0.18786410\n",
      "Iteration 93, loss = 0.13530454\n",
      "Iteration 341, loss = 0.06939813\n",
      "Iteration 94, loss = 0.13436747\n",
      "Iteration 135, loss = 0.20761851\n",
      "Iteration 39, loss = 0.22393042\n",
      "Iteration 40, loss = 0.22033266\n",
      "Iteration 41, loss = 0.21692912\n",
      "Iteration 342, loss = 0.06929933\n",
      "Iteration 343, loss = 0.06920128\n",
      "Iteration 95, loss = 0.13347543\n",
      "Iteration 344, loss = 0.06911006\n",
      "Iteration 217, loss = 0.15945252\n",
      "Iteration 345, loss = 0.06901113\n",
      "Iteration 42, loss = 0.21356445\n",
      "Iteration 346, loss = 0.06891031\n",
      "Iteration 291, loss = 0.12853701\n",
      "Iteration 43, loss = 0.21034077\n",
      "Iteration 96, loss = 0.13261242\n",
      "Iteration 44, loss = 0.20733673\n",
      "Iteration 45, loss = 0.20427990\n",
      "Iteration 347, loss = 0.06881700\n",
      "Iteration 46, loss = 0.20140069\n",
      "Iteration 348, loss = 0.06872133\n",
      "Iteration 349, loss = 0.06862984\n",
      "Iteration 151, loss = 0.18730417\n",
      "Iteration 97, loss = 0.13172189\n",
      "Iteration 98, loss = 0.13091449\n",
      "Iteration 292, loss = 0.12842949\n",
      "Iteration 99, loss = 0.13003276\n",
      "Iteration 350, loss = 0.06853394\n",
      "Iteration 136, loss = 0.20720533\n",
      "Iteration 351, loss = 0.06843987\n",
      "Iteration 352, loss = 0.06834234\n",
      "Iteration 100, loss = 0.12923044\n",
      "Iteration 101, loss = 0.12844346\n",
      "Iteration 102, loss = 0.12761379\n",
      "Iteration 47, loss = 0.19863017\n",
      "Iteration 48, loss = 0.19595671\n",
      "Iteration 103, loss = 0.12685380\n",
      "Iteration 49, loss = 0.19342360\n",
      "Iteration 218, loss = 0.15926957\n",
      "Iteration 353, loss = 0.06825218\n",
      "Iteration 293, loss = 0.12834635\n",
      "Iteration 354, loss = 0.06815880\n",
      "Iteration 355, loss = 0.06806474\n",
      "Iteration 356, loss = 0.06797878\n",
      "Iteration 357, loss = 0.06788564\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 70, loss = 0.29698816\n",
      "Iteration 50, loss = 0.19088081\n",
      "Iteration 51, loss = 0.18844930\n",
      "Iteration 152, loss = 0.18673772\n",
      "Iteration 104, loss = 0.12608945\n",
      "Iteration 52, loss = 0.18616731\n",
      "Iteration 105, loss = 0.12533566\n",
      "Iteration 106, loss = 0.12458928\n",
      "Iteration 153, loss = 0.18618991\n",
      "Iteration 107, loss = 0.12386707\n",
      "Iteration 1, loss = 0.74401628\n",
      "Iteration 108, loss = 0.12312658\n",
      "Iteration 294, loss = 0.12827144Iteration 53, loss = 0.18388328\n",
      "\n",
      "Iteration 109, loss = 0.12243292\n",
      "Iteration 54, loss = 0.18170491\n",
      "Iteration 55, loss = 0.17958731\n",
      "Iteration 219, loss = 0.15906556\n",
      "Iteration 137, loss = 0.20685758\n",
      "Iteration 56, loss = 0.17757284\n",
      "Iteration 295, loss = 0.12817625\n",
      "Iteration 57, loss = 0.17561149\n",
      "Iteration 110, loss = 0.12171182\n",
      "Iteration 111, loss = 0.12106773\n",
      "Iteration 2, loss = 0.70749919\n",
      "Iteration 58, loss = 0.17367602\n",
      "Iteration 112, loss = 0.12034432\n",
      "Iteration 113, loss = 0.11968409\n",
      "Iteration 3, loss = 0.66012916\n",
      "Iteration 59, loss = 0.17181638\n",
      "Iteration 4, loss = 0.61221443\n",
      "Iteration 114, loss = 0.11900964\n",
      "Iteration 154, loss = 0.18565121\n",
      "Iteration 60, loss = 0.17000442\n",
      "Iteration 115, loss = 0.11834568\n",
      "Iteration 61, loss = 0.16829211Iteration 116, loss = 0.11774071\n",
      "\n",
      "Iteration 117, loss = 0.11708714\n",
      "Iteration 118, loss = 0.11645654\n",
      "Iteration 119, loss = 0.11584254\n",
      "Iteration 71, loss = 0.29525370\n",
      "Iteration 155, loss = 0.18512503\n",
      "Iteration 5, loss = 0.56914660\n",
      "Iteration 6, loss = 0.53187276\n",
      "Iteration 7, loss = 0.50087726\n",
      "Iteration 62, loss = 0.16656443\n",
      "Iteration 220, loss = 0.15887439\n",
      "Iteration 296, loss = 0.12807364\n",
      "Iteration 120, loss = 0.11524001\n",
      "Iteration 121, loss = 0.11464208\n",
      "Iteration 63, loss = 0.16495065\n",
      "Iteration 122, loss = 0.11406706\n",
      "Iteration 64, loss = 0.16332547\n",
      "Iteration 65, loss = 0.16178674\n",
      "Iteration 123, loss = 0.11350750\n",
      "Iteration 124, loss = 0.11290568\n",
      "Iteration 66, loss = 0.16024665\n",
      "Iteration 125, loss = 0.11236403\n",
      "Iteration 8, loss = 0.47402561\n",
      "Iteration 126, loss = 0.11184710\n",
      "Iteration 138, loss = 0.20653426\n",
      "Iteration 297, loss = 0.12798166\n",
      "Iteration 67, loss = 0.15874412\n",
      "Iteration 9, loss = 0.45134599\n",
      "Iteration 127, loss = 0.11127594\n",
      "Iteration 10, loss = 0.43250126Iteration 298, loss = 0.12791222\n",
      "Iteration 221, loss = 0.15869539\n",
      "Iteration 156, loss = 0.18462787\n",
      "\n",
      "Iteration 68, loss = 0.15733248\n",
      "Iteration 128, loss = 0.11074919\n",
      "Iteration 129, loss = 0.11021982\n",
      "Iteration 130, loss = 0.10969326\n",
      "Iteration 69, loss = 0.15591493\n",
      "Iteration 131, loss = 0.10918470\n",
      "Iteration 70, loss = 0.15457413\n",
      "Iteration 71, loss = 0.15324020\n",
      "Iteration 11, loss = 0.41588558\n",
      "Iteration 72, loss = 0.15196166\n",
      "Iteration 12, loss = 0.40159731\n",
      "Iteration 132, loss = 0.10866594\n",
      "Iteration 139, loss = 0.20613590\n",
      "Iteration 72, loss = 0.29375973\n",
      "Iteration 157, loss = 0.18409818\n",
      "Iteration 133, loss = 0.10820071\n",
      "Iteration 73, loss = 0.15065312\n",
      "Iteration 13, loss = 0.38894660\n",
      "Iteration 14, loss = 0.37753415\n",
      "Iteration 134, loss = 0.10768678\n",
      "Iteration 135, loss = 0.10719854\n",
      "Iteration 299, loss = 0.12784291\n",
      "Iteration 136, loss = 0.10672285\n",
      "Iteration 15, loss = 0.36698614\n",
      "Iteration 158, loss = 0.18357471\n",
      "Iteration 137, loss = 0.10625811\n",
      "Iteration 138, loss = 0.10576443\n",
      "Iteration 222, loss = 0.15851253\n",
      "Iteration 139, loss = 0.10531352\n",
      "Iteration 140, loss = 0.10488528\n",
      "Iteration 74, loss = 0.14945185\n",
      "Iteration 75, loss = 0.14824194\n",
      "Iteration 76, loss = 0.14707431\n",
      "Iteration 141, loss = 0.10440606\n",
      "Iteration 16, loss = 0.35756528\n",
      "Iteration 17, loss = 0.34897612Iteration 77, loss = 0.14591833\n",
      "Iteration 300, loss = 0.12773774\n",
      "Iteration 142, loss = 0.10395175\n",
      "\n",
      "Iteration 18, loss = 0.34070701\n",
      "Iteration 19, loss = 0.33306159\n",
      "Iteration 143, loss = 0.10352117\n",
      "Iteration 73, loss = 0.29190641\n",
      "Iteration 78, loss = 0.14486826\n",
      "Iteration 159, loss = 0.18306989\n",
      "Iteration 144, loss = 0.10311017\n",
      "Iteration 79, loss = 0.14369712\n",
      "Iteration 145, loss = 0.10267415\n",
      "Iteration 80, loss = 0.14266330\n",
      "Iteration 223, loss = 0.15831926\n",
      "Iteration 146, loss = 0.10224888\n",
      "Iteration 81, loss = 0.14160832\n",
      "Iteration 20, loss = 0.32583449\n",
      "Iteration 147, loss = 0.10184523\n",
      "Iteration 82, loss = 0.14058457\n",
      "Iteration 301, loss = 0.12767935\n",
      "Iteration 21, loss = 0.31899491\n",
      "Iteration 83, loss = 0.13963437\n",
      "Iteration 84, loss = 0.13866086\n",
      "Iteration 22, loss = 0.31258094\n",
      "Iteration 148, loss = 0.10141426\n",
      "Iteration 23, loss = 0.30625485\n",
      "Iteration 149, loss = 0.10102053\n",
      "Iteration 140, loss = 0.20582249\n",
      "Iteration 85, loss = 0.13771607\n",
      "Iteration 86, loss = 0.13678874\n",
      "Iteration 160, loss = 0.18258054\n",
      "Iteration 24, loss = 0.30030929\n",
      "Iteration 25, loss = 0.29448278\n",
      "Iteration 87, loss = 0.13587468\n",
      "Iteration 150, loss = 0.10062873\n",
      "Iteration 26, loss = 0.28901446\n",
      "Iteration 88, loss = 0.13501154\n",
      "Iteration 151, loss = 0.10023841\n",
      "Iteration 152, loss = 0.09983139\n",
      "Iteration 89, loss = 0.13411917\n",
      "Iteration 90, loss = 0.13328880\n",
      "Iteration 153, loss = 0.09947006\n",
      "Iteration 224, loss = 0.15813423\n",
      "Iteration 154, loss = 0.09907133\n",
      "Iteration 302, loss = 0.12756574\n",
      "Iteration 155, loss = 0.09869309\n",
      "Iteration 91, loss = 0.13244014\n",
      "Iteration 27, loss = 0.28373142\n",
      "Iteration 156, loss = 0.09831959\n",
      "Iteration 161, loss = 0.18212404\n",
      "Iteration 303, loss = 0.12747401\n",
      "Iteration 74, loss = 0.29035145\n",
      "Iteration 157, loss = 0.09797873\n",
      "Iteration 158, loss = 0.09759652\n",
      "Iteration 28, loss = 0.27861346\n",
      "Iteration 159, loss = 0.09725172\n",
      "Iteration 160, loss = 0.09689954\n",
      "Iteration 29, loss = 0.27357349\n",
      "Iteration 92, loss = 0.13162661\n",
      "Iteration 161, loss = 0.09655862\n",
      "Iteration 162, loss = 0.09620198\n",
      "Iteration 30, loss = 0.26865765\n",
      "Iteration 93, loss = 0.13081452\n",
      "Iteration 94, loss = 0.13001942\n",
      "Iteration 95, loss = 0.12926795\n",
      "Iteration 96, loss = 0.12849320\n",
      "Iteration 97, loss = 0.12774236\n",
      "Iteration 163, loss = 0.09586917\n",
      "Iteration 162, loss = 0.18164628\n",
      "Iteration 304, loss = 0.12741365\n",
      "Iteration 141, loss = 0.20543936\n",
      "Iteration 225, loss = 0.15796354\n",
      "Iteration 164, loss = 0.09553380\n",
      "Iteration 165, loss = 0.09519699\n",
      "Iteration 166, loss = 0.09489484\n",
      "Iteration 98, loss = 0.12701925\n",
      "Iteration 99, loss = 0.12630512\n",
      "Iteration 31, loss = 0.26397961\n",
      "Iteration 167, loss = 0.09455625\n",
      "Iteration 168, loss = 0.09423725\n",
      "Iteration 169, loss = 0.09394022\n",
      "Iteration 170, loss = 0.09361497\n",
      "Iteration 75, loss = 0.28887535\n",
      "Iteration 171, loss = 0.09329890\n",
      "Iteration 32, loss = 0.25943875\n",
      "Iteration 172, loss = 0.09299478\n",
      "Iteration 173, loss = 0.09269362\n",
      "Iteration 100, loss = 0.12559720\n",
      "Iteration 33, loss = 0.25504800\n",
      "Iteration 174, loss = 0.09238469\n",
      "Iteration 305, loss = 0.12732666\n",
      "Iteration 34, loss = 0.25078151\n",
      "Iteration 35, loss = 0.24666967\n",
      "Iteration 101, loss = 0.12496798\n",
      "Iteration 163, loss = 0.18118404\n",
      "Iteration 142, loss = 0.20512929\n",
      "Iteration 226, loss = 0.15782298\n",
      "Iteration 102, loss = 0.12424866Iteration 36, loss = 0.24275429\n",
      "Iteration 175, loss = 0.09209657\n",
      "Iteration 37, loss = 0.23887050\n",
      "\n",
      "Iteration 103, loss = 0.12357268\n",
      "Iteration 38, loss = 0.23515778\n",
      "Iteration 104, loss = 0.12294090\n",
      "Iteration 176, loss = 0.09179136\n",
      "Iteration 39, loss = 0.23145193\n",
      "Iteration 105, loss = 0.12230043\n",
      "Iteration 177, loss = 0.09150261\n",
      "Iteration 178, loss = 0.09121268\n",
      "Iteration 106, loss = 0.12166869\n",
      "Iteration 179, loss = 0.09091120\n",
      "Iteration 180, loss = 0.09064075Iteration 107, loss = 0.12105451\n",
      "\n",
      "Iteration 181, loss = 0.09034785\n",
      "Iteration 108, loss = 0.12043341\n",
      "Iteration 306, loss = 0.12727127\n",
      "Iteration 182, loss = 0.09006710\n",
      "Iteration 183, loss = 0.08979489\n",
      "Iteration 109, loss = 0.11982937\n",
      "Iteration 184, loss = 0.08951537\n",
      "Iteration 110, loss = 0.11923904\n",
      "Iteration 111, loss = 0.11865612\n",
      "Iteration 112, loss = 0.11808678\n",
      "Iteration 307, loss = 0.12714756\n",
      "Iteration 227, loss = 0.15759282\n",
      "Iteration 164, loss = 0.18071212\n",
      "Iteration 185, loss = 0.08924833\n",
      "Iteration 186, loss = 0.08896730\n",
      "Iteration 40, loss = 0.22801719\n",
      "Iteration 187, loss = 0.08868917\n",
      "Iteration 143, loss = 0.20480055\n",
      "Iteration 41, loss = 0.22462372\n",
      "Iteration 113, loss = 0.11750687\n",
      "Iteration 114, loss = 0.11695710\n",
      "Iteration 76, loss = 0.28738872\n",
      "Iteration 308, loss = 0.12707232\n",
      "Iteration 115, loss = 0.11641333\n",
      "Iteration 188, loss = 0.08843063\n",
      "Iteration 42, loss = 0.22128639\n",
      "Iteration 189, loss = 0.08816556\n",
      "Iteration 190, loss = 0.08790298\n",
      "Iteration 43, loss = 0.21815160\n",
      "Iteration 191, loss = 0.08764015\n",
      "Iteration 192, loss = 0.08738710\n",
      "Iteration 228, loss = 0.15743563\n",
      "Iteration 165, loss = 0.18029917\n",
      "Iteration 193, loss = 0.08712374\n",
      "Iteration 44, loss = 0.21510833\n",
      "Iteration 116, loss = 0.11587783\n",
      "Iteration 194, loss = 0.08686794\n",
      "Iteration 195, loss = 0.08662792\n",
      "Iteration 196, loss = 0.08637795\n",
      "Iteration 45, loss = 0.21216604\n",
      "Iteration 197, loss = 0.08611034\n",
      "Iteration 309, loss = 0.12700418\n",
      "Iteration 117, loss = 0.11532805\n",
      "Iteration 166, loss = 0.17984980\n",
      "Iteration 198, loss = 0.08586634\n",
      "Iteration 118, loss = 0.11482044\n",
      "Iteration 46, loss = 0.20918363\n",
      "Iteration 47, loss = 0.20647673Iteration 229, loss = 0.15725570\n",
      "\n",
      "Iteration 199, loss = 0.08563548\n",
      "Iteration 119, loss = 0.11428812\n",
      "Iteration 200, loss = 0.08537792\n",
      "Iteration 120, loss = 0.11379055\n",
      "Iteration 201, loss = 0.08514605\n",
      "Iteration 121, loss = 0.11328496\n",
      "Iteration 122, loss = 0.11279922\n",
      "Iteration 202, loss = 0.08490712\n",
      "Iteration 48, loss = 0.20375175\n",
      "Iteration 310, loss = 0.12695764\n",
      "Iteration 49, loss = 0.20109229\n",
      "Iteration 144, loss = 0.20445262\n",
      "Iteration 203, loss = 0.08466341\n",
      "Iteration 123, loss = 0.11230115\n",
      "Iteration 124, loss = 0.11182843\n",
      "Iteration 50, loss = 0.19857611\n",
      "Iteration 77, loss = 0.28606585\n",
      "Iteration 125, loss = 0.11134240\n",
      "Iteration 126, loss = 0.11088179\n",
      "Iteration 204, loss = 0.08443607\n",
      "Iteration 205, loss = 0.08420735\n",
      "Iteration 206, loss = 0.08396781\n",
      "Iteration 127, loss = 0.11041279\n",
      "Iteration 167, loss = 0.17940314\n",
      "Iteration 311, loss = 0.12683750\n",
      "Iteration 128, loss = 0.10996649\n",
      "Iteration 51, loss = 0.19614395\n",
      "Iteration 129, loss = 0.10950282\n",
      "Iteration 130, loss = 0.10905408\n",
      "Iteration 207, loss = 0.08374196\n",
      "Iteration 52, loss = 0.19371055\n",
      "Iteration 131, loss = 0.10863252\n",
      "Iteration 53, loss = 0.19141522Iteration 208, loss = 0.08351579\n",
      "Iteration 230, loss = 0.15710548\n",
      "Iteration 209, loss = 0.08329624\n",
      "\n",
      "Iteration 210, loss = 0.08306392\n",
      "Iteration 132, loss = 0.10819508\n",
      "Iteration 312, loss = 0.12677782\n",
      "Iteration 211, loss = 0.08284691\n",
      "Iteration 168, loss = 0.17900091\n",
      "Iteration 145, loss = 0.20411523\n",
      "Iteration 54, loss = 0.18921657\n",
      "Iteration 212, loss = 0.08262983\n",
      "Iteration 213, loss = 0.08241629\n",
      "Iteration 214, loss = 0.08220011\n",
      "Iteration 55, loss = 0.18700685\n",
      "Iteration 78, loss = 0.28469857\n",
      "Iteration 215, loss = 0.08198349\n",
      "Iteration 216, loss = 0.08178175\n",
      "Iteration 217, loss = 0.08157138\n",
      "Iteration 56, loss = 0.18487174\n",
      "Iteration 231, loss = 0.15692391\n",
      "Iteration 133, loss = 0.10777208\n",
      "Iteration 134, loss = 0.10735843\n",
      "Iteration 218, loss = 0.08136891\n",
      "Iteration 219, loss = 0.08115036\n",
      "Iteration 135, loss = 0.10694524\n",
      "Iteration 220, loss = 0.08095599\n",
      "Iteration 221, loss = 0.08073753\n",
      "Iteration 222, loss = 0.08054671\n",
      "Iteration 136, loss = 0.10653736\n",
      "Iteration 223, loss = 0.08034124\n",
      "Iteration 137, loss = 0.10612426\n",
      "Iteration 224, loss = 0.08015658\n",
      "Iteration 225, loss = 0.07994356\n",
      "Iteration 138, loss = 0.10572011\n",
      "Iteration 226, loss = 0.07975619\n",
      "Iteration 169, loss = 0.17862160\n",
      "Iteration 57, loss = 0.18287102\n",
      "Iteration 227, loss = 0.07956514\n",
      "Iteration 58, loss = 0.18087821\n",
      "Iteration 313, loss = 0.12670803\n",
      "Iteration 139, loss = 0.10531506\n",
      "Iteration 59, loss = 0.17891813\n",
      "Iteration 232, loss = 0.15674318\n",
      "Iteration 140, loss = 0.10494270\n",
      "Iteration 146, loss = 0.20377847\n",
      "Iteration 60, loss = 0.17705512\n",
      "Iteration 141, loss = 0.10455550\n",
      "Iteration 228, loss = 0.07936199\n",
      "Iteration 229, loss = 0.07918618\n",
      "Iteration 314, loss = 0.12662263\n",
      "Iteration 230, loss = 0.07900810\n",
      "Iteration 231, loss = 0.07879918\n",
      "Iteration 232, loss = 0.07861337\n",
      "Iteration 79, loss = 0.28343783\n",
      "Iteration 233, loss = 0.07843250\n",
      "Iteration 61, loss = 0.17527989\n",
      "Iteration 315, loss = 0.12654744\n",
      "Iteration 142, loss = 0.10416906\n",
      "Iteration 143, loss = 0.10378867\n",
      "Iteration 62, loss = 0.17347708\n",
      "Iteration 234, loss = 0.07824796\n",
      "Iteration 170, loss = 0.17820125\n",
      "Iteration 235, loss = 0.07805803\n",
      "Iteration 144, loss = 0.10339449\n",
      "Iteration 236, loss = 0.07788571\n",
      "Iteration 237, loss = 0.07771774\n",
      "Iteration 238, loss = 0.07753029\n",
      "Iteration 63, loss = 0.17173434\n",
      "Iteration 239, loss = 0.07735142\n",
      "Iteration 233, loss = 0.15657360\n",
      "Iteration 145, loss = 0.10304448\n",
      "Iteration 240, loss = 0.07718396\n",
      "Iteration 241, loss = 0.07700381Iteration 146, loss = 0.10266166\n",
      "Iteration 171, loss = 0.17778079\n",
      "Iteration 64, loss = 0.17006641\n",
      "\n",
      "Iteration 316, loss = 0.12649062\n",
      "Iteration 147, loss = 0.20348377\n",
      "Iteration 147, loss = 0.10230918\n",
      "Iteration 242, loss = 0.07683671\n",
      "Iteration 65, loss = 0.16846202\n",
      "Iteration 317, loss = 0.12639134\n",
      "Iteration 148, loss = 0.10195068\n",
      "Iteration 243, loss = 0.07668615\n",
      "Iteration 244, loss = 0.07649122\n",
      "Iteration 80, loss = 0.28216683\n",
      "Iteration 149, loss = 0.10159327\n",
      "Iteration 66, loss = 0.16689494\n",
      "Iteration 67, loss = 0.16533075\n",
      "Iteration 148, loss = 0.20317510\n",
      "Iteration 68, loss = 0.16388131\n",
      "Iteration 69, loss = 0.16239424\n",
      "Iteration 245, loss = 0.07633283\n",
      "Iteration 246, loss = 0.07616415\n",
      "Iteration 234, loss = 0.15640198\n",
      "Iteration 150, loss = 0.10123477\n",
      "Iteration 70, loss = 0.16099449\n",
      "Iteration 151, loss = 0.10089361\n",
      "Iteration 71, loss = 0.15962994\n",
      "Iteration 318, loss = 0.12633287\n",
      "Iteration 152, loss = 0.10055527\n",
      "Iteration 247, loss = 0.07601284\n",
      "Iteration 248, loss = 0.07582865\n",
      "Iteration 153, loss = 0.10021350\n",
      "Iteration 249, loss = 0.07566897\n",
      "Iteration 319, loss = 0.12628188\n",
      "Iteration 72, loss = 0.15828856\n",
      "Iteration 250, loss = 0.07550934\n",
      "Iteration 172, loss = 0.17740091\n",
      "Iteration 154, loss = 0.09986929\n",
      "Iteration 251, loss = 0.07534438\n",
      "Iteration 73, loss = 0.15698288\n",
      "Iteration 252, loss = 0.07519619\n",
      "Iteration 155, loss = 0.09953023\n",
      "Iteration 253, loss = 0.07503228\n",
      "Iteration 156, loss = 0.09920278\n",
      "Iteration 254, loss = 0.07488068\n",
      "Iteration 255, loss = 0.07472396\n",
      "Iteration 256, loss = 0.07455899\n",
      "Iteration 257, loss = 0.07441413\n",
      "Iteration 157, loss = 0.09886603\n",
      "Iteration 258, loss = 0.07425102\n",
      "Iteration 74, loss = 0.15576665\n",
      "Iteration 158, loss = 0.09855616\n",
      "Iteration 235, loss = 0.15624979\n",
      "Iteration 259, loss = 0.07410889\n",
      "Iteration 173, loss = 0.17706511\n",
      "Iteration 159, loss = 0.09822200\n",
      "Iteration 81, loss = 0.28097353\n",
      "Iteration 260, loss = 0.07395157\n",
      "Iteration 160, loss = 0.09791476\n",
      "Iteration 261, loss = 0.07380506\n",
      "Iteration 161, loss = 0.09758944\n",
      "Iteration 262, loss = 0.07365413\n",
      "Iteration 320, loss = 0.12618689\n",
      "Iteration 149, loss = 0.20283824\n",
      "Iteration 75, loss = 0.15449652\n",
      "Iteration 174, loss = 0.17665551\n",
      "Iteration 162, loss = 0.09727816\n",
      "Iteration 163, loss = 0.09698454\n",
      "Iteration 263, loss = 0.07350693\n",
      "Iteration 164, loss = 0.09667384\n",
      "Iteration 264, loss = 0.07335824\n",
      "Iteration 321, loss = 0.12617499\n",
      "Iteration 165, loss = 0.09636088\n",
      "Iteration 265, loss = 0.07321487\n",
      "Iteration 266, loss = 0.07306656\n",
      "Iteration 267, loss = 0.07292760\n",
      "Iteration 268, loss = 0.07278841\n",
      "Iteration 269, loss = 0.07263324\n",
      "Iteration 270, loss = 0.07249405\n",
      "Iteration 76, loss = 0.15331069\n",
      "Iteration 166, loss = 0.09606608\n",
      "Iteration 236, loss = 0.15607739\n",
      "Iteration 322, loss = 0.12602170\n",
      "Iteration 77, loss = 0.15212867\n",
      "Iteration 167, loss = 0.09577424\n",
      "Iteration 271, loss = 0.07235496\n",
      "Iteration 78, loss = 0.15097356\n",
      "Iteration 175, loss = 0.17626253\n",
      "Iteration 168, loss = 0.09548320\n",
      "Iteration 272, loss = 0.07221924\n",
      "Iteration 273, loss = 0.07207424\n",
      "Iteration 150, loss = 0.20252548\n",
      "Iteration 274, loss = 0.07193509\n",
      "Iteration 82, loss = 0.27976280\n",
      "Iteration 169, loss = 0.09519667\n",
      "Iteration 275, loss = 0.07179463\n",
      "Iteration 170, loss = 0.09490136\n",
      "Iteration 276, loss = 0.07166650\n",
      "Iteration 277, loss = 0.07151394\n",
      "Iteration 278, loss = 0.07138355\n",
      "Iteration 79, loss = 0.14984864\n",
      "Iteration 279, loss = 0.07125538\n",
      "Iteration 80, loss = 0.14878551\n",
      "Iteration 81, loss = 0.14772741\n",
      "Iteration 82, loss = 0.14668528\n",
      "Iteration 171, loss = 0.09462610\n",
      "Iteration 83, loss = 0.14564972\n",
      "Iteration 323, loss = 0.12598923\n",
      "Iteration 172, loss = 0.09434779\n",
      "Iteration 237, loss = 0.15592668\n",
      "Iteration 173, loss = 0.09406460\n",
      "Iteration 176, loss = 0.17589031\n",
      "Iteration 280, loss = 0.07111271\n",
      "Iteration 174, loss = 0.09377833\n",
      "Iteration 84, loss = 0.14465094\n",
      "Iteration 281, loss = 0.07098186\n",
      "Iteration 175, loss = 0.09352007\n",
      "Iteration 176, loss = 0.09324552\n",
      "Iteration 85, loss = 0.14368173\n",
      "Iteration 282, loss = 0.07083572\n",
      "Iteration 86, loss = 0.14275197\n",
      "Iteration 177, loss = 0.09296243\n",
      "Iteration 283, loss = 0.07071186\n",
      "Iteration 324, loss = 0.12590721\n",
      "Iteration 284, loss = 0.07056326\n",
      "Iteration 151, loss = 0.20220177\n",
      "Iteration 87, loss = 0.14182876\n",
      "Iteration 285, loss = 0.07044691\n",
      "Iteration 286, loss = 0.07031125\n",
      "Iteration 83, loss = 0.27869811\n",
      "Iteration 287, loss = 0.07017369\n",
      "Iteration 177, loss = 0.17553122\n",
      "Iteration 288, loss = 0.07004927\n",
      "Iteration 178, loss = 0.09270155\n",
      "Iteration 289, loss = 0.06990840\n",
      "Iteration 179, loss = 0.09245279\n",
      "Iteration 88, loss = 0.14091337\n",
      "Iteration 238, loss = 0.15574422\n",
      "Iteration 290, loss = 0.06978618\n",
      "Iteration 291, loss = 0.06965962\n",
      "Iteration 292, loss = 0.06953972\n",
      "Iteration 152, loss = 0.20193634\n",
      "Iteration 180, loss = 0.09219078\n",
      "Iteration 89, loss = 0.14001719\n",
      "Iteration 181, loss = 0.09193691\n",
      "Iteration 182, loss = 0.09165997\n",
      "Iteration 325, loss = 0.12584510\n",
      "Iteration 293, loss = 0.06941971\n",
      "Iteration 183, loss = 0.09141048\n",
      "Iteration 184, loss = 0.09116094\n",
      "Iteration 90, loss = 0.13916312\n",
      "Iteration 185, loss = 0.09091300\n",
      "Iteration 294, loss = 0.06928090\n",
      "Iteration 295, loss = 0.06917044\n",
      "Iteration 296, loss = 0.06904459\n",
      "Iteration 178, loss = 0.17519211\n",
      "Iteration 297, loss = 0.06890734\n",
      "Iteration 91, loss = 0.13828997\n",
      "Iteration 239, loss = 0.15557418\n",
      "Iteration 326, loss = 0.12574697\n",
      "Iteration 298, loss = 0.06879432\n",
      "Iteration 92, loss = 0.13745909\n",
      "Iteration 186, loss = 0.09067121\n",
      "Iteration 93, loss = 0.13664332\n",
      "Iteration 299, loss = 0.06868644\n",
      "Iteration 187, loss = 0.09041466\n",
      "Iteration 188, loss = 0.09016812\n",
      "Iteration 94, loss = 0.13583571\n",
      "Iteration 300, loss = 0.06854486\n",
      "Iteration 179, loss = 0.17483360\n",
      "Iteration 153, loss = 0.20163366\n",
      "Iteration 189, loss = 0.08993664\n",
      "Iteration 84, loss = 0.27759278\n",
      "Iteration 95, loss = 0.13505099\n",
      "Iteration 327, loss = 0.12569807\n",
      "Iteration 301, loss = 0.06843088\n",
      "Iteration 302, loss = 0.06830589\n",
      "Iteration 190, loss = 0.08969975\n",
      "Iteration 303, loss = 0.06819023\n",
      "Iteration 96, loss = 0.13427594\n",
      "Iteration 328, loss = 0.12561985\n",
      "Iteration 191, loss = 0.08945642\n",
      "Iteration 304, loss = 0.06806940\n",
      "Iteration 305, loss = 0.06796755\n",
      "Iteration 192, loss = 0.08924010\n",
      "Iteration 240, loss = 0.15542494\n",
      "Iteration 306, loss = 0.06785429\n",
      "Iteration 193, loss = 0.08898840\n",
      "Iteration 194, loss = 0.08877173\n",
      "Iteration 307, loss = 0.06774100\n",
      "Iteration 195, loss = 0.08853689\n",
      "Iteration 196, loss = 0.08831760\n",
      "Iteration 308, loss = 0.06762874\n",
      "Iteration 197, loss = 0.08808365\n",
      "Iteration 198, loss = 0.08785797\n",
      "Iteration 97, loss = 0.13351202\n",
      "Iteration 309, loss = 0.06750208\n",
      "Iteration 199, loss = 0.08763183\n",
      "Iteration 241, loss = 0.15528199\n",
      "Iteration 329, loss = 0.12556339\n",
      "Iteration 310, loss = 0.06739406\n",
      "Iteration 200, loss = 0.08743497\n",
      "Iteration 98, loss = 0.13278925\n",
      "Iteration 311, loss = 0.06728288\n",
      "Iteration 312, loss = 0.06717050\n",
      "Iteration 313, loss = 0.06706611\n",
      "Iteration 180, loss = 0.17456885\n",
      "Iteration 314, loss = 0.06695580\n",
      "Iteration 315, loss = 0.06686028Iteration 154, loss = 0.20130876\n",
      "Iteration 85, loss = 0.27654666\n",
      "\n",
      "Iteration 181, loss = 0.17415629\n",
      "Iteration 99, loss = 0.13205628\n",
      "Iteration 316, loss = 0.06673945\n",
      "Iteration 317, loss = 0.06663023\n",
      "Iteration 318, loss = 0.06652157\n",
      "Iteration 201, loss = 0.08720727\n",
      "Iteration 100, loss = 0.13134307\n",
      "Iteration 202, loss = 0.08700660\n",
      "Iteration 101, loss = 0.13064965\n",
      "Iteration 330, loss = 0.12548615Iteration 319, loss = 0.06641216\n",
      "Iteration 102, loss = 0.12997946\n",
      "\n",
      "Iteration 320, loss = 0.06630628\n",
      "Iteration 321, loss = 0.06620579\n",
      "Iteration 203, loss = 0.08677323\n",
      "Iteration 204, loss = 0.08657426\n",
      "Iteration 322, loss = 0.06609216\n",
      "Iteration 323, loss = 0.06600027\n",
      "Iteration 205, loss = 0.08637020\n",
      "Iteration 182, loss = 0.17381604\n",
      "Iteration 242, loss = 0.15510405\n",
      "Iteration 324, loss = 0.06588364\n",
      "Iteration 325, loss = 0.06577858\n",
      "Iteration 103, loss = 0.12927606\n",
      "Iteration 326, loss = 0.06568301\n",
      "Iteration 86, loss = 0.27548551\n",
      "Iteration 104, loss = 0.12861963\n",
      "Iteration 331, loss = 0.12542330\n",
      "Iteration 327, loss = 0.06558179\n",
      "Iteration 206, loss = 0.08614921\n",
      "Iteration 328, loss = 0.06549526\n",
      "Iteration 105, loss = 0.12798661\n",
      "Iteration 155, loss = 0.20106697\n",
      "Iteration 207, loss = 0.08595167\n",
      "Iteration 329, loss = 0.06537929\n",
      "Iteration 208, loss = 0.08573425\n",
      "Iteration 330, loss = 0.06528829\n",
      "Iteration 243, loss = 0.15497127\n",
      "Iteration 106, loss = 0.12734080\n",
      "Iteration 107, loss = 0.12669409\n",
      "Iteration 331, loss = 0.06518502\n",
      "Iteration 332, loss = 0.06509337\n",
      "Iteration 333, loss = 0.06499028\n",
      "Iteration 332, loss = 0.12536469\n",
      "Iteration 108, loss = 0.12609871\n",
      "Iteration 209, loss = 0.08553795\n",
      "Iteration 183, loss = 0.17350154\n",
      "Iteration 334, loss = 0.06489309\n",
      "Iteration 335, loss = 0.06480281\n",
      "Iteration 210, loss = 0.08534307\n",
      "Iteration 336, loss = 0.06470275\n",
      "Iteration 337, loss = 0.06460896\n",
      "Iteration 211, loss = 0.08512815\n",
      "Iteration 212, loss = 0.08493382\n",
      "Iteration 213, loss = 0.08474107\n",
      "Iteration 109, loss = 0.12548805\n",
      "Iteration 333, loss = 0.12529367\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 87, loss = 0.27450622\n",
      "Iteration 338, loss = 0.06451444\n",
      "Iteration 110, loss = 0.12489251\n",
      "Iteration 339, loss = 0.06442812\n",
      "Iteration 156, loss = 0.20074859\n",
      "Iteration 340, loss = 0.06433868\n",
      "Iteration 111, loss = 0.12430447\n",
      "Iteration 341, loss = 0.06423709\n",
      "Iteration 1, loss = 0.79789446\n",
      "Iteration 244, loss = 0.15478647\n",
      "Iteration 342, loss = 0.06414404\n",
      "Iteration 343, loss = 0.06405494\n",
      "Iteration 344, loss = 0.06395972\n",
      "Iteration 112, loss = 0.12372635\n",
      "Iteration 214, loss = 0.08454465\n",
      "Iteration 345, loss = 0.06387294\n",
      "Iteration 346, loss = 0.06378517\n",
      "Iteration 2, loss = 0.75877850\n",
      "Iteration 347, loss = 0.06370438\n",
      "Iteration 348, loss = 0.06360297\n",
      "Iteration 349, loss = 0.06351830\n",
      "Iteration 3, loss = 0.70755327\n",
      "Iteration 350, loss = 0.06344994\n",
      "Iteration 351, loss = 0.06333347\n",
      "Iteration 184, loss = 0.17323619\n",
      "Iteration 352, loss = 0.06325281\n",
      "Iteration 113, loss = 0.12314482\n",
      "Iteration 215, loss = 0.08434504\n",
      "Iteration 216, loss = 0.08415160\n",
      "Iteration 353, loss = 0.06316455\n",
      "Iteration 354, loss = 0.06307633\n",
      "Iteration 355, loss = 0.06298447\n",
      "Iteration 4, loss = 0.65650335\n",
      "Iteration 185, loss = 0.17285717\n",
      "Iteration 114, loss = 0.12258949\n",
      "Iteration 217, loss = 0.08395774\n",
      "Iteration 245, loss = 0.15464577\n",
      "Iteration 218, loss = 0.08376779\n",
      "Iteration 115, loss = 0.12204553\n",
      "Iteration 219, loss = 0.08357729\n",
      "Iteration 220, loss = 0.08339403\n",
      "Iteration 5, loss = 0.60995112\n",
      "Iteration 116, loss = 0.12150709\n",
      "Iteration 356, loss = 0.06289547\n",
      "Iteration 221, loss = 0.08320834\n",
      "Iteration 357, loss = 0.06281493\n",
      "Iteration 117, loss = 0.12096070\n",
      "Iteration 358, loss = 0.06272704\n",
      "Iteration 222, loss = 0.08302421\n",
      "Iteration 359, loss = 0.06263991\n",
      "Iteration 360, loss = 0.06255193\n",
      "Iteration 223, loss = 0.08285145\n",
      "Iteration 361, loss = 0.06247727\n",
      "Iteration 118, loss = 0.12043018\n",
      "Iteration 88, loss = 0.27354164\n",
      "Iteration 224, loss = 0.08266610\n",
      "Iteration 362, loss = 0.06239559\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 157, loss = 0.20045889\n",
      "Iteration 246, loss = 0.15447930Iteration 119, loss = 0.11991514\n",
      "Iteration 6, loss = 0.57055347\n",
      "\n",
      "Iteration 186, loss = 0.17256038Iteration 225, loss = 0.08248193\n",
      "\n",
      "Iteration 7, loss = 0.53595445\n",
      "Iteration 120, loss = 0.11939739\n",
      "Iteration 226, loss = 0.08230137\n",
      "Iteration 227, loss = 0.08211845\n",
      "Iteration 121, loss = 0.11891827\n",
      "Iteration 228, loss = 0.08194434\n",
      "Iteration 229, loss = 0.08178508\n",
      "Iteration 230, loss = 0.08160183\n",
      "Iteration 231, loss = 0.08143920\n",
      "Iteration 1, loss = 0.68267898\n",
      "Iteration 232, loss = 0.08126221\n",
      "Iteration 122, loss = 0.11839869\n",
      "Iteration 8, loss = 0.50679216\n",
      "Iteration 2, loss = 0.66102550\n",
      "Iteration 123, loss = 0.11794135\n",
      "Iteration 9, loss = 0.48230593\n",
      "Iteration 3, loss = 0.63215275\n",
      "Iteration 4, loss = 0.60214666\n",
      "Iteration 5, loss = 0.57351277\n",
      "Iteration 187, loss = 0.17224079\n",
      "Iteration 233, loss = 0.08109659\n",
      "Iteration 6, loss = 0.54871617\n",
      "Iteration 7, loss = 0.52603621\n",
      "Iteration 234, loss = 0.08092784\n",
      "Iteration 247, loss = 0.15433615\n",
      "Iteration 10, loss = 0.46074390\n",
      "Iteration 124, loss = 0.11742878\n",
      "Iteration 158, loss = 0.20018592\n",
      "Iteration 8, loss = 0.50607314\n",
      "Iteration 125, loss = 0.11696438\n",
      "Iteration 89, loss = 0.27260198\n",
      "Iteration 188, loss = 0.17200941\n",
      "Iteration 235, loss = 0.08074536\n",
      "Iteration 11, loss = 0.44187841\n",
      "Iteration 236, loss = 0.08058965\n",
      "Iteration 237, loss = 0.08041823\n",
      "Iteration 9, loss = 0.48861262\n",
      "Iteration 126, loss = 0.11649219\n",
      "Iteration 238, loss = 0.08024825\n",
      "Iteration 12, loss = 0.42497399\n",
      "Iteration 127, loss = 0.11602671\n",
      "Iteration 239, loss = 0.08009557\n",
      "Iteration 128, loss = 0.11556950\n",
      "Iteration 240, loss = 0.07992666\n",
      "Iteration 10, loss = 0.47325562\n",
      "Iteration 11, loss = 0.45974555\n",
      "Iteration 241, loss = 0.07975740\n",
      "Iteration 159, loss = 0.19993959\n",
      "Iteration 13, loss = 0.40999395\n",
      "Iteration 12, loss = 0.44735847\n",
      "Iteration 242, loss = 0.07960386\n",
      "Iteration 13, loss = 0.43657619\n",
      "Iteration 129, loss = 0.11512535\n",
      "Iteration 14, loss = 0.42676158\n",
      "Iteration 248, loss = 0.15416939\n",
      "Iteration 15, loss = 0.41767491\n",
      "Iteration 189, loss = 0.17164355\n",
      "Iteration 130, loss = 0.11469661\n",
      "Iteration 16, loss = 0.40937909\n",
      "Iteration 243, loss = 0.07944468\n",
      "Iteration 14, loss = 0.39607732\n",
      "Iteration 131, loss = 0.11423865\n",
      "Iteration 90, loss = 0.27164782\n",
      "Iteration 244, loss = 0.07927370\n",
      "Iteration 17, loss = 0.40167193\n",
      "Iteration 18, loss = 0.39449869\n",
      "Iteration 245, loss = 0.07912348\n",
      "Iteration 19, loss = 0.38775241\n",
      "Iteration 246, loss = 0.07896993\n",
      "Iteration 20, loss = 0.38144957\n",
      "Iteration 132, loss = 0.11380920Iteration 21, loss = 0.37550010Iteration 15, loss = 0.38339917\n",
      "\n",
      "\n",
      "Iteration 190, loss = 0.17138411\n",
      "Iteration 133, loss = 0.11337962\n",
      "Iteration 16, loss = 0.37179211Iteration 249, loss = 0.15401985\n",
      "\n",
      "Iteration 22, loss = 0.36978432\n",
      "Iteration 247, loss = 0.07881135\n",
      "Iteration 23, loss = 0.36433675\n",
      "Iteration 248, loss = 0.07866476\n",
      "Iteration 24, loss = 0.35909693\n",
      "Iteration 25, loss = 0.35406032\n",
      "Iteration 249, loss = 0.07850735\n",
      "Iteration 26, loss = 0.34918393\n",
      "Iteration 250, loss = 0.07834376\n",
      "Iteration 27, loss = 0.34461571\n",
      "Iteration 251, loss = 0.07819674\n",
      "Iteration 28, loss = 0.34006345\n",
      "Iteration 134, loss = 0.11296319\n",
      "Iteration 29, loss = 0.33570081\n",
      "Iteration 30, loss = 0.33137442\n",
      "Iteration 252, loss = 0.07803793\n",
      "Iteration 135, loss = 0.11255291\n",
      "Iteration 31, loss = 0.32723214\n",
      "Iteration 136, loss = 0.11213667\n",
      "Iteration 32, loss = 0.32309207\n",
      "Iteration 250, loss = 0.15388570\n",
      "Iteration 17, loss = 0.36072161\n",
      "Iteration 253, loss = 0.07789889\n",
      "Iteration 137, loss = 0.11173244\n",
      "Iteration 254, loss = 0.07774037\n",
      "Iteration 18, loss = 0.35045672\n",
      "Iteration 160, loss = 0.19969332\n",
      "Iteration 138, loss = 0.11133706\n",
      "Iteration 191, loss = 0.17107594\n",
      "Iteration 255, loss = 0.07759897\n",
      "Iteration 19, loss = 0.34076543\n",
      "Iteration 256, loss = 0.07744154\n",
      "Iteration 257, loss = 0.07730916\n",
      "Iteration 20, loss = 0.33149749\n",
      "Iteration 139, loss = 0.11093436\n",
      "Iteration 258, loss = 0.07716476\n",
      "Iteration 21, loss = 0.32285267\n",
      "Iteration 259, loss = 0.07700399\n",
      "Iteration 140, loss = 0.11055157\n",
      "Iteration 22, loss = 0.31455600Iteration 192, loss = 0.17079538\n",
      "\n",
      "Iteration 33, loss = 0.31914905\n",
      "Iteration 91, loss = 0.27078306\n",
      "Iteration 161, loss = 0.19935255\n",
      "Iteration 34, loss = 0.31525493\n",
      "Iteration 260, loss = 0.07686185\n",
      "Iteration 141, loss = 0.11016466\n",
      "Iteration 261, loss = 0.07671367\n",
      "Iteration 142, loss = 0.10977788\n",
      "Iteration 23, loss = 0.30678052\n",
      "Iteration 143, loss = 0.10941928\n",
      "Iteration 251, loss = 0.15370492\n",
      "Iteration 35, loss = 0.31134337\n",
      "Iteration 262, loss = 0.07656981\n",
      "Iteration 36, loss = 0.30757999\n",
      "Iteration 37, loss = 0.30376411\n",
      "Iteration 263, loss = 0.07643013\n",
      "Iteration 38, loss = 0.30008396\n",
      "Iteration 144, loss = 0.10902883\n",
      "Iteration 24, loss = 0.29920410\n",
      "Iteration 145, loss = 0.10866644\n",
      "Iteration 193, loss = 0.17050238\n",
      "Iteration 264, loss = 0.07628641\n",
      "Iteration 25, loss = 0.29208855\n",
      "Iteration 146, loss = 0.10830298\n",
      "Iteration 265, loss = 0.07614567\n",
      "Iteration 147, loss = 0.10794805\n",
      "Iteration 266, loss = 0.07601208\n",
      "Iteration 39, loss = 0.29639385\n",
      "Iteration 26, loss = 0.28539829\n",
      "Iteration 40, loss = 0.29276048\n",
      "Iteration 267, loss = 0.07587313\n",
      "Iteration 268, loss = 0.07572767Iteration 194, loss = 0.17022225\n",
      "Iteration 27, loss = 0.27883906\n",
      "Iteration 92, loss = 0.26999986\n",
      "Iteration 252, loss = 0.15356750\n",
      "Iteration 41, loss = 0.28905404\n",
      "Iteration 42, loss = 0.28546237\n",
      "\n",
      "Iteration 148, loss = 0.10757212\n",
      "Iteration 43, loss = 0.28186650\n",
      "Iteration 162, loss = 0.19913031\n",
      "Iteration 44, loss = 0.27837611\n",
      "Iteration 269, loss = 0.07558878\n",
      "Iteration 28, loss = 0.27270999\n",
      "Iteration 149, loss = 0.10723014\n",
      "Iteration 45, loss = 0.27487221\n",
      "Iteration 29, loss = 0.26676965\n",
      "Iteration 46, loss = 0.27140750\n",
      "Iteration 270, loss = 0.07545525\n",
      "Iteration 271, loss = 0.07531391\n",
      "Iteration 272, loss = 0.07518728\n",
      "Iteration 150, loss = 0.10688345\n",
      "Iteration 273, loss = 0.07503837\n",
      "Iteration 195, loss = 0.16994414\n",
      "Iteration 253, loss = 0.15343346\n",
      "Iteration 151, loss = 0.10652773\n",
      "Iteration 47, loss = 0.26808124\n",
      "Iteration 30, loss = 0.26118486\n",
      "Iteration 152, loss = 0.10619909\n",
      "Iteration 48, loss = 0.26466984\n",
      "Iteration 153, loss = 0.10585218\n",
      "Iteration 31, loss = 0.25584997\n",
      "Iteration 154, loss = 0.10551543\n",
      "Iteration 49, loss = 0.26125125\n",
      "Iteration 155, loss = 0.10518632\n",
      "Iteration 163, loss = 0.19885775\n",
      "Iteration 274, loss = 0.07491617\n",
      "Iteration 32, loss = 0.25062680\n",
      "Iteration 50, loss = 0.25800117\n",
      "Iteration 156, loss = 0.10486434\n",
      "Iteration 275, loss = 0.07478302\n",
      "Iteration 51, loss = 0.25486982\n",
      "Iteration 276, loss = 0.07463083\n",
      "Iteration 277, loss = 0.07450886\n",
      "Iteration 52, loss = 0.25160139\n",
      "Iteration 157, loss = 0.10453701\n",
      "Iteration 196, loss = 0.16969001\n",
      "Iteration 158, loss = 0.10425456\n",
      "Iteration 53, loss = 0.24841985\n",
      "Iteration 278, loss = 0.07438721\n",
      "Iteration 33, loss = 0.24562923\n",
      "Iteration 279, loss = 0.07423766\n",
      "Iteration 54, loss = 0.24531184\n",
      "Iteration 34, loss = 0.24090981\n",
      "Iteration 93, loss = 0.26906717\n",
      "Iteration 254, loss = 0.15327450\n",
      "Iteration 55, loss = 0.24219049\n",
      "Iteration 159, loss = 0.10390723\n",
      "Iteration 56, loss = 0.23910789\n",
      "Iteration 35, loss = 0.23641907\n",
      "Iteration 57, loss = 0.23617724\n",
      "Iteration 280, loss = 0.07410354\n",
      "Iteration 281, loss = 0.07398178\n",
      "Iteration 282, loss = 0.07386294\n",
      "Iteration 197, loss = 0.16943241\n",
      "Iteration 283, loss = 0.07371953\n",
      "Iteration 284, loss = 0.07359134\n",
      "Iteration 160, loss = 0.10359482\n",
      "Iteration 285, loss = 0.07345832\n",
      "Iteration 58, loss = 0.23329677\n",
      "Iteration 255, loss = 0.15314065\n",
      "Iteration 164, loss = 0.19855771\n",
      "Iteration 286, loss = 0.07333437\n",
      "Iteration 161, loss = 0.10329245\n",
      "Iteration 36, loss = 0.23218458\n",
      "Iteration 59, loss = 0.23028141\n",
      "Iteration 162, loss = 0.10298889\n",
      "Iteration 60, loss = 0.22745903\n",
      "Iteration 61, loss = 0.22460401\n",
      "Iteration 37, loss = 0.22790005\n",
      "Iteration 38, loss = 0.22389422\n",
      "Iteration 287, loss = 0.07322191\n",
      "Iteration 62, loss = 0.22183207\n",
      "Iteration 163, loss = 0.10268371\n",
      "Iteration 198, loss = 0.16916310\n",
      "Iteration 39, loss = 0.22012072\n",
      "Iteration 94, loss = 0.26830726\n",
      "Iteration 40, loss = 0.21638425\n",
      "Iteration 63, loss = 0.21908333\n",
      "Iteration 288, loss = 0.07307771\n",
      "Iteration 289, loss = 0.07294615\n",
      "Iteration 164, loss = 0.10237721\n",
      "Iteration 290, loss = 0.07282792\n",
      "Iteration 165, loss = 0.10209485\n",
      "Iteration 64, loss = 0.21634977\n",
      "Iteration 166, loss = 0.10180063\n",
      "Iteration 167, loss = 0.10150551\n",
      "Iteration 291, loss = 0.07270090\n",
      "Iteration 199, loss = 0.16893893\n",
      "Iteration 256, loss = 0.15298088\n",
      "Iteration 292, loss = 0.07258057\n",
      "Iteration 168, loss = 0.10123690\n",
      "Iteration 65, loss = 0.21366269\n",
      "Iteration 165, loss = 0.19829079\n",
      "Iteration 41, loss = 0.21292156\n",
      "Iteration 66, loss = 0.21111025\n",
      "Iteration 42, loss = 0.20957431\n",
      "Iteration 293, loss = 0.07244959\n",
      "Iteration 67, loss = 0.20853939\n",
      "Iteration 68, loss = 0.20600036\n",
      "Iteration 43, loss = 0.20626705\n",
      "Iteration 69, loss = 0.20346322\n",
      "Iteration 294, loss = 0.07234009\n",
      "Iteration 295, loss = 0.07221324\n",
      "Iteration 296, loss = 0.07209504\n",
      "Iteration 169, loss = 0.10094618\n",
      "Iteration 297, loss = 0.07198499\n",
      "Iteration 298, loss = 0.07184886\n",
      "Iteration 200, loss = 0.16864481\n",
      "Iteration 170, loss = 0.10066945\n",
      "Iteration 70, loss = 0.20101058\n",
      "Iteration 71, loss = 0.19865072\n",
      "Iteration 171, loss = 0.10038113\n",
      "Iteration 72, loss = 0.19632179\n",
      "Iteration 299, loss = 0.07173938\n",
      "Iteration 73, loss = 0.19399181\n",
      "Iteration 300, loss = 0.07163223\n",
      "Iteration 74, loss = 0.19171453\n",
      "Iteration 301, loss = 0.07150676\n",
      "Iteration 257, loss = 0.15283240\n",
      "Iteration 95, loss = 0.26757954\n",
      "Iteration 201, loss = 0.16841540\n",
      "Iteration 302, loss = 0.07138748Iteration 75, loss = 0.18953829\n",
      "Iteration 172, loss = 0.10011360\n",
      "Iteration 166, loss = 0.19806405\n",
      "\n",
      "Iteration 44, loss = 0.20315243\n",
      "Iteration 202, loss = 0.16815896\n",
      "Iteration 76, loss = 0.18739129\n",
      "Iteration 45, loss = 0.20022489\n",
      "Iteration 173, loss = 0.09984599\n",
      "Iteration 174, loss = 0.09956608\n",
      "Iteration 303, loss = 0.07127851\n",
      "Iteration 46, loss = 0.19728037\n",
      "Iteration 304, loss = 0.07116172\n",
      "Iteration 175, loss = 0.09932150\n",
      "Iteration 258, loss = 0.15271022\n",
      "Iteration 47, loss = 0.19446948\n",
      "Iteration 77, loss = 0.18533103\n",
      "Iteration 78, loss = 0.18327422\n",
      "Iteration 305, loss = 0.07104165\n",
      "Iteration 48, loss = 0.19181680\n",
      "Iteration 176, loss = 0.09903660\n",
      "Iteration 306, loss = 0.07093338\n",
      "Iteration 79, loss = 0.18125649\n",
      "Iteration 177, loss = 0.09876980\n",
      "Iteration 80, loss = 0.17928532\n",
      "Iteration 307, loss = 0.07082206\n",
      "Iteration 81, loss = 0.17736662\n",
      "Iteration 49, loss = 0.18926645\n",
      "Iteration 167, loss = 0.19779190\n",
      "Iteration 178, loss = 0.09851578\n",
      "Iteration 82, loss = 0.17555624\n",
      "Iteration 308, loss = 0.07070459\n",
      "Iteration 179, loss = 0.09826152\n",
      "Iteration 96, loss = 0.26671975\n",
      "Iteration 309, loss = 0.07059942\n",
      "Iteration 83, loss = 0.17376789\n",
      "Iteration 203, loss = 0.16789406\n",
      "Iteration 310, loss = 0.07048083\n",
      "Iteration 311, loss = 0.07037750\n",
      "Iteration 84, loss = 0.17199400\n",
      "Iteration 180, loss = 0.09800504\n",
      "Iteration 312, loss = 0.07026241\n",
      "Iteration 181, loss = 0.09776280\n",
      "Iteration 313, loss = 0.07016928\n",
      "Iteration 50, loss = 0.18677670\n",
      "Iteration 259, loss = 0.15254807\n",
      "Iteration 85, loss = 0.17030884\n",
      "Iteration 51, loss = 0.18440847\n",
      "Iteration 86, loss = 0.16863521\n",
      "Iteration 182, loss = 0.09751256\n",
      "Iteration 87, loss = 0.16700592\n",
      "Iteration 314, loss = 0.07006031\n",
      "Iteration 204, loss = 0.16765801\n",
      "Iteration 315, loss = 0.06994525\n",
      "Iteration 52, loss = 0.18210636\n",
      "Iteration 88, loss = 0.16540261\n",
      "Iteration 316, loss = 0.06983623\n",
      "Iteration 317, loss = 0.06973331\n",
      "Iteration 183, loss = 0.09726182\n",
      "Iteration 318, loss = 0.06962968\n",
      "Iteration 53, loss = 0.17987364\n",
      "Iteration 89, loss = 0.16386041\n",
      "Iteration 319, loss = 0.06952474\n",
      "Iteration 184, loss = 0.09702046\n",
      "Iteration 320, loss = 0.06941939\n",
      "Iteration 260, loss = 0.15243650\n",
      "Iteration 54, loss = 0.17772884\n",
      "Iteration 168, loss = 0.19753522\n",
      "Iteration 321, loss = 0.06931242\n",
      "Iteration 55, loss = 0.17567044\n",
      "Iteration 90, loss = 0.16231156\n",
      "Iteration 322, loss = 0.06921557\n",
      "Iteration 91, loss = 0.16083007\n",
      "Iteration 97, loss = 0.26585996\n",
      "Iteration 323, loss = 0.06910734\n",
      "Iteration 185, loss = 0.09678003\n",
      "Iteration 324, loss = 0.06901219\n",
      "Iteration 56, loss = 0.17367360\n",
      "Iteration 205, loss = 0.16742907\n",
      "Iteration 325, loss = 0.06889898\n",
      "Iteration 261, loss = 0.15228572\n",
      "Iteration 186, loss = 0.09653294\n",
      "Iteration 326, loss = 0.06881732\n",
      "Iteration 57, loss = 0.17172624\n",
      "Iteration 327, loss = 0.06869261\n",
      "Iteration 92, loss = 0.15937735\n",
      "Iteration 169, loss = 0.19730180\n",
      "Iteration 328, loss = 0.06859808\n",
      "Iteration 58, loss = 0.16988946\n",
      "Iteration 329, loss = 0.06850487\n",
      "Iteration 93, loss = 0.15799083\n",
      "Iteration 330, loss = 0.06839498\n",
      "Iteration 94, loss = 0.15656924\n",
      "Iteration 187, loss = 0.09629911\n",
      "Iteration 95, loss = 0.15517804\n",
      "Iteration 59, loss = 0.16806749\n",
      "Iteration 96, loss = 0.15384194\n",
      "Iteration 206, loss = 0.16720030\n",
      "Iteration 188, loss = 0.09605592\n",
      "Iteration 97, loss = 0.15258185\n",
      "Iteration 98, loss = 0.15130298\n",
      "Iteration 331, loss = 0.06830177\n",
      "Iteration 189, loss = 0.09582542\n",
      "Iteration 99, loss = 0.15002527\n",
      "Iteration 60, loss = 0.16635726\n",
      "Iteration 332, loss = 0.06820040\n",
      "Iteration 100, loss = 0.14881546\n",
      "Iteration 207, loss = 0.16697140\n",
      "Iteration 190, loss = 0.09559846\n",
      "Iteration 98, loss = 0.26524946\n",
      "Iteration 333, loss = 0.06810533\n",
      "Iteration 191, loss = 0.09536720\n",
      "Iteration 262, loss = 0.15214066\n",
      "Iteration 101, loss = 0.14759443\n",
      "Iteration 102, loss = 0.14642268\n",
      "Iteration 192, loss = 0.09513685\n",
      "Iteration 334, loss = 0.06801173\n",
      "Iteration 103, loss = 0.14529703\n",
      "Iteration 61, loss = 0.16466398\n",
      "Iteration 335, loss = 0.06793045\n",
      "Iteration 104, loss = 0.14419732\n",
      "Iteration 336, loss = 0.06782229\n",
      "Iteration 208, loss = 0.16677221\n",
      "Iteration 105, loss = 0.14308181\n",
      "Iteration 193, loss = 0.09490060\n",
      "Iteration 170, loss = 0.19706271\n",
      "Iteration 62, loss = 0.16300270\n",
      "Iteration 337, loss = 0.06774758\n",
      "Iteration 263, loss = 0.15202296\n",
      "Iteration 194, loss = 0.09467532\n",
      "Iteration 106, loss = 0.14202565\n",
      "Iteration 338, loss = 0.06763260\n",
      "Iteration 63, loss = 0.16144398\n",
      "Iteration 64, loss = 0.15991597\n",
      "Iteration 339, loss = 0.06753735\n",
      "Iteration 107, loss = 0.14094771\n",
      "Iteration 108, loss = 0.13993234\n",
      "Iteration 195, loss = 0.09444637\n",
      "Iteration 109, loss = 0.13891820\n",
      "Iteration 99, loss = 0.26440093\n",
      "Iteration 65, loss = 0.15842227\n",
      "Iteration 196, loss = 0.09423436\n",
      "Iteration 340, loss = 0.06746347\n",
      "Iteration 209, loss = 0.16653082\n",
      "Iteration 110, loss = 0.13794520\n",
      "Iteration 111, loss = 0.13699764\n",
      "Iteration 171, loss = 0.19685356\n",
      "Iteration 66, loss = 0.15698589\n",
      "Iteration 112, loss = 0.13603294\n",
      "Iteration 113, loss = 0.13511591\n",
      "Iteration 67, loss = 0.15560334\n",
      "Iteration 341, loss = 0.06735705\n",
      "Iteration 114, loss = 0.13418446\n",
      "Iteration 342, loss = 0.06726889\n",
      "Iteration 197, loss = 0.09402171\n",
      "Iteration 343, loss = 0.06717168\n",
      "Iteration 344, loss = 0.06708627\n",
      "Iteration 345, loss = 0.06699519\n",
      "Iteration 198, loss = 0.09378924\n",
      "Iteration 346, loss = 0.06690601\n",
      "Iteration 210, loss = 0.16628744\n",
      "Iteration 199, loss = 0.09358593\n",
      "Iteration 264, loss = 0.15187384\n",
      "Iteration 68, loss = 0.15421360\n",
      "Iteration 200, loss = 0.09335749\n",
      "Iteration 115, loss = 0.13332597\n",
      "Iteration 201, loss = 0.09314277\n",
      "Iteration 347, loss = 0.06681307\n",
      "Iteration 116, loss = 0.13242690\n",
      "Iteration 348, loss = 0.06672710\n",
      "Iteration 117, loss = 0.13159864\n",
      "Iteration 69, loss = 0.15293304\n",
      "Iteration 202, loss = 0.09293723\n",
      "Iteration 203, loss = 0.09273500\n",
      "Iteration 118, loss = 0.13075248\n",
      "Iteration 265, loss = 0.15173443\n",
      "Iteration 349, loss = 0.06663560\n",
      "Iteration 204, loss = 0.09251576\n",
      "Iteration 70, loss = 0.15161582\n",
      "Iteration 205, loss = 0.09231065\n",
      "Iteration 350, loss = 0.06654595\n",
      "Iteration 351, loss = 0.06645845\n",
      "Iteration 211, loss = 0.16605299\n",
      "Iteration 100, loss = 0.26370362\n",
      "Iteration 206, loss = 0.09211849\n",
      "Iteration 207, loss = 0.09190446\n",
      "Iteration 119, loss = 0.12994358\n",
      "Iteration 208, loss = 0.09171860\n",
      "Iteration 120, loss = 0.12913675\n",
      "Iteration 352, loss = 0.06637579\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 121, loss = 0.12831628\n",
      "Iteration 122, loss = 0.12755401\n",
      "Iteration 71, loss = 0.15035945\n",
      "Iteration 172, loss = 0.19664476\n",
      "Iteration 72, loss = 0.14914340\n",
      "Iteration 1, loss = 0.87243590\n",
      "Iteration 123, loss = 0.12679374\n",
      "Iteration 73, loss = 0.14797856\n",
      "Iteration 124, loss = 0.12600979\n",
      "Iteration 125, loss = 0.12528977\n",
      "Iteration 74, loss = 0.14679779\n",
      "Iteration 212, loss = 0.16585313\n",
      "Iteration 209, loss = 0.09151563\n",
      "Iteration 126, loss = 0.12456212\n",
      "Iteration 173, loss = 0.19633946\n",
      "Iteration 2, loss = 0.80591245\n",
      "Iteration 127, loss = 0.12384279\n",
      "Iteration 3, loss = 0.72470575\n",
      "Iteration 75, loss = 0.14569078\n",
      "Iteration 128, loss = 0.12316552\n",
      "Iteration 266, loss = 0.15160743\n",
      "Iteration 101, loss = 0.26300222\n",
      "Iteration 129, loss = 0.12243473\n",
      "Iteration 210, loss = 0.09133359\n",
      "Iteration 130, loss = 0.12176911\n",
      "Iteration 211, loss = 0.09110627\n",
      "Iteration 76, loss = 0.14459066\n",
      "Iteration 212, loss = 0.09094305\n",
      "Iteration 4, loss = 0.65048715\n",
      "Iteration 131, loss = 0.12110259\n",
      "Iteration 213, loss = 0.09072856\n",
      "Iteration 213, loss = 0.16565968\n",
      "Iteration 5, loss = 0.58956281\n",
      "Iteration 267, loss = 0.15147489\n",
      "Iteration 174, loss = 0.19610504\n",
      "Iteration 132, loss = 0.12041727\n",
      "Iteration 214, loss = 0.09054698\n",
      "Iteration 215, loss = 0.09035713\n",
      "Iteration 133, loss = 0.11976325\n",
      "Iteration 77, loss = 0.14352213\n",
      "Iteration 134, loss = 0.11916823\n",
      "Iteration 78, loss = 0.14249059\n",
      "Iteration 6, loss = 0.54169522\n",
      "Iteration 135, loss = 0.11851170\n",
      "Iteration 216, loss = 0.09016047\n",
      "Iteration 136, loss = 0.11789287\n",
      "Iteration 217, loss = 0.08998103\n",
      "Iteration 218, loss = 0.08979120\n",
      "Iteration 7, loss = 0.50656270\n",
      "Iteration 137, loss = 0.11727830\n",
      "Iteration 138, loss = 0.11664513\n",
      "Iteration 139, loss = 0.11606346\n",
      "Iteration 268, loss = 0.15134624\n",
      "Iteration 214, loss = 0.16543530\n",
      "Iteration 140, loss = 0.11546816\n",
      "Iteration 79, loss = 0.14149371\n",
      "Iteration 141, loss = 0.11486334\n",
      "Iteration 80, loss = 0.14047598\n",
      "Iteration 102, loss = 0.26230392\n",
      "Iteration 81, loss = 0.13954999\n",
      "Iteration 219, loss = 0.08960922\n",
      "Iteration 142, loss = 0.11429191\n",
      "Iteration 143, loss = 0.11370614\n",
      "Iteration 220, loss = 0.08942084\n",
      "Iteration 144, loss = 0.11318373\n",
      "Iteration 175, loss = 0.19588754\n",
      "Iteration 215, loss = 0.16522676\n",
      "Iteration 221, loss = 0.08924484\n",
      "Iteration 8, loss = 0.47830622\n",
      "Iteration 222, loss = 0.08908190\n",
      "Iteration 223, loss = 0.08888469\n",
      "Iteration 269, loss = 0.15122674\n",
      "Iteration 145, loss = 0.11262683\n",
      "Iteration 82, loss = 0.13858579\n",
      "Iteration 9, loss = 0.45643474\n",
      "Iteration 146, loss = 0.11208598\n",
      "Iteration 83, loss = 0.13766583\n",
      "Iteration 84, loss = 0.13679341\n",
      "Iteration 176, loss = 0.19562180\n",
      "Iteration 85, loss = 0.13589422\n",
      "Iteration 147, loss = 0.11153396\n",
      "Iteration 86, loss = 0.13506073\n",
      "Iteration 10, loss = 0.43873188\n",
      "Iteration 224, loss = 0.08870632\n",
      "Iteration 11, loss = 0.42458429\n",
      "Iteration 148, loss = 0.11100242\n",
      "Iteration 225, loss = 0.08853672\n",
      "Iteration 216, loss = 0.16502882\n",
      "Iteration 226, loss = 0.08835216\n",
      "Iteration 227, loss = 0.08818708\n",
      "Iteration 103, loss = 0.26164749\n",
      "Iteration 149, loss = 0.11046756\n",
      "Iteration 150, loss = 0.10995814\n",
      "Iteration 151, loss = 0.10945889\n",
      "Iteration 87, loss = 0.13422619\n",
      "Iteration 152, loss = 0.10896497\n",
      "Iteration 153, loss = 0.10846322\n",
      "Iteration 217, loss = 0.16480651\n",
      "Iteration 12, loss = 0.41177637\n",
      "Iteration 228, loss = 0.08800583\n",
      "Iteration 154, loss = 0.10795741Iteration 88, loss = 0.13340067\n",
      "\n",
      "Iteration 270, loss = 0.15108476\n",
      "Iteration 13, loss = 0.40087429\n",
      "Iteration 229, loss = 0.08786006\n",
      "Iteration 89, loss = 0.13259348\n",
      "Iteration 218, loss = 0.16461948\n",
      "Iteration 230, loss = 0.08766892\n",
      "Iteration 155, loss = 0.10750969\n",
      "Iteration 177, loss = 0.19543182\n",
      "Iteration 90, loss = 0.13182917\n",
      "Iteration 156, loss = 0.10702593\n",
      "Iteration 157, loss = 0.10653603\n",
      "Iteration 14, loss = 0.39093602\n",
      "Iteration 231, loss = 0.08750599\n",
      "Iteration 271, loss = 0.15096281\n",
      "Iteration 15, loss = 0.38181162\n",
      "Iteration 232, loss = 0.08734225\n",
      "Iteration 91, loss = 0.13105397\n",
      "Iteration 233, loss = 0.08716335\n",
      "Iteration 158, loss = 0.10609283\n",
      "Iteration 159, loss = 0.10560869\n",
      "Iteration 160, loss = 0.10514797\n",
      "Iteration 92, loss = 0.13031879\n",
      "Iteration 161, loss = 0.10470232\n",
      "Iteration 16, loss = 0.37323369\n",
      "Iteration 104, loss = 0.26094237\n",
      "Iteration 234, loss = 0.08701119\n",
      "Iteration 17, loss = 0.36515672\n",
      "Iteration 219, loss = 0.16441570\n",
      "Iteration 93, loss = 0.12957122\n",
      "Iteration 235, loss = 0.08683845\n",
      "Iteration 272, loss = 0.15083003\n",
      "Iteration 178, loss = 0.19519534\n",
      "Iteration 162, loss = 0.10424345\n",
      "Iteration 236, loss = 0.08668055\n",
      "Iteration 237, loss = 0.08652337\n",
      "Iteration 18, loss = 0.35735009\n",
      "Iteration 163, loss = 0.10383135\n",
      "Iteration 164, loss = 0.10338233\n",
      "Iteration 94, loss = 0.12886477\n",
      "Iteration 238, loss = 0.08635880\n",
      "Iteration 95, loss = 0.12815425\n",
      "Iteration 96, loss = 0.12745111\n",
      "Iteration 165, loss = 0.10293644\n",
      "Iteration 239, loss = 0.08619226\n",
      "Iteration 19, loss = 0.34968771\n",
      "Iteration 166, loss = 0.10251915\n",
      "Iteration 97, loss = 0.12680375\n",
      "Iteration 167, loss = 0.10211033\n",
      "Iteration 220, loss = 0.16422372\n",
      "Iteration 179, loss = 0.19496759\n",
      "Iteration 273, loss = 0.15071650\n",
      "Iteration 98, loss = 0.12612768\n",
      "Iteration 20, loss = 0.34241960\n",
      "Iteration 168, loss = 0.10169035\n",
      "Iteration 240, loss = 0.08603298\n",
      "Iteration 241, loss = 0.08587917\n",
      "Iteration 21, loss = 0.33554673\n",
      "Iteration 169, loss = 0.10128827\n",
      "Iteration 242, loss = 0.08572486\n",
      "Iteration 243, loss = 0.08556938\n",
      "Iteration 105, loss = 0.26038329\n",
      "Iteration 170, loss = 0.10086539\n",
      "Iteration 99, loss = 0.12547011\n",
      "Iteration 100, loss = 0.12484936\n",
      "Iteration 244, loss = 0.08541460\n",
      "Iteration 22, loss = 0.32851717\n",
      "Iteration 221, loss = 0.16402076\n",
      "Iteration 171, loss = 0.10047182\n",
      "Iteration 245, loss = 0.08525758\n",
      "Iteration 172, loss = 0.10009474\n",
      "Iteration 246, loss = 0.08511956\n",
      "Iteration 101, loss = 0.12421327\n",
      "Iteration 102, loss = 0.12358691\n",
      "Iteration 103, loss = 0.12300033\n",
      "Iteration 173, loss = 0.09969148\n",
      "Iteration 23, loss = 0.32175301\n",
      "Iteration 174, loss = 0.09930456\n",
      "Iteration 247, loss = 0.08496787\n",
      "Iteration 24, loss = 0.31532694\n",
      "Iteration 274, loss = 0.15059766\n",
      "Iteration 222, loss = 0.16383707\n",
      "Iteration 180, loss = 0.19473743\n",
      "Iteration 248, loss = 0.08480938\n",
      "Iteration 175, loss = 0.09893806\n",
      "Iteration 249, loss = 0.08466364\n",
      "Iteration 176, loss = 0.09855677\n",
      "Iteration 104, loss = 0.12239136\n",
      "Iteration 177, loss = 0.09820320\n",
      "Iteration 275, loss = 0.15046862\n",
      "Iteration 178, loss = 0.09785461\n",
      "Iteration 25, loss = 0.30891901\n",
      "Iteration 179, loss = 0.09746741\n",
      "Iteration 106, loss = 0.25975587\n",
      "Iteration 26, loss = 0.30285142\n",
      "Iteration 223, loss = 0.16366213\n",
      "Iteration 250, loss = 0.08452267\n",
      "Iteration 276, loss = 0.15034685\n",
      "Iteration 180, loss = 0.09712812\n",
      "Iteration 105, loss = 0.12181400\n",
      "Iteration 251, loss = 0.08437830\n",
      "Iteration 181, loss = 0.19455744\n",
      "Iteration 252, loss = 0.08422599\n",
      "Iteration 253, loss = 0.08408242\n",
      "Iteration 106, loss = 0.12123661\n",
      "Iteration 181, loss = 0.09676659\n",
      "Iteration 254, loss = 0.08392691\n",
      "Iteration 182, loss = 0.09643570\n",
      "Iteration 183, loss = 0.09608951\n",
      "Iteration 27, loss = 0.29683216\n",
      "Iteration 184, loss = 0.09575254\n",
      "Iteration 224, loss = 0.16348581\n",
      "Iteration 185, loss = 0.09541245\n",
      "Iteration 28, loss = 0.29107491\n",
      "Iteration 29, loss = 0.28541487\n",
      "Iteration 186, loss = 0.09508103\n",
      "Iteration 255, loss = 0.08379443\n",
      "Iteration 187, loss = 0.09475084\n",
      "Iteration 107, loss = 0.12067586\n",
      "Iteration 188, loss = 0.09443147\n",
      "Iteration 189, loss = 0.09411194\n",
      "Iteration 190, loss = 0.09379283\n",
      "Iteration 108, loss = 0.12013031\n",
      "Iteration 277, loss = 0.15021916\n",
      "Iteration 107, loss = 0.25903022\n",
      "Iteration 225, loss = 0.16328271\n",
      "Iteration 109, loss = 0.11959721\n",
      "Iteration 30, loss = 0.28013157\n",
      "Iteration 256, loss = 0.08365128\n",
      "Iteration 31, loss = 0.27468611\n",
      "Iteration 182, loss = 0.19432611\n",
      "Iteration 191, loss = 0.09350547\n",
      "Iteration 192, loss = 0.09317399\n",
      "Iteration 257, loss = 0.08351647\n",
      "Iteration 193, loss = 0.09286122\n",
      "Iteration 258, loss = 0.08336420\n",
      "Iteration 32, loss = 0.26958820\n",
      "Iteration 110, loss = 0.11905946\n",
      "Iteration 111, loss = 0.11852843\n",
      "Iteration 33, loss = 0.26467402\n",
      "Iteration 259, loss = 0.08323046\n",
      "Iteration 194, loss = 0.09258146\n",
      "Iteration 112, loss = 0.11802029\n",
      "Iteration 195, loss = 0.09225036\n",
      "Iteration 113, loss = 0.11753044\n",
      "Iteration 278, loss = 0.15010274\n",
      "Iteration 196, loss = 0.09196449\n",
      "Iteration 260, loss = 0.08308741\n",
      "Iteration 114, loss = 0.11703131\n",
      "Iteration 226, loss = 0.16307521\n",
      "Iteration 261, loss = 0.08295210\n",
      "Iteration 197, loss = 0.09166638\n",
      "Iteration 34, loss = 0.25990157\n",
      "Iteration 115, loss = 0.11653092\n",
      "Iteration 35, loss = 0.25517965\n",
      "Iteration 198, loss = 0.09138501\n",
      "Iteration 116, loss = 0.11605787\n",
      "Iteration 108, loss = 0.25842960\n",
      "Iteration 199, loss = 0.09106708\n",
      "Iteration 200, loss = 0.09081319Iteration 183, loss = 0.19410763\n",
      "\n",
      "Iteration 36, loss = 0.25078553\n",
      "Iteration 201, loss = 0.09050764\n",
      "Iteration 262, loss = 0.08282301\n",
      "Iteration 202, loss = 0.09026515\n",
      "Iteration 203, loss = 0.08996819\n",
      "Iteration 263, loss = 0.08267716\n",
      "Iteration 204, loss = 0.08966496\n",
      "Iteration 227, loss = 0.16293470\n",
      "Iteration 117, loss = 0.11559439\n",
      "Iteration 205, loss = 0.08939308\n",
      "Iteration 279, loss = 0.14998668\n",
      "Iteration 264, loss = 0.08253909\n",
      "Iteration 118, loss = 0.11512811\n",
      "Iteration 280, loss = 0.14986512\n",
      "Iteration 265, loss = 0.08240125\n",
      "Iteration 37, loss = 0.24644683\n",
      "Iteration 266, loss = 0.08227101\n",
      "Iteration 228, loss = 0.16273367\n",
      "Iteration 38, loss = 0.24228832\n",
      "Iteration 206, loss = 0.08913573\n",
      "Iteration 119, loss = 0.11467025\n",
      "Iteration 207, loss = 0.08886540\n",
      "Iteration 267, loss = 0.08214182\n",
      "Iteration 268, loss = 0.08199481\n",
      "Iteration 269, loss = 0.08187693\n",
      "Iteration 208, loss = 0.08859705\n",
      "Iteration 39, loss = 0.23827937\n",
      "Iteration 209, loss = 0.08837450\n",
      "Iteration 120, loss = 0.11424032\n",
      "Iteration 270, loss = 0.08175286\n",
      "Iteration 210, loss = 0.08808505\n",
      "Iteration 271, loss = 0.08160844\n",
      "Iteration 184, loss = 0.19389466\n",
      "Iteration 121, loss = 0.11378001\n",
      "Iteration 211, loss = 0.08783413\n",
      "Iteration 40, loss = 0.23438771\n",
      "Iteration 109, loss = 0.25783474\n",
      "Iteration 281, loss = 0.14974706\n",
      "Iteration 229, loss = 0.16255060\n",
      "Iteration 272, loss = 0.08149891\n",
      "Iteration 273, loss = 0.08136117\n",
      "Iteration 122, loss = 0.11333635\n",
      "Iteration 274, loss = 0.08122235\n",
      "Iteration 212, loss = 0.08757103\n",
      "Iteration 213, loss = 0.08731809\n",
      "Iteration 41, loss = 0.23069443\n",
      "Iteration 214, loss = 0.08707828\n",
      "Iteration 215, loss = 0.08684444\n",
      "Iteration 123, loss = 0.11290235\n",
      "Iteration 216, loss = 0.08658618\n",
      "Iteration 217, loss = 0.08635383Iteration 42, loss = 0.22699373\n",
      "Iteration 275, loss = 0.08108915\n",
      "Iteration 124, loss = 0.11249501\n",
      "\n",
      "Iteration 218, loss = 0.08612816\n",
      "Iteration 219, loss = 0.08586942\n",
      "Iteration 276, loss = 0.08097560\n",
      "Iteration 185, loss = 0.19369510\n",
      "Iteration 110, loss = 0.25727443\n",
      "Iteration 277, loss = 0.08084991\n",
      "Iteration 230, loss = 0.16246625\n",
      "Iteration 43, loss = 0.22370447\n",
      "Iteration 278, loss = 0.08070988\n",
      "Iteration 44, loss = 0.22020239\n",
      "Iteration 279, loss = 0.08060209\n",
      "Iteration 282, loss = 0.14963151\n",
      "Iteration 220, loss = 0.08565701\n",
      "Iteration 221, loss = 0.08541778\n",
      "Iteration 231, loss = 0.16220689\n",
      "Iteration 45, loss = 0.21699774\n",
      "Iteration 222, loss = 0.08517819\n",
      "Iteration 223, loss = 0.08494026\n",
      "Iteration 224, loss = 0.08473387\n",
      "Iteration 125, loss = 0.11208185\n",
      "Iteration 46, loss = 0.21384055\n",
      "Iteration 126, loss = 0.11165948\n",
      "Iteration 47, loss = 0.21089555\n",
      "Iteration 127, loss = 0.11125755\n",
      "Iteration 128, loss = 0.11086510\n",
      "Iteration 225, loss = 0.08452486\n",
      "Iteration 129, loss = 0.11047692\n",
      "Iteration 226, loss = 0.08429218\n",
      "Iteration 186, loss = 0.19351882\n",
      "Iteration 227, loss = 0.08406054\n",
      "Iteration 228, loss = 0.08383880\n",
      "Iteration 130, loss = 0.11009742\n",
      "Iteration 229, loss = 0.08363283\n",
      "Iteration 283, loss = 0.14952939\n",
      "Iteration 280, loss = 0.08046682\n",
      "Iteration 230, loss = 0.08342106\n",
      "Iteration 48, loss = 0.20804542\n",
      "Iteration 131, loss = 0.10970177\n",
      "Iteration 281, loss = 0.08034786\n",
      "Iteration 111, loss = 0.25667740\n",
      "Iteration 282, loss = 0.08021656\n",
      "Iteration 283, loss = 0.08009710\n",
      "Iteration 284, loss = 0.07997760\n",
      "Iteration 49, loss = 0.20524730\n",
      "Iteration 231, loss = 0.08319382\n",
      "Iteration 232, loss = 0.16202312\n",
      "Iteration 232, loss = 0.08298564\n",
      "Iteration 50, loss = 0.20258739\n",
      "Iteration 233, loss = 0.08279049\n",
      "Iteration 234, loss = 0.08259426\n",
      "Iteration 285, loss = 0.07985309\n",
      "Iteration 132, loss = 0.10932327\n",
      "Iteration 233, loss = 0.16186635\n",
      "Iteration 286, loss = 0.07973685\n",
      "Iteration 287, loss = 0.07963543\n",
      "Iteration 235, loss = 0.08238543\n",
      "Iteration 288, loss = 0.07950589\n",
      "Iteration 187, loss = 0.19330417\n",
      "Iteration 284, loss = 0.14942764\n",
      "Iteration 236, loss = 0.08217393\n",
      "Iteration 51, loss = 0.20005496\n",
      "Iteration 133, loss = 0.10895440\n",
      "Iteration 237, loss = 0.08198753\n",
      "Iteration 289, loss = 0.07938087\n",
      "Iteration 290, loss = 0.07927057\n",
      "Iteration 52, loss = 0.19748711\n",
      "Iteration 238, loss = 0.08178295\n",
      "Iteration 112, loss = 0.25608982\n",
      "Iteration 291, loss = 0.07915568\n",
      "Iteration 134, loss = 0.10860331\n",
      "Iteration 234, loss = 0.16168640\n",
      "Iteration 239, loss = 0.08160094\n",
      "Iteration 240, loss = 0.08141567\n",
      "Iteration 292, loss = 0.07903906\n",
      "Iteration 241, loss = 0.08120800\n",
      "Iteration 293, loss = 0.07891796\n",
      "Iteration 285, loss = 0.14929815\n",
      "Iteration 53, loss = 0.19513320\n",
      "Iteration 188, loss = 0.19307688\n",
      "Iteration 294, loss = 0.07879812\n",
      "Iteration 135, loss = 0.10823026\n",
      "Iteration 242, loss = 0.08101115\n",
      "Iteration 136, loss = 0.10786993\n",
      "Iteration 243, loss = 0.08081912\n",
      "Iteration 137, loss = 0.10754879\n",
      "Iteration 54, loss = 0.19287207\n",
      "Iteration 295, loss = 0.07869622\n",
      "Iteration 138, loss = 0.10718229\n",
      "Iteration 244, loss = 0.08063394\n",
      "Iteration 245, loss = 0.08045391\n",
      "Iteration 235, loss = 0.16153913\n",
      "Iteration 296, loss = 0.07858272\n",
      "Iteration 55, loss = 0.19054140\n",
      "Iteration 56, loss = 0.18843014\n",
      "Iteration 139, loss = 0.10684322\n",
      "Iteration 246, loss = 0.08025856\n",
      "Iteration 297, loss = 0.07846949\n",
      "Iteration 298, loss = 0.07836154\n",
      "Iteration 247, loss = 0.08006999\n",
      "Iteration 248, loss = 0.07988476\n",
      "Iteration 113, loss = 0.25552814\n",
      "Iteration 57, loss = 0.18638508\n",
      "Iteration 249, loss = 0.07971253\n",
      "Iteration 250, loss = 0.07951156\n",
      "Iteration 286, loss = 0.14920370\n",
      "Iteration 251, loss = 0.07934636\n",
      "Iteration 252, loss = 0.07915756Iteration 236, loss = 0.16136848\n",
      "Iteration 189, loss = 0.19287907\n",
      "Iteration 140, loss = 0.10649741\n",
      "Iteration 299, loss = 0.07823829\n",
      "\n",
      "Iteration 300, loss = 0.07814158\n",
      "Iteration 301, loss = 0.07802171\n",
      "Iteration 141, loss = 0.10616147\n",
      "Iteration 302, loss = 0.07791418\n",
      "Iteration 237, loss = 0.16118913\n",
      "Iteration 253, loss = 0.07898002\n",
      "Iteration 287, loss = 0.14907042\n",
      "Iteration 58, loss = 0.18440708\n",
      "Iteration 254, loss = 0.07878640\n",
      "Iteration 303, loss = 0.07781599\n",
      "Iteration 255, loss = 0.07860098\n",
      "Iteration 256, loss = 0.07844833\n",
      "Iteration 190, loss = 0.19266973\n",
      "Iteration 304, loss = 0.07771357\n",
      "Iteration 257, loss = 0.07824425\n",
      "Iteration 258, loss = 0.07808243\n",
      "Iteration 142, loss = 0.10584757\n",
      "Iteration 259, loss = 0.07789689\n",
      "Iteration 260, loss = 0.07773252\n",
      "Iteration 59, loss = 0.18252961\n",
      "Iteration 143, loss = 0.10551322\n",
      "Iteration 114, loss = 0.25497984\n",
      "Iteration 144, loss = 0.10520326\n",
      "Iteration 261, loss = 0.07755288\n",
      "Iteration 145, loss = 0.10488303\n",
      "Iteration 262, loss = 0.07736936\n",
      "Iteration 146, loss = 0.10457618\n",
      "Iteration 263, loss = 0.07718927\n",
      "Iteration 191, loss = 0.19249543\n",
      "Iteration 147, loss = 0.10426064\n",
      "Iteration 264, loss = 0.07701822\n",
      "Iteration 265, loss = 0.07685046\n",
      "Iteration 60, loss = 0.18066303\n",
      "Iteration 266, loss = 0.07668551\n",
      "Iteration 305, loss = 0.07759676\n",
      "Iteration 267, loss = 0.07652570\n",
      "Iteration 268, loss = 0.07636397\n",
      "Iteration 306, loss = 0.07748917Iteration 269, loss = 0.07618463\n",
      "\n",
      "Iteration 270, loss = 0.07603191\n",
      "Iteration 61, loss = 0.17884830\n",
      "Iteration 271, loss = 0.07587202\n",
      "Iteration 288, loss = 0.14895718\n",
      "Iteration 238, loss = 0.16104359\n",
      "Iteration 62, loss = 0.17712787\n",
      "Iteration 272, loss = 0.07570772\n",
      "Iteration 63, loss = 0.17547992\n",
      "Iteration 64, loss = 0.17390087\n",
      "Iteration 307, loss = 0.07738303\n",
      "Iteration 148, loss = 0.10395280\n",
      "Iteration 115, loss = 0.25439843\n",
      "Iteration 308, loss = 0.07728043\n",
      "Iteration 149, loss = 0.10366278\n",
      "Iteration 309, loss = 0.07717516\n",
      "Iteration 273, loss = 0.07554525\n",
      "Iteration 150, loss = 0.10335707\n",
      "Iteration 65, loss = 0.17234107\n",
      "Iteration 239, loss = 0.16085664\n",
      "Iteration 289, loss = 0.14886739\n",
      "Iteration 151, loss = 0.10305598\n",
      "Iteration 310, loss = 0.07707359\n",
      "Iteration 192, loss = 0.19232185\n",
      "Iteration 311, loss = 0.07696577\n",
      "Iteration 274, loss = 0.07537568\n",
      "Iteration 240, loss = 0.16073253\n",
      "Iteration 66, loss = 0.17084965\n",
      "Iteration 275, loss = 0.07521639\n",
      "Iteration 312, loss = 0.07686608\n",
      "Iteration 313, loss = 0.07676746\n",
      "Iteration 276, loss = 0.07507974\n",
      "Iteration 277, loss = 0.07490373\n",
      "Iteration 314, loss = 0.07666061\n",
      "Iteration 152, loss = 0.10277154\n",
      "Iteration 315, loss = 0.07655691\n",
      "Iteration 67, loss = 0.16943181\n",
      "Iteration 316, loss = 0.07646075\n",
      "Iteration 278, loss = 0.07475791\n",
      "Iteration 153, loss = 0.10248148\n",
      "Iteration 279, loss = 0.07461228\n",
      "Iteration 68, loss = 0.16798920\n",
      "Iteration 280, loss = 0.07445611\n",
      "Iteration 317, loss = 0.07635913\n",
      "Iteration 281, loss = 0.07429146\n",
      "Iteration 318, loss = 0.07625008Iteration 290, loss = 0.14873843\n",
      "\n",
      "Iteration 282, loss = 0.07414545\n",
      "Iteration 241, loss = 0.16057613\n",
      "Iteration 193, loss = 0.19210860\n",
      "Iteration 283, loss = 0.07399304\n",
      "Iteration 69, loss = 0.16666325\n",
      "Iteration 70, loss = 0.16536173\n",
      "Iteration 284, loss = 0.07384704\n",
      "Iteration 154, loss = 0.10221230\n",
      "Iteration 71, loss = 0.16410431\n",
      "Iteration 319, loss = 0.07616042\n",
      "Iteration 320, loss = 0.07606355\n",
      "Iteration 285, loss = 0.07370912\n",
      "Iteration 155, loss = 0.10192090\n",
      "Iteration 321, loss = 0.07596125\n",
      "Iteration 291, loss = 0.14864778\n",
      "Iteration 116, loss = 0.25388782\n",
      "Iteration 72, loss = 0.16286740\n",
      "Iteration 286, loss = 0.07355321\n",
      "Iteration 156, loss = 0.10164533\n",
      "Iteration 322, loss = 0.07586189\n",
      "Iteration 157, loss = 0.10136645\n",
      "Iteration 242, loss = 0.16040639\n",
      "Iteration 287, loss = 0.07342512\n",
      "Iteration 292, loss = 0.14852070\n",
      "Iteration 73, loss = 0.16170392\n",
      "Iteration 158, loss = 0.10110731\n",
      "Iteration 159, loss = 0.10083272\n",
      "Iteration 74, loss = 0.16057200\n",
      "Iteration 288, loss = 0.07327454\n",
      "Iteration 194, loss = 0.19188702\n",
      "Iteration 323, loss = 0.07575359\n",
      "Iteration 289, loss = 0.07315405\n",
      "Iteration 324, loss = 0.07566807\n",
      "Iteration 290, loss = 0.07300999\n",
      "Iteration 325, loss = 0.07557323\n",
      "Iteration 160, loss = 0.10056511\n",
      "Iteration 243, loss = 0.16027860\n",
      "Iteration 291, loss = 0.07285897\n",
      "Iteration 326, loss = 0.07546389\n",
      "Iteration 292, loss = 0.07272176\n",
      "Iteration 327, loss = 0.07537154\n",
      "Iteration 293, loss = 0.07258041\n",
      "Iteration 75, loss = 0.15945043\n",
      "Iteration 294, loss = 0.07244394\n",
      "Iteration 328, loss = 0.07528266\n",
      "Iteration 295, loss = 0.07231132\n",
      "Iteration 329, loss = 0.07519291\n",
      "Iteration 296, loss = 0.07217668\n",
      "Iteration 297, loss = 0.07203999\n",
      "Iteration 330, loss = 0.07509460\n",
      "Iteration 161, loss = 0.10030455\n",
      "Iteration 76, loss = 0.15837479\n",
      "Iteration 293, loss = 0.14842969\n",
      "Iteration 244, loss = 0.16014709\n",
      "Iteration 298, loss = 0.07190661Iteration 117, loss = 0.25333621\n",
      "\n",
      "Iteration 195, loss = 0.19172824\n",
      "Iteration 299, loss = 0.07177397\n",
      "Iteration 331, loss = 0.07499589\n",
      "Iteration 77, loss = 0.15730914\n",
      "Iteration 300, loss = 0.07164177\n",
      "Iteration 332, loss = 0.07490453\n",
      "Iteration 301, loss = 0.07151579\n",
      "Iteration 78, loss = 0.15631626\n",
      "Iteration 333, loss = 0.07480746\n",
      "Iteration 302, loss = 0.07138922\n",
      "Iteration 162, loss = 0.10004967\n",
      "Iteration 79, loss = 0.15530117\n",
      "Iteration 303, loss = 0.07126914\n",
      "Iteration 163, loss = 0.09980632\n",
      "Iteration 304, loss = 0.07111812\n",
      "Iteration 305, loss = 0.07101779\n",
      "Iteration 306, loss = 0.07087004\n",
      "Iteration 334, loss = 0.07471130\n",
      "Iteration 164, loss = 0.09955012\n",
      "Iteration 245, loss = 0.15994770\n",
      "Iteration 118, loss = 0.25284284\n",
      "Iteration 80, loss = 0.15434539\n",
      "Iteration 335, loss = 0.07462962\n",
      "Iteration 196, loss = 0.19152689\n",
      "Iteration 165, loss = 0.09928350\n",
      "Iteration 307, loss = 0.07074983\n",
      "Iteration 336, loss = 0.07454126\n",
      "Iteration 308, loss = 0.07061889\n",
      "Iteration 309, loss = 0.07049353\n",
      "Iteration 337, loss = 0.07444191\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 294, loss = 0.14832446\n",
      "Iteration 81, loss = 0.15341231\n",
      "Iteration 246, loss = 0.15979940Iteration 166, loss = 0.09904644\n",
      "\n",
      "Iteration 167, loss = 0.09881509\n",
      "Iteration 310, loss = 0.07036828\n",
      "Iteration 168, loss = 0.09856518\n",
      "Iteration 82, loss = 0.15248607\n",
      "Iteration 311, loss = 0.07026202\n",
      "Iteration 295, loss = 0.14822542\n",
      "Iteration 312, loss = 0.07012976\n",
      "Iteration 169, loss = 0.09832536\n",
      "Iteration 247, loss = 0.15964372Iteration 313, loss = 0.07000413\n",
      "\n",
      "Iteration 83, loss = 0.15162164\n",
      "Iteration 197, loss = 0.19132797\n",
      "Iteration 1, loss = 0.86555514\n",
      "Iteration 314, loss = 0.06986444\n",
      "Iteration 170, loss = 0.09807862\n",
      "Iteration 119, loss = 0.25235783\n",
      "Iteration 84, loss = 0.15073221\n",
      "Iteration 171, loss = 0.09784667\n",
      "Iteration 315, loss = 0.06975104\n",
      "Iteration 296, loss = 0.14810119\n",
      "Iteration 316, loss = 0.06962220\n",
      "Iteration 248, loss = 0.15952806\n",
      "Iteration 317, loss = 0.06950672\n",
      "Iteration 318, loss = 0.06937779\n",
      "Iteration 2, loss = 0.81600132\n",
      "Iteration 172, loss = 0.09762536\n",
      "Iteration 85, loss = 0.14989829Iteration 319, loss = 0.06927358\n",
      "Iteration 320, loss = 0.06915243\n",
      "\n",
      "Iteration 173, loss = 0.09738664\n",
      "Iteration 321, loss = 0.06901166\n",
      "Iteration 174, loss = 0.09716926\n",
      "Iteration 175, loss = 0.09694381\n",
      "Iteration 322, loss = 0.06893320\n",
      "Iteration 323, loss = 0.06878905\n",
      "Iteration 86, loss = 0.14906293\n",
      "Iteration 324, loss = 0.06869578\n",
      "Iteration 325, loss = 0.06856141\n",
      "Iteration 87, loss = 0.14828561\n",
      "Iteration 249, loss = 0.15935994\n",
      "Iteration 3, loss = 0.75328872\n",
      "Iteration 326, loss = 0.06843413\n",
      "Iteration 297, loss = 0.14801469\n",
      "Iteration 198, loss = 0.19115020\n",
      "Iteration 327, loss = 0.06832397\n",
      "Iteration 88, loss = 0.14754052\n",
      "Iteration 328, loss = 0.06822239\n",
      "Iteration 176, loss = 0.09671319\n",
      "Iteration 4, loss = 0.69362083\n",
      "Iteration 177, loss = 0.09649353\n",
      "Iteration 329, loss = 0.06810179\n",
      "Iteration 330, loss = 0.06798220\n",
      "Iteration 331, loss = 0.06787064\n",
      "Iteration 250, loss = 0.15928059\n",
      "Iteration 332, loss = 0.06776843\n",
      "Iteration 333, loss = 0.06766866\n",
      "Iteration 120, loss = 0.25179348Iteration 334, loss = 0.06754235\n",
      "\n",
      "Iteration 89, loss = 0.14672751\n",
      "Iteration 5, loss = 0.64144700\n",
      "Iteration 199, loss = 0.19096301\n",
      "Iteration 178, loss = 0.09628417\n",
      "Iteration 90, loss = 0.14595378\n",
      "Iteration 335, loss = 0.06744071\n",
      "Iteration 251, loss = 0.15910838\n",
      "Iteration 179, loss = 0.09606177\n",
      "Iteration 298, loss = 0.14791083\n",
      "Iteration 336, loss = 0.06733376\n",
      "Iteration 6, loss = 0.59945334\n",
      "Iteration 91, loss = 0.14522651\n",
      "Iteration 180, loss = 0.09584422\n",
      "Iteration 121, loss = 0.25132696\n",
      "Iteration 337, loss = 0.06722179\n",
      "Iteration 181, loss = 0.09566592\n",
      "Iteration 338, loss = 0.06712362\n",
      "Iteration 92, loss = 0.14453071\n",
      "Iteration 200, loss = 0.19078539\n",
      "Iteration 339, loss = 0.06700850\n",
      "Iteration 299, loss = 0.14781326\n",
      "Iteration 340, loss = 0.06691319\n",
      "Iteration 252, loss = 0.15896581\n",
      "Iteration 182, loss = 0.09543462\n",
      "Iteration 341, loss = 0.06681170\n",
      "Iteration 93, loss = 0.14381893\n",
      "Iteration 342, loss = 0.06670265\n",
      "Iteration 7, loss = 0.56427033\n",
      "Iteration 343, loss = 0.06659778\n",
      "Iteration 300, loss = 0.14770760\n",
      "Iteration 94, loss = 0.14313693\n",
      "Iteration 344, loss = 0.06649055\n",
      "Iteration 183, loss = 0.09523176\n",
      "Iteration 345, loss = 0.06639616\n",
      "Iteration 95, loss = 0.14245071\n",
      "Iteration 184, loss = 0.09501933\n",
      "Iteration 8, loss = 0.53658329\n",
      "Iteration 96, loss = 0.14182737\n",
      "Iteration 185, loss = 0.09483334\n",
      "Iteration 346, loss = 0.06628162\n",
      "Iteration 347, loss = 0.06619563\n",
      "Iteration 97, loss = 0.14115698\n",
      "Iteration 348, loss = 0.06608439\n",
      "Iteration 349, loss = 0.06600435\n",
      "Iteration 253, loss = 0.15881158\n",
      "Iteration 350, loss = 0.06589907\n",
      "Iteration 351, loss = 0.06579235\n",
      "Iteration 352, loss = 0.06569451\n",
      "Iteration 301, loss = 0.14759764\n",
      "Iteration 98, loss = 0.14053205\n",
      "Iteration 353, loss = 0.06559206\n",
      "Iteration 354, loss = 0.06548830\n",
      "Iteration 9, loss = 0.51352578\n",
      "Iteration 186, loss = 0.09461748\n",
      "Iteration 254, loss = 0.15868680\n",
      "Iteration 187, loss = 0.09441910\n",
      "Iteration 188, loss = 0.09422472\n",
      "Iteration 201, loss = 0.19058065\n",
      "Iteration 99, loss = 0.13990347\n",
      "Iteration 122, loss = 0.25081550\n",
      "Iteration 355, loss = 0.06540763\n",
      "Iteration 189, loss = 0.09403265\n",
      "Iteration 356, loss = 0.06532799\n",
      "Iteration 357, loss = 0.06523620\n",
      "Iteration 358, loss = 0.06513861\n",
      "Iteration 359, loss = 0.06502768\n",
      "Iteration 10, loss = 0.49390642\n",
      "Iteration 360, loss = 0.06493066\n",
      "Iteration 302, loss = 0.14751349\n",
      "Iteration 190, loss = 0.09383187\n",
      "Iteration 255, loss = 0.15855494\n",
      "Iteration 100, loss = 0.13931368\n",
      "Iteration 202, loss = 0.19047542\n",
      "Iteration 101, loss = 0.13872151\n",
      "Iteration 191, loss = 0.09364373\n",
      "Iteration 192, loss = 0.09345525\n",
      "Iteration 361, loss = 0.06486535\n",
      "Iteration 102, loss = 0.13813740\n",
      "Iteration 362, loss = 0.06473601\n",
      "Iteration 303, loss = 0.14741519\n",
      "Iteration 363, loss = 0.06464775\n",
      "Iteration 11, loss = 0.47727354\n",
      "Iteration 364, loss = 0.06454345\n",
      "Iteration 193, loss = 0.09326130\n",
      "Iteration 256, loss = 0.15841578\n",
      "Iteration 12, loss = 0.46275329\n",
      "Iteration 365, loss = 0.06446683\n",
      "Iteration 366, loss = 0.06437808\n",
      "Iteration 103, loss = 0.13755701\n",
      "Iteration 367, loss = 0.06428276\n",
      "Iteration 194, loss = 0.09307773\n",
      "Iteration 123, loss = 0.25039624\n",
      "Iteration 104, loss = 0.13701925\n",
      "Iteration 195, loss = 0.09289500\n",
      "Iteration 257, loss = 0.15826095\n",
      "Iteration 304, loss = 0.14732341\n",
      "Iteration 368, loss = 0.06419268\n",
      "Iteration 196, loss = 0.09271361\n",
      "Iteration 203, loss = 0.19023231\n",
      "Iteration 13, loss = 0.44992515\n",
      "Iteration 105, loss = 0.13643287\n",
      "Iteration 369, loss = 0.06412080\n",
      "Iteration 370, loss = 0.06402722\n",
      "Iteration 106, loss = 0.13589739\n",
      "Iteration 371, loss = 0.06393178\n",
      "Iteration 197, loss = 0.09253581\n",
      "Iteration 372, loss = 0.06384460\n",
      "Iteration 198, loss = 0.09234713\n",
      "Iteration 107, loss = 0.13535798\n",
      "Iteration 258, loss = 0.15815378\n",
      "Iteration 373, loss = 0.06376663\n",
      "Iteration 108, loss = 0.13484966\n",
      "Iteration 199, loss = 0.09217244\n",
      "Iteration 305, loss = 0.14722284\n",
      "Iteration 374, loss = 0.06367071\n",
      "Iteration 14, loss = 0.43846208\n",
      "Iteration 375, loss = 0.06357690\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 124, loss = 0.24992733\n",
      "Iteration 200, loss = 0.09199332\n",
      "Iteration 204, loss = 0.19009894\n",
      "Iteration 109, loss = 0.13430765\n",
      "Iteration 1, loss = 0.81424768\n",
      "Iteration 201, loss = 0.09181710\n",
      "Iteration 259, loss = 0.15802442\n",
      "Iteration 2, loss = 0.78642845\n",
      "Iteration 110, loss = 0.13381454\n",
      "Iteration 111, loss = 0.13334120\n",
      "Iteration 15, loss = 0.42806779\n",
      "Iteration 3, loss = 0.74899393\n",
      "Iteration 306, loss = 0.14712575\n",
      "Iteration 202, loss = 0.09164730\n",
      "Iteration 4, loss = 0.71091223\n",
      "Iteration 205, loss = 0.18990823\n",
      "Iteration 203, loss = 0.09147822\n",
      "Iteration 16, loss = 0.41821312\n",
      "Iteration 260, loss = 0.15788902\n",
      "Iteration 204, loss = 0.09130490\n",
      "Iteration 112, loss = 0.13281399\n",
      "Iteration 125, loss = 0.24944877\n",
      "Iteration 5, loss = 0.67391091\n",
      "Iteration 113, loss = 0.13236358\n",
      "Iteration 6, loss = 0.64079111\n",
      "Iteration 7, loss = 0.61141707\n",
      "Iteration 205, loss = 0.09113165\n",
      "Iteration 8, loss = 0.58511666\n",
      "Iteration 9, loss = 0.56189380\n",
      "Iteration 114, loss = 0.13186402\n",
      "Iteration 10, loss = 0.54048693\n",
      "Iteration 17, loss = 0.40918360\n",
      "Iteration 307, loss = 0.14701558\n",
      "Iteration 11, loss = 0.52190655\n",
      "Iteration 261, loss = 0.15774318\n",
      "Iteration 115, loss = 0.13139906\n",
      "Iteration 206, loss = 0.18973125\n",
      "Iteration 18, loss = 0.40077037\n",
      "Iteration 206, loss = 0.09096277\n",
      "Iteration 12, loss = 0.50451656\n",
      "Iteration 207, loss = 0.09079847\n",
      "Iteration 13, loss = 0.48966713\n",
      "Iteration 14, loss = 0.47544116\n",
      "Iteration 15, loss = 0.46299820\n",
      "Iteration 208, loss = 0.09062798\n",
      "Iteration 308, loss = 0.14692302\n",
      "Iteration 116, loss = 0.13094108\n",
      "Iteration 19, loss = 0.39250372\n",
      "Iteration 117, loss = 0.13047282\n",
      "Iteration 209, loss = 0.09047144\n",
      "Iteration 262, loss = 0.15763743\n",
      "Iteration 16, loss = 0.45172057\n",
      "Iteration 20, loss = 0.38493474\n",
      "Iteration 118, loss = 0.13003938\n",
      "Iteration 17, loss = 0.44140584\n",
      "Iteration 210, loss = 0.09031664\n",
      "Iteration 18, loss = 0.43225361\n",
      "Iteration 309, loss = 0.14684174\n",
      "Iteration 19, loss = 0.42379426\n",
      "Iteration 20, loss = 0.41628576\n",
      "Iteration 211, loss = 0.09014652\n",
      "Iteration 21, loss = 0.40924330\n",
      "Iteration 207, loss = 0.18956709\n",
      "Iteration 22, loss = 0.40288800\n",
      "Iteration 126, loss = 0.24896628\n",
      "Iteration 263, loss = 0.15750979\n",
      "Iteration 119, loss = 0.12958957\n",
      "Iteration 212, loss = 0.08998932\n",
      "Iteration 310, loss = 0.14673979\n",
      "Iteration 120, loss = 0.12913450\n",
      "Iteration 213, loss = 0.08982519\n",
      "Iteration 21, loss = 0.37732489\n",
      "Iteration 23, loss = 0.39686062\n",
      "Iteration 24, loss = 0.39142133\n",
      "Iteration 214, loss = 0.08967339\n",
      "Iteration 264, loss = 0.15737326\n",
      "Iteration 121, loss = 0.12872832\n",
      "Iteration 25, loss = 0.38635127\n",
      "Iteration 215, loss = 0.08951573\n",
      "Iteration 26, loss = 0.38140089\n",
      "Iteration 22, loss = 0.37020996\n",
      "Iteration 27, loss = 0.37690390\n",
      "Iteration 311, loss = 0.14665240\n",
      "Iteration 208, loss = 0.18939311\n",
      "Iteration 122, loss = 0.12828861\n",
      "Iteration 216, loss = 0.08936670\n",
      "Iteration 217, loss = 0.08920378\n",
      "Iteration 123, loss = 0.12787324\n",
      "Iteration 265, loss = 0.15724825\n",
      "Iteration 28, loss = 0.37247627\n",
      "Iteration 127, loss = 0.24849095\n",
      "Iteration 29, loss = 0.36826364\n",
      "Iteration 30, loss = 0.36425536\n",
      "Iteration 23, loss = 0.36348535\n",
      "Iteration 124, loss = 0.12745746\n",
      "Iteration 24, loss = 0.35684000\n",
      "Iteration 125, loss = 0.12706790\n",
      "Iteration 31, loss = 0.36026460\n",
      "Iteration 32, loss = 0.35657575\n",
      "Iteration 218, loss = 0.08905675\n",
      "Iteration 312, loss = 0.14656097\n",
      "Iteration 126, loss = 0.12665999\n",
      "Iteration 219, loss = 0.08889851\n",
      "Iteration 266, loss = 0.15713852\n",
      "Iteration 209, loss = 0.18922697\n",
      "Iteration 33, loss = 0.35279635\n",
      "Iteration 220, loss = 0.08877568\n",
      "Iteration 25, loss = 0.35056440\n",
      "Iteration 221, loss = 0.08860413\n",
      "Iteration 127, loss = 0.12626101\n",
      "Iteration 222, loss = 0.08844988\n",
      "Iteration 128, loss = 0.24801537\n",
      "Iteration 34, loss = 0.34917566\n",
      "Iteration 313, loss = 0.14645991\n",
      "Iteration 35, loss = 0.34564466\n",
      "Iteration 128, loss = 0.12586940\n",
      "Iteration 26, loss = 0.34433696\n",
      "Iteration 267, loss = 0.15701418\n",
      "Iteration 129, loss = 0.12548310\n",
      "Iteration 223, loss = 0.08831293\n",
      "Iteration 36, loss = 0.34222133\n",
      "Iteration 210, loss = 0.18908246\n",
      "Iteration 224, loss = 0.08815232\n",
      "Iteration 225, loss = 0.08802580\n",
      "Iteration 268, loss = 0.15689406\n",
      "Iteration 37, loss = 0.33868335\n",
      "Iteration 38, loss = 0.33530139\n",
      "Iteration 130, loss = 0.12511226\n",
      "Iteration 314, loss = 0.14637608\n",
      "Iteration 27, loss = 0.33839111\n",
      "Iteration 39, loss = 0.33192113\n",
      "Iteration 226, loss = 0.08787085\n",
      "Iteration 40, loss = 0.32850969\n",
      "Iteration 131, loss = 0.12471247\n",
      "Iteration 132, loss = 0.12434788\n",
      "Iteration 315, loss = 0.14629030\n",
      "Iteration 133, loss = 0.12398789\n",
      "Iteration 41, loss = 0.32519970\n",
      "Iteration 269, loss = 0.15676202\n",
      "Iteration 42, loss = 0.32179882\n",
      "Iteration 134, loss = 0.12361956\n",
      "Iteration 43, loss = 0.31858261\n",
      "Iteration 227, loss = 0.08772383\n",
      "Iteration 211, loss = 0.18890561\n",
      "Iteration 129, loss = 0.24755168\n",
      "Iteration 228, loss = 0.08759545Iteration 28, loss = 0.33259612\n",
      "\n",
      "Iteration 44, loss = 0.31517046\n",
      "Iteration 29, loss = 0.32701409\n",
      "Iteration 45, loss = 0.31193146\n",
      "Iteration 229, loss = 0.08744780\n",
      "Iteration 46, loss = 0.30861056\n",
      "Iteration 47, loss = 0.30537566\n",
      "Iteration 270, loss = 0.15664653\n",
      "Iteration 316, loss = 0.14619663\n",
      "Iteration 48, loss = 0.30213131\n",
      "Iteration 135, loss = 0.12325636\n",
      "Iteration 30, loss = 0.32161386\n",
      "Iteration 49, loss = 0.29881057\n",
      "Iteration 50, loss = 0.29555677\n",
      "Iteration 230, loss = 0.08730238\n",
      "Iteration 212, loss = 0.18875613\n",
      "Iteration 51, loss = 0.29240976\n",
      "Iteration 231, loss = 0.08717547\n",
      "Iteration 52, loss = 0.28912498\n",
      "Iteration 232, loss = 0.08702895\n",
      "Iteration 317, loss = 0.14611977\n",
      "Iteration 130, loss = 0.24713063\n",
      "Iteration 136, loss = 0.12291241\n",
      "Iteration 271, loss = 0.15653782\n",
      "Iteration 137, loss = 0.12256965\n",
      "Iteration 31, loss = 0.31641572\n",
      "Iteration 233, loss = 0.08689546\n",
      "Iteration 53, loss = 0.28583308\n",
      "Iteration 138, loss = 0.12222294\n",
      "Iteration 272, loss = 0.15643029\n",
      "Iteration 54, loss = 0.28259378\n",
      "Iteration 234, loss = 0.08676984\n",
      "Iteration 235, loss = 0.08664394\n",
      "Iteration 213, loss = 0.18859977\n",
      "Iteration 236, loss = 0.08650518\n",
      "Iteration 55, loss = 0.27943611\n",
      "Iteration 139, loss = 0.12189188\n",
      "Iteration 56, loss = 0.27626923\n",
      "Iteration 318, loss = 0.14603311\n",
      "Iteration 237, loss = 0.08637596\n",
      "Iteration 273, loss = 0.15630561\n",
      "Iteration 32, loss = 0.31122747\n",
      "Iteration 57, loss = 0.27304144\n",
      "Iteration 58, loss = 0.26992694\n",
      "Iteration 140, loss = 0.12154845\n",
      "Iteration 238, loss = 0.08623860\n",
      "Iteration 33, loss = 0.30632746\n",
      "Iteration 131, loss = 0.24665224\n",
      "Iteration 319, loss = 0.14595124\n",
      "Iteration 59, loss = 0.26674908\n",
      "Iteration 239, loss = 0.08610230\n",
      "Iteration 274, loss = 0.15621555\n",
      "Iteration 141, loss = 0.12121994\n",
      "Iteration 60, loss = 0.26362655\n",
      "Iteration 240, loss = 0.08598734\n",
      "Iteration 142, loss = 0.12088394\n",
      "Iteration 34, loss = 0.30154073\n",
      "Iteration 214, loss = 0.18845621\n",
      "Iteration 61, loss = 0.26051528\n",
      "Iteration 143, loss = 0.12056141\n",
      "Iteration 144, loss = 0.12023903\n",
      "Iteration 62, loss = 0.25746576\n",
      "Iteration 241, loss = 0.08585727\n",
      "Iteration 320, loss = 0.14585150\n",
      "Iteration 275, loss = 0.15608327\n",
      "Iteration 35, loss = 0.29695758\n",
      "Iteration 242, loss = 0.08572612\n",
      "Iteration 63, loss = 0.25428922\n",
      "Iteration 64, loss = 0.25131425\n",
      "Iteration 243, loss = 0.08560114\n",
      "Iteration 65, loss = 0.24815248\n",
      "Iteration 132, loss = 0.24627223\n",
      "Iteration 145, loss = 0.11992531\n",
      "Iteration 36, loss = 0.29242421\n",
      "Iteration 244, loss = 0.08547762\n",
      "Iteration 146, loss = 0.11962039\n",
      "Iteration 66, loss = 0.24517554\n",
      "Iteration 276, loss = 0.15594591\n",
      "Iteration 321, loss = 0.14576834\n",
      "Iteration 215, loss = 0.18828600\n",
      "Iteration 67, loss = 0.24218201\n",
      "Iteration 147, loss = 0.11928267\n",
      "Iteration 245, loss = 0.08535503\n",
      "Iteration 68, loss = 0.23923235\n",
      "Iteration 69, loss = 0.23630459\n",
      "Iteration 70, loss = 0.23335881\n",
      "Iteration 37, loss = 0.28815682\n",
      "Iteration 246, loss = 0.08522155\n",
      "Iteration 71, loss = 0.23046996\n",
      "Iteration 148, loss = 0.11902217\n",
      "Iteration 72, loss = 0.22761588\n",
      "Iteration 149, loss = 0.11868626\n",
      "Iteration 247, loss = 0.08510018\n",
      "Iteration 248, loss = 0.08498474\n",
      "Iteration 133, loss = 0.24579787\n",
      "Iteration 249, loss = 0.08486264Iteration 150, loss = 0.11839858\n",
      "\n",
      "Iteration 216, loss = 0.18813235\n",
      "Iteration 277, loss = 0.15585484\n",
      "Iteration 38, loss = 0.28394300\n",
      "Iteration 73, loss = 0.22478972\n",
      "Iteration 322, loss = 0.14568295\n",
      "Iteration 278, loss = 0.15582153\n",
      "Iteration 74, loss = 0.22191511\n",
      "Iteration 75, loss = 0.21920564\n",
      "Iteration 250, loss = 0.08473509\n",
      "Iteration 76, loss = 0.21646870\n",
      "Iteration 39, loss = 0.27989849\n",
      "Iteration 151, loss = 0.11811507\n",
      "Iteration 251, loss = 0.08462152\n",
      "Iteration 279, loss = 0.15561741\n",
      "Iteration 77, loss = 0.21382449\n",
      "Iteration 152, loss = 0.11780807\n",
      "Iteration 252, loss = 0.08449731\n",
      "Iteration 323, loss = 0.14561492\n",
      "Iteration 153, loss = 0.11752055\n",
      "Iteration 253, loss = 0.08438063\n",
      "Iteration 254, loss = 0.08428369\n",
      "Iteration 78, loss = 0.21118758\n",
      "Iteration 217, loss = 0.18799382\n",
      "Iteration 79, loss = 0.20866716\n",
      "Iteration 40, loss = 0.27599564\n",
      "Iteration 154, loss = 0.11723951\n",
      "Iteration 134, loss = 0.24539774\n",
      "Iteration 280, loss = 0.15551793\n",
      "Iteration 255, loss = 0.08414104\n",
      "Iteration 80, loss = 0.20605288\n",
      "Iteration 81, loss = 0.20351818\n",
      "Iteration 82, loss = 0.20107566\n",
      "Iteration 155, loss = 0.11695130\n",
      "Iteration 324, loss = 0.14552290\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 256, loss = 0.08403532\n",
      "Iteration 156, loss = 0.11667597\n",
      "Iteration 83, loss = 0.19861687\n",
      "Iteration 157, loss = 0.11639672\n",
      "Iteration 84, loss = 0.19611565\n",
      "Iteration 41, loss = 0.27217011\n",
      "Iteration 281, loss = 0.15542102\n",
      "Iteration 85, loss = 0.19379513\n",
      "Iteration 257, loss = 0.08391426\n",
      "Iteration 42, loss = 0.26849839\n",
      "Iteration 218, loss = 0.18783075\n",
      "Iteration 86, loss = 0.19149842\n",
      "Iteration 158, loss = 0.11610563\n",
      "Iteration 43, loss = 0.26498671\n",
      "Iteration 87, loss = 0.18916841\n",
      "Iteration 258, loss = 0.08381175\n",
      "Iteration 135, loss = 0.24493722\n",
      "Iteration 1, loss = 0.79605309\n",
      "Iteration 159, loss = 0.11583792\n",
      "Iteration 259, loss = 0.08368699\n",
      "Iteration 260, loss = 0.08356530\n",
      "Iteration 160, loss = 0.11555983\n",
      "Iteration 88, loss = 0.18696383\n",
      "Iteration 161, loss = 0.11529533\n",
      "Iteration 89, loss = 0.18480062\n",
      "Iteration 219, loss = 0.18768526\n",
      "Iteration 282, loss = 0.15528541\n",
      "Iteration 261, loss = 0.08347253\n",
      "Iteration 90, loss = 0.18273867\n",
      "Iteration 44, loss = 0.26151898Iteration 162, loss = 0.11503517\n",
      "Iteration 91, loss = 0.18064049\n",
      "Iteration 283, loss = 0.15516586\n",
      "Iteration 262, loss = 0.08334425\n",
      "\n",
      "Iteration 2, loss = 0.75619478\n",
      "Iteration 163, loss = 0.11476274\n",
      "Iteration 263, loss = 0.08324072\n",
      "Iteration 92, loss = 0.17862726\n",
      "Iteration 284, loss = 0.15507884\n",
      "Iteration 136, loss = 0.24457897\n",
      "Iteration 93, loss = 0.17667068\n",
      "Iteration 164, loss = 0.11448528\n",
      "Iteration 3, loss = 0.70543894\n",
      "Iteration 94, loss = 0.17467969\n",
      "Iteration 95, loss = 0.17277611\n",
      "Iteration 220, loss = 0.18756402\n",
      "Iteration 264, loss = 0.08312517\n",
      "Iteration 4, loss = 0.65592616\n",
      "Iteration 96, loss = 0.17090415\n",
      "Iteration 165, loss = 0.11425404\n",
      "Iteration 97, loss = 0.16915117\n",
      "Iteration 265, loss = 0.08302393\n",
      "Iteration 45, loss = 0.25814646\n",
      "Iteration 98, loss = 0.16729840\n",
      "Iteration 266, loss = 0.08289916\n",
      "Iteration 99, loss = 0.16552542\n",
      "Iteration 285, loss = 0.15498364\n",
      "Iteration 267, loss = 0.08280563\n",
      "Iteration 46, loss = 0.25493243\n",
      "Iteration 166, loss = 0.11398602\n",
      "Iteration 5, loss = 0.61169175\n",
      "Iteration 268, loss = 0.08268382\n",
      "Iteration 137, loss = 0.24415697\n",
      "Iteration 100, loss = 0.16385328\n",
      "Iteration 269, loss = 0.08257374\n",
      "Iteration 167, loss = 0.11372111\n",
      "Iteration 101, loss = 0.16225774\n",
      "Iteration 168, loss = 0.11347104\n",
      "Iteration 102, loss = 0.16055408\n",
      "Iteration 221, loss = 0.18740093\n",
      "Iteration 270, loss = 0.08247167\n",
      "Iteration 286, loss = 0.15488828\n",
      "Iteration 47, loss = 0.25176060\n",
      "Iteration 103, loss = 0.15899491Iteration 48, loss = 0.24872417\n",
      "\n",
      "Iteration 169, loss = 0.11324029\n",
      "Iteration 6, loss = 0.57499673\n",
      "Iteration 104, loss = 0.15736532\n",
      "Iteration 138, loss = 0.24374209\n",
      "Iteration 271, loss = 0.08237174\n",
      "Iteration 287, loss = 0.15481618\n",
      "Iteration 105, loss = 0.15580688\n",
      "Iteration 106, loss = 0.15435929\n",
      "Iteration 272, loss = 0.08226202\n",
      "Iteration 170, loss = 0.11298207\n",
      "Iteration 107, loss = 0.15288982\n",
      "Iteration 108, loss = 0.15145337\n",
      "Iteration 7, loss = 0.54455337\n",
      "Iteration 109, loss = 0.15007751\n",
      "Iteration 49, loss = 0.24582507\n",
      "Iteration 222, loss = 0.18726342\n",
      "Iteration 273, loss = 0.08215462\n",
      "Iteration 171, loss = 0.11273878\n",
      "Iteration 8, loss = 0.51918614\n",
      "Iteration 172, loss = 0.11248269\n",
      "Iteration 288, loss = 0.15467468\n",
      "Iteration 274, loss = 0.08204536\n",
      "Iteration 173, loss = 0.11227493\n",
      "Iteration 275, loss = 0.08194606\n",
      "Iteration 110, loss = 0.14868513\n",
      "Iteration 50, loss = 0.24298542\n",
      "Iteration 174, loss = 0.11201926\n",
      "Iteration 9, loss = 0.49863478\n",
      "Iteration 276, loss = 0.08184638\n",
      "Iteration 289, loss = 0.15453008\n",
      "Iteration 175, loss = 0.11179418\n",
      "Iteration 277, loss = 0.08174491\n",
      "Iteration 111, loss = 0.14732941\n",
      "Iteration 112, loss = 0.14601155\n",
      "Iteration 10, loss = 0.48177476\n",
      "Iteration 113, loss = 0.14473622\n",
      "Iteration 223, loss = 0.18710427\n",
      "Iteration 176, loss = 0.11154047\n",
      "Iteration 114, loss = 0.14348398\n",
      "Iteration 51, loss = 0.24020095\n",
      "Iteration 290, loss = 0.15445067\n",
      "Iteration 177, loss = 0.11134692\n",
      "Iteration 115, loss = 0.14227245\n",
      "Iteration 139, loss = 0.24337249\n",
      "Iteration 278, loss = 0.08164281\n",
      "Iteration 116, loss = 0.14101378\n",
      "Iteration 178, loss = 0.11108909\n",
      "Iteration 11, loss = 0.46698043\n",
      "Iteration 117, loss = 0.13981253\n",
      "Iteration 279, loss = 0.08153599\n",
      "Iteration 52, loss = 0.23744527\n",
      "Iteration 118, loss = 0.13862194\n",
      "Iteration 119, loss = 0.13753631\n",
      "Iteration 120, loss = 0.13633587\n",
      "Iteration 280, loss = 0.08144276\n",
      "Iteration 121, loss = 0.13523671\n",
      "Iteration 224, loss = 0.18699948\n",
      "Iteration 12, loss = 0.45482211\n",
      "Iteration 53, loss = 0.23492084\n",
      "Iteration 13, loss = 0.44382969\n",
      "Iteration 140, loss = 0.24296337\n",
      "Iteration 281, loss = 0.08133545\n",
      "Iteration 282, loss = 0.08124209\n",
      "Iteration 122, loss = 0.13417879\n",
      "Iteration 291, loss = 0.15434143\n",
      "Iteration 14, loss = 0.43459486\n",
      "Iteration 179, loss = 0.11087524\n",
      "Iteration 123, loss = 0.13311668\n",
      "Iteration 283, loss = 0.08113997\n",
      "Iteration 54, loss = 0.23236050\n",
      "Iteration 284, loss = 0.08102988\n",
      "Iteration 180, loss = 0.11064509\n",
      "Iteration 124, loss = 0.13201928\n",
      "Iteration 125, loss = 0.13103713\n",
      "Iteration 126, loss = 0.13004339\n",
      "Iteration 225, loss = 0.18687644\n",
      "Iteration 181, loss = 0.11041992\n",
      "Iteration 292, loss = 0.15426362\n",
      "Iteration 182, loss = 0.11019710\n",
      "Iteration 127, loss = 0.12903521\n",
      "Iteration 128, loss = 0.12808883\n",
      "Iteration 285, loss = 0.08094435\n",
      "Iteration 129, loss = 0.12713764\n",
      "Iteration 15, loss = 0.42621677\n",
      "Iteration 286, loss = 0.08083357\n",
      "Iteration 55, loss = 0.23000360\n",
      "Iteration 16, loss = 0.41844633Iteration 183, loss = 0.10997836\n",
      "Iteration 141, loss = 0.24257431\n",
      "\n",
      "Iteration 184, loss = 0.10976465\n",
      "Iteration 287, loss = 0.08075465\n",
      "Iteration 130, loss = 0.12625325\n",
      "Iteration 293, loss = 0.15414038\n",
      "Iteration 131, loss = 0.12530813\n",
      "Iteration 226, loss = 0.18668014\n",
      "Iteration 288, loss = 0.08064697\n",
      "Iteration 132, loss = 0.12446291\n",
      "Iteration 133, loss = 0.12362459\n",
      "Iteration 289, loss = 0.08055108\n",
      "Iteration 134, loss = 0.12275654\n",
      "Iteration 17, loss = 0.41112815\n",
      "Iteration 290, loss = 0.08045936\n",
      "Iteration 185, loss = 0.10955820\n",
      "Iteration 56, loss = 0.22760481\n",
      "Iteration 291, loss = 0.08035909\n",
      "Iteration 186, loss = 0.10933444\n",
      "Iteration 294, loss = 0.15404399\n",
      "Iteration 135, loss = 0.12191286\n",
      "Iteration 187, loss = 0.10912127\n",
      "Iteration 136, loss = 0.12115177\n",
      "Iteration 57, loss = 0.22530033\n",
      "Iteration 137, loss = 0.12029920\n",
      "Iteration 292, loss = 0.08027069\n",
      "Iteration 188, loss = 0.10890643\n",
      "Iteration 295, loss = 0.15395436\n",
      "Iteration 138, loss = 0.11951288\n",
      "Iteration 139, loss = 0.11875862\n",
      "Iteration 18, loss = 0.40440941\n",
      "Iteration 227, loss = 0.18664668\n",
      "Iteration 140, loss = 0.11802714\n",
      "Iteration 293, loss = 0.08016691\n",
      "Iteration 142, loss = 0.24227891\n",
      "Iteration 141, loss = 0.11728330\n",
      "Iteration 19, loss = 0.39801898\n",
      "Iteration 296, loss = 0.15387624\n",
      "Iteration 58, loss = 0.22309050\n",
      "Iteration 189, loss = 0.10869945\n",
      "Iteration 142, loss = 0.11656197\n",
      "Iteration 20, loss = 0.39173450\n",
      "Iteration 190, loss = 0.10850264\n",
      "Iteration 143, loss = 0.11586665\n",
      "Iteration 294, loss = 0.08008854\n",
      "Iteration 191, loss = 0.10829774\n",
      "Iteration 228, loss = 0.18643506\n",
      "Iteration 295, loss = 0.07998660\n",
      "Iteration 144, loss = 0.11515097\n",
      "Iteration 59, loss = 0.22098255\n",
      "Iteration 192, loss = 0.10807320\n",
      "Iteration 296, loss = 0.07988743\n",
      "Iteration 21, loss = 0.38577213\n",
      "Iteration 297, loss = 0.07980008\n",
      "Iteration 145, loss = 0.11449013\n",
      "Iteration 297, loss = 0.15374762\n",
      "Iteration 146, loss = 0.11383234\n",
      "Iteration 193, loss = 0.10789267\n",
      "Iteration 298, loss = 0.07970465\n",
      "Iteration 147, loss = 0.11317509\n",
      "Iteration 143, loss = 0.24177882\n",
      "Iteration 299, loss = 0.07962373\n",
      "Iteration 22, loss = 0.37981596\n",
      "Iteration 148, loss = 0.11255897Iteration 60, loss = 0.21892264\n",
      "\n",
      "Iteration 194, loss = 0.10768889\n",
      "Iteration 149, loss = 0.11193104\n",
      "Iteration 23, loss = 0.37398889\n",
      "Iteration 150, loss = 0.11129995\n",
      "Iteration 195, loss = 0.10752297\n",
      "Iteration 300, loss = 0.07952147\n",
      "Iteration 151, loss = 0.11071243\n",
      "Iteration 61, loss = 0.21687917\n",
      "Iteration 229, loss = 0.18628674\n",
      "Iteration 298, loss = 0.15365183\n",
      "Iteration 152, loss = 0.11011336\n",
      "Iteration 196, loss = 0.10727368\n",
      "Iteration 301, loss = 0.07943880\n",
      "Iteration 24, loss = 0.36832455\n",
      "Iteration 299, loss = 0.15361138\n",
      "Iteration 302, loss = 0.07934838\n",
      "Iteration 25, loss = 0.36289099\n",
      "Iteration 153, loss = 0.10959215\n",
      "Iteration 197, loss = 0.10709639\n",
      "Iteration 154, loss = 0.10896968\n",
      "Iteration 155, loss = 0.10842515\n",
      "Iteration 144, loss = 0.24148620\n",
      "Iteration 62, loss = 0.21494926\n",
      "Iteration 303, loss = 0.07925731\n",
      "Iteration 198, loss = 0.10689452\n",
      "Iteration 26, loss = 0.35736295\n",
      "Iteration 304, loss = 0.07917873\n",
      "Iteration 199, loss = 0.10672393\n",
      "Iteration 230, loss = 0.18616461\n",
      "Iteration 156, loss = 0.10786279\n",
      "Iteration 63, loss = 0.21311209\n",
      "Iteration 305, loss = 0.07909073\n",
      "Iteration 157, loss = 0.10733205\n",
      "Iteration 300, loss = 0.15347391\n",
      "Iteration 200, loss = 0.10652778\n",
      "Iteration 158, loss = 0.10687707\n",
      "Iteration 201, loss = 0.10632503\n",
      "Iteration 27, loss = 0.35199987\n",
      "Iteration 64, loss = 0.21125861\n",
      "Iteration 202, loss = 0.10613647\n",
      "Iteration 231, loss = 0.18602161\n",
      "Iteration 159, loss = 0.10630161\n",
      "Iteration 160, loss = 0.10580265\n",
      "Iteration 301, loss = 0.15334848\n",
      "Iteration 306, loss = 0.07899341\n",
      "Iteration 28, loss = 0.34664768\n",
      "Iteration 203, loss = 0.10598305\n",
      "Iteration 161, loss = 0.10531403\n",
      "Iteration 145, loss = 0.24113816\n",
      "Iteration 65, loss = 0.20944258\n",
      "Iteration 29, loss = 0.34151739\n",
      "Iteration 162, loss = 0.10484126\n",
      "Iteration 307, loss = 0.07891640\n",
      "Iteration 163, loss = 0.10437881\n",
      "Iteration 232, loss = 0.18588553\n",
      "Iteration 308, loss = 0.07883079\n",
      "Iteration 30, loss = 0.33626137\n",
      "Iteration 204, loss = 0.10578761\n",
      "Iteration 164, loss = 0.10391906\n",
      "Iteration 302, loss = 0.15327163\n",
      "Iteration 165, loss = 0.10346442\n",
      "Iteration 205, loss = 0.10559466\n",
      "Iteration 166, loss = 0.10302044\n",
      "Iteration 309, loss = 0.07873579\n",
      "Iteration 167, loss = 0.10259813\n",
      "Iteration 31, loss = 0.33110952\n",
      "Iteration 168, loss = 0.10218159\n",
      "Iteration 66, loss = 0.20771161\n",
      "Iteration 310, loss = 0.07865122\n",
      "Iteration 67, loss = 0.20605996\n",
      "Iteration 206, loss = 0.10539676\n",
      "Iteration 169, loss = 0.10177182\n",
      "Iteration 170, loss = 0.10136368\n",
      "Iteration 311, loss = 0.07856138\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 207, loss = 0.10522559\n",
      "Iteration 171, loss = 0.10094372\n",
      "Iteration 172, loss = 0.10057033\n",
      "Iteration 32, loss = 0.32608183\n",
      "Iteration 303, loss = 0.15320251\n",
      "Iteration 233, loss = 0.18576443\n",
      "Iteration 208, loss = 0.10505223\n",
      "Iteration 146, loss = 0.24073181\n",
      "Iteration 33, loss = 0.32100320\n",
      "Iteration 173, loss = 0.10021041\n",
      "Iteration 209, loss = 0.10487329\n",
      "Iteration 304, loss = 0.15315780\n",
      "Iteration 210, loss = 0.10470605\n",
      "Iteration 174, loss = 0.09981318\n",
      "Iteration 68, loss = 0.20440777\n",
      "Iteration 1, loss = 0.87742494Iteration 34, loss = 0.31603876\n",
      "\n",
      "Iteration 175, loss = 0.09943268\n",
      "Iteration 234, loss = 0.18567561\n",
      "Iteration 69, loss = 0.20282708\n",
      "Iteration 211, loss = 0.10453231\n",
      "Iteration 176, loss = 0.09907533\n",
      "Iteration 305, loss = 0.15299977\n",
      "Iteration 147, loss = 0.24035985\n",
      "Iteration 212, loss = 0.10436880\n",
      "Iteration 35, loss = 0.31118695\n",
      "Iteration 177, loss = 0.09872169\n",
      "Iteration 70, loss = 0.20124919\n",
      "Iteration 213, loss = 0.10419095\n",
      "Iteration 178, loss = 0.09834828\n",
      "Iteration 306, loss = 0.15294904\n",
      "Iteration 2, loss = 0.83549930\n",
      "Iteration 36, loss = 0.30629620\n",
      "Iteration 307, loss = 0.15283330Iteration 179, loss = 0.09802707\n",
      "Iteration 214, loss = 0.10401025\n",
      "\n",
      "Iteration 37, loss = 0.30160500\n",
      "Iteration 215, loss = 0.10384382\n",
      "Iteration 180, loss = 0.09767270\n",
      "Iteration 38, loss = 0.29679781\n",
      "Iteration 235, loss = 0.18548817\n",
      "Iteration 71, loss = 0.19975368\n",
      "Iteration 216, loss = 0.10366753\n",
      "Iteration 3, loss = 0.78196871\n",
      "Iteration 308, loss = 0.15273706\n",
      "Iteration 181, loss = 0.09732510\n",
      "Iteration 182, loss = 0.09698482\n",
      "Iteration 183, loss = 0.09668550\n",
      "Iteration 72, loss = 0.19827220\n",
      "Iteration 217, loss = 0.10350686\n",
      "Iteration 184, loss = 0.09637136\n",
      "Iteration 185, loss = 0.09604019\n",
      "Iteration 186, loss = 0.09571581\n",
      "Iteration 236, loss = 0.18536745\n",
      "Iteration 218, loss = 0.10334591\n",
      "Iteration 73, loss = 0.19688574\n",
      "Iteration 187, loss = 0.09546903\n",
      "Iteration 39, loss = 0.29228564\n",
      "Iteration 219, loss = 0.10316903\n",
      "Iteration 188, loss = 0.09510802\n",
      "Iteration 148, loss = 0.24005062\n",
      "Iteration 4, loss = 0.73119096\n",
      "Iteration 189, loss = 0.09479749\n",
      "Iteration 40, loss = 0.28784179\n",
      "Iteration 237, loss = 0.18524458\n",
      "Iteration 190, loss = 0.09449278\n",
      "Iteration 41, loss = 0.28334547Iteration 309, loss = 0.15263682\n",
      "\n",
      "Iteration 191, loss = 0.09419979\n",
      "Iteration 220, loss = 0.10302153\n",
      "Iteration 74, loss = 0.19549996\n",
      "Iteration 149, loss = 0.23965411\n",
      "Iteration 5, loss = 0.68624301\n",
      "Iteration 42, loss = 0.27892523\n",
      "Iteration 221, loss = 0.10285512\n",
      "Iteration 192, loss = 0.09392317\n",
      "Iteration 193, loss = 0.09363139\n",
      "Iteration 310, loss = 0.15256196\n",
      "Iteration 194, loss = 0.09336769\n",
      "Iteration 75, loss = 0.19415915\n",
      "Iteration 195, loss = 0.09305602\n",
      "Iteration 222, loss = 0.10269725\n",
      "Iteration 196, loss = 0.09278656\n",
      "Iteration 238, loss = 0.18514258\n",
      "Iteration 43, loss = 0.27462411\n",
      "Iteration 223, loss = 0.10253593\n",
      "Iteration 6, loss = 0.64965615\n",
      "Iteration 197, loss = 0.09250423\n",
      "Iteration 311, loss = 0.15244586\n",
      "Iteration 198, loss = 0.09223520\n",
      "Iteration 199, loss = 0.09200509\n",
      "Iteration 44, loss = 0.27048907\n",
      "Iteration 200, loss = 0.09169316\n",
      "Iteration 76, loss = 0.19283482\n",
      "Iteration 201, loss = 0.09146607\n",
      "Iteration 224, loss = 0.10237686\n",
      "Iteration 150, loss = 0.23930387\n",
      "Iteration 202, loss = 0.09120193\n",
      "Iteration 203, loss = 0.09094405\n",
      "Iteration 7, loss = 0.62102148\n",
      "Iteration 225, loss = 0.10221117\n",
      "Iteration 312, loss = 0.15236357\n",
      "Iteration 226, loss = 0.10206726\n",
      "Iteration 45, loss = 0.26630993\n",
      "Iteration 204, loss = 0.09071345\n",
      "Iteration 227, loss = 0.10189541\n",
      "Iteration 46, loss = 0.26214424\n",
      "Iteration 228, loss = 0.10174243\n",
      "Iteration 313, loss = 0.15228128\n",
      "Iteration 47, loss = 0.25823058\n",
      "Iteration 229, loss = 0.10159842\n",
      "Iteration 205, loss = 0.09049304\n",
      "Iteration 206, loss = 0.09020047Iteration 77, loss = 0.19161164\n",
      "\n",
      "Iteration 239, loss = 0.18497668\n",
      "Iteration 151, loss = 0.23894878\n",
      "Iteration 48, loss = 0.25449025\n",
      "Iteration 314, loss = 0.15220312\n",
      "Iteration 207, loss = 0.08996529\n",
      "Iteration 208, loss = 0.08970790\n",
      "Iteration 78, loss = 0.19032860\n",
      "Iteration 230, loss = 0.10142288\n",
      "Iteration 8, loss = 0.59495442\n",
      "Iteration 209, loss = 0.08947576\n",
      "Iteration 210, loss = 0.08923713\n",
      "Iteration 211, loss = 0.08902541\n",
      "Iteration 231, loss = 0.10128708\n",
      "Iteration 79, loss = 0.18907156\n",
      "Iteration 240, loss = 0.18487132\n",
      "Iteration 232, loss = 0.10111995\n",
      "Iteration 212, loss = 0.08877993\n",
      "Iteration 49, loss = 0.25062785\n",
      "Iteration 9, loss = 0.57503692\n",
      "Iteration 233, loss = 0.10096637\n",
      "Iteration 213, loss = 0.08855362\n",
      "Iteration 214, loss = 0.08833089\n",
      "Iteration 315, loss = 0.15210750\n",
      "Iteration 50, loss = 0.24688393\n",
      "Iteration 80, loss = 0.18796652\n",
      "Iteration 215, loss = 0.08813420\n",
      "Iteration 51, loss = 0.24339017\n",
      "Iteration 234, loss = 0.10082295\n",
      "Iteration 152, loss = 0.23864044\n",
      "Iteration 81, loss = 0.18677183\n",
      "Iteration 10, loss = 0.55838671\n",
      "Iteration 216, loss = 0.08793113\n",
      "Iteration 52, loss = 0.23990784\n",
      "Iteration 316, loss = 0.15204612\n",
      "Iteration 217, loss = 0.08768725\n",
      "Iteration 235, loss = 0.10068167\n",
      "Iteration 218, loss = 0.08747032\n",
      "Iteration 219, loss = 0.08726925\n",
      "Iteration 220, loss = 0.08706753\n",
      "Iteration 236, loss = 0.10053543\n",
      "Iteration 82, loss = 0.18569306\n",
      "Iteration 221, loss = 0.08684713\n",
      "Iteration 222, loss = 0.08666440\n",
      "Iteration 241, loss = 0.18474710\n",
      "Iteration 223, loss = 0.08646763\n",
      "Iteration 53, loss = 0.23652754\n",
      "Iteration 224, loss = 0.08623885\n",
      "Iteration 237, loss = 0.10038394\n",
      "Iteration 153, loss = 0.23831118\n",
      "Iteration 317, loss = 0.15197195\n",
      "Iteration 238, loss = 0.10022562\n",
      "Iteration 54, loss = 0.23337913\n",
      "Iteration 11, loss = 0.54392258\n",
      "Iteration 242, loss = 0.18461425\n",
      "Iteration 225, loss = 0.08604650\n",
      "Iteration 55, loss = 0.23007964\n",
      "Iteration 226, loss = 0.08584119\n",
      "Iteration 227, loss = 0.08563902\n",
      "Iteration 83, loss = 0.18455161\n",
      "Iteration 318, loss = 0.15189113\n",
      "Iteration 228, loss = 0.08546738\n",
      "Iteration 239, loss = 0.10008217\n",
      "Iteration 229, loss = 0.08527280\n",
      "Iteration 240, loss = 0.09993789\n",
      "Iteration 12, loss = 0.53162396\n",
      "Iteration 56, loss = 0.22708988\n",
      "Iteration 84, loss = 0.18349189\n",
      "Iteration 57, loss = 0.22423580\n",
      "Iteration 319, loss = 0.15178971\n",
      "Iteration 241, loss = 0.09980932\n",
      "Iteration 230, loss = 0.08506658\n",
      "Iteration 231, loss = 0.08489344\n",
      "Iteration 232, loss = 0.08471670\n",
      "Iteration 242, loss = 0.09965130\n",
      "Iteration 154, loss = 0.23801195\n",
      "Iteration 233, loss = 0.08450718\n",
      "Iteration 243, loss = 0.18448282\n",
      "Iteration 234, loss = 0.08434533\n",
      "Iteration 85, loss = 0.18249186\n",
      "Iteration 320, loss = 0.15169765\n",
      "Iteration 235, loss = 0.08414299\n",
      "Iteration 243, loss = 0.09950045\n",
      "Iteration 13, loss = 0.52087539\n",
      "Iteration 58, loss = 0.22127359Iteration 236, loss = 0.08398929\n",
      "\n",
      "Iteration 86, loss = 0.18145959\n",
      "Iteration 244, loss = 0.09937037\n",
      "Iteration 237, loss = 0.08380374\n",
      "Iteration 87, loss = 0.18049378\n",
      "Iteration 14, loss = 0.51140840\n",
      "Iteration 244, loss = 0.18437931\n",
      "Iteration 245, loss = 0.09923289\n",
      "Iteration 321, loss = 0.15162986\n",
      "Iteration 59, loss = 0.21858255\n",
      "Iteration 155, loss = 0.23767746\n",
      "Iteration 246, loss = 0.09907528\n",
      "Iteration 238, loss = 0.08362329\n",
      "Iteration 239, loss = 0.08350302\n",
      "Iteration 60, loss = 0.21587650\n",
      "Iteration 240, loss = 0.08327316\n",
      "Iteration 88, loss = 0.17951684\n",
      "Iteration 247, loss = 0.09896484Iteration 241, loss = 0.08310865\n",
      "Iteration 61, loss = 0.21339750\n",
      "Iteration 242, loss = 0.08293065\n",
      "\n",
      "Iteration 243, loss = 0.08277588\n",
      "Iteration 244, loss = 0.08262879\n",
      "Iteration 245, loss = 0.08243802\n",
      "Iteration 248, loss = 0.09879781\n",
      "Iteration 246, loss = 0.08228216\n",
      "Iteration 322, loss = 0.15153642\n",
      "Iteration 15, loss = 0.50232166\n",
      "Iteration 249, loss = 0.09866473\n",
      "Iteration 245, loss = 0.18427727\n",
      "Iteration 62, loss = 0.21086338\n",
      "Iteration 323, loss = 0.15147606\n",
      "Iteration 89, loss = 0.17859866\n",
      "Iteration 247, loss = 0.08212886\n",
      "Iteration 250, loss = 0.09853030\n",
      "Iteration 156, loss = 0.23731873\n",
      "Iteration 251, loss = 0.09840593\n",
      "Iteration 248, loss = 0.08198576\n",
      "Iteration 63, loss = 0.20859809\n",
      "Iteration 249, loss = 0.08180406\n",
      "Iteration 250, loss = 0.08164679\n",
      "Iteration 251, loss = 0.08150096\n",
      "Iteration 16, loss = 0.49425548\n",
      "Iteration 90, loss = 0.17769523\n",
      "Iteration 252, loss = 0.08136625\n",
      "Iteration 324, loss = 0.15141385\n",
      "Iteration 252, loss = 0.09826276\n",
      "Iteration 253, loss = 0.08119260\n",
      "Iteration 246, loss = 0.18412470Iteration 254, loss = 0.08102886\n",
      "\n",
      "Iteration 253, loss = 0.09815750\n",
      "Iteration 91, loss = 0.17678905\n",
      "Iteration 254, loss = 0.09798568\n",
      "Iteration 255, loss = 0.08089851\n",
      "Iteration 247, loss = 0.18400816\n",
      "Iteration 64, loss = 0.20637554\n",
      "Iteration 325, loss = 0.15131276\n",
      "Iteration 256, loss = 0.08071823\n",
      "Iteration 17, loss = 0.48667696\n",
      "Iteration 255, loss = 0.09787025\n",
      "Iteration 256, loss = 0.09772991\n",
      "Iteration 92, loss = 0.17594130\n",
      "Iteration 65, loss = 0.20409563\n",
      "Iteration 257, loss = 0.08057376\n",
      "Iteration 326, loss = 0.15124258\n",
      "Iteration 258, loss = 0.08040160\n",
      "Iteration 157, loss = 0.23704157\n",
      "Iteration 259, loss = 0.08028202\n",
      "Iteration 66, loss = 0.20206203\n",
      "Iteration 260, loss = 0.08010534\n",
      "Iteration 261, loss = 0.07995442\n",
      "Iteration 327, loss = 0.15113792\n",
      "Iteration 262, loss = 0.07982165\n",
      "Iteration 18, loss = 0.47949344\n",
      "Iteration 257, loss = 0.09760815\n",
      "Iteration 263, loss = 0.07967501\n",
      "Iteration 67, loss = 0.19998315\n",
      "Iteration 264, loss = 0.07954028\n",
      "Iteration 93, loss = 0.17511729\n",
      "Iteration 258, loss = 0.09749228\n",
      "Iteration 328, loss = 0.15107510\n",
      "Iteration 265, loss = 0.07940899\n",
      "Iteration 259, loss = 0.09735643\n",
      "Iteration 266, loss = 0.07924777\n",
      "Iteration 94, loss = 0.17425239\n",
      "Iteration 260, loss = 0.09723133\n",
      "Iteration 19, loss = 0.47263581\n",
      "Iteration 68, loss = 0.19813256\n",
      "Iteration 158, loss = 0.23677322\n",
      "Iteration 267, loss = 0.07912851\n",
      "Iteration 248, loss = 0.18393796\n",
      "Iteration 69, loss = 0.19622192\n",
      "Iteration 261, loss = 0.09709500\n",
      "Iteration 268, loss = 0.07897677\n",
      "Iteration 95, loss = 0.17344297\n",
      "Iteration 269, loss = 0.07883990\n",
      "Iteration 270, loss = 0.07870325\n",
      "Iteration 329, loss = 0.15101651\n",
      "Iteration 262, loss = 0.09697336\n",
      "Iteration 271, loss = 0.07856475\n",
      "Iteration 70, loss = 0.19442526\n",
      "Iteration 20, loss = 0.46601501\n",
      "Iteration 263, loss = 0.09685101\n",
      "Iteration 71, loss = 0.19270257\n",
      "Iteration 264, loss = 0.09673995\n",
      "Iteration 96, loss = 0.17266550\n",
      "Iteration 249, loss = 0.18381291\n",
      "Iteration 330, loss = 0.15093523\n",
      "Iteration 272, loss = 0.07842793\n",
      "Iteration 273, loss = 0.07829562\n",
      "Iteration 72, loss = 0.19097857\n",
      "Iteration 274, loss = 0.07820334\n",
      "Iteration 275, loss = 0.07804140\n",
      "Iteration 73, loss = 0.18938763\n",
      "Iteration 276, loss = 0.07792907\n",
      "Iteration 265, loss = 0.09660145\n",
      "Iteration 266, loss = 0.09649319\n",
      "Iteration 159, loss = 0.23646762\n",
      "Iteration 331, loss = 0.15084498\n",
      "Iteration 97, loss = 0.17190564\n",
      "Iteration 267, loss = 0.09635949\n",
      "Iteration 277, loss = 0.07778213\n",
      "Iteration 21, loss = 0.45948146\n",
      "Iteration 278, loss = 0.07765823\n",
      "Iteration 74, loss = 0.18789635\n",
      "Iteration 279, loss = 0.07752394\n",
      "Iteration 250, loss = 0.18368430\n",
      "Iteration 98, loss = 0.17120005\n",
      "Iteration 75, loss = 0.18634273\n",
      "Iteration 268, loss = 0.09624472\n",
      "Iteration 332, loss = 0.15074645\n",
      "Iteration 76, loss = 0.18486136\n",
      "Iteration 160, loss = 0.23612815\n",
      "Iteration 280, loss = 0.07739912\n",
      "Iteration 22, loss = 0.45327795\n",
      "Iteration 269, loss = 0.09611824\n",
      "Iteration 251, loss = 0.18356266\n",
      "Iteration 281, loss = 0.07726823\n",
      "Iteration 282, loss = 0.07714098\n",
      "Iteration 77, loss = 0.18348142\n",
      "Iteration 270, loss = 0.09600511\n",
      "Iteration 99, loss = 0.17040487\n",
      "Iteration 333, loss = 0.15069354\n",
      "Iteration 283, loss = 0.07701705\n",
      "Iteration 100, loss = 0.16967968\n",
      "Iteration 271, loss = 0.09589742\n",
      "Iteration 284, loss = 0.07690660\n",
      "Iteration 285, loss = 0.07678506\n",
      "Iteration 23, loss = 0.44718767\n",
      "Iteration 286, loss = 0.07664652\n",
      "Iteration 78, loss = 0.18215014\n",
      "Iteration 287, loss = 0.07652760\n",
      "Iteration 288, loss = 0.07641348\n",
      "Iteration 101, loss = 0.16898410\n",
      "Iteration 334, loss = 0.15063626\n",
      "Iteration 272, loss = 0.09575977\n",
      "Iteration 289, loss = 0.07632448\n",
      "Iteration 161, loss = 0.23576473\n",
      "Iteration 252, loss = 0.18342151\n",
      "Iteration 79, loss = 0.18083355\n",
      "Iteration 290, loss = 0.07617397\n",
      "Iteration 273, loss = 0.09565131\n",
      "Iteration 80, loss = 0.17957328\n",
      "Iteration 291, loss = 0.07605554\n",
      "Iteration 274, loss = 0.09553787\n",
      "Iteration 24, loss = 0.44123628Iteration 335, loss = 0.15055312\n",
      "\n",
      "Iteration 102, loss = 0.16827089\n",
      "Iteration 275, loss = 0.09542021\n",
      "Iteration 81, loss = 0.17833420\n",
      "Iteration 292, loss = 0.07594181\n",
      "Iteration 293, loss = 0.07585358\n",
      "Iteration 294, loss = 0.07570037\n",
      "Iteration 336, loss = 0.15046651\n",
      "Iteration 82, loss = 0.17716523\n",
      "Iteration 276, loss = 0.09530745\n",
      "Iteration 253, loss = 0.18332859\n",
      "Iteration 103, loss = 0.16759423\n",
      "Iteration 25, loss = 0.43531886\n",
      "Iteration 295, loss = 0.07561165\n",
      "Iteration 296, loss = 0.07549115\n",
      "Iteration 277, loss = 0.09519867\n",
      "Iteration 297, loss = 0.07537704\n",
      "Iteration 298, loss = 0.07526329\n",
      "Iteration 83, loss = 0.17601097\n",
      "Iteration 299, loss = 0.07513914\n",
      "Iteration 337, loss = 0.15039369\n",
      "Iteration 162, loss = 0.23544801\n",
      "Iteration 300, loss = 0.07505318\n",
      "Iteration 104, loss = 0.16693119\n",
      "Iteration 301, loss = 0.07496225\n",
      "Iteration 278, loss = 0.09507907\n",
      "Iteration 302, loss = 0.07483324\n",
      "Iteration 303, loss = 0.07471878\n",
      "Iteration 304, loss = 0.07459649\n",
      "Iteration 26, loss = 0.42944710\n",
      "Iteration 305, loss = 0.07450309\n",
      "Iteration 84, loss = 0.17496796\n",
      "Iteration 279, loss = 0.09497595\n",
      "Iteration 280, loss = 0.09484512\n",
      "Iteration 105, loss = 0.16626972\n",
      "Iteration 254, loss = 0.18321097\n",
      "Iteration 338, loss = 0.15030666\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 281, loss = 0.09475408\n",
      "Iteration 85, loss = 0.17382527\n",
      "Iteration 306, loss = 0.07440006\n",
      "Iteration 163, loss = 0.23518438\n",
      "Iteration 106, loss = 0.16561896\n",
      "Iteration 27, loss = 0.42380726\n",
      "Iteration 307, loss = 0.07431840\n",
      "Iteration 308, loss = 0.07417829\n",
      "Iteration 1, loss = 0.63088973\n",
      "Iteration 309, loss = 0.07406936\n",
      "Iteration 282, loss = 0.09462821\n",
      "Iteration 2, loss = 0.61331468\n",
      "Iteration 86, loss = 0.17284518\n",
      "Iteration 255, loss = 0.18309013\n",
      "Iteration 310, loss = 0.07399192\n",
      "Iteration 3, loss = 0.58926309\n",
      "Iteration 311, loss = 0.07388549\n",
      "Iteration 87, loss = 0.17181091\n",
      "Iteration 283, loss = 0.09452513\n",
      "Iteration 28, loss = 0.41825942\n",
      "Iteration 107, loss = 0.16497447\n",
      "Iteration 312, loss = 0.07375977\n",
      "Iteration 284, loss = 0.09442389\n",
      "Iteration 313, loss = 0.07367395\n",
      "Iteration 285, loss = 0.09430403\n",
      "Iteration 4, loss = 0.56288225\n",
      "Iteration 314, loss = 0.07357338\n",
      "Iteration 315, loss = 0.07346728\n",
      "Iteration 316, loss = 0.07336533\n",
      "Iteration 88, loss = 0.17080865\n",
      "Iteration 5, loss = 0.53725488\n",
      "Iteration 6, loss = 0.51320231\n",
      "Iteration 164, loss = 0.23492621\n",
      "Iteration 286, loss = 0.09420034\n",
      "Iteration 108, loss = 0.16433569\n",
      "Iteration 256, loss = 0.18299549Iteration 7, loss = 0.49181992\n",
      "\n",
      "Iteration 287, loss = 0.09408930\n",
      "Iteration 8, loss = 0.47300547\n",
      "Iteration 288, loss = 0.09397353\n",
      "Iteration 9, loss = 0.45629952\n",
      "Iteration 10, loss = 0.44207600\n",
      "Iteration 289, loss = 0.09388101\n",
      "Iteration 317, loss = 0.07328778\n",
      "Iteration 11, loss = 0.42987050\n",
      "Iteration 109, loss = 0.16371295\n",
      "Iteration 257, loss = 0.18287922\n",
      "Iteration 89, loss = 0.16984422\n",
      "Iteration 29, loss = 0.41261907\n",
      "Iteration 318, loss = 0.07317574\n",
      "Iteration 12, loss = 0.41914424\n",
      "Iteration 319, loss = 0.07307193\n",
      "Iteration 320, loss = 0.07297932\n",
      "Iteration 321, loss = 0.07290260\n",
      "Iteration 90, loss = 0.16891776\n",
      "Iteration 322, loss = 0.07279946\n",
      "Iteration 110, loss = 0.16315464\n",
      "Iteration 290, loss = 0.09377333\n",
      "Iteration 323, loss = 0.07270640\n",
      "Iteration 324, loss = 0.07259091\n",
      "Iteration 13, loss = 0.41033229\n",
      "Iteration 91, loss = 0.16804187\n",
      "Iteration 30, loss = 0.40720606\n",
      "Iteration 14, loss = 0.40256787\n",
      "Iteration 325, loss = 0.07251120\n",
      "Iteration 165, loss = 0.23455836\n",
      "Iteration 291, loss = 0.09366648\n",
      "Iteration 111, loss = 0.16250843\n",
      "Iteration 326, loss = 0.07242451\n",
      "Iteration 327, loss = 0.07235118\n",
      "Iteration 15, loss = 0.39575181\n",
      "Iteration 292, loss = 0.09358486\n",
      "Iteration 16, loss = 0.39008490\n",
      "Iteration 92, loss = 0.16714784\n",
      "Iteration 293, loss = 0.09347635\n",
      "Iteration 328, loss = 0.07224529\n",
      "Iteration 31, loss = 0.40166136\n",
      "Iteration 93, loss = 0.16629849\n",
      "Iteration 112, loss = 0.16195378\n",
      "Iteration 329, loss = 0.07213853\n",
      "Iteration 17, loss = 0.38463450\n",
      "Iteration 166, loss = 0.23439490\n",
      "Iteration 258, loss = 0.18276806\n",
      "Iteration 330, loss = 0.07207093\n",
      "Iteration 331, loss = 0.07200480\n",
      "Iteration 18, loss = 0.37989240\n",
      "Iteration 332, loss = 0.07188307\n",
      "Iteration 294, loss = 0.09335241\n",
      "Iteration 333, loss = 0.07178862\n",
      "Iteration 295, loss = 0.09327690\n",
      "Iteration 32, loss = 0.39637246\n",
      "Iteration 296, loss = 0.09315454\n",
      "Iteration 334, loss = 0.07172482\n",
      "Iteration 113, loss = 0.16136185\n",
      "Iteration 335, loss = 0.07165682\n",
      "Iteration 19, loss = 0.37548913\n",
      "Iteration 94, loss = 0.16544967\n",
      "Iteration 33, loss = 0.39101005\n",
      "Iteration 336, loss = 0.07152860\n",
      "Iteration 20, loss = 0.37144694\n",
      "Iteration 259, loss = 0.18266802\n",
      "Iteration 297, loss = 0.09304622\n",
      "Iteration 95, loss = 0.16463681\n",
      "Iteration 21, loss = 0.36754625\n",
      "Iteration 337, loss = 0.07145390\n",
      "Iteration 22, loss = 0.36378770\n",
      "Iteration 298, loss = 0.09294167\n",
      "Iteration 114, loss = 0.16079281\n",
      "Iteration 299, loss = 0.09284612\n",
      "Iteration 338, loss = 0.07135308\n",
      "Iteration 167, loss = 0.23399684\n",
      "Iteration 23, loss = 0.36027925\n",
      "Iteration 96, loss = 0.16383723\n",
      "Iteration 24, loss = 0.35671958\n",
      "Iteration 339, loss = 0.07129189\n",
      "Iteration 115, loss = 0.16023943\n",
      "Iteration 340, loss = 0.07120245\n",
      "Iteration 25, loss = 0.35324287\n",
      "Iteration 300, loss = 0.09274427\n",
      "Iteration 301, loss = 0.09264074\n",
      "Iteration 341, loss = 0.07112113\n",
      "Iteration 26, loss = 0.34984179\n",
      "Iteration 302, loss = 0.09254494\n",
      "Iteration 97, loss = 0.16306053\n",
      "Iteration 342, loss = 0.07103923\n",
      "Iteration 343, loss = 0.07094423\n",
      "Iteration 116, loss = 0.15972959\n",
      "Iteration 344, loss = 0.07086763\n",
      "Iteration 260, loss = 0.18256462\n",
      "Iteration 303, loss = 0.09244734\n",
      "Iteration 27, loss = 0.34639593\n",
      "Iteration 34, loss = 0.38567466\n",
      "Iteration 98, loss = 0.16229873\n",
      "Iteration 345, loss = 0.07078097\n",
      "Iteration 28, loss = 0.34296119\n",
      "Iteration 117, loss = 0.15917708\n",
      "Iteration 346, loss = 0.07070787\n",
      "Iteration 304, loss = 0.09235259\n",
      "Iteration 168, loss = 0.23373954\n",
      "Iteration 29, loss = 0.33958972\n",
      "Iteration 35, loss = 0.38046644\n",
      "Iteration 347, loss = 0.07066229\n",
      "Iteration 348, loss = 0.07060747\n",
      "Iteration 99, loss = 0.16156373\n",
      "Iteration 30, loss = 0.33621713\n",
      "Iteration 36, loss = 0.37517766Iteration 305, loss = 0.09224561\n",
      "\n",
      "Iteration 306, loss = 0.09216511\n",
      "Iteration 118, loss = 0.15863654\n",
      "Iteration 31, loss = 0.33282405\n",
      "Iteration 307, loss = 0.09205842\n",
      "Iteration 349, loss = 0.07047325\n",
      "Iteration 32, loss = 0.32945644\n",
      "Iteration 100, loss = 0.16081876\n",
      "Iteration 350, loss = 0.07042662\n",
      "Iteration 261, loss = 0.18244016\n",
      "Iteration 351, loss = 0.07033824\n",
      "Iteration 33, loss = 0.32609691\n",
      "Iteration 34, loss = 0.32268186\n",
      "Iteration 352, loss = 0.07024780\n",
      "Iteration 35, loss = 0.31923058\n",
      "Iteration 36, loss = 0.31578971\n",
      "Iteration 101, loss = 0.16011018\n",
      "Iteration 308, loss = 0.09197032\n",
      "Iteration 309, loss = 0.09187922\n",
      "Iteration 169, loss = 0.23347337\n",
      "Iteration 119, loss = 0.15811306\n",
      "Iteration 353, loss = 0.07016949\n",
      "Iteration 354, loss = 0.07008950\n",
      "Iteration 310, loss = 0.09177700\n",
      "Iteration 102, loss = 0.15939691\n",
      "Iteration 37, loss = 0.31246807\n",
      "Iteration 355, loss = 0.07002370\n",
      "Iteration 37, loss = 0.37000165\n",
      "Iteration 262, loss = 0.18233840\n",
      "Iteration 103, loss = 0.15869400\n",
      "Iteration 120, loss = 0.15760788\n",
      "Iteration 356, loss = 0.06993977\n",
      "Iteration 38, loss = 0.30903744\n",
      "Iteration 39, loss = 0.30576663\n",
      "Iteration 357, loss = 0.06988871\n",
      "Iteration 358, loss = 0.06980321\n",
      "Iteration 311, loss = 0.09169504\n",
      "Iteration 40, loss = 0.30244072\n",
      "Iteration 312, loss = 0.09159628\n",
      "Iteration 313, loss = 0.09147681\n",
      "Iteration 121, loss = 0.15710322\n",
      "Iteration 104, loss = 0.15801680\n",
      "Iteration 122, loss = 0.15660639\n",
      "Iteration 359, loss = 0.06978239\n",
      "Iteration 41, loss = 0.29903662\n",
      "Iteration 42, loss = 0.29576987\n",
      "Iteration 170, loss = 0.23318726\n",
      "Iteration 38, loss = 0.36476113\n",
      "Iteration 360, loss = 0.06965786\n",
      "Iteration 314, loss = 0.09139435\n",
      "Iteration 361, loss = 0.06960810\n",
      "Iteration 43, loss = 0.29247576\n",
      "Iteration 44, loss = 0.28923293\n",
      "Iteration 105, loss = 0.15734512\n",
      "Iteration 315, loss = 0.09130291\n",
      "Iteration 362, loss = 0.06950725\n",
      "Iteration 263, loss = 0.18221751\n",
      "Iteration 45, loss = 0.28605517\n",
      "Iteration 123, loss = 0.15611065\n",
      "Iteration 363, loss = 0.06949635\n",
      "Iteration 106, loss = 0.15670668\n",
      "Iteration 316, loss = 0.09119743\n",
      "Iteration 39, loss = 0.35962759\n",
      "Iteration 364, loss = 0.06937684\n",
      "Iteration 46, loss = 0.28293467\n",
      "Iteration 365, loss = 0.06930417\n",
      "Iteration 317, loss = 0.09110433\n",
      "Iteration 366, loss = 0.06926192\n",
      "Iteration 47, loss = 0.27978988\n",
      "Iteration 48, loss = 0.27675816\n",
      "Iteration 124, loss = 0.15565359\n",
      "Iteration 107, loss = 0.15605576\n",
      "Iteration 318, loss = 0.09102191\n",
      "Iteration 264, loss = 0.18213897\n",
      "Iteration 367, loss = 0.06915957\n",
      "Iteration 49, loss = 0.27370939\n",
      "Iteration 368, loss = 0.06912863\n",
      "Iteration 369, loss = 0.06903514\n",
      "Iteration 50, loss = 0.27084519\n",
      "Iteration 108, loss = 0.15542653\n",
      "Iteration 370, loss = 0.06897724\n",
      "Iteration 125, loss = 0.15514172\n",
      "Iteration 319, loss = 0.09093193\n",
      "Iteration 109, loss = 0.15478659\n",
      "Iteration 51, loss = 0.26793482\n",
      "Iteration 40, loss = 0.35458917\n",
      "Iteration 171, loss = 0.23294634\n",
      "Iteration 371, loss = 0.06890360\n",
      "Iteration 320, loss = 0.09083693\n",
      "Iteration 372, loss = 0.06884459Iteration 52, loss = 0.26511241\n",
      "Iteration 110, loss = 0.15418751\n",
      "Iteration 321, loss = 0.09073077\n",
      "Iteration 53, loss = 0.26230204\n",
      "\n",
      "Iteration 54, loss = 0.25962111\n",
      "Iteration 265, loss = 0.18201771\n",
      "Iteration 322, loss = 0.09064407\n",
      "Iteration 55, loss = 0.25693372\n",
      "Iteration 373, loss = 0.06877145\n",
      "Iteration 126, loss = 0.15467469\n",
      "Iteration 172, loss = 0.23269369\n",
      "Iteration 323, loss = 0.09056945\n",
      "Iteration 56, loss = 0.25430733\n",
      "Iteration 374, loss = 0.06872054\n",
      "Iteration 41, loss = 0.34962031\n",
      "Iteration 57, loss = 0.25167493\n",
      "Iteration 111, loss = 0.15358009\n",
      "Iteration 127, loss = 0.15420442\n",
      "Iteration 324, loss = 0.09047171\n",
      "Iteration 112, loss = 0.15299146\n",
      "Iteration 113, loss = 0.15243448\n",
      "Iteration 375, loss = 0.06864482\n",
      "Iteration 58, loss = 0.24916312\n",
      "Iteration 42, loss = 0.34467728\n",
      "Iteration 376, loss = 0.06863730\n",
      "Iteration 325, loss = 0.09039626\n",
      "Iteration 128, loss = 0.15373547\n",
      "Iteration 114, loss = 0.15184811\n",
      "Iteration 377, loss = 0.06852662\n",
      "Iteration 59, loss = 0.24657720\n",
      "Iteration 378, loss = 0.06847654\n",
      "Iteration 60, loss = 0.24406259\n",
      "Iteration 61, loss = 0.24159431\n",
      "Iteration 173, loss = 0.23242063\n",
      "Iteration 379, loss = 0.06839602\n",
      "Iteration 62, loss = 0.23910550\n",
      "Iteration 380, loss = 0.06833336\n",
      "Iteration 63, loss = 0.23676163\n",
      "Iteration 326, loss = 0.09030170\n",
      "Iteration 266, loss = 0.18193547\n",
      "Iteration 64, loss = 0.23427517\n",
      "Iteration 115, loss = 0.15134845\n",
      "Iteration 381, loss = 0.06827906\n",
      "Iteration 65, loss = 0.23191311\n",
      "Iteration 43, loss = 0.33977529\n",
      "Iteration 66, loss = 0.22958868\n",
      "Iteration 382, loss = 0.06820700\n",
      "Iteration 67, loss = 0.22721794\n",
      "Iteration 327, loss = 0.09020093\n",
      "Iteration 68, loss = 0.22497112\n",
      "Iteration 69, loss = 0.22273045\n",
      "Iteration 383, loss = 0.06815586\n",
      "Iteration 70, loss = 0.22054238\n",
      "Iteration 328, loss = 0.09012603\n",
      "Iteration 129, loss = 0.15329138\n",
      "Iteration 384, loss = 0.06810244\n",
      "Iteration 71, loss = 0.21840186\n",
      "Iteration 116, loss = 0.15075107\n",
      "Iteration 72, loss = 0.21632533\n",
      "Iteration 73, loss = 0.21423294Iteration 117, loss = 0.15021506\n",
      "\n",
      "Iteration 385, loss = 0.06803317\n",
      "Iteration 130, loss = 0.15285526\n",
      "Iteration 329, loss = 0.09003115\n",
      "Iteration 267, loss = 0.18183803\n",
      "Iteration 44, loss = 0.33481741\n",
      "Iteration 386, loss = 0.06799962\n",
      "Iteration 330, loss = 0.08995541\n",
      "Iteration 131, loss = 0.15243175\n",
      "Iteration 387, loss = 0.06792951\n",
      "Iteration 118, loss = 0.14968017\n",
      "Iteration 388, loss = 0.06786903\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 45, loss = 0.32997754\n",
      "Iteration 331, loss = 0.08986437\n",
      "Iteration 332, loss = 0.08977814\n",
      "Iteration 174, loss = 0.23213948\n",
      "Iteration 119, loss = 0.14918176\n",
      "Iteration 74, loss = 0.21224252\n",
      "Iteration 333, loss = 0.08969343\n",
      "Iteration 268, loss = 0.18170717\n",
      "Iteration 75, loss = 0.21021920\n",
      "Iteration 175, loss = 0.23187802\n",
      "Iteration 132, loss = 0.15198417\n",
      "Iteration 76, loss = 0.20820805\n",
      "Iteration 334, loss = 0.08960387\n",
      "Iteration 120, loss = 0.14863668\n",
      "Iteration 335, loss = 0.08952061\n",
      "Iteration 77, loss = 0.20632100\n",
      "Iteration 121, loss = 0.14813138\n",
      "Iteration 1, loss = 0.82890132\n",
      "Iteration 269, loss = 0.18163934\n",
      "Iteration 78, loss = 0.20446798\n",
      "Iteration 46, loss = 0.32543063\n",
      "Iteration 79, loss = 0.20254667\n",
      "Iteration 336, loss = 0.08942770\n",
      "Iteration 133, loss = 0.15156506\n",
      "Iteration 2, loss = 0.78981523\n",
      "Iteration 80, loss = 0.20068714\n",
      "Iteration 337, loss = 0.08934603\n",
      "Iteration 122, loss = 0.14763934\n",
      "Iteration 47, loss = 0.32080297\n",
      "Iteration 338, loss = 0.08928656\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 81, loss = 0.19888998\n",
      "Iteration 3, loss = 0.73954733\n",
      "Iteration 134, loss = 0.15114148\n",
      "Iteration 176, loss = 0.23166719\n",
      "Iteration 123, loss = 0.14713239\n",
      "Iteration 48, loss = 0.31641622\n",
      "Iteration 270, loss = 0.18155602\n",
      "Iteration 82, loss = 0.19705528\n",
      "Iteration 4, loss = 0.68857755\n",
      "Iteration 83, loss = 0.19528773\n",
      "Iteration 135, loss = 0.15074743\n",
      "Iteration 84, loss = 0.19348838\n",
      "Iteration 124, loss = 0.14664058\n",
      "Iteration 5, loss = 0.64254492\n",
      "Iteration 85, loss = 0.19178846\n",
      "Iteration 136, loss = 0.15033517\n",
      "Iteration 86, loss = 0.19009889\n",
      "Iteration 87, loss = 0.18835549\n",
      "Iteration 88, loss = 0.18660283\n",
      "Iteration 271, loss = 0.18139254\n",
      "Iteration 125, loss = 0.14615626\n",
      "Iteration 49, loss = 0.31206346\n",
      "Iteration 1, loss = 0.93321412\n",
      "Iteration 89, loss = 0.18495816\n",
      "Iteration 6, loss = 0.60281517\n",
      "Iteration 126, loss = 0.14567861\n",
      "Iteration 177, loss = 0.23150945\n",
      "Iteration 90, loss = 0.18325187\n",
      "Iteration 91, loss = 0.18157577\n",
      "Iteration 137, loss = 0.14992285\n",
      "Iteration 92, loss = 0.17989357\n",
      "Iteration 93, loss = 0.17832162\n",
      "Iteration 7, loss = 0.57044470\n",
      "Iteration 50, loss = 0.30784515\n",
      "Iteration 127, loss = 0.14520031\n",
      "Iteration 272, loss = 0.18130586\n",
      "Iteration 138, loss = 0.14953502\n",
      "Iteration 94, loss = 0.17669731\n",
      "Iteration 2, loss = 0.88835584\n",
      "Iteration 178, loss = 0.23115620\n",
      "Iteration 8, loss = 0.54262979\n",
      "Iteration 128, loss = 0.14476453\n",
      "Iteration 95, loss = 0.17508847\n",
      "Iteration 139, loss = 0.14914037\n",
      "Iteration 9, loss = 0.52027608\n",
      "Iteration 51, loss = 0.30374507\n",
      "Iteration 129, loss = 0.14426111\n",
      "Iteration 96, loss = 0.17351685\n",
      "Iteration 97, loss = 0.17194463\n",
      "Iteration 130, loss = 0.14382210\n",
      "Iteration 98, loss = 0.17041861\n",
      "Iteration 10, loss = 0.50202643\n",
      "Iteration 273, loss = 0.18118835\n",
      "Iteration 140, loss = 0.14878054\n",
      "Iteration 3, loss = 0.82975844\n",
      "Iteration 99, loss = 0.16886210\n",
      "Iteration 100, loss = 0.16734139\n",
      "Iteration 11, loss = 0.48629184\n",
      "Iteration 52, loss = 0.29979637\n",
      "Iteration 141, loss = 0.14836821\n",
      "Iteration 101, loss = 0.16581097\n",
      "Iteration 179, loss = 0.23088924\n",
      "Iteration 102, loss = 0.16438290\n",
      "Iteration 12, loss = 0.47384806\n",
      "Iteration 131, loss = 0.14342089\n",
      "Iteration 4, loss = 0.77384281\n",
      "Iteration 103, loss = 0.16290908\n",
      "Iteration 132, loss = 0.14291622\n",
      "Iteration 274, loss = 0.18110795\n",
      "Iteration 180, loss = 0.23065148\n",
      "Iteration 133, loss = 0.14246647\n",
      "Iteration 104, loss = 0.16144361\n",
      "Iteration 142, loss = 0.14797871\n",
      "Iteration 105, loss = 0.16004831\n",
      "Iteration 53, loss = 0.29590420\n",
      "Iteration 106, loss = 0.15858993\n",
      "Iteration 107, loss = 0.15720260\n",
      "Iteration 143, loss = 0.14761429\n",
      "Iteration 108, loss = 0.15577620\n",
      "Iteration 134, loss = 0.14205510\n",
      "Iteration 13, loss = 0.46296210\n",
      "Iteration 54, loss = 0.29208811\n",
      "Iteration 14, loss = 0.45406330\n",
      "Iteration 135, loss = 0.14159746\n",
      "Iteration 109, loss = 0.15442458\n",
      "Iteration 181, loss = 0.23042027\n",
      "Iteration 110, loss = 0.15311927\n",
      "Iteration 111, loss = 0.15175424\n",
      "Iteration 15, loss = 0.44572475\n",
      "Iteration 275, loss = 0.18102162\n",
      "Iteration 136, loss = 0.14116826\n",
      "Iteration 5, loss = 0.72227731\n",
      "Iteration 144, loss = 0.14725774\n",
      "Iteration 16, loss = 0.43870551\n",
      "Iteration 145, loss = 0.14687515\n",
      "Iteration 112, loss = 0.15049596\n",
      "Iteration 113, loss = 0.14919468\n",
      "Iteration 137, loss = 0.14076872Iteration 6, loss = 0.68044352\n",
      "\n",
      "Iteration 55, loss = 0.28832902\n",
      "Iteration 276, loss = 0.18090961\n",
      "Iteration 114, loss = 0.14790366\n",
      "Iteration 115, loss = 0.14667097\n",
      "Iteration 182, loss = 0.23017692\n",
      "Iteration 138, loss = 0.14031829\n",
      "Iteration 116, loss = 0.14547209\n",
      "Iteration 146, loss = 0.14649299\n",
      "Iteration 17, loss = 0.43221617\n",
      "Iteration 117, loss = 0.14425983\n",
      "Iteration 139, loss = 0.13990775\n",
      "Iteration 140, loss = 0.13950160\n",
      "Iteration 147, loss = 0.14615222\n",
      "Iteration 118, loss = 0.14307879\n",
      "Iteration 119, loss = 0.14192187\n",
      "Iteration 148, loss = 0.14578209\n",
      "Iteration 18, loss = 0.42625705\n",
      "Iteration 183, loss = 0.22996859\n",
      "Iteration 141, loss = 0.13911848\n",
      "Iteration 277, loss = 0.18084265\n",
      "Iteration 56, loss = 0.28483248\n",
      "Iteration 120, loss = 0.14073574\n",
      "Iteration 7, loss = 0.64516488\n",
      "Iteration 121, loss = 0.13962523\n",
      "Iteration 122, loss = 0.13855189\n",
      "Iteration 123, loss = 0.13747218\n",
      "Iteration 19, loss = 0.42072017\n",
      "Iteration 142, loss = 0.13869430\n",
      "Iteration 57, loss = 0.28149837\n",
      "Iteration 149, loss = 0.14546023\n",
      "Iteration 124, loss = 0.13645074\n",
      "Iteration 8, loss = 0.61634408\n",
      "Iteration 125, loss = 0.13537570\n",
      "Iteration 278, loss = 0.18072689\n",
      "Iteration 126, loss = 0.13436900\n",
      "Iteration 143, loss = 0.13829877\n",
      "Iteration 20, loss = 0.41540214\n",
      "Iteration 144, loss = 0.13791714\n",
      "Iteration 150, loss = 0.14508762\n",
      "Iteration 21, loss = 0.41016602\n",
      "Iteration 127, loss = 0.13334686\n",
      "Iteration 184, loss = 0.22973463\n",
      "Iteration 128, loss = 0.13241694\n",
      "Iteration 58, loss = 0.27799544\n",
      "Iteration 129, loss = 0.13142555\n",
      "Iteration 145, loss = 0.13752913\n",
      "Iteration 130, loss = 0.13054351\n",
      "Iteration 151, loss = 0.14475394\n",
      "Iteration 9, loss = 0.59336760\n",
      "Iteration 146, loss = 0.13714633\n",
      "Iteration 279, loss = 0.18062078\n",
      "Iteration 22, loss = 0.40516166\n",
      "Iteration 131, loss = 0.12959044\n",
      "Iteration 59, loss = 0.27476756\n",
      "Iteration 132, loss = 0.12871168\n",
      "Iteration 23, loss = 0.40022168\n",
      "Iteration 133, loss = 0.12779566\n",
      "Iteration 134, loss = 0.12695021\n",
      "Iteration 135, loss = 0.12607882\n",
      "Iteration 152, loss = 0.14441726\n",
      "Iteration 10, loss = 0.57455321\n",
      "Iteration 147, loss = 0.13676653\n",
      "Iteration 24, loss = 0.39528123\n",
      "Iteration 153, loss = 0.14408795\n",
      "Iteration 185, loss = 0.22945982\n",
      "Iteration 60, loss = 0.27158994\n",
      "Iteration 148, loss = 0.13639319\n",
      "Iteration 280, loss = 0.18052350\n",
      "Iteration 136, loss = 0.12525678\n",
      "Iteration 137, loss = 0.12445922\n",
      "Iteration 25, loss = 0.39056281\n",
      "Iteration 138, loss = 0.12367236\n",
      "Iteration 149, loss = 0.13602176\n",
      "Iteration 154, loss = 0.14376872\n",
      "Iteration 26, loss = 0.38564230\n",
      "Iteration 139, loss = 0.12290621\n",
      "Iteration 61, loss = 0.26860878\n",
      "Iteration 155, loss = 0.14345348\n",
      "Iteration 140, loss = 0.12213399\n",
      "Iteration 141, loss = 0.12135771\n",
      "Iteration 150, loss = 0.13564884\n",
      "Iteration 186, loss = 0.22925908\n",
      "Iteration 142, loss = 0.12064006\n",
      "Iteration 27, loss = 0.38081789\n",
      "Iteration 156, loss = 0.14309760\n",
      "Iteration 11, loss = 0.55834414\n",
      "Iteration 281, loss = 0.18045824\n",
      "Iteration 143, loss = 0.11992039\n",
      "Iteration 62, loss = 0.26568072\n",
      "Iteration 144, loss = 0.11918432\n",
      "Iteration 151, loss = 0.13531602\n",
      "Iteration 152, loss = 0.13493415\n",
      "Iteration 28, loss = 0.37603249\n",
      "Iteration 145, loss = 0.11854458\n",
      "Iteration 157, loss = 0.14280253\n",
      "Iteration 146, loss = 0.11786866\n",
      "Iteration 63, loss = 0.26281797\n",
      "Iteration 153, loss = 0.13459358\n",
      "Iteration 187, loss = 0.22906840\n",
      "Iteration 147, loss = 0.11719654\n",
      "Iteration 282, loss = 0.18033693\n",
      "Iteration 29, loss = 0.37131320\n",
      "Iteration 12, loss = 0.54489583\n",
      "Iteration 148, loss = 0.11650371\n",
      "Iteration 154, loss = 0.13426904\n",
      "Iteration 158, loss = 0.14247216\n",
      "Iteration 149, loss = 0.11591070\n",
      "Iteration 150, loss = 0.11523147\n",
      "Iteration 30, loss = 0.36657492\n",
      "Iteration 155, loss = 0.13395115\n",
      "Iteration 64, loss = 0.26009771\n",
      "Iteration 151, loss = 0.11464104\n",
      "Iteration 159, loss = 0.14215786\n",
      "Iteration 152, loss = 0.11404083\n",
      "Iteration 13, loss = 0.53324435\n",
      "Iteration 156, loss = 0.13357212\n",
      "Iteration 283, loss = 0.18028046\n",
      "Iteration 153, loss = 0.11346013\n",
      "Iteration 154, loss = 0.11285920\n",
      "Iteration 157, loss = 0.13324785\n",
      "Iteration 188, loss = 0.22882394\n",
      "Iteration 31, loss = 0.36193027\n",
      "Iteration 65, loss = 0.25748598\n",
      "Iteration 155, loss = 0.11229614\n",
      "Iteration 156, loss = 0.11174226\n",
      "Iteration 160, loss = 0.14185821\n",
      "Iteration 32, loss = 0.35721042\n",
      "Iteration 157, loss = 0.11119084\n",
      "Iteration 284, loss = 0.18018518\n",
      "Iteration 161, loss = 0.14154566\n",
      "Iteration 158, loss = 0.11068158\n",
      "Iteration 158, loss = 0.13294112\n",
      "Iteration 66, loss = 0.25493698\n",
      "Iteration 159, loss = 0.13259447\n",
      "Iteration 14, loss = 0.52268612\n",
      "Iteration 159, loss = 0.11013697\n",
      "Iteration 160, loss = 0.13227467\n",
      "Iteration 189, loss = 0.22859604\n",
      "Iteration 162, loss = 0.14124749\n",
      "Iteration 160, loss = 0.10962694\n",
      "Iteration 163, loss = 0.14094658\n",
      "Iteration 33, loss = 0.35254369\n",
      "Iteration 161, loss = 0.13196381\n",
      "Iteration 67, loss = 0.25257398\n",
      "Iteration 161, loss = 0.10914559\n",
      "Iteration 34, loss = 0.34808952\n",
      "Iteration 162, loss = 0.10867994\n",
      "Iteration 285, loss = 0.18009946\n",
      "Iteration 162, loss = 0.13165853\n",
      "Iteration 163, loss = 0.10816215\n",
      "Iteration 163, loss = 0.13134843\n",
      "Iteration 164, loss = 0.10770656\n",
      "Iteration 35, loss = 0.34350512\n",
      "Iteration 165, loss = 0.10725254\n",
      "Iteration 164, loss = 0.14066882\n",
      "Iteration 68, loss = 0.25018417\n",
      "Iteration 15, loss = 0.51290187\n",
      "Iteration 190, loss = 0.22839256\n",
      "Iteration 166, loss = 0.10680426\n",
      "Iteration 164, loss = 0.13107899\n",
      "Iteration 36, loss = 0.33882749\n",
      "Iteration 167, loss = 0.10635727\n",
      "Iteration 168, loss = 0.10590229\n",
      "Iteration 169, loss = 0.10547636\n",
      "Iteration 170, loss = 0.10508510\n",
      "Iteration 165, loss = 0.13072759\n",
      "Iteration 171, loss = 0.10466089\n",
      "Iteration 286, loss = 0.18001034\n",
      "Iteration 165, loss = 0.14034962\n",
      "Iteration 172, loss = 0.10426866\n",
      "Iteration 37, loss = 0.33432479\n",
      "Iteration 69, loss = 0.24780771\n",
      "Iteration 166, loss = 0.14003500\n",
      "Iteration 166, loss = 0.13045550\n",
      "Iteration 16, loss = 0.50392445\n",
      "Iteration 191, loss = 0.22824440\n",
      "Iteration 38, loss = 0.32997741\n",
      "Iteration 287, loss = 0.17990099\n",
      "Iteration 70, loss = 0.24566024\n",
      "Iteration 173, loss = 0.10386699\n",
      "Iteration 167, loss = 0.13013699\n",
      "Iteration 174, loss = 0.10348136\n",
      "Iteration 168, loss = 0.12985021\n",
      "Iteration 39, loss = 0.32545194\n",
      "Iteration 175, loss = 0.10309046\n",
      "Iteration 167, loss = 0.13977264\n",
      "Iteration 176, loss = 0.10271318\n",
      "Iteration 40, loss = 0.32108503\n",
      "Iteration 177, loss = 0.10234700\n",
      "Iteration 41, loss = 0.31667059\n",
      "Iteration 169, loss = 0.12957799\n",
      "Iteration 178, loss = 0.10203714\n",
      "Iteration 179, loss = 0.10162174\n",
      "Iteration 17, loss = 0.49534567\n",
      "Iteration 288, loss = 0.17980171\n",
      "Iteration 170, loss = 0.12925449\n",
      "Iteration 192, loss = 0.22794917\n",
      "Iteration 168, loss = 0.13945884\n",
      "Iteration 71, loss = 0.24345690\n",
      "Iteration 180, loss = 0.10126181\n",
      "Iteration 181, loss = 0.10092146\n",
      "Iteration 182, loss = 0.10057364\n",
      "Iteration 72, loss = 0.24144997\n",
      "Iteration 183, loss = 0.10022714\n",
      "Iteration 169, loss = 0.13919276\n",
      "Iteration 184, loss = 0.09992316\n",
      "Iteration 171, loss = 0.12897064\n",
      "Iteration 185, loss = 0.09958820\n",
      "Iteration 42, loss = 0.31258274\n",
      "Iteration 18, loss = 0.48697843\n",
      "Iteration 289, loss = 0.17973531\n",
      "Iteration 43, loss = 0.30839856\n",
      "Iteration 186, loss = 0.09922493\n",
      "Iteration 172, loss = 0.12871043\n",
      "Iteration 170, loss = 0.13891047\n",
      "Iteration 193, loss = 0.22775722\n",
      "Iteration 187, loss = 0.09890289\n",
      "Iteration 173, loss = 0.12841833\n",
      "Iteration 188, loss = 0.09858044\n",
      "Iteration 73, loss = 0.23944944\n",
      "Iteration 189, loss = 0.09829491\n",
      "Iteration 44, loss = 0.30436687\n",
      "Iteration 190, loss = 0.09797675\n",
      "Iteration 171, loss = 0.13861224\n",
      "Iteration 174, loss = 0.12815709\n",
      "Iteration 74, loss = 0.23756769\n",
      "Iteration 45, loss = 0.30044050\n",
      "Iteration 19, loss = 0.47904044\n",
      "Iteration 290, loss = 0.17964736\n",
      "Iteration 191, loss = 0.09767656\n",
      "Iteration 175, loss = 0.12786036\n",
      "Iteration 194, loss = 0.22754267\n",
      "Iteration 192, loss = 0.09739060\n",
      "Iteration 172, loss = 0.13835471\n",
      "Iteration 46, loss = 0.29630417\n",
      "Iteration 193, loss = 0.09707400\n",
      "Iteration 173, loss = 0.13805594\n",
      "Iteration 194, loss = 0.09681621\n",
      "Iteration 176, loss = 0.12757966\n",
      "Iteration 195, loss = 0.09651373\n",
      "Iteration 291, loss = 0.17953290\n",
      "Iteration 177, loss = 0.12733747\n",
      "Iteration 75, loss = 0.23564263\n",
      "Iteration 178, loss = 0.12704955\n",
      "Iteration 196, loss = 0.09626368\n",
      "Iteration 174, loss = 0.13777979\n",
      "Iteration 197, loss = 0.09593772\n",
      "Iteration 47, loss = 0.29260648\n",
      "Iteration 179, loss = 0.12682059\n",
      "Iteration 195, loss = 0.22736349\n",
      "Iteration 20, loss = 0.47133066\n",
      "Iteration 198, loss = 0.09565444\n",
      "Iteration 76, loss = 0.23389379\n",
      "Iteration 48, loss = 0.28864208\n",
      "Iteration 199, loss = 0.09536621\n",
      "Iteration 180, loss = 0.12653300\n",
      "Iteration 200, loss = 0.09512564\n",
      "Iteration 175, loss = 0.13749837\n",
      "Iteration 292, loss = 0.17946972\n",
      "Iteration 201, loss = 0.09482976\n",
      "Iteration 181, loss = 0.12627471\n",
      "Iteration 77, loss = 0.23214251\n",
      "Iteration 21, loss = 0.46384189\n",
      "Iteration 49, loss = 0.28503648\n",
      "Iteration 182, loss = 0.12603338\n",
      "Iteration 202, loss = 0.09455696\n",
      "Iteration 203, loss = 0.09430012\n",
      "Iteration 204, loss = 0.09407029\n",
      "Iteration 205, loss = 0.09376366\n",
      "Iteration 50, loss = 0.28135225\n",
      "Iteration 196, loss = 0.22713877\n",
      "Iteration 176, loss = 0.13723517\n",
      "Iteration 177, loss = 0.13698063\n",
      "Iteration 78, loss = 0.23044661\n",
      "Iteration 183, loss = 0.12579078\n",
      "Iteration 51, loss = 0.27786277\n",
      "Iteration 293, loss = 0.17936537\n",
      "Iteration 22, loss = 0.45648584\n",
      "Iteration 178, loss = 0.13670854\n",
      "Iteration 206, loss = 0.09351295\n",
      "Iteration 184, loss = 0.12551227\n",
      "Iteration 207, loss = 0.09327022\n",
      "Iteration 208, loss = 0.09300690\n",
      "Iteration 52, loss = 0.27435756\n",
      "Iteration 79, loss = 0.22888101\n",
      "Iteration 209, loss = 0.09274884\n",
      "Iteration 185, loss = 0.12526524\n",
      "Iteration 210, loss = 0.09252193\n",
      "Iteration 197, loss = 0.22695878\n",
      "Iteration 211, loss = 0.09226463\n",
      "Iteration 294, loss = 0.17934171\n",
      "Iteration 179, loss = 0.13644042\n",
      "Iteration 23, loss = 0.44916526\n",
      "Iteration 186, loss = 0.12501341\n",
      "Iteration 187, loss = 0.12478488\n",
      "Iteration 212, loss = 0.09206293\n",
      "Iteration 80, loss = 0.22722534\n",
      "Iteration 180, loss = 0.13618059\n",
      "Iteration 53, loss = 0.27100678\n",
      "Iteration 213, loss = 0.09178821\n",
      "Iteration 214, loss = 0.09158520\n",
      "Iteration 215, loss = 0.09133923\n",
      "Iteration 54, loss = 0.26754230\n",
      "Iteration 295, loss = 0.17920174\n",
      "Iteration 81, loss = 0.22569122\n",
      "Iteration 198, loss = 0.22676356\n",
      "Iteration 181, loss = 0.13591808\n",
      "Iteration 188, loss = 0.12454080\n",
      "Iteration 24, loss = 0.44196806\n",
      "Iteration 55, loss = 0.26435612\n",
      "Iteration 216, loss = 0.09109875\n",
      "Iteration 217, loss = 0.09087710\n",
      "Iteration 218, loss = 0.09066001\n",
      "Iteration 82, loss = 0.22419862\n",
      "Iteration 189, loss = 0.12429655\n",
      "Iteration 219, loss = 0.09046042\n",
      "Iteration 220, loss = 0.09021600\n",
      "Iteration 56, loss = 0.26121146\n",
      "Iteration 182, loss = 0.13567261\n",
      "Iteration 296, loss = 0.17911924\n",
      "Iteration 25, loss = 0.43505107\n",
      "Iteration 57, loss = 0.25808617\n",
      "Iteration 190, loss = 0.12408867\n",
      "Iteration 199, loss = 0.22661233\n",
      "Iteration 183, loss = 0.13541213\n",
      "Iteration 221, loss = 0.09000284\n",
      "Iteration 83, loss = 0.22276325\n",
      "Iteration 222, loss = 0.08984530\n",
      "Iteration 184, loss = 0.13518017\n",
      "Iteration 191, loss = 0.12386343\n",
      "Iteration 58, loss = 0.25507478\n",
      "Iteration 223, loss = 0.08961354\n",
      "Iteration 297, loss = 0.17905231\n",
      "Iteration 192, loss = 0.12360174\n",
      "Iteration 224, loss = 0.08937135\n",
      "Iteration 59, loss = 0.25205504\n",
      "Iteration 225, loss = 0.08915889\n",
      "Iteration 193, loss = 0.12338148\n",
      "Iteration 84, loss = 0.22137998\n",
      "Iteration 26, loss = 0.42812437\n",
      "Iteration 185, loss = 0.13492840\n",
      "Iteration 60, loss = 0.24916276\n",
      "Iteration 200, loss = 0.22639750\n",
      "Iteration 226, loss = 0.08897467\n",
      "Iteration 298, loss = 0.17894450\n",
      "Iteration 194, loss = 0.12315050\n",
      "Iteration 227, loss = 0.08877149\n",
      "Iteration 195, loss = 0.12291608\n",
      "Iteration 228, loss = 0.08856236\n",
      "Iteration 229, loss = 0.08836781\n",
      "Iteration 61, loss = 0.24635735\n",
      "Iteration 186, loss = 0.13466184\n",
      "Iteration 85, loss = 0.21998922\n",
      "Iteration 196, loss = 0.12268681\n",
      "Iteration 230, loss = 0.08818214\n",
      "Iteration 197, loss = 0.12247568\n",
      "Iteration 62, loss = 0.24367095\n",
      "Iteration 27, loss = 0.42114156\n",
      "Iteration 231, loss = 0.08802645\n",
      "Iteration 299, loss = 0.17885840\n",
      "Iteration 232, loss = 0.08781391\n",
      "Iteration 187, loss = 0.13442621\n",
      "Iteration 201, loss = 0.22622191\n",
      "Iteration 188, loss = 0.13418906\n",
      "Iteration 233, loss = 0.08762193\n",
      "Iteration 198, loss = 0.12226202\n",
      "Iteration 234, loss = 0.08742134\n",
      "Iteration 63, loss = 0.24099897\n",
      "Iteration 199, loss = 0.12205216\n",
      "Iteration 235, loss = 0.08729513\n",
      "Iteration 86, loss = 0.21873559\n",
      "Iteration 236, loss = 0.08708485\n",
      "Iteration 200, loss = 0.12182686\n",
      "Iteration 28, loss = 0.41428641\n",
      "Iteration 189, loss = 0.13396962\n",
      "Iteration 201, loss = 0.12161458\n",
      "Iteration 64, loss = 0.23845377\n",
      "Iteration 87, loss = 0.21739957\n",
      "Iteration 300, loss = 0.17877277\n",
      "Iteration 237, loss = 0.08692165\n",
      "Iteration 238, loss = 0.08674479\n",
      "Iteration 202, loss = 0.22608884\n",
      "Iteration 190, loss = 0.13372702\n",
      "Iteration 239, loss = 0.08660818\n",
      "Iteration 29, loss = 0.40759039\n",
      "Iteration 202, loss = 0.12142513\n",
      "Iteration 65, loss = 0.23596330\n",
      "Iteration 240, loss = 0.08640804\n",
      "Iteration 203, loss = 0.12118682\n",
      "Iteration 88, loss = 0.21618805\n",
      "Iteration 241, loss = 0.08622767\n",
      "Iteration 66, loss = 0.23356010\n",
      "Iteration 204, loss = 0.12100602\n",
      "Iteration 191, loss = 0.13350016\n",
      "Iteration 301, loss = 0.17868501\n",
      "Iteration 242, loss = 0.08604965\n",
      "Iteration 30, loss = 0.40130580\n",
      "Iteration 192, loss = 0.13327526\n",
      "Iteration 67, loss = 0.23116784\n",
      "Iteration 89, loss = 0.21500232\n",
      "Iteration 243, loss = 0.08588491\n",
      "Iteration 205, loss = 0.12080554\n",
      "Iteration 203, loss = 0.22581489\n",
      "Iteration 244, loss = 0.08573263\n",
      "Iteration 302, loss = 0.17862297\n",
      "Iteration 245, loss = 0.08559095\n",
      "Iteration 193, loss = 0.13305388\n",
      "Iteration 206, loss = 0.12060242\n",
      "Iteration 246, loss = 0.08540143\n",
      "Iteration 90, loss = 0.21387096\n",
      "Iteration 31, loss = 0.39472688\n",
      "Iteration 68, loss = 0.22891688\n",
      "Iteration 247, loss = 0.08526708\n",
      "Iteration 194, loss = 0.13282590\n",
      "Iteration 207, loss = 0.12040024\n",
      "Iteration 69, loss = 0.22666722\n",
      "Iteration 248, loss = 0.08510027\n",
      "Iteration 204, loss = 0.22568397\n",
      "Iteration 249, loss = 0.08497021\n",
      "Iteration 208, loss = 0.12018672\n",
      "Iteration 70, loss = 0.22450510\n",
      "Iteration 195, loss = 0.13260365\n",
      "Iteration 91, loss = 0.21269668\n",
      "Iteration 32, loss = 0.38863828\n",
      "Iteration 250, loss = 0.08480330Iteration 209, loss = 0.11998496\n",
      "\n",
      "Iteration 303, loss = 0.17852463\n",
      "Iteration 251, loss = 0.08464358\n",
      "Iteration 71, loss = 0.22239933\n",
      "Iteration 252, loss = 0.08452436\n",
      "Iteration 205, loss = 0.22545723\n",
      "Iteration 253, loss = 0.08436849\n",
      "Iteration 254, loss = 0.08429519\n",
      "Iteration 92, loss = 0.21160967\n",
      "Iteration 210, loss = 0.11978977\n",
      "Iteration 72, loss = 0.22043392\n",
      "Iteration 304, loss = 0.17846633\n",
      "Iteration 196, loss = 0.13236754\n",
      "Iteration 33, loss = 0.38250741\n",
      "Iteration 73, loss = 0.21845817\n",
      "Iteration 255, loss = 0.08405711\n",
      "Iteration 211, loss = 0.11961222\n",
      "Iteration 197, loss = 0.13216782\n",
      "Iteration 256, loss = 0.08394724\n",
      "Iteration 198, loss = 0.13194584\n",
      "Iteration 257, loss = 0.08376855\n",
      "Iteration 258, loss = 0.08361735\n",
      "Iteration 212, loss = 0.11943136\n",
      "Iteration 259, loss = 0.08349679\n",
      "Iteration 74, loss = 0.21659438\n",
      "Iteration 93, loss = 0.21052076\n",
      "Iteration 305, loss = 0.17838073\n",
      "Iteration 213, loss = 0.11924478\n",
      "Iteration 199, loss = 0.13175061\n",
      "Iteration 206, loss = 0.22524973\n",
      "Iteration 34, loss = 0.37646373\n",
      "Iteration 260, loss = 0.08337384\n",
      "Iteration 94, loss = 0.20952210\n",
      "Iteration 75, loss = 0.21472397\n",
      "Iteration 214, loss = 0.11907321\n",
      "Iteration 261, loss = 0.08324258\n",
      "Iteration 306, loss = 0.17828602\n",
      "Iteration 95, loss = 0.20848507\n",
      "Iteration 262, loss = 0.08308090\n",
      "Iteration 76, loss = 0.21302448\n",
      "Iteration 263, loss = 0.08299186\n",
      "Iteration 215, loss = 0.11886039\n",
      "Iteration 200, loss = 0.13150983\n",
      "Iteration 264, loss = 0.08285254\n",
      "Iteration 77, loss = 0.21126471\n",
      "Iteration 265, loss = 0.08273062\n",
      "Iteration 216, loss = 0.11869100\n",
      "Iteration 266, loss = 0.08258473\n",
      "Iteration 201, loss = 0.13132632\n",
      "Iteration 267, loss = 0.08244650\n",
      "Iteration 35, loss = 0.37059391\n",
      "Iteration 207, loss = 0.22510022\n",
      "Iteration 217, loss = 0.11852118\n",
      "Iteration 268, loss = 0.08234964\n",
      "Iteration 96, loss = 0.20750785\n",
      "Iteration 269, loss = 0.08219714\n",
      "Iteration 202, loss = 0.13110876\n",
      "Iteration 307, loss = 0.17822042\n",
      "Iteration 218, loss = 0.11832568\n",
      "Iteration 270, loss = 0.08213638\n",
      "Iteration 271, loss = 0.08195366\n",
      "Iteration 36, loss = 0.36488497\n",
      "Iteration 208, loss = 0.22490962\n",
      "Iteration 78, loss = 0.20968288\n",
      "Iteration 219, loss = 0.11813018\n",
      "Iteration 272, loss = 0.08186675\n",
      "Iteration 203, loss = 0.13090520\n",
      "Iteration 79, loss = 0.20803669\n",
      "Iteration 97, loss = 0.20654284\n",
      "Iteration 273, loss = 0.08171856\n",
      "Iteration 204, loss = 0.13068869\n",
      "Iteration 274, loss = 0.08159726\n",
      "Iteration 308, loss = 0.17813490\n",
      "Iteration 80, loss = 0.20649000\n",
      "Iteration 37, loss = 0.35941640\n",
      "Iteration 220, loss = 0.11796385\n",
      "Iteration 275, loss = 0.08149529\n",
      "Iteration 276, loss = 0.08139607\n",
      "Iteration 277, loss = 0.08127002\n",
      "Iteration 209, loss = 0.22473389\n",
      "Iteration 221, loss = 0.11779389\n",
      "Iteration 205, loss = 0.13048111\n",
      "Iteration 278, loss = 0.08113358\n",
      "Iteration 81, loss = 0.20497288\n",
      "Iteration 279, loss = 0.08100794\n",
      "Iteration 280, loss = 0.08091348\n",
      "Iteration 281, loss = 0.08078348\n",
      "Iteration 82, loss = 0.20351404\n",
      "Iteration 282, loss = 0.08067349\n",
      "Iteration 98, loss = 0.20560083\n",
      "Iteration 38, loss = 0.35395106\n",
      "Iteration 309, loss = 0.17806325\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 206, loss = 0.13028052\n",
      "Iteration 222, loss = 0.11762178\n",
      "Iteration 283, loss = 0.08058460\n",
      "Iteration 83, loss = 0.20210612\n",
      "Iteration 99, loss = 0.20469570\n",
      "Iteration 100, loss = 0.20381663\n",
      "Iteration 284, loss = 0.08044600\n",
      "Iteration 223, loss = 0.11745207\n",
      "Iteration 285, loss = 0.08035426\n",
      "Iteration 1, loss = 0.74122940\n",
      "Iteration 84, loss = 0.20074288\n",
      "Iteration 210, loss = 0.22454673\n",
      "Iteration 85, loss = 0.19939328\n",
      "Iteration 2, loss = 0.72026118\n",
      "Iteration 39, loss = 0.34870828\n",
      "Iteration 207, loss = 0.13008289\n",
      "Iteration 3, loss = 0.69230847\n",
      "Iteration 224, loss = 0.11727571\n",
      "Iteration 286, loss = 0.08022017\n",
      "Iteration 4, loss = 0.66286262Iteration 287, loss = 0.08012390\n",
      "Iteration 86, loss = 0.19811592\n",
      "Iteration 288, loss = 0.07999676\n",
      "\n",
      "Iteration 289, loss = 0.07990088\n",
      "Iteration 208, loss = 0.12986784\n",
      "Iteration 225, loss = 0.11711730\n",
      "Iteration 87, loss = 0.19681216\n",
      "Iteration 290, loss = 0.07979556\n",
      "Iteration 209, loss = 0.12968171\n",
      "Iteration 291, loss = 0.07971296\n",
      "Iteration 5, loss = 0.63547841\n",
      "Iteration 101, loss = 0.20293733\n",
      "Iteration 292, loss = 0.07957585\n",
      "Iteration 226, loss = 0.11694887\n",
      "Iteration 40, loss = 0.34356669\n",
      "Iteration 88, loss = 0.19562224\n",
      "Iteration 211, loss = 0.22443158\n",
      "Iteration 6, loss = 0.60996569\n",
      "Iteration 293, loss = 0.07947569\n",
      "Iteration 7, loss = 0.58751654\n",
      "Iteration 210, loss = 0.12949324\n",
      "Iteration 8, loss = 0.56795578\n",
      "Iteration 227, loss = 0.11677406\n",
      "Iteration 294, loss = 0.07936566\n",
      "Iteration 89, loss = 0.19442586\n",
      "Iteration 228, loss = 0.11664240\n",
      "Iteration 102, loss = 0.20210237\n",
      "Iteration 295, loss = 0.07928887\n",
      "Iteration 9, loss = 0.54992902\n",
      "Iteration 41, loss = 0.33871467\n",
      "Iteration 296, loss = 0.07917015\n",
      "Iteration 229, loss = 0.11646624\n",
      "Iteration 297, loss = 0.07907002\n",
      "Iteration 10, loss = 0.53446730\n",
      "Iteration 212, loss = 0.22426332\n",
      "Iteration 298, loss = 0.07897149\n",
      "Iteration 11, loss = 0.51991454\n",
      "Iteration 211, loss = 0.12929161\n",
      "Iteration 299, loss = 0.07887890\n",
      "Iteration 300, loss = 0.07878152\n",
      "Iteration 103, loss = 0.20128448\n",
      "Iteration 90, loss = 0.19322461\n",
      "Iteration 230, loss = 0.11629387\n",
      "Iteration 301, loss = 0.07867166\n",
      "Iteration 231, loss = 0.11612429\n",
      "Iteration 302, loss = 0.07859268\n",
      "Iteration 12, loss = 0.50762448\n",
      "Iteration 212, loss = 0.12910076\n",
      "Iteration 13, loss = 0.49599306\n",
      "Iteration 213, loss = 0.12890195\n",
      "Iteration 91, loss = 0.19218645\n",
      "Iteration 42, loss = 0.33392740\n",
      "Iteration 14, loss = 0.48571198\n",
      "Iteration 104, loss = 0.20048856\n",
      "Iteration 15, loss = 0.47620557\n",
      "Iteration 303, loss = 0.07849387\n",
      "Iteration 16, loss = 0.46779067\n",
      "Iteration 304, loss = 0.07837827\n",
      "Iteration 305, loss = 0.07831108\n",
      "Iteration 214, loss = 0.12875360\n",
      "Iteration 232, loss = 0.11599630\n",
      "Iteration 213, loss = 0.22407979\n",
      "Iteration 233, loss = 0.11582765\n",
      "Iteration 92, loss = 0.19098175\n",
      "Iteration 306, loss = 0.07819478\n",
      "Iteration 93, loss = 0.18991209\n",
      "Iteration 234, loss = 0.11566098\n",
      "Iteration 105, loss = 0.19968546\n",
      "Iteration 43, loss = 0.32933133\n",
      "Iteration 307, loss = 0.07811239\n",
      "Iteration 17, loss = 0.46008183\n",
      "Iteration 215, loss = 0.12854425\n",
      "Iteration 308, loss = 0.07803730\n",
      "Iteration 216, loss = 0.12834378\n",
      "Iteration 214, loss = 0.22390066\n",
      "Iteration 94, loss = 0.18884788\n",
      "Iteration 217, loss = 0.12815529\n",
      "Iteration 309, loss = 0.07791186\n",
      "Iteration 18, loss = 0.45287792\n",
      "Iteration 218, loss = 0.12797627\n",
      "Iteration 310, loss = 0.07785441\n",
      "Iteration 19, loss = 0.44631109\n",
      "Iteration 311, loss = 0.07775611\n",
      "Iteration 44, loss = 0.32494373\n",
      "Iteration 20, loss = 0.44038957\n",
      "Iteration 95, loss = 0.18781355\n",
      "Iteration 312, loss = 0.07766787\n",
      "Iteration 313, loss = 0.07755234\n",
      "Iteration 21, loss = 0.43508569\n",
      "Iteration 22, loss = 0.43008481\n",
      "Iteration 23, loss = 0.42571986\n",
      "Iteration 219, loss = 0.12780601\n",
      "Iteration 314, loss = 0.07750707\n",
      "Iteration 215, loss = 0.22371362\n",
      "Iteration 96, loss = 0.18682747\n",
      "Iteration 315, loss = 0.07739349\n",
      "Iteration 24, loss = 0.42160384\n",
      "Iteration 316, loss = 0.07728583\n",
      "Iteration 317, loss = 0.07722252\n",
      "Iteration 235, loss = 0.11550147\n",
      "Iteration 25, loss = 0.41799566\n",
      "Iteration 236, loss = 0.11535241\n",
      "Iteration 97, loss = 0.18588845\n",
      "Iteration 106, loss = 0.19892993\n",
      "Iteration 26, loss = 0.41470823\n",
      "Iteration 237, loss = 0.11520672\n",
      "Iteration 107, loss = 0.19818140\n",
      "Iteration 318, loss = 0.07714520\n",
      "Iteration 27, loss = 0.41168839\n",
      "Iteration 238, loss = 0.11505372\n",
      "Iteration 45, loss = 0.32064568\n",
      "Iteration 28, loss = 0.40907112\n",
      "Iteration 239, loss = 0.11488978\n",
      "Iteration 319, loss = 0.07706585\n",
      "Iteration 220, loss = 0.12762857\n",
      "Iteration 98, loss = 0.18483834\n",
      "Iteration 46, loss = 0.31657354\n",
      "Iteration 221, loss = 0.12743901\n",
      "Iteration 320, loss = 0.07699996\n",
      "Iteration 321, loss = 0.07689469\n",
      "Iteration 322, loss = 0.07680009\n",
      "Iteration 216, loss = 0.22356359\n",
      "Iteration 99, loss = 0.18396124\n",
      "Iteration 240, loss = 0.11475405\n",
      "Iteration 29, loss = 0.40659294\n",
      "Iteration 108, loss = 0.19745607\n",
      "Iteration 323, loss = 0.07673603\n",
      "Iteration 324, loss = 0.07662589\n",
      "Iteration 30, loss = 0.40408967\n",
      "Iteration 325, loss = 0.07655769\n",
      "Iteration 241, loss = 0.11462991\n",
      "Iteration 31, loss = 0.40195353\n",
      "Iteration 326, loss = 0.07648029\n",
      "Iteration 222, loss = 0.12727227\n",
      "Iteration 32, loss = 0.40016584\n",
      "Iteration 327, loss = 0.07641151\n",
      "Iteration 33, loss = 0.39819485\n",
      "Iteration 242, loss = 0.11445202Iteration 109, loss = 0.19670578\n",
      "Iteration 47, loss = 0.31270053\n",
      "\n",
      "Iteration 217, loss = 0.22338767\n",
      "Iteration 100, loss = 0.18298507\n",
      "Iteration 34, loss = 0.39644564\n",
      "Iteration 328, loss = 0.07632263\n",
      "Iteration 329, loss = 0.07624868\n",
      "Iteration 35, loss = 0.39486305\n",
      "Iteration 223, loss = 0.12709208\n",
      "Iteration 243, loss = 0.11430704\n",
      "Iteration 36, loss = 0.39328453\n",
      "Iteration 330, loss = 0.07617615\n",
      "Iteration 101, loss = 0.18212676\n",
      "Iteration 110, loss = 0.19604264\n",
      "Iteration 224, loss = 0.12691630\n",
      "Iteration 331, loss = 0.07610555\n",
      "Iteration 244, loss = 0.11417147\n",
      "Iteration 102, loss = 0.18122162\n",
      "Iteration 332, loss = 0.07603052\n",
      "Iteration 37, loss = 0.39185202\n",
      "Iteration 38, loss = 0.39047083\n",
      "Iteration 218, loss = 0.22322241\n",
      "Iteration 245, loss = 0.11405085\n",
      "Iteration 48, loss = 0.30894714\n",
      "Iteration 39, loss = 0.38908870\n",
      "Iteration 40, loss = 0.38780532\n",
      "Iteration 111, loss = 0.19533380\n",
      "Iteration 225, loss = 0.12673989\n",
      "Iteration 333, loss = 0.07595556\n",
      "Iteration 334, loss = 0.07585751\n",
      "Iteration 103, loss = 0.18040058\n",
      "Iteration 41, loss = 0.38647272\n",
      "Iteration 246, loss = 0.11389009\n",
      "Iteration 226, loss = 0.12656622\n",
      "Iteration 112, loss = 0.19463363\n",
      "Iteration 335, loss = 0.07581163\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 104, loss = 0.17951995\n",
      "Iteration 42, loss = 0.38522731\n",
      "Iteration 247, loss = 0.11374451\n",
      "Iteration 49, loss = 0.30530420\n",
      "Iteration 219, loss = 0.22307883\n",
      "Iteration 113, loss = 0.19396520\n",
      "Iteration 43, loss = 0.38397096\n",
      "Iteration 248, loss = 0.11362550\n",
      "Iteration 105, loss = 0.17874727\n",
      "Iteration 44, loss = 0.38275736\n",
      "Iteration 227, loss = 0.12639053\n",
      "Iteration 45, loss = 0.38152863\n",
      "Iteration 1, loss = 0.79618707\n",
      "Iteration 249, loss = 0.11348074\n",
      "Iteration 250, loss = 0.11334066\n",
      "Iteration 46, loss = 0.38035819\n",
      "Iteration 228, loss = 0.12623199\n",
      "Iteration 106, loss = 0.17788542\n",
      "Iteration 50, loss = 0.30186544\n",
      "Iteration 107, loss = 0.17708480\n",
      "Iteration 2, loss = 0.77240052\n",
      "Iteration 229, loss = 0.12606160\n",
      "Iteration 47, loss = 0.37908878\n",
      "Iteration 48, loss = 0.37787362\n",
      "Iteration 49, loss = 0.37667020\n",
      "Iteration 251, loss = 0.11320583\n",
      "Iteration 50, loss = 0.37542228\n",
      "Iteration 51, loss = 0.37420316\n",
      "Iteration 220, loss = 0.22289431\n",
      "Iteration 52, loss = 0.37291491\n",
      "Iteration 108, loss = 0.17631477\n",
      "Iteration 53, loss = 0.37167527\n",
      "Iteration 3, loss = 0.74050885\n",
      "Iteration 114, loss = 0.19335708\n",
      "Iteration 4, loss = 0.70706275\n",
      "Iteration 252, loss = 0.11307429\n",
      "Iteration 230, loss = 0.12589626\n",
      "Iteration 253, loss = 0.11294087\n",
      "Iteration 51, loss = 0.29863939\n",
      "Iteration 254, loss = 0.11281326\n",
      "Iteration 115, loss = 0.19267273\n",
      "Iteration 54, loss = 0.37034726\n",
      "Iteration 231, loss = 0.12573651\n",
      "Iteration 109, loss = 0.17556694\n",
      "Iteration 55, loss = 0.36905462\n",
      "Iteration 110, loss = 0.17481516Iteration 255, loss = 0.11268481\n",
      "Iteration 221, loss = 0.22276857\n",
      "Iteration 5, loss = 0.67545383\n",
      "\n",
      "Iteration 56, loss = 0.36776798\n",
      "Iteration 52, loss = 0.29541275\n",
      "Iteration 116, loss = 0.19206535\n",
      "Iteration 57, loss = 0.36634827\n",
      "Iteration 256, loss = 0.11255430\n",
      "Iteration 232, loss = 0.12557626\n",
      "Iteration 58, loss = 0.36499964\n",
      "Iteration 6, loss = 0.64651497\n",
      "Iteration 59, loss = 0.36360108\n",
      "Iteration 111, loss = 0.17404291\n",
      "Iteration 233, loss = 0.12541965\n",
      "Iteration 257, loss = 0.11242768\n",
      "Iteration 60, loss = 0.36208384\n",
      "Iteration 258, loss = 0.11229357\n",
      "Iteration 61, loss = 0.36062994\n",
      "Iteration 53, loss = 0.29245020\n",
      "Iteration 112, loss = 0.17333309\n",
      "Iteration 117, loss = 0.19144003\n",
      "Iteration 113, loss = 0.17268153\n",
      "Iteration 7, loss = 0.62067484\n",
      "Iteration 234, loss = 0.12525499\n",
      "Iteration 259, loss = 0.11217575\n",
      "Iteration 62, loss = 0.35909268\n",
      "Iteration 222, loss = 0.22260015\n",
      "Iteration 8, loss = 0.59802027\n",
      "Iteration 260, loss = 0.11206669\n",
      "Iteration 63, loss = 0.35747635\n",
      "Iteration 64, loss = 0.35591081\n",
      "Iteration 118, loss = 0.19086697\n",
      "Iteration 65, loss = 0.35432527\n",
      "Iteration 114, loss = 0.17199498\n",
      "Iteration 235, loss = 0.12507535\n",
      "Iteration 261, loss = 0.11192197\n",
      "Iteration 115, loss = 0.17123367\n",
      "Iteration 54, loss = 0.28947142\n",
      "Iteration 66, loss = 0.35269477\n",
      "Iteration 9, loss = 0.57821533\n",
      "Iteration 262, loss = 0.11180226\n",
      "Iteration 236, loss = 0.12492015\n",
      "Iteration 119, loss = 0.19026603\n",
      "Iteration 223, loss = 0.22246405\n",
      "Iteration 263, loss = 0.11168900\n",
      "Iteration 67, loss = 0.35097754\n",
      "Iteration 10, loss = 0.56005303\n",
      "Iteration 68, loss = 0.34931053\n",
      "Iteration 55, loss = 0.28669764\n",
      "Iteration 69, loss = 0.34760185\n",
      "Iteration 116, loss = 0.17057360\n",
      "Iteration 264, loss = 0.11156956\n",
      "Iteration 11, loss = 0.54466965\n",
      "Iteration 120, loss = 0.18967592\n",
      "Iteration 70, loss = 0.34579614\n",
      "Iteration 237, loss = 0.12476591\n",
      "Iteration 71, loss = 0.34406963\n",
      "Iteration 56, loss = 0.28413206\n",
      "Iteration 12, loss = 0.53080393\n",
      "Iteration 117, loss = 0.16993582\n",
      "Iteration 72, loss = 0.34233099\n",
      "Iteration 224, loss = 0.22237300\n",
      "Iteration 265, loss = 0.11143029\n",
      "Iteration 121, loss = 0.18914107\n",
      "Iteration 118, loss = 0.16930343\n",
      "Iteration 238, loss = 0.12461803\n",
      "Iteration 73, loss = 0.34053285\n",
      "Iteration 266, loss = 0.11133513\n",
      "Iteration 239, loss = 0.12445616\n",
      "Iteration 119, loss = 0.16866093\n",
      "Iteration 74, loss = 0.33872561\n",
      "Iteration 267, loss = 0.11120110\n",
      "Iteration 75, loss = 0.33682921\n",
      "Iteration 268, loss = 0.11108685\n",
      "Iteration 120, loss = 0.16801180\n",
      "Iteration 13, loss = 0.51827440\n",
      "Iteration 57, loss = 0.28147228\n",
      "Iteration 240, loss = 0.12431481\n",
      "Iteration 122, loss = 0.18858550\n",
      "Iteration 225, loss = 0.22217947\n",
      "Iteration 76, loss = 0.33496812\n",
      "Iteration 77, loss = 0.33312392\n",
      "Iteration 269, loss = 0.11099157\n",
      "Iteration 241, loss = 0.12415728\n",
      "Iteration 14, loss = 0.50759901\n",
      "Iteration 121, loss = 0.16743587\n",
      "Iteration 78, loss = 0.33115846\n",
      "Iteration 123, loss = 0.18799663\n",
      "Iteration 58, loss = 0.27907436\n",
      "Iteration 79, loss = 0.32927588\n",
      "Iteration 270, loss = 0.11086467\n",
      "Iteration 15, loss = 0.49800560\n",
      "Iteration 80, loss = 0.32734528\n",
      "Iteration 122, loss = 0.16682311\n",
      "Iteration 226, loss = 0.22206232\n",
      "Iteration 123, loss = 0.16628714\n",
      "Iteration 81, loss = 0.32538221\n",
      "Iteration 82, loss = 0.32344023\n",
      "Iteration 242, loss = 0.12401297\n",
      "Iteration 16, loss = 0.48939385\n",
      "Iteration 243, loss = 0.12385868\n",
      "Iteration 271, loss = 0.11073755\n",
      "Iteration 83, loss = 0.32150832\n",
      "Iteration 84, loss = 0.31950555\n",
      "Iteration 85, loss = 0.31743746\n",
      "Iteration 272, loss = 0.11062986\n",
      "Iteration 124, loss = 0.18745170\n",
      "Iteration 86, loss = 0.31543028\n",
      "Iteration 244, loss = 0.12372230\n",
      "Iteration 59, loss = 0.27671747\n",
      "Iteration 124, loss = 0.16568555\n",
      "Iteration 17, loss = 0.48159377\n",
      "Iteration 87, loss = 0.31329907Iteration 273, loss = 0.11050655\n",
      "Iteration 60, loss = 0.27446298\n",
      "Iteration 18, loss = 0.47507572\n",
      "Iteration 125, loss = 0.18694205\n",
      "\n",
      "Iteration 125, loss = 0.16512783\n",
      "Iteration 245, loss = 0.12358314\n",
      "Iteration 274, loss = 0.11040888\n",
      "Iteration 88, loss = 0.31119955\n",
      "Iteration 227, loss = 0.22186301\n",
      "Iteration 89, loss = 0.30901435\n",
      "Iteration 90, loss = 0.30680967\n",
      "Iteration 275, loss = 0.11030222\n",
      "Iteration 91, loss = 0.30454823\n",
      "Iteration 246, loss = 0.12341790\n",
      "Iteration 126, loss = 0.16455144\n",
      "Iteration 19, loss = 0.46869339\n",
      "Iteration 126, loss = 0.18641776\n",
      "Iteration 61, loss = 0.27233179\n",
      "Iteration 276, loss = 0.11017805\n",
      "Iteration 92, loss = 0.30230234\n",
      "Iteration 247, loss = 0.12327365\n",
      "Iteration 277, loss = 0.11009061\n",
      "Iteration 93, loss = 0.30007260\n",
      "Iteration 127, loss = 0.16404449\n",
      "Iteration 94, loss = 0.29766157\n",
      "Iteration 127, loss = 0.18591904\n",
      "Iteration 95, loss = 0.29536875\n",
      "Iteration 20, loss = 0.46315199\n",
      "Iteration 248, loss = 0.12314430\n",
      "Iteration 96, loss = 0.29297107\n",
      "Iteration 128, loss = 0.16352000\n",
      "Iteration 97, loss = 0.29067959\n",
      "Iteration 228, loss = 0.22174836\n",
      "Iteration 278, loss = 0.10998481\n",
      "Iteration 21, loss = 0.45804171\n",
      "Iteration 128, loss = 0.18539031\n",
      "Iteration 249, loss = 0.12299347\n",
      "Iteration 129, loss = 0.16298722\n",
      "Iteration 98, loss = 0.28822489\n",
      "Iteration 279, loss = 0.10986594\n",
      "Iteration 99, loss = 0.28576245\n",
      "Iteration 62, loss = 0.27022218\n",
      "Iteration 229, loss = 0.22161901\n",
      "Iteration 280, loss = 0.10974937\n",
      "Iteration 250, loss = 0.12285437\n",
      "Iteration 100, loss = 0.28331850\n",
      "Iteration 129, loss = 0.18490254\n",
      "Iteration 22, loss = 0.45330724\n",
      "Iteration 130, loss = 0.16246599\n",
      "Iteration 101, loss = 0.28075343\n",
      "Iteration 131, loss = 0.16196896\n",
      "Iteration 63, loss = 0.26824048\n",
      "Iteration 102, loss = 0.27814416\n",
      "Iteration 281, loss = 0.10967331\n",
      "Iteration 103, loss = 0.27559758\n",
      "Iteration 132, loss = 0.16148002\n",
      "Iteration 104, loss = 0.27304375\n",
      "Iteration 23, loss = 0.44878899\n",
      "Iteration 251, loss = 0.12271044\n",
      "Iteration 282, loss = 0.10954504\n",
      "Iteration 130, loss = 0.18439853\n",
      "Iteration 24, loss = 0.44480883\n",
      "Iteration 283, loss = 0.10944049\n",
      "Iteration 230, loss = 0.22150491\n",
      "Iteration 252, loss = 0.12257089\n",
      "Iteration 133, loss = 0.16098725\n",
      "Iteration 105, loss = 0.27035794\n",
      "Iteration 284, loss = 0.10932809\n",
      "Iteration 25, loss = 0.44078826\n",
      "Iteration 106, loss = 0.26781452\n",
      "Iteration 285, loss = 0.10925141\n",
      "Iteration 64, loss = 0.26628906\n",
      "Iteration 131, loss = 0.18395487\n",
      "Iteration 26, loss = 0.43706815\n",
      "Iteration 107, loss = 0.26517188\n",
      "Iteration 134, loss = 0.16051479\n",
      "Iteration 108, loss = 0.26252831\n",
      "Iteration 253, loss = 0.12244406\n",
      "Iteration 286, loss = 0.10913406\n",
      "Iteration 109, loss = 0.25983798\n",
      "Iteration 135, loss = 0.16004249\n",
      "Iteration 231, loss = 0.22133663\n",
      "Iteration 110, loss = 0.25726199\n",
      "Iteration 65, loss = 0.26442531\n",
      "Iteration 136, loss = 0.15959749\n",
      "Iteration 111, loss = 0.25450102\n",
      "Iteration 132, loss = 0.18345022\n",
      "Iteration 287, loss = 0.10902927\n",
      "Iteration 27, loss = 0.43330218\n",
      "Iteration 254, loss = 0.12230032\n",
      "Iteration 112, loss = 0.25176768\n",
      "Iteration 113, loss = 0.24903593\n",
      "Iteration 137, loss = 0.15914538\n",
      "Iteration 288, loss = 0.10896305\n",
      "Iteration 28, loss = 0.42979888\n",
      "Iteration 114, loss = 0.24648054\n",
      "Iteration 133, loss = 0.18297153\n",
      "Iteration 255, loss = 0.12216611\n",
      "Iteration 138, loss = 0.15867942\n",
      "Iteration 289, loss = 0.10885926\n",
      "Iteration 290, loss = 0.10874740\n",
      "Iteration 66, loss = 0.26267801\n",
      "Iteration 115, loss = 0.24370187\n",
      "Iteration 291, loss = 0.10865373\n",
      "Iteration 116, loss = 0.24103404\n",
      "Iteration 29, loss = 0.42624199\n",
      "Iteration 232, loss = 0.22114120\n",
      "Iteration 134, loss = 0.18250774\n",
      "Iteration 292, loss = 0.10855888\n",
      "Iteration 256, loss = 0.12203688\n",
      "Iteration 117, loss = 0.23838612\n",
      "Iteration 118, loss = 0.23568169\n",
      "Iteration 119, loss = 0.23307184\n",
      "Iteration 139, loss = 0.15823125\n",
      "Iteration 257, loss = 0.12192344\n",
      "Iteration 30, loss = 0.42283055\n",
      "Iteration 67, loss = 0.26093391\n",
      "Iteration 120, loss = 0.23040169\n",
      "Iteration 135, loss = 0.18205017\n",
      "Iteration 140, loss = 0.15782820\n",
      "Iteration 258, loss = 0.12177488\n",
      "Iteration 121, loss = 0.22786862\n",
      "Iteration 233, loss = 0.22100984\n",
      "Iteration 293, loss = 0.10846657\n",
      "Iteration 122, loss = 0.22524721\n",
      "Iteration 31, loss = 0.41947721\n",
      "Iteration 294, loss = 0.10836957\n",
      "Iteration 136, loss = 0.18160025\n",
      "Iteration 295, loss = 0.10828066\n",
      "Iteration 259, loss = 0.12163690\n",
      "Iteration 141, loss = 0.15740355\n",
      "Iteration 296, loss = 0.10819626\n",
      "Iteration 123, loss = 0.22264720\n",
      "Iteration 124, loss = 0.22012086\n",
      "Iteration 297, loss = 0.10814761\n",
      "Iteration 68, loss = 0.25926915\n",
      "Iteration 142, loss = 0.15697894\n",
      "Iteration 32, loss = 0.41609449\n",
      "Iteration 298, loss = 0.10800920\n",
      "Iteration 125, loss = 0.21767357\n",
      "Iteration 299, loss = 0.10791365\n",
      "Iteration 126, loss = 0.21514173\n",
      "Iteration 300, loss = 0.10783484\n",
      "Iteration 260, loss = 0.12152139\n",
      "Iteration 301, loss = 0.10776628\n",
      "Iteration 234, loss = 0.22088724\n",
      "Iteration 127, loss = 0.21266327\n",
      "Iteration 137, loss = 0.18115826\n",
      "Iteration 128, loss = 0.21032270\n",
      "Iteration 302, loss = 0.10768695\n",
      "Iteration 129, loss = 0.20793774\n",
      "Iteration 303, loss = 0.10756639\n",
      "Iteration 69, loss = 0.25763465\n",
      "Iteration 304, loss = 0.10749018\n",
      "Iteration 143, loss = 0.15655758\n",
      "Iteration 33, loss = 0.41280404\n",
      "Iteration 305, loss = 0.10740957\n",
      "Iteration 138, loss = 0.18071834\n",
      "Iteration 261, loss = 0.12139169\n",
      "Iteration 130, loss = 0.20567862\n",
      "Iteration 131, loss = 0.20335957\n",
      "Iteration 262, loss = 0.12125110\n",
      "Iteration 144, loss = 0.15614446\n",
      "Iteration 34, loss = 0.40952297\n",
      "Iteration 132, loss = 0.20113878\n",
      "Iteration 145, loss = 0.15575012\n",
      "Iteration 70, loss = 0.25605381\n",
      "Iteration 235, loss = 0.22073896\n",
      "Iteration 146, loss = 0.15535531\n",
      "Iteration 133, loss = 0.19891385\n",
      "Iteration 263, loss = 0.12112089\n",
      "Iteration 134, loss = 0.19680435\n",
      "Iteration 35, loss = 0.40628815\n",
      "Iteration 147, loss = 0.15500017\n",
      "Iteration 135, loss = 0.19462110\n",
      "Iteration 71, loss = 0.25454431\n",
      "Iteration 139, loss = 0.18031696\n",
      "Iteration 306, loss = 0.10733736\n",
      "Iteration 136, loss = 0.19268159\n",
      "Iteration 307, loss = 0.10723036\n",
      "Iteration 137, loss = 0.19054089\n",
      "Iteration 236, loss = 0.22057507\n",
      "Iteration 264, loss = 0.12099348\n",
      "Iteration 36, loss = 0.40301933\n",
      "Iteration 138, loss = 0.18850471\n",
      "Iteration 140, loss = 0.17988500\n",
      "Iteration 265, loss = 0.12088509\n",
      "Iteration 148, loss = 0.15461173\n",
      "Iteration 308, loss = 0.10714036\n",
      "Iteration 139, loss = 0.18664824\n",
      "Iteration 140, loss = 0.18481747\n",
      "Iteration 266, loss = 0.12075284\n",
      "Iteration 72, loss = 0.25311440\n",
      "Iteration 141, loss = 0.18295843\n",
      "Iteration 309, loss = 0.10706899\n",
      "Iteration 37, loss = 0.39981751\n",
      "Iteration 149, loss = 0.15420425\n",
      "Iteration 142, loss = 0.18126289\n",
      "Iteration 310, loss = 0.10699528\n",
      "Iteration 141, loss = 0.17945441\n",
      "Iteration 38, loss = 0.39666714\n",
      "Iteration 267, loss = 0.12062688\n",
      "Iteration 311, loss = 0.10690934\n",
      "Iteration 143, loss = 0.17948413\n",
      "Iteration 144, loss = 0.17782863\n",
      "Iteration 145, loss = 0.17619515\n",
      "Iteration 150, loss = 0.15385155\n",
      "Iteration 237, loss = 0.22045765\n",
      "Iteration 142, loss = 0.17904284\n",
      "Iteration 312, loss = 0.10685350\n",
      "Iteration 39, loss = 0.39358673\n",
      "Iteration 146, loss = 0.17466121\n",
      "Iteration 268, loss = 0.12049247\n",
      "Iteration 73, loss = 0.25167677\n",
      "Iteration 143, loss = 0.17865475\n",
      "Iteration 313, loss = 0.10675792\n",
      "Iteration 147, loss = 0.17311070\n",
      "Iteration 151, loss = 0.15348750\n",
      "Iteration 148, loss = 0.17162924\n",
      "Iteration 149, loss = 0.17014096Iteration 238, loss = 0.22032129\n",
      "\n",
      "Iteration 314, loss = 0.10668305\n",
      "Iteration 152, loss = 0.15311725\n",
      "Iteration 269, loss = 0.12037915\n",
      "Iteration 150, loss = 0.16871961\n",
      "Iteration 144, loss = 0.17824011Iteration 74, loss = 0.25033915\n",
      "\n",
      "Iteration 40, loss = 0.39041979\n",
      "Iteration 151, loss = 0.16733776\n",
      "Iteration 270, loss = 0.12025686\n",
      "Iteration 152, loss = 0.16600648\n",
      "Iteration 315, loss = 0.10658584\n",
      "Iteration 153, loss = 0.15277541\n",
      "Iteration 153, loss = 0.16462635\n",
      "Iteration 316, loss = 0.10650883\n",
      "Iteration 271, loss = 0.12013334\n",
      "Iteration 41, loss = 0.38735375\n",
      "Iteration 317, loss = 0.10642906\n",
      "Iteration 239, loss = 0.22018102\n",
      "Iteration 154, loss = 0.16335787\n",
      "Iteration 154, loss = 0.15243202\n",
      "Iteration 272, loss = 0.12001819\n",
      "Iteration 42, loss = 0.38428795\n",
      "Iteration 155, loss = 0.15206999\n",
      "Iteration 155, loss = 0.16203734\n",
      "Iteration 156, loss = 0.15176824\n",
      "Iteration 156, loss = 0.16084391\n",
      "Iteration 240, loss = 0.22010718\n",
      "Iteration 157, loss = 0.15969442\n",
      "Iteration 157, loss = 0.15141655\n",
      "Iteration 158, loss = 0.15846349\n",
      "Iteration 159, loss = 0.15729038\n",
      "Iteration 158, loss = 0.15103876\n",
      "Iteration 160, loss = 0.15614598\n",
      "Iteration 75, loss = 0.24898006\n",
      "Iteration 145, loss = 0.17786556\n",
      "Iteration 318, loss = 0.10634050\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 161, loss = 0.15506100\n",
      "Iteration 146, loss = 0.17747198\n",
      "Iteration 273, loss = 0.11990464\n",
      "Iteration 76, loss = 0.24768743\n",
      "Iteration 43, loss = 0.38113951\n",
      "Iteration 274, loss = 0.11978095\n",
      "Iteration 275, loss = 0.11968827\n",
      "Iteration 241, loss = 0.21995670\n",
      "Iteration 159, loss = 0.15070936\n",
      "Iteration 44, loss = 0.37807891\n",
      "Iteration 162, loss = 0.15396336\n",
      "Iteration 163, loss = 0.15291237\n",
      "Iteration 160, loss = 0.15041068\n",
      "Iteration 147, loss = 0.17707248\n",
      "Iteration 45, loss = 0.37499338\n",
      "Iteration 1, loss = 0.97455606\n",
      "Iteration 242, loss = 0.21981625\n",
      "Iteration 164, loss = 0.15190041\n",
      "Iteration 276, loss = 0.11955792\n",
      "Iteration 161, loss = 0.15010855\n",
      "Iteration 77, loss = 0.24641873\n",
      "Iteration 165, loss = 0.15089059\n",
      "Iteration 46, loss = 0.37203184\n",
      "Iteration 166, loss = 0.14985052\n",
      "Iteration 167, loss = 0.14887650\n",
      "Iteration 277, loss = 0.11943766\n",
      "Iteration 162, loss = 0.14975194\n",
      "Iteration 148, loss = 0.17672654\n",
      "Iteration 168, loss = 0.14794531\n",
      "Iteration 47, loss = 0.36891624\n",
      "Iteration 278, loss = 0.11933421\n",
      "Iteration 2, loss = 0.93443671\n",
      "Iteration 78, loss = 0.24524150\n",
      "Iteration 243, loss = 0.21975141\n",
      "Iteration 149, loss = 0.17633807\n",
      "Iteration 169, loss = 0.14693993\n",
      "Iteration 163, loss = 0.14943350\n",
      "Iteration 170, loss = 0.14606547\n",
      "Iteration 48, loss = 0.36600993\n",
      "Iteration 164, loss = 0.14915104\n",
      "Iteration 279, loss = 0.11921703\n",
      "Iteration 165, loss = 0.14880930\n",
      "Iteration 171, loss = 0.14512468\n",
      "Iteration 172, loss = 0.14424063\n",
      "Iteration 173, loss = 0.14337939\n",
      "Iteration 150, loss = 0.17598964\n",
      "Iteration 79, loss = 0.24413232\n",
      "Iteration 49, loss = 0.36294546\n",
      "Iteration 280, loss = 0.11908915\n",
      "Iteration 3, loss = 0.88360702\n",
      "Iteration 244, loss = 0.21963001\n",
      "Iteration 166, loss = 0.14851942\n",
      "Iteration 174, loss = 0.14254387\n",
      "Iteration 151, loss = 0.17563711\n",
      "Iteration 167, loss = 0.14822925\n",
      "Iteration 281, loss = 0.11899769\n",
      "Iteration 175, loss = 0.14167741\n",
      "Iteration 176, loss = 0.14089546\n",
      "Iteration 50, loss = 0.36001106\n",
      "Iteration 80, loss = 0.24290160\n",
      "Iteration 152, loss = 0.17525777\n",
      "Iteration 177, loss = 0.14003550\n",
      "Iteration 178, loss = 0.13926146\n",
      "Iteration 282, loss = 0.11886974\n",
      "Iteration 51, loss = 0.35701489\n",
      "Iteration 179, loss = 0.13849024\n",
      "Iteration 245, loss = 0.21947592\n",
      "Iteration 168, loss = 0.14793391\n",
      "Iteration 81, loss = 0.24181939\n",
      "Iteration 180, loss = 0.13772670\n",
      "Iteration 181, loss = 0.13698598\n",
      "Iteration 4, loss = 0.83478477\n",
      "Iteration 169, loss = 0.14763201\n",
      "Iteration 182, loss = 0.13621696\n",
      "Iteration 52, loss = 0.35407918\n",
      "Iteration 153, loss = 0.17490755\n",
      "Iteration 170, loss = 0.14733942\n",
      "Iteration 283, loss = 0.11875765\n",
      "Iteration 183, loss = 0.13551729\n",
      "Iteration 184, loss = 0.13477340\n",
      "Iteration 185, loss = 0.13409879\n",
      "Iteration 53, loss = 0.35118016\n",
      "Iteration 186, loss = 0.13346817\n",
      "Iteration 154, loss = 0.17456980\n",
      "Iteration 187, loss = 0.13274317\n",
      "Iteration 188, loss = 0.13208529\n",
      "Iteration 284, loss = 0.11864902\n",
      "Iteration 82, loss = 0.24073735\n",
      "Iteration 171, loss = 0.14705669\n",
      "Iteration 246, loss = 0.21935249\n",
      "Iteration 54, loss = 0.34827972\n",
      "Iteration 189, loss = 0.13150309\n",
      "Iteration 5, loss = 0.79032924\n",
      "Iteration 172, loss = 0.14679588\n",
      "Iteration 55, loss = 0.34531869\n",
      "Iteration 285, loss = 0.11853176\n",
      "Iteration 155, loss = 0.17420988\n",
      "Iteration 190, loss = 0.13085799\n",
      "Iteration 286, loss = 0.11843022\n",
      "Iteration 191, loss = 0.13018258\n",
      "Iteration 247, loss = 0.21926773\n",
      "Iteration 192, loss = 0.12966649\n",
      "Iteration 56, loss = 0.34241757\n",
      "Iteration 173, loss = 0.14646696\n",
      "Iteration 83, loss = 0.23970014\n",
      "Iteration 6, loss = 0.75257698\n",
      "Iteration 193, loss = 0.12898660\n",
      "Iteration 194, loss = 0.12841198\n",
      "Iteration 287, loss = 0.11833660\n",
      "Iteration 174, loss = 0.14621384\n",
      "Iteration 156, loss = 0.17387102\n",
      "Iteration 195, loss = 0.12789773\n",
      "Iteration 288, loss = 0.11821148\n",
      "Iteration 84, loss = 0.23869570\n",
      "Iteration 196, loss = 0.12736639\n",
      "Iteration 57, loss = 0.33963359\n",
      "Iteration 197, loss = 0.12681954\n",
      "Iteration 58, loss = 0.33679441\n",
      "Iteration 248, loss = 0.21909397\n",
      "Iteration 198, loss = 0.12627025\n",
      "Iteration 175, loss = 0.14592082\n",
      "Iteration 7, loss = 0.71897364\n",
      "Iteration 199, loss = 0.12576076\n",
      "Iteration 157, loss = 0.17355114\n",
      "Iteration 289, loss = 0.11814396\n",
      "Iteration 176, loss = 0.14565647\n",
      "Iteration 85, loss = 0.23770853\n",
      "Iteration 290, loss = 0.11799264\n",
      "Iteration 177, loss = 0.14540515\n",
      "Iteration 59, loss = 0.33392242\n",
      "Iteration 158, loss = 0.17320203\n",
      "Iteration 200, loss = 0.12524387\n",
      "Iteration 201, loss = 0.12481006\n",
      "Iteration 291, loss = 0.11790561\n",
      "Iteration 202, loss = 0.12429711\n",
      "Iteration 159, loss = 0.17288048\n",
      "Iteration 203, loss = 0.12384473\n",
      "Iteration 204, loss = 0.12334553\n",
      "Iteration 178, loss = 0.14512494\n",
      "Iteration 249, loss = 0.21899599Iteration 60, loss = 0.33120407\n",
      "\n",
      "Iteration 205, loss = 0.12288180Iteration 86, loss = 0.23674476\n",
      "\n",
      "Iteration 8, loss = 0.69029921\n",
      "Iteration 179, loss = 0.14488977\n",
      "Iteration 292, loss = 0.11779831\n",
      "Iteration 180, loss = 0.14464279\n",
      "Iteration 160, loss = 0.17255207\n",
      "Iteration 61, loss = 0.32851010\n",
      "Iteration 206, loss = 0.12247361\n",
      "Iteration 9, loss = 0.66659835\n",
      "Iteration 181, loss = 0.14436438\n",
      "Iteration 293, loss = 0.11768535\n",
      "Iteration 207, loss = 0.12204220\n",
      "Iteration 87, loss = 0.23581157\n",
      "Iteration 250, loss = 0.21886020\n",
      "Iteration 62, loss = 0.32590313Iteration 208, loss = 0.12157185\n",
      "\n",
      "Iteration 209, loss = 0.12113957\n",
      "Iteration 294, loss = 0.11758165\n",
      "Iteration 88, loss = 0.23486354\n",
      "Iteration 161, loss = 0.17224286\n",
      "Iteration 182, loss = 0.14413107\n",
      "Iteration 210, loss = 0.12072857\n",
      "Iteration 211, loss = 0.12036089\n",
      "Iteration 63, loss = 0.32317701\n",
      "Iteration 10, loss = 0.64509950Iteration 64, loss = 0.32059703Iteration 251, loss = 0.21877808\n",
      "\n",
      "\n",
      "Iteration 212, loss = 0.11989748\n",
      "Iteration 183, loss = 0.14389479\n",
      "Iteration 65, loss = 0.31804821\n",
      "Iteration 213, loss = 0.11948867\n",
      "Iteration 162, loss = 0.17193371\n",
      "Iteration 214, loss = 0.11911770\n",
      "Iteration 295, loss = 0.11747685\n",
      "Iteration 89, loss = 0.23399470\n",
      "Iteration 184, loss = 0.14363203\n",
      "Iteration 296, loss = 0.11738358\n",
      "Iteration 215, loss = 0.11875287\n",
      "Iteration 163, loss = 0.17161942\n",
      "Iteration 185, loss = 0.14335506\n",
      "Iteration 216, loss = 0.11837523\n",
      "Iteration 252, loss = 0.21864616\n",
      "Iteration 297, loss = 0.11727434\n",
      "Iteration 217, loss = 0.11800774\n",
      "Iteration 66, loss = 0.31548438Iteration 90, loss = 0.23310562\n",
      "\n",
      "Iteration 11, loss = 0.62662643\n",
      "Iteration 164, loss = 0.17131314\n",
      "Iteration 218, loss = 0.11765678\n",
      "Iteration 298, loss = 0.11717821\n",
      "Iteration 219, loss = 0.11727194\n",
      "Iteration 186, loss = 0.14314665\n",
      "Iteration 67, loss = 0.31303140\n",
      "Iteration 220, loss = 0.11693755\n",
      "Iteration 187, loss = 0.14289993\n",
      "Iteration 91, loss = 0.23223963\n",
      "Iteration 165, loss = 0.17097487\n",
      "Iteration 221, loss = 0.11658250\n",
      "Iteration 68, loss = 0.31059945\n",
      "Iteration 253, loss = 0.21856223\n",
      "Iteration 222, loss = 0.11624248\n",
      "Iteration 299, loss = 0.11707805\n",
      "Iteration 188, loss = 0.14267108\n",
      "Iteration 300, loss = 0.11698344\n",
      "Iteration 12, loss = 0.61113207\n",
      "Iteration 166, loss = 0.17068996\n",
      "Iteration 223, loss = 0.11589323\n",
      "Iteration 92, loss = 0.23144838\n",
      "Iteration 189, loss = 0.14242109\n",
      "Iteration 224, loss = 0.11556011\n",
      "Iteration 225, loss = 0.11525894\n",
      "Iteration 69, loss = 0.30815933\n",
      "Iteration 190, loss = 0.14220641\n",
      "Iteration 301, loss = 0.11688146\n",
      "Iteration 191, loss = 0.14199741\n",
      "Iteration 226, loss = 0.11490880\n",
      "Iteration 13, loss = 0.59756382\n",
      "Iteration 70, loss = 0.30569922\n",
      "Iteration 167, loss = 0.17037526\n",
      "Iteration 302, loss = 0.11677830\n",
      "Iteration 227, loss = 0.11461495\n",
      "Iteration 254, loss = 0.21845575\n",
      "Iteration 93, loss = 0.23056127\n",
      "Iteration 303, loss = 0.11670070\n",
      "Iteration 192, loss = 0.14175210\n",
      "Iteration 228, loss = 0.11428761\n",
      "Iteration 71, loss = 0.30330206\n",
      "Iteration 168, loss = 0.17006206\n",
      "Iteration 229, loss = 0.11398262\n",
      "Iteration 193, loss = 0.14154631\n",
      "Iteration 230, loss = 0.11364490\n",
      "Iteration 14, loss = 0.58509209\n",
      "Iteration 72, loss = 0.30099327\n",
      "Iteration 255, loss = 0.21833334\n",
      "Iteration 231, loss = 0.11337363\n",
      "Iteration 304, loss = 0.11661370Iteration 94, loss = 0.22983466\n",
      "\n",
      "Iteration 169, loss = 0.16977643\n",
      "Iteration 232, loss = 0.11308687\n",
      "Iteration 170, loss = 0.16947843\n",
      "Iteration 305, loss = 0.11649293\n",
      "Iteration 194, loss = 0.14129954\n",
      "Iteration 233, loss = 0.11275169\n",
      "Iteration 73, loss = 0.29870554\n",
      "Iteration 95, loss = 0.22899724\n",
      "Iteration 306, loss = 0.11639938\n",
      "Iteration 195, loss = 0.14112173\n",
      "Iteration 234, loss = 0.11243184\n",
      "Iteration 235, loss = 0.11218554\n",
      "Iteration 15, loss = 0.57501744\n",
      "Iteration 236, loss = 0.11189937\n",
      "Iteration 74, loss = 0.29627489\n",
      "Iteration 171, loss = 0.16919310\n",
      "Iteration 237, loss = 0.11157975\n",
      "Iteration 307, loss = 0.11631607\n",
      "Iteration 256, loss = 0.21822191\n",
      "Iteration 238, loss = 0.11127809\n",
      "Iteration 196, loss = 0.14089867\n",
      "Iteration 239, loss = 0.11101488\n",
      "Iteration 240, loss = 0.11077777\n",
      "Iteration 241, loss = 0.11049205\n",
      "Iteration 96, loss = 0.22821778\n",
      "Iteration 75, loss = 0.29409624\n",
      "Iteration 16, loss = 0.56567635\n",
      "Iteration 197, loss = 0.14068623\n",
      "Iteration 308, loss = 0.11621371\n",
      "Iteration 172, loss = 0.16890819\n",
      "Iteration 242, loss = 0.11020004\n",
      "Iteration 309, loss = 0.11611818\n",
      "Iteration 243, loss = 0.10992196\n",
      "Iteration 257, loss = 0.21807558\n",
      "Iteration 76, loss = 0.29181519\n",
      "Iteration 198, loss = 0.14048899\n",
      "Iteration 244, loss = 0.10966628\n",
      "Iteration 97, loss = 0.22744649\n",
      "Iteration 173, loss = 0.16862682\n",
      "Iteration 245, loss = 0.10940087\n",
      "Iteration 310, loss = 0.11602473\n",
      "Iteration 77, loss = 0.28955631\n",
      "Iteration 246, loss = 0.10915106\n",
      "Iteration 199, loss = 0.14028217\n",
      "Iteration 247, loss = 0.10891923\n",
      "Iteration 200, loss = 0.14005245\n",
      "Iteration 17, loss = 0.55710330\n",
      "Iteration 98, loss = 0.22672590\n",
      "Iteration 248, loss = 0.10864865\n",
      "Iteration 311, loss = 0.11593677\n",
      "Iteration 78, loss = 0.28731477\n",
      "Iteration 174, loss = 0.16834942\n",
      "Iteration 258, loss = 0.21798034\n",
      "Iteration 249, loss = 0.10841561\n",
      "Iteration 250, loss = 0.10820673\n",
      "Iteration 312, loss = 0.11583553\n",
      "Iteration 79, loss = 0.28514861\n",
      "Iteration 175, loss = 0.16808765\n",
      "Iteration 251, loss = 0.10793426\n",
      "Iteration 201, loss = 0.13986041\n",
      "Iteration 18, loss = 0.54948664\n",
      "Iteration 252, loss = 0.10769141\n",
      "Iteration 253, loss = 0.10745796\n",
      "Iteration 313, loss = 0.11576282\n",
      "Iteration 99, loss = 0.22600819\n",
      "Iteration 202, loss = 0.13970843\n",
      "Iteration 254, loss = 0.10721753\n",
      "Iteration 255, loss = 0.10702699\n",
      "Iteration 176, loss = 0.16778823\n",
      "Iteration 203, loss = 0.13945670\n",
      "Iteration 314, loss = 0.11567028\n",
      "Iteration 80, loss = 0.28302113\n",
      "Iteration 256, loss = 0.10672094\n",
      "Iteration 100, loss = 0.22532530\n",
      "Iteration 257, loss = 0.10650026\n",
      "Iteration 315, loss = 0.11558138\n",
      "Iteration 258, loss = 0.10633827\n",
      "Iteration 204, loss = 0.13927597\n",
      "Iteration 19, loss = 0.54249850\n",
      "Iteration 81, loss = 0.28087347\n",
      "Iteration 259, loss = 0.21788835\n",
      "Iteration 177, loss = 0.16754169\n",
      "Iteration 205, loss = 0.13909107\n",
      "Iteration 259, loss = 0.10607354\n",
      "Iteration 82, loss = 0.27877864\n",
      "Iteration 260, loss = 0.10585203\n",
      "Iteration 83, loss = 0.27669997\n",
      "Iteration 261, loss = 0.10562978\n",
      "Iteration 178, loss = 0.16726178\n",
      "Iteration 316, loss = 0.11549985\n",
      "Iteration 101, loss = 0.22462787\n",
      "Iteration 179, loss = 0.16703406\n",
      "Iteration 20, loss = 0.53595606\n",
      "Iteration 206, loss = 0.13890063\n",
      "Iteration 262, loss = 0.10540679\n",
      "Iteration 260, loss = 0.21776621\n",
      "Iteration 263, loss = 0.10520703\n",
      "Iteration 317, loss = 0.11538254\n",
      "Iteration 264, loss = 0.10501144\n",
      "Iteration 84, loss = 0.27451781\n",
      "Iteration 265, loss = 0.10483559\n",
      "Iteration 207, loss = 0.13870936\n",
      "Iteration 102, loss = 0.22393197\n",
      "Iteration 180, loss = 0.16672293\n",
      "Iteration 85, loss = 0.27251683\n",
      "Iteration 318, loss = 0.11531217\n",
      "Iteration 208, loss = 0.13852144\n",
      "Iteration 261, loss = 0.21770759\n",
      "Iteration 266, loss = 0.10461351\n",
      "Iteration 209, loss = 0.13838679\n",
      "Iteration 267, loss = 0.10439195\n",
      "Iteration 103, loss = 0.22327191\n",
      "Iteration 181, loss = 0.16645570\n",
      "Iteration 268, loss = 0.10418096\n",
      "Iteration 21, loss = 0.52983169\n",
      "Iteration 319, loss = 0.11522065\n",
      "Iteration 269, loss = 0.10405727\n",
      "Iteration 210, loss = 0.13815161\n",
      "Iteration 270, loss = 0.10375948\n",
      "Iteration 86, loss = 0.27045110\n",
      "Iteration 211, loss = 0.13795613\n",
      "Iteration 104, loss = 0.22260722\n",
      "Iteration 182, loss = 0.16622089\n",
      "Iteration 320, loss = 0.11514609\n",
      "Iteration 262, loss = 0.21754462\n",
      "Iteration 271, loss = 0.10358739\n",
      "Iteration 22, loss = 0.52365939\n",
      "Iteration 321, loss = 0.11504323\n",
      "Iteration 87, loss = 0.26837906\n",
      "Iteration 212, loss = 0.13781723\n",
      "Iteration 272, loss = 0.10338686\n",
      "Iteration 273, loss = 0.10319960\n",
      "Iteration 274, loss = 0.10300672\n",
      "Iteration 183, loss = 0.16593322\n",
      "Iteration 322, loss = 0.11496072\n",
      "Iteration 213, loss = 0.13760678\n",
      "Iteration 275, loss = 0.10282812\n",
      "Iteration 276, loss = 0.10263727\n",
      "Iteration 277, loss = 0.10251194\n",
      "Iteration 323, loss = 0.11487709\n",
      "Iteration 105, loss = 0.22193966\n",
      "Iteration 214, loss = 0.13743138\n",
      "Iteration 88, loss = 0.26637990\n",
      "Iteration 184, loss = 0.16573184\n",
      "Iteration 215, loss = 0.13725752\n",
      "Iteration 263, loss = 0.21744007\n",
      "Iteration 89, loss = 0.26432317\n",
      "Iteration 278, loss = 0.10232193\n",
      "Iteration 23, loss = 0.51780852\n",
      "Iteration 279, loss = 0.10207126\n",
      "Iteration 216, loss = 0.13707736\n",
      "Iteration 324, loss = 0.11479655\n",
      "Iteration 280, loss = 0.10196452\n",
      "Iteration 106, loss = 0.22128182\n",
      "Iteration 90, loss = 0.26231512\n",
      "Iteration 185, loss = 0.16544080\n",
      "Iteration 281, loss = 0.10175154\n",
      "Iteration 91, loss = 0.26037167\n",
      "Iteration 282, loss = 0.10154251\n",
      "Iteration 186, loss = 0.16521000\n",
      "Iteration 217, loss = 0.13689012\n",
      "Iteration 325, loss = 0.11475086\n",
      "Iteration 283, loss = 0.10139988\n",
      "Iteration 284, loss = 0.10120642\n",
      "Iteration 285, loss = 0.10103568\n",
      "Iteration 24, loss = 0.51200787\n",
      "Iteration 264, loss = 0.21743833\n",
      "Iteration 326, loss = 0.11462066\n",
      "Iteration 286, loss = 0.10086264\n",
      "Iteration 287, loss = 0.10068330\n",
      "Iteration 107, loss = 0.22073924\n",
      "Iteration 218, loss = 0.13676968\n",
      "Iteration 288, loss = 0.10067679\n",
      "Iteration 219, loss = 0.13659619\n",
      "Iteration 92, loss = 0.25833132\n",
      "Iteration 327, loss = 0.11453959\n",
      "Iteration 289, loss = 0.10038051\n",
      "Iteration 220, loss = 0.13642027\n",
      "Iteration 93, loss = 0.25643881\n",
      "Iteration 25, loss = 0.50625045\n",
      "Iteration 328, loss = 0.11444398\n",
      "Iteration 265, loss = 0.21724417\n",
      "Iteration 290, loss = 0.10022191\n",
      "Iteration 187, loss = 0.16492364\n",
      "Iteration 108, loss = 0.22006165\n",
      "Iteration 291, loss = 0.10006053\n",
      "Iteration 292, loss = 0.09987834\n",
      "Iteration 293, loss = 0.09971704\n",
      "Iteration 329, loss = 0.11437235\n",
      "Iteration 221, loss = 0.13628822\n",
      "Iteration 294, loss = 0.09955768\n",
      "Iteration 109, loss = 0.21945297\n",
      "Iteration 94, loss = 0.25445740\n",
      "Iteration 295, loss = 0.09941652\n",
      "Iteration 330, loss = 0.11429190\n",
      "Iteration 296, loss = 0.09926734\n",
      "Iteration 188, loss = 0.16470304\n",
      "Iteration 266, loss = 0.21713702\n",
      "Iteration 222, loss = 0.13609169\n",
      "Iteration 223, loss = 0.13593123\n",
      "Iteration 26, loss = 0.50051970\n",
      "Iteration 224, loss = 0.13576855\n",
      "Iteration 297, loss = 0.09907468\n",
      "Iteration 298, loss = 0.09896404\n",
      "Iteration 189, loss = 0.16444247\n",
      "Iteration 267, loss = 0.21702883\n",
      "Iteration 299, loss = 0.09886905\n",
      "Iteration 95, loss = 0.25250953\n",
      "Iteration 331, loss = 0.11420646\n",
      "Iteration 190, loss = 0.16420436\n",
      "Iteration 300, loss = 0.09861234\n",
      "Iteration 110, loss = 0.21888143\n",
      "Iteration 301, loss = 0.09848781\n",
      "Iteration 302, loss = 0.09831753\n",
      "Iteration 303, loss = 0.09818415\n",
      "Iteration 304, loss = 0.09805934\n",
      "Iteration 332, loss = 0.11412132\n",
      "Iteration 225, loss = 0.13561915\n",
      "Iteration 96, loss = 0.25072814\n",
      "Iteration 305, loss = 0.09786574\n",
      "Iteration 111, loss = 0.21837213\n",
      "Iteration 333, loss = 0.11404573\n",
      "Iteration 27, loss = 0.49466130\n",
      "Iteration 191, loss = 0.16401015\n",
      "Iteration 97, loss = 0.24882773\n",
      "Iteration 226, loss = 0.13546722\n",
      "Iteration 306, loss = 0.09778431\n",
      "Iteration 307, loss = 0.09760864\n",
      "Iteration 227, loss = 0.13530203\n",
      "Iteration 308, loss = 0.09744273\n",
      "Iteration 268, loss = 0.21694090\n",
      "Iteration 334, loss = 0.11397013\n",
      "Iteration 192, loss = 0.16377207\n",
      "Iteration 309, loss = 0.09730846\n",
      "Iteration 98, loss = 0.24696409\n",
      "Iteration 99, loss = 0.24519146\n",
      "Iteration 28, loss = 0.48900921\n",
      "Iteration 228, loss = 0.13516374\n",
      "Iteration 112, loss = 0.21778351\n",
      "Iteration 335, loss = 0.11388851\n",
      "Iteration 310, loss = 0.09714389\n",
      "Iteration 311, loss = 0.09705579\n",
      "Iteration 193, loss = 0.16347836\n",
      "Iteration 312, loss = 0.09693847\n",
      "Iteration 336, loss = 0.11381127\n",
      "Iteration 269, loss = 0.21684986\n",
      "Iteration 229, loss = 0.13501691\n",
      "Iteration 313, loss = 0.09676736\n",
      "Iteration 100, loss = 0.24345302\n",
      "Iteration 113, loss = 0.21721328\n",
      "Iteration 314, loss = 0.09658353\n",
      "Iteration 315, loss = 0.09648408\n",
      "Iteration 230, loss = 0.13488257\n",
      "Iteration 337, loss = 0.11373015\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 101, loss = 0.24168176\n",
      "Iteration 29, loss = 0.48310772\n",
      "Iteration 194, loss = 0.16326479\n",
      "Iteration 231, loss = 0.13474646\n",
      "Iteration 316, loss = 0.09635999\n",
      "Iteration 1, loss = 1.05027863\n",
      "Iteration 2, loss = 0.98513063\n",
      "Iteration 3, loss = 0.90008503\n",
      "Iteration 4, loss = 0.81265323\n",
      "Iteration 5, loss = 0.73325220\n",
      "Iteration 317, loss = 0.09619698\n",
      "Iteration 6, loss = 0.66469351\n",
      "Iteration 7, loss = 0.60662842\n",
      "Iteration 195, loss = 0.16304111\n",
      "Iteration 8, loss = 0.55937410\n",
      "Iteration 9, loss = 0.51894050\n",
      "Iteration 318, loss = 0.09606569\n",
      "Iteration 102, loss = 0.23997855\n",
      "Iteration 232, loss = 0.13459198\n",
      "Iteration 114, loss = 0.21666463\n",
      "Iteration 10, loss = 0.48617498\n",
      "Iteration 30, loss = 0.47733680\n",
      "Iteration 11, loss = 0.45731718\n",
      "Iteration 270, loss = 0.21676193\n",
      "Iteration 12, loss = 0.43316925\n",
      "Iteration 319, loss = 0.09595287\n",
      "Iteration 13, loss = 0.41229050\n",
      "Iteration 14, loss = 0.39419378\n",
      "Iteration 15, loss = 0.37823306\n",
      "Iteration 16, loss = 0.36373006\n",
      "Iteration 17, loss = 0.35100839\n",
      "Iteration 18, loss = 0.33945708\n",
      "Iteration 320, loss = 0.09580121\n",
      "Iteration 19, loss = 0.32888982\n",
      "Iteration 20, loss = 0.31914394\n",
      "Iteration 21, loss = 0.31031149\n",
      "Iteration 22, loss = 0.30202853\n",
      "Iteration 23, loss = 0.29442840\n",
      "Iteration 24, loss = 0.28733445\n",
      "Iteration 25, loss = 0.28075170\n",
      "Iteration 196, loss = 0.16279495\n",
      "Iteration 26, loss = 0.27460989\n",
      "Iteration 27, loss = 0.26881400\n",
      "Iteration 28, loss = 0.26332036\n",
      "Iteration 29, loss = 0.25829271\n",
      "Iteration 321, loss = 0.09571880\n",
      "Iteration 322, loss = 0.09555983\n",
      "Iteration 30, loss = 0.25344615\n",
      "Iteration 31, loss = 0.24885854\n",
      "Iteration 32, loss = 0.24457799\n",
      "Iteration 33, loss = 0.24043184\n",
      "Iteration 34, loss = 0.23653940\n",
      "Iteration 35, loss = 0.23284407\n",
      "Iteration 36, loss = 0.22932507\n",
      "Iteration 323, loss = 0.09542209\n",
      "Iteration 233, loss = 0.13441924\n",
      "Iteration 103, loss = 0.23833300\n",
      "Iteration 37, loss = 0.22595961\n",
      "Iteration 115, loss = 0.21621807\n",
      "Iteration 38, loss = 0.22273223\n",
      "Iteration 39, loss = 0.21966764\n",
      "Iteration 40, loss = 0.21671341\n",
      "Iteration 41, loss = 0.21389483\n",
      "Iteration 42, loss = 0.21116774\n",
      "Iteration 43, loss = 0.20856305\n",
      "Iteration 44, loss = 0.20608014\n",
      "Iteration 45, loss = 0.20364839\n",
      "Iteration 46, loss = 0.20135336\n",
      "Iteration 47, loss = 0.19911193\n",
      "Iteration 48, loss = 0.19693024\n",
      "Iteration 49, loss = 0.19489152\n",
      "Iteration 50, loss = 0.19284687\n",
      "Iteration 51, loss = 0.19090337\n",
      "Iteration 52, loss = 0.18904926\n",
      "Iteration 53, loss = 0.18720592\n",
      "Iteration 271, loss = 0.21665048\n",
      "Iteration 234, loss = 0.13439150\n",
      "Iteration 54, loss = 0.18540652\n",
      "Iteration 324, loss = 0.09533983\n",
      "Iteration 31, loss = 0.47148229\n",
      "Iteration 55, loss = 0.18369502\n",
      "Iteration 56, loss = 0.18205853\n",
      "Iteration 57, loss = 0.18042421\n",
      "Iteration 104, loss = 0.23668040\n",
      "Iteration 58, loss = 0.17887589\n",
      "Iteration 59, loss = 0.17732581\n",
      "Iteration 60, loss = 0.17585206\n",
      "Iteration 197, loss = 0.16257673\n",
      "Iteration 325, loss = 0.09515892\n",
      "Iteration 61, loss = 0.17441069\n",
      "Iteration 62, loss = 0.17298756\n",
      "Iteration 63, loss = 0.17164691\n",
      "Iteration 64, loss = 0.17029456\n",
      "Iteration 326, loss = 0.09505466\n",
      "Iteration 65, loss = 0.16897675\n",
      "Iteration 66, loss = 0.16771055\n",
      "Iteration 327, loss = 0.09492144\n",
      "Iteration 67, loss = 0.16647023\n",
      "Iteration 68, loss = 0.16527050\n",
      "Iteration 69, loss = 0.16408658\n",
      "Iteration 70, loss = 0.16294370\n",
      "Iteration 116, loss = 0.21562676\n",
      "Iteration 71, loss = 0.16183297\n",
      "Iteration 198, loss = 0.16236785\n",
      "Iteration 72, loss = 0.16073231\n",
      "Iteration 73, loss = 0.15964609\n",
      "Iteration 74, loss = 0.15857706\n",
      "Iteration 105, loss = 0.23505118\n",
      "Iteration 75, loss = 0.15755862\n",
      "Iteration 76, loss = 0.15654413\n",
      "Iteration 77, loss = 0.15554460\n",
      "Iteration 235, loss = 0.13415458\n",
      "Iteration 78, loss = 0.15457785\n",
      "Iteration 328, loss = 0.09478502\n",
      "Iteration 79, loss = 0.15363067\n",
      "Iteration 80, loss = 0.15268921\n",
      "Iteration 199, loss = 0.16211368\n",
      "Iteration 81, loss = 0.15178173\n",
      "Iteration 272, loss = 0.21652735\n",
      "Iteration 117, loss = 0.21511415\n",
      "Iteration 82, loss = 0.15087401\n",
      "Iteration 236, loss = 0.13402594\n",
      "Iteration 83, loss = 0.14998081\n",
      "Iteration 84, loss = 0.14910692\n",
      "Iteration 85, loss = 0.14827778\n",
      "Iteration 86, loss = 0.14743775\n",
      "Iteration 87, loss = 0.14660603\n",
      "Iteration 88, loss = 0.14581246\n",
      "Iteration 89, loss = 0.14501342\n",
      "Iteration 90, loss = 0.14423845\n",
      "Iteration 91, loss = 0.14348285\n",
      "Iteration 329, loss = 0.09466311\n",
      "Iteration 92, loss = 0.14271629\n",
      "Iteration 93, loss = 0.14197236\n",
      "Iteration 94, loss = 0.14125630\n",
      "Iteration 95, loss = 0.14053482\n",
      "Iteration 96, loss = 0.13981047\n",
      "Iteration 97, loss = 0.13912242\n",
      "Iteration 98, loss = 0.13843350\n",
      "Iteration 106, loss = 0.23352286\n",
      "Iteration 99, loss = 0.13777310\n",
      "Iteration 100, loss = 0.13709588\n",
      "Iteration 237, loss = 0.13388434\n",
      "Iteration 330, loss = 0.09452972\n",
      "Iteration 107, loss = 0.23191327\n",
      "Iteration 331, loss = 0.09442202\n",
      "Iteration 32, loss = 0.46579122\n",
      "Iteration 200, loss = 0.16189691\n",
      "Iteration 332, loss = 0.09426592\n",
      "Iteration 101, loss = 0.13643517\n",
      "Iteration 118, loss = 0.21460378\n",
      "Iteration 238, loss = 0.13374085\n",
      "Iteration 333, loss = 0.09417764\n",
      "Iteration 334, loss = 0.09404831\n",
      "Iteration 108, loss = 0.23040943\n",
      "Iteration 102, loss = 0.13579014\n",
      "Iteration 103, loss = 0.13514815\n",
      "Iteration 104, loss = 0.13453140\n",
      "Iteration 273, loss = 0.21647898\n",
      "Iteration 105, loss = 0.13390816\n",
      "Iteration 106, loss = 0.13328780\n",
      "Iteration 335, loss = 0.09389894\n",
      "Iteration 107, loss = 0.13270061\n",
      "Iteration 108, loss = 0.13210207\n",
      "Iteration 109, loss = 0.13151393\n",
      "Iteration 239, loss = 0.13360306\n",
      "Iteration 110, loss = 0.13094003\n",
      "Iteration 109, loss = 0.22894013\n",
      "Iteration 33, loss = 0.45996147Iteration 336, loss = 0.09387446\n",
      "Iteration 201, loss = 0.16168587\n",
      "Iteration 111, loss = 0.13038069\n",
      "Iteration 112, loss = 0.12980441\n",
      "Iteration 113, loss = 0.12924775\n",
      "Iteration 114, loss = 0.12871168\n",
      "\n",
      "Iteration 115, loss = 0.12817056\n",
      "Iteration 116, loss = 0.12763415\n",
      "Iteration 117, loss = 0.12711092\n",
      "Iteration 118, loss = 0.12659571\n",
      "Iteration 119, loss = 0.12607776\n",
      "Iteration 120, loss = 0.12557346\n",
      "Iteration 337, loss = 0.09370305\n",
      "Iteration 121, loss = 0.12506746\n",
      "Iteration 122, loss = 0.12458159\n",
      "Iteration 119, loss = 0.21409977\n",
      "Iteration 123, loss = 0.12410515\n",
      "Iteration 124, loss = 0.12361588\n",
      "Iteration 125, loss = 0.12314263Iteration 202, loss = 0.16146337\n",
      "\n",
      "Iteration 274, loss = 0.21638264\n",
      "Iteration 126, loss = 0.12267839Iteration 110, loss = 0.22739290\n",
      "\n",
      "Iteration 338, loss = 0.09361568\n",
      "Iteration 127, loss = 0.12221026\n",
      "Iteration 128, loss = 0.12175730\n",
      "Iteration 129, loss = 0.12131120\n",
      "Iteration 130, loss = 0.12085662\n",
      "Iteration 240, loss = 0.13348006\n",
      "Iteration 339, loss = 0.09348287\n",
      "Iteration 131, loss = 0.12042221\n",
      "Iteration 132, loss = 0.11997929\n",
      "Iteration 133, loss = 0.11954382\n",
      "Iteration 134, loss = 0.11911192\n",
      "Iteration 135, loss = 0.11869354\n",
      "Iteration 136, loss = 0.11827251\n",
      "Iteration 340, loss = 0.09335119\n",
      "Iteration 137, loss = 0.11785618\n",
      "Iteration 138, loss = 0.11744017\n",
      "Iteration 139, loss = 0.11702983\n",
      "Iteration 140, loss = 0.11664180\n",
      "Iteration 141, loss = 0.11623515\n",
      "Iteration 142, loss = 0.11584259\n",
      "Iteration 143, loss = 0.11545348\n",
      "Iteration 241, loss = 0.13332292\n",
      "Iteration 111, loss = 0.22598433\n",
      "Iteration 144, loss = 0.11505839\n",
      "Iteration 203, loss = 0.16125381\n",
      "Iteration 145, loss = 0.11467748\n",
      "Iteration 146, loss = 0.11429685\n",
      "Iteration 341, loss = 0.09321726\n",
      "Iteration 147, loss = 0.11391401\n",
      "Iteration 148, loss = 0.11354949\n",
      "Iteration 242, loss = 0.13319044\n",
      "Iteration 149, loss = 0.11316854\n",
      "Iteration 150, loss = 0.11280295\n",
      "Iteration 151, loss = 0.11243829\n",
      "Iteration 152, loss = 0.11207406\n",
      "Iteration 120, loss = 0.21364259\n",
      "Iteration 153, loss = 0.11172767\n",
      "Iteration 154, loss = 0.11136783\n",
      "Iteration 155, loss = 0.11102379\n",
      "Iteration 156, loss = 0.11067823\n",
      "Iteration 157, loss = 0.11033000\n",
      "Iteration 158, loss = 0.10997841\n",
      "Iteration 159, loss = 0.10964748\n",
      "Iteration 342, loss = 0.09309426\n",
      "Iteration 34, loss = 0.45438175\n",
      "Iteration 243, loss = 0.13307221\n",
      "Iteration 160, loss = 0.10932004\n",
      "Iteration 161, loss = 0.10899305\n",
      "Iteration 112, loss = 0.22456639\n",
      "Iteration 343, loss = 0.09302870\n",
      "Iteration 162, loss = 0.10866492\n",
      "Iteration 163, loss = 0.10833645\n",
      "Iteration 204, loss = 0.16103197\n",
      "Iteration 164, loss = 0.10801214\n",
      "Iteration 165, loss = 0.10769230\n",
      "Iteration 275, loss = 0.21626335\n",
      "Iteration 121, loss = 0.21316763\n",
      "Iteration 344, loss = 0.09289035\n",
      "Iteration 166, loss = 0.10739084\n",
      "Iteration 167, loss = 0.10706278\n",
      "Iteration 168, loss = 0.10675209\n",
      "Iteration 345, loss = 0.09281594\n",
      "Iteration 169, loss = 0.10645477\n",
      "Iteration 113, loss = 0.22317029\n",
      "Iteration 122, loss = 0.21268208\n",
      "Iteration 170, loss = 0.10614780\n",
      "Iteration 244, loss = 0.13295172\n",
      "Iteration 35, loss = 0.44859569\n",
      "Iteration 171, loss = 0.10584123\n",
      "Iteration 114, loss = 0.22179487\n",
      "Iteration 172, loss = 0.10555261\n",
      "Iteration 173, loss = 0.10524495\n",
      "Iteration 346, loss = 0.09269935\n",
      "Iteration 174, loss = 0.10495350\n",
      "Iteration 245, loss = 0.13282286\n",
      "Iteration 205, loss = 0.16081455\n",
      "Iteration 175, loss = 0.10465331\n",
      "Iteration 347, loss = 0.09256682\n",
      "Iteration 176, loss = 0.10435986\n",
      "Iteration 177, loss = 0.10407269\n",
      "Iteration 178, loss = 0.10379416\n",
      "Iteration 179, loss = 0.10349849\n",
      "Iteration 180, loss = 0.10321133\n",
      "Iteration 276, loss = 0.21617440\n",
      "Iteration 181, loss = 0.10293193\n",
      "Iteration 182, loss = 0.10265961\n",
      "Iteration 183, loss = 0.10237313\n",
      "Iteration 348, loss = 0.09243581\n",
      "Iteration 184, loss = 0.10210021\n",
      "Iteration 185, loss = 0.10183079\n",
      "Iteration 186, loss = 0.10155620\n",
      "Iteration 246, loss = 0.13269044\n",
      "Iteration 349, loss = 0.09232058\n",
      "Iteration 187, loss = 0.10128914\n",
      "Iteration 206, loss = 0.16062406\n",
      "Iteration 188, loss = 0.10101405\n",
      "Iteration 189, loss = 0.10075091\n",
      "Iteration 190, loss = 0.10048762\n",
      "Iteration 191, loss = 0.10022959\n",
      "Iteration 36, loss = 0.44300122\n",
      "Iteration 350, loss = 0.09224318\n",
      "Iteration 192, loss = 0.09997050\n",
      "Iteration 123, loss = 0.21227749\n",
      "Iteration 193, loss = 0.09971188\n",
      "Iteration 194, loss = 0.09945164\n",
      "Iteration 195, loss = 0.09918901\n",
      "Iteration 196, loss = 0.09894210\n",
      "Iteration 115, loss = 0.22046963\n",
      "Iteration 197, loss = 0.09869156\n",
      "Iteration 198, loss = 0.09843994\n",
      "Iteration 351, loss = 0.09218782\n",
      "Iteration 199, loss = 0.09819464\n",
      "Iteration 200, loss = 0.09795028\n",
      "Iteration 201, loss = 0.09770149\n",
      "Iteration 202, loss = 0.09745917\n",
      "Iteration 203, loss = 0.09721387\n",
      "Iteration 204, loss = 0.09697424\n",
      "Iteration 205, loss = 0.09674635\n",
      "Iteration 247, loss = 0.13257108\n",
      "Iteration 206, loss = 0.09649590\n",
      "Iteration 207, loss = 0.09625965\n",
      "Iteration 208, loss = 0.09602356\n",
      "Iteration 209, loss = 0.09579261\n",
      "Iteration 210, loss = 0.09555645\n",
      "Iteration 211, loss = 0.09532406\n",
      "Iteration 212, loss = 0.09509664\n",
      "Iteration 116, loss = 0.21910295\n",
      "Iteration 248, loss = 0.13244312\n",
      "Iteration 213, loss = 0.09485742\n",
      "Iteration 214, loss = 0.09463202\n",
      "Iteration 352, loss = 0.09201855\n",
      "Iteration 353, loss = 0.09192645\n",
      "Iteration 277, loss = 0.21606504\n",
      "Iteration 207, loss = 0.16039970\n",
      "Iteration 215, loss = 0.09441074\n",
      "Iteration 216, loss = 0.09419447\n",
      "Iteration 217, loss = 0.09395848\n",
      "Iteration 218, loss = 0.09373539\n",
      "Iteration 219, loss = 0.09351374\n",
      "Iteration 124, loss = 0.21177625\n",
      "Iteration 220, loss = 0.09329562\n",
      "Iteration 117, loss = 0.21787777\n",
      "Iteration 221, loss = 0.09307377\n",
      "Iteration 222, loss = 0.09285801\n",
      "Iteration 223, loss = 0.09263875\n",
      "Iteration 224, loss = 0.09241954\n",
      "Iteration 225, loss = 0.09220250\n",
      "Iteration 354, loss = 0.09180505\n",
      "Iteration 355, loss = 0.09168179\n",
      "Iteration 226, loss = 0.09199107\n",
      "Iteration 249, loss = 0.13231111\n",
      "Iteration 227, loss = 0.09177901\n",
      "Iteration 228, loss = 0.09157467\n",
      "Iteration 229, loss = 0.09135956\n",
      "Iteration 230, loss = 0.09114499\n",
      "Iteration 231, loss = 0.09094484\n",
      "Iteration 356, loss = 0.09159603\n",
      "Iteration 250, loss = 0.13221098\n",
      "Iteration 208, loss = 0.16020687\n",
      "Iteration 232, loss = 0.09072958\n",
      "Iteration 233, loss = 0.09052782\n",
      "Iteration 125, loss = 0.21130459\n",
      "Iteration 37, loss = 0.43729649\n",
      "Iteration 234, loss = 0.09032403\n",
      "Iteration 235, loss = 0.09011522\n",
      "Iteration 236, loss = 0.08990998\n",
      "Iteration 237, loss = 0.08970639\n",
      "Iteration 238, loss = 0.08950721\n",
      "Iteration 118, loss = 0.21655329\n",
      "Iteration 278, loss = 0.21600278\n",
      "Iteration 357, loss = 0.09150963\n",
      "Iteration 358, loss = 0.09138352\n",
      "Iteration 239, loss = 0.08930997\n",
      "Iteration 240, loss = 0.08911413\n",
      "Iteration 241, loss = 0.08891455\n",
      "Iteration 242, loss = 0.08871578\n",
      "Iteration 243, loss = 0.08852536\n",
      "Iteration 244, loss = 0.08833028\n",
      "Iteration 245, loss = 0.08813953\n",
      "Iteration 359, loss = 0.09127459\n",
      "Iteration 246, loss = 0.08794595\n",
      "Iteration 247, loss = 0.08775445\n",
      "Iteration 248, loss = 0.08756662\n",
      "Iteration 209, loss = 0.15999203\n",
      "Iteration 249, loss = 0.08737681\n",
      "Iteration 250, loss = 0.08719236\n",
      "Iteration 251, loss = 0.08700807\n",
      "Iteration 252, loss = 0.08681630\n",
      "Iteration 253, loss = 0.08662865\n",
      "Iteration 360, loss = 0.09118174\n",
      "Iteration 254, loss = 0.08644486\n",
      "Iteration 255, loss = 0.08625688\n",
      "Iteration 256, loss = 0.08607658\n",
      "Iteration 257, loss = 0.08589400\n",
      "Iteration 258, loss = 0.08571025\n",
      "Iteration 259, loss = 0.08552930\n",
      "Iteration 260, loss = 0.08535300\n",
      "Iteration 119, loss = 0.21534084\n",
      "Iteration 361, loss = 0.09108975\n",
      "Iteration 362, loss = 0.09097711Iteration 261, loss = 0.08517107\n",
      "\n",
      "Iteration 251, loss = 0.13211367\n",
      "Iteration 262, loss = 0.08499022\n",
      "Iteration 263, loss = 0.08482270\n",
      "Iteration 264, loss = 0.08463860\n",
      "Iteration 126, loss = 0.21085473\n",
      "Iteration 363, loss = 0.09090142\n",
      "Iteration 265, loss = 0.08446029\n",
      "Iteration 38, loss = 0.43166723\n",
      "Iteration 266, loss = 0.08428356\n",
      "Iteration 267, loss = 0.08410484\n",
      "Iteration 268, loss = 0.08393131\n",
      "Iteration 269, loss = 0.08375813\n",
      "Iteration 270, loss = 0.08357674\n",
      "Iteration 271, loss = 0.08341598\n",
      "Iteration 272, loss = 0.08324271\n",
      "Iteration 252, loss = 0.13199989\n",
      "Iteration 210, loss = 0.15981868\n",
      "Iteration 279, loss = 0.21587910\n",
      "Iteration 253, loss = 0.13186267\n",
      "Iteration 364, loss = 0.09079007\n",
      "Iteration 120, loss = 0.21408434\n",
      "Iteration 365, loss = 0.09071724\n",
      "Iteration 273, loss = 0.08306859\n",
      "Iteration 274, loss = 0.08290035\n",
      "Iteration 275, loss = 0.08273418\n",
      "Iteration 276, loss = 0.08255702\n",
      "Iteration 366, loss = 0.09057074\n",
      "Iteration 277, loss = 0.08239228\n",
      "Iteration 278, loss = 0.08222714\n",
      "Iteration 279, loss = 0.08206284\n",
      "Iteration 121, loss = 0.21292426\n",
      "Iteration 280, loss = 0.08189391\n",
      "Iteration 281, loss = 0.08172889\n",
      "Iteration 282, loss = 0.08156877\n",
      "Iteration 127, loss = 0.21049979\n",
      "Iteration 283, loss = 0.08140516\n",
      "Iteration 284, loss = 0.08124642\n",
      "Iteration 285, loss = 0.08108129\n",
      "Iteration 286, loss = 0.08092694\n",
      "Iteration 287, loss = 0.08076060\n",
      "Iteration 288, loss = 0.08060700\n",
      "Iteration 367, loss = 0.09049420\n",
      "Iteration 289, loss = 0.08044698\n",
      "Iteration 290, loss = 0.08028529\n",
      "Iteration 291, loss = 0.08013115\n",
      "Iteration 292, loss = 0.07997840\n",
      "Iteration 293, loss = 0.07982679\n",
      "Iteration 294, loss = 0.07966668\n",
      "Iteration 211, loss = 0.15960574\n",
      "Iteration 254, loss = 0.13175424\n",
      "Iteration 39, loss = 0.42619976\n",
      "Iteration 368, loss = 0.09047112\n",
      "Iteration 122, loss = 0.21168205\n",
      "Iteration 295, loss = 0.07951832\n",
      "Iteration 296, loss = 0.07936598\n",
      "Iteration 297, loss = 0.07921916\n",
      "Iteration 212, loss = 0.15942987\n",
      "Iteration 298, loss = 0.07906700\n",
      "Iteration 280, loss = 0.21585382\n",
      "Iteration 299, loss = 0.07891747\n",
      "Iteration 300, loss = 0.07876809\n",
      "Iteration 301, loss = 0.07862105\n",
      "Iteration 302, loss = 0.07847448\n",
      "Iteration 255, loss = 0.13165411\n",
      "Iteration 369, loss = 0.09039033\n",
      "Iteration 303, loss = 0.07832828\n",
      "Iteration 304, loss = 0.07818484\n",
      "Iteration 305, loss = 0.07804051\n",
      "Iteration 306, loss = 0.07789223\n",
      "Iteration 307, loss = 0.07775409\n",
      "Iteration 370, loss = 0.09021986\n",
      "Iteration 128, loss = 0.21002023\n",
      "Iteration 308, loss = 0.07760791\n",
      "Iteration 371, loss = 0.09014250\n",
      "Iteration 309, loss = 0.07747163\n",
      "Iteration 123, loss = 0.21056545\n",
      "Iteration 310, loss = 0.07732333\n",
      "Iteration 256, loss = 0.13154521\n",
      "Iteration 311, loss = 0.07718437\n",
      "Iteration 372, loss = 0.09005400\n",
      "Iteration 312, loss = 0.07704580\n",
      "Iteration 313, loss = 0.07690612\n",
      "Iteration 314, loss = 0.07676350\n",
      "Iteration 315, loss = 0.07662301\n",
      "Iteration 316, loss = 0.07648274\n",
      "Iteration 124, loss = 0.20938449\n",
      "Iteration 213, loss = 0.15920566\n",
      "Iteration 317, loss = 0.07634066\n",
      "Iteration 318, loss = 0.07620961\n",
      "Iteration 319, loss = 0.07606598\n",
      "Iteration 320, loss = 0.07592774\n",
      "Iteration 321, loss = 0.07578961\n",
      "Iteration 322, loss = 0.07565703\n",
      "Iteration 323, loss = 0.07551139\n",
      "Iteration 324, loss = 0.07538370\n",
      "Iteration 325, loss = 0.07524361\n",
      "Iteration 326, loss = 0.07511787\n",
      "Iteration 327, loss = 0.07498779\n",
      "Iteration 328, loss = 0.07485794\n",
      "Iteration 329, loss = 0.07472896\n",
      "Iteration 257, loss = 0.13141983\n",
      "Iteration 330, loss = 0.07459675\n",
      "Iteration 281, loss = 0.21571067\n",
      "Iteration 40, loss = 0.42082333\n",
      "Iteration 373, loss = 0.08994764\n",
      "Iteration 129, loss = 0.20957292\n",
      "Iteration 331, loss = 0.07446850\n",
      "Iteration 374, loss = 0.08985469\n",
      "Iteration 258, loss = 0.13132489\n",
      "Iteration 332, loss = 0.07433504\n",
      "Iteration 333, loss = 0.07421102\n",
      "Iteration 334, loss = 0.07407478\n",
      "Iteration 214, loss = 0.15902391\n",
      "Iteration 125, loss = 0.20827852\n",
      "Iteration 335, loss = 0.07394723\n",
      "Iteration 375, loss = 0.08980415\n",
      "Iteration 259, loss = 0.13127479\n",
      "Iteration 376, loss = 0.08966773\n",
      "Iteration 126, loss = 0.20719538\n",
      "Iteration 377, loss = 0.08955489\n",
      "Iteration 282, loss = 0.21564075\n",
      "Iteration 336, loss = 0.07381608\n",
      "Iteration 337, loss = 0.07368654\n",
      "Iteration 338, loss = 0.07355564\n",
      "Iteration 339, loss = 0.07342627\n",
      "Iteration 340, loss = 0.07330749\n",
      "Iteration 341, loss = 0.07317637\n",
      "Iteration 342, loss = 0.07305553\n",
      "Iteration 130, loss = 0.20917186\n",
      "Iteration 343, loss = 0.07292184\n",
      "Iteration 344, loss = 0.07279581\n",
      "Iteration 345, loss = 0.07266967\n",
      "Iteration 346, loss = 0.07254739\n",
      "Iteration 215, loss = 0.15881826\n",
      "Iteration 260, loss = 0.13112117\n",
      "Iteration 347, loss = 0.07242330\n",
      "Iteration 348, loss = 0.07229620\n",
      "Iteration 349, loss = 0.07216995\n",
      "Iteration 350, loss = 0.07204504\n",
      "Iteration 351, loss = 0.07192475\n",
      "Iteration 352, loss = 0.07179722\n",
      "Iteration 378, loss = 0.08952144\n",
      "Iteration 41, loss = 0.41543290\n",
      "Iteration 261, loss = 0.13105544\n",
      "Iteration 353, loss = 0.07166991\n",
      "Iteration 379, loss = 0.08943958\n",
      "Iteration 354, loss = 0.07155371\n",
      "Iteration 355, loss = 0.07143223\n",
      "Iteration 356, loss = 0.07131084\n",
      "Iteration 357, loss = 0.07118869\n",
      "Iteration 216, loss = 0.15864849\n",
      "Iteration 380, loss = 0.08933556\n",
      "Iteration 358, loss = 0.07107224\n",
      "Iteration 381, loss = 0.08924037\n",
      "Iteration 382, loss = 0.08911474\n",
      "Iteration 131, loss = 0.20876163\n",
      "Iteration 127, loss = 0.20608601\n",
      "Iteration 359, loss = 0.07095533\n",
      "Iteration 360, loss = 0.07083624\n",
      "Iteration 361, loss = 0.07071430\n",
      "Iteration 362, loss = 0.07060243\n",
      "Iteration 363, loss = 0.07048064\n",
      "Iteration 42, loss = 0.41016892\n",
      "Iteration 364, loss = 0.07037294\n",
      "Iteration 383, loss = 0.08907616\n",
      "Iteration 262, loss = 0.13090204\n",
      "Iteration 365, loss = 0.07025764\n",
      "Iteration 366, loss = 0.07013867\n",
      "Iteration 263, loss = 0.13077825\n",
      "Iteration 367, loss = 0.07002936\n",
      "Iteration 283, loss = 0.21557595\n",
      "Iteration 368, loss = 0.06991742\n",
      "Iteration 369, loss = 0.06979973\n",
      "Iteration 370, loss = 0.06969420\n",
      "Iteration 371, loss = 0.06958007\n",
      "Iteration 372, loss = 0.06946993\n",
      "Iteration 373, loss = 0.06935496\n",
      "Iteration 374, loss = 0.06924776\n",
      "Iteration 375, loss = 0.06913925\n",
      "Iteration 376, loss = 0.06903290\n",
      "Iteration 377, loss = 0.06891806\n",
      "Iteration 378, loss = 0.06881452\n",
      "Iteration 379, loss = 0.06870987\n",
      "Iteration 380, loss = 0.06859342\n",
      "Iteration 381, loss = 0.06848713\n",
      "Iteration 382, loss = 0.06838389\n",
      "Iteration 383, loss = 0.06827746\n",
      "Iteration 264, loss = 0.13067198\n",
      "Iteration 384, loss = 0.06817154\n",
      "Iteration 385, loss = 0.06806553\n",
      "Iteration 386, loss = 0.06796379\n",
      "Iteration 387, loss = 0.06786277\n",
      "Iteration 388, loss = 0.06776108\n",
      "Iteration 389, loss = 0.06765759\n",
      "Iteration 390, loss = 0.06755981\n",
      "Iteration 391, loss = 0.06745544\n",
      "Iteration 392, loss = 0.06735499\n",
      "Iteration 393, loss = 0.06725943\n",
      "Iteration 394, loss = 0.06715766\n",
      "Iteration 395, loss = 0.06705320\n",
      "Iteration 396, loss = 0.06695190\n",
      "Iteration 397, loss = 0.06685528\n",
      "Iteration 398, loss = 0.06675612\n",
      "Iteration 384, loss = 0.08898958\n",
      "Iteration 399, loss = 0.06665641\n",
      "Iteration 400, loss = 0.06655887\n",
      "Iteration 401, loss = 0.06646259\n",
      "Iteration 402, loss = 0.06636513\n",
      "Iteration 128, loss = 0.20508465\n",
      "Iteration 385, loss = 0.08888783\n",
      "Iteration 43, loss = 0.40515102\n",
      "Iteration 132, loss = 0.20839197\n",
      "Iteration 403, loss = 0.06626745Iteration 217, loss = 0.15843343\n",
      "Iteration 129, loss = 0.20401998\n",
      "\n",
      "Iteration 386, loss = 0.08880285\n",
      "Iteration 404, loss = 0.06617288\n",
      "Iteration 265, loss = 0.13061718\n",
      "Iteration 405, loss = 0.06608227\n",
      "Iteration 284, loss = 0.21545916\n",
      "Iteration 406, loss = 0.06598724\n",
      "Iteration 407, loss = 0.06588847\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 130, loss = 0.20300758\n",
      "Iteration 1, loss = 0.69096605\n",
      "Iteration 2, loss = 0.65968780\n",
      "Iteration 387, loss = 0.08870669\n",
      "Iteration 218, loss = 0.15824621Iteration 388, loss = 0.08862390\n",
      "\n",
      "Iteration 133, loss = 0.20801307\n",
      "Iteration 3, loss = 0.61920610\n",
      "Iteration 4, loss = 0.57740908\n",
      "Iteration 5, loss = 0.53813077\n",
      "Iteration 6, loss = 0.50413824\n",
      "Iteration 131, loss = 0.20204089\n",
      "Iteration 266, loss = 0.13048236\n",
      "Iteration 7, loss = 0.47449131\n",
      "Iteration 8, loss = 0.44929140\n",
      "Iteration 389, loss = 0.08854333\n",
      "Iteration 390, loss = 0.08847117\n",
      "Iteration 9, loss = 0.42719575\n",
      "Iteration 391, loss = 0.08838835\n",
      "Iteration 10, loss = 0.40828822\n",
      "Iteration 11, loss = 0.39151685\n",
      "Iteration 12, loss = 0.37680981\n",
      "Iteration 13, loss = 0.36370735\n",
      "Iteration 14, loss = 0.35185906\n",
      "Iteration 15, loss = 0.34100075\n",
      "Iteration 16, loss = 0.33121961\n",
      "Iteration 17, loss = 0.32201667\n",
      "Iteration 18, loss = 0.31385328\n",
      "Iteration 19, loss = 0.30594406\n",
      "Iteration 44, loss = 0.39996116\n",
      "Iteration 219, loss = 0.15804351\n",
      "Iteration 285, loss = 0.21534362\n",
      "Iteration 267, loss = 0.13037961\n",
      "Iteration 392, loss = 0.08833529\n",
      "Iteration 132, loss = 0.20108748\n",
      "Iteration 268, loss = 0.13027851\n",
      "Iteration 20, loss = 0.29869537\n",
      "Iteration 21, loss = 0.29184015\n",
      "Iteration 393, loss = 0.08823442\n",
      "Iteration 134, loss = 0.20761418\n",
      "Iteration 22, loss = 0.28548654\n",
      "Iteration 220, loss = 0.15787553\n",
      "Iteration 23, loss = 0.27942955\n",
      "Iteration 24, loss = 0.27375657\n",
      "Iteration 269, loss = 0.13018093\n",
      "Iteration 25, loss = 0.26839980\n",
      "Iteration 26, loss = 0.26324699\n",
      "Iteration 27, loss = 0.25829901\n",
      "Iteration 28, loss = 0.25378987\n",
      "Iteration 133, loss = 0.20022572\n",
      "Iteration 45, loss = 0.39497606\n",
      "Iteration 394, loss = 0.08815258\n",
      "Iteration 286, loss = 0.21523531\n",
      "Iteration 29, loss = 0.24939245\n",
      "Iteration 30, loss = 0.24516154\n",
      "Iteration 31, loss = 0.24118871\n",
      "Iteration 32, loss = 0.23734021\n",
      "Iteration 33, loss = 0.23365085\n",
      "Iteration 270, loss = 0.13008145\n",
      "Iteration 34, loss = 0.23016544\n",
      "Iteration 395, loss = 0.08813335\n",
      "Iteration 135, loss = 0.20725391\n",
      "Iteration 35, loss = 0.22682308\n",
      "Iteration 36, loss = 0.22356274\n",
      "Iteration 396, loss = 0.08806207\n",
      "Iteration 397, loss = 0.08795824\n",
      "Iteration 37, loss = 0.22043204\n",
      "Iteration 38, loss = 0.21750314\n",
      "Iteration 39, loss = 0.21460339\n",
      "Iteration 40, loss = 0.21182678\n",
      "Iteration 41, loss = 0.20916569\n",
      "Iteration 398, loss = 0.08784729\n",
      "Iteration 134, loss = 0.19922581\n",
      "Iteration 399, loss = 0.08776988\n",
      "Iteration 221, loss = 0.15768117\n",
      "Iteration 400, loss = 0.08778365\n",
      "Iteration 42, loss = 0.20657306\n",
      "Iteration 271, loss = 0.13000108\n",
      "Iteration 287, loss = 0.21518595\n",
      "Iteration 43, loss = 0.20408087\n",
      "Iteration 46, loss = 0.39008511\n",
      "Iteration 44, loss = 0.20162198\n",
      "Iteration 401, loss = 0.08773592\n",
      "Iteration 45, loss = 0.19932436\n",
      "Iteration 272, loss = 0.12989523\n",
      "Iteration 46, loss = 0.19706886\n",
      "Iteration 135, loss = 0.19832764\n",
      "Iteration 136, loss = 0.20683605\n",
      "Iteration 47, loss = 0.19485296\n",
      "Iteration 402, loss = 0.08757323\n",
      "Iteration 48, loss = 0.19271926\n",
      "Iteration 222, loss = 0.15751639\n",
      "Iteration 49, loss = 0.19068762\n",
      "Iteration 403, loss = 0.08750984\n",
      "Iteration 50, loss = 0.18866574\n",
      "Iteration 51, loss = 0.18674628\n",
      "Iteration 52, loss = 0.18485273\n",
      "Iteration 53, loss = 0.18301069\n",
      "Iteration 404, loss = 0.08742360\n",
      "Iteration 54, loss = 0.18122272\n",
      "Iteration 55, loss = 0.17949881\n",
      "Iteration 56, loss = 0.17781835\n",
      "Iteration 57, loss = 0.17617859\n",
      "Iteration 58, loss = 0.17454577\n",
      "Iteration 405, loss = 0.08740226\n",
      "Iteration 273, loss = 0.12984018\n",
      "Iteration 59, loss = 0.17301212\n",
      "Iteration 136, loss = 0.19748400\n",
      "Iteration 60, loss = 0.17148031\n",
      "Iteration 61, loss = 0.17000045\n",
      "Iteration 62, loss = 0.16856986\n",
      "Iteration 63, loss = 0.16715110\n",
      "Iteration 64, loss = 0.16575103\n",
      "Iteration 288, loss = 0.21513587\n",
      "Iteration 65, loss = 0.16444833\n",
      "Iteration 66, loss = 0.16311191\n",
      "Iteration 406, loss = 0.08731148\n",
      "Iteration 67, loss = 0.16184522\n",
      "Iteration 223, loss = 0.15731890\n",
      "Iteration 274, loss = 0.12971647\n",
      "Iteration 407, loss = 0.08723149\n",
      "Iteration 275, loss = 0.12960506\n",
      "Iteration 137, loss = 0.20649998\n",
      "Iteration 47, loss = 0.38516309\n",
      "Iteration 137, loss = 0.19661297\n",
      "Iteration 68, loss = 0.16057828\n",
      "Iteration 69, loss = 0.15935607\n",
      "Iteration 70, loss = 0.15814410\n",
      "Iteration 71, loss = 0.15698878\n",
      "Iteration 72, loss = 0.15585130\n",
      "Iteration 73, loss = 0.15471608\n",
      "Iteration 408, loss = 0.08718683\n",
      "Iteration 74, loss = 0.15361559\n",
      "Iteration 75, loss = 0.15254239\n",
      "Iteration 224, loss = 0.15712323\n",
      "Iteration 76, loss = 0.15149761\n",
      "Iteration 77, loss = 0.15043803\n",
      "Iteration 409, loss = 0.08710944\n",
      "Iteration 78, loss = 0.14944237\n",
      "Iteration 138, loss = 0.19576023\n",
      "Iteration 79, loss = 0.14844294\n",
      "Iteration 80, loss = 0.14747679\n",
      "Iteration 81, loss = 0.14651049\n",
      "Iteration 82, loss = 0.14556885\n",
      "Iteration 83, loss = 0.14468722\n",
      "Iteration 84, loss = 0.14377078\n",
      "Iteration 410, loss = 0.08702409\n",
      "Iteration 85, loss = 0.14290112\n",
      "Iteration 276, loss = 0.12953887\n",
      "Iteration 86, loss = 0.14200882\n",
      "Iteration 225, loss = 0.15693508\n",
      "Iteration 138, loss = 0.20612354\n",
      "Iteration 139, loss = 0.19492374\n",
      "Iteration 87, loss = 0.14115636\n",
      "Iteration 88, loss = 0.14032911\n",
      "Iteration 277, loss = 0.12942941\n",
      "Iteration 289, loss = 0.21501417\n",
      "Iteration 89, loss = 0.13949312\n",
      "Iteration 90, loss = 0.13867702\n",
      "Iteration 91, loss = 0.13788283\n",
      "Iteration 92, loss = 0.13711492\n",
      "Iteration 411, loss = 0.08697737\n",
      "Iteration 93, loss = 0.13634930\n",
      "Iteration 412, loss = 0.08695430\n",
      "Iteration 48, loss = 0.38053342Iteration 278, loss = 0.12934747\n",
      "Iteration 94, loss = 0.13559169\n",
      "Iteration 413, loss = 0.08691484\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 95, loss = 0.13484568\n",
      "\n",
      "Iteration 96, loss = 0.13411779\n",
      "Iteration 97, loss = 0.13339294\n",
      "Iteration 98, loss = 0.13269129\n",
      "Iteration 99, loss = 0.13198503\n",
      "Iteration 226, loss = 0.15680672\n",
      "Iteration 1, loss = 0.70408967\n",
      "Iteration 2, loss = 0.66860667\n",
      "Iteration 3, loss = 0.62257785\n",
      "Iteration 4, loss = 0.57595941\n",
      "Iteration 5, loss = 0.53440329\n",
      "Iteration 139, loss = 0.20579823\n",
      "Iteration 6, loss = 0.49799866\n",
      "Iteration 100, loss = 0.13130940\n",
      "Iteration 140, loss = 0.19414923\n",
      "Iteration 7, loss = 0.46782513\n",
      "Iteration 8, loss = 0.44242092\n",
      "Iteration 9, loss = 0.42052636\n",
      "Iteration 10, loss = 0.40233604\n",
      "Iteration 227, loss = 0.15659051\n",
      "Iteration 101, loss = 0.13062643\n",
      "Iteration 102, loss = 0.12997486\n",
      "Iteration 103, loss = 0.12932051\n",
      "Iteration 104, loss = 0.12867212\n",
      "Iteration 279, loss = 0.12925713\n",
      "Iteration 11, loss = 0.38620728\n",
      "Iteration 105, loss = 0.12805460\n",
      "Iteration 290, loss = 0.21495644\n",
      "Iteration 106, loss = 0.12741421\n",
      "Iteration 107, loss = 0.12680174\n",
      "Iteration 108, loss = 0.12620566\n",
      "Iteration 109, loss = 0.12559941\n",
      "Iteration 110, loss = 0.12500911\n",
      "Iteration 280, loss = 0.12916524\n",
      "Iteration 111, loss = 0.12444852\n",
      "Iteration 141, loss = 0.19331819\n",
      "Iteration 12, loss = 0.37223459\n",
      "Iteration 112, loss = 0.12386347\n",
      "Iteration 281, loss = 0.12906511\n",
      "Iteration 113, loss = 0.12329560\n",
      "Iteration 114, loss = 0.12272680\n",
      "Iteration 115, loss = 0.12218876\n",
      "Iteration 116, loss = 0.12163907\n",
      "Iteration 117, loss = 0.12109959\n",
      "Iteration 13, loss = 0.35968825\n",
      "Iteration 118, loss = 0.12057527\n",
      "Iteration 142, loss = 0.19253590\n",
      "Iteration 119, loss = 0.12004649\n",
      "Iteration 120, loss = 0.11954281\n",
      "Iteration 121, loss = 0.11902303\n",
      "Iteration 291, loss = 0.21483305\n",
      "Iteration 228, loss = 0.15642078\n",
      "Iteration 122, loss = 0.11852229\n",
      "Iteration 123, loss = 0.11804102\n",
      "Iteration 124, loss = 0.11752854\n",
      "Iteration 125, loss = 0.11704478\n",
      "Iteration 126, loss = 0.11657156\n",
      "Iteration 127, loss = 0.11611169\n",
      "Iteration 128, loss = 0.11562192\n",
      "Iteration 14, loss = 0.34846745\n",
      "Iteration 129, loss = 0.11517070\n",
      "Iteration 15, loss = 0.33832332\n",
      "Iteration 130, loss = 0.11471641\n",
      "Iteration 16, loss = 0.32898489\n",
      "Iteration 143, loss = 0.19177088\n",
      "Iteration 17, loss = 0.32035304\n",
      "Iteration 282, loss = 0.12904214\n",
      "Iteration 49, loss = 0.37586857\n",
      "Iteration 140, loss = 0.20542861\n",
      "Iteration 18, loss = 0.31232390\n",
      "Iteration 19, loss = 0.30480943\n",
      "Iteration 20, loss = 0.29777893\n",
      "Iteration 21, loss = 0.29124700\n",
      "Iteration 131, loss = 0.11426761\n",
      "Iteration 283, loss = 0.12894395\n",
      "Iteration 229, loss = 0.15623379\n",
      "Iteration 132, loss = 0.11381488\n",
      "Iteration 22, loss = 0.28501088\n",
      "Iteration 133, loss = 0.11338501\n",
      "Iteration 23, loss = 0.27908561\n",
      "Iteration 24, loss = 0.27357323\n",
      "Iteration 134, loss = 0.11294081\n",
      "Iteration 25, loss = 0.26817628\n",
      "Iteration 135, loss = 0.11250831\n",
      "Iteration 136, loss = 0.11208366\n",
      "Iteration 137, loss = 0.11166720\n",
      "Iteration 26, loss = 0.26314337\n",
      "Iteration 138, loss = 0.11124680\n",
      "Iteration 27, loss = 0.25829821\n",
      "Iteration 139, loss = 0.11083687\n",
      "Iteration 28, loss = 0.25372084\n",
      "Iteration 29, loss = 0.24928735\n",
      "Iteration 144, loss = 0.19102385\n",
      "Iteration 140, loss = 0.11042929\n",
      "Iteration 30, loss = 0.24504220\n",
      "Iteration 50, loss = 0.37147388\n",
      "Iteration 31, loss = 0.24097343\n",
      "Iteration 32, loss = 0.23710342\n",
      "Iteration 33, loss = 0.23335313\n",
      "Iteration 34, loss = 0.22971634\n",
      "Iteration 141, loss = 0.11001550\n",
      "Iteration 142, loss = 0.10962339\n",
      "Iteration 284, loss = 0.12880779\n",
      "Iteration 141, loss = 0.20505419\n",
      "Iteration 143, loss = 0.10921976\n",
      "Iteration 292, loss = 0.21478618\n",
      "Iteration 35, loss = 0.22624981\n",
      "Iteration 36, loss = 0.22290063\n",
      "Iteration 144, loss = 0.10884036\n",
      "Iteration 145, loss = 0.10844260\n",
      "Iteration 230, loss = 0.15612425Iteration 146, loss = 0.10805858\n",
      "\n",
      "Iteration 147, loss = 0.10768054\n",
      "Iteration 37, loss = 0.21972459\n",
      "Iteration 285, loss = 0.12872287\n",
      "Iteration 148, loss = 0.10731936\n",
      "Iteration 38, loss = 0.21662429\n",
      "Iteration 149, loss = 0.10693619\n",
      "Iteration 150, loss = 0.10656668\n",
      "Iteration 145, loss = 0.19030850\n",
      "Iteration 39, loss = 0.21355440\n",
      "Iteration 151, loss = 0.10619495\n",
      "Iteration 152, loss = 0.10583669\n",
      "Iteration 40, loss = 0.21067488\n",
      "Iteration 142, loss = 0.20475855\n",
      "Iteration 41, loss = 0.20788524\n",
      "Iteration 42, loss = 0.20514722\n",
      "Iteration 153, loss = 0.10547282\n",
      "Iteration 146, loss = 0.18957563\n",
      "Iteration 43, loss = 0.20251766\n",
      "Iteration 154, loss = 0.10511516\n",
      "Iteration 231, loss = 0.15591644\n",
      "Iteration 44, loss = 0.19996287\n",
      "Iteration 155, loss = 0.10476753\n",
      "Iteration 51, loss = 0.36702171\n",
      "Iteration 45, loss = 0.19752277\n",
      "Iteration 156, loss = 0.10441358\n",
      "Iteration 157, loss = 0.10408236\n",
      "Iteration 158, loss = 0.10372376\n",
      "Iteration 147, loss = 0.18888252\n",
      "Iteration 293, loss = 0.21465072\n",
      "Iteration 159, loss = 0.10338518Iteration 46, loss = 0.19511413\n",
      "\n",
      "Iteration 160, loss = 0.10305491\n",
      "Iteration 161, loss = 0.10271840\n",
      "Iteration 47, loss = 0.19281443\n",
      "Iteration 162, loss = 0.10238799\n",
      "Iteration 48, loss = 0.19055353\n",
      "Iteration 49, loss = 0.18841073\n",
      "Iteration 232, loss = 0.15573540\n",
      "Iteration 50, loss = 0.18629781\n",
      "Iteration 51, loss = 0.18420729\n",
      "Iteration 52, loss = 0.18223749\n",
      "Iteration 53, loss = 0.18026908\n",
      "Iteration 54, loss = 0.17834887\n",
      "Iteration 286, loss = 0.12862272\n",
      "Iteration 163, loss = 0.10205110\n",
      "Iteration 164, loss = 0.10173652\n",
      "Iteration 165, loss = 0.10140854\n",
      "Iteration 166, loss = 0.10110204\n",
      "Iteration 167, loss = 0.10077265\n",
      "Iteration 168, loss = 0.10046483\n",
      "Iteration 143, loss = 0.20438172\n",
      "Iteration 287, loss = 0.12860418\n",
      "Iteration 169, loss = 0.10014994\n",
      "Iteration 233, loss = 0.15555953\n",
      "Iteration 55, loss = 0.17652926Iteration 170, loss = 0.09984751\n",
      "Iteration 171, loss = 0.09953003\n",
      "Iteration 172, loss = 0.09922469\n",
      "\n",
      "Iteration 173, loss = 0.09892477\n",
      "Iteration 148, loss = 0.18813551\n",
      "Iteration 52, loss = 0.36264111\n",
      "Iteration 56, loss = 0.17473990\n",
      "Iteration 57, loss = 0.17302992\n",
      "Iteration 58, loss = 0.17130858\n",
      "Iteration 174, loss = 0.09863209\n",
      "Iteration 288, loss = 0.12846988\n",
      "Iteration 59, loss = 0.16967822\n",
      "Iteration 60, loss = 0.16805192\n",
      "Iteration 61, loss = 0.16647386\n",
      "Iteration 62, loss = 0.16493304\n",
      "Iteration 289, loss = 0.12837577\n",
      "Iteration 149, loss = 0.18748052\n",
      "Iteration 294, loss = 0.21457245\n",
      "Iteration 234, loss = 0.15539233\n",
      "Iteration 290, loss = 0.12829359\n",
      "Iteration 175, loss = 0.09833256\n",
      "Iteration 63, loss = 0.16343793\n",
      "Iteration 176, loss = 0.09804925\n",
      "Iteration 177, loss = 0.09774846\n",
      "Iteration 64, loss = 0.16199258\n",
      "Iteration 178, loss = 0.09745288\n",
      "Iteration 179, loss = 0.09717940\n",
      "Iteration 180, loss = 0.09689656\n",
      "Iteration 65, loss = 0.16054611\n",
      "Iteration 181, loss = 0.09661599\n",
      "Iteration 182, loss = 0.09633619\n",
      "Iteration 183, loss = 0.09605091\n",
      "Iteration 66, loss = 0.15918682\n",
      "Iteration 184, loss = 0.09578408\n",
      "Iteration 67, loss = 0.15779304\n",
      "Iteration 185, loss = 0.09550878\n",
      "Iteration 68, loss = 0.15645677\n",
      "Iteration 186, loss = 0.09523878\n",
      "Iteration 187, loss = 0.09497814\n",
      "Iteration 69, loss = 0.15518939\n",
      "Iteration 188, loss = 0.09470682\n",
      "Iteration 70, loss = 0.15389018\n",
      "Iteration 71, loss = 0.15264477\n",
      "Iteration 189, loss = 0.09444395\n",
      "Iteration 190, loss = 0.09418371\n",
      "Iteration 72, loss = 0.15143510\n",
      "Iteration 191, loss = 0.09392308\n",
      "Iteration 192, loss = 0.09367741\n",
      "Iteration 193, loss = 0.09340680\n",
      "Iteration 194, loss = 0.09316047\n",
      "Iteration 144, loss = 0.20404600\n",
      "Iteration 195, loss = 0.09290253\n",
      "Iteration 150, loss = 0.18681321\n",
      "Iteration 295, loss = 0.21451481\n",
      "Iteration 53, loss = 0.35849594\n",
      "Iteration 73, loss = 0.15023125\n",
      "Iteration 196, loss = 0.09265521\n",
      "Iteration 291, loss = 0.12821647\n",
      "Iteration 74, loss = 0.14908235Iteration 197, loss = 0.09240267\n",
      "Iteration 198, loss = 0.09215637\n",
      "\n",
      "Iteration 199, loss = 0.09191930\n",
      "Iteration 200, loss = 0.09167463\n",
      "Iteration 75, loss = 0.14794428\n",
      "Iteration 201, loss = 0.09142341\n",
      "Iteration 235, loss = 0.15523162\n",
      "Iteration 76, loss = 0.14682471\n",
      "Iteration 202, loss = 0.09118756\n",
      "Iteration 203, loss = 0.09095413\n",
      "Iteration 77, loss = 0.14573317\n",
      "Iteration 204, loss = 0.09071197Iteration 78, loss = 0.14464655\n",
      "\n",
      "Iteration 79, loss = 0.14360326\n",
      "Iteration 205, loss = 0.09047435\n",
      "Iteration 80, loss = 0.14257933\n",
      "Iteration 151, loss = 0.18621331\n",
      "Iteration 206, loss = 0.09024320\n",
      "Iteration 81, loss = 0.14158535\n",
      "Iteration 145, loss = 0.20371865\n",
      "Iteration 207, loss = 0.09001855\n",
      "Iteration 208, loss = 0.08978840\n",
      "Iteration 82, loss = 0.14058327\n",
      "Iteration 209, loss = 0.08955290\n",
      "Iteration 292, loss = 0.12813207\n",
      "Iteration 210, loss = 0.08933126\n",
      "Iteration 211, loss = 0.08910723\n",
      "Iteration 212, loss = 0.08888161\n",
      "Iteration 213, loss = 0.08865745\n",
      "Iteration 236, loss = 0.15505424\n",
      "Iteration 83, loss = 0.13963638\n",
      "Iteration 152, loss = 0.18553817\n",
      "Iteration 84, loss = 0.13867798\n",
      "Iteration 85, loss = 0.13774863\n",
      "Iteration 214, loss = 0.08843688\n",
      "Iteration 86, loss = 0.13683307\n",
      "Iteration 293, loss = 0.12805813\n",
      "Iteration 87, loss = 0.13597785\n",
      "Iteration 215, loss = 0.08820394\n",
      "Iteration 146, loss = 0.20339480\n",
      "Iteration 216, loss = 0.08799100\n",
      "Iteration 88, loss = 0.13507453\n",
      "Iteration 54, loss = 0.35447341\n",
      "Iteration 217, loss = 0.08777758\n",
      "Iteration 218, loss = 0.08755869\n",
      "Iteration 296, loss = 0.21440110\n",
      "Iteration 219, loss = 0.08734690\n",
      "Iteration 89, loss = 0.13420491\n",
      "Iteration 220, loss = 0.08713729\n",
      "Iteration 90, loss = 0.13338305\n",
      "Iteration 294, loss = 0.12796242\n",
      "Iteration 91, loss = 0.13253288\n",
      "Iteration 92, loss = 0.13173439\n",
      "Iteration 93, loss = 0.13092716\n",
      "Iteration 153, loss = 0.18492866\n",
      "Iteration 94, loss = 0.13014524\n",
      "Iteration 221, loss = 0.08693900\n",
      "Iteration 95, loss = 0.12938612\n",
      "Iteration 96, loss = 0.12863078\n",
      "Iteration 222, loss = 0.08671307\n",
      "Iteration 223, loss = 0.08650366\n",
      "Iteration 224, loss = 0.08629166\n",
      "Iteration 295, loss = 0.12790075\n",
      "Iteration 225, loss = 0.08608316\n",
      "Iteration 237, loss = 0.15490008\n",
      "Iteration 226, loss = 0.08587192\n",
      "Iteration 97, loss = 0.12789384\n",
      "Iteration 227, loss = 0.08567895\n",
      "Iteration 228, loss = 0.08547482\n",
      "Iteration 229, loss = 0.08527624\n",
      "Iteration 98, loss = 0.12716011\n",
      "Iteration 99, loss = 0.12643497\n",
      "Iteration 230, loss = 0.08507386\n",
      "Iteration 55, loss = 0.35059304\n",
      "Iteration 147, loss = 0.20307276\n",
      "Iteration 296, loss = 0.12782166\n",
      "Iteration 100, loss = 0.12573144\n",
      "Iteration 231, loss = 0.08487571\n",
      "Iteration 297, loss = 0.12771618\n",
      "Iteration 232, loss = 0.08468642\n",
      "Iteration 101, loss = 0.12505282\n",
      "Iteration 233, loss = 0.08448996\n",
      "Iteration 102, loss = 0.12436824\n",
      "Iteration 154, loss = 0.18431117\n",
      "Iteration 234, loss = 0.08429055\n",
      "Iteration 103, loss = 0.12369100\n",
      "Iteration 235, loss = 0.08409959\n",
      "Iteration 297, loss = 0.21431203\n",
      "Iteration 236, loss = 0.08390942\n",
      "Iteration 104, loss = 0.12304661\n",
      "Iteration 237, loss = 0.08371712\n",
      "Iteration 238, loss = 0.08352519\n",
      "Iteration 239, loss = 0.08333789\n",
      "Iteration 105, loss = 0.12239135\n",
      "Iteration 238, loss = 0.15473245\n",
      "Iteration 240, loss = 0.08314580\n",
      "Iteration 106, loss = 0.12175112\n",
      "Iteration 241, loss = 0.08295973\n",
      "Iteration 107, loss = 0.12112152\n",
      "Iteration 56, loss = 0.34671627\n",
      "Iteration 155, loss = 0.18370959\n",
      "Iteration 108, loss = 0.12051683\n",
      "Iteration 242, loss = 0.08276974\n",
      "Iteration 109, loss = 0.11989979\n",
      "Iteration 243, loss = 0.08258767\n",
      "Iteration 110, loss = 0.11930124\n",
      "Iteration 148, loss = 0.20275165\n",
      "Iteration 244, loss = 0.08240868\n",
      "Iteration 245, loss = 0.08222123\n",
      "Iteration 111, loss = 0.11870867\n",
      "Iteration 246, loss = 0.08204092\n",
      "Iteration 247, loss = 0.08186237\n",
      "Iteration 248, loss = 0.08167830\n",
      "Iteration 298, loss = 0.12765031\n",
      "Iteration 112, loss = 0.11812142\n",
      "Iteration 239, loss = 0.15456933\n",
      "Iteration 249, loss = 0.08149676\n",
      "Iteration 250, loss = 0.08132064\n",
      "Iteration 251, loss = 0.08114645\n",
      "Iteration 113, loss = 0.11756570\n",
      "Iteration 299, loss = 0.12758876\n",
      "Iteration 252, loss = 0.08096606\n",
      "Iteration 156, loss = 0.18317751\n",
      "Iteration 114, loss = 0.11699101\n",
      "Iteration 115, loss = 0.11644808\n",
      "Iteration 298, loss = 0.21425585\n",
      "Iteration 116, loss = 0.11589226\n",
      "Iteration 253, loss = 0.08078786\n",
      "Iteration 117, loss = 0.11536270\n",
      "Iteration 254, loss = 0.08061918\n",
      "Iteration 118, loss = 0.11482125\n",
      "Iteration 255, loss = 0.08044535\n",
      "Iteration 157, loss = 0.18257515\n",
      "Iteration 256, loss = 0.08027008\n",
      "Iteration 119, loss = 0.11430682\n",
      "Iteration 120, loss = 0.11377798\n",
      "Iteration 121, loss = 0.11326759\n",
      "Iteration 257, loss = 0.08010620\n",
      "Iteration 122, loss = 0.11277106\n",
      "Iteration 258, loss = 0.07992644\n",
      "Iteration 259, loss = 0.07975870\n",
      "Iteration 57, loss = 0.34315397\n",
      "Iteration 123, loss = 0.11227945\n",
      "Iteration 300, loss = 0.12751141\n",
      "Iteration 260, loss = 0.07959376\n",
      "Iteration 261, loss = 0.07942543\n",
      "Iteration 158, loss = 0.18203650\n",
      "Iteration 262, loss = 0.07925665\n",
      "Iteration 240, loss = 0.15440533\n",
      "Iteration 263, loss = 0.07909604\n",
      "Iteration 264, loss = 0.07892557\n",
      "Iteration 265, loss = 0.07877162\n",
      "Iteration 266, loss = 0.07860409\n",
      "Iteration 149, loss = 0.20241685\n",
      "Iteration 124, loss = 0.11178242\n",
      "Iteration 125, loss = 0.11129102\n",
      "Iteration 267, loss = 0.07844132\n",
      "Iteration 126, loss = 0.11082329\n",
      "Iteration 299, loss = 0.21418946\n",
      "Iteration 268, loss = 0.07828461\n",
      "Iteration 127, loss = 0.11035717\n",
      "Iteration 128, loss = 0.10987603\n",
      "Iteration 129, loss = 0.10941984\n",
      "Iteration 130, loss = 0.10896450\n",
      "Iteration 131, loss = 0.10852387\n",
      "Iteration 301, loss = 0.12748811\n",
      "Iteration 269, loss = 0.07812155\n",
      "Iteration 270, loss = 0.07795906\n",
      "Iteration 132, loss = 0.10809312\n",
      "Iteration 271, loss = 0.07780731\n",
      "Iteration 133, loss = 0.10765014\n",
      "Iteration 134, loss = 0.10720649\n",
      "Iteration 159, loss = 0.18147951\n",
      "Iteration 135, loss = 0.10678518\n",
      "Iteration 241, loss = 0.15425946\n",
      "Iteration 272, loss = 0.07765302\n",
      "Iteration 136, loss = 0.10637500\n",
      "Iteration 273, loss = 0.07749170\n",
      "Iteration 137, loss = 0.10595289\n",
      "Iteration 138, loss = 0.10553987Iteration 274, loss = 0.07733752\n",
      "\n",
      "Iteration 302, loss = 0.12735205\n",
      "Iteration 139, loss = 0.10514033\n",
      "Iteration 275, loss = 0.07718650\n",
      "Iteration 276, loss = 0.07703382\n",
      "Iteration 140, loss = 0.10474513\n",
      "Iteration 277, loss = 0.07687903\n",
      "Iteration 278, loss = 0.07673244\n",
      "Iteration 58, loss = 0.33952400\n",
      "Iteration 279, loss = 0.07657715\n",
      "Iteration 141, loss = 0.10434566\n",
      "Iteration 280, loss = 0.07643403\n",
      "Iteration 303, loss = 0.12726525\n",
      "Iteration 281, loss = 0.07628915\n",
      "Iteration 142, loss = 0.10395723\n",
      "Iteration 150, loss = 0.20209714\n",
      "Iteration 242, loss = 0.15408924\n",
      "Iteration 160, loss = 0.18094860\n",
      "Iteration 282, loss = 0.07612840\n",
      "Iteration 283, loss = 0.07598613\n",
      "Iteration 284, loss = 0.07583926\n",
      "Iteration 285, loss = 0.07569305\n",
      "Iteration 286, loss = 0.07555371\n",
      "Iteration 287, loss = 0.07540678\n",
      "Iteration 143, loss = 0.10358215\n",
      "Iteration 304, loss = 0.12720640\n",
      "Iteration 288, loss = 0.07526870\n",
      "Iteration 151, loss = 0.20180942\n",
      "Iteration 243, loss = 0.15396261\n",
      "Iteration 289, loss = 0.07512592\n",
      "Iteration 290, loss = 0.07498546\n",
      "Iteration 291, loss = 0.07484224\n",
      "Iteration 292, loss = 0.07470465\n",
      "Iteration 293, loss = 0.07457012\n",
      "Iteration 294, loss = 0.07442767\n",
      "Iteration 295, loss = 0.07429280\n",
      "Iteration 59, loss = 0.33608940\n",
      "Iteration 144, loss = 0.10318611\n",
      "Iteration 296, loss = 0.07415418\n",
      "Iteration 297, loss = 0.07401904\n",
      "Iteration 161, loss = 0.18045238\n",
      "Iteration 145, loss = 0.10282120\n",
      "Iteration 146, loss = 0.10244613\n",
      "Iteration 300, loss = 0.21410489\n",
      "Iteration 298, loss = 0.07388008\n",
      "Iteration 147, loss = 0.10207324\n",
      "Iteration 148, loss = 0.10170713\n",
      "Iteration 149, loss = 0.10134806\n",
      "Iteration 150, loss = 0.10099359\n",
      "Iteration 299, loss = 0.07374450\n",
      "Iteration 151, loss = 0.10064233\n",
      "Iteration 300, loss = 0.07361645\n",
      "Iteration 301, loss = 0.07347925\n",
      "Iteration 162, loss = 0.17993513\n",
      "Iteration 302, loss = 0.07335199\n",
      "Iteration 305, loss = 0.12714621\n",
      "Iteration 303, loss = 0.07322915\n",
      "Iteration 304, loss = 0.07308793\n",
      "Iteration 152, loss = 0.10029190\n",
      "Iteration 153, loss = 0.09994733\n",
      "Iteration 154, loss = 0.09960717\n",
      "Iteration 155, loss = 0.09926890\n",
      "Iteration 305, loss = 0.07295935\n",
      "Iteration 306, loss = 0.07282793\n",
      "Iteration 156, loss = 0.09892810\n",
      "Iteration 157, loss = 0.09860290\n",
      "Iteration 307, loss = 0.07270145\n",
      "Iteration 158, loss = 0.09827495\n",
      "Iteration 306, loss = 0.12709880\n",
      "Iteration 159, loss = 0.09794730\n",
      "Iteration 244, loss = 0.15377806\n",
      "Iteration 160, loss = 0.09763083\n",
      "Iteration 308, loss = 0.07257324\n",
      "Iteration 60, loss = 0.33280513\n",
      "Iteration 161, loss = 0.09731232\n",
      "Iteration 309, loss = 0.07244473\n",
      "Iteration 301, loss = 0.21401804\n",
      "Iteration 310, loss = 0.07231849\n",
      "Iteration 311, loss = 0.07219579\n",
      "Iteration 163, loss = 0.17940910\n",
      "Iteration 162, loss = 0.09700469\n",
      "Iteration 163, loss = 0.09668681\n",
      "Iteration 312, loss = 0.07206628\n",
      "Iteration 313, loss = 0.07194178\n",
      "Iteration 314, loss = 0.07181874\n",
      "Iteration 315, loss = 0.07168757\n",
      "Iteration 316, loss = 0.07157095\n",
      "Iteration 307, loss = 0.12698192\n",
      "Iteration 317, loss = 0.07144957\n",
      "Iteration 318, loss = 0.07132360\n",
      "Iteration 152, loss = 0.20149790\n",
      "Iteration 319, loss = 0.07119780\n",
      "Iteration 320, loss = 0.07108256\n",
      "Iteration 164, loss = 0.17894008\n",
      "Iteration 164, loss = 0.09637896\n",
      "Iteration 321, loss = 0.07096248\n",
      "Iteration 165, loss = 0.09607009\n",
      "Iteration 166, loss = 0.09577085\n",
      "Iteration 167, loss = 0.09547380\n",
      "Iteration 245, loss = 0.15363623\n",
      "Iteration 168, loss = 0.09518163\n",
      "Iteration 169, loss = 0.09487862\n",
      "Iteration 322, loss = 0.07084560\n",
      "Iteration 308, loss = 0.12690699\n",
      "Iteration 323, loss = 0.07072341\n",
      "Iteration 324, loss = 0.07060756\n",
      "Iteration 170, loss = 0.09459034\n",
      "Iteration 171, loss = 0.09430289\n",
      "Iteration 325, loss = 0.07048442\n",
      "Iteration 153, loss = 0.20122009\n",
      "Iteration 172, loss = 0.09401690\n",
      "Iteration 173, loss = 0.09373938\n",
      "Iteration 326, loss = 0.07037708\n",
      "Iteration 165, loss = 0.17844205\n",
      "Iteration 327, loss = 0.07025623\n",
      "Iteration 328, loss = 0.07014554\n",
      "Iteration 174, loss = 0.09344900\n",
      "Iteration 329, loss = 0.07002910\n",
      "Iteration 61, loss = 0.32964068\n",
      "Iteration 175, loss = 0.09317215\n",
      "Iteration 309, loss = 0.12687149\n",
      "Iteration 302, loss = 0.21398795\n",
      "Iteration 330, loss = 0.06991368\n",
      "Iteration 176, loss = 0.09289642\n",
      "Iteration 246, loss = 0.15345822\n",
      "Iteration 177, loss = 0.09262662\n",
      "Iteration 331, loss = 0.06980537\n",
      "Iteration 166, loss = 0.17798801\n",
      "Iteration 332, loss = 0.06969305\n",
      "Iteration 310, loss = 0.12674954\n",
      "Iteration 333, loss = 0.06958426\n",
      "Iteration 178, loss = 0.09235411\n",
      "Iteration 334, loss = 0.06947362\n",
      "Iteration 179, loss = 0.09208927\n",
      "Iteration 335, loss = 0.06935833\n",
      "Iteration 180, loss = 0.09182941\n",
      "Iteration 336, loss = 0.06925348\n",
      "Iteration 337, loss = 0.06913904\n",
      "Iteration 338, loss = 0.06902982\n",
      "Iteration 339, loss = 0.06891972\n",
      "Iteration 303, loss = 0.21388076\n",
      "Iteration 154, loss = 0.20089554\n",
      "Iteration 181, loss = 0.09156681\n",
      "Iteration 340, loss = 0.06881107\n",
      "Iteration 341, loss = 0.06871163\n",
      "Iteration 247, loss = 0.15336877\n",
      "Iteration 342, loss = 0.06859841\n",
      "Iteration 311, loss = 0.12669085\n",
      "Iteration 343, loss = 0.06849612\n",
      "Iteration 167, loss = 0.17750298\n",
      "Iteration 344, loss = 0.06838404\n",
      "Iteration 345, loss = 0.06827841\n",
      "Iteration 346, loss = 0.06817004\n",
      "Iteration 347, loss = 0.06806872\n",
      "Iteration 182, loss = 0.09130204\n",
      "Iteration 348, loss = 0.06796675\n",
      "Iteration 183, loss = 0.09105361\n",
      "Iteration 349, loss = 0.06786441\n",
      "Iteration 184, loss = 0.09078926\n",
      "Iteration 185, loss = 0.09053228\n",
      "Iteration 186, loss = 0.09028492\n",
      "Iteration 187, loss = 0.09003711\n",
      "Iteration 312, loss = 0.12664265\n",
      "Iteration 168, loss = 0.17704534\n",
      "Iteration 350, loss = 0.06775565Iteration 188, loss = 0.08980218\n",
      "\n",
      "Iteration 351, loss = 0.06765837\n",
      "Iteration 189, loss = 0.08955247\n",
      "Iteration 62, loss = 0.32662928\n",
      "Iteration 190, loss = 0.08930627\n",
      "Iteration 352, loss = 0.06755148\n",
      "Iteration 191, loss = 0.08905727\n",
      "Iteration 353, loss = 0.06745136\n",
      "Iteration 354, loss = 0.06735005\n",
      "Iteration 355, loss = 0.06724505\n",
      "Iteration 192, loss = 0.08882230\n",
      "Iteration 356, loss = 0.06714822\n",
      "Iteration 357, loss = 0.06705306\n",
      "Iteration 358, loss = 0.06694838\n",
      "Iteration 359, loss = 0.06684873\n",
      "Iteration 193, loss = 0.08858613\n",
      "Iteration 360, loss = 0.06675261\n",
      "Iteration 361, loss = 0.06665422\n",
      "Iteration 362, loss = 0.06656816\n",
      "Iteration 194, loss = 0.08835950\n",
      "Iteration 363, loss = 0.06645806\n",
      "Iteration 364, loss = 0.06635808\n",
      "Iteration 365, loss = 0.06627200\n",
      "Iteration 366, loss = 0.06616956\n",
      "Iteration 313, loss = 0.12655730\n",
      "Iteration 248, loss = 0.15319658\n",
      "Iteration 195, loss = 0.08812368\n",
      "Iteration 196, loss = 0.08788997\n",
      "Iteration 367, loss = 0.06607521\n",
      "Iteration 314, loss = 0.12648308\n",
      "Iteration 197, loss = 0.08766042\n",
      "Iteration 169, loss = 0.17661261\n",
      "Iteration 249, loss = 0.15303591\n",
      "Iteration 368, loss = 0.06598003\n",
      "Iteration 369, loss = 0.06588005\n",
      "Iteration 370, loss = 0.06578756\n",
      "Iteration 155, loss = 0.20063538\n",
      "Iteration 198, loss = 0.08743323\n",
      "Iteration 199, loss = 0.08721166\n",
      "Iteration 304, loss = 0.21381381\n",
      "Iteration 200, loss = 0.08698226\n",
      "Iteration 201, loss = 0.08676685\n",
      "Iteration 63, loss = 0.32365339\n",
      "Iteration 202, loss = 0.08654653\n",
      "Iteration 371, loss = 0.06569812\n",
      "Iteration 372, loss = 0.06560890\n",
      "Iteration 373, loss = 0.06551298\n",
      "Iteration 315, loss = 0.12646238\n",
      "Iteration 374, loss = 0.06541717\n",
      "Iteration 203, loss = 0.08632496\n",
      "Iteration 375, loss = 0.06532446\n",
      "Iteration 376, loss = 0.06523977\n",
      "Iteration 377, loss = 0.06514505\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 204, loss = 0.08611620\n",
      "Iteration 205, loss = 0.08589730\n",
      "Iteration 156, loss = 0.20031317\n",
      "Iteration 206, loss = 0.08568832\n",
      "Iteration 1, loss = 0.70446355\n",
      "Iteration 170, loss = 0.17612465Iteration 207, loss = 0.08547175\n",
      "\n",
      "Iteration 250, loss = 0.15289599\n",
      "Iteration 316, loss = 0.12636254\n",
      "Iteration 208, loss = 0.08526556\n",
      "Iteration 171, loss = 0.17574787\n",
      "Iteration 64, loss = 0.32092569\n",
      "Iteration 251, loss = 0.15274383\n",
      "Iteration 209, loss = 0.08505721\n",
      "Iteration 2, loss = 0.67257038\n",
      "Iteration 210, loss = 0.08485385\n",
      "Iteration 317, loss = 0.12628005\n",
      "Iteration 3, loss = 0.63137904\n",
      "Iteration 4, loss = 0.58922083\n",
      "Iteration 172, loss = 0.17529195\n",
      "Iteration 5, loss = 0.55084349\n",
      "Iteration 6, loss = 0.51741627\n",
      "Iteration 7, loss = 0.48889379\n",
      "Iteration 305, loss = 0.21369197\n",
      "Iteration 8, loss = 0.46439928\n",
      "Iteration 9, loss = 0.44445032\n",
      "Iteration 10, loss = 0.42642107Iteration 211, loss = 0.08464584\n",
      "Iteration 212, loss = 0.08444639\n",
      "Iteration 213, loss = 0.08424306\n",
      "Iteration 214, loss = 0.08404193\n",
      "\n",
      "Iteration 215, loss = 0.08384267\n",
      "Iteration 11, loss = 0.41091344\n",
      "Iteration 216, loss = 0.08364824\n",
      "Iteration 12, loss = 0.39749145\n",
      "Iteration 318, loss = 0.12621201\n",
      "Iteration 217, loss = 0.08345403\n",
      "Iteration 157, loss = 0.20003946\n",
      "Iteration 13, loss = 0.38526304\n",
      "Iteration 173, loss = 0.17490434\n",
      "Iteration 14, loss = 0.37453166\n",
      "Iteration 218, loss = 0.08325811\n",
      "Iteration 219, loss = 0.08306436\n",
      "Iteration 15, loss = 0.36450282\n",
      "Iteration 16, loss = 0.35532507\n",
      "Iteration 220, loss = 0.08287566\n",
      "Iteration 17, loss = 0.34690263\n",
      "Iteration 221, loss = 0.08267929\n",
      "Iteration 18, loss = 0.33903407\n",
      "Iteration 222, loss = 0.08249385\n",
      "Iteration 19, loss = 0.33153231\n",
      "Iteration 319, loss = 0.12614941\n",
      "Iteration 174, loss = 0.17441782\n",
      "Iteration 223, loss = 0.08231053\n",
      "Iteration 224, loss = 0.08212860\n",
      "Iteration 20, loss = 0.32446458\n",
      "Iteration 306, loss = 0.21363782\n",
      "Iteration 225, loss = 0.08193183\n",
      "Iteration 21, loss = 0.31795152\n",
      "Iteration 226, loss = 0.08175063\n",
      "Iteration 227, loss = 0.08156700\n",
      "Iteration 252, loss = 0.15259509\n",
      "Iteration 65, loss = 0.31822658\n",
      "Iteration 228, loss = 0.08139152\n",
      "Iteration 22, loss = 0.31151338\n",
      "Iteration 23, loss = 0.30542478\n",
      "Iteration 24, loss = 0.29967753\n",
      "Iteration 229, loss = 0.08120983\n",
      "Iteration 25, loss = 0.29388610\n",
      "Iteration 320, loss = 0.12608736\n",
      "Iteration 158, loss = 0.19973526\n",
      "Iteration 253, loss = 0.15249038\n",
      "Iteration 230, loss = 0.08103032\n",
      "Iteration 26, loss = 0.28868228\n",
      "Iteration 231, loss = 0.08085512\n",
      "Iteration 27, loss = 0.28342825\n",
      "Iteration 175, loss = 0.17399902\n",
      "Iteration 28, loss = 0.27834693\n",
      "Iteration 254, loss = 0.15231790\n",
      "Iteration 232, loss = 0.08067912\n",
      "Iteration 321, loss = 0.12609893\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 29, loss = 0.27360336\n",
      "Iteration 233, loss = 0.08049952\n",
      "Iteration 234, loss = 0.08033301\n",
      "Iteration 235, loss = 0.08015299\n",
      "Iteration 30, loss = 0.26901813\n",
      "Iteration 31, loss = 0.26435621\n",
      "Iteration 236, loss = 0.07998661\n",
      "Iteration 32, loss = 0.25993395\n",
      "Iteration 237, loss = 0.07981841\n",
      "Iteration 33, loss = 0.25574749\n",
      "Iteration 238, loss = 0.07964541\n",
      "Iteration 34, loss = 0.25158562\n",
      "Iteration 35, loss = 0.24765262\n",
      "Iteration 239, loss = 0.07947923\n",
      "Iteration 307, loss = 0.21356815\n",
      "Iteration 36, loss = 0.24367344\n",
      "Iteration 240, loss = 0.07930717\n",
      "Iteration 66, loss = 0.31564360\n",
      "Iteration 241, loss = 0.07914376\n",
      "Iteration 1, loss = 0.57049935\n",
      "Iteration 37, loss = 0.23989310\n",
      "Iteration 159, loss = 0.19948058\n",
      "Iteration 2, loss = 0.55697139\n",
      "Iteration 176, loss = 0.17363140\n",
      "Iteration 3, loss = 0.53870529\n",
      "Iteration 242, loss = 0.07898642\n",
      "Iteration 4, loss = 0.51895765\n",
      "Iteration 5, loss = 0.49959305\n",
      "Iteration 6, loss = 0.48144088\n",
      "Iteration 7, loss = 0.46468939\n",
      "Iteration 243, loss = 0.07881534\n",
      "Iteration 244, loss = 0.07865107\n",
      "Iteration 38, loss = 0.23637940\n",
      "Iteration 245, loss = 0.07848616\n",
      "Iteration 39, loss = 0.23267443\n",
      "Iteration 255, loss = 0.15217639\n",
      "Iteration 40, loss = 0.22930869\n",
      "Iteration 246, loss = 0.07832886\n",
      "Iteration 41, loss = 0.22595957\n",
      "Iteration 247, loss = 0.07817615\n",
      "Iteration 248, loss = 0.07800857\n",
      "Iteration 249, loss = 0.07784616\n",
      "Iteration 177, loss = 0.17322303\n",
      "Iteration 250, loss = 0.07769267\n",
      "Iteration 251, loss = 0.07753799\n",
      "Iteration 8, loss = 0.44940395\n",
      "Iteration 42, loss = 0.22270507\n",
      "Iteration 160, loss = 0.19919822\n",
      "Iteration 9, loss = 0.43515716\n",
      "Iteration 43, loss = 0.21942151\n",
      "Iteration 44, loss = 0.21640217\n",
      "Iteration 45, loss = 0.21337747\n",
      "Iteration 46, loss = 0.21038191\n",
      "Iteration 10, loss = 0.42235239\n",
      "Iteration 47, loss = 0.20757224\n",
      "Iteration 48, loss = 0.20483688\n",
      "Iteration 49, loss = 0.20204822\n",
      "Iteration 178, loss = 0.17282532\n",
      "Iteration 50, loss = 0.19947068\n",
      "Iteration 252, loss = 0.07738321\n",
      "Iteration 253, loss = 0.07723103\n",
      "Iteration 51, loss = 0.19687736\n",
      "Iteration 11, loss = 0.41010962\n",
      "Iteration 254, loss = 0.07707282\n",
      "Iteration 12, loss = 0.39880232\n",
      "Iteration 52, loss = 0.19441569\n",
      "Iteration 13, loss = 0.38834274\n",
      "Iteration 53, loss = 0.19201340\n",
      "Iteration 255, loss = 0.07692263\n",
      "Iteration 256, loss = 0.15206257\n",
      "Iteration 256, loss = 0.07677043\n",
      "Iteration 257, loss = 0.07661792\n",
      "Iteration 14, loss = 0.37836483\n",
      "Iteration 258, loss = 0.07646424\n",
      "Iteration 54, loss = 0.18962914\n",
      "Iteration 55, loss = 0.18734872\n",
      "Iteration 15, loss = 0.36887796\n",
      "Iteration 16, loss = 0.35996108\n",
      "Iteration 56, loss = 0.18504061\n",
      "Iteration 57, loss = 0.18284366\n",
      "Iteration 259, loss = 0.07631669\n",
      "Iteration 17, loss = 0.35145268\n",
      "Iteration 67, loss = 0.31314560\n",
      "Iteration 260, loss = 0.07616953\n",
      "Iteration 18, loss = 0.34331940\n",
      "Iteration 58, loss = 0.18074375\n",
      "Iteration 19, loss = 0.33551786\n",
      "Iteration 179, loss = 0.17244024\n",
      "Iteration 308, loss = 0.21349764\n",
      "Iteration 261, loss = 0.07602553\n",
      "Iteration 262, loss = 0.07588161\n",
      "Iteration 161, loss = 0.19889499\n",
      "Iteration 20, loss = 0.32790330\n",
      "Iteration 263, loss = 0.07573209\n",
      "Iteration 264, loss = 0.07558444\n",
      "Iteration 59, loss = 0.17865898\n",
      "Iteration 60, loss = 0.17661278\n",
      "Iteration 257, loss = 0.15189198\n",
      "Iteration 265, loss = 0.07544044\n",
      "Iteration 61, loss = 0.17464798\n",
      "Iteration 62, loss = 0.17269060\n",
      "Iteration 63, loss = 0.17086951\n",
      "Iteration 21, loss = 0.32088429\n",
      "Iteration 180, loss = 0.17213253\n",
      "Iteration 22, loss = 0.31388118\n",
      "Iteration 64, loss = 0.16899360\n",
      "Iteration 65, loss = 0.16717737\n",
      "Iteration 23, loss = 0.30719759\n",
      "Iteration 266, loss = 0.07530419\n",
      "Iteration 66, loss = 0.16547741\n",
      "Iteration 24, loss = 0.30072875\n",
      "Iteration 67, loss = 0.16379234\n",
      "Iteration 68, loss = 0.16210890\n",
      "Iteration 69, loss = 0.16054668\n",
      "Iteration 25, loss = 0.29442542\n",
      "Iteration 258, loss = 0.15176191\n",
      "Iteration 26, loss = 0.28834195\n",
      "Iteration 267, loss = 0.07515101\n",
      "Iteration 68, loss = 0.31082240\n",
      "Iteration 70, loss = 0.15893830\n",
      "Iteration 268, loss = 0.07501422\n",
      "Iteration 71, loss = 0.15739328\n",
      "Iteration 72, loss = 0.15592573\n",
      "Iteration 269, loss = 0.07487367\n",
      "Iteration 27, loss = 0.28246713\n",
      "Iteration 270, loss = 0.07473475\n",
      "Iteration 271, loss = 0.07459612\n",
      "Iteration 162, loss = 0.19866660\n",
      "Iteration 309, loss = 0.21338804\n",
      "Iteration 272, loss = 0.07445598\n",
      "Iteration 73, loss = 0.15448118\n",
      "Iteration 28, loss = 0.27688709\n",
      "Iteration 74, loss = 0.15299383\n",
      "Iteration 181, loss = 0.17169515\n",
      "Iteration 273, loss = 0.07431944\n",
      "Iteration 75, loss = 0.15160918\n",
      "Iteration 274, loss = 0.07417752\n",
      "Iteration 275, loss = 0.07404510\n",
      "Iteration 29, loss = 0.27145775\n",
      "Iteration 30, loss = 0.26608788\n",
      "Iteration 76, loss = 0.15024084\n",
      "Iteration 276, loss = 0.07391287\n",
      "Iteration 77, loss = 0.14888313\n",
      "Iteration 277, loss = 0.07378700\n",
      "Iteration 278, loss = 0.07364312\n",
      "Iteration 78, loss = 0.14759561\n",
      "Iteration 31, loss = 0.26108678\n",
      "Iteration 182, loss = 0.17132544\n",
      "Iteration 79, loss = 0.14635147\n",
      "Iteration 32, loss = 0.25619546\n",
      "Iteration 80, loss = 0.14504744\n",
      "Iteration 33, loss = 0.25137020\n",
      "Iteration 81, loss = 0.14384287\n",
      "Iteration 34, loss = 0.24677808\n",
      "Iteration 163, loss = 0.19840078\n",
      "Iteration 35, loss = 0.24225540\n",
      "Iteration 279, loss = 0.07351096\n",
      "Iteration 259, loss = 0.15162886\n",
      "Iteration 280, loss = 0.07338837\n",
      "Iteration 281, loss = 0.07324636\n",
      "Iteration 282, loss = 0.07311957\n",
      "Iteration 82, loss = 0.14264072\n",
      "Iteration 283, loss = 0.07298706\n",
      "Iteration 284, loss = 0.07286257\n",
      "Iteration 83, loss = 0.14146189\n",
      "Iteration 36, loss = 0.23803624\n",
      "Iteration 84, loss = 0.14034989\n",
      "Iteration 183, loss = 0.17101476\n",
      "Iteration 85, loss = 0.13920273\n",
      "Iteration 285, loss = 0.07273406\n",
      "Iteration 310, loss = 0.21341684\n",
      "Iteration 37, loss = 0.23371740\n",
      "Iteration 86, loss = 0.13808007\n",
      "Iteration 38, loss = 0.22972053\n",
      "Iteration 260, loss = 0.15152016\n",
      "Iteration 87, loss = 0.13701313\n",
      "Iteration 39, loss = 0.22594083\n",
      "Iteration 88, loss = 0.13596865\n",
      "Iteration 89, loss = 0.13490329\n",
      "Iteration 40, loss = 0.22213576\n",
      "Iteration 90, loss = 0.13388268\n",
      "Iteration 41, loss = 0.21851425\n",
      "Iteration 69, loss = 0.30851858\n",
      "Iteration 286, loss = 0.07260944\n",
      "Iteration 287, loss = 0.07248647\n",
      "Iteration 42, loss = 0.21496916\n",
      "Iteration 43, loss = 0.21158206\n",
      "Iteration 44, loss = 0.20840678\n",
      "Iteration 91, loss = 0.13292967\n",
      "Iteration 288, loss = 0.07235059\n",
      "Iteration 92, loss = 0.13195122\n",
      "Iteration 93, loss = 0.13097922\n",
      "Iteration 94, loss = 0.13002494\n",
      "Iteration 289, loss = 0.07222649\n",
      "Iteration 290, loss = 0.07210258\n",
      "Iteration 291, loss = 0.07198039\n",
      "Iteration 95, loss = 0.12911872\n",
      "Iteration 184, loss = 0.17066664\n",
      "Iteration 292, loss = 0.07185640\n",
      "Iteration 96, loss = 0.12821919\n",
      "Iteration 293, loss = 0.07172929\n",
      "Iteration 294, loss = 0.07160824\n",
      "Iteration 295, loss = 0.07148546\n",
      "Iteration 45, loss = 0.20509318\n",
      "Iteration 46, loss = 0.20196334\n",
      "Iteration 97, loss = 0.12733330\n",
      "Iteration 70, loss = 0.30626376\n",
      "Iteration 98, loss = 0.12647951\n",
      "Iteration 99, loss = 0.12560431\n",
      "Iteration 100, loss = 0.12476788\n",
      "Iteration 185, loss = 0.17026994\n",
      "Iteration 47, loss = 0.19905389\n",
      "Iteration 101, loss = 0.12395788\n",
      "Iteration 48, loss = 0.19616383\n",
      "Iteration 102, loss = 0.12313794\n",
      "Iteration 49, loss = 0.19342742\n",
      "Iteration 103, loss = 0.12234797\n",
      "Iteration 104, loss = 0.12157136\n",
      "Iteration 105, loss = 0.12078733\n",
      "Iteration 50, loss = 0.19078644\n",
      "Iteration 106, loss = 0.12003612\n",
      "Iteration 51, loss = 0.18810625\n",
      "Iteration 296, loss = 0.07136431\n",
      "Iteration 164, loss = 0.19809548\n",
      "Iteration 297, loss = 0.07124596\n",
      "Iteration 298, loss = 0.07112174\n",
      "Iteration 299, loss = 0.07100982\n",
      "Iteration 52, loss = 0.18564681\n",
      "Iteration 261, loss = 0.15136240\n",
      "Iteration 107, loss = 0.11928406\n",
      "Iteration 311, loss = 0.21327425\n",
      "Iteration 300, loss = 0.07088306\n",
      "Iteration 301, loss = 0.07076585\n",
      "Iteration 53, loss = 0.18321316\n",
      "Iteration 302, loss = 0.07064625\n",
      "Iteration 108, loss = 0.11854663\n",
      "Iteration 303, loss = 0.07053500\n",
      "Iteration 186, loss = 0.16993089\n",
      "Iteration 262, loss = 0.15124428\n",
      "Iteration 54, loss = 0.18087705\n",
      "Iteration 304, loss = 0.07041858\n",
      "Iteration 305, loss = 0.07030018\n",
      "Iteration 306, loss = 0.07018674\n",
      "Iteration 109, loss = 0.11785121\n",
      "Iteration 307, loss = 0.07006911\n",
      "Iteration 308, loss = 0.06995380\n",
      "Iteration 309, loss = 0.06985215\n",
      "Iteration 110, loss = 0.11712094\n",
      "Iteration 55, loss = 0.17862533\n",
      "Iteration 165, loss = 0.19786571\n",
      "Iteration 310, loss = 0.06973030\n",
      "Iteration 111, loss = 0.11647074\n",
      "Iteration 71, loss = 0.30419798\n",
      "Iteration 112, loss = 0.11573119\n",
      "Iteration 311, loss = 0.06961779\n",
      "Iteration 312, loss = 0.06950372\n",
      "Iteration 313, loss = 0.06939162\n",
      "Iteration 113, loss = 0.11508503\n",
      "Iteration 314, loss = 0.06927982\n",
      "Iteration 263, loss = 0.15112260\n",
      "Iteration 114, loss = 0.11439605\n",
      "Iteration 56, loss = 0.17639627\n",
      "Iteration 187, loss = 0.16959967\n",
      "Iteration 315, loss = 0.06916864\n",
      "Iteration 115, loss = 0.11374824\n",
      "Iteration 116, loss = 0.11311788\n",
      "Iteration 57, loss = 0.17426469\n",
      "Iteration 117, loss = 0.11247131\n",
      "Iteration 312, loss = 0.21318809\n",
      "Iteration 118, loss = 0.11185541\n",
      "Iteration 316, loss = 0.06906138\n",
      "Iteration 58, loss = 0.17220125\n",
      "Iteration 119, loss = 0.11124765\n",
      "Iteration 59, loss = 0.17016829\n",
      "Iteration 120, loss = 0.11065750\n",
      "Iteration 60, loss = 0.16821250\n",
      "Iteration 121, loss = 0.11003478\n",
      "Iteration 122, loss = 0.10945011\n",
      "Iteration 317, loss = 0.06894973\n",
      "Iteration 61, loss = 0.16633489\n",
      "Iteration 264, loss = 0.15097970\n",
      "Iteration 318, loss = 0.06884160\n",
      "Iteration 62, loss = 0.16442613\n",
      "Iteration 123, loss = 0.10888328\n",
      "Iteration 63, loss = 0.16266791\n",
      "Iteration 124, loss = 0.10830217\n",
      "Iteration 319, loss = 0.06872938\n",
      "Iteration 125, loss = 0.10774482\n",
      "Iteration 320, loss = 0.06862397\n",
      "Iteration 126, loss = 0.10721894\n",
      "Iteration 188, loss = 0.16931634\n",
      "Iteration 321, loss = 0.06851748\n",
      "Iteration 127, loss = 0.10665034\n",
      "Iteration 322, loss = 0.06840849\n",
      "Iteration 128, loss = 0.10611218\n",
      "Iteration 323, loss = 0.06830023\n",
      "Iteration 129, loss = 0.10557651\n",
      "Iteration 324, loss = 0.06819498\n",
      "Iteration 325, loss = 0.06809884\n",
      "Iteration 326, loss = 0.06798115\n",
      "Iteration 64, loss = 0.16088703\n",
      "Iteration 327, loss = 0.06787678\n",
      "Iteration 328, loss = 0.06777681\n",
      "Iteration 65, loss = 0.15921921\n",
      "Iteration 166, loss = 0.19760211Iteration 66, loss = 0.15753664\n",
      "\n",
      "Iteration 130, loss = 0.10504991\n",
      "Iteration 131, loss = 0.10454069\n",
      "Iteration 67, loss = 0.15593647\n",
      "Iteration 132, loss = 0.10402917\n",
      "Iteration 133, loss = 0.10353692\n",
      "Iteration 329, loss = 0.06767140\n",
      "Iteration 68, loss = 0.15441335\n",
      "Iteration 72, loss = 0.30213676\n",
      "Iteration 189, loss = 0.16894597\n",
      "Iteration 134, loss = 0.10302685\n",
      "Iteration 69, loss = 0.15287750\n",
      "Iteration 330, loss = 0.06757259\n",
      "Iteration 313, loss = 0.21318992\n",
      "Iteration 331, loss = 0.06746548\n",
      "Iteration 135, loss = 0.10253906\n",
      "Iteration 70, loss = 0.15142190\n",
      "Iteration 136, loss = 0.10205478\n",
      "Iteration 332, loss = 0.06736705\n",
      "Iteration 265, loss = 0.15085735\n",
      "Iteration 333, loss = 0.06726297\n",
      "Iteration 334, loss = 0.06716662\n",
      "Iteration 335, loss = 0.06706596\n",
      "Iteration 336, loss = 0.06697184\n",
      "Iteration 137, loss = 0.10158366\n",
      "Iteration 167, loss = 0.19734316\n",
      "Iteration 138, loss = 0.10110693\n",
      "Iteration 190, loss = 0.16864717\n",
      "Iteration 139, loss = 0.10065943\n",
      "Iteration 71, loss = 0.14999992\n",
      "Iteration 140, loss = 0.10021370\n",
      "Iteration 141, loss = 0.09973772\n",
      "Iteration 72, loss = 0.14863074\n",
      "Iteration 142, loss = 0.09928139\n",
      "Iteration 143, loss = 0.09884545\n",
      "Iteration 337, loss = 0.06686087\n",
      "Iteration 73, loss = 0.14723693\n",
      "Iteration 144, loss = 0.09842638\n",
      "Iteration 145, loss = 0.09798698\n",
      "Iteration 74, loss = 0.14593480\n",
      "Iteration 338, loss = 0.06676492\n",
      "Iteration 146, loss = 0.09755832\n",
      "Iteration 339, loss = 0.06666705\n",
      "Iteration 340, loss = 0.06657294\n",
      "Iteration 75, loss = 0.14466662\n",
      "Iteration 341, loss = 0.06647274\n",
      "Iteration 266, loss = 0.15074689\n",
      "Iteration 342, loss = 0.06637803\n",
      "Iteration 343, loss = 0.06628143\n",
      "Iteration 147, loss = 0.09713284\n",
      "Iteration 76, loss = 0.14341993\n",
      "Iteration 344, loss = 0.06619008\n",
      "Iteration 77, loss = 0.14220351\n",
      "Iteration 78, loss = 0.14106551\n",
      "Iteration 73, loss = 0.30007954\n",
      "Iteration 314, loss = 0.21302593\n",
      "Iteration 79, loss = 0.13988027\n",
      "Iteration 345, loss = 0.06609306\n",
      "Iteration 346, loss = 0.06599222\n",
      "Iteration 148, loss = 0.09670742\n",
      "Iteration 347, loss = 0.06590337\n",
      "Iteration 149, loss = 0.09629152Iteration 80, loss = 0.13877916\n",
      "Iteration 348, loss = 0.06580463\n",
      "Iteration 349, loss = 0.06570992\n",
      "\n",
      "Iteration 350, loss = 0.06561914\n",
      "Iteration 351, loss = 0.06552714\n",
      "Iteration 352, loss = 0.06542796\n",
      "Iteration 150, loss = 0.09589219\n",
      "Iteration 191, loss = 0.16833904\n",
      "Iteration 81, loss = 0.13768827\n",
      "Iteration 168, loss = 0.19707882\n",
      "Iteration 151, loss = 0.09548693\n",
      "Iteration 267, loss = 0.15059909\n",
      "Iteration 82, loss = 0.13661292\n",
      "Iteration 353, loss = 0.06534370\n",
      "Iteration 354, loss = 0.06524968\n",
      "Iteration 355, loss = 0.06515546\n",
      "Iteration 152, loss = 0.09508474\n",
      "Iteration 192, loss = 0.16802198\n",
      "Iteration 153, loss = 0.09469063\n",
      "Iteration 356, loss = 0.06507128\n",
      "Iteration 357, loss = 0.06497583\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 83, loss = 0.13561447\n",
      "Iteration 74, loss = 0.29816980\n",
      "Iteration 84, loss = 0.13456516\n",
      "Iteration 154, loss = 0.09429698\n",
      "Iteration 85, loss = 0.13360334\n",
      "Iteration 1, loss = 0.74607433\n",
      "Iteration 86, loss = 0.13260999\n",
      "Iteration 155, loss = 0.09391182\n",
      "Iteration 156, loss = 0.09353631\n",
      "Iteration 157, loss = 0.09316169\n",
      "Iteration 87, loss = 0.13168906\n",
      "Iteration 193, loss = 0.16773339\n",
      "Iteration 88, loss = 0.13078757\n",
      "Iteration 2, loss = 0.71022545\n",
      "Iteration 158, loss = 0.09277792\n",
      "Iteration 89, loss = 0.12985772\n",
      "Iteration 159, loss = 0.09241660\n",
      "Iteration 160, loss = 0.09205426\n",
      "Iteration 169, loss = 0.19684874\n",
      "Iteration 161, loss = 0.09169540\n",
      "Iteration 162, loss = 0.09132799\n",
      "Iteration 163, loss = 0.09096989\n",
      "Iteration 164, loss = 0.09062835\n",
      "Iteration 90, loss = 0.12899133\n",
      "Iteration 268, loss = 0.15051347\n",
      "Iteration 165, loss = 0.09028044\n",
      "Iteration 91, loss = 0.12811145\n",
      "Iteration 3, loss = 0.66377237\n",
      "Iteration 166, loss = 0.08995250\n",
      "Iteration 92, loss = 0.12726839\n",
      "Iteration 315, loss = 0.21297318\n",
      "Iteration 167, loss = 0.08959358\n",
      "Iteration 168, loss = 0.08925884\n",
      "Iteration 169, loss = 0.08893099\n",
      "Iteration 170, loss = 0.08861203\n",
      "Iteration 194, loss = 0.16741709\n",
      "Iteration 93, loss = 0.12644573\n",
      "Iteration 4, loss = 0.61631068\n",
      "Iteration 171, loss = 0.08828360\n",
      "Iteration 269, loss = 0.15037651\n",
      "Iteration 5, loss = 0.57391879\n",
      "Iteration 94, loss = 0.12562174\n",
      "Iteration 6, loss = 0.53692377\n",
      "Iteration 172, loss = 0.08795441\n",
      "Iteration 95, loss = 0.12482109\n",
      "Iteration 7, loss = 0.50602188\n",
      "Iteration 96, loss = 0.12403479\n",
      "Iteration 173, loss = 0.08764059\n",
      "Iteration 97, loss = 0.12325763\n",
      "Iteration 170, loss = 0.19657109\n",
      "Iteration 174, loss = 0.08731451\n",
      "Iteration 75, loss = 0.29630918\n",
      "Iteration 98, loss = 0.12250601\n",
      "Iteration 175, loss = 0.08700625\n",
      "Iteration 176, loss = 0.08669485\n",
      "Iteration 316, loss = 0.21289128\n",
      "Iteration 177, loss = 0.08639593\n",
      "Iteration 178, loss = 0.08609171\n",
      "Iteration 195, loss = 0.16712999\n",
      "Iteration 99, loss = 0.12175429\n",
      "Iteration 179, loss = 0.08578356\n",
      "Iteration 100, loss = 0.12102340\n",
      "Iteration 180, loss = 0.08548501\n",
      "Iteration 181, loss = 0.08519299\n",
      "Iteration 8, loss = 0.47922463\n",
      "Iteration 182, loss = 0.08490413\n",
      "Iteration 183, loss = 0.08462932\n",
      "Iteration 9, loss = 0.45656059\n",
      "Iteration 10, loss = 0.43764298\n",
      "Iteration 270, loss = 0.15024903\n",
      "Iteration 184, loss = 0.08432555\n",
      "Iteration 101, loss = 0.12034245\n",
      "Iteration 102, loss = 0.11962212\n",
      "Iteration 11, loss = 0.42081639\n",
      "Iteration 103, loss = 0.11891356\n",
      "Iteration 104, loss = 0.11822861\n",
      "Iteration 185, loss = 0.08404493\n",
      "Iteration 12, loss = 0.40632923\n",
      "Iteration 196, loss = 0.16686153\n",
      "Iteration 13, loss = 0.39361676\n",
      "Iteration 14, loss = 0.38208030Iteration 271, loss = 0.15013105\n",
      "\n",
      "Iteration 171, loss = 0.19637277\n",
      "Iteration 186, loss = 0.08375281\n",
      "Iteration 15, loss = 0.37134195\n",
      "Iteration 105, loss = 0.11757500\n",
      "Iteration 16, loss = 0.36172140\n",
      "Iteration 317, loss = 0.21285902\n",
      "Iteration 76, loss = 0.29446521\n",
      "Iteration 197, loss = 0.16658355\n",
      "Iteration 17, loss = 0.35292159Iteration 187, loss = 0.08346958\n",
      "Iteration 106, loss = 0.11691731\n",
      "\n",
      "Iteration 188, loss = 0.08319684\n",
      "Iteration 107, loss = 0.11629368\n",
      "Iteration 18, loss = 0.34453784\n",
      "Iteration 108, loss = 0.11564875\n",
      "Iteration 189, loss = 0.08292201\n",
      "Iteration 19, loss = 0.33676133\n",
      "Iteration 190, loss = 0.08265660\n",
      "Iteration 191, loss = 0.08237838\n",
      "Iteration 192, loss = 0.08212138Iteration 20, loss = 0.32936151\n",
      "\n",
      "Iteration 193, loss = 0.08185586\n",
      "Iteration 194, loss = 0.08158891\n",
      "Iteration 172, loss = 0.19613245\n",
      "Iteration 109, loss = 0.11501047\n",
      "Iteration 195, loss = 0.08132942\n",
      "Iteration 196, loss = 0.08108068\n",
      "Iteration 197, loss = 0.08081022\n",
      "Iteration 110, loss = 0.11441652\n",
      "Iteration 198, loss = 0.08055966\n",
      "Iteration 272, loss = 0.14999604\n",
      "Iteration 111, loss = 0.11382074\n",
      "Iteration 199, loss = 0.08030806\n",
      "Iteration 318, loss = 0.21275257\n",
      "Iteration 200, loss = 0.08005875\n",
      "Iteration 112, loss = 0.11322155\n",
      "Iteration 198, loss = 0.16627642\n",
      "Iteration 21, loss = 0.32236897\n",
      "Iteration 113, loss = 0.11264206\n",
      "Iteration 77, loss = 0.29285897\n",
      "Iteration 114, loss = 0.11207063\n",
      "Iteration 115, loss = 0.11151327\n",
      "Iteration 201, loss = 0.07981768\n",
      "Iteration 116, loss = 0.11095711\n",
      "Iteration 202, loss = 0.07957205\n",
      "Iteration 203, loss = 0.07933330\n",
      "Iteration 273, loss = 0.14990495\n",
      "Iteration 22, loss = 0.31575044\n",
      "Iteration 173, loss = 0.19587317\n",
      "Iteration 204, loss = 0.07909455\n",
      "Iteration 23, loss = 0.30929726\n",
      "Iteration 199, loss = 0.16606117\n",
      "Iteration 117, loss = 0.11039322\n",
      "Iteration 205, loss = 0.07885925\n",
      "Iteration 24, loss = 0.30316640\n",
      "Iteration 118, loss = 0.10986703\n",
      "Iteration 206, loss = 0.07861724\n",
      "Iteration 207, loss = 0.07838330\n",
      "Iteration 119, loss = 0.10932671\n",
      "Iteration 274, loss = 0.14980288\n",
      "Iteration 208, loss = 0.07814110\n",
      "Iteration 120, loss = 0.10879860\n",
      "Iteration 25, loss = 0.29724660\n",
      "Iteration 209, loss = 0.07792654\n",
      "Iteration 26, loss = 0.29151958\n",
      "Iteration 200, loss = 0.16571405\n",
      "Iteration 210, loss = 0.07769155\n",
      "Iteration 121, loss = 0.10828193\n",
      "Iteration 211, loss = 0.07747338\n",
      "Iteration 78, loss = 0.29112154\n",
      "Iteration 212, loss = 0.07724452\n",
      "Iteration 213, loss = 0.07701728\n",
      "Iteration 174, loss = 0.19560807\n",
      "Iteration 214, loss = 0.07679841\n",
      "Iteration 215, loss = 0.07658285\n",
      "Iteration 27, loss = 0.28610945\n",
      "Iteration 122, loss = 0.10776827\n",
      "Iteration 216, loss = 0.07636276\n",
      "Iteration 123, loss = 0.10727561\n",
      "Iteration 217, loss = 0.07616456\n",
      "Iteration 28, loss = 0.28074434\n",
      "Iteration 124, loss = 0.10677916\n",
      "Iteration 319, loss = 0.21270378\n",
      "Iteration 125, loss = 0.10628894\n",
      "Iteration 201, loss = 0.16550711\n",
      "Iteration 218, loss = 0.07593188\n",
      "Iteration 29, loss = 0.27564332\n",
      "Iteration 275, loss = 0.14966836\n",
      "Iteration 219, loss = 0.07572258\n",
      "Iteration 30, loss = 0.27058022\n",
      "Iteration 31, loss = 0.26563687\n",
      "Iteration 126, loss = 0.10582681\n",
      "Iteration 79, loss = 0.28953229\n",
      "Iteration 220, loss = 0.07550649\n",
      "Iteration 32, loss = 0.26099489\n",
      "Iteration 33, loss = 0.25648337\n",
      "Iteration 127, loss = 0.10533428\n",
      "Iteration 221, loss = 0.07529581\n",
      "Iteration 222, loss = 0.07508636\n",
      "Iteration 223, loss = 0.07488046\n",
      "Iteration 128, loss = 0.10487333\n",
      "Iteration 224, loss = 0.07468566\n",
      "Iteration 129, loss = 0.10441647\n",
      "Iteration 225, loss = 0.07446304\n",
      "Iteration 34, loss = 0.25205417\n",
      "Iteration 130, loss = 0.10395991\n",
      "Iteration 226, loss = 0.07426499\n",
      "Iteration 131, loss = 0.10352298\n",
      "Iteration 202, loss = 0.16522431\n",
      "Iteration 227, loss = 0.07406157\n",
      "Iteration 132, loss = 0.10308901\n",
      "Iteration 175, loss = 0.19542147\n",
      "Iteration 35, loss = 0.24778336\n",
      "Iteration 133, loss = 0.10264497\n",
      "Iteration 320, loss = 0.21266710\n",
      "Iteration 228, loss = 0.07385852\n",
      "Iteration 134, loss = 0.10222145\n",
      "Iteration 276, loss = 0.14954345\n",
      "Iteration 229, loss = 0.07366556\n",
      "Iteration 135, loss = 0.10180551\n",
      "Iteration 230, loss = 0.07346175\n",
      "Iteration 231, loss = 0.07326179\n",
      "Iteration 36, loss = 0.24359887\n",
      "Iteration 37, loss = 0.23958646\n",
      "Iteration 136, loss = 0.10138793\n",
      "Iteration 232, loss = 0.07307327\n",
      "Iteration 38, loss = 0.23576278\n",
      "Iteration 233, loss = 0.07287236\n",
      "Iteration 39, loss = 0.23194002\n",
      "Iteration 137, loss = 0.10095814\n",
      "Iteration 203, loss = 0.16492849\n",
      "Iteration 138, loss = 0.10056274\n",
      "Iteration 139, loss = 0.10015370\n",
      "Iteration 140, loss = 0.09975437\n",
      "Iteration 141, loss = 0.09936519\n",
      "Iteration 234, loss = 0.07269573\n",
      "Iteration 235, loss = 0.07248287\n",
      "Iteration 40, loss = 0.22841438\n",
      "Iteration 236, loss = 0.07230535Iteration 204, loss = 0.16468336\n",
      "\n",
      "Iteration 80, loss = 0.28794551\n",
      "Iteration 237, loss = 0.07212061\n",
      "Iteration 176, loss = 0.19511691\n",
      "Iteration 41, loss = 0.22482592\n",
      "Iteration 238, loss = 0.07192845\n",
      "Iteration 239, loss = 0.07173615\n",
      "Iteration 277, loss = 0.14941731\n",
      "Iteration 240, loss = 0.07155179\n",
      "Iteration 142, loss = 0.09897523\n",
      "Iteration 42, loss = 0.22136522\n",
      "Iteration 143, loss = 0.09857960\n",
      "Iteration 241, loss = 0.07135448\n",
      "Iteration 242, loss = 0.07118696\n",
      "Iteration 243, loss = 0.07100405\n",
      "Iteration 244, loss = 0.07082298\n",
      "Iteration 278, loss = 0.14931146\n",
      "Iteration 43, loss = 0.21804780\n",
      "Iteration 321, loss = 0.21259561\n",
      "Iteration 205, loss = 0.16444414\n",
      "Iteration 144, loss = 0.09819493\n",
      "Iteration 245, loss = 0.07063558\n",
      "Iteration 246, loss = 0.07046051\n",
      "Iteration 145, loss = 0.09783066\n",
      "Iteration 247, loss = 0.07029441\n",
      "Iteration 44, loss = 0.21482191\n",
      "Iteration 45, loss = 0.21184516\n",
      "Iteration 46, loss = 0.20863720\n",
      "Iteration 47, loss = 0.20575302\n",
      "Iteration 48, loss = 0.20289790\n",
      "Iteration 146, loss = 0.09744851\n",
      "Iteration 248, loss = 0.07010591\n",
      "Iteration 249, loss = 0.06993941\n",
      "Iteration 147, loss = 0.09707757\n",
      "Iteration 177, loss = 0.19492644\n",
      "Iteration 250, loss = 0.06977665\n",
      "Iteration 251, loss = 0.06960523\n",
      "Iteration 148, loss = 0.09670870\n",
      "Iteration 252, loss = 0.06944023\n",
      "Iteration 253, loss = 0.06928484\n",
      "Iteration 49, loss = 0.20008359\n",
      "Iteration 50, loss = 0.19743105\n",
      "Iteration 206, loss = 0.16417946\n",
      "Iteration 149, loss = 0.09636314\n",
      "Iteration 254, loss = 0.06912093\n",
      "Iteration 150, loss = 0.09599606\n",
      "Iteration 151, loss = 0.09564705\n",
      "Iteration 255, loss = 0.06896409\n",
      "Iteration 279, loss = 0.14922202\n",
      "Iteration 81, loss = 0.28652151\n",
      "Iteration 256, loss = 0.06879954\n",
      "Iteration 51, loss = 0.19486732\n",
      "Iteration 52, loss = 0.19231303\n",
      "Iteration 257, loss = 0.06865072\n",
      "Iteration 258, loss = 0.06848580\n",
      "Iteration 152, loss = 0.09530764\n",
      "Iteration 259, loss = 0.06834911\n",
      "Iteration 260, loss = 0.06819155\n",
      "Iteration 153, loss = 0.09496350\n",
      "Iteration 53, loss = 0.18985482\n",
      "Iteration 261, loss = 0.06803492\n",
      "Iteration 322, loss = 0.21248216\n",
      "Iteration 262, loss = 0.06788718\n",
      "Iteration 263, loss = 0.06773623\n",
      "Iteration 264, loss = 0.06759121\n",
      "Iteration 265, loss = 0.06745110\n",
      "Iteration 266, loss = 0.06730305\n",
      "Iteration 178, loss = 0.19472198\n",
      "Iteration 267, loss = 0.06716313\n",
      "Iteration 268, loss = 0.06701813\n",
      "Iteration 269, loss = 0.06685891\n",
      "Iteration 154, loss = 0.09461418\n",
      "Iteration 207, loss = 0.16394163\n",
      "Iteration 280, loss = 0.14907897\n",
      "Iteration 155, loss = 0.09428132\n",
      "Iteration 54, loss = 0.18752826\n",
      "Iteration 55, loss = 0.18520677\n",
      "Iteration 156, loss = 0.09395875\n",
      "Iteration 270, loss = 0.06671914\n",
      "Iteration 157, loss = 0.09361999\n",
      "Iteration 208, loss = 0.16376922\n",
      "Iteration 158, loss = 0.09331480\n",
      "Iteration 159, loss = 0.09297941\n",
      "Iteration 56, loss = 0.18297709\n",
      "Iteration 271, loss = 0.06658369\n",
      "Iteration 57, loss = 0.18080914\n",
      "Iteration 160, loss = 0.09266517\n",
      "Iteration 161, loss = 0.09235534\n",
      "Iteration 272, loss = 0.06644406\n",
      "Iteration 273, loss = 0.06630323\n",
      "Iteration 274, loss = 0.06615823\n",
      "Iteration 275, loss = 0.06602118\n",
      "Iteration 162, loss = 0.09203794Iteration 209, loss = 0.16346882\n",
      "\n",
      "Iteration 323, loss = 0.21243023\n",
      "Iteration 58, loss = 0.17868395\n",
      "Iteration 281, loss = 0.14900391\n",
      "Iteration 59, loss = 0.17664977\n",
      "Iteration 60, loss = 0.17468911\n",
      "Iteration 276, loss = 0.06588792\n",
      "Iteration 61, loss = 0.17277360\n",
      "Iteration 62, loss = 0.17090904\n",
      "Iteration 277, loss = 0.06574459\n",
      "Iteration 278, loss = 0.06561910\n",
      "Iteration 163, loss = 0.09174876\n",
      "Iteration 63, loss = 0.16907398\n",
      "Iteration 279, loss = 0.06548159\n",
      "Iteration 179, loss = 0.19446758\n",
      "Iteration 280, loss = 0.06534403\n",
      "Iteration 281, loss = 0.06522050\n",
      "Iteration 164, loss = 0.09143856\n",
      "Iteration 282, loss = 0.06508086\n",
      "Iteration 210, loss = 0.16321426\n",
      "Iteration 283, loss = 0.06495534\n",
      "Iteration 282, loss = 0.14886930\n",
      "Iteration 165, loss = 0.09113024\n",
      "Iteration 82, loss = 0.28492697\n",
      "Iteration 284, loss = 0.06481268\n",
      "Iteration 166, loss = 0.09083224\n",
      "Iteration 285, loss = 0.06469316\n",
      "Iteration 167, loss = 0.09054424\n",
      "Iteration 286, loss = 0.06455803\n",
      "Iteration 64, loss = 0.16735598\n",
      "Iteration 168, loss = 0.09025661\n",
      "Iteration 287, loss = 0.06443588\n",
      "Iteration 288, loss = 0.06430936\n",
      "Iteration 65, loss = 0.16560527\n",
      "Iteration 169, loss = 0.08996667\n",
      "Iteration 170, loss = 0.08967765\n",
      "Iteration 171, loss = 0.08940228\n",
      "Iteration 289, loss = 0.06418051\n",
      "Iteration 172, loss = 0.08911533\n",
      "Iteration 173, loss = 0.08885121\n",
      "Iteration 290, loss = 0.06405875\n",
      "Iteration 174, loss = 0.08856044\n",
      "Iteration 66, loss = 0.16398260\n",
      "Iteration 175, loss = 0.08830616\n",
      "Iteration 67, loss = 0.16233560\n",
      "Iteration 176, loss = 0.08803098\n",
      "Iteration 180, loss = 0.19424023\n",
      "Iteration 177, loss = 0.08776153\n",
      "Iteration 83, loss = 0.28357361\n",
      "Iteration 291, loss = 0.06392424\n",
      "Iteration 68, loss = 0.16079251\n",
      "Iteration 292, loss = 0.06380427\n",
      "Iteration 293, loss = 0.06367977\n",
      "Iteration 69, loss = 0.15923035\n",
      "Iteration 178, loss = 0.08749944Iteration 211, loss = 0.16297668\n",
      "Iteration 294, loss = 0.06355458\n",
      "\n",
      "Iteration 295, loss = 0.06344265\n",
      "Iteration 296, loss = 0.06332822\n",
      "Iteration 297, loss = 0.06320456\n",
      "Iteration 298, loss = 0.06308566\n",
      "Iteration 324, loss = 0.21233401\n",
      "Iteration 212, loss = 0.16274057\n",
      "Iteration 299, loss = 0.06297242\n",
      "Iteration 283, loss = 0.14876392\n",
      "Iteration 70, loss = 0.15776064\n",
      "Iteration 300, loss = 0.06284449\n",
      "Iteration 179, loss = 0.08725266\n",
      "Iteration 301, loss = 0.06274041\n",
      "Iteration 71, loss = 0.15629891\n",
      "Iteration 302, loss = 0.06262314\n",
      "Iteration 72, loss = 0.15490992\n",
      "Iteration 303, loss = 0.06250687\n",
      "Iteration 180, loss = 0.08699401\n",
      "Iteration 73, loss = 0.15353309\n",
      "Iteration 181, loss = 0.08674974\n",
      "Iteration 74, loss = 0.15225204\n",
      "Iteration 304, loss = 0.06239385\n",
      "Iteration 182, loss = 0.08647725\n",
      "Iteration 305, loss = 0.06228688\n",
      "Iteration 284, loss = 0.14865226\n",
      "Iteration 84, loss = 0.28215366\n",
      "Iteration 183, loss = 0.08623115\n",
      "Iteration 306, loss = 0.06218435\n",
      "Iteration 213, loss = 0.16253394\n",
      "Iteration 184, loss = 0.08599052\n",
      "Iteration 307, loss = 0.06207234\n",
      "Iteration 308, loss = 0.06196291\n",
      "Iteration 309, loss = 0.06184587\n",
      "Iteration 181, loss = 0.19405841\n",
      "Iteration 75, loss = 0.15091121\n",
      "Iteration 285, loss = 0.14855100\n",
      "Iteration 310, loss = 0.06173017\n",
      "Iteration 76, loss = 0.14964762\n",
      "Iteration 185, loss = 0.08574705\n",
      "Iteration 77, loss = 0.14840566\n",
      "Iteration 325, loss = 0.21229572\n",
      "Iteration 186, loss = 0.08550505\n",
      "Iteration 311, loss = 0.06161511\n",
      "Iteration 312, loss = 0.06151376\n",
      "Iteration 313, loss = 0.06141412\n",
      "Iteration 314, loss = 0.06130015\n",
      "Iteration 315, loss = 0.06119526\n",
      "Iteration 214, loss = 0.16233399\n",
      "Iteration 316, loss = 0.06108329\n",
      "Iteration 317, loss = 0.06098481\n",
      "Iteration 318, loss = 0.06087267\n",
      "Iteration 319, loss = 0.06076672\n",
      "Iteration 215, loss = 0.16207911\n",
      "Iteration 187, loss = 0.08526589\n",
      "Iteration 78, loss = 0.14721919\n",
      "Iteration 320, loss = 0.06066414\n",
      "Iteration 188, loss = 0.08501164\n",
      "Iteration 182, loss = 0.19381513\n",
      "Iteration 321, loss = 0.06056657\n",
      "Iteration 189, loss = 0.08477949\n",
      "Iteration 85, loss = 0.28083203\n",
      "Iteration 79, loss = 0.14602295\n",
      "Iteration 322, loss = 0.06045848\n",
      "Iteration 80, loss = 0.14489592\n",
      "Iteration 190, loss = 0.08455695\n",
      "Iteration 81, loss = 0.14380014\n",
      "Iteration 191, loss = 0.08432827\n",
      "Iteration 192, loss = 0.08410072\n",
      "Iteration 323, loss = 0.06036291\n",
      "Iteration 324, loss = 0.06025884\n",
      "Iteration 325, loss = 0.06015659\n",
      "Iteration 326, loss = 0.06006248\n",
      "Iteration 327, loss = 0.05995830\n",
      "Iteration 193, loss = 0.08386688\n",
      "Iteration 286, loss = 0.14844390\n",
      "Iteration 82, loss = 0.14271375\n",
      "Iteration 194, loss = 0.08364098\n",
      "Iteration 83, loss = 0.14162578\n",
      "Iteration 84, loss = 0.14059204\n",
      "Iteration 326, loss = 0.21220776\n",
      "Iteration 85, loss = 0.13958357\n",
      "Iteration 328, loss = 0.05987343\n",
      "Iteration 329, loss = 0.05977012\n",
      "Iteration 330, loss = 0.05967762\n",
      "Iteration 195, loss = 0.08342159\n",
      "Iteration 216, loss = 0.16185219\n",
      "Iteration 287, loss = 0.14833205\n",
      "Iteration 86, loss = 0.13858891\n",
      "Iteration 196, loss = 0.08320682\n",
      "Iteration 197, loss = 0.08298205\n",
      "Iteration 331, loss = 0.05957638\n",
      "Iteration 87, loss = 0.13764150\n",
      "Iteration 88, loss = 0.13670005\n",
      "Iteration 332, loss = 0.05949077\n",
      "Iteration 217, loss = 0.16162295\n",
      "Iteration 89, loss = 0.13575883\n",
      "Iteration 333, loss = 0.05939162\n",
      "Iteration 334, loss = 0.05929519\n",
      "Iteration 335, loss = 0.05920451\n",
      "Iteration 336, loss = 0.05910517\n",
      "Iteration 337, loss = 0.05902573\n",
      "Iteration 338, loss = 0.05892607\n",
      "Iteration 198, loss = 0.08276062\n",
      "Iteration 183, loss = 0.19358280\n",
      "Iteration 199, loss = 0.08254060\n",
      "Iteration 200, loss = 0.08234174\n",
      "Iteration 86, loss = 0.27956075\n",
      "Iteration 288, loss = 0.14822736\n",
      "Iteration 90, loss = 0.13488643\n",
      "Iteration 339, loss = 0.05883388\n",
      "Iteration 91, loss = 0.13397692\n",
      "Iteration 201, loss = 0.08212426\n",
      "Iteration 340, loss = 0.05874242\n",
      "Iteration 202, loss = 0.08191441\n",
      "Iteration 203, loss = 0.08170700\n",
      "Iteration 92, loss = 0.13311520\n",
      "Iteration 218, loss = 0.16145278\n",
      "Iteration 204, loss = 0.08150050\n",
      "Iteration 341, loss = 0.05865189\n",
      "Iteration 184, loss = 0.19335975\n",
      "Iteration 342, loss = 0.05856183\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 205, loss = 0.08129154\n",
      "Iteration 327, loss = 0.21212925\n",
      "Iteration 93, loss = 0.13226414\n",
      "Iteration 206, loss = 0.08109584\n",
      "Iteration 207, loss = 0.08089676\n",
      "Iteration 1, loss = 0.79602595\n",
      "Iteration 208, loss = 0.08069381\n",
      "Iteration 2, loss = 0.75760580\n",
      "Iteration 289, loss = 0.14814439\n",
      "Iteration 3, loss = 0.70721433\n",
      "Iteration 94, loss = 0.13143318\n",
      "Iteration 4, loss = 0.65719224\n",
      "Iteration 209, loss = 0.08049629\n",
      "Iteration 87, loss = 0.27821597\n",
      "Iteration 210, loss = 0.08031740\n",
      "Iteration 219, loss = 0.16122273\n",
      "Iteration 5, loss = 0.61138357\n",
      "Iteration 6, loss = 0.57247630\n",
      "Iteration 95, loss = 0.13061339\n",
      "Iteration 96, loss = 0.12981793\n",
      "Iteration 211, loss = 0.08011153\n",
      "Iteration 97, loss = 0.12902854\n",
      "Iteration 98, loss = 0.12827693\n",
      "Iteration 7, loss = 0.53891634\n",
      "Iteration 220, loss = 0.16104578\n",
      "Iteration 99, loss = 0.12751468\n",
      "Iteration 290, loss = 0.14801494\n",
      "Iteration 212, loss = 0.07991090\n",
      "Iteration 185, loss = 0.19316848\n",
      "Iteration 213, loss = 0.07973382\n",
      "Iteration 214, loss = 0.07954284\n",
      "Iteration 215, loss = 0.07934899\n",
      "Iteration 100, loss = 0.12677510\n",
      "Iteration 8, loss = 0.50984243\n",
      "Iteration 328, loss = 0.21208192\n",
      "Iteration 101, loss = 0.12605067\n",
      "Iteration 102, loss = 0.12534727\n",
      "Iteration 216, loss = 0.07916029\n",
      "Iteration 217, loss = 0.07897911\n",
      "Iteration 291, loss = 0.14792812\n",
      "Iteration 218, loss = 0.07880543\n",
      "Iteration 221, loss = 0.16081598\n",
      "Iteration 9, loss = 0.48571526Iteration 219, loss = 0.07861007\n",
      "Iteration 88, loss = 0.27699843\n",
      "\n",
      "Iteration 220, loss = 0.07843443\n",
      "Iteration 103, loss = 0.12464083\n",
      "Iteration 221, loss = 0.07825483\n",
      "Iteration 10, loss = 0.46442865\n",
      "Iteration 186, loss = 0.19302344\n",
      "Iteration 104, loss = 0.12394818\n",
      "Iteration 222, loss = 0.07808299\n",
      "Iteration 223, loss = 0.07791503\n",
      "Iteration 105, loss = 0.12329416\n",
      "Iteration 224, loss = 0.07773784\n",
      "Iteration 11, loss = 0.44603731\n",
      "Iteration 225, loss = 0.07756286\n",
      "Iteration 106, loss = 0.12262264\n",
      "Iteration 329, loss = 0.21199447\n",
      "Iteration 292, loss = 0.14780749\n",
      "Iteration 12, loss = 0.42918812\n",
      "Iteration 107, loss = 0.12195043\n",
      "Iteration 226, loss = 0.07738853\n",
      "Iteration 222, loss = 0.16061913\n",
      "Iteration 89, loss = 0.27582682\n",
      "Iteration 227, loss = 0.07721727\n",
      "Iteration 108, loss = 0.12132259\n",
      "Iteration 109, loss = 0.12070265\n",
      "Iteration 13, loss = 0.41407425\n",
      "Iteration 228, loss = 0.07704126\n",
      "Iteration 229, loss = 0.07688574\n",
      "Iteration 14, loss = 0.40031506\n",
      "Iteration 223, loss = 0.16043141\n",
      "Iteration 293, loss = 0.14773065\n",
      "Iteration 187, loss = 0.19278634\n",
      "Iteration 230, loss = 0.07671318\n",
      "Iteration 15, loss = 0.38756128\n",
      "Iteration 110, loss = 0.12008633\n",
      "Iteration 231, loss = 0.07656180\n",
      "Iteration 111, loss = 0.11947355\n",
      "Iteration 232, loss = 0.07638718\n",
      "Iteration 233, loss = 0.07623785\n",
      "Iteration 234, loss = 0.07606257\n",
      "Iteration 16, loss = 0.37593004\n",
      "Iteration 235, loss = 0.07590340\n",
      "Iteration 224, loss = 0.16024154\n",
      "Iteration 112, loss = 0.11888262\n",
      "Iteration 294, loss = 0.14761632\n",
      "Iteration 236, loss = 0.07575927\n",
      "Iteration 113, loss = 0.11826931\n",
      "Iteration 17, loss = 0.36480356\n",
      "Iteration 330, loss = 0.21195623\n",
      "Iteration 114, loss = 0.11770801\n",
      "Iteration 115, loss = 0.11715763\n",
      "Iteration 237, loss = 0.07559340\n",
      "Iteration 238, loss = 0.07543152\n",
      "Iteration 188, loss = 0.19258104\n",
      "Iteration 295, loss = 0.14751607\n",
      "Iteration 239, loss = 0.07528876\n",
      "Iteration 116, loss = 0.11657506\n",
      "Iteration 90, loss = 0.27464928\n",
      "Iteration 240, loss = 0.07513005\n",
      "Iteration 18, loss = 0.35443656\n",
      "Iteration 225, loss = 0.16009281\n",
      "Iteration 117, loss = 0.11601956\n",
      "Iteration 19, loss = 0.34464454\n",
      "Iteration 241, loss = 0.07497700\n",
      "Iteration 20, loss = 0.33529623\n",
      "Iteration 242, loss = 0.07481497\n",
      "Iteration 21, loss = 0.32644149\n",
      "Iteration 118, loss = 0.11548265\n",
      "Iteration 22, loss = 0.31800677\n",
      "Iteration 119, loss = 0.11495887\n",
      "Iteration 23, loss = 0.30999513\n",
      "Iteration 120, loss = 0.11442304\n",
      "Iteration 226, loss = 0.15988254\n",
      "Iteration 243, loss = 0.07467205\n",
      "Iteration 121, loss = 0.11389905\n",
      "Iteration 24, loss = 0.30225598\n",
      "Iteration 122, loss = 0.11339554\n",
      "Iteration 244, loss = 0.07450758\n",
      "Iteration 91, loss = 0.27348100\n",
      "Iteration 245, loss = 0.07436642\n",
      "Iteration 246, loss = 0.07420799\n",
      "Iteration 123, loss = 0.11289734\n",
      "Iteration 247, loss = 0.07405714\n",
      "Iteration 248, loss = 0.07392481\n",
      "Iteration 249, loss = 0.07377842\n",
      "Iteration 124, loss = 0.11238200Iteration 250, loss = 0.07361966\n",
      "\n",
      "Iteration 189, loss = 0.19234713\n",
      "Iteration 251, loss = 0.07347649\n",
      "Iteration 252, loss = 0.07334185\n",
      "Iteration 25, loss = 0.29487469\n",
      "Iteration 253, loss = 0.07319270\n",
      "Iteration 296, loss = 0.14742189\n",
      "Iteration 331, loss = 0.21190585\n",
      "Iteration 125, loss = 0.11192292\n",
      "Iteration 26, loss = 0.28799062\n",
      "Iteration 227, loss = 0.15967827\n",
      "Iteration 126, loss = 0.11142758\n",
      "Iteration 127, loss = 0.11094872\n",
      "Iteration 128, loss = 0.11048057\n",
      "Iteration 27, loss = 0.28123023\n",
      "Iteration 254, loss = 0.07304472\n",
      "Iteration 255, loss = 0.07290978\n",
      "Iteration 28, loss = 0.27481886\n",
      "Iteration 256, loss = 0.07275646\n",
      "Iteration 29, loss = 0.26864433\n",
      "Iteration 257, loss = 0.07262732\n",
      "Iteration 258, loss = 0.07249158\n",
      "Iteration 297, loss = 0.14732583\n",
      "Iteration 30, loss = 0.26287028\n",
      "Iteration 259, loss = 0.07234439\n",
      "Iteration 190, loss = 0.19215646Iteration 129, loss = 0.11002536\n",
      "\n",
      "Iteration 228, loss = 0.15948240\n",
      "Iteration 130, loss = 0.10959439\n",
      "Iteration 131, loss = 0.10913004\n",
      "Iteration 260, loss = 0.07221133\n",
      "Iteration 132, loss = 0.10867763\n",
      "Iteration 261, loss = 0.07207368\n",
      "Iteration 31, loss = 0.25724864\n",
      "Iteration 262, loss = 0.07193854\n",
      "Iteration 263, loss = 0.07181105\n",
      "Iteration 332, loss = 0.21185273\n",
      "Iteration 264, loss = 0.07167946\n",
      "Iteration 133, loss = 0.10823651\n",
      "Iteration 265, loss = 0.07154234\n",
      "Iteration 266, loss = 0.07140899\n",
      "Iteration 92, loss = 0.27247421\n",
      "Iteration 267, loss = 0.07128100\n",
      "Iteration 268, loss = 0.07115744\n",
      "Iteration 269, loss = 0.07102594\n",
      "Iteration 298, loss = 0.14722650\n",
      "Iteration 32, loss = 0.25185047\n",
      "Iteration 229, loss = 0.15937777\n",
      "Iteration 134, loss = 0.10781409\n",
      "Iteration 33, loss = 0.24661211\n",
      "Iteration 299, loss = 0.14713225\n",
      "Iteration 34, loss = 0.24166549\n",
      "Iteration 135, loss = 0.10738610\n",
      "Iteration 136, loss = 0.10696459\n",
      "Iteration 191, loss = 0.19198534\n",
      "Iteration 270, loss = 0.07089747\n",
      "Iteration 35, loss = 0.23696020\n",
      "Iteration 230, loss = 0.15924833\n",
      "Iteration 93, loss = 0.27134828\n",
      "Iteration 137, loss = 0.10656675\n",
      "Iteration 271, loss = 0.07077904\n",
      "Iteration 36, loss = 0.23248013\n",
      "Iteration 138, loss = 0.10614594\n",
      "Iteration 272, loss = 0.07065357\n",
      "Iteration 273, loss = 0.07052058\n",
      "Iteration 37, loss = 0.22806256\n",
      "Iteration 274, loss = 0.07039608\n",
      "Iteration 275, loss = 0.07028466\n",
      "Iteration 276, loss = 0.07015096\n",
      "Iteration 277, loss = 0.07005113\n",
      "Iteration 38, loss = 0.22383943Iteration 278, loss = 0.06991643\n",
      "Iteration 300, loss = 0.14703236\n",
      "Iteration 139, loss = 0.10574229\n",
      "Iteration 140, loss = 0.10535091\n",
      "\n",
      "Iteration 279, loss = 0.06979316Iteration 141, loss = 0.10495107\n",
      "Iteration 142, loss = 0.10456643\n",
      "Iteration 231, loss = 0.15895680\n",
      "Iteration 333, loss = 0.21175286\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 143, loss = 0.10419627\n",
      "\n",
      "Iteration 39, loss = 0.21981382\n",
      "Iteration 232, loss = 0.15880153\n",
      "Iteration 192, loss = 0.19180269\n",
      "Iteration 280, loss = 0.06966567\n",
      "Iteration 281, loss = 0.06955953\n",
      "Iteration 282, loss = 0.06944395\n",
      "Iteration 40, loss = 0.21589305\n",
      "Iteration 94, loss = 0.27030884\n",
      "Iteration 301, loss = 0.14693283\n",
      "Iteration 41, loss = 0.21224623\n",
      "Iteration 283, loss = 0.06931434\n",
      "Iteration 284, loss = 0.06920179\n",
      "Iteration 42, loss = 0.20871403\n",
      "Iteration 144, loss = 0.10380668\n",
      "Iteration 145, loss = 0.10344396\n",
      "Iteration 285, loss = 0.06908709\n",
      "Iteration 146, loss = 0.10307242\n",
      "Iteration 286, loss = 0.06897380\n",
      "Iteration 193, loss = 0.19160288\n",
      "Iteration 43, loss = 0.20524299\n",
      "Iteration 287, loss = 0.06886357\n",
      "Iteration 233, loss = 0.15865137\n",
      "Iteration 1, loss = 0.68638793\n",
      "Iteration 288, loss = 0.06875706\n",
      "Iteration 302, loss = 0.14683902\n",
      "Iteration 2, loss = 0.66498171\n",
      "Iteration 3, loss = 0.63664932\n",
      "Iteration 147, loss = 0.10270862\n",
      "Iteration 44, loss = 0.20195846\n",
      "Iteration 289, loss = 0.06863407\n",
      "Iteration 148, loss = 0.10234477\n",
      "Iteration 45, loss = 0.19886037\n",
      "Iteration 290, loss = 0.06852977\n",
      "Iteration 149, loss = 0.10198671\n",
      "Iteration 4, loss = 0.60647633\n",
      "Iteration 5, loss = 0.57871123\n",
      "Iteration 6, loss = 0.55377881\n",
      "Iteration 234, loss = 0.15847435\n",
      "Iteration 150, loss = 0.10164278\n",
      "Iteration 291, loss = 0.06840819\n",
      "Iteration 292, loss = 0.06830447\n",
      "Iteration 303, loss = 0.14677632\n",
      "Iteration 293, loss = 0.06818793\n",
      "Iteration 7, loss = 0.53142994\n",
      "Iteration 194, loss = 0.19138789\n",
      "Iteration 294, loss = 0.06808146\n",
      "Iteration 8, loss = 0.51178282\n",
      "Iteration 295, loss = 0.06796791\n",
      "Iteration 151, loss = 0.10129156\n",
      "Iteration 46, loss = 0.19579373\n",
      "Iteration 95, loss = 0.26948589\n",
      "Iteration 152, loss = 0.10095723\n",
      "Iteration 296, loss = 0.06787557\n",
      "Iteration 9, loss = 0.49425572\n",
      "Iteration 47, loss = 0.19284935\n",
      "Iteration 10, loss = 0.47895949\n",
      "Iteration 153, loss = 0.10062170\n",
      "Iteration 11, loss = 0.46564784Iteration 297, loss = 0.06776616\n",
      "Iteration 235, loss = 0.15831071\n",
      "Iteration 298, loss = 0.06765254\n",
      "\n",
      "Iteration 236, loss = 0.15814259\n",
      "Iteration 48, loss = 0.19009090\n",
      "Iteration 12, loss = 0.45317723\n",
      "Iteration 304, loss = 0.14666144\n",
      "Iteration 154, loss = 0.10029014\n",
      "Iteration 96, loss = 0.26837666\n",
      "Iteration 299, loss = 0.06754451\n",
      "Iteration 49, loss = 0.18740722\n",
      "Iteration 155, loss = 0.09994848\n",
      "Iteration 300, loss = 0.06745301\n",
      "Iteration 13, loss = 0.44255754\n",
      "Iteration 301, loss = 0.06734497\n",
      "Iteration 237, loss = 0.15798145\n",
      "Iteration 14, loss = 0.43275510\n",
      "Iteration 50, loss = 0.18477009\n",
      "Iteration 51, loss = 0.18231273\n",
      "Iteration 156, loss = 0.09962937\n",
      "Iteration 238, loss = 0.15785120\n",
      "Iteration 52, loss = 0.17987448\n",
      "Iteration 15, loss = 0.42356224\n",
      "Iteration 305, loss = 0.14657323\n",
      "Iteration 16, loss = 0.41519192\n",
      "Iteration 53, loss = 0.17753522\n",
      "Iteration 17, loss = 0.40746722\n",
      "Iteration 18, loss = 0.40023098\n",
      "Iteration 302, loss = 0.06723165\n",
      "Iteration 195, loss = 0.19122732\n",
      "Iteration 157, loss = 0.09931038\n",
      "Iteration 158, loss = 0.09901577\n",
      "Iteration 303, loss = 0.06713112\n",
      "Iteration 159, loss = 0.09867533\n",
      "Iteration 304, loss = 0.06702717\n",
      "Iteration 160, loss = 0.09836790\n",
      "Iteration 305, loss = 0.06693180\n",
      "Iteration 306, loss = 0.06682298\n",
      "Iteration 307, loss = 0.06672069\n",
      "Iteration 19, loss = 0.39334512\n",
      "Iteration 20, loss = 0.38694190\n",
      "Iteration 308, loss = 0.06662559\n",
      "Iteration 21, loss = 0.38087851\n",
      "Iteration 161, loss = 0.09807635\n",
      "Iteration 196, loss = 0.19106300\n",
      "Iteration 22, loss = 0.37511025\n",
      "Iteration 54, loss = 0.17530584\n",
      "Iteration 23, loss = 0.36955835\n",
      "Iteration 162, loss = 0.09775437\n",
      "Iteration 24, loss = 0.36434655\n",
      "Iteration 163, loss = 0.09746045\n",
      "Iteration 309, loss = 0.06652723\n",
      "Iteration 164, loss = 0.09716039\n",
      "Iteration 310, loss = 0.06642275\n",
      "Iteration 306, loss = 0.14647339\n",
      "Iteration 97, loss = 0.26736145\n",
      "Iteration 25, loss = 0.35917651\n",
      "Iteration 311, loss = 0.06633561\n",
      "Iteration 55, loss = 0.17311006\n",
      "Iteration 165, loss = 0.09686652\n",
      "Iteration 312, loss = 0.06622815\n",
      "Iteration 26, loss = 0.35421870\n",
      "Iteration 56, loss = 0.17102206\n",
      "Iteration 166, loss = 0.09658233\n",
      "Iteration 313, loss = 0.06612717\n",
      "Iteration 314, loss = 0.06604552\n",
      "Iteration 57, loss = 0.16900568\n",
      "Iteration 315, loss = 0.06593873\n",
      "Iteration 27, loss = 0.34950703\n",
      "Iteration 58, loss = 0.16707223\n",
      "Iteration 316, loss = 0.06584896\n",
      "Iteration 28, loss = 0.34492853\n",
      "Iteration 317, loss = 0.06575318\n",
      "Iteration 59, loss = 0.16516978\n",
      "Iteration 29, loss = 0.34049328\n",
      "Iteration 318, loss = 0.06565919\n",
      "Iteration 30, loss = 0.33604786\n",
      "Iteration 31, loss = 0.33184403\n",
      "Iteration 319, loss = 0.06555992\n",
      "Iteration 32, loss = 0.32771948\n",
      "Iteration 239, loss = 0.15771021\n",
      "Iteration 320, loss = 0.06547383\n",
      "Iteration 167, loss = 0.09629695\n",
      "Iteration 197, loss = 0.19084534\n",
      "Iteration 33, loss = 0.32376005\n",
      "Iteration 168, loss = 0.09602300\n",
      "Iteration 240, loss = 0.15753974\n",
      "Iteration 60, loss = 0.16335058\n",
      "Iteration 307, loss = 0.14637399\n",
      "Iteration 34, loss = 0.31978513\n",
      "Iteration 169, loss = 0.09573999\n",
      "Iteration 170, loss = 0.09547077\n",
      "Iteration 321, loss = 0.06538170\n",
      "Iteration 171, loss = 0.09518023\n",
      "Iteration 61, loss = 0.16160313\n",
      "Iteration 98, loss = 0.26661289\n",
      "Iteration 172, loss = 0.09492568\n",
      "Iteration 35, loss = 0.31574636\n",
      "Iteration 308, loss = 0.14628099\n",
      "Iteration 322, loss = 0.06528711\n",
      "Iteration 36, loss = 0.31192794\n",
      "Iteration 62, loss = 0.15985522\n",
      "Iteration 37, loss = 0.30815103\n",
      "Iteration 173, loss = 0.09465188\n",
      "Iteration 241, loss = 0.15737679\n",
      "Iteration 323, loss = 0.06518910\n",
      "Iteration 324, loss = 0.06510588\n",
      "Iteration 174, loss = 0.09438307\n",
      "Iteration 325, loss = 0.06500534\n",
      "Iteration 38, loss = 0.30441896\n",
      "Iteration 39, loss = 0.30059600\n",
      "Iteration 63, loss = 0.15822513\n",
      "Iteration 326, loss = 0.06492574\n",
      "Iteration 175, loss = 0.09413335\n",
      "Iteration 40, loss = 0.29696081\n",
      "Iteration 327, loss = 0.06483071\n",
      "Iteration 41, loss = 0.29323493\n",
      "Iteration 198, loss = 0.19069618\n",
      "Iteration 42, loss = 0.28962339\n",
      "Iteration 176, loss = 0.09385840\n",
      "Iteration 64, loss = 0.15663769\n",
      "Iteration 328, loss = 0.06474985\n",
      "Iteration 242, loss = 0.15721285\n",
      "Iteration 43, loss = 0.28603303\n",
      "Iteration 65, loss = 0.15508816\n",
      "Iteration 44, loss = 0.28250076\n",
      "Iteration 329, loss = 0.06465126\n",
      "Iteration 309, loss = 0.14620551\n",
      "Iteration 330, loss = 0.06456311\n",
      "Iteration 177, loss = 0.09359560\n",
      "Iteration 99, loss = 0.26554902\n",
      "Iteration 178, loss = 0.09334793\n",
      "Iteration 66, loss = 0.15357918\n",
      "Iteration 179, loss = 0.09309607\n",
      "Iteration 331, loss = 0.06447911\n",
      "Iteration 45, loss = 0.27891807\n",
      "Iteration 46, loss = 0.27542851\n",
      "Iteration 332, loss = 0.06439165\n",
      "Iteration 243, loss = 0.15709355\n",
      "Iteration 67, loss = 0.15211854\n",
      "Iteration 47, loss = 0.27197037\n",
      "Iteration 48, loss = 0.26853263\n",
      "Iteration 49, loss = 0.26501599\n",
      "Iteration 50, loss = 0.26163413\n",
      "Iteration 180, loss = 0.09284456\n",
      "Iteration 333, loss = 0.06430385\n",
      "Iteration 68, loss = 0.15070045\n",
      "Iteration 310, loss = 0.14612442\n",
      "Iteration 199, loss = 0.19053655\n",
      "Iteration 181, loss = 0.09260059\n",
      "Iteration 244, loss = 0.15693402\n",
      "Iteration 69, loss = 0.14935037\n",
      "Iteration 334, loss = 0.06421689\n",
      "Iteration 70, loss = 0.14802349\n",
      "Iteration 182, loss = 0.09236184\n",
      "Iteration 71, loss = 0.14671997\n",
      "Iteration 311, loss = 0.14601406\n",
      "Iteration 335, loss = 0.06414067\n",
      "Iteration 72, loss = 0.14547614\n",
      "Iteration 336, loss = 0.06405143\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 51, loss = 0.25842109\n",
      "Iteration 183, loss = 0.09210788\n",
      "Iteration 184, loss = 0.09187909\n",
      "Iteration 52, loss = 0.25506295\n",
      "Iteration 53, loss = 0.25180963\n",
      "Iteration 54, loss = 0.24859845\n",
      "Iteration 55, loss = 0.24545597\n",
      "Iteration 1, loss = 0.86927709\n",
      "Iteration 56, loss = 0.24223545\n",
      "Iteration 100, loss = 0.26468953\n",
      "Iteration 57, loss = 0.23922713\n",
      "Iteration 185, loss = 0.09164132\n",
      "Iteration 58, loss = 0.23623056\n",
      "Iteration 2, loss = 0.80347168\n",
      "Iteration 59, loss = 0.23317918\n",
      "Iteration 73, loss = 0.14424974\n",
      "Iteration 200, loss = 0.19035419\n",
      "Iteration 60, loss = 0.23021320\n",
      "Iteration 186, loss = 0.09139863\n",
      "Iteration 74, loss = 0.14305746\n",
      "Iteration 187, loss = 0.09116925\n",
      "Iteration 245, loss = 0.15678103\n",
      "Iteration 3, loss = 0.72337595\n",
      "Iteration 312, loss = 0.14596635\n",
      "Iteration 75, loss = 0.14191388\n",
      "Iteration 61, loss = 0.22732881\n",
      "Iteration 188, loss = 0.09093190\n",
      "Iteration 76, loss = 0.14081214\n",
      "Iteration 4, loss = 0.64948513\n",
      "Iteration 62, loss = 0.22444533\n",
      "Iteration 63, loss = 0.22153090\n",
      "Iteration 64, loss = 0.21875397\n",
      "Iteration 189, loss = 0.09071594\n",
      "Iteration 65, loss = 0.21604670\n",
      "Iteration 246, loss = 0.15662782\n",
      "Iteration 66, loss = 0.21333807\n",
      "Iteration 190, loss = 0.09049704\n",
      "Iteration 77, loss = 0.13969834\n",
      "Iteration 67, loss = 0.21067168\n",
      "Iteration 68, loss = 0.20812346\n",
      "Iteration 69, loss = 0.20540259\n",
      "Iteration 70, loss = 0.20292881\n",
      "Iteration 201, loss = 0.19014071\n",
      "Iteration 71, loss = 0.20045286\n",
      "Iteration 5, loss = 0.58877218\n",
      "Iteration 191, loss = 0.09026325\n",
      "Iteration 192, loss = 0.09004777\n",
      "Iteration 78, loss = 0.13866010\n",
      "Iteration 313, loss = 0.14584865\n",
      "Iteration 193, loss = 0.08982282\n",
      "Iteration 194, loss = 0.08960500\n",
      "Iteration 101, loss = 0.26384877\n",
      "Iteration 6, loss = 0.54184895\n",
      "Iteration 247, loss = 0.15650777\n",
      "Iteration 195, loss = 0.08938779\n",
      "Iteration 7, loss = 0.50624842\n",
      "Iteration 72, loss = 0.19797112\n",
      "Iteration 202, loss = 0.19006816\n",
      "Iteration 73, loss = 0.19556280\n",
      "Iteration 79, loss = 0.13761300\n",
      "Iteration 74, loss = 0.19323441\n",
      "Iteration 75, loss = 0.19091907\n",
      "Iteration 80, loss = 0.13660207\n",
      "Iteration 76, loss = 0.18865029\n",
      "Iteration 77, loss = 0.18648528\n",
      "Iteration 102, loss = 0.26301657\n",
      "Iteration 78, loss = 0.18427349\n",
      "Iteration 8, loss = 0.47817467\n",
      "Iteration 196, loss = 0.08918250\n",
      "Iteration 79, loss = 0.18216003\n",
      "Iteration 248, loss = 0.15637472\n",
      "Iteration 203, loss = 0.18981512\n",
      "Iteration 81, loss = 0.13564008\n",
      "Iteration 197, loss = 0.08897139\n",
      "Iteration 82, loss = 0.13467171\n",
      "Iteration 314, loss = 0.14576064\n",
      "Iteration 198, loss = 0.08874885\n",
      "Iteration 9, loss = 0.45638393\n",
      "Iteration 199, loss = 0.08856692\n",
      "Iteration 80, loss = 0.18007665\n",
      "Iteration 249, loss = 0.15618600\n",
      "Iteration 83, loss = 0.13372731\n",
      "Iteration 81, loss = 0.17804086\n",
      "Iteration 200, loss = 0.08836128\n",
      "Iteration 82, loss = 0.17610837\n",
      "Iteration 10, loss = 0.43878633\n",
      "Iteration 250, loss = 0.15616542\n",
      "Iteration 11, loss = 0.42461587\n",
      "Iteration 201, loss = 0.08812769\n",
      "Iteration 202, loss = 0.08792845\n",
      "Iteration 203, loss = 0.08771984\n",
      "Iteration 315, loss = 0.14569073\n",
      "Iteration 204, loss = 0.08752430\n",
      "Iteration 205, loss = 0.08732724\n",
      "Iteration 206, loss = 0.08712950\n",
      "Iteration 83, loss = 0.17417937\n",
      "Iteration 207, loss = 0.08691813\n",
      "Iteration 103, loss = 0.26217705\n",
      "Iteration 84, loss = 0.17227133\n",
      "Iteration 316, loss = 0.14560300\n",
      "Iteration 85, loss = 0.17044469\n",
      "Iteration 208, loss = 0.08673681\n",
      "Iteration 84, loss = 0.13284158\n",
      "Iteration 86, loss = 0.16864471\n",
      "Iteration 87, loss = 0.16686312\n",
      "Iteration 251, loss = 0.15592201\n",
      "Iteration 85, loss = 0.13192749\n",
      "Iteration 12, loss = 0.41194376\n",
      "Iteration 88, loss = 0.16511341\n",
      "Iteration 13, loss = 0.40105791\n",
      "Iteration 317, loss = 0.14553125Iteration 86, loss = 0.13106049\n",
      "Iteration 89, loss = 0.16346989\n",
      "Iteration 87, loss = 0.13023071\n",
      "Iteration 204, loss = 0.18965592\n",
      "Iteration 209, loss = 0.08654311\n",
      "\n",
      "Iteration 252, loss = 0.15579997Iteration 14, loss = 0.39127461\n",
      "Iteration 90, loss = 0.16178941\n",
      "\n",
      "Iteration 91, loss = 0.16015392\n",
      "Iteration 88, loss = 0.12938802\n",
      "Iteration 92, loss = 0.15857014\n",
      "Iteration 93, loss = 0.15707035\n",
      "Iteration 15, loss = 0.38220629\n",
      "Iteration 94, loss = 0.15554907\n",
      "Iteration 210, loss = 0.08635220\n",
      "Iteration 211, loss = 0.08615306\n",
      "Iteration 95, loss = 0.15404194\n",
      "Iteration 89, loss = 0.12857050\n",
      "Iteration 16, loss = 0.37363798\n",
      "Iteration 17, loss = 0.36556403\n",
      "Iteration 96, loss = 0.15259812\n",
      "Iteration 212, loss = 0.08598080\n",
      "Iteration 213, loss = 0.08577700\n",
      "Iteration 104, loss = 0.26137306\n",
      "Iteration 97, loss = 0.15121623\n",
      "Iteration 90, loss = 0.12779060\n",
      "Iteration 205, loss = 0.18949348\n",
      "Iteration 214, loss = 0.08560011\n",
      "Iteration 318, loss = 0.14542720\n",
      "Iteration 98, loss = 0.14982698\n",
      "Iteration 99, loss = 0.14844248\n",
      "Iteration 215, loss = 0.08541802\n",
      "Iteration 91, loss = 0.12700908\n",
      "Iteration 100, loss = 0.14711848\n",
      "Iteration 216, loss = 0.08523547\n",
      "Iteration 253, loss = 0.15566007\n",
      "Iteration 101, loss = 0.14581273\n",
      "Iteration 102, loss = 0.14453200\n",
      "Iteration 18, loss = 0.35788281\n",
      "Iteration 19, loss = 0.35029393\n",
      "Iteration 217, loss = 0.08506646\n",
      "Iteration 92, loss = 0.12626550\n",
      "Iteration 218, loss = 0.08488592\n",
      "Iteration 103, loss = 0.14330173\n",
      "Iteration 219, loss = 0.08469093\n",
      "Iteration 93, loss = 0.12551277\n",
      "Iteration 20, loss = 0.34312563\n",
      "Iteration 104, loss = 0.14210486\n",
      "Iteration 220, loss = 0.08451848\n",
      "Iteration 94, loss = 0.12480417\n",
      "Iteration 95, loss = 0.12405947\n",
      "Iteration 105, loss = 0.14090113\n",
      "Iteration 206, loss = 0.18933529\n",
      "Iteration 106, loss = 0.13975788\n",
      "Iteration 221, loss = 0.08435209\n",
      "Iteration 21, loss = 0.33620552\n",
      "Iteration 319, loss = 0.14533675\n",
      "Iteration 105, loss = 0.26067428\n",
      "Iteration 107, loss = 0.13858063\n",
      "Iteration 254, loss = 0.15554672\n",
      "Iteration 96, loss = 0.12337025\n",
      "Iteration 222, loss = 0.08417809\n",
      "Iteration 108, loss = 0.13748110\n",
      "Iteration 22, loss = 0.32921679\n",
      "Iteration 223, loss = 0.08399389\n",
      "Iteration 109, loss = 0.13632381\n",
      "Iteration 97, loss = 0.12268703\n",
      "Iteration 224, loss = 0.08384224\n",
      "Iteration 255, loss = 0.15539779\n",
      "Iteration 110, loss = 0.13529493\n",
      "Iteration 23, loss = 0.32250013\n",
      "Iteration 111, loss = 0.13423551\n",
      "Iteration 24, loss = 0.31614298\n",
      "Iteration 98, loss = 0.12204668\n",
      "Iteration 225, loss = 0.08365416\n",
      "Iteration 226, loss = 0.08349810\n",
      "Iteration 99, loss = 0.12136577\n",
      "Iteration 112, loss = 0.13318016\n",
      "Iteration 100, loss = 0.12072689\n",
      "Iteration 320, loss = 0.14524788\n",
      "Iteration 207, loss = 0.18917331\n",
      "Iteration 106, loss = 0.25987979\n",
      "Iteration 113, loss = 0.13219829\n",
      "Iteration 25, loss = 0.30970146\n",
      "Iteration 227, loss = 0.08331753\n",
      "Iteration 114, loss = 0.13115946\n",
      "Iteration 256, loss = 0.15526781\n",
      "Iteration 321, loss = 0.14516579\n",
      "Iteration 26, loss = 0.30358920\n",
      "Iteration 115, loss = 0.13022013\n",
      "Iteration 101, loss = 0.12009385\n",
      "Iteration 228, loss = 0.08315569\n",
      "Iteration 229, loss = 0.08300765\n",
      "Iteration 27, loss = 0.29744752\n",
      "Iteration 230, loss = 0.08282559\n",
      "Iteration 231, loss = 0.08266920\n",
      "Iteration 257, loss = 0.15513846\n",
      "Iteration 232, loss = 0.08251880\n",
      "Iteration 116, loss = 0.12922880\n",
      "Iteration 322, loss = 0.14509672\n",
      "Iteration 107, loss = 0.25910822\n",
      "Iteration 102, loss = 0.11947747\n",
      "Iteration 117, loss = 0.12833942\n",
      "Iteration 233, loss = 0.08234512\n",
      "Iteration 118, loss = 0.12741943\n",
      "Iteration 234, loss = 0.08219718\n",
      "Iteration 119, loss = 0.12653876\n",
      "Iteration 103, loss = 0.11888222\n",
      "Iteration 120, loss = 0.12567262\n",
      "Iteration 208, loss = 0.18901902\n",
      "Iteration 121, loss = 0.12481728\n",
      "Iteration 104, loss = 0.11827763\n",
      "Iteration 28, loss = 0.29157922\n",
      "Iteration 122, loss = 0.12398966\n",
      "Iteration 123, loss = 0.12317280\n",
      "Iteration 124, loss = 0.12237233\n",
      "Iteration 125, loss = 0.12157254\n",
      "Iteration 258, loss = 0.15502366\n",
      "Iteration 235, loss = 0.08202926\n",
      "Iteration 29, loss = 0.28600249\n",
      "Iteration 105, loss = 0.11770745\n",
      "Iteration 126, loss = 0.12082305\n",
      "Iteration 236, loss = 0.08187506\n",
      "Iteration 127, loss = 0.12005811\n",
      "Iteration 106, loss = 0.11713347\n",
      "Iteration 237, loss = 0.08172576\n",
      "Iteration 30, loss = 0.28050787\n",
      "Iteration 107, loss = 0.11656337\n",
      "Iteration 259, loss = 0.15489335\n",
      "Iteration 238, loss = 0.08155906\n",
      "Iteration 239, loss = 0.08140863\n",
      "Iteration 240, loss = 0.08126259\n",
      "Iteration 108, loss = 0.11601612\n",
      "Iteration 241, loss = 0.08111300\n",
      "Iteration 128, loss = 0.11933733\n",
      "Iteration 129, loss = 0.11856756\n",
      "Iteration 130, loss = 0.11786228\n",
      "Iteration 31, loss = 0.27500020\n",
      "Iteration 323, loss = 0.14501820\n",
      "Iteration 209, loss = 0.18886793\n",
      "Iteration 109, loss = 0.11548527\n",
      "Iteration 108, loss = 0.25835621\n",
      "Iteration 131, loss = 0.11717465\n",
      "Iteration 132, loss = 0.11646351\n",
      "Iteration 133, loss = 0.11576347\n",
      "Iteration 32, loss = 0.26984805\n",
      "Iteration 242, loss = 0.08095674\n",
      "Iteration 260, loss = 0.15476971\n",
      "Iteration 110, loss = 0.11494358\n",
      "Iteration 243, loss = 0.08081172\n",
      "Iteration 134, loss = 0.11513114\n",
      "Iteration 324, loss = 0.14493174\n",
      "Iteration 135, loss = 0.11446558\n",
      "Iteration 136, loss = 0.11380003\n",
      "Iteration 137, loss = 0.11318004\n",
      "Iteration 210, loss = 0.18872984\n",
      "Iteration 138, loss = 0.11254057\n",
      "Iteration 33, loss = 0.26479647\n",
      "Iteration 139, loss = 0.11194986\n",
      "Iteration 244, loss = 0.08065863Iteration 111, loss = 0.11442008\n",
      "Iteration 140, loss = 0.11132290\n",
      "\n",
      "Iteration 245, loss = 0.08051578\n",
      "Iteration 325, loss = 0.14484774\n",
      "Iteration 246, loss = 0.08037219\n",
      "Iteration 247, loss = 0.08023028\n",
      "Iteration 112, loss = 0.11391170\n",
      "Iteration 261, loss = 0.15468047\n",
      "Iteration 34, loss = 0.25983781\n",
      "Iteration 248, loss = 0.08008176\n",
      "Iteration 113, loss = 0.11341632\n",
      "Iteration 249, loss = 0.07993595\n",
      "Iteration 250, loss = 0.07980410\n",
      "Iteration 141, loss = 0.11072183\n",
      "Iteration 114, loss = 0.11292284\n",
      "Iteration 109, loss = 0.25771260\n",
      "Iteration 35, loss = 0.25500463\n",
      "Iteration 142, loss = 0.11014465\n",
      "Iteration 143, loss = 0.10956903\n",
      "Iteration 36, loss = 0.25050073\n",
      "Iteration 144, loss = 0.10900682\n",
      "Iteration 211, loss = 0.18854830\n",
      "Iteration 262, loss = 0.15457670\n",
      "Iteration 251, loss = 0.07965913\n",
      "Iteration 145, loss = 0.10845585\n",
      "Iteration 146, loss = 0.10789933\n",
      "Iteration 147, loss = 0.10735995\n",
      "Iteration 115, loss = 0.11242302\n",
      "Iteration 326, loss = 0.14477116\n",
      "Iteration 37, loss = 0.24590322\n",
      "Iteration 116, loss = 0.11194540\n",
      "Iteration 148, loss = 0.10681847\n",
      "Iteration 38, loss = 0.24166420\n",
      "Iteration 149, loss = 0.10628784\n",
      "Iteration 117, loss = 0.11147073\n",
      "Iteration 252, loss = 0.07951809\n",
      "Iteration 150, loss = 0.10577856\n",
      "Iteration 263, loss = 0.15440547\n",
      "Iteration 151, loss = 0.10525834\n",
      "Iteration 39, loss = 0.23741267\n",
      "Iteration 152, loss = 0.10475107\n",
      "Iteration 110, loss = 0.25700606\n",
      "Iteration 253, loss = 0.07938904\n",
      "Iteration 254, loss = 0.07923787\n",
      "Iteration 255, loss = 0.07911442\n",
      "Iteration 256, loss = 0.07897148\n",
      "Iteration 153, loss = 0.10426578\n",
      "Iteration 257, loss = 0.07884002\n",
      "Iteration 327, loss = 0.14471393\n",
      "Iteration 118, loss = 0.11100520\n",
      "Iteration 258, loss = 0.07870039\n",
      "Iteration 212, loss = 0.18841306\n",
      "Iteration 259, loss = 0.07856141\n",
      "Iteration 40, loss = 0.23329583\n",
      "Iteration 154, loss = 0.10375494\n",
      "Iteration 119, loss = 0.11054606\n",
      "Iteration 41, loss = 0.22940967\n",
      "Iteration 155, loss = 0.10328858\n",
      "Iteration 260, loss = 0.07844079\n",
      "Iteration 42, loss = 0.22558365\n",
      "Iteration 120, loss = 0.11010536\n",
      "Iteration 261, loss = 0.07830801\n",
      "Iteration 156, loss = 0.10282134\n",
      "Iteration 121, loss = 0.10966652\n",
      "Iteration 264, loss = 0.15429449\n",
      "Iteration 157, loss = 0.10232442\n",
      "Iteration 122, loss = 0.10921243\n",
      "Iteration 158, loss = 0.10187662\n",
      "Iteration 328, loss = 0.14463214\n",
      "Iteration 123, loss = 0.10877369\n",
      "Iteration 43, loss = 0.22195843\n",
      "Iteration 124, loss = 0.10835871\n",
      "Iteration 125, loss = 0.10795805\n",
      "Iteration 159, loss = 0.10139729\n",
      "Iteration 160, loss = 0.10093165\n",
      "Iteration 213, loss = 0.18826012\n",
      "Iteration 262, loss = 0.07818544\n",
      "Iteration 263, loss = 0.07804603\n",
      "Iteration 265, loss = 0.15416541\n",
      "Iteration 264, loss = 0.07791246\n",
      "Iteration 161, loss = 0.10048330\n",
      "Iteration 111, loss = 0.25631971\n",
      "Iteration 44, loss = 0.21840912\n",
      "Iteration 162, loss = 0.10003280\n",
      "Iteration 163, loss = 0.09961730\n",
      "Iteration 164, loss = 0.09918249\n",
      "Iteration 45, loss = 0.21498278\n",
      "Iteration 165, loss = 0.09874311\n",
      "Iteration 265, loss = 0.07778595\n",
      "Iteration 166, loss = 0.09831896\n",
      "Iteration 266, loss = 0.07765328\n",
      "Iteration 267, loss = 0.07754072\n",
      "Iteration 126, loss = 0.10753611\n",
      "Iteration 167, loss = 0.09790724\n",
      "Iteration 329, loss = 0.14458535\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 266, loss = 0.15407696\n",
      "Iteration 127, loss = 0.10712333\n",
      "Iteration 112, loss = 0.25565502\n",
      "Iteration 128, loss = 0.10673447\n",
      "Iteration 268, loss = 0.07740041\n",
      "Iteration 168, loss = 0.09748123\n",
      "Iteration 46, loss = 0.21165627\n",
      "Iteration 129, loss = 0.10634507\n",
      "Iteration 269, loss = 0.07728561\n",
      "Iteration 267, loss = 0.15396688\n",
      "Iteration 130, loss = 0.10595869\n",
      "Iteration 169, loss = 0.09707943\n",
      "Iteration 1, loss = 0.86469935\n",
      "Iteration 170, loss = 0.09666045\n",
      "Iteration 214, loss = 0.18812306\n",
      "Iteration 47, loss = 0.20856447\n",
      "Iteration 171, loss = 0.09625159\n",
      "Iteration 172, loss = 0.09585150\n",
      "Iteration 270, loss = 0.07717451\n",
      "Iteration 48, loss = 0.20557320\n",
      "Iteration 173, loss = 0.09543531\n",
      "Iteration 174, loss = 0.09504371\n",
      "Iteration 131, loss = 0.10557631\n",
      "Iteration 175, loss = 0.09466588\n",
      "Iteration 271, loss = 0.07703462\n",
      "Iteration 268, loss = 0.15387085\n",
      "Iteration 272, loss = 0.07692927\n",
      "Iteration 176, loss = 0.09427016\n",
      "Iteration 177, loss = 0.09389475\n",
      "Iteration 49, loss = 0.20256716\n",
      "Iteration 178, loss = 0.09353933\n",
      "Iteration 132, loss = 0.10519446\n",
      "Iteration 273, loss = 0.07679661\n",
      "Iteration 133, loss = 0.10483965\n",
      "Iteration 179, loss = 0.09312147\n",
      "Iteration 274, loss = 0.07668077\n",
      "Iteration 180, loss = 0.09276863\n",
      "Iteration 275, loss = 0.07654289\n",
      "Iteration 2, loss = 0.81636745\n",
      "Iteration 181, loss = 0.09238758Iteration 134, loss = 0.10446736\n",
      "\n",
      "Iteration 215, loss = 0.18795747\n",
      "Iteration 3, loss = 0.75484851\n",
      "Iteration 276, loss = 0.07642876\n",
      "Iteration 135, loss = 0.10409894\n",
      "Iteration 50, loss = 0.19975462\n",
      "Iteration 269, loss = 0.15373586\n",
      "Iteration 136, loss = 0.10374787\n",
      "Iteration 113, loss = 0.25505398\n",
      "Iteration 182, loss = 0.09203128\n",
      "Iteration 277, loss = 0.07631393\n",
      "Iteration 51, loss = 0.19707257\n",
      "Iteration 183, loss = 0.09167867\n",
      "Iteration 137, loss = 0.10340666\n",
      "Iteration 270, loss = 0.15365730\n",
      "Iteration 52, loss = 0.19440661\n",
      "Iteration 278, loss = 0.07618847\n",
      "Iteration 216, loss = 0.18780766\n",
      "Iteration 184, loss = 0.09132431\n",
      "Iteration 53, loss = 0.19187433\n",
      "Iteration 279, loss = 0.07608572\n",
      "Iteration 4, loss = 0.69630132\n",
      "Iteration 185, loss = 0.09096372\n",
      "Iteration 54, loss = 0.18948456\n",
      "Iteration 186, loss = 0.09062553\n",
      "Iteration 187, loss = 0.09027850\n",
      "Iteration 138, loss = 0.10305986\n",
      "Iteration 271, loss = 0.15354545\n",
      "Iteration 139, loss = 0.10272504\n",
      "Iteration 188, loss = 0.08994162\n",
      "Iteration 140, loss = 0.10237392\n",
      "Iteration 280, loss = 0.07596959\n",
      "Iteration 5, loss = 0.64441100\n",
      "Iteration 281, loss = 0.07585244\n",
      "Iteration 141, loss = 0.10204740\n",
      "Iteration 189, loss = 0.08959830\n",
      "Iteration 190, loss = 0.08927700\n",
      "Iteration 142, loss = 0.10172702\n",
      "Iteration 55, loss = 0.18706139\n",
      "Iteration 272, loss = 0.15340895\n",
      "Iteration 114, loss = 0.25440970\n",
      "Iteration 191, loss = 0.08896561\n",
      "Iteration 192, loss = 0.08862941\n",
      "Iteration 56, loss = 0.18482802\n",
      "Iteration 193, loss = 0.08830593\n",
      "Iteration 194, loss = 0.08801263\n",
      "Iteration 282, loss = 0.07572558\n",
      "Iteration 195, loss = 0.08767839\n",
      "Iteration 57, loss = 0.18266200\n",
      "Iteration 196, loss = 0.08737148\n",
      "Iteration 273, loss = 0.15335483\n",
      "Iteration 283, loss = 0.07561077\n",
      "Iteration 217, loss = 0.18768042\n",
      "Iteration 58, loss = 0.18058343\n",
      "Iteration 197, loss = 0.08706663\n",
      "Iteration 284, loss = 0.07550898\n",
      "Iteration 143, loss = 0.10139577\n",
      "Iteration 285, loss = 0.07539135\n",
      "Iteration 115, loss = 0.25372867\n",
      "Iteration 6, loss = 0.60266610\n",
      "Iteration 144, loss = 0.10107666\n",
      "Iteration 286, loss = 0.07529059\n",
      "Iteration 198, loss = 0.08677711\n",
      "Iteration 145, loss = 0.10077099\n",
      "Iteration 287, loss = 0.07518500\n",
      "Iteration 7, loss = 0.56812326\n",
      "Iteration 59, loss = 0.17855805\n",
      "Iteration 199, loss = 0.08644253\n",
      "Iteration 200, loss = 0.08617287\n",
      "Iteration 288, loss = 0.07505946\n",
      "Iteration 201, loss = 0.08585872\n",
      "Iteration 146, loss = 0.10044522\n",
      "Iteration 218, loss = 0.18752638\n",
      "Iteration 274, loss = 0.15322624\n",
      "Iteration 8, loss = 0.54022901\n",
      "Iteration 289, loss = 0.07494850\n",
      "Iteration 202, loss = 0.08559427\n",
      "Iteration 203, loss = 0.08529734\n",
      "Iteration 204, loss = 0.08499865\n",
      "Iteration 290, loss = 0.07483972\n",
      "Iteration 60, loss = 0.17657405\n",
      "Iteration 147, loss = 0.10014776\n",
      "Iteration 205, loss = 0.08472128\n",
      "Iteration 291, loss = 0.07473247\n",
      "Iteration 206, loss = 0.08443697\n",
      "Iteration 61, loss = 0.17469660\n",
      "Iteration 148, loss = 0.09984403\n",
      "Iteration 116, loss = 0.25318311\n",
      "Iteration 292, loss = 0.07462957\n",
      "Iteration 62, loss = 0.17285084\n",
      "Iteration 149, loss = 0.09954418\n",
      "Iteration 207, loss = 0.08415154\n",
      "Iteration 208, loss = 0.08388166\n",
      "Iteration 293, loss = 0.07451981\n",
      "Iteration 275, loss = 0.15308684\n",
      "Iteration 209, loss = 0.08363304\n",
      "Iteration 9, loss = 0.51691122Iteration 219, loss = 0.18740606\n",
      "\n",
      "Iteration 294, loss = 0.07441092\n",
      "Iteration 150, loss = 0.09924652\n",
      "Iteration 210, loss = 0.08334073\n",
      "Iteration 295, loss = 0.07430452\n",
      "Iteration 63, loss = 0.17113408\n",
      "Iteration 151, loss = 0.09895376\n",
      "Iteration 211, loss = 0.08307010\n",
      "Iteration 64, loss = 0.16943646\n",
      "Iteration 212, loss = 0.08281591\n",
      "Iteration 152, loss = 0.09866875\n",
      "Iteration 213, loss = 0.08254909\n",
      "Iteration 214, loss = 0.08229608\n",
      "Iteration 296, loss = 0.07419997\n",
      "Iteration 215, loss = 0.08202755\n",
      "Iteration 297, loss = 0.07410025\n",
      "Iteration 276, loss = 0.15298636\n",
      "Iteration 153, loss = 0.09838132\n",
      "Iteration 216, loss = 0.08177521\n",
      "Iteration 298, loss = 0.07400674\n",
      "Iteration 217, loss = 0.08152975\n",
      "Iteration 218, loss = 0.08128687\n",
      "Iteration 10, loss = 0.49761758\n",
      "Iteration 154, loss = 0.09811811\n",
      "Iteration 65, loss = 0.16779285\n",
      "Iteration 155, loss = 0.09783129\n",
      "Iteration 219, loss = 0.08102501\n",
      "Iteration 220, loss = 0.08079100\n",
      "Iteration 299, loss = 0.07388566\n",
      "Iteration 66, loss = 0.16624863\n",
      "Iteration 300, loss = 0.07378768\n",
      "Iteration 11, loss = 0.48055967\n",
      "Iteration 117, loss = 0.25257996\n",
      "Iteration 301, loss = 0.07368339\n",
      "Iteration 221, loss = 0.08055087\n",
      "Iteration 156, loss = 0.09756772\n",
      "Iteration 302, loss = 0.07357580\n",
      "Iteration 157, loss = 0.09728916\n",
      "Iteration 277, loss = 0.15292504\n",
      "Iteration 303, loss = 0.07348931\n",
      "Iteration 220, loss = 0.18727409\n",
      "Iteration 222, loss = 0.08031233\n",
      "Iteration 67, loss = 0.16469615\n",
      "Iteration 158, loss = 0.09703041\n",
      "Iteration 304, loss = 0.07338724\n",
      "Iteration 223, loss = 0.08006602\n",
      "Iteration 12, loss = 0.46608244\n",
      "Iteration 224, loss = 0.07983639Iteration 159, loss = 0.09676086\n",
      "\n",
      "Iteration 278, loss = 0.15277707\n",
      "Iteration 305, loss = 0.07328289\n",
      "Iteration 225, loss = 0.07963121\n",
      "Iteration 226, loss = 0.07939130\n",
      "Iteration 306, loss = 0.07318689\n",
      "Iteration 160, loss = 0.09650299\n",
      "Iteration 13, loss = 0.45320467\n",
      "Iteration 221, loss = 0.18711640\n",
      "Iteration 227, loss = 0.07915777\n",
      "Iteration 68, loss = 0.16323287\n",
      "Iteration 161, loss = 0.09624498\n",
      "Iteration 228, loss = 0.07892201\n",
      "Iteration 307, loss = 0.07308869\n",
      "Iteration 162, loss = 0.09599307\n",
      "Iteration 229, loss = 0.07872804\n",
      "Iteration 308, loss = 0.07298421\n",
      "Iteration 230, loss = 0.07849322\n",
      "Iteration 69, loss = 0.16184929\n",
      "Iteration 231, loss = 0.07826148\n",
      "Iteration 163, loss = 0.09575416\n",
      "Iteration 279, loss = 0.15269640\n",
      "Iteration 232, loss = 0.07805708\n",
      "Iteration 118, loss = 0.25194473\n",
      "Iteration 233, loss = 0.07784708\n",
      "Iteration 309, loss = 0.07289774\n",
      "Iteration 164, loss = 0.09549528\n",
      "Iteration 70, loss = 0.16044542\n",
      "Iteration 234, loss = 0.07763170\n",
      "Iteration 310, loss = 0.07280060\n",
      "Iteration 235, loss = 0.07743007\n",
      "Iteration 14, loss = 0.44159912\n",
      "Iteration 236, loss = 0.07721103\n",
      "Iteration 71, loss = 0.15913538\n",
      "Iteration 222, loss = 0.18698142\n",
      "Iteration 72, loss = 0.15785281\n",
      "Iteration 165, loss = 0.09525055\n",
      "Iteration 311, loss = 0.07270282\n",
      "Iteration 15, loss = 0.43132084\n",
      "Iteration 166, loss = 0.09502047\n",
      "Iteration 280, loss = 0.15258520\n",
      "Iteration 167, loss = 0.09478221\n",
      "Iteration 312, loss = 0.07260376Iteration 73, loss = 0.15661851\n",
      "\n",
      "Iteration 168, loss = 0.09453793\n",
      "Iteration 237, loss = 0.07701563\n",
      "Iteration 313, loss = 0.07251084\n",
      "Iteration 314, loss = 0.07242010\n",
      "Iteration 238, loss = 0.07681520\n",
      "Iteration 281, loss = 0.15251547\n",
      "Iteration 169, loss = 0.09430348\n",
      "Iteration 239, loss = 0.07662762Iteration 16, loss = 0.42158112\n",
      "\n",
      "Iteration 74, loss = 0.15543289\n",
      "Iteration 240, loss = 0.07641194\n",
      "Iteration 315, loss = 0.07231908\n",
      "Iteration 75, loss = 0.15426208\n",
      "Iteration 223, loss = 0.18684646Iteration 316, loss = 0.07223607\n",
      "\n",
      "Iteration 170, loss = 0.09407803\n",
      "Iteration 241, loss = 0.07623076\n",
      "Iteration 76, loss = 0.15314326\n",
      "Iteration 282, loss = 0.15237077\n",
      "Iteration 242, loss = 0.07601538\n",
      "Iteration 243, loss = 0.07583126\n",
      "Iteration 119, loss = 0.25140780\n",
      "Iteration 244, loss = 0.07562838\n",
      "Iteration 77, loss = 0.15202119\n",
      "Iteration 317, loss = 0.07214214\n",
      "Iteration 245, loss = 0.07544588\n",
      "Iteration 318, loss = 0.07204104\n",
      "Iteration 17, loss = 0.41236324\n",
      "Iteration 78, loss = 0.15097748\n",
      "Iteration 319, loss = 0.07194736\n",
      "Iteration 320, loss = 0.07187245\n",
      "Iteration 321, loss = 0.07177228\n",
      "Iteration 246, loss = 0.07523733\n",
      "Iteration 171, loss = 0.09383997\n",
      "Iteration 283, loss = 0.15228721\n",
      "Iteration 322, loss = 0.07168217\n",
      "Iteration 247, loss = 0.07506109\n",
      "Iteration 79, loss = 0.14993392\n",
      "Iteration 248, loss = 0.07486619\n",
      "Iteration 18, loss = 0.40393923\n",
      "Iteration 323, loss = 0.07159949\n",
      "Iteration 172, loss = 0.09361863\n",
      "Iteration 249, loss = 0.07469851\n",
      "Iteration 250, loss = 0.07450716\n",
      "Iteration 173, loss = 0.09339997\n",
      "Iteration 251, loss = 0.07433437\n",
      "Iteration 252, loss = 0.07414234\n",
      "Iteration 253, loss = 0.07395676\n",
      "Iteration 224, loss = 0.18674022\n",
      "Iteration 324, loss = 0.07150584\n",
      "Iteration 80, loss = 0.14893923\n",
      "Iteration 325, loss = 0.07141852\n",
      "Iteration 284, loss = 0.15219127\n",
      "Iteration 254, loss = 0.07378126\n",
      "Iteration 255, loss = 0.07359577\n",
      "Iteration 326, loss = 0.07132213\n",
      "Iteration 256, loss = 0.07345035\n",
      "Iteration 327, loss = 0.07124082\n",
      "Iteration 81, loss = 0.14796770\n",
      "Iteration 174, loss = 0.09317764\n",
      "Iteration 257, loss = 0.07324370\n",
      "Iteration 328, loss = 0.07116104\n",
      "Iteration 258, loss = 0.07308057\n",
      "Iteration 175, loss = 0.09295622\n",
      "Iteration 259, loss = 0.07289998\n",
      "Iteration 260, loss = 0.07273236\n",
      "Iteration 120, loss = 0.25078664\n",
      "Iteration 261, loss = 0.07256891\n",
      "Iteration 262, loss = 0.07237657\n",
      "Iteration 263, loss = 0.07220547\n",
      "Iteration 264, loss = 0.07203593\n",
      "Iteration 265, loss = 0.07187805\n",
      "Iteration 19, loss = 0.39573178\n",
      "Iteration 266, loss = 0.07170953\n",
      "Iteration 20, loss = 0.38799384\n",
      "Iteration 176, loss = 0.09273639\n",
      "Iteration 329, loss = 0.07105931\n",
      "Iteration 285, loss = 0.15209136\n",
      "Iteration 267, loss = 0.07153958\n",
      "Iteration 330, loss = 0.07097656\n",
      "Iteration 177, loss = 0.09252157\n",
      "Iteration 268, loss = 0.07140277\n",
      "Iteration 82, loss = 0.14699368\n",
      "Iteration 269, loss = 0.07122620\n",
      "Iteration 270, loss = 0.07106050\n",
      "Iteration 331, loss = 0.07089745\n",
      "Iteration 332, loss = 0.07080695\n",
      "Iteration 271, loss = 0.07090136\n",
      "Iteration 333, loss = 0.07071625\n",
      "Iteration 178, loss = 0.09232015\n",
      "Iteration 83, loss = 0.14609547\n",
      "Iteration 179, loss = 0.09210954\n",
      "Iteration 272, loss = 0.07077198\n",
      "Iteration 84, loss = 0.14518654\n",
      "Iteration 180, loss = 0.09189952\n",
      "Iteration 21, loss = 0.38059718\n",
      "Iteration 286, loss = 0.15201143\n",
      "Iteration 225, loss = 0.18664645\n",
      "Iteration 334, loss = 0.07063854\n",
      "Iteration 181, loss = 0.09171240\n",
      "Iteration 335, loss = 0.07055263\n",
      "Iteration 182, loss = 0.09149854\n",
      "Iteration 273, loss = 0.07059363\n",
      "Iteration 121, loss = 0.25024387\n",
      "Iteration 85, loss = 0.14432264\n",
      "Iteration 274, loss = 0.07043040\n",
      "Iteration 275, loss = 0.07028740\n",
      "Iteration 276, loss = 0.07015533\n",
      "Iteration 86, loss = 0.14345149\n",
      "Iteration 287, loss = 0.15198998\n",
      "Iteration 336, loss = 0.07046800\n",
      "Iteration 22, loss = 0.37337781\n",
      "Iteration 337, loss = 0.07038724\n",
      "Iteration 87, loss = 0.14263110\n",
      "Iteration 338, loss = 0.07030198\n",
      "Iteration 277, loss = 0.06998584\n",
      "Iteration 88, loss = 0.14187016\n",
      "Iteration 339, loss = 0.07022892\n",
      "Iteration 278, loss = 0.06983514\n",
      "Iteration 183, loss = 0.09129666\n",
      "Iteration 279, loss = 0.06968712\n",
      "Iteration 340, loss = 0.07014224\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 89, loss = 0.14101030\n",
      "Iteration 184, loss = 0.09109494\n",
      "Iteration 280, loss = 0.06954801\n",
      "Iteration 281, loss = 0.06940136\n",
      "Iteration 282, loss = 0.06925541\n",
      "Iteration 283, loss = 0.06910599\n",
      "Iteration 284, loss = 0.06897114\n",
      "Iteration 285, loss = 0.06883617\n",
      "Iteration 288, loss = 0.15184200\n",
      "Iteration 185, loss = 0.09090571\n",
      "Iteration 1, loss = 0.81515196\n",
      "Iteration 90, loss = 0.14023778\n",
      "Iteration 2, loss = 0.78780460\n",
      "Iteration 23, loss = 0.36670980\n",
      "Iteration 3, loss = 0.75035898\n",
      "Iteration 4, loss = 0.71245422\n",
      "Iteration 226, loss = 0.18645242\n",
      "Iteration 122, loss = 0.24971423\n",
      "Iteration 91, loss = 0.13949481\n",
      "Iteration 186, loss = 0.09070093\n",
      "Iteration 24, loss = 0.35996005\n",
      "Iteration 286, loss = 0.06868922\n",
      "Iteration 187, loss = 0.09050755\n",
      "Iteration 5, loss = 0.67586553\n",
      "Iteration 188, loss = 0.09032491\n",
      "Iteration 287, loss = 0.06855671\n",
      "Iteration 6, loss = 0.64304015\n",
      "Iteration 189, loss = 0.09012844\n",
      "Iteration 289, loss = 0.15170871\n",
      "Iteration 190, loss = 0.08994075\n",
      "Iteration 92, loss = 0.13878259\n",
      "Iteration 227, loss = 0.18642447\n",
      "Iteration 25, loss = 0.35365560\n",
      "Iteration 93, loss = 0.13803305\n",
      "Iteration 7, loss = 0.61399869\n",
      "Iteration 288, loss = 0.06842131\n",
      "Iteration 290, loss = 0.15165358\n",
      "Iteration 8, loss = 0.58814034\n",
      "Iteration 9, loss = 0.56525767\n",
      "Iteration 10, loss = 0.54383919\n",
      "Iteration 191, loss = 0.08975491\n",
      "Iteration 11, loss = 0.52560126\n",
      "Iteration 12, loss = 0.50856458\n",
      "Iteration 291, loss = 0.15154811\n",
      "Iteration 192, loss = 0.08956945\n",
      "Iteration 193, loss = 0.08939482\n",
      "Iteration 289, loss = 0.06830579\n",
      "Iteration 13, loss = 0.49389127\n",
      "Iteration 194, loss = 0.08921589\n",
      "Iteration 290, loss = 0.06815430\n",
      "Iteration 14, loss = 0.47968119\n",
      "Iteration 291, loss = 0.06802721\n",
      "Iteration 94, loss = 0.13735522\n",
      "Iteration 292, loss = 0.06790261\n",
      "Iteration 123, loss = 0.24920380\n",
      "Iteration 293, loss = 0.06776251\n",
      "Iteration 294, loss = 0.06762997\n",
      "Iteration 15, loss = 0.46753014\n",
      "Iteration 16, loss = 0.45622670\n",
      "Iteration 195, loss = 0.08903173\n",
      "Iteration 295, loss = 0.06750429\n",
      "Iteration 296, loss = 0.06737248\n",
      "Iteration 26, loss = 0.34738907\n",
      "Iteration 297, loss = 0.06725070\n",
      "Iteration 95, loss = 0.13665222\n",
      "Iteration 196, loss = 0.08885645\n",
      "Iteration 298, loss = 0.06712100\n",
      "Iteration 299, loss = 0.06698905\n",
      "Iteration 96, loss = 0.13602440\n",
      "Iteration 27, loss = 0.34139722\n",
      "Iteration 300, loss = 0.06687973\n",
      "Iteration 301, loss = 0.06674564\n",
      "Iteration 197, loss = 0.08868059\n",
      "Iteration 292, loss = 0.15146689\n",
      "Iteration 97, loss = 0.13533609\n",
      "Iteration 198, loss = 0.08850432\n",
      "Iteration 228, loss = 0.18620909\n",
      "Iteration 302, loss = 0.06662754\n",
      "Iteration 17, loss = 0.44623968\n",
      "Iteration 303, loss = 0.06651404\n",
      "Iteration 304, loss = 0.06637958\n",
      "Iteration 18, loss = 0.43708815\n",
      "Iteration 305, loss = 0.06629588\n",
      "Iteration 28, loss = 0.33568667\n",
      "Iteration 306, loss = 0.06614209\n",
      "Iteration 98, loss = 0.13467913\n",
      "Iteration 307, loss = 0.06602789\n",
      "Iteration 19, loss = 0.42881301\n",
      "Iteration 199, loss = 0.08834805\n",
      "Iteration 99, loss = 0.13405723\n",
      "Iteration 200, loss = 0.08816410\n",
      "Iteration 20, loss = 0.42129722\n",
      "Iteration 100, loss = 0.13344593Iteration 124, loss = 0.24870586\n",
      "Iteration 293, loss = 0.15136524\n",
      "Iteration 308, loss = 0.06590240\n",
      "\n",
      "Iteration 309, loss = 0.06578569\n",
      "Iteration 101, loss = 0.13285071\n",
      "Iteration 310, loss = 0.06566473\n",
      "Iteration 102, loss = 0.13224217\n",
      "Iteration 311, loss = 0.06556126\n",
      "Iteration 29, loss = 0.32990592\n",
      "Iteration 229, loss = 0.18608952\n",
      "Iteration 201, loss = 0.08800277\n",
      "Iteration 21, loss = 0.41423289\n",
      "Iteration 312, loss = 0.06543440\n",
      "Iteration 30, loss = 0.32445017\n",
      "Iteration 294, loss = 0.15127421\n",
      "Iteration 313, loss = 0.06531803\n",
      "Iteration 103, loss = 0.13164651\n",
      "Iteration 22, loss = 0.40788513\n",
      "Iteration 314, loss = 0.06519115\n",
      "Iteration 202, loss = 0.08784445\n",
      "Iteration 23, loss = 0.40197467\n",
      "Iteration 230, loss = 0.18595830\n",
      "Iteration 203, loss = 0.08767259\n",
      "Iteration 24, loss = 0.39652456\n",
      "Iteration 104, loss = 0.13112057\n",
      "Iteration 204, loss = 0.08750592\n",
      "Iteration 25, loss = 0.39148212\n",
      "Iteration 295, loss = 0.15119615\n",
      "Iteration 26, loss = 0.38656134\n",
      "Iteration 315, loss = 0.06508196\n",
      "Iteration 205, loss = 0.08734720\n",
      "Iteration 316, loss = 0.06496553\n",
      "Iteration 206, loss = 0.08718139\n",
      "Iteration 105, loss = 0.13051640\n",
      "Iteration 296, loss = 0.15119252\n",
      "Iteration 317, loss = 0.06485253\n",
      "Iteration 125, loss = 0.24810883\n",
      "Iteration 27, loss = 0.38212296\n",
      "Iteration 207, loss = 0.08702740\n",
      "Iteration 28, loss = 0.37779047\n",
      "Iteration 208, loss = 0.08686200\n",
      "Iteration 29, loss = 0.37355873\n",
      "Iteration 318, loss = 0.06474634\n",
      "Iteration 30, loss = 0.36956067\n",
      "Iteration 319, loss = 0.06463969\n",
      "Iteration 209, loss = 0.08671725\n",
      "Iteration 31, loss = 0.36556337\n",
      "Iteration 320, loss = 0.06454416\n",
      "Iteration 31, loss = 0.31914011\n",
      "Iteration 32, loss = 0.36183312\n",
      "Iteration 33, loss = 0.35798199\n",
      "Iteration 34, loss = 0.35431254\n",
      "Iteration 297, loss = 0.15101561\n",
      "Iteration 231, loss = 0.18582790\n",
      "Iteration 106, loss = 0.12995551\n",
      "Iteration 210, loss = 0.08656869\n",
      "Iteration 321, loss = 0.06441635\n",
      "Iteration 32, loss = 0.31393207\n",
      "Iteration 322, loss = 0.06433587\n",
      "Iteration 323, loss = 0.06420047\n",
      "Iteration 324, loss = 0.06410676\n",
      "Iteration 325, loss = 0.06400867\n",
      "Iteration 107, loss = 0.12941100\n",
      "Iteration 326, loss = 0.06388807\n",
      "Iteration 211, loss = 0.08640068\n",
      "Iteration 327, loss = 0.06378854\n",
      "Iteration 35, loss = 0.35065894\n",
      "Iteration 328, loss = 0.06368265\n",
      "Iteration 329, loss = 0.06358824\n",
      "Iteration 108, loss = 0.12889814\n",
      "Iteration 330, loss = 0.06348717\n",
      "Iteration 298, loss = 0.15090557\n",
      "Iteration 331, loss = 0.06339029\n",
      "Iteration 109, loss = 0.12835257\n",
      "Iteration 36, loss = 0.34711053\n",
      "Iteration 33, loss = 0.30884315\n",
      "Iteration 126, loss = 0.24759012\n",
      "Iteration 332, loss = 0.06328952\n",
      "Iteration 110, loss = 0.12785199\n",
      "Iteration 333, loss = 0.06319645\n",
      "Iteration 212, loss = 0.08625318\n",
      "Iteration 37, loss = 0.34357151\n",
      "Iteration 38, loss = 0.34006922\n",
      "Iteration 213, loss = 0.08609955\n",
      "Iteration 34, loss = 0.30395427\n",
      "Iteration 214, loss = 0.08594802\n",
      "Iteration 39, loss = 0.33661511\n",
      "Iteration 299, loss = 0.15085099\n",
      "Iteration 40, loss = 0.33312067\n",
      "Iteration 111, loss = 0.12735843\n",
      "Iteration 334, loss = 0.06307840\n",
      "Iteration 335, loss = 0.06297514\n",
      "Iteration 336, loss = 0.06290254\n",
      "Iteration 337, loss = 0.06278936\n",
      "Iteration 232, loss = 0.18568948\n",
      "Iteration 338, loss = 0.06269630\n",
      "Iteration 215, loss = 0.08581556\n",
      "Iteration 216, loss = 0.08565779\n",
      "Iteration 339, loss = 0.06260105\n",
      "Iteration 41, loss = 0.32971541\n",
      "Iteration 217, loss = 0.08551099\n",
      "Iteration 42, loss = 0.32621590\n",
      "Iteration 43, loss = 0.32289888\n",
      "Iteration 112, loss = 0.12684575\n",
      "Iteration 300, loss = 0.15077413Iteration 44, loss = 0.31948734\n",
      "Iteration 35, loss = 0.29912906\n",
      "\n",
      "Iteration 113, loss = 0.12636233\n",
      "Iteration 114, loss = 0.12587292\n",
      "Iteration 218, loss = 0.08536829\n",
      "Iteration 340, loss = 0.06250006\n",
      "Iteration 45, loss = 0.31602149\n",
      "Iteration 341, loss = 0.06241742\n",
      "Iteration 342, loss = 0.06233481\n",
      "Iteration 343, loss = 0.06222306\n",
      "Iteration 127, loss = 0.24710885\n",
      "Iteration 301, loss = 0.15067964\n",
      "Iteration 46, loss = 0.31267095\n",
      "Iteration 233, loss = 0.18560265\n",
      "Iteration 219, loss = 0.08521813\n",
      "Iteration 36, loss = 0.29448140\n",
      "Iteration 344, loss = 0.06213082\n",
      "Iteration 47, loss = 0.30934210\n",
      "Iteration 345, loss = 0.06203963\n",
      "Iteration 115, loss = 0.12542138\n",
      "Iteration 346, loss = 0.06194919Iteration 116, loss = 0.12496055\n",
      "Iteration 220, loss = 0.08508292\n",
      "\n",
      "Iteration 48, loss = 0.30594592\n",
      "Iteration 221, loss = 0.08494036\n",
      "Iteration 49, loss = 0.30256866\n",
      "Iteration 347, loss = 0.06185986\n",
      "Iteration 117, loss = 0.12448414\n",
      "Iteration 348, loss = 0.06176753\n",
      "Iteration 302, loss = 0.15060924\n",
      "Iteration 50, loss = 0.29920686\n",
      "Iteration 349, loss = 0.06169641\n",
      "Iteration 37, loss = 0.29002031\n",
      "Iteration 222, loss = 0.08479507\n",
      "Iteration 350, loss = 0.06159898\n",
      "Iteration 351, loss = 0.06150429\n",
      "Iteration 51, loss = 0.29589287\n",
      "Iteration 52, loss = 0.29263265\n",
      "Iteration 118, loss = 0.12404903\n",
      "Iteration 53, loss = 0.28919359\n",
      "Iteration 223, loss = 0.08466617\n",
      "Iteration 128, loss = 0.24662678\n",
      "Iteration 119, loss = 0.12359221\n",
      "Iteration 54, loss = 0.28593153\n",
      "Iteration 234, loss = 0.18547807\n",
      "Iteration 55, loss = 0.28256005\n",
      "Iteration 224, loss = 0.08451358\n",
      "Iteration 38, loss = 0.28563518\n",
      "Iteration 352, loss = 0.06141696\n",
      "Iteration 56, loss = 0.27925662\n",
      "Iteration 353, loss = 0.06133891\n",
      "Iteration 120, loss = 0.12315684\n",
      "Iteration 354, loss = 0.06124027\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 303, loss = 0.15054056\n",
      "Iteration 225, loss = 0.08438678\n",
      "Iteration 57, loss = 0.27595018\n",
      "Iteration 39, loss = 0.28146399\n",
      "Iteration 226, loss = 0.08424512\n",
      "Iteration 1, loss = 0.79412371\n",
      "Iteration 227, loss = 0.08410826\n",
      "Iteration 58, loss = 0.27264895\n",
      "Iteration 121, loss = 0.12273881\n",
      "Iteration 122, loss = 0.12229083\n",
      "Iteration 2, loss = 0.75507519\n",
      "Iteration 235, loss = 0.18534283\n",
      "Iteration 304, loss = 0.15052267\n",
      "Iteration 123, loss = 0.12186832\n",
      "Iteration 228, loss = 0.08398051\n",
      "Iteration 59, loss = 0.26933300\n",
      "Iteration 60, loss = 0.26608405\n",
      "Iteration 229, loss = 0.08385067\n",
      "Iteration 124, loss = 0.12147569\n",
      "Iteration 40, loss = 0.27733981\n",
      "Iteration 61, loss = 0.26276471\n",
      "Iteration 230, loss = 0.08371515\n",
      "Iteration 62, loss = 0.25945914\n",
      "Iteration 231, loss = 0.08359017\n",
      "Iteration 3, loss = 0.70534351\n",
      "Iteration 232, loss = 0.08345552\n",
      "Iteration 129, loss = 0.24608722\n",
      "Iteration 233, loss = 0.08332492\n",
      "Iteration 305, loss = 0.15035840\n",
      "Iteration 234, loss = 0.08320140\n",
      "Iteration 63, loss = 0.25617041\n",
      "Iteration 4, loss = 0.65620906\n",
      "Iteration 235, loss = 0.08307723\n",
      "Iteration 64, loss = 0.25298775\n",
      "Iteration 125, loss = 0.12108258\n",
      "Iteration 236, loss = 0.08294763\n",
      "Iteration 5, loss = 0.61265673\n",
      "Iteration 237, loss = 0.08284640\n",
      "Iteration 126, loss = 0.12067092\n",
      "Iteration 65, loss = 0.24974145\n",
      "Iteration 236, loss = 0.18523622\n",
      "Iteration 238, loss = 0.08270196\n",
      "Iteration 66, loss = 0.24661504\n",
      "Iteration 41, loss = 0.27341304\n",
      "Iteration 239, loss = 0.08258697\n",
      "Iteration 67, loss = 0.24337543\n",
      "Iteration 240, loss = 0.08247356\n",
      "Iteration 306, loss = 0.15028949\n",
      "Iteration 127, loss = 0.12027000\n",
      "Iteration 241, loss = 0.08234642\n",
      "Iteration 68, loss = 0.24028678\n",
      "Iteration 242, loss = 0.08222507\n",
      "Iteration 6, loss = 0.57622865\n",
      "Iteration 42, loss = 0.26942585\n",
      "Iteration 243, loss = 0.08210216\n",
      "Iteration 244, loss = 0.08199743\n",
      "Iteration 69, loss = 0.23707856\n",
      "Iteration 128, loss = 0.11989822\n",
      "Iteration 307, loss = 0.15022230\n",
      "Iteration 7, loss = 0.54593483\n",
      "Iteration 130, loss = 0.24564553\n",
      "Iteration 70, loss = 0.23398066\n",
      "Iteration 129, loss = 0.11950797\n",
      "Iteration 8, loss = 0.52094071\n",
      "Iteration 43, loss = 0.26572155\n",
      "Iteration 71, loss = 0.23090921\n",
      "Iteration 308, loss = 0.15016033\n",
      "Iteration 72, loss = 0.22785708\n",
      "Iteration 237, loss = 0.18509356\n",
      "Iteration 9, loss = 0.50014980\n",
      "Iteration 44, loss = 0.26207004\n",
      "Iteration 73, loss = 0.22480606\n",
      "Iteration 130, loss = 0.11913048\n",
      "Iteration 131, loss = 0.11875074\n",
      "Iteration 10, loss = 0.48316226\n",
      "Iteration 309, loss = 0.15002814\n",
      "Iteration 132, loss = 0.11839992Iteration 74, loss = 0.22172912\n",
      "Iteration 238, loss = 0.18499530\n",
      "\n",
      "Iteration 75, loss = 0.21883397\n",
      "Iteration 45, loss = 0.25853818\n",
      "Iteration 76, loss = 0.21584705\n",
      "Iteration 133, loss = 0.11802472\n",
      "Iteration 245, loss = 0.08187644\n",
      "Iteration 77, loss = 0.21295276\n",
      "Iteration 78, loss = 0.21009739\n",
      "Iteration 131, loss = 0.24510477\n",
      "Iteration 134, loss = 0.11770472\n",
      "Iteration 79, loss = 0.20733295\n",
      "Iteration 310, loss = 0.14996687\n",
      "Iteration 80, loss = 0.20448142\n",
      "Iteration 135, loss = 0.11731077\n",
      "Iteration 81, loss = 0.20171913\n",
      "Iteration 11, loss = 0.46862661\n",
      "Iteration 82, loss = 0.19903898\n",
      "Iteration 136, loss = 0.11695865\n",
      "Iteration 246, loss = 0.08175901\n",
      "Iteration 83, loss = 0.19640238\n",
      "Iteration 46, loss = 0.25509994\n",
      "Iteration 137, loss = 0.11662409\n",
      "Iteration 84, loss = 0.19382404\n",
      "Iteration 85, loss = 0.19123813\n",
      "Iteration 138, loss = 0.11627205\n",
      "Iteration 12, loss = 0.45629585\n",
      "Iteration 86, loss = 0.18875654\n",
      "Iteration 139, loss = 0.11592882\n",
      "Iteration 47, loss = 0.25181907\n",
      "Iteration 13, loss = 0.44557441\n",
      "Iteration 140, loss = 0.11562512\n",
      "Iteration 239, loss = 0.18486301\n",
      "Iteration 247, loss = 0.08164642\n",
      "Iteration 132, loss = 0.24470097\n",
      "Iteration 87, loss = 0.18635129\n",
      "Iteration 311, loss = 0.14995320Iteration 248, loss = 0.08153864\n",
      "\n",
      "Iteration 14, loss = 0.43617422\n",
      "Iteration 249, loss = 0.08142014\n",
      "Iteration 312, loss = 0.14981437\n",
      "Iteration 48, loss = 0.24860838\n",
      "Iteration 250, loss = 0.08130970\n",
      "Iteration 240, loss = 0.18478919\n",
      "Iteration 251, loss = 0.08119240\n",
      "Iteration 88, loss = 0.18392598\n",
      "Iteration 15, loss = 0.42786394\n",
      "Iteration 89, loss = 0.18161894\n",
      "Iteration 252, loss = 0.08108031\n",
      "Iteration 49, loss = 0.24552770\n",
      "Iteration 313, loss = 0.14972079\n",
      "Iteration 90, loss = 0.17941865\n",
      "Iteration 253, loss = 0.08097508\n",
      "Iteration 91, loss = 0.17712804\n",
      "Iteration 16, loss = 0.42023581\n",
      "Iteration 141, loss = 0.11527959\n",
      "Iteration 17, loss = 0.41310945\n",
      "Iteration 142, loss = 0.11495609\n",
      "Iteration 92, loss = 0.17503401\n",
      "Iteration 143, loss = 0.11463353\n",
      "Iteration 93, loss = 0.17296910\n",
      "Iteration 241, loss = 0.18465363\n",
      "Iteration 94, loss = 0.17085218Iteration 144, loss = 0.11431308\n",
      "\n",
      "Iteration 254, loss = 0.08088006\n",
      "Iteration 50, loss = 0.24253538\n",
      "Iteration 133, loss = 0.24421065\n",
      "Iteration 314, loss = 0.14965149\n",
      "Iteration 95, loss = 0.16878947\n",
      "Iteration 255, loss = 0.08075341\n",
      "Iteration 18, loss = 0.40643746\n",
      "Iteration 145, loss = 0.11400979\n",
      "Iteration 256, loss = 0.08065798Iteration 96, loss = 0.16681132\n",
      "\n",
      "Iteration 97, loss = 0.16489611\n",
      "Iteration 51, loss = 0.23954309\n",
      "Iteration 98, loss = 0.16300653\n",
      "Iteration 146, loss = 0.11369734\n",
      "Iteration 19, loss = 0.40008875\n",
      "Iteration 99, loss = 0.16110107\n",
      "Iteration 147, loss = 0.11337785\n",
      "Iteration 257, loss = 0.08053847\n",
      "Iteration 100, loss = 0.15928055\n",
      "Iteration 101, loss = 0.15753437\n",
      "Iteration 148, loss = 0.11310481\n",
      "Iteration 102, loss = 0.15574343\n",
      "Iteration 52, loss = 0.23671972\n",
      "Iteration 103, loss = 0.15402634\n",
      "Iteration 20, loss = 0.39394432\n",
      "Iteration 315, loss = 0.14956935\n",
      "Iteration 242, loss = 0.18452691\n",
      "Iteration 258, loss = 0.08044405\n",
      "Iteration 149, loss = 0.11278767\n",
      "Iteration 104, loss = 0.15242240\n",
      "Iteration 134, loss = 0.24375319\n",
      "Iteration 21, loss = 0.38804863\n",
      "Iteration 316, loss = 0.14951895\n",
      "Iteration 259, loss = 0.08033779\n",
      "Iteration 150, loss = 0.11250987\n",
      "Iteration 105, loss = 0.15069047\n",
      "Iteration 106, loss = 0.14912933\n",
      "Iteration 53, loss = 0.23401016\n",
      "Iteration 151, loss = 0.11221638\n",
      "Iteration 107, loss = 0.14757184\n",
      "Iteration 260, loss = 0.08022156\n",
      "Iteration 22, loss = 0.38217474\n",
      "Iteration 108, loss = 0.14603866\n",
      "Iteration 261, loss = 0.08012784\n",
      "Iteration 109, loss = 0.14460988\n",
      "Iteration 110, loss = 0.14305845\n",
      "Iteration 111, loss = 0.14161917\n",
      "Iteration 317, loss = 0.14943676\n",
      "Iteration 262, loss = 0.08002088\n",
      "Iteration 112, loss = 0.14021655\n",
      "Iteration 54, loss = 0.23134190\n",
      "Iteration 263, loss = 0.07991610\n",
      "Iteration 152, loss = 0.11192086\n",
      "Iteration 243, loss = 0.18440813\n",
      "Iteration 23, loss = 0.37634853\n",
      "Iteration 264, loss = 0.07981072\n",
      "Iteration 153, loss = 0.11165062\n",
      "Iteration 24, loss = 0.37080622\n",
      "Iteration 113, loss = 0.13887023\n",
      "Iteration 114, loss = 0.13752202\n",
      "Iteration 265, loss = 0.07971142\n",
      "Iteration 115, loss = 0.13624986\n",
      "Iteration 154, loss = 0.11137511\n",
      "Iteration 116, loss = 0.13490335\n",
      "Iteration 55, loss = 0.22884921\n",
      "Iteration 117, loss = 0.13365790\n",
      "Iteration 155, loss = 0.11108941\n",
      "Iteration 266, loss = 0.07960406\n",
      "Iteration 318, loss = 0.14938623\n",
      "Iteration 135, loss = 0.24328192\n",
      "Iteration 118, loss = 0.13244264\n",
      "Iteration 56, loss = 0.22624914\n",
      "Iteration 25, loss = 0.36536179\n",
      "Iteration 119, loss = 0.13125080\n",
      "Iteration 26, loss = 0.35981356\n",
      "Iteration 57, loss = 0.22384322\n",
      "Iteration 120, loss = 0.12999950\n",
      "Iteration 267, loss = 0.07952181\n",
      "Iteration 136, loss = 0.24289174\n",
      "Iteration 156, loss = 0.11081281\n",
      "Iteration 319, loss = 0.14928879\n",
      "Iteration 121, loss = 0.12886881\n",
      "Iteration 268, loss = 0.07941156\n",
      "Iteration 269, loss = 0.07930830\n",
      "Iteration 27, loss = 0.35441377\n",
      "Iteration 244, loss = 0.18430568\n",
      "Iteration 157, loss = 0.11054527\n",
      "Iteration 122, loss = 0.12774106\n",
      "Iteration 158, loss = 0.11028148\n",
      "Iteration 28, loss = 0.34908588\n",
      "Iteration 159, loss = 0.11000893\n",
      "Iteration 270, loss = 0.07920966\n",
      "Iteration 271, loss = 0.07911840\n",
      "Iteration 272, loss = 0.07901116\n",
      "Iteration 123, loss = 0.12668287\n",
      "Iteration 320, loss = 0.14920246\n",
      "Iteration 29, loss = 0.34383492\n",
      "Iteration 124, loss = 0.12555287\n",
      "Iteration 273, loss = 0.07890851\n",
      "Iteration 160, loss = 0.10974525\n",
      "Iteration 245, loss = 0.18419789\n",
      "Iteration 125, loss = 0.12452054\n",
      "Iteration 58, loss = 0.22151675\n",
      "Iteration 126, loss = 0.12351160\n",
      "Iteration 274, loss = 0.07881387\n",
      "Iteration 321, loss = 0.14912522\n",
      "Iteration 161, loss = 0.10948204\n",
      "Iteration 275, loss = 0.07872848\n",
      "Iteration 127, loss = 0.12248359\n",
      "Iteration 128, loss = 0.12151427\n",
      "Iteration 162, loss = 0.10923717\n",
      "Iteration 129, loss = 0.12051371\n",
      "Iteration 59, loss = 0.21927478\n",
      "Iteration 130, loss = 0.11962271\n",
      "Iteration 276, loss = 0.07864973\n",
      "Iteration 30, loss = 0.33848231\n",
      "Iteration 31, loss = 0.33334431\n",
      "Iteration 163, loss = 0.10897405\n",
      "Iteration 277, loss = 0.07853397\n",
      "Iteration 60, loss = 0.21711070\n",
      "Iteration 131, loss = 0.11865909\n",
      "Iteration 132, loss = 0.11776568\n",
      "Iteration 278, loss = 0.07843820\n",
      "Iteration 133, loss = 0.11688438\n",
      "Iteration 279, loss = 0.07835273\n",
      "Iteration 322, loss = 0.14905958\n",
      "Iteration 32, loss = 0.32826661\n",
      "Iteration 164, loss = 0.10871327\n",
      "Iteration 134, loss = 0.11600869\n",
      "Iteration 137, loss = 0.24242184\n",
      "Iteration 246, loss = 0.18411475\n",
      "Iteration 165, loss = 0.10847099\n",
      "Iteration 61, loss = 0.21498202\n",
      "Iteration 323, loss = 0.14903080\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 280, loss = 0.07824723\n",
      "Iteration 135, loss = 0.11513524\n",
      "Iteration 62, loss = 0.21290567\n",
      "Iteration 33, loss = 0.32305359\n",
      "Iteration 136, loss = 0.11436699\n",
      "Iteration 281, loss = 0.07816636\n",
      "Iteration 166, loss = 0.10824037\n",
      "Iteration 247, loss = 0.18398083\n",
      "Iteration 167, loss = 0.10797183\n",
      "Iteration 137, loss = 0.11346749\n",
      "Iteration 138, loss = 0.11266526\n",
      "Iteration 282, loss = 0.07807766\n",
      "Iteration 34, loss = 0.31802776\n",
      "Iteration 283, loss = 0.07798348Iteration 139, loss = 0.11189769\n",
      "\n",
      "Iteration 138, loss = 0.24197267\n",
      "Iteration 168, loss = 0.10773620\n",
      "Iteration 284, loss = 0.07788427\n",
      "Iteration 35, loss = 0.31303463Iteration 169, loss = 0.10751334\n",
      "\n",
      "Iteration 140, loss = 0.11114492\n",
      "Iteration 141, loss = 0.11037181\n",
      "Iteration 63, loss = 0.21096008\n",
      "Iteration 36, loss = 0.30818156\n",
      "Iteration 1, loss = 0.87643093\n",
      "Iteration 142, loss = 0.10966809\n",
      "Iteration 170, loss = 0.10725166\n",
      "Iteration 285, loss = 0.07780624\n",
      "Iteration 143, loss = 0.10891757\n",
      "Iteration 286, loss = 0.07770420\n",
      "Iteration 144, loss = 0.10819689\n",
      "Iteration 287, loss = 0.07763692\n",
      "Iteration 145, loss = 0.10750290\n",
      "Iteration 248, loss = 0.18393083\n",
      "Iteration 146, loss = 0.10680991\n",
      "Iteration 147, loss = 0.10613726\n",
      "Iteration 64, loss = 0.20902477\n",
      "Iteration 171, loss = 0.10702542\n",
      "Iteration 37, loss = 0.30344108\n",
      "Iteration 288, loss = 0.07753413\n",
      "Iteration 2, loss = 0.83527368\n",
      "Iteration 172, loss = 0.10677830\n",
      "Iteration 173, loss = 0.10656639\n",
      "Iteration 148, loss = 0.10549284\n",
      "Iteration 149, loss = 0.10486708\n",
      "Iteration 65, loss = 0.20712775\n",
      "Iteration 249, loss = 0.18379820\n",
      "Iteration 289, loss = 0.07744704Iteration 38, loss = 0.29842784\n",
      "Iteration 139, loss = 0.24159119\n",
      "\n",
      "Iteration 174, loss = 0.10632212\n",
      "Iteration 290, loss = 0.07735791\n",
      "Iteration 150, loss = 0.10424276\n",
      "Iteration 39, loss = 0.29375258\n",
      "Iteration 151, loss = 0.10361187\n",
      "Iteration 3, loss = 0.78146867\n",
      "Iteration 66, loss = 0.20526680\n",
      "Iteration 152, loss = 0.10301256\n",
      "Iteration 175, loss = 0.10611100\n",
      "Iteration 153, loss = 0.10247211\n",
      "Iteration 176, loss = 0.10586906\n",
      "Iteration 291, loss = 0.07726776\n",
      "Iteration 154, loss = 0.10183533\n",
      "Iteration 292, loss = 0.07718522\n",
      "Iteration 40, loss = 0.28927445\n",
      "Iteration 67, loss = 0.20355595\n",
      "Iteration 140, loss = 0.24114772\n",
      "Iteration 155, loss = 0.10129915\n",
      "Iteration 41, loss = 0.28466309\n",
      "Iteration 293, loss = 0.07709228\n",
      "Iteration 177, loss = 0.10565616\n",
      "Iteration 4, loss = 0.73148708\n",
      "Iteration 156, loss = 0.10074061\n",
      "Iteration 294, loss = 0.07701628\n",
      "Iteration 157, loss = 0.10019725\n",
      "Iteration 295, loss = 0.07692263\n",
      "Iteration 250, loss = 0.18368500\n",
      "Iteration 68, loss = 0.20177558\n",
      "Iteration 296, loss = 0.07684781\n",
      "Iteration 158, loss = 0.09974383\n",
      "Iteration 159, loss = 0.09916532\n",
      "Iteration 160, loss = 0.09867722\n",
      "Iteration 161, loss = 0.09816399\n",
      "Iteration 42, loss = 0.28007840\n",
      "Iteration 178, loss = 0.10543231\n",
      "Iteration 5, loss = 0.68710481\n",
      "Iteration 43, loss = 0.27563001\n",
      "Iteration 141, loss = 0.24077357\n",
      "Iteration 6, loss = 0.65055458\n",
      "Iteration 162, loss = 0.09769117\n",
      "Iteration 179, loss = 0.10523194\n",
      "Iteration 163, loss = 0.09719032\n",
      "Iteration 297, loss = 0.07675535\n",
      "Iteration 164, loss = 0.09673703\n",
      "Iteration 180, loss = 0.10500187\n",
      "Iteration 69, loss = 0.20010449\n",
      "Iteration 165, loss = 0.09626758\n",
      "Iteration 166, loss = 0.09581465\n",
      "Iteration 181, loss = 0.10478663\n",
      "Iteration 298, loss = 0.07667434\n",
      "Iteration 299, loss = 0.07660246\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 251, loss = 0.18357338\n",
      "Iteration 167, loss = 0.09536604\n",
      "Iteration 44, loss = 0.27141504\n",
      "Iteration 70, loss = 0.19846427\n",
      "Iteration 168, loss = 0.09492744\n",
      "Iteration 182, loss = 0.10457493\n",
      "Iteration 183, loss = 0.10437751\n",
      "Iteration 71, loss = 0.19690050\n",
      "Iteration 1, loss = 0.63238883\n",
      "Iteration 45, loss = 0.26710149\n",
      "Iteration 2, loss = 0.61516874\n",
      "Iteration 252, loss = 0.18348283\n",
      "Iteration 7, loss = 0.62158682\n",
      "Iteration 3, loss = 0.59152175\n",
      "Iteration 46, loss = 0.26272813\n",
      "Iteration 169, loss = 0.09449626\n",
      "Iteration 184, loss = 0.10415097Iteration 47, loss = 0.25865773\n",
      "\n",
      "Iteration 170, loss = 0.09410291\n",
      "Iteration 4, loss = 0.56542357\n",
      "Iteration 185, loss = 0.10396233\n",
      "Iteration 72, loss = 0.19535645\n",
      "Iteration 142, loss = 0.24035125\n",
      "Iteration 186, loss = 0.10375942\n",
      "Iteration 171, loss = 0.09370883\n",
      "Iteration 172, loss = 0.09328091\n",
      "Iteration 187, loss = 0.10354875\n",
      "Iteration 5, loss = 0.54030414\n",
      "Iteration 173, loss = 0.09289279\n",
      "Iteration 188, loss = 0.10335066\n",
      "Iteration 73, loss = 0.19386485\n",
      "Iteration 174, loss = 0.09250958\n",
      "Iteration 6, loss = 0.51666554\n",
      "Iteration 253, loss = 0.18339148Iteration 8, loss = 0.59572525\n",
      "Iteration 175, loss = 0.09211553\n",
      "\n",
      "Iteration 7, loss = 0.49529538\n",
      "Iteration 48, loss = 0.25471318\n",
      "Iteration 189, loss = 0.10313751\n",
      "Iteration 176, loss = 0.09175149\n",
      "Iteration 74, loss = 0.19237882\n",
      "Iteration 177, loss = 0.09138957\n",
      "Iteration 49, loss = 0.25076531\n",
      "Iteration 178, loss = 0.09099982\n",
      "Iteration 190, loss = 0.10295591\n",
      "Iteration 8, loss = 0.47680993\n",
      "Iteration 179, loss = 0.09066446\n",
      "Iteration 143, loss = 0.23992811\n",
      "Iteration 191, loss = 0.10275441\n",
      "Iteration 180, loss = 0.09031952\n",
      "Iteration 75, loss = 0.19096914\n",
      "Iteration 192, loss = 0.10256069\n",
      "Iteration 9, loss = 0.57558970Iteration 9, loss = 0.45971238\n",
      "Iteration 50, loss = 0.24685276\n",
      "\n",
      "Iteration 254, loss = 0.18324514\n",
      "Iteration 181, loss = 0.08996247\n",
      "Iteration 10, loss = 0.44556389\n",
      "Iteration 51, loss = 0.24318660\n",
      "Iteration 11, loss = 0.43318146\n",
      "Iteration 182, loss = 0.08962451\n",
      "Iteration 193, loss = 0.10237875\n",
      "Iteration 12, loss = 0.42261642\n",
      "Iteration 52, loss = 0.23955629\n",
      "Iteration 144, loss = 0.23958466\n",
      "Iteration 76, loss = 0.18958736\n",
      "Iteration 13, loss = 0.41336959\n",
      "Iteration 194, loss = 0.10217688\n",
      "Iteration 53, loss = 0.23606666\n",
      "Iteration 14, loss = 0.40591137\n",
      "Iteration 10, loss = 0.55903924\n",
      "Iteration 15, loss = 0.39871490\n",
      "Iteration 16, loss = 0.39304629\n",
      "Iteration 54, loss = 0.23263986\n",
      "Iteration 183, loss = 0.08930351\n",
      "Iteration 77, loss = 0.18834987\n",
      "Iteration 195, loss = 0.10200825\n",
      "Iteration 17, loss = 0.38760894\n",
      "Iteration 55, loss = 0.22924452\n",
      "Iteration 184, loss = 0.08897262\n",
      "Iteration 255, loss = 0.18312796\n",
      "Iteration 185, loss = 0.08867346\n",
      "Iteration 186, loss = 0.08836106\n",
      "Iteration 196, loss = 0.10180455\n",
      "Iteration 187, loss = 0.08805172\n",
      "Iteration 18, loss = 0.38264437\n",
      "Iteration 197, loss = 0.10164182\n",
      "Iteration 188, loss = 0.08777148\n",
      "Iteration 78, loss = 0.18697027\n",
      "Iteration 145, loss = 0.23922283\n",
      "Iteration 189, loss = 0.08742915\n",
      "Iteration 79, loss = 0.18567420\n",
      "Iteration 19, loss = 0.37820889\n",
      "Iteration 198, loss = 0.10143399\n",
      "Iteration 11, loss = 0.54462068\n",
      "Iteration 56, loss = 0.22617370\n",
      "Iteration 190, loss = 0.08712389\n",
      "Iteration 20, loss = 0.37407034\n",
      "Iteration 199, loss = 0.10125276\n",
      "Iteration 21, loss = 0.37010264\n",
      "Iteration 80, loss = 0.18446461\n",
      "Iteration 22, loss = 0.36628935\n",
      "Iteration 191, loss = 0.08684606\n",
      "Iteration 200, loss = 0.10106689\n",
      "Iteration 23, loss = 0.36266006\n",
      "Iteration 192, loss = 0.08656496\n",
      "Iteration 57, loss = 0.22314830\n",
      "Iteration 256, loss = 0.18304544\n",
      "Iteration 193, loss = 0.08627781\n",
      "Iteration 194, loss = 0.08602062\n",
      "Iteration 195, loss = 0.08572925\n",
      "Iteration 196, loss = 0.08546806\n",
      "Iteration 197, loss = 0.08520272\n",
      "Iteration 201, loss = 0.10089182\n",
      "Iteration 12, loss = 0.53221692\n",
      "Iteration 202, loss = 0.10070778\n",
      "Iteration 146, loss = 0.23883501\n",
      "Iteration 24, loss = 0.35897937\n",
      "Iteration 58, loss = 0.22003163\n",
      "Iteration 25, loss = 0.35543299\n",
      "Iteration 203, loss = 0.10055812\n",
      "Iteration 59, loss = 0.21717828\n",
      "Iteration 81, loss = 0.18324597\n",
      "Iteration 26, loss = 0.35188370\n",
      "Iteration 204, loss = 0.10036964\n",
      "Iteration 27, loss = 0.34840243\n",
      "Iteration 198, loss = 0.08496067\n",
      "Iteration 257, loss = 0.18294021\n",
      "Iteration 199, loss = 0.08471294\n",
      "Iteration 200, loss = 0.08445075\n",
      "Iteration 28, loss = 0.34487237\n",
      "Iteration 60, loss = 0.21431259\n",
      "Iteration 205, loss = 0.10018228\n",
      "Iteration 201, loss = 0.08420529\n",
      "Iteration 202, loss = 0.08395031\n",
      "Iteration 203, loss = 0.08372544\n",
      "Iteration 29, loss = 0.34139707\n",
      "Iteration 61, loss = 0.21168059\n",
      "Iteration 13, loss = 0.52144659\n",
      "Iteration 30, loss = 0.33792678\n",
      "Iteration 82, loss = 0.18208717\n",
      "Iteration 31, loss = 0.33446048\n",
      "Iteration 206, loss = 0.10001731\n",
      "Iteration 32, loss = 0.33098614\n",
      "Iteration 204, loss = 0.08346069\n",
      "Iteration 147, loss = 0.23841885\n",
      "Iteration 83, loss = 0.18091664\n",
      "Iteration 62, loss = 0.20904391\n",
      "Iteration 33, loss = 0.32742042\n",
      "Iteration 207, loss = 0.09984132\n",
      "Iteration 205, loss = 0.08330536\n",
      "Iteration 258, loss = 0.18287110\n",
      "Iteration 14, loss = 0.51207708\n",
      "Iteration 84, loss = 0.17980768\n",
      "Iteration 208, loss = 0.09966555\n",
      "Iteration 85, loss = 0.17873628\n",
      "Iteration 63, loss = 0.20663199\n",
      "Iteration 206, loss = 0.08301815\n",
      "Iteration 34, loss = 0.32396460\n",
      "Iteration 207, loss = 0.08279608\n",
      "Iteration 15, loss = 0.50304074\n",
      "Iteration 208, loss = 0.08255640\n",
      "Iteration 35, loss = 0.32037547\n",
      "Iteration 209, loss = 0.09950036\n",
      "Iteration 36, loss = 0.31681274\n",
      "Iteration 148, loss = 0.23810151\n",
      "Iteration 210, loss = 0.09936723\n",
      "Iteration 209, loss = 0.08233424\n",
      "Iteration 64, loss = 0.20423401\n",
      "Iteration 37, loss = 0.31335721\n",
      "Iteration 210, loss = 0.08214514\n",
      "Iteration 211, loss = 0.09916621\n",
      "Iteration 211, loss = 0.08192879\n",
      "Iteration 38, loss = 0.30980863\n",
      "Iteration 212, loss = 0.09903182\n",
      "Iteration 86, loss = 0.17767658\n",
      "Iteration 65, loss = 0.20183257\n",
      "Iteration 259, loss = 0.18276428\n",
      "Iteration 213, loss = 0.09884549\n",
      "Iteration 212, loss = 0.08168984\n",
      "Iteration 213, loss = 0.08148590Iteration 149, loss = 0.23767848\n",
      "\n",
      "Iteration 39, loss = 0.30637392\n",
      "Iteration 40, loss = 0.30289415\n",
      "Iteration 41, loss = 0.29928265\n",
      "Iteration 214, loss = 0.08128389\n",
      "Iteration 66, loss = 0.19967557\n",
      "Iteration 16, loss = 0.49513025\n",
      "Iteration 87, loss = 0.17665542\n",
      "Iteration 215, loss = 0.08107166Iteration 42, loss = 0.29589452\n",
      "\n",
      "Iteration 260, loss = 0.18276357\n",
      "Iteration 43, loss = 0.29252042\n",
      "Iteration 67, loss = 0.19742865\n",
      "Iteration 216, loss = 0.08090492\n",
      "Iteration 217, loss = 0.08067787\n",
      "Iteration 44, loss = 0.28926180\n",
      "Iteration 218, loss = 0.08047659\n",
      "Iteration 214, loss = 0.09868265\n",
      "Iteration 88, loss = 0.17566027\n",
      "Iteration 68, loss = 0.19545104\n",
      "Iteration 219, loss = 0.08027887\n",
      "Iteration 215, loss = 0.09852629\n",
      "Iteration 220, loss = 0.08009952\n",
      "Iteration 17, loss = 0.48770735\n",
      "Iteration 45, loss = 0.28591053\n",
      "Iteration 221, loss = 0.07990379\n",
      "Iteration 216, loss = 0.09837217\n",
      "Iteration 46, loss = 0.28277532\n",
      "Iteration 89, loss = 0.17466590\n",
      "Iteration 217, loss = 0.09821643\n",
      "Iteration 261, loss = 0.18255432\n",
      "Iteration 47, loss = 0.27957249\n",
      "Iteration 222, loss = 0.07971566\n",
      "Iteration 223, loss = 0.07952764\n",
      "Iteration 69, loss = 0.19347361\n",
      "Iteration 224, loss = 0.07933988\n",
      "Iteration 218, loss = 0.09805170\n",
      "Iteration 150, loss = 0.23735303\n",
      "Iteration 48, loss = 0.27642332\n",
      "Iteration 18, loss = 0.48066863\n",
      "Iteration 90, loss = 0.17374229\n",
      "Iteration 70, loss = 0.19152849\n",
      "Iteration 219, loss = 0.09789457\n",
      "Iteration 220, loss = 0.09774150\n",
      "Iteration 19, loss = 0.47389492\n",
      "Iteration 91, loss = 0.17279231\n",
      "Iteration 262, loss = 0.18248553\n",
      "Iteration 49, loss = 0.27327239\n",
      "Iteration 50, loss = 0.27024652\n",
      "Iteration 225, loss = 0.07916949\n",
      "Iteration 151, loss = 0.23695875\n",
      "Iteration 51, loss = 0.26726578\n",
      "Iteration 20, loss = 0.46735825\n",
      "Iteration 221, loss = 0.09758718\n",
      "Iteration 71, loss = 0.18970513\n",
      "Iteration 222, loss = 0.09744551\n",
      "Iteration 226, loss = 0.07898619\n",
      "Iteration 72, loss = 0.18790603\n",
      "Iteration 52, loss = 0.26421877\n",
      "Iteration 227, loss = 0.07881176\n",
      "Iteration 228, loss = 0.07863758\n",
      "Iteration 229, loss = 0.07846445\n",
      "Iteration 53, loss = 0.26129248\n",
      "Iteration 92, loss = 0.17188013\n",
      "Iteration 54, loss = 0.25848551\n",
      "Iteration 223, loss = 0.09728251\n",
      "Iteration 230, loss = 0.07829082\n",
      "Iteration 73, loss = 0.18618211\n",
      "Iteration 55, loss = 0.25557027\n",
      "Iteration 231, loss = 0.07811901\n",
      "Iteration 56, loss = 0.25263622\n",
      "Iteration 57, loss = 0.24976518\n",
      "Iteration 232, loss = 0.07797184\n",
      "Iteration 58, loss = 0.24695056\n",
      "Iteration 93, loss = 0.17105913\n",
      "Iteration 233, loss = 0.07779480\n",
      "Iteration 224, loss = 0.09714132\n",
      "Iteration 263, loss = 0.18237171\n",
      "Iteration 234, loss = 0.07762529\n",
      "Iteration 235, loss = 0.07748629\n",
      "Iteration 152, loss = 0.23666985\n",
      "Iteration 21, loss = 0.46098183\n",
      "Iteration 59, loss = 0.24418661\n",
      "Iteration 225, loss = 0.09698451\n",
      "Iteration 74, loss = 0.18458378\n",
      "Iteration 236, loss = 0.07731486\n",
      "Iteration 237, loss = 0.07714902\n",
      "Iteration 238, loss = 0.07698322\n",
      "Iteration 239, loss = 0.07683517\n",
      "Iteration 94, loss = 0.17014982\n",
      "Iteration 240, loss = 0.07665822\n",
      "Iteration 75, loss = 0.18299920\n",
      "Iteration 241, loss = 0.07651104\n",
      "Iteration 242, loss = 0.07635891\n",
      "Iteration 60, loss = 0.24140541\n",
      "Iteration 243, loss = 0.07622692\n",
      "Iteration 226, loss = 0.09684415\n",
      "Iteration 61, loss = 0.23871810\n",
      "Iteration 264, loss = 0.18233100\n",
      "Iteration 22, loss = 0.45487924\n",
      "Iteration 76, loss = 0.18141648\n",
      "Iteration 62, loss = 0.23608528\n",
      "Iteration 244, loss = 0.07606821\n",
      "Iteration 95, loss = 0.16932656\n",
      "Iteration 227, loss = 0.09669732\n",
      "Iteration 63, loss = 0.23351700\n",
      "Iteration 64, loss = 0.23084688\n",
      "Iteration 77, loss = 0.17994906\n",
      "Iteration 23, loss = 0.44888197\n",
      "Iteration 96, loss = 0.16849765\n",
      "Iteration 245, loss = 0.07591307\n",
      "Iteration 246, loss = 0.07574810\n",
      "Iteration 65, loss = 0.22833547\n",
      "Iteration 228, loss = 0.09655091\n",
      "Iteration 247, loss = 0.07562089\n",
      "Iteration 248, loss = 0.07547994\n",
      "Iteration 78, loss = 0.17856202\n",
      "Iteration 249, loss = 0.07533385\n",
      "Iteration 66, loss = 0.22584295\n",
      "Iteration 229, loss = 0.09641613\n",
      "Iteration 153, loss = 0.23632644\n",
      "Iteration 67, loss = 0.22336115\n",
      "Iteration 230, loss = 0.09626689\n",
      "Iteration 250, loss = 0.07521775\n",
      "Iteration 79, loss = 0.17719609\n",
      "Iteration 97, loss = 0.16770704\n",
      "Iteration 251, loss = 0.07506468\n",
      "Iteration 98, loss = 0.16694962\n",
      "Iteration 24, loss = 0.44291720\n",
      "Iteration 252, loss = 0.07492029\n",
      "Iteration 231, loss = 0.09614875\n",
      "Iteration 68, loss = 0.22098509\n",
      "Iteration 265, loss = 0.18220576\n",
      "Iteration 80, loss = 0.17591336\n",
      "Iteration 253, loss = 0.07480762\n",
      "Iteration 69, loss = 0.21860826\n",
      "Iteration 232, loss = 0.09598901\n",
      "Iteration 254, loss = 0.07463821\n",
      "Iteration 255, loss = 0.07450712\n",
      "Iteration 256, loss = 0.07441173\n",
      "Iteration 81, loss = 0.17461793\n",
      "Iteration 257, loss = 0.07425794\n",
      "Iteration 82, loss = 0.17340360\n",
      "Iteration 25, loss = 0.43702446\n",
      "Iteration 70, loss = 0.21624635\n",
      "Iteration 99, loss = 0.16614854\n",
      "Iteration 71, loss = 0.21397706\n",
      "Iteration 258, loss = 0.07411239\n",
      "Iteration 233, loss = 0.09585317\n",
      "Iteration 72, loss = 0.21173290\n",
      "Iteration 154, loss = 0.23599825\n",
      "Iteration 73, loss = 0.20952714\n",
      "Iteration 259, loss = 0.07399504\n",
      "Iteration 260, loss = 0.07387155\n",
      "Iteration 261, loss = 0.07373580\n",
      "Iteration 26, loss = 0.43120069\n",
      "Iteration 262, loss = 0.07360226\n",
      "Iteration 263, loss = 0.07349853\n",
      "Iteration 234, loss = 0.09571802\n",
      "Iteration 83, loss = 0.17220137\n",
      "Iteration 74, loss = 0.20741012\n",
      "Iteration 75, loss = 0.20528750\n",
      "Iteration 266, loss = 0.18211079\n",
      "Iteration 100, loss = 0.16539962\n",
      "Iteration 235, loss = 0.09556719\n",
      "Iteration 84, loss = 0.17111703\n",
      "Iteration 85, loss = 0.16994865\n",
      "Iteration 264, loss = 0.07335678\n",
      "Iteration 76, loss = 0.20320918\n",
      "Iteration 77, loss = 0.20123071\n",
      "Iteration 101, loss = 0.16468038\n",
      "Iteration 236, loss = 0.09543857\n",
      "Iteration 267, loss = 0.18200348\n",
      "Iteration 78, loss = 0.19930493\n",
      "Iteration 265, loss = 0.07325663\n",
      "Iteration 266, loss = 0.07311350\n",
      "Iteration 237, loss = 0.09529817\n",
      "Iteration 79, loss = 0.19736029\n",
      "Iteration 86, loss = 0.16889381\n",
      "Iteration 238, loss = 0.09516130\n",
      "Iteration 239, loss = 0.09503580\n",
      "Iteration 267, loss = 0.07299958\n",
      "Iteration 155, loss = 0.23565423\n",
      "Iteration 80, loss = 0.19542180\n",
      "Iteration 268, loss = 0.07287981\n",
      "Iteration 240, loss = 0.09490553\n",
      "Iteration 87, loss = 0.16784309\n",
      "Iteration 102, loss = 0.16395428\n",
      "Iteration 269, loss = 0.07277772\n",
      "Iteration 81, loss = 0.19355010\n",
      "Iteration 27, loss = 0.42573814\n",
      "Iteration 268, loss = 0.18187833Iteration 270, loss = 0.07263820\n",
      "\n",
      "Iteration 82, loss = 0.19169403\n",
      "Iteration 88, loss = 0.16683088\n",
      "Iteration 241, loss = 0.09477705\n",
      "Iteration 103, loss = 0.16323247\n",
      "Iteration 271, loss = 0.07251637\n",
      "Iteration 89, loss = 0.16587600\n",
      "Iteration 242, loss = 0.09462887\n",
      "Iteration 28, loss = 0.42002245\n",
      "Iteration 272, loss = 0.07242782\n",
      "Iteration 83, loss = 0.18980175\n",
      "Iteration 243, loss = 0.09450118\n",
      "Iteration 273, loss = 0.07231646\n",
      "Iteration 84, loss = 0.18796730\n",
      "Iteration 104, loss = 0.16255180\n",
      "Iteration 85, loss = 0.18618845\n",
      "Iteration 274, loss = 0.07218573\n",
      "Iteration 269, loss = 0.18182058\n",
      "Iteration 90, loss = 0.16490688\n",
      "Iteration 275, loss = 0.07207823\n",
      "Iteration 156, loss = 0.23533767\n",
      "Iteration 86, loss = 0.18440976\n",
      "Iteration 244, loss = 0.09436853\n",
      "Iteration 276, loss = 0.07197521\n",
      "Iteration 91, loss = 0.16400397\n",
      "Iteration 277, loss = 0.07188459\n",
      "Iteration 87, loss = 0.18263537\n",
      "Iteration 88, loss = 0.18091031\n",
      "Iteration 92, loss = 0.16309662\n",
      "Iteration 105, loss = 0.16187221\n",
      "Iteration 29, loss = 0.41449679\n",
      "Iteration 278, loss = 0.07175722\n",
      "Iteration 270, loss = 0.18170987\n",
      "Iteration 89, loss = 0.17919327\n",
      "Iteration 245, loss = 0.09424781\n",
      "Iteration 279, loss = 0.07163248\n",
      "Iteration 90, loss = 0.17751628\n",
      "Iteration 246, loss = 0.09413133\n",
      "Iteration 280, loss = 0.07151330\n",
      "Iteration 91, loss = 0.17579574\n",
      "Iteration 30, loss = 0.40912983\n",
      "Iteration 281, loss = 0.07141794\n",
      "Iteration 282, loss = 0.07130972\n",
      "Iteration 106, loss = 0.16119767\n",
      "Iteration 93, loss = 0.16222785\n",
      "Iteration 283, loss = 0.07120768\n",
      "Iteration 247, loss = 0.09399195\n",
      "Iteration 92, loss = 0.17411759\n",
      "Iteration 107, loss = 0.16056569\n",
      "Iteration 94, loss = 0.16136113\n",
      "Iteration 157, loss = 0.23504060\n",
      "Iteration 248, loss = 0.09386048\n",
      "Iteration 95, loss = 0.16052142\n",
      "Iteration 249, loss = 0.09372732\n",
      "Iteration 284, loss = 0.07110311\n",
      "Iteration 93, loss = 0.17254819\n",
      "Iteration 96, loss = 0.15971626\n",
      "Iteration 250, loss = 0.09360996\n",
      "Iteration 285, loss = 0.07099916\n",
      "Iteration 286, loss = 0.07088741\n",
      "Iteration 271, loss = 0.18161697\n",
      "Iteration 251, loss = 0.09349282\n",
      "Iteration 97, loss = 0.15896049\n",
      "Iteration 287, loss = 0.07079111\n",
      "Iteration 288, loss = 0.07069142\n",
      "Iteration 94, loss = 0.17093071\n",
      "Iteration 252, loss = 0.09338021\n",
      "Iteration 253, loss = 0.09325294\n",
      "Iteration 31, loss = 0.40355205\n",
      "Iteration 95, loss = 0.16931632\n",
      "Iteration 108, loss = 0.15989654\n",
      "Iteration 96, loss = 0.16774421\n",
      "Iteration 289, loss = 0.07062849\n",
      "Iteration 290, loss = 0.07049103\n",
      "Iteration 291, loss = 0.07041227\n",
      "Iteration 158, loss = 0.23468701\n",
      "Iteration 32, loss = 0.39829065\n",
      "Iteration 98, loss = 0.15814289\n",
      "Iteration 97, loss = 0.16618682\n",
      "Iteration 292, loss = 0.07029758\n",
      "Iteration 98, loss = 0.16466677\n",
      "Iteration 99, loss = 0.15738772\n",
      "Iteration 254, loss = 0.09313219\n",
      "Iteration 100, loss = 0.15664614\n",
      "Iteration 293, loss = 0.07021253\n",
      "Iteration 272, loss = 0.18157808\n",
      "Iteration 101, loss = 0.15591664\n",
      "Iteration 255, loss = 0.09299940\n",
      "Iteration 294, loss = 0.07009448\n",
      "Iteration 99, loss = 0.16307540\n",
      "Iteration 295, loss = 0.07001823\n",
      "Iteration 33, loss = 0.39284226\n",
      "Iteration 296, loss = 0.06991167\n",
      "Iteration 297, loss = 0.06981656\n",
      "Iteration 100, loss = 0.16155012\n",
      "Iteration 101, loss = 0.16004202\n",
      "Iteration 102, loss = 0.15853389\n",
      "Iteration 256, loss = 0.09289496\n",
      "Iteration 159, loss = 0.23443173\n",
      "Iteration 103, loss = 0.15710951\n",
      "Iteration 298, loss = 0.06973577\n",
      "Iteration 257, loss = 0.09277076\n",
      "Iteration 102, loss = 0.15522219\n",
      "Iteration 299, loss = 0.06962395\n",
      "Iteration 300, loss = 0.06955251\n",
      "Iteration 258, loss = 0.09265946\n",
      "Iteration 273, loss = 0.18143264\n",
      "Iteration 301, loss = 0.06947011\n",
      "Iteration 104, loss = 0.15558341\n",
      "Iteration 302, loss = 0.06935884\n",
      "Iteration 303, loss = 0.06927150\n",
      "Iteration 109, loss = 0.15927151\n",
      "Iteration 304, loss = 0.06919388\n",
      "Iteration 105, loss = 0.15418357\n",
      "Iteration 110, loss = 0.15870160\n",
      "Iteration 34, loss = 0.38750884\n",
      "Iteration 106, loss = 0.15271210\n",
      "Iteration 103, loss = 0.15453447\n",
      "Iteration 305, loss = 0.06912049\n",
      "Iteration 306, loss = 0.06900720\n",
      "Iteration 160, loss = 0.23403176\n",
      "Iteration 259, loss = 0.09254054\n",
      "Iteration 307, loss = 0.06892168\n",
      "Iteration 111, loss = 0.15803380\n",
      "Iteration 260, loss = 0.09243608\n",
      "Iteration 35, loss = 0.38232023\n",
      "Iteration 261, loss = 0.09230325\n",
      "Iteration 112, loss = 0.15745232\n",
      "Iteration 107, loss = 0.15131976\n",
      "Iteration 262, loss = 0.09220409\n",
      "Iteration 274, loss = 0.18133947\n",
      "Iteration 308, loss = 0.06883982\n",
      "Iteration 104, loss = 0.15383000\n",
      "Iteration 108, loss = 0.14989719\n",
      "Iteration 109, loss = 0.14852488\n",
      "Iteration 105, loss = 0.15317238\n",
      "Iteration 110, loss = 0.14719300\n",
      "Iteration 309, loss = 0.06874464\n",
      "Iteration 111, loss = 0.14584084\n",
      "Iteration 106, loss = 0.15253965\n",
      "Iteration 113, loss = 0.15687225\n",
      "Iteration 112, loss = 0.14454125\n",
      "Iteration 107, loss = 0.15188471\n",
      "Iteration 310, loss = 0.06866473\n",
      "Iteration 36, loss = 0.37703142\n",
      "Iteration 263, loss = 0.09207999\n",
      "Iteration 311, loss = 0.06857278\n",
      "Iteration 114, loss = 0.15628612\n",
      "Iteration 161, loss = 0.23373020\n",
      "Iteration 312, loss = 0.06850158\n",
      "Iteration 264, loss = 0.09198494\n",
      "Iteration 313, loss = 0.06841889\n",
      "Iteration 113, loss = 0.14327654\n",
      "Iteration 108, loss = 0.15124445\n",
      "Iteration 114, loss = 0.14197336\n",
      "Iteration 265, loss = 0.09185325\n",
      "Iteration 115, loss = 0.14069859\n",
      "Iteration 275, loss = 0.18127302\n",
      "Iteration 314, loss = 0.06832227\n",
      "Iteration 315, loss = 0.06824563\n",
      "Iteration 115, loss = 0.15572884\n",
      "Iteration 109, loss = 0.15061700\n",
      "Iteration 116, loss = 0.13947306\n",
      "Iteration 316, loss = 0.06817189\n",
      "Iteration 37, loss = 0.37181963\n",
      "Iteration 266, loss = 0.09175247\n",
      "Iteration 117, loss = 0.13825724\n",
      "Iteration 317, loss = 0.06810320\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 118, loss = 0.13706636\n",
      "Iteration 267, loss = 0.09163693\n",
      "Iteration 268, loss = 0.09153749\n",
      "Iteration 119, loss = 0.13590823\n",
      "Iteration 116, loss = 0.15519881\n",
      "Iteration 110, loss = 0.15000988\n",
      "Iteration 120, loss = 0.13474985\n",
      "Iteration 269, loss = 0.09141462\n",
      "Iteration 276, loss = 0.18118295\n",
      "Iteration 162, loss = 0.23341624\n",
      "Iteration 117, loss = 0.15464476\n",
      "Iteration 121, loss = 0.13362710\n",
      "Iteration 270, loss = 0.09131929\n",
      "Iteration 38, loss = 0.36662942\n",
      "Iteration 271, loss = 0.09120612\n",
      "Iteration 111, loss = 0.14941350\n",
      "Iteration 112, loss = 0.14880720\n",
      "Iteration 113, loss = 0.14826348\n",
      "Iteration 122, loss = 0.13257603\n",
      "Iteration 277, loss = 0.18115280\n",
      "Iteration 1, loss = 0.83000440\n",
      "Iteration 118, loss = 0.15410651\n",
      "Iteration 123, loss = 0.13146178\n",
      "Iteration 124, loss = 0.13040711\n",
      "Iteration 272, loss = 0.09109217\n",
      "Iteration 114, loss = 0.14767587\n",
      "Iteration 2, loss = 0.79132897\n",
      "Iteration 39, loss = 0.36142470\n",
      "Iteration 273, loss = 0.09098467\n",
      "Iteration 125, loss = 0.12937186\n",
      "Iteration 3, loss = 0.74133578\n",
      "Iteration 119, loss = 0.15359701\n",
      "Iteration 278, loss = 0.18101734\n",
      "Iteration 163, loss = 0.23315167\n",
      "Iteration 126, loss = 0.12837603\n",
      "Iteration 274, loss = 0.09089071\n",
      "Iteration 40, loss = 0.35634125\n",
      "Iteration 127, loss = 0.12737088\n",
      "Iteration 120, loss = 0.15305440\n",
      "Iteration 128, loss = 0.12637300\n",
      "Iteration 275, loss = 0.09078745\n",
      "Iteration 115, loss = 0.14712895\n",
      "Iteration 129, loss = 0.12543549\n",
      "Iteration 116, loss = 0.14657053\n",
      "Iteration 4, loss = 0.69076933\n",
      "Iteration 130, loss = 0.12451328\n",
      "Iteration 5, loss = 0.64513647\n",
      "Iteration 276, loss = 0.09066961\n",
      "Iteration 117, loss = 0.14602815\n",
      "Iteration 41, loss = 0.35138573\n",
      "Iteration 277, loss = 0.09056765\n",
      "Iteration 131, loss = 0.12358867\n",
      "Iteration 278, loss = 0.09047083\n",
      "Iteration 164, loss = 0.23291296\n",
      "Iteration 118, loss = 0.14549433\n",
      "Iteration 6, loss = 0.60544645\n",
      "Iteration 121, loss = 0.15255672\n",
      "Iteration 279, loss = 0.09038056\n",
      "Iteration 132, loss = 0.12270488\n",
      "Iteration 279, loss = 0.18090851\n",
      "Iteration 119, loss = 0.14496889\n",
      "Iteration 122, loss = 0.15207046\n",
      "Iteration 280, loss = 0.09025180\n",
      "Iteration 120, loss = 0.14444275\n",
      "Iteration 7, loss = 0.57261430\n",
      "Iteration 133, loss = 0.12179250\n",
      "Iteration 281, loss = 0.09016067\n",
      "Iteration 134, loss = 0.12093775\n",
      "Iteration 42, loss = 0.34626530\n",
      "Iteration 280, loss = 0.18081571\n",
      "Iteration 8, loss = 0.54524002\n",
      "Iteration 135, loss = 0.12008916\n",
      "Iteration 282, loss = 0.09006514\n",
      "Iteration 136, loss = 0.11925944\n",
      "Iteration 9, loss = 0.52267172\n",
      "Iteration 165, loss = 0.23253804\n",
      "Iteration 137, loss = 0.11844584\n",
      "Iteration 283, loss = 0.08995141\n",
      "Iteration 43, loss = 0.34132022\n",
      "Iteration 138, loss = 0.11764727\n",
      "Iteration 123, loss = 0.15155235\n",
      "Iteration 121, loss = 0.14393144\n",
      "Iteration 284, loss = 0.08986493\n",
      "Iteration 122, loss = 0.14345196\n",
      "Iteration 123, loss = 0.14294775\n",
      "Iteration 124, loss = 0.15110395\n",
      "Iteration 139, loss = 0.11684944\n",
      "Iteration 281, loss = 0.18074274\n",
      "Iteration 140, loss = 0.11609297\n",
      "Iteration 141, loss = 0.11531091\n",
      "Iteration 44, loss = 0.33634586\n",
      "Iteration 125, loss = 0.15058119\n",
      "Iteration 10, loss = 0.50413265\n",
      "Iteration 285, loss = 0.08975528\n",
      "Iteration 142, loss = 0.11458409\n",
      "Iteration 124, loss = 0.14245453\n",
      "Iteration 286, loss = 0.08965764\n",
      "Iteration 143, loss = 0.11385447\n",
      "Iteration 166, loss = 0.23229551\n",
      "Iteration 144, loss = 0.11309860\n",
      "Iteration 11, loss = 0.48841678\n",
      "Iteration 125, loss = 0.14197632\n",
      "Iteration 287, loss = 0.08954988\n",
      "Iteration 45, loss = 0.33145279\n",
      "Iteration 126, loss = 0.15012587\n",
      "Iteration 288, loss = 0.08945030\n",
      "Iteration 145, loss = 0.11244002\n",
      "Iteration 127, loss = 0.14964707\n",
      "Iteration 146, loss = 0.11176439\n",
      "Iteration 126, loss = 0.14151468\n",
      "Iteration 282, loss = 0.18069871\n",
      "Iteration 12, loss = 0.47570015\n",
      "Iteration 147, loss = 0.11109501\n",
      "Iteration 148, loss = 0.11039056\n",
      "Iteration 289, loss = 0.08936397\n",
      "Iteration 290, loss = 0.08925602\n",
      "Iteration 13, loss = 0.46490100\n",
      "Iteration 128, loss = 0.14917529\n",
      "Iteration 149, loss = 0.10978953\n",
      "Iteration 129, loss = 0.14873245\n",
      "Iteration 150, loss = 0.10913400\n",
      "Iteration 127, loss = 0.14105326\n",
      "Iteration 283, loss = 0.18056153\n",
      "Iteration 291, loss = 0.08915479\n",
      "Iteration 46, loss = 0.32689172\n",
      "Iteration 151, loss = 0.10852440\n",
      "Iteration 292, loss = 0.08906337\n",
      "Iteration 14, loss = 0.45562161\n",
      "Iteration 152, loss = 0.10790446\n",
      "Iteration 293, loss = 0.08899412\n",
      "Iteration 167, loss = 0.23202959\n",
      "Iteration 128, loss = 0.14061675\n",
      "Iteration 153, loss = 0.10730772\n",
      "Iteration 154, loss = 0.10673455\n",
      "Iteration 155, loss = 0.10616580\n",
      "Iteration 156, loss = 0.10559794\n",
      "Iteration 15, loss = 0.44739894\n",
      "Iteration 157, loss = 0.10503254\n",
      "Iteration 130, loss = 0.14829647\n",
      "Iteration 158, loss = 0.10450329\n",
      "Iteration 168, loss = 0.23177429\n",
      "Iteration 294, loss = 0.08888395\n",
      "Iteration 159, loss = 0.10396584\n",
      "Iteration 47, loss = 0.32211742\n",
      "Iteration 295, loss = 0.08877775\n",
      "Iteration 160, loss = 0.10346709\n",
      "Iteration 161, loss = 0.10296424\n",
      "Iteration 162, loss = 0.10249094\n",
      "Iteration 163, loss = 0.10196083\n",
      "Iteration 284, loss = 0.18047696\n",
      "Iteration 48, loss = 0.31758693\n",
      "Iteration 164, loss = 0.10147605\n",
      "Iteration 129, loss = 0.14013485\n",
      "Iteration 16, loss = 0.44041374\n",
      "Iteration 131, loss = 0.14786441\n",
      "Iteration 296, loss = 0.08868312\n",
      "Iteration 17, loss = 0.43392662\n",
      "Iteration 130, loss = 0.13971119\n",
      "Iteration 132, loss = 0.14741756\n",
      "Iteration 297, loss = 0.08859673\n",
      "Iteration 18, loss = 0.42794887\n",
      "Iteration 298, loss = 0.08850120\n",
      "Iteration 285, loss = 0.18045862\n",
      "Iteration 49, loss = 0.31311246\n",
      "Iteration 299, loss = 0.08841087\n",
      "Iteration 131, loss = 0.13930435\n",
      "Iteration 133, loss = 0.14699176\n",
      "Iteration 300, loss = 0.08831452\n",
      "Iteration 169, loss = 0.23142623\n",
      "Iteration 132, loss = 0.13883477\n",
      "Iteration 165, loss = 0.10103443\n",
      "Iteration 19, loss = 0.42240698\n",
      "Iteration 166, loss = 0.10057188\n",
      "Iteration 167, loss = 0.10009872\n",
      "Iteration 133, loss = 0.13842813\n",
      "Iteration 20, loss = 0.41719987\n",
      "Iteration 134, loss = 0.14657399\n",
      "Iteration 168, loss = 0.09966249\n",
      "Iteration 301, loss = 0.08822369\n",
      "Iteration 169, loss = 0.09920632\n",
      "Iteration 21, loss = 0.41197572\n",
      "Iteration 170, loss = 0.09876225\n",
      "Iteration 50, loss = 0.30870292\n",
      "Iteration 302, loss = 0.08812866\n",
      "Iteration 134, loss = 0.13801345\n",
      "Iteration 303, loss = 0.08803245\n",
      "Iteration 170, loss = 0.23118366\n",
      "Iteration 135, loss = 0.14617611\n",
      "Iteration 51, loss = 0.30451195\n",
      "Iteration 304, loss = 0.08795682\n",
      "Iteration 286, loss = 0.18036610\n",
      "Iteration 22, loss = 0.40699067\n",
      "Iteration 171, loss = 0.09835868\n",
      "Iteration 172, loss = 0.09798350\n",
      "Iteration 173, loss = 0.09753303\n",
      "Iteration 135, loss = 0.13760635\n",
      "Iteration 23, loss = 0.40204007\n",
      "Iteration 174, loss = 0.09709942\n",
      "Iteration 175, loss = 0.09669962\n",
      "Iteration 136, loss = 0.13720163\n",
      "Iteration 305, loss = 0.08785921\n",
      "Iteration 136, loss = 0.14575991\n",
      "Iteration 306, loss = 0.08777513\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 52, loss = 0.30043048\n",
      "Iteration 137, loss = 0.14537258\n",
      "Iteration 287, loss = 0.18023880\n",
      "Iteration 171, loss = 0.23093706\n",
      "Iteration 24, loss = 0.39717336\n",
      "Iteration 137, loss = 0.13680339\n",
      "Iteration 176, loss = 0.09631904\n",
      "Iteration 177, loss = 0.09595237\n",
      "Iteration 25, loss = 0.39251552\n",
      "Iteration 178, loss = 0.09556739\n",
      "Iteration 138, loss = 0.13640776\n",
      "Iteration 179, loss = 0.09519380\n",
      "Iteration 139, loss = 0.13601508\n",
      "Iteration 53, loss = 0.29628276\n",
      "Iteration 138, loss = 0.14498965\n",
      "Iteration 180, loss = 0.09486961\n",
      "Iteration 181, loss = 0.09446100\n",
      "Iteration 288, loss = 0.18014561\n",
      "Iteration 182, loss = 0.09412128\n",
      "Iteration 1, loss = 0.93363401\n",
      "Iteration 183, loss = 0.09377236\n",
      "Iteration 140, loss = 0.13564233\n",
      "Iteration 26, loss = 0.38754204\n",
      "Iteration 54, loss = 0.29228187\n",
      "Iteration 184, loss = 0.09344624\n",
      "Iteration 139, loss = 0.14459444\n",
      "Iteration 141, loss = 0.13526589\n",
      "Iteration 185, loss = 0.09311201\n",
      "Iteration 172, loss = 0.23069232\n",
      "Iteration 2, loss = 0.88877705\n",
      "Iteration 27, loss = 0.38284758\n",
      "Iteration 140, loss = 0.14422539\n",
      "Iteration 186, loss = 0.09276275\n",
      "Iteration 28, loss = 0.37795315\n",
      "Iteration 289, loss = 0.18007548\n",
      "Iteration 141, loss = 0.14381253\n",
      "Iteration 142, loss = 0.13491020\n",
      "Iteration 187, loss = 0.09246499\n",
      "Iteration 55, loss = 0.28836320\n",
      "Iteration 188, loss = 0.09212702\n",
      "Iteration 142, loss = 0.14345696\n",
      "Iteration 189, loss = 0.09181825\n",
      "Iteration 143, loss = 0.13450329\n",
      "Iteration 173, loss = 0.23044416\n",
      "Iteration 190, loss = 0.09152158\n",
      "Iteration 56, loss = 0.28465768\n",
      "Iteration 143, loss = 0.14309350\n",
      "Iteration 191, loss = 0.09120944\n",
      "Iteration 192, loss = 0.09090761\n",
      "Iteration 144, loss = 0.13414971\n",
      "Iteration 29, loss = 0.37326144\n",
      "Iteration 193, loss = 0.09062258\n",
      "Iteration 194, loss = 0.09037378\n",
      "Iteration 195, loss = 0.09006695\n",
      "Iteration 145, loss = 0.13379553\n",
      "Iteration 174, loss = 0.23019131\n",
      "Iteration 290, loss = 0.18001401\n",
      "Iteration 3, loss = 0.83061643\n",
      "Iteration 30, loss = 0.36858637\n",
      "Iteration 196, loss = 0.08982113\n",
      "Iteration 144, loss = 0.14275767\n",
      "Iteration 31, loss = 0.36380973\n",
      "Iteration 197, loss = 0.08952534\n",
      "Iteration 57, loss = 0.28105487\n",
      "Iteration 198, loss = 0.08925015\n",
      "Iteration 146, loss = 0.13341425\n",
      "Iteration 147, loss = 0.13309476\n",
      "Iteration 145, loss = 0.14238188\n",
      "Iteration 32, loss = 0.35909424\n",
      "Iteration 199, loss = 0.08898756\n",
      "Iteration 4, loss = 0.77440965\n",
      "Iteration 200, loss = 0.08871047\n",
      "Iteration 33, loss = 0.35438821\n",
      "Iteration 148, loss = 0.13273802\n",
      "Iteration 146, loss = 0.14201332\n",
      "Iteration 291, loss = 0.17993068\n",
      "Iteration 149, loss = 0.13236096\n",
      "Iteration 175, loss = 0.22991756\n",
      "Iteration 58, loss = 0.27743727\n",
      "Iteration 150, loss = 0.13201715\n",
      "Iteration 201, loss = 0.08846312\n",
      "Iteration 202, loss = 0.08824492\n",
      "Iteration 151, loss = 0.13170585\n",
      "Iteration 34, loss = 0.34984274\n",
      "Iteration 203, loss = 0.08799518\n",
      "Iteration 147, loss = 0.14167819\n",
      "Iteration 5, loss = 0.72399400\n",
      "Iteration 204, loss = 0.08771445\n",
      "Iteration 292, loss = 0.17986909\n",
      "Iteration 205, loss = 0.08749114\n",
      "Iteration 206, loss = 0.08726363\n",
      "Iteration 207, loss = 0.08701159\n",
      "Iteration 59, loss = 0.27400248\n",
      "Iteration 35, loss = 0.34529914\n",
      "Iteration 152, loss = 0.13136188\n",
      "Iteration 148, loss = 0.14133475\n",
      "Iteration 149, loss = 0.14100779\n",
      "Iteration 208, loss = 0.08677971\n",
      "Iteration 6, loss = 0.68134533\n",
      "Iteration 176, loss = 0.22963712\n",
      "Iteration 293, loss = 0.17976302\n",
      "Iteration 153, loss = 0.13105583\n",
      "Iteration 36, loss = 0.34060683\n",
      "Iteration 60, loss = 0.27065216\n",
      "Iteration 209, loss = 0.08655223\n",
      "Iteration 150, loss = 0.14064547\n",
      "Iteration 37, loss = 0.33601603\n",
      "Iteration 210, loss = 0.08635666\n",
      "Iteration 211, loss = 0.08613694\n",
      "Iteration 212, loss = 0.08591641\n",
      "Iteration 38, loss = 0.33163993\n",
      "Iteration 177, loss = 0.22951962\n",
      "Iteration 154, loss = 0.13071760\n",
      "Iteration 39, loss = 0.32701652\n",
      "Iteration 294, loss = 0.17970086\n",
      "Iteration 155, loss = 0.13040280\n",
      "Iteration 61, loss = 0.26748013\n",
      "Iteration 7, loss = 0.64652935\n",
      "Iteration 213, loss = 0.08568960\n",
      "Iteration 156, loss = 0.13009402\n",
      "Iteration 214, loss = 0.08549002\n",
      "Iteration 151, loss = 0.14032535\n",
      "Iteration 157, loss = 0.12978514\n",
      "Iteration 215, loss = 0.08530164\n",
      "Iteration 8, loss = 0.61747335\n",
      "Iteration 216, loss = 0.08508422\n",
      "Iteration 62, loss = 0.26442270\n",
      "Iteration 217, loss = 0.08489327\n",
      "Iteration 152, loss = 0.13998955\n",
      "Iteration 40, loss = 0.32257482\n",
      "Iteration 295, loss = 0.17960904\n",
      "Iteration 218, loss = 0.08468491\n",
      "Iteration 158, loss = 0.12947915\n",
      "Iteration 178, loss = 0.22918638\n",
      "Iteration 41, loss = 0.31800607\n",
      "Iteration 153, loss = 0.13966826\n",
      "Iteration 219, loss = 0.08449135\n",
      "Iteration 296, loss = 0.17954363\n",
      "Iteration 9, loss = 0.59451696\n",
      "Iteration 220, loss = 0.08428529\n",
      "Iteration 42, loss = 0.31370894\n",
      "Iteration 221, loss = 0.08411315\n",
      "Iteration 222, loss = 0.08393199\n",
      "Iteration 63, loss = 0.26141572\n",
      "Iteration 159, loss = 0.12917817\n",
      "Iteration 223, loss = 0.08371247\n",
      "Iteration 154, loss = 0.13936591\n",
      "Iteration 160, loss = 0.12887512\n",
      "Iteration 224, loss = 0.08353555\n",
      "Iteration 161, loss = 0.12859902\n",
      "Iteration 179, loss = 0.22896922\n",
      "Iteration 43, loss = 0.30945228\n",
      "Iteration 225, loss = 0.08336376\n",
      "Iteration 162, loss = 0.12831444\n",
      "Iteration 64, loss = 0.25856704\n",
      "Iteration 44, loss = 0.30510406\n",
      "Iteration 226, loss = 0.08315915\n",
      "Iteration 227, loss = 0.08299146\n",
      "Iteration 297, loss = 0.17946335\n",
      "Iteration 228, loss = 0.08281821\n",
      "Iteration 45, loss = 0.30115776\n",
      "Iteration 10, loss = 0.57587010\n",
      "Iteration 229, loss = 0.08264870\n",
      "Iteration 180, loss = 0.22872838\n",
      "Iteration 298, loss = 0.17937436\n",
      "Iteration 155, loss = 0.13904132\n",
      "Iteration 163, loss = 0.12803931\n",
      "Iteration 65, loss = 0.25576826\n",
      "Iteration 230, loss = 0.08247110\n",
      "Iteration 164, loss = 0.12774922\n",
      "Iteration 46, loss = 0.29675676\n",
      "Iteration 156, loss = 0.13872697\n",
      "Iteration 165, loss = 0.12745813\n",
      "Iteration 11, loss = 0.55952001\n",
      "Iteration 157, loss = 0.13842821\n",
      "Iteration 231, loss = 0.08233903\n",
      "Iteration 66, loss = 0.25305521\n",
      "Iteration 47, loss = 0.29294091\n",
      "Iteration 158, loss = 0.13811295\n",
      "Iteration 299, loss = 0.17928275\n",
      "Iteration 232, loss = 0.08217941\n",
      "Iteration 166, loss = 0.12718162\n",
      "Iteration 233, loss = 0.08201424\n",
      "Iteration 167, loss = 0.12689478\n",
      "Iteration 234, loss = 0.08185915\n",
      "Iteration 235, loss = 0.08170538\n",
      "Iteration 67, loss = 0.25054242\n",
      "Iteration 236, loss = 0.08153187\n",
      "Iteration 48, loss = 0.28885826\n",
      "Iteration 237, loss = 0.08138935Iteration 159, loss = 0.13782969\n",
      "Iteration 12, loss = 0.54659992\n",
      "Iteration 168, loss = 0.12665897\n",
      "\n",
      "Iteration 181, loss = 0.22848886\n",
      "Iteration 169, loss = 0.12637611\n",
      "Iteration 238, loss = 0.08125164\n",
      "Iteration 300, loss = 0.17922113\n",
      "Iteration 49, loss = 0.28500414\n",
      "Iteration 160, loss = 0.13754679\n",
      "Iteration 239, loss = 0.08108605\n",
      "Iteration 240, loss = 0.08096526\n",
      "Iteration 241, loss = 0.08080910\n",
      "Iteration 68, loss = 0.24799985\n",
      "Iteration 170, loss = 0.12610782\n",
      "Iteration 242, loss = 0.08066704\n",
      "Iteration 50, loss = 0.28124139\n",
      "Iteration 161, loss = 0.13724195\n",
      "Iteration 301, loss = 0.17916003\n",
      "Iteration 171, loss = 0.12581773\n",
      "Iteration 243, loss = 0.08051185\n",
      "Iteration 182, loss = 0.22826365\n",
      "Iteration 51, loss = 0.27761877\n",
      "Iteration 162, loss = 0.13694888\n",
      "Iteration 13, loss = 0.53490259\n",
      "Iteration 52, loss = 0.27381811\n",
      "Iteration 69, loss = 0.24551339\n",
      "Iteration 244, loss = 0.08038648\n",
      "Iteration 172, loss = 0.12558688\n",
      "Iteration 245, loss = 0.08023242\n",
      "Iteration 246, loss = 0.08010022\n",
      "Iteration 163, loss = 0.13664689\n",
      "Iteration 173, loss = 0.12538495\n",
      "Iteration 247, loss = 0.07999094\n",
      "Iteration 174, loss = 0.12505224\n",
      "Iteration 53, loss = 0.27033152\n",
      "Iteration 302, loss = 0.17907942\n",
      "Iteration 248, loss = 0.07983502\n",
      "Iteration 14, loss = 0.52453475\n",
      "Iteration 175, loss = 0.12481750\n",
      "Iteration 70, loss = 0.24320951\n",
      "Iteration 183, loss = 0.22811251\n",
      "Iteration 249, loss = 0.07969613\n",
      "Iteration 164, loss = 0.13639801\n",
      "Iteration 54, loss = 0.26672698\n",
      "Iteration 250, loss = 0.07958749\n",
      "Iteration 165, loss = 0.13609012\n",
      "Iteration 251, loss = 0.07943568\n",
      "Iteration 166, loss = 0.13581476\n",
      "Iteration 303, loss = 0.17898512\n",
      "Iteration 176, loss = 0.12453564\n",
      "Iteration 252, loss = 0.07934057\n",
      "Iteration 253, loss = 0.07917540\n",
      "Iteration 15, loss = 0.51498094\n",
      "Iteration 71, loss = 0.24088417\n",
      "Iteration 55, loss = 0.26342604\n",
      "Iteration 254, loss = 0.07907826\n",
      "Iteration 177, loss = 0.12431788\n",
      "Iteration 304, loss = 0.17894799\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 255, loss = 0.07896203\n",
      "Iteration 167, loss = 0.13555681\n",
      "Iteration 178, loss = 0.12405268\n",
      "Iteration 56, loss = 0.25997615\n",
      "Iteration 72, loss = 0.23874095\n",
      "Iteration 184, loss = 0.22781988\n",
      "Iteration 256, loss = 0.07882129\n",
      "Iteration 16, loss = 0.50632488\n",
      "Iteration 1, loss = 0.74092842\n",
      "Iteration 257, loss = 0.07870111\n",
      "Iteration 258, loss = 0.07856278\n",
      "Iteration 168, loss = 0.13527031\n",
      "Iteration 179, loss = 0.12381945\n",
      "Iteration 57, loss = 0.25669245\n",
      "Iteration 2, loss = 0.72013687\n",
      "Iteration 73, loss = 0.23661923\n",
      "Iteration 259, loss = 0.07844846\n",
      "Iteration 180, loss = 0.12357402\n",
      "Iteration 260, loss = 0.07834549\n",
      "Iteration 58, loss = 0.25352263\n",
      "Iteration 169, loss = 0.13501328\n",
      "Iteration 261, loss = 0.07821226\n",
      "Iteration 74, loss = 0.23462197\n",
      "Iteration 59, loss = 0.25039731\n",
      "Iteration 185, loss = 0.22760389\n",
      "Iteration 181, loss = 0.12332799\n",
      "Iteration 17, loss = 0.49790375\n",
      "Iteration 262, loss = 0.07810544\n",
      "Iteration 3, loss = 0.69225260\n",
      "Iteration 263, loss = 0.07799085\n",
      "Iteration 182, loss = 0.12311804\n",
      "Iteration 170, loss = 0.13473663\n",
      "Iteration 264, loss = 0.07792828\n",
      "Iteration 4, loss = 0.66331941\n",
      "Iteration 60, loss = 0.24732140\n",
      "Iteration 265, loss = 0.07777922\n",
      "Iteration 5, loss = 0.63566737\n",
      "Iteration 266, loss = 0.07764756\n",
      "Iteration 6, loss = 0.61026694\n",
      "Iteration 267, loss = 0.07753812\n",
      "Iteration 183, loss = 0.12286072\n",
      "Iteration 61, loss = 0.24439920\n",
      "Iteration 18, loss = 0.48962548\n",
      "Iteration 171, loss = 0.13448918\n",
      "Iteration 75, loss = 0.23257995\n",
      "Iteration 268, loss = 0.07745293\n",
      "Iteration 269, loss = 0.07733727\n",
      "Iteration 270, loss = 0.07723795\n",
      "Iteration 271, loss = 0.07712507\n",
      "Iteration 272, loss = 0.07705648\n",
      "Iteration 172, loss = 0.13422498\n",
      "Iteration 7, loss = 0.58774947\n",
      "Iteration 62, loss = 0.24158080\n",
      "Iteration 184, loss = 0.12262357\n",
      "Iteration 186, loss = 0.22740915\n",
      "Iteration 185, loss = 0.12240139\n",
      "Iteration 8, loss = 0.56828369\n",
      "Iteration 9, loss = 0.55045442\n",
      "Iteration 186, loss = 0.12216426\n",
      "Iteration 10, loss = 0.53514535\n",
      "Iteration 19, loss = 0.48204683\n",
      "Iteration 63, loss = 0.23876633\n",
      "Iteration 273, loss = 0.07692131\n",
      "Iteration 76, loss = 0.23068635\n",
      "Iteration 173, loss = 0.13397719\n",
      "Iteration 274, loss = 0.07687120\n",
      "Iteration 11, loss = 0.52079075\n",
      "Iteration 187, loss = 0.12195378\n",
      "Iteration 64, loss = 0.23611055\n",
      "Iteration 12, loss = 0.50831893\n",
      "Iteration 275, loss = 0.07672447\n",
      "Iteration 174, loss = 0.13370156\n",
      "Iteration 77, loss = 0.22880584\n",
      "Iteration 276, loss = 0.07663619\n",
      "Iteration 13, loss = 0.49696367\n",
      "Iteration 65, loss = 0.23345551\n",
      "Iteration 277, loss = 0.07654325\n",
      "Iteration 175, loss = 0.13345480\n",
      "Iteration 14, loss = 0.48686286\n",
      "Iteration 278, loss = 0.07644166\n",
      "Iteration 66, loss = 0.23086846\n",
      "Iteration 279, loss = 0.07634631Iteration 176, loss = 0.13319720\n",
      "Iteration 188, loss = 0.12173268\n",
      "\n",
      "Iteration 280, loss = 0.07624292\n",
      "Iteration 15, loss = 0.47736675\n",
      "Iteration 189, loss = 0.12153087\n",
      "Iteration 20, loss = 0.47436933\n",
      "Iteration 187, loss = 0.22721573\n",
      "Iteration 190, loss = 0.12131003\n",
      "Iteration 67, loss = 0.22836398\n",
      "Iteration 281, loss = 0.07615454\n",
      "Iteration 78, loss = 0.22701987\n",
      "Iteration 191, loss = 0.12107710\n",
      "Iteration 68, loss = 0.22590797\n",
      "Iteration 282, loss = 0.07605858\n",
      "Iteration 16, loss = 0.46904426\n",
      "Iteration 17, loss = 0.46154737\n",
      "Iteration 283, loss = 0.07597509\n",
      "Iteration 177, loss = 0.13298398\n",
      "Iteration 18, loss = 0.45430330\n",
      "Iteration 21, loss = 0.46713094\n",
      "Iteration 79, loss = 0.22534215\n",
      "Iteration 284, loss = 0.07589656\n",
      "Iteration 188, loss = 0.22698367\n",
      "Iteration 192, loss = 0.12087170\n",
      "Iteration 178, loss = 0.13271397\n",
      "Iteration 19, loss = 0.44784466\n",
      "Iteration 193, loss = 0.12065056\n",
      "Iteration 69, loss = 0.22369281\n",
      "Iteration 285, loss = 0.07580201\n",
      "Iteration 20, loss = 0.44199907\n",
      "Iteration 286, loss = 0.07571103\n",
      "Iteration 21, loss = 0.43670206\n",
      "Iteration 287, loss = 0.07563228\n",
      "Iteration 179, loss = 0.13246581\n",
      "Iteration 80, loss = 0.22360642\n",
      "Iteration 288, loss = 0.07553625\n",
      "Iteration 194, loss = 0.12046667\n",
      "Iteration 22, loss = 0.45989073\n",
      "Iteration 70, loss = 0.22140133\n",
      "Iteration 22, loss = 0.43170975\n",
      "Iteration 289, loss = 0.07544793\n",
      "Iteration 180, loss = 0.13223364\n",
      "Iteration 23, loss = 0.42727167\n",
      "Iteration 290, loss = 0.07539151\n",
      "Iteration 81, loss = 0.22196458\n",
      "Iteration 195, loss = 0.12024951\n",
      "Iteration 189, loss = 0.22677908\n",
      "Iteration 196, loss = 0.12001069\n",
      "Iteration 71, loss = 0.21922835\n",
      "Iteration 197, loss = 0.11981712\n",
      "Iteration 72, loss = 0.21717324\n",
      "Iteration 24, loss = 0.42317946\n",
      "Iteration 291, loss = 0.07529041\n",
      "Iteration 181, loss = 0.13197988\n",
      "Iteration 292, loss = 0.07520483\n",
      "Iteration 82, loss = 0.22038325\n",
      "Iteration 23, loss = 0.45286469\n",
      "Iteration 25, loss = 0.41950843\n",
      "Iteration 26, loss = 0.41621358\n",
      "Iteration 198, loss = 0.11962621\n",
      "Iteration 27, loss = 0.41319247\n",
      "Iteration 293, loss = 0.07516525\n",
      "Iteration 182, loss = 0.13177603\n",
      "Iteration 294, loss = 0.07505579\n",
      "Iteration 28, loss = 0.41050860\n",
      "Iteration 73, loss = 0.21515297\n",
      "Iteration 295, loss = 0.07497253\n",
      "Iteration 296, loss = 0.07488553\n",
      "Iteration 29, loss = 0.40797702\n",
      "Iteration 297, loss = 0.07480540\n",
      "Iteration 30, loss = 0.40545631\n",
      "Iteration 298, loss = 0.07475015\n",
      "Iteration 24, loss = 0.44575219\n",
      "Iteration 31, loss = 0.40332156\n",
      "Iteration 299, loss = 0.07467478\n",
      "Iteration 32, loss = 0.40150852\n",
      "Iteration 190, loss = 0.22658186\n",
      "Iteration 83, loss = 0.21887764\n",
      "Iteration 300, loss = 0.07463597\n",
      "Iteration 199, loss = 0.11941912\n",
      "Iteration 74, loss = 0.21322625\n",
      "Iteration 33, loss = 0.39950568\n",
      "Iteration 183, loss = 0.13151669\n",
      "Iteration 34, loss = 0.39775915\n",
      "Iteration 301, loss = 0.07453168\n",
      "Iteration 200, loss = 0.11921832\n",
      "Iteration 201, loss = 0.11900710\n",
      "Iteration 302, loss = 0.07443935\n",
      "Iteration 75, loss = 0.21131869\n",
      "Iteration 84, loss = 0.21736329\n",
      "Iteration 303, loss = 0.07438197\n",
      "Iteration 191, loss = 0.22642114\n",
      "Iteration 25, loss = 0.43902834\n",
      "Iteration 184, loss = 0.13130008\n",
      "Iteration 304, loss = 0.07429004\n",
      "Iteration 35, loss = 0.39621383\n",
      "Iteration 305, loss = 0.07424830\n",
      "Iteration 306, loss = 0.07415012\n",
      "Iteration 76, loss = 0.20950918\n",
      "Iteration 202, loss = 0.11881003\n",
      "Iteration 185, loss = 0.13107332\n",
      "Iteration 36, loss = 0.39457336\n",
      "Iteration 203, loss = 0.11861353\n",
      "Iteration 307, loss = 0.07407984\n",
      "Iteration 26, loss = 0.43224573\n",
      "Iteration 308, loss = 0.07404314\n",
      "Iteration 37, loss = 0.39308569\n",
      "Iteration 309, loss = 0.07394854\n",
      "Iteration 310, loss = 0.07389625\n",
      "Iteration 186, loss = 0.13083077\n",
      "Iteration 38, loss = 0.39166265\n",
      "Iteration 77, loss = 0.20777420\n",
      "Iteration 85, loss = 0.21591295\n",
      "Iteration 39, loss = 0.39029959\n",
      "Iteration 204, loss = 0.11840891\n",
      "Iteration 78, loss = 0.20608947\n",
      "Iteration 192, loss = 0.22618317\n",
      "Iteration 311, loss = 0.07381543\n",
      "Iteration 187, loss = 0.13061984\n",
      "Iteration 40, loss = 0.38895511\n",
      "Iteration 86, loss = 0.21457420\n",
      "Iteration 41, loss = 0.38760274\n",
      "Iteration 312, loss = 0.07374634\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 42, loss = 0.38632077\n",
      "Iteration 205, loss = 0.11823084\n",
      "Iteration 43, loss = 0.38501371\n",
      "Iteration 79, loss = 0.20441922\n",
      "Iteration 206, loss = 0.11802817\n",
      "Iteration 44, loss = 0.38372235\n",
      "Iteration 207, loss = 0.11783682\n",
      "Iteration 27, loss = 0.42547356\n",
      "Iteration 80, loss = 0.20286745\n",
      "Iteration 81, loss = 0.20131538Iteration 188, loss = 0.13039841\n",
      "\n",
      "Iteration 87, loss = 0.21318537Iteration 45, loss = 0.38244156\n",
      "Iteration 208, loss = 0.11763694\n",
      "\n",
      "Iteration 1, loss = 0.79654584\n",
      "Iteration 189, loss = 0.13017667\n",
      "Iteration 193, loss = 0.22605725\n",
      "Iteration 46, loss = 0.38120499\n",
      "Iteration 209, loss = 0.11745980\n",
      "Iteration 28, loss = 0.41882775\n",
      "Iteration 88, loss = 0.21186611\n",
      "Iteration 47, loss = 0.37992410\n",
      "Iteration 2, loss = 0.77297267\n",
      "Iteration 82, loss = 0.19981382\n",
      "Iteration 48, loss = 0.37865255\n",
      "Iteration 210, loss = 0.11726341\n",
      "Iteration 190, loss = 0.12995880\n",
      "Iteration 49, loss = 0.37736046\n",
      "Iteration 50, loss = 0.37602830\n",
      "Iteration 89, loss = 0.21061391\n",
      "Iteration 29, loss = 0.41217930\n",
      "Iteration 191, loss = 0.12974917\n",
      "Iteration 83, loss = 0.19837076\n",
      "Iteration 211, loss = 0.11708080\n",
      "Iteration 212, loss = 0.11690502\n",
      "Iteration 3, loss = 0.74138667\n",
      "Iteration 51, loss = 0.37474223\n",
      "Iteration 52, loss = 0.37335508\n",
      "Iteration 53, loss = 0.37205441\n",
      "Iteration 84, loss = 0.19699347\n",
      "Iteration 90, loss = 0.20938145\n",
      "Iteration 194, loss = 0.22583050\n",
      "Iteration 213, loss = 0.11672155\n",
      "Iteration 54, loss = 0.37065881\n",
      "Iteration 4, loss = 0.70816140\n",
      "Iteration 55, loss = 0.36928445\n",
      "Iteration 91, loss = 0.20815902\n",
      "Iteration 30, loss = 0.40591005\n",
      "Iteration 192, loss = 0.12952820\n",
      "Iteration 56, loss = 0.36793213\n",
      "Iteration 214, loss = 0.11656702\n",
      "Iteration 85, loss = 0.19560262\n",
      "Iteration 215, loss = 0.11638322\n",
      "Iteration 193, loss = 0.12934284\n",
      "Iteration 57, loss = 0.36644351\n",
      "Iteration 5, loss = 0.67635808\n",
      "Iteration 86, loss = 0.19426424\n",
      "Iteration 58, loss = 0.36498443\n",
      "Iteration 216, loss = 0.11620390\n",
      "Iteration 59, loss = 0.36352634\n",
      "Iteration 195, loss = 0.22564744\n",
      "Iteration 92, loss = 0.20701170\n",
      "Iteration 217, loss = 0.11602478\n",
      "Iteration 31, loss = 0.39913693\n",
      "Iteration 6, loss = 0.64753101\n",
      "Iteration 87, loss = 0.19297616\n",
      "Iteration 60, loss = 0.36192067\n",
      "Iteration 194, loss = 0.12912947\n",
      "Iteration 218, loss = 0.11584629\n",
      "Iteration 219, loss = 0.11567082\n",
      "Iteration 61, loss = 0.36033821\n",
      "Iteration 7, loss = 0.62143075\n",
      "Iteration 88, loss = 0.19171983\n",
      "Iteration 195, loss = 0.12891514\n",
      "Iteration 62, loss = 0.35869685\n",
      "Iteration 32, loss = 0.39302543\n",
      "Iteration 63, loss = 0.35701808\n",
      "Iteration 220, loss = 0.11551731\n",
      "Iteration 64, loss = 0.35536879\n",
      "Iteration 89, loss = 0.19054345\n",
      "Iteration 65, loss = 0.35368737\n",
      "Iteration 66, loss = 0.35195657\n",
      "Iteration 8, loss = 0.59878587\n",
      "Iteration 196, loss = 0.12871353\n",
      "Iteration 67, loss = 0.35016341\n",
      "Iteration 33, loss = 0.38682102\n",
      "Iteration 68, loss = 0.34836210\n",
      "Iteration 69, loss = 0.34654403\n",
      "Iteration 196, loss = 0.22543220\n",
      "Iteration 9, loss = 0.57887437\n",
      "Iteration 93, loss = 0.20584748\n",
      "Iteration 70, loss = 0.34466196\n",
      "Iteration 197, loss = 0.12853440\n",
      "Iteration 94, loss = 0.20478404\n",
      "Iteration 90, loss = 0.18930569\n",
      "Iteration 10, loss = 0.56083288\n",
      "Iteration 221, loss = 0.11534178\n",
      "Iteration 198, loss = 0.12831517\n",
      "Iteration 222, loss = 0.11519510\n",
      "Iteration 71, loss = 0.34282337\n",
      "Iteration 199, loss = 0.12813148\n",
      "Iteration 34, loss = 0.38056131\n",
      "Iteration 197, loss = 0.22525434\n",
      "Iteration 223, loss = 0.11500775\n",
      "Iteration 91, loss = 0.18820655\n",
      "Iteration 72, loss = 0.34100485\n",
      "Iteration 95, loss = 0.20369021\n",
      "Iteration 11, loss = 0.54547324\n",
      "Iteration 73, loss = 0.33895633\n",
      "Iteration 92, loss = 0.18701366\n",
      "Iteration 224, loss = 0.11483457\n",
      "Iteration 93, loss = 0.18595392\n",
      "Iteration 200, loss = 0.12791566\n",
      "Iteration 96, loss = 0.20266733\n",
      "Iteration 225, loss = 0.11469776\n",
      "Iteration 12, loss = 0.53140109\n",
      "Iteration 226, loss = 0.11454912\n",
      "Iteration 35, loss = 0.37460540\n",
      "Iteration 227, loss = 0.11435134\n",
      "Iteration 74, loss = 0.33707259\n",
      "Iteration 201, loss = 0.12772979\n",
      "Iteration 198, loss = 0.22511431\n",
      "Iteration 97, loss = 0.20164447\n",
      "Iteration 75, loss = 0.33500582\n",
      "Iteration 13, loss = 0.51915997\n",
      "Iteration 76, loss = 0.33299527\n",
      "Iteration 94, loss = 0.18487770\n",
      "Iteration 77, loss = 0.33102372\n",
      "Iteration 95, loss = 0.18385710\n",
      "Iteration 78, loss = 0.32885202\n",
      "Iteration 228, loss = 0.11422701\n",
      "Iteration 14, loss = 0.50818891\n",
      "Iteration 98, loss = 0.20067243\n",
      "Iteration 79, loss = 0.32685059\n",
      "Iteration 36, loss = 0.36874670Iteration 202, loss = 0.12753225\n",
      "\n",
      "Iteration 229, loss = 0.11404837\n",
      "Iteration 80, loss = 0.32464325\n",
      "Iteration 199, loss = 0.22491226\n",
      "Iteration 203, loss = 0.12736864\n",
      "Iteration 96, loss = 0.18285453\n",
      "Iteration 81, loss = 0.32247192\n",
      "Iteration 82, loss = 0.32020348\n",
      "Iteration 230, loss = 0.11390210\n",
      "Iteration 15, loss = 0.49869073\n",
      "Iteration 83, loss = 0.31806404\n",
      "Iteration 99, loss = 0.19971502\n",
      "Iteration 37, loss = 0.36306332\n",
      "Iteration 231, loss = 0.11373013\n",
      "Iteration 204, loss = 0.12716340\n",
      "Iteration 97, loss = 0.18187068\n",
      "Iteration 232, loss = 0.11362329\n",
      "Iteration 16, loss = 0.49013059\n",
      "Iteration 205, loss = 0.12696848\n",
      "Iteration 98, loss = 0.18089547\n",
      "Iteration 233, loss = 0.11341537\n",
      "Iteration 84, loss = 0.31580031\n",
      "Iteration 85, loss = 0.31343288\n",
      "Iteration 86, loss = 0.31110386\n",
      "Iteration 100, loss = 0.19879268\n",
      "Iteration 17, loss = 0.48227803\n",
      "Iteration 234, loss = 0.11328123\n",
      "Iteration 87, loss = 0.30877899\n",
      "Iteration 38, loss = 0.35756005\n",
      "Iteration 88, loss = 0.30637131\n",
      "Iteration 200, loss = 0.22477069\n",
      "Iteration 99, loss = 0.17999030\n",
      "Iteration 89, loss = 0.30387577\n",
      "Iteration 18, loss = 0.47567881\n",
      "Iteration 90, loss = 0.30141494\n",
      "Iteration 101, loss = 0.19788116\n",
      "Iteration 206, loss = 0.12678698\n",
      "Iteration 91, loss = 0.29891510\n",
      "Iteration 235, loss = 0.11313159\n",
      "Iteration 100, loss = 0.17901943\n",
      "Iteration 19, loss = 0.46936003\n",
      "Iteration 236, loss = 0.11298744\n",
      "Iteration 101, loss = 0.17817454\n",
      "Iteration 92, loss = 0.29634964\n",
      "Iteration 39, loss = 0.35203529\n",
      "Iteration 93, loss = 0.29386563\n",
      "Iteration 207, loss = 0.12660076\n",
      "Iteration 94, loss = 0.29116192\n",
      "Iteration 102, loss = 0.19697764\n",
      "Iteration 237, loss = 0.11284972\n",
      "Iteration 208, loss = 0.12640963\n",
      "Iteration 201, loss = 0.22463752\n",
      "Iteration 95, loss = 0.28855583\n",
      "Iteration 103, loss = 0.19612413\n",
      "Iteration 96, loss = 0.28585367\n",
      "Iteration 102, loss = 0.17730043\n",
      "Iteration 97, loss = 0.28323789\n",
      "Iteration 20, loss = 0.46381986\n",
      "Iteration 98, loss = 0.28042275\n",
      "Iteration 238, loss = 0.11267320\n",
      "Iteration 209, loss = 0.12624258\n",
      "Iteration 239, loss = 0.11252690\n",
      "Iteration 40, loss = 0.34667171\n",
      "Iteration 99, loss = 0.27763989\n",
      "Iteration 240, loss = 0.11238021\n",
      "Iteration 103, loss = 0.17647699\n",
      "Iteration 210, loss = 0.12607539\n",
      "Iteration 21, loss = 0.45868775\n",
      "Iteration 241, loss = 0.11225145\n",
      "Iteration 100, loss = 0.27482046\n",
      "Iteration 101, loss = 0.27187447\n",
      "Iteration 202, loss = 0.22440106\n",
      "Iteration 104, loss = 0.17561172\n",
      "Iteration 104, loss = 0.19530930\n",
      "Iteration 105, loss = 0.17482563\n",
      "Iteration 102, loss = 0.26900834\n",
      "Iteration 22, loss = 0.45393047\n",
      "Iteration 211, loss = 0.12587198\n",
      "Iteration 41, loss = 0.34144094\n",
      "Iteration 103, loss = 0.26609025\n",
      "Iteration 105, loss = 0.19448359\n",
      "Iteration 212, loss = 0.12569910\n",
      "Iteration 106, loss = 0.17401325\n",
      "Iteration 242, loss = 0.11209110\n",
      "Iteration 104, loss = 0.26315267\n",
      "Iteration 243, loss = 0.11196655\n",
      "Iteration 23, loss = 0.44938684\n",
      "Iteration 203, loss = 0.22421118\n",
      "Iteration 105, loss = 0.26010952\n",
      "Iteration 107, loss = 0.17322357\n",
      "Iteration 213, loss = 0.12552871\n",
      "Iteration 244, loss = 0.11182323\n",
      "Iteration 106, loss = 0.19366729\n",
      "Iteration 106, loss = 0.25710859\n",
      "Iteration 214, loss = 0.12536029\n",
      "Iteration 107, loss = 0.25408632\n",
      "Iteration 24, loss = 0.44537641\n",
      "Iteration 108, loss = 0.17246505\n",
      "Iteration 42, loss = 0.33650058\n",
      "Iteration 108, loss = 0.25111439\n",
      "Iteration 109, loss = 0.24796533\n",
      "Iteration 245, loss = 0.11171992\n",
      "Iteration 246, loss = 0.11153972\n",
      "Iteration 109, loss = 0.17174234\n",
      "Iteration 215, loss = 0.12518720\n",
      "Iteration 110, loss = 0.24499023\n",
      "Iteration 25, loss = 0.44138958\n",
      "Iteration 247, loss = 0.11138800\n",
      "Iteration 111, loss = 0.24192523\n",
      "Iteration 204, loss = 0.22412794\n",
      "Iteration 110, loss = 0.17102510\n",
      "Iteration 248, loss = 0.11125741\n",
      "Iteration 112, loss = 0.23887149\n",
      "Iteration 113, loss = 0.23585870\n",
      "Iteration 26, loss = 0.43757734\n",
      "Iteration 249, loss = 0.11114616\n",
      "Iteration 111, loss = 0.17032182\n",
      "Iteration 43, loss = 0.33165026\n",
      "Iteration 250, loss = 0.11101963\n",
      "Iteration 251, loss = 0.11085832\n",
      "Iteration 107, loss = 0.19287434\n",
      "Iteration 216, loss = 0.12501022\n",
      "Iteration 114, loss = 0.23300721\n",
      "Iteration 205, loss = 0.22391558\n",
      "Iteration 108, loss = 0.19211902\n",
      "Iteration 27, loss = 0.43384619\n",
      "Iteration 115, loss = 0.22991758\n",
      "Iteration 112, loss = 0.16959821\n",
      "Iteration 116, loss = 0.22694614\n",
      "Iteration 217, loss = 0.12483241\n",
      "Iteration 44, loss = 0.32694823\n",
      "Iteration 28, loss = 0.43037151\n",
      "Iteration 252, loss = 0.11075771\n",
      "Iteration 117, loss = 0.22406953\n",
      "Iteration 218, loss = 0.12466286\n",
      "Iteration 109, loss = 0.19135145\n",
      "Iteration 113, loss = 0.16897645Iteration 29, loss = 0.42684432\n",
      "\n",
      "Iteration 253, loss = 0.11060776\n",
      "Iteration 118, loss = 0.22113662\n",
      "Iteration 30, loss = 0.42343772\n",
      "Iteration 219, loss = 0.12449770\n",
      "Iteration 206, loss = 0.22372296\n",
      "Iteration 45, loss = 0.32239178\n",
      "Iteration 119, loss = 0.21839867\n",
      "Iteration 114, loss = 0.16830666\n",
      "Iteration 120, loss = 0.21555460\n",
      "Iteration 254, loss = 0.11047447\n",
      "Iteration 115, loss = 0.16757782\n",
      "Iteration 255, loss = 0.11034978\n",
      "Iteration 121, loss = 0.21288973\n",
      "Iteration 122, loss = 0.21014933\n",
      "Iteration 46, loss = 0.31805813\n",
      "Iteration 110, loss = 0.19064775\n",
      "Iteration 220, loss = 0.12433439\n",
      "Iteration 31, loss = 0.42008961\n",
      "Iteration 256, loss = 0.11022099\n",
      "Iteration 207, loss = 0.22357598\n",
      "Iteration 123, loss = 0.20747269\n",
      "Iteration 116, loss = 0.16693521\n",
      "Iteration 124, loss = 0.20487082\n",
      "Iteration 221, loss = 0.12416748\n",
      "Iteration 257, loss = 0.11010536\n",
      "Iteration 111, loss = 0.18994841\n",
      "Iteration 125, loss = 0.20232186\n",
      "Iteration 32, loss = 0.41678588\n",
      "Iteration 126, loss = 0.19973103\n",
      "Iteration 127, loss = 0.19726495\n",
      "Iteration 117, loss = 0.16633944\n",
      "Iteration 258, loss = 0.10995974\n",
      "Iteration 128, loss = 0.19485999\n",
      "Iteration 222, loss = 0.12402937\n",
      "Iteration 129, loss = 0.19252443\n",
      "Iteration 259, loss = 0.10984933\n",
      "Iteration 112, loss = 0.18922175\n",
      "Iteration 118, loss = 0.16571231\n",
      "Iteration 47, loss = 0.31384917\n",
      "Iteration 260, loss = 0.10973324\n",
      "Iteration 33, loss = 0.41352205\n",
      "Iteration 208, loss = 0.22340531\n",
      "Iteration 113, loss = 0.18852841\n",
      "Iteration 130, loss = 0.19034786\n",
      "Iteration 223, loss = 0.12385059\n",
      "Iteration 114, loss = 0.18795595\n",
      "Iteration 34, loss = 0.41031725\n",
      "Iteration 119, loss = 0.16510230\n",
      "Iteration 131, loss = 0.18805982\n",
      "Iteration 224, loss = 0.12368455\n",
      "Iteration 261, loss = 0.10960316\n",
      "Iteration 132, loss = 0.18582576\n",
      "Iteration 48, loss = 0.30978843\n",
      "Iteration 225, loss = 0.12351829\n",
      "Iteration 120, loss = 0.16452925\n",
      "Iteration 35, loss = 0.40711232\n",
      "Iteration 262, loss = 0.10946993\n",
      "Iteration 133, loss = 0.18369057\n",
      "Iteration 209, loss = 0.22326174\n",
      "Iteration 36, loss = 0.40391233\n",
      "Iteration 134, loss = 0.18173342\n",
      "Iteration 115, loss = 0.18722589\n",
      "Iteration 263, loss = 0.10934709\n",
      "Iteration 49, loss = 0.30590453\n",
      "Iteration 121, loss = 0.16393245\n",
      "Iteration 226, loss = 0.12335658\n",
      "Iteration 264, loss = 0.10924823\n",
      "Iteration 135, loss = 0.17970140\n",
      "Iteration 227, loss = 0.12320568\n",
      "Iteration 265, loss = 0.10913996\n",
      "Iteration 122, loss = 0.16336070\n",
      "Iteration 116, loss = 0.18662061\n",
      "Iteration 136, loss = 0.17787647\n",
      "Iteration 37, loss = 0.40077052\n",
      "Iteration 137, loss = 0.17589373\n",
      "Iteration 266, loss = 0.10902761\n",
      "Iteration 138, loss = 0.17406385\n",
      "Iteration 139, loss = 0.17237401\n",
      "Iteration 267, loss = 0.10888294\n",
      "Iteration 123, loss = 0.16289536\n",
      "Iteration 117, loss = 0.18596998\n",
      "Iteration 210, loss = 0.22308115\n",
      "Iteration 50, loss = 0.30220855\n",
      "Iteration 228, loss = 0.12305466\n",
      "Iteration 140, loss = 0.17066284\n",
      "Iteration 38, loss = 0.39768340\n",
      "Iteration 141, loss = 0.16895190\n",
      "Iteration 124, loss = 0.16228730\n",
      "Iteration 229, loss = 0.12290695\n",
      "Iteration 39, loss = 0.39458570\n",
      "Iteration 125, loss = 0.16175221\n",
      "Iteration 268, loss = 0.10876428\n",
      "Iteration 142, loss = 0.16741624\n",
      "Iteration 230, loss = 0.12274502\n",
      "Iteration 118, loss = 0.18538311\n",
      "Iteration 269, loss = 0.10868089\n",
      "Iteration 51, loss = 0.29865989\n",
      "Iteration 211, loss = 0.22297486\n",
      "Iteration 270, loss = 0.10855115\n",
      "Iteration 40, loss = 0.39151547\n",
      "Iteration 119, loss = 0.18476630\n",
      "Iteration 143, loss = 0.16578368\n",
      "Iteration 41, loss = 0.38847349\n",
      "Iteration 144, loss = 0.16425221\n",
      "Iteration 52, loss = 0.29527238\n",
      "Iteration 145, loss = 0.16276480\n",
      "Iteration 271, loss = 0.10842268\n",
      "Iteration 126, loss = 0.16124476\n",
      "Iteration 231, loss = 0.12258831\n",
      "Iteration 146, loss = 0.16125678\n",
      "Iteration 127, loss = 0.16071802\n",
      "Iteration 232, loss = 0.12244161\n",
      "Iteration 147, loss = 0.15988038\n",
      "Iteration 272, loss = 0.10832062\n",
      "Iteration 148, loss = 0.15850437\n",
      "Iteration 212, loss = 0.22278097\n",
      "Iteration 120, loss = 0.18417329\n",
      "Iteration 42, loss = 0.38539936\n",
      "Iteration 149, loss = 0.15709325\n",
      "Iteration 53, loss = 0.29194740\n",
      "Iteration 121, loss = 0.18363581\n",
      "Iteration 150, loss = 0.15579080\n",
      "Iteration 233, loss = 0.12228497\n",
      "Iteration 273, loss = 0.10819535\n",
      "Iteration 151, loss = 0.15447661\n",
      "Iteration 43, loss = 0.38229805\n",
      "Iteration 274, loss = 0.10809689\n",
      "Iteration 234, loss = 0.12214516\n",
      "Iteration 152, loss = 0.15322593\n",
      "Iteration 54, loss = 0.28878494\n",
      "Iteration 44, loss = 0.37930606\n",
      "Iteration 122, loss = 0.18307597\n",
      "Iteration 235, loss = 0.12199293\n",
      "Iteration 275, loss = 0.10798681\n",
      "Iteration 236, loss = 0.12184520\n",
      "Iteration 213, loss = 0.22264108\n",
      "Iteration 153, loss = 0.15197274\n",
      "Iteration 276, loss = 0.10787811\n",
      "Iteration 128, loss = 0.16023475\n",
      "Iteration 154, loss = 0.15077534\n",
      "Iteration 123, loss = 0.18248006\n",
      "Iteration 155, loss = 0.14959049\n",
      "Iteration 129, loss = 0.15974750\n",
      "Iteration 156, loss = 0.14848435\n",
      "Iteration 277, loss = 0.10779451\n",
      "Iteration 237, loss = 0.12169207\n",
      "Iteration 45, loss = 0.37620410\n",
      "Iteration 238, loss = 0.12156033\n",
      "Iteration 130, loss = 0.15925350\n",
      "Iteration 214, loss = 0.22250092\n",
      "Iteration 55, loss = 0.28581163\n",
      "Iteration 157, loss = 0.14743411\n",
      "Iteration 278, loss = 0.10768064\n",
      "Iteration 158, loss = 0.14628212\n",
      "Iteration 46, loss = 0.37319395\n",
      "Iteration 124, loss = 0.18193737\n",
      "Iteration 131, loss = 0.15878149\n",
      "Iteration 56, loss = 0.28297415\n",
      "Iteration 159, loss = 0.14526427\n",
      "Iteration 132, loss = 0.15831465\n",
      "Iteration 47, loss = 0.37019218\n",
      "Iteration 133, loss = 0.15785554\n",
      "Iteration 279, loss = 0.10757188\n",
      "Iteration 239, loss = 0.12141857\n",
      "Iteration 160, loss = 0.14417291\n",
      "Iteration 161, loss = 0.14317344\n",
      "Iteration 280, loss = 0.10745447\n",
      "Iteration 240, loss = 0.12129524Iteration 134, loss = 0.15738893\n",
      "\n",
      "Iteration 125, loss = 0.18142025\n",
      "Iteration 162, loss = 0.14221345\n",
      "Iteration 57, loss = 0.28012232\n",
      "Iteration 48, loss = 0.36717671\n",
      "Iteration 163, loss = 0.14123595\n",
      "Iteration 215, loss = 0.22235353\n",
      "Iteration 281, loss = 0.10734833\n",
      "Iteration 164, loss = 0.14032647\n",
      "Iteration 282, loss = 0.10724125\n",
      "Iteration 283, loss = 0.10714296\n",
      "Iteration 135, loss = 0.15697435\n",
      "Iteration 284, loss = 0.10703440\n",
      "Iteration 49, loss = 0.36410121\n",
      "Iteration 165, loss = 0.13939873\n",
      "Iteration 241, loss = 0.12114006\n",
      "Iteration 166, loss = 0.13844745\n",
      "Iteration 126, loss = 0.18090446\n",
      "Iteration 167, loss = 0.13765483\n",
      "Iteration 136, loss = 0.15651372\n",
      "Iteration 285, loss = 0.10695297\n",
      "Iteration 50, loss = 0.36111301\n",
      "Iteration 216, loss = 0.22221953\n",
      "Iteration 168, loss = 0.13677347\n",
      "Iteration 242, loss = 0.12099308\n",
      "Iteration 58, loss = 0.27751928\n",
      "Iteration 169, loss = 0.13585015\n",
      "Iteration 170, loss = 0.13508268\n",
      "Iteration 51, loss = 0.35810797\n",
      "Iteration 127, loss = 0.18040731\n",
      "Iteration 286, loss = 0.10684694\n",
      "Iteration 137, loss = 0.15610314\n",
      "Iteration 171, loss = 0.13419345\n",
      "Iteration 243, loss = 0.12085705\n",
      "Iteration 52, loss = 0.35521376\n",
      "Iteration 172, loss = 0.13339054\n",
      "Iteration 173, loss = 0.13262601\n",
      "Iteration 174, loss = 0.13181901\n",
      "Iteration 287, loss = 0.10674385\n",
      "Iteration 244, loss = 0.12072938\n",
      "Iteration 175, loss = 0.13104414\n",
      "Iteration 138, loss = 0.15566467\n",
      "Iteration 176, loss = 0.13032675\n",
      "Iteration 59, loss = 0.27500340\n",
      "Iteration 139, loss = 0.15526411\n",
      "Iteration 128, loss = 0.17989296\n",
      "Iteration 288, loss = 0.10667052\n",
      "Iteration 217, loss = 0.22208962\n",
      "Iteration 177, loss = 0.12954474\n",
      "Iteration 289, loss = 0.10660977\n",
      "Iteration 53, loss = 0.35226587\n",
      "Iteration 245, loss = 0.12060629\n",
      "Iteration 60, loss = 0.27254380\n",
      "Iteration 129, loss = 0.17940311\n",
      "Iteration 290, loss = 0.10645261\n",
      "Iteration 246, loss = 0.12044061\n",
      "Iteration 54, loss = 0.34938028\n",
      "Iteration 130, loss = 0.17891074\n",
      "Iteration 140, loss = 0.15486383\n",
      "Iteration 291, loss = 0.10636816\n",
      "Iteration 178, loss = 0.12880381\n",
      "Iteration 179, loss = 0.12809652\n",
      "Iteration 180, loss = 0.12742848\n",
      "Iteration 61, loss = 0.27030468\n",
      "Iteration 141, loss = 0.15448593\n",
      "Iteration 218, loss = 0.22191074\n",
      "Iteration 181, loss = 0.12674169\n",
      "Iteration 292, loss = 0.10627871\n",
      "Iteration 182, loss = 0.12600663\n",
      "Iteration 142, loss = 0.15405422\n",
      "Iteration 247, loss = 0.12032018\n",
      "Iteration 55, loss = 0.34639298\n",
      "Iteration 131, loss = 0.17845700\n",
      "Iteration 293, loss = 0.10618512\n",
      "Iteration 143, loss = 0.15369245\n",
      "Iteration 248, loss = 0.12018254\n",
      "Iteration 62, loss = 0.26799998\n",
      "Iteration 144, loss = 0.15330661\n",
      "Iteration 183, loss = 0.12534762\n",
      "Iteration 56, loss = 0.34347427\n",
      "Iteration 294, loss = 0.10608617\n",
      "Iteration 219, loss = 0.22184903\n",
      "Iteration 184, loss = 0.12463808\n",
      "Iteration 132, loss = 0.17797549\n",
      "Iteration 185, loss = 0.12404878\n",
      "Iteration 186, loss = 0.12341212\n",
      "Iteration 57, loss = 0.34058492\n",
      "Iteration 295, loss = 0.10600078\n",
      "Iteration 249, loss = 0.12004608\n",
      "Iteration 296, loss = 0.10588981\n",
      "Iteration 133, loss = 0.17751886\n",
      "Iteration 145, loss = 0.15295872\n",
      "Iteration 297, loss = 0.10583125\n",
      "Iteration 220, loss = 0.22164952\n",
      "Iteration 58, loss = 0.33765321\n",
      "Iteration 187, loss = 0.12278303\n",
      "Iteration 250, loss = 0.11991132\n",
      "Iteration 146, loss = 0.15260003\n",
      "Iteration 63, loss = 0.26585612\n",
      "Iteration 188, loss = 0.12215208\n",
      "Iteration 189, loss = 0.12164366\n",
      "Iteration 147, loss = 0.15222339\n",
      "Iteration 190, loss = 0.12100278\n",
      "Iteration 134, loss = 0.17705983\n",
      "Iteration 59, loss = 0.33480539\n",
      "Iteration 298, loss = 0.10571262\n",
      "Iteration 221, loss = 0.22153407\n",
      "Iteration 191, loss = 0.12042701\n",
      "Iteration 299, loss = 0.10562557\n",
      "Iteration 251, loss = 0.11978966\n",
      "Iteration 300, loss = 0.10553855\n",
      "Iteration 252, loss = 0.11964780\n",
      "Iteration 148, loss = 0.15187428\n",
      "Iteration 192, loss = 0.11989236\n",
      "Iteration 64, loss = 0.26378783\n",
      "Iteration 135, loss = 0.17661096\n",
      "Iteration 193, loss = 0.11930391\n",
      "Iteration 60, loss = 0.33202370\n",
      "Iteration 149, loss = 0.15153403\n",
      "Iteration 301, loss = 0.10546546\n",
      "Iteration 194, loss = 0.11879603\n",
      "Iteration 195, loss = 0.11829613\n",
      "Iteration 253, loss = 0.11953287\n",
      "Iteration 196, loss = 0.11778673\n",
      "Iteration 136, loss = 0.17618532\n",
      "Iteration 254, loss = 0.11940227\n",
      "Iteration 197, loss = 0.11729244\n",
      "Iteration 61, loss = 0.32916022\n",
      "Iteration 150, loss = 0.15118969\n",
      "Iteration 302, loss = 0.10538941\n",
      "Iteration 65, loss = 0.26180711\n",
      "Iteration 303, loss = 0.10526818\n",
      "Iteration 304, loss = 0.10517673\n",
      "Iteration 151, loss = 0.15084674\n",
      "Iteration 222, loss = 0.22140265\n",
      "Iteration 198, loss = 0.11675889Iteration 62, loss = 0.32648379\n",
      "Iteration 255, loss = 0.11926846\n",
      "\n",
      "Iteration 256, loss = 0.11915171\n",
      "Iteration 63, loss = 0.32375365\n",
      "Iteration 199, loss = 0.11629479\n",
      "Iteration 137, loss = 0.17575417\n",
      "Iteration 152, loss = 0.15053358\n",
      "Iteration 257, loss = 0.11903932\n",
      "Iteration 200, loss = 0.11582706\n",
      "Iteration 66, loss = 0.25989407\n",
      "Iteration 64, loss = 0.32107739\n",
      "Iteration 305, loss = 0.10511131\n",
      "Iteration 201, loss = 0.11536403\n",
      "Iteration 306, loss = 0.10501502\n",
      "Iteration 258, loss = 0.11888761\n",
      "Iteration 138, loss = 0.17534822\n",
      "Iteration 202, loss = 0.11493820\n",
      "Iteration 65, loss = 0.31847655\n",
      "Iteration 223, loss = 0.22126174\n",
      "Iteration 153, loss = 0.15021703\n",
      "Iteration 203, loss = 0.11456895\n",
      "Iteration 154, loss = 0.14993213\n",
      "Iteration 307, loss = 0.10492962\n",
      "Iteration 204, loss = 0.11402265\n",
      "Iteration 308, loss = 0.10484676\n",
      "Iteration 67, loss = 0.25808183\n",
      "Iteration 259, loss = 0.11876963\n",
      "Iteration 205, loss = 0.11356530\n",
      "Iteration 155, loss = 0.14958730\n",
      "Iteration 206, loss = 0.11314038\n",
      "Iteration 66, loss = 0.31591867\n",
      "Iteration 207, loss = 0.11272989\n",
      "Iteration 224, loss = 0.22117197\n",
      "Iteration 309, loss = 0.10476368\n",
      "Iteration 139, loss = 0.17494418\n",
      "Iteration 260, loss = 0.11865643\n",
      "Iteration 156, loss = 0.14928296\n",
      "Iteration 68, loss = 0.25636911\n",
      "Iteration 208, loss = 0.11231829\n",
      "Iteration 310, loss = 0.10468436\n",
      "Iteration 209, loss = 0.11190007\n",
      "Iteration 210, loss = 0.11151672\n",
      "Iteration 311, loss = 0.10460610Iteration 67, loss = 0.31337453\n",
      "\n",
      "Iteration 211, loss = 0.11115895\n",
      "Iteration 140, loss = 0.17453817\n",
      "Iteration 261, loss = 0.11852762\n",
      "Iteration 212, loss = 0.11070099\n",
      "Iteration 157, loss = 0.14895847\n",
      "Iteration 262, loss = 0.11841774\n",
      "Iteration 158, loss = 0.14866002\n",
      "Iteration 263, loss = 0.11830065\n",
      "Iteration 159, loss = 0.14835621\n",
      "Iteration 264, loss = 0.11818013\n",
      "Iteration 312, loss = 0.10454326\n",
      "Iteration 213, loss = 0.11031881\n",
      "Iteration 69, loss = 0.25456168\n",
      "Iteration 141, loss = 0.17411476\n",
      "Iteration 313, loss = 0.10442938\n",
      "Iteration 68, loss = 0.31092694\n",
      "Iteration 69, loss = 0.30839818\n",
      "Iteration 214, loss = 0.10992956\n",
      "Iteration 225, loss = 0.22102146\n",
      "Iteration 215, loss = 0.10957900\n",
      "Iteration 314, loss = 0.10439437\n",
      "Iteration 216, loss = 0.10919261\n",
      "Iteration 160, loss = 0.14809470\n",
      "Iteration 217, loss = 0.10883426\n",
      "Iteration 142, loss = 0.17371346\n",
      "Iteration 161, loss = 0.14781056\n",
      "Iteration 218, loss = 0.10846078\n",
      "Iteration 70, loss = 0.25291457\n",
      "Iteration 219, loss = 0.10810540\n",
      "Iteration 70, loss = 0.30603470\n",
      "Iteration 265, loss = 0.11805905\n",
      "Iteration 315, loss = 0.10427841\n",
      "Iteration 226, loss = 0.22095058\n",
      "Iteration 143, loss = 0.17333548\n",
      "Iteration 220, loss = 0.10775740\n",
      "Iteration 71, loss = 0.30356919\n",
      "Iteration 162, loss = 0.14751064\n",
      "Iteration 163, loss = 0.14721798\n",
      "Iteration 72, loss = 0.30115377\n",
      "Iteration 316, loss = 0.10420071\n",
      "Iteration 221, loss = 0.10740147\n",
      "Iteration 73, loss = 0.29886868\n",
      "Iteration 222, loss = 0.10707603\n",
      "Iteration 266, loss = 0.11793771\n",
      "Iteration 223, loss = 0.10671834\n",
      "Iteration 71, loss = 0.25132923\n",
      "Iteration 74, loss = 0.29643201\n",
      "Iteration 267, loss = 0.11782851\n",
      "Iteration 317, loss = 0.10412295\n",
      "Iteration 144, loss = 0.17294242\n",
      "Iteration 75, loss = 0.29419655\n",
      "Iteration 318, loss = 0.10404752\n",
      "Iteration 224, loss = 0.10641556\n",
      "Iteration 268, loss = 0.11770706\n",
      "Iteration 164, loss = 0.14695904\n",
      "Iteration 319, loss = 0.10396234\n",
      "Iteration 225, loss = 0.10612194\n",
      "Iteration 72, loss = 0.24977598\n",
      "Iteration 227, loss = 0.22076007\n",
      "Iteration 320, loss = 0.10388009\n",
      "Iteration 226, loss = 0.10576681\n",
      "Iteration 145, loss = 0.17256732\n",
      "Iteration 269, loss = 0.11759714\n",
      "Iteration 227, loss = 0.10547683\n",
      "Iteration 165, loss = 0.14665651\n",
      "Iteration 228, loss = 0.10517518\n",
      "Iteration 229, loss = 0.10485794\n",
      "Iteration 146, loss = 0.17220217\n",
      "Iteration 321, loss = 0.10380340\n",
      "Iteration 228, loss = 0.22064441\n",
      "Iteration 166, loss = 0.14638696\n",
      "Iteration 270, loss = 0.11749897\n",
      "Iteration 230, loss = 0.10452388\n",
      "Iteration 73, loss = 0.24828348\n",
      "Iteration 322, loss = 0.10373592\n",
      "Iteration 76, loss = 0.29184462\n",
      "Iteration 231, loss = 0.10425965\n",
      "Iteration 271, loss = 0.11737375\n",
      "Iteration 232, loss = 0.10399556\n",
      "Iteration 167, loss = 0.14611264\n",
      "Iteration 147, loss = 0.17182511\n",
      "Iteration 323, loss = 0.10366898\n",
      "Iteration 77, loss = 0.28965410\n",
      "Iteration 233, loss = 0.10367110\n",
      "Iteration 74, loss = 0.24691289\n",
      "Iteration 324, loss = 0.10358754\n",
      "Iteration 272, loss = 0.11725851\n",
      "Iteration 234, loss = 0.10335977Iteration 148, loss = 0.17148673\n",
      "\n",
      "Iteration 235, loss = 0.10309982\n",
      "Iteration 168, loss = 0.14586918\n",
      "Iteration 75, loss = 0.24548736\n",
      "Iteration 236, loss = 0.10284139\n",
      "Iteration 325, loss = 0.10353192\n",
      "Iteration 273, loss = 0.11715021\n",
      "Iteration 237, loss = 0.10251649\n",
      "Iteration 238, loss = 0.10227900\n",
      "Iteration 326, loss = 0.10342999\n",
      "Iteration 229, loss = 0.22050145\n",
      "Iteration 169, loss = 0.14559712\n",
      "Iteration 78, loss = 0.28733212\n",
      "Iteration 170, loss = 0.14534651\n",
      "Iteration 327, loss = 0.10338505\n",
      "Iteration 274, loss = 0.11704831\n",
      "Iteration 239, loss = 0.10197352\n",
      "Iteration 149, loss = 0.17109330\n",
      "Iteration 240, loss = 0.10177921\n",
      "Iteration 241, loss = 0.10145838\n",
      "Iteration 171, loss = 0.14507112\n",
      "Iteration 230, loss = 0.22037542\n",
      "Iteration 328, loss = 0.10328649\n",
      "Iteration 79, loss = 0.28515583\n",
      "Iteration 275, loss = 0.11693339\n",
      "Iteration 172, loss = 0.14483919\n",
      "Iteration 329, loss = 0.10322677\n",
      "Iteration 76, loss = 0.24415111\n",
      "Iteration 80, loss = 0.28296171\n",
      "Iteration 150, loss = 0.17075893Iteration 330, loss = 0.10314964\n",
      "\n",
      "Iteration 242, loss = 0.10121220\n",
      "Iteration 276, loss = 0.11682194\n",
      "Iteration 173, loss = 0.14453559\n",
      "Iteration 277, loss = 0.11672632\n",
      "Iteration 243, loss = 0.10094025\n",
      "Iteration 244, loss = 0.10069325\n",
      "Iteration 151, loss = 0.17040573\n",
      "Iteration 245, loss = 0.10043478\n",
      "Iteration 174, loss = 0.14428985\n",
      "Iteration 331, loss = 0.10306851\n",
      "Iteration 81, loss = 0.28078350\n",
      "Iteration 278, loss = 0.11660965\n",
      "Iteration 246, loss = 0.10018265\n",
      "Iteration 247, loss = 0.09996450\n",
      "Iteration 279, loss = 0.11651815\n",
      "Iteration 332, loss = 0.10302048\n",
      "Iteration 175, loss = 0.14409392\n",
      "Iteration 231, loss = 0.22025800\n",
      "Iteration 82, loss = 0.27865620\n",
      "Iteration 333, loss = 0.10292849\n",
      "Iteration 176, loss = 0.14381776\n",
      "Iteration 248, loss = 0.09971287\n",
      "Iteration 152, loss = 0.17004234\n",
      "Iteration 77, loss = 0.24282483\n",
      "Iteration 334, loss = 0.10285562\n",
      "Iteration 249, loss = 0.09947312\n",
      "Iteration 335, loss = 0.10279731\n",
      "Iteration 250, loss = 0.09927808\n",
      "Iteration 251, loss = 0.09898490\n",
      "Iteration 83, loss = 0.27656940\n",
      "Iteration 78, loss = 0.24160257\n",
      "Iteration 84, loss = 0.27437995\n",
      "Iteration 177, loss = 0.14359887\n",
      "Iteration 232, loss = 0.22011055\n",
      "Iteration 153, loss = 0.16971935\n",
      "Iteration 252, loss = 0.09878673\n",
      "Iteration 280, loss = 0.11638481\n",
      "Iteration 336, loss = 0.10271658\n",
      "Iteration 253, loss = 0.09854661\n",
      "Iteration 79, loss = 0.24042968\n",
      "Iteration 254, loss = 0.09832245\n",
      "Iteration 255, loss = 0.09816233\n",
      "Iteration 233, loss = 0.22002049\n",
      "Iteration 256, loss = 0.09787947\n",
      "Iteration 154, loss = 0.16940281\n",
      "Iteration 80, loss = 0.23913595\n",
      "Iteration 257, loss = 0.09766806\n",
      "Iteration 281, loss = 0.11630432\n",
      "Iteration 258, loss = 0.09744608\n",
      "Iteration 85, loss = 0.27234925\n",
      "Iteration 178, loss = 0.14335017\n",
      "Iteration 337, loss = 0.10265300\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 282, loss = 0.11618236\n",
      "Iteration 86, loss = 0.27030405\n",
      "Iteration 179, loss = 0.14311851\n",
      "Iteration 155, loss = 0.16905333\n",
      "Iteration 259, loss = 0.09724305\n",
      "Iteration 87, loss = 0.26826229\n",
      "Iteration 260, loss = 0.09708391\n",
      "Iteration 283, loss = 0.11607960\n",
      "Iteration 1, loss = 0.97415125\n",
      "Iteration 284, loss = 0.11598825\n",
      "Iteration 180, loss = 0.14292855\n",
      "Iteration 261, loss = 0.09682126\n",
      "Iteration 181, loss = 0.14265805\n",
      "Iteration 262, loss = 0.09663364\n",
      "Iteration 234, loss = 0.21990188\n",
      "Iteration 88, loss = 0.26629064\n",
      "Iteration 156, loss = 0.16873735\n",
      "Iteration 182, loss = 0.14246749\n",
      "Iteration 183, loss = 0.14219227\n",
      "Iteration 184, loss = 0.14196264\n",
      "Iteration 81, loss = 0.23801884\n",
      "Iteration 185, loss = 0.14172252\n",
      "Iteration 157, loss = 0.16842371\n",
      "Iteration 285, loss = 0.11586240\n",
      "Iteration 263, loss = 0.09643444\n",
      "Iteration 264, loss = 0.09625965\n",
      "Iteration 235, loss = 0.21982960\n",
      "Iteration 286, loss = 0.11577359\n",
      "Iteration 82, loss = 0.23692214\n",
      "Iteration 186, loss = 0.14152187\n",
      "Iteration 265, loss = 0.09610480\n",
      "Iteration 89, loss = 0.26425662\n",
      "Iteration 266, loss = 0.09587414\n",
      "Iteration 90, loss = 0.26230238\n",
      "Iteration 267, loss = 0.09569847\n",
      "Iteration 2, loss = 0.93427263\n",
      "Iteration 287, loss = 0.11567381\n",
      "Iteration 158, loss = 0.16810564\n",
      "Iteration 268, loss = 0.09548776\n",
      "Iteration 269, loss = 0.09535648\n",
      "Iteration 83, loss = 0.23579698\n",
      "Iteration 270, loss = 0.09512681\n",
      "Iteration 159, loss = 0.16777564\n",
      "Iteration 271, loss = 0.09495551\n",
      "Iteration 91, loss = 0.26037730\n",
      "Iteration 272, loss = 0.09478319\n",
      "Iteration 288, loss = 0.11555947\n",
      "Iteration 273, loss = 0.09460736\n",
      "Iteration 274, loss = 0.09445688\n",
      "Iteration 187, loss = 0.14131890\n",
      "Iteration 289, loss = 0.11550270\n",
      "Iteration 3, loss = 0.88389128\n",
      "Iteration 188, loss = 0.14110273\n",
      "Iteration 236, loss = 0.21965933\n",
      "Iteration 92, loss = 0.25841723\n",
      "Iteration 160, loss = 0.16749746\n",
      "Iteration 275, loss = 0.09432881\n",
      "Iteration 290, loss = 0.11535054\n",
      "Iteration 84, loss = 0.23476732\n",
      "Iteration 189, loss = 0.14089202\n",
      "Iteration 276, loss = 0.09410635\n",
      "Iteration 190, loss = 0.14069805\n",
      "Iteration 277, loss = 0.09401373\n",
      "Iteration 4, loss = 0.83512055\n",
      "Iteration 278, loss = 0.09384192\n",
      "Iteration 191, loss = 0.14047069\n",
      "Iteration 291, loss = 0.11526924\n",
      "Iteration 93, loss = 0.25655637\n",
      "Iteration 161, loss = 0.16718517\n",
      "Iteration 237, loss = 0.21956131\n",
      "Iteration 292, loss = 0.11516303\n",
      "Iteration 279, loss = 0.09362426\n",
      "Iteration 85, loss = 0.23377247\n",
      "Iteration 192, loss = 0.14027092\n",
      "Iteration 280, loss = 0.09357856\n",
      "Iteration 162, loss = 0.16689254\n",
      "Iteration 281, loss = 0.09330782\n",
      "Iteration 193, loss = 0.14012668\n",
      "Iteration 282, loss = 0.09314949\n",
      "Iteration 94, loss = 0.25465111\n",
      "Iteration 293, loss = 0.11506747\n",
      "Iteration 283, loss = 0.09302466\n",
      "Iteration 284, loss = 0.09288370\n",
      "Iteration 194, loss = 0.13984140\n",
      "Iteration 294, loss = 0.11496459\n",
      "Iteration 86, loss = 0.23273195\n",
      "Iteration 5, loss = 0.79024274\n",
      "Iteration 238, loss = 0.21941262\n",
      "Iteration 95, loss = 0.25272941\n",
      "Iteration 163, loss = 0.16659750\n",
      "Iteration 195, loss = 0.13968202\n",
      "Iteration 285, loss = 0.09271672\n",
      "Iteration 295, loss = 0.11486877\n",
      "Iteration 286, loss = 0.09257773\n",
      "Iteration 287, loss = 0.09247778\n",
      "Iteration 164, loss = 0.16629408\n",
      "Iteration 96, loss = 0.25095989\n",
      "Iteration 296, loss = 0.11479004\n",
      "Iteration 87, loss = 0.23177683\n",
      "Iteration 97, loss = 0.24916022\n",
      "Iteration 6, loss = 0.75262341\n",
      "Iteration 196, loss = 0.13944964\n",
      "Iteration 288, loss = 0.09229396\n",
      "Iteration 165, loss = 0.16598362\n",
      "Iteration 289, loss = 0.09212890\n",
      "Iteration 297, loss = 0.11468502\n",
      "Iteration 88, loss = 0.23081071\n",
      "Iteration 197, loss = 0.13927338\n",
      "Iteration 239, loss = 0.21932635\n",
      "Iteration 198, loss = 0.13908509\n",
      "Iteration 290, loss = 0.09199845\n",
      "Iteration 98, loss = 0.24735704\n",
      "Iteration 199, loss = 0.13887755\n",
      "Iteration 291, loss = 0.09182153\n",
      "Iteration 7, loss = 0.71949428\n",
      "Iteration 292, loss = 0.09168867\n",
      "Iteration 293, loss = 0.09158146\n",
      "Iteration 166, loss = 0.16571642\n",
      "Iteration 99, loss = 0.24561154\n",
      "Iteration 298, loss = 0.11458043\n",
      "Iteration 299, loss = 0.11447987\n",
      "Iteration 89, loss = 0.22991709\n",
      "Iteration 294, loss = 0.09142376\n",
      "Iteration 240, loss = 0.21924951\n",
      "Iteration 295, loss = 0.09130089\n",
      "Iteration 200, loss = 0.13868800\n",
      "Iteration 300, loss = 0.11440695\n",
      "Iteration 296, loss = 0.09119881\n",
      "Iteration 297, loss = 0.09102323\n",
      "Iteration 301, loss = 0.11429249\n",
      "Iteration 201, loss = 0.13847959\n",
      "Iteration 8, loss = 0.69064012\n",
      "Iteration 167, loss = 0.16542136\n",
      "Iteration 100, loss = 0.24385825\n",
      "Iteration 298, loss = 0.09089193\n",
      "Iteration 299, loss = 0.09085845\n",
      "Iteration 300, loss = 0.09066380\n",
      "Iteration 90, loss = 0.22901168\n",
      "Iteration 301, loss = 0.09057246\n",
      "Iteration 302, loss = 0.11419201\n",
      "Iteration 168, loss = 0.16513840\n",
      "Iteration 303, loss = 0.11410065\n",
      "Iteration 202, loss = 0.13834367\n",
      "Iteration 101, loss = 0.24214492\n",
      "Iteration 241, loss = 0.21909336\n",
      "Iteration 9, loss = 0.66694463\n",
      "Iteration 203, loss = 0.13811242\n",
      "Iteration 169, loss = 0.16488704\n",
      "Iteration 302, loss = 0.09040926\n",
      "Iteration 204, loss = 0.13797600\n",
      "Iteration 102, loss = 0.24045533\n",
      "Iteration 91, loss = 0.22810384\n",
      "Iteration 303, loss = 0.09029344\n",
      "Iteration 304, loss = 0.11401849\n",
      "Iteration 304, loss = 0.09015228\n",
      "Iteration 170, loss = 0.16461708\n",
      "Iteration 103, loss = 0.23880572\n",
      "Iteration 205, loss = 0.13773054\n",
      "Iteration 305, loss = 0.11391390\n",
      "Iteration 242, loss = 0.21904247\n",
      "Iteration 305, loss = 0.09003019\n",
      "Iteration 92, loss = 0.22726642\n",
      "Iteration 306, loss = 0.08993063\n",
      "Iteration 206, loss = 0.13755945\n",
      "Iteration 171, loss = 0.16432665\n",
      "Iteration 10, loss = 0.64570720\n",
      "Iteration 307, loss = 0.08983309\n",
      "Iteration 308, loss = 0.08967295\n",
      "Iteration 104, loss = 0.23713135\n",
      "Iteration 306, loss = 0.11381967\n",
      "Iteration 207, loss = 0.13737400\n",
      "Iteration 208, loss = 0.13718570\n",
      "Iteration 307, loss = 0.11372869\n",
      "Iteration 209, loss = 0.13702320\n",
      "Iteration 93, loss = 0.22640976\n",
      "Iteration 308, loss = 0.11364536\n",
      "Iteration 309, loss = 0.08956286\n",
      "Iteration 105, loss = 0.23551996\n",
      "Iteration 172, loss = 0.16406626\n",
      "Iteration 243, loss = 0.21886774\n",
      "Iteration 11, loss = 0.62744906\n",
      "Iteration 310, loss = 0.08946332\n",
      "Iteration 106, loss = 0.23393392\n",
      "Iteration 311, loss = 0.08934750\n",
      "Iteration 312, loss = 0.08922744\n",
      "Iteration 313, loss = 0.08913814\n",
      "Iteration 314, loss = 0.08900991Iteration 94, loss = 0.22560876\n",
      "\n",
      "Iteration 173, loss = 0.16382392\n",
      "Iteration 309, loss = 0.11355081\n",
      "Iteration 210, loss = 0.13682321\n",
      "Iteration 315, loss = 0.08889183\n",
      "Iteration 310, loss = 0.11345044\n",
      "Iteration 12, loss = 0.61165680\n",
      "Iteration 316, loss = 0.08877569\n",
      "Iteration 244, loss = 0.21882525\n",
      "Iteration 317, loss = 0.08867554\n",
      "Iteration 174, loss = 0.16356503\n",
      "Iteration 211, loss = 0.13664184\n",
      "Iteration 107, loss = 0.23231006\n",
      "Iteration 311, loss = 0.11337313\n",
      "Iteration 95, loss = 0.22482151\n",
      "Iteration 318, loss = 0.08855432\n",
      "Iteration 319, loss = 0.08846832\n",
      "Iteration 212, loss = 0.13649744\n",
      "Iteration 312, loss = 0.11327761\n",
      "Iteration 320, loss = 0.08834564\n",
      "Iteration 213, loss = 0.13630476\n",
      "Iteration 108, loss = 0.23081214\n",
      "Iteration 109, loss = 0.22925011\n",
      "Iteration 321, loss = 0.08829010\n",
      "Iteration 175, loss = 0.16333134\n",
      "Iteration 96, loss = 0.22403210\n",
      "Iteration 13, loss = 0.59823372\n",
      "Iteration 322, loss = 0.08817000\n",
      "Iteration 245, loss = 0.21867013\n",
      "Iteration 313, loss = 0.11319104\n",
      "Iteration 214, loss = 0.13616547\n",
      "Iteration 323, loss = 0.08804848\n",
      "Iteration 176, loss = 0.16305058\n",
      "Iteration 110, loss = 0.22772076\n",
      "Iteration 324, loss = 0.08795528\n",
      "Iteration 325, loss = 0.08784613\n",
      "Iteration 314, loss = 0.11310546\n",
      "Iteration 97, loss = 0.22322306\n",
      "Iteration 315, loss = 0.11303872\n",
      "Iteration 215, loss = 0.13598084\n",
      "Iteration 14, loss = 0.58617596\n",
      "Iteration 246, loss = 0.21861150\n",
      "Iteration 326, loss = 0.08779259\n",
      "Iteration 177, loss = 0.16281842\n",
      "Iteration 327, loss = 0.08767440\n",
      "Iteration 111, loss = 0.22627630\n",
      "Iteration 316, loss = 0.11295383\n",
      "Iteration 328, loss = 0.08757917\n",
      "Iteration 216, loss = 0.13581516\n",
      "Iteration 329, loss = 0.08746724\n",
      "Iteration 112, loss = 0.22485108\n",
      "Iteration 247, loss = 0.21849148\n",
      "Iteration 317, loss = 0.11284996\n",
      "Iteration 330, loss = 0.08739365\n",
      "Iteration 178, loss = 0.16257157\n",
      "Iteration 98, loss = 0.22250803\n",
      "Iteration 113, loss = 0.22342550\n",
      "Iteration 217, loss = 0.13561673\n",
      "Iteration 331, loss = 0.08727635\n",
      "Iteration 332, loss = 0.08719184\n",
      "Iteration 15, loss = 0.57601963\n",
      "Iteration 333, loss = 0.08710952\n",
      "Iteration 334, loss = 0.08703035\n",
      "Iteration 318, loss = 0.11276862\n",
      "Iteration 248, loss = 0.21836838\n",
      "Iteration 319, loss = 0.11267778\n",
      "Iteration 218, loss = 0.13548799\n",
      "Iteration 114, loss = 0.22198166\n",
      "Iteration 99, loss = 0.22177595\n",
      "Iteration 179, loss = 0.16235204\n",
      "Iteration 335, loss = 0.08692014\n",
      "Iteration 219, loss = 0.13533147\n",
      "Iteration 336, loss = 0.08689756\n",
      "Iteration 337, loss = 0.08676163\n",
      "Iteration 220, loss = 0.13516745\n",
      "Iteration 320, loss = 0.11260044\n",
      "Iteration 100, loss = 0.22105623\n",
      "Iteration 16, loss = 0.56680232\n",
      "Iteration 180, loss = 0.16212213\n",
      "Iteration 115, loss = 0.22065641\n",
      "Iteration 338, loss = 0.08672020\n",
      "Iteration 221, loss = 0.13504666\n",
      "Iteration 249, loss = 0.21824780\n",
      "Iteration 321, loss = 0.11250661\n",
      "Iteration 339, loss = 0.08658621\n",
      "Iteration 116, loss = 0.21928852Iteration 181, loss = 0.16184366\n",
      "\n",
      "Iteration 222, loss = 0.13485495\n",
      "Iteration 322, loss = 0.11243322\n",
      "Iteration 340, loss = 0.08651171\n",
      "Iteration 341, loss = 0.08641897\n",
      "Iteration 101, loss = 0.22034100\n",
      "Iteration 342, loss = 0.08633119\n",
      "Iteration 323, loss = 0.11234911\n",
      "Iteration 17, loss = 0.55831413\n",
      "Iteration 102, loss = 0.21965715\n",
      "Iteration 343, loss = 0.08625641\n",
      "Iteration 117, loss = 0.21798371\n",
      "Iteration 223, loss = 0.13467548\n",
      "Iteration 182, loss = 0.16165263\n",
      "Iteration 344, loss = 0.08616637\n",
      "Iteration 224, loss = 0.13455189\n",
      "Iteration 324, loss = 0.11226964\n",
      "Iteration 345, loss = 0.08614327\n",
      "Iteration 118, loss = 0.21667123\n",
      "Iteration 250, loss = 0.21820338\n",
      "Iteration 18, loss = 0.55089438\n",
      "Iteration 346, loss = 0.08601674\n",
      "Iteration 225, loss = 0.13435999\n",
      "Iteration 226, loss = 0.13422328\n",
      "Iteration 119, loss = 0.21541693\n",
      "Iteration 325, loss = 0.11219259\n",
      "Iteration 183, loss = 0.16137980\n",
      "Iteration 347, loss = 0.08592077\n",
      "Iteration 103, loss = 0.21900774\n",
      "Iteration 348, loss = 0.08588602\n",
      "Iteration 326, loss = 0.11209435\n",
      "Iteration 19, loss = 0.54405430\n",
      "Iteration 227, loss = 0.13409264\n",
      "Iteration 349, loss = 0.08578570\n",
      "Iteration 184, loss = 0.16123134\n",
      "Iteration 350, loss = 0.08573470\n",
      "Iteration 120, loss = 0.21418289\n",
      "Iteration 251, loss = 0.21805972\n",
      "Iteration 185, loss = 0.16094632\n",
      "Iteration 327, loss = 0.11201911\n",
      "Iteration 104, loss = 0.21832768\n",
      "Iteration 351, loss = 0.08565924\n",
      "Iteration 352, loss = 0.08554794\n",
      "Iteration 228, loss = 0.13390988\n",
      "Iteration 252, loss = 0.21809250\n",
      "Iteration 353, loss = 0.08548586\n",
      "Iteration 354, loss = 0.08543905\n",
      "Iteration 355, loss = 0.08534221\n",
      "Iteration 121, loss = 0.21299409\n",
      "Iteration 20, loss = 0.53768617\n",
      "Iteration 356, loss = 0.08527292\n",
      "Iteration 186, loss = 0.16072829\n",
      "Iteration 328, loss = 0.11193311\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 229, loss = 0.13377539\n",
      "Iteration 105, loss = 0.21768722\n",
      "Iteration 1, loss = 1.07626223\n",
      "Iteration 2, loss = 1.00735814\n",
      "Iteration 3, loss = 0.91677405\n",
      "Iteration 4, loss = 0.82448760\n",
      "Iteration 122, loss = 0.21172914\n",
      "Iteration 5, loss = 0.74112367\n",
      "Iteration 6, loss = 0.66864629\n",
      "Iteration 7, loss = 0.60785786\n",
      "Iteration 8, loss = 0.55840441\n",
      "Iteration 9, loss = 0.51729070\n",
      "Iteration 10, loss = 0.48295499\n",
      "Iteration 11, loss = 0.45350655\n",
      "Iteration 230, loss = 0.13363359\n",
      "Iteration 12, loss = 0.42904135\n",
      "Iteration 13, loss = 0.40767335\n",
      "Iteration 14, loss = 0.38937862\n",
      "Iteration 15, loss = 0.37323330\n",
      "Iteration 16, loss = 0.35874384\n",
      "Iteration 17, loss = 0.34580272\n",
      "Iteration 18, loss = 0.33440347\n",
      "Iteration 123, loss = 0.21066069\n",
      "Iteration 19, loss = 0.32379257\n",
      "Iteration 20, loss = 0.31416380\n",
      "Iteration 21, loss = 0.30543763\n",
      "Iteration 22, loss = 0.29721691\n",
      "Iteration 23, loss = 0.28982752\n",
      "Iteration 24, loss = 0.28284450\n",
      "Iteration 25, loss = 0.27639946\n",
      "Iteration 231, loss = 0.13349797\n",
      "Iteration 26, loss = 0.27037403\n",
      "Iteration 27, loss = 0.26479868\n",
      "Iteration 28, loss = 0.25950246\n",
      "Iteration 357, loss = 0.08519421\n",
      "Iteration 187, loss = 0.16049622\n",
      "Iteration 358, loss = 0.08513766\n",
      "Iteration 359, loss = 0.08507129\n",
      "Iteration 360, loss = 0.08499787\n",
      "Iteration 124, loss = 0.20939402\n",
      "Iteration 253, loss = 0.21783308\n",
      "Iteration 232, loss = 0.13333787\n",
      "Iteration 29, loss = 0.25461858\n",
      "Iteration 30, loss = 0.24989184\n",
      "Iteration 21, loss = 0.53168176\n",
      "Iteration 106, loss = 0.21705740\n",
      "Iteration 31, loss = 0.24551213\n",
      "Iteration 32, loss = 0.24144435\n",
      "Iteration 33, loss = 0.23743147\n",
      "Iteration 361, loss = 0.08493651\n",
      "Iteration 34, loss = 0.23373522\n",
      "Iteration 35, loss = 0.23018142\n",
      "Iteration 36, loss = 0.22681842\n",
      "Iteration 37, loss = 0.22365337\n",
      "Iteration 38, loss = 0.22055844\n",
      "Iteration 39, loss = 0.21765832\n",
      "Iteration 362, loss = 0.08484858\n",
      "Iteration 40, loss = 0.21486663\n",
      "Iteration 41, loss = 0.21221281\n",
      "Iteration 42, loss = 0.20964501\n",
      "Iteration 43, loss = 0.20719643\n",
      "Iteration 44, loss = 0.20484651\n",
      "Iteration 125, loss = 0.20827601\n",
      "Iteration 45, loss = 0.20255132\n",
      "Iteration 188, loss = 0.16026743\n",
      "Iteration 46, loss = 0.20039015\n",
      "Iteration 363, loss = 0.08484935\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 107, loss = 0.21650445\n",
      "Iteration 47, loss = 0.19828459\n",
      "Iteration 48, loss = 0.19623161\n",
      "Iteration 49, loss = 0.19431532\n",
      "Iteration 50, loss = 0.19241303\n",
      "Iteration 1, loss = 0.69439855\n",
      "Iteration 51, loss = 0.19054789\n",
      "Iteration 2, loss = 0.66221289\n",
      "Iteration 52, loss = 0.18881038\n",
      "Iteration 3, loss = 0.62031308\n",
      "Iteration 53, loss = 0.18706128\n",
      "Iteration 4, loss = 0.57702493\n",
      "Iteration 233, loss = 0.13320908\n",
      "Iteration 54, loss = 0.18539857\n",
      "Iteration 55, loss = 0.18379942\n",
      "Iteration 56, loss = 0.18224404\n",
      "Iteration 126, loss = 0.20718098\n",
      "Iteration 57, loss = 0.18068405\n",
      "Iteration 58, loss = 0.17921810\n",
      "Iteration 234, loss = 0.13311083\n",
      "Iteration 59, loss = 0.17778364\n",
      "Iteration 189, loss = 0.16005089\n",
      "Iteration 60, loss = 0.17638288\n",
      "Iteration 22, loss = 0.52571544\n",
      "Iteration 61, loss = 0.17502317\n",
      "Iteration 62, loss = 0.17367950\n",
      "Iteration 63, loss = 0.17240492\n",
      "Iteration 64, loss = 0.17111342\n",
      "Iteration 65, loss = 0.16988536\n",
      "Iteration 66, loss = 0.16866521\n",
      "Iteration 67, loss = 0.16749909\n",
      "Iteration 68, loss = 0.16633516\n",
      "Iteration 69, loss = 0.16520598\n",
      "Iteration 70, loss = 0.16410288\n",
      "Iteration 71, loss = 0.16304192\n",
      "Iteration 72, loss = 0.16196714\n",
      "Iteration 127, loss = 0.20606142\n",
      "Iteration 73, loss = 0.16094505\n",
      "Iteration 5, loss = 0.53668088\n",
      "Iteration 254, loss = 0.21780099\n",
      "Iteration 6, loss = 0.50157457\n",
      "Iteration 7, loss = 0.47111344\n",
      "Iteration 8, loss = 0.44504902\n",
      "Iteration 9, loss = 0.42258135\n",
      "Iteration 10, loss = 0.40327236\n",
      "Iteration 11, loss = 0.38607437\n",
      "Iteration 12, loss = 0.37113697\n",
      "Iteration 13, loss = 0.35779876\n",
      "Iteration 14, loss = 0.34591673\n",
      "Iteration 108, loss = 0.21583234\n",
      "Iteration 15, loss = 0.33488415\n",
      "Iteration 16, loss = 0.32508236\n",
      "Iteration 17, loss = 0.31581334\n",
      "Iteration 74, loss = 0.15991327\n",
      "Iteration 18, loss = 0.30752794\n",
      "Iteration 19, loss = 0.29974840\n",
      "Iteration 20, loss = 0.29252271\n",
      "Iteration 21, loss = 0.28578228\n",
      "Iteration 22, loss = 0.27952790\n",
      "Iteration 23, loss = 0.27355624\n",
      "Iteration 75, loss = 0.15893753\n",
      "Iteration 76, loss = 0.15796750\n",
      "Iteration 77, loss = 0.15700402\n",
      "Iteration 78, loss = 0.15608877\n",
      "Iteration 24, loss = 0.26800410\n",
      "Iteration 79, loss = 0.15516911\n",
      "Iteration 80, loss = 0.15429039\n",
      "Iteration 81, loss = 0.15338971\n",
      "Iteration 82, loss = 0.15254148\n",
      "Iteration 83, loss = 0.15167905\n",
      "Iteration 235, loss = 0.13291105\n",
      "Iteration 84, loss = 0.15084578\n",
      "Iteration 190, loss = 0.15984784\n",
      "Iteration 85, loss = 0.15003738\n",
      "Iteration 86, loss = 0.14924359\n",
      "Iteration 87, loss = 0.14843864\n",
      "Iteration 88, loss = 0.14767777\n",
      "Iteration 89, loss = 0.14691557\n",
      "Iteration 236, loss = 0.13278657\n",
      "Iteration 90, loss = 0.14616370\n",
      "Iteration 255, loss = 0.21765081\n",
      "Iteration 128, loss = 0.20500523\n",
      "Iteration 91, loss = 0.14542281\n",
      "Iteration 25, loss = 0.26282206\n",
      "Iteration 237, loss = 0.13264447\n",
      "Iteration 26, loss = 0.25769455\n",
      "Iteration 92, loss = 0.14470276\n",
      "Iteration 93, loss = 0.14399632\n",
      "Iteration 94, loss = 0.14329392\n",
      "Iteration 27, loss = 0.25294985\n",
      "Iteration 95, loss = 0.14260703\n",
      "Iteration 109, loss = 0.21523324\n",
      "Iteration 96, loss = 0.14192571\n",
      "Iteration 28, loss = 0.24861198\n",
      "Iteration 23, loss = 0.52006538\n",
      "Iteration 97, loss = 0.14127340\n",
      "Iteration 98, loss = 0.14059519\n",
      "Iteration 99, loss = 0.13996812\n",
      "Iteration 100, loss = 0.13932030\n",
      "Iteration 101, loss = 0.13868017\n",
      "Iteration 29, loss = 0.24433045\n",
      "Iteration 102, loss = 0.13808010\n",
      "Iteration 191, loss = 0.15965445\n",
      "Iteration 129, loss = 0.20393634\n",
      "Iteration 30, loss = 0.24026207\n",
      "Iteration 31, loss = 0.23640586\n",
      "Iteration 32, loss = 0.23269494\n",
      "Iteration 103, loss = 0.13746212\n",
      "Iteration 33, loss = 0.22921166\n",
      "Iteration 130, loss = 0.20290872\n",
      "Iteration 34, loss = 0.22583610\n",
      "Iteration 35, loss = 0.22258355\n",
      "Iteration 104, loss = 0.13686932\n",
      "Iteration 36, loss = 0.21947625\n",
      "Iteration 105, loss = 0.13626798\n",
      "Iteration 37, loss = 0.21647915\n",
      "Iteration 106, loss = 0.13568283\n",
      "Iteration 38, loss = 0.21362234\n",
      "Iteration 107, loss = 0.13512657\n",
      "Iteration 108, loss = 0.13455940\n",
      "Iteration 39, loss = 0.21090163\n",
      "Iteration 40, loss = 0.20823220\n",
      "Iteration 109, loss = 0.13399467\n",
      "Iteration 41, loss = 0.20565713\n",
      "Iteration 42, loss = 0.20318520\n",
      "Iteration 43, loss = 0.20080192\n",
      "Iteration 192, loss = 0.15944605\n",
      "Iteration 110, loss = 0.13344238\n",
      "Iteration 111, loss = 0.13289882\n",
      "Iteration 112, loss = 0.13237153\n",
      "Iteration 238, loss = 0.13252425\n",
      "Iteration 113, loss = 0.13183293\n",
      "Iteration 114, loss = 0.13133308\n",
      "Iteration 115, loss = 0.13081520\n",
      "Iteration 116, loss = 0.13030947\n",
      "Iteration 117, loss = 0.12980450\n",
      "Iteration 118, loss = 0.12930984\n",
      "Iteration 119, loss = 0.12882107\n",
      "Iteration 239, loss = 0.13241564\n",
      "Iteration 24, loss = 0.51456295\n",
      "Iteration 120, loss = 0.12834695\n",
      "Iteration 121, loss = 0.12786409\n",
      "Iteration 122, loss = 0.12739576\n",
      "Iteration 123, loss = 0.12695060\n",
      "Iteration 124, loss = 0.12647780\n",
      "Iteration 125, loss = 0.12601790\n",
      "Iteration 126, loss = 0.12557683\n",
      "Iteration 127, loss = 0.12513680\n",
      "Iteration 128, loss = 0.12470377\n",
      "Iteration 44, loss = 0.19850363\n",
      "Iteration 45, loss = 0.19625204\n",
      "Iteration 46, loss = 0.19411183\n",
      "Iteration 47, loss = 0.19202260\n",
      "Iteration 48, loss = 0.18996968\n",
      "Iteration 129, loss = 0.12428022\n",
      "Iteration 49, loss = 0.18801960\n",
      "Iteration 130, loss = 0.12384952\n",
      "Iteration 50, loss = 0.18608508\n",
      "Iteration 110, loss = 0.21470097\n",
      "Iteration 131, loss = 0.12342652\n",
      "Iteration 132, loss = 0.12301388\n",
      "Iteration 131, loss = 0.20193465\n",
      "Iteration 51, loss = 0.18424663\n",
      "Iteration 52, loss = 0.18244123\n",
      "Iteration 53, loss = 0.18066847\n",
      "Iteration 133, loss = 0.12260400\n",
      "Iteration 134, loss = 0.12219847\n",
      "Iteration 135, loss = 0.12180683\n",
      "Iteration 136, loss = 0.12141096\n",
      "Iteration 137, loss = 0.12101535\n",
      "Iteration 240, loss = 0.13226315\n",
      "Iteration 54, loss = 0.17895266\n",
      "Iteration 138, loss = 0.12062768\n",
      "Iteration 193, loss = 0.15921953\n",
      "Iteration 139, loss = 0.12024044\n",
      "Iteration 55, loss = 0.17730715\n",
      "Iteration 140, loss = 0.11986055\n",
      "Iteration 256, loss = 0.21762809\n",
      "Iteration 56, loss = 0.17569117\n",
      "Iteration 141, loss = 0.11948903\n",
      "Iteration 142, loss = 0.11911447\n",
      "Iteration 57, loss = 0.17414751\n",
      "Iteration 143, loss = 0.11874323\n",
      "Iteration 58, loss = 0.17255743\n",
      "Iteration 59, loss = 0.17107732\n",
      "Iteration 144, loss = 0.11837469\n",
      "Iteration 241, loss = 0.13211191\n",
      "Iteration 145, loss = 0.11801390\n",
      "Iteration 146, loss = 0.11766456\n",
      "Iteration 147, loss = 0.11729624\n",
      "Iteration 148, loss = 0.11696117\n",
      "Iteration 60, loss = 0.16961123\n",
      "Iteration 132, loss = 0.20092309\n",
      "Iteration 194, loss = 0.15902202\n",
      "Iteration 25, loss = 0.50887473\n",
      "Iteration 61, loss = 0.16816745\n",
      "Iteration 242, loss = 0.13199325\n",
      "Iteration 133, loss = 0.19994470\n",
      "Iteration 149, loss = 0.11659529\n",
      "Iteration 62, loss = 0.16681250\n",
      "Iteration 150, loss = 0.11625215\n",
      "Iteration 111, loss = 0.21412085\n",
      "Iteration 151, loss = 0.11591099\n",
      "Iteration 152, loss = 0.11556956\n",
      "Iteration 153, loss = 0.11524122\n",
      "Iteration 63, loss = 0.16542951\n",
      "Iteration 154, loss = 0.11490315\n",
      "Iteration 64, loss = 0.16409241\n",
      "Iteration 155, loss = 0.11458502\n",
      "Iteration 156, loss = 0.11426247\n",
      "Iteration 65, loss = 0.16283121\n",
      "Iteration 66, loss = 0.16156070\n",
      "Iteration 157, loss = 0.11393298\n",
      "Iteration 67, loss = 0.16036198\n",
      "Iteration 158, loss = 0.11360492\n",
      "Iteration 68, loss = 0.15914873\n",
      "Iteration 159, loss = 0.11329527\n",
      "Iteration 69, loss = 0.15794291\n",
      "Iteration 160, loss = 0.11298748\n",
      "Iteration 161, loss = 0.11267699\n",
      "Iteration 70, loss = 0.15679698\n",
      "Iteration 162, loss = 0.11236810\n",
      "Iteration 71, loss = 0.15569191\n",
      "Iteration 163, loss = 0.11206894\n",
      "Iteration 164, loss = 0.11175307\n",
      "Iteration 165, loss = 0.11145205\n",
      "Iteration 257, loss = 0.21746443\n",
      "Iteration 166, loss = 0.11116777\n",
      "Iteration 72, loss = 0.15459202\n",
      "Iteration 73, loss = 0.15351695\n",
      "Iteration 74, loss = 0.15245354\n",
      "Iteration 75, loss = 0.15144556\n",
      "Iteration 76, loss = 0.15042429\n",
      "Iteration 167, loss = 0.11086530\n",
      "Iteration 77, loss = 0.14942957\n",
      "Iteration 78, loss = 0.14846281\n",
      "Iteration 168, loss = 0.11057263\n",
      "Iteration 169, loss = 0.11029055\n",
      "Iteration 79, loss = 0.14751430\n",
      "Iteration 170, loss = 0.11000234Iteration 243, loss = 0.13184901\n",
      "Iteration 80, loss = 0.14657702\n",
      "\n",
      "Iteration 171, loss = 0.10971684\n",
      "Iteration 81, loss = 0.14565424\n",
      "Iteration 82, loss = 0.14475051\n",
      "Iteration 172, loss = 0.10944689\n",
      "Iteration 195, loss = 0.15886024\n",
      "Iteration 173, loss = 0.10916365\n",
      "Iteration 174, loss = 0.10888487\n",
      "Iteration 175, loss = 0.10860942\n",
      "Iteration 176, loss = 0.10833166\n",
      "Iteration 83, loss = 0.14389659\n",
      "Iteration 177, loss = 0.10805885\n",
      "Iteration 84, loss = 0.14300498\n",
      "Iteration 134, loss = 0.19898698\n",
      "Iteration 85, loss = 0.14216501\n",
      "Iteration 178, loss = 0.10779862\n",
      "Iteration 179, loss = 0.10752744\n",
      "Iteration 86, loss = 0.14131913\n",
      "Iteration 87, loss = 0.14049413\n",
      "Iteration 88, loss = 0.13970813\n",
      "Iteration 196, loss = 0.15859560\n",
      "Iteration 180, loss = 0.10726014\n",
      "Iteration 181, loss = 0.10699874\n",
      "Iteration 182, loss = 0.10673397\n",
      "Iteration 183, loss = 0.10647618\n",
      "Iteration 244, loss = 0.13174868\n",
      "Iteration 184, loss = 0.10622162\n",
      "Iteration 185, loss = 0.10596191\n",
      "Iteration 186, loss = 0.10570796\n",
      "Iteration 187, loss = 0.10545484\n",
      "Iteration 112, loss = 0.21355167\n",
      "Iteration 188, loss = 0.10520893\n",
      "Iteration 189, loss = 0.10495868\n",
      "Iteration 190, loss = 0.10471022\n",
      "Iteration 191, loss = 0.10446437\n",
      "Iteration 192, loss = 0.10422930\n",
      "Iteration 193, loss = 0.10398553\n",
      "Iteration 194, loss = 0.10374122\n",
      "Iteration 195, loss = 0.10350098\n",
      "Iteration 196, loss = 0.10326586\n",
      "Iteration 197, loss = 0.10302687\n",
      "Iteration 198, loss = 0.10278697\n",
      "Iteration 199, loss = 0.10256317\n",
      "Iteration 200, loss = 0.10232799\n",
      "Iteration 89, loss = 0.13888062\n",
      "Iteration 201, loss = 0.10209203\n",
      "Iteration 202, loss = 0.10186728\n",
      "Iteration 203, loss = 0.10163600\n",
      "Iteration 258, loss = 0.21739451\n",
      "Iteration 204, loss = 0.10140678\n",
      "Iteration 90, loss = 0.13811058\n",
      "Iteration 205, loss = 0.10119205\n",
      "Iteration 206, loss = 0.10095732\n",
      "Iteration 91, loss = 0.13733727\n",
      "Iteration 207, loss = 0.10072841\n",
      "Iteration 208, loss = 0.10050629\n",
      "Iteration 209, loss = 0.10028527\n",
      "Iteration 210, loss = 0.10006335\n",
      "Iteration 211, loss = 0.09984011\n",
      "Iteration 92, loss = 0.13659072\n",
      "Iteration 135, loss = 0.19803485\n",
      "Iteration 212, loss = 0.09962312\n",
      "Iteration 93, loss = 0.13585136\n",
      "Iteration 26, loss = 0.50351987\n",
      "Iteration 245, loss = 0.13162215\n",
      "Iteration 94, loss = 0.13511235\n",
      "Iteration 95, loss = 0.13438888\n",
      "Iteration 213, loss = 0.09940226\n",
      "Iteration 96, loss = 0.13370045\n",
      "Iteration 97, loss = 0.13297676\n",
      "Iteration 98, loss = 0.13230906\n",
      "Iteration 214, loss = 0.09918390\n",
      "Iteration 99, loss = 0.13162966\n",
      "Iteration 100, loss = 0.13096378\n",
      "Iteration 101, loss = 0.13030913\n",
      "Iteration 102, loss = 0.12967034\n",
      "Iteration 215, loss = 0.09897609\n",
      "Iteration 216, loss = 0.09876670\n",
      "Iteration 217, loss = 0.09854758\n",
      "Iteration 103, loss = 0.12904710\n",
      "Iteration 218, loss = 0.09833367\n",
      "Iteration 104, loss = 0.12840282\n",
      "Iteration 219, loss = 0.09812976\n",
      "Iteration 197, loss = 0.15840080\n",
      "Iteration 105, loss = 0.12781274\n",
      "Iteration 220, loss = 0.09792418\n",
      "Iteration 106, loss = 0.12717849\n",
      "Iteration 221, loss = 0.09771463\n",
      "Iteration 222, loss = 0.09751011\n",
      "Iteration 107, loss = 0.12658832\n",
      "Iteration 223, loss = 0.09730357\n",
      "Iteration 108, loss = 0.12600871\n",
      "Iteration 224, loss = 0.09709554\n",
      "Iteration 136, loss = 0.19719764\n",
      "Iteration 225, loss = 0.09689232\n",
      "Iteration 109, loss = 0.12541365\n",
      "Iteration 226, loss = 0.09668898\n",
      "Iteration 227, loss = 0.09649954\n",
      "Iteration 110, loss = 0.12483913Iteration 113, loss = 0.21301110\n",
      "\n",
      "Iteration 246, loss = 0.13149424\n",
      "Iteration 198, loss = 0.15821164\n",
      "Iteration 137, loss = 0.19628280\n",
      "Iteration 228, loss = 0.09630088\n",
      "Iteration 27, loss = 0.49788335\n",
      "Iteration 229, loss = 0.09609707\n",
      "Iteration 111, loss = 0.12429534\n",
      "Iteration 247, loss = 0.13135455\n",
      "Iteration 230, loss = 0.09589936\n",
      "Iteration 112, loss = 0.12371943\n",
      "Iteration 113, loss = 0.12317288\n",
      "Iteration 231, loss = 0.09570646\n",
      "Iteration 114, loss = 0.12262334\n",
      "Iteration 232, loss = 0.09550734\n",
      "Iteration 115, loss = 0.12210184\n",
      "Iteration 233, loss = 0.09531613\n",
      "Iteration 234, loss = 0.09512926\n",
      "Iteration 116, loss = 0.12156677\n",
      "Iteration 117, loss = 0.12105338\n",
      "Iteration 118, loss = 0.12053656\n",
      "Iteration 248, loss = 0.13126616\n",
      "Iteration 119, loss = 0.12002647\n",
      "Iteration 120, loss = 0.11953809\n",
      "Iteration 235, loss = 0.09494085\n",
      "Iteration 121, loss = 0.11903827\n",
      "Iteration 236, loss = 0.09474759\n",
      "Iteration 122, loss = 0.11854634\n",
      "Iteration 123, loss = 0.11807140\n",
      "Iteration 237, loss = 0.09455750\n",
      "Iteration 238, loss = 0.09436538\n",
      "Iteration 239, loss = 0.09418385\n",
      "Iteration 240, loss = 0.09400074\n",
      "Iteration 259, loss = 0.21728403\n",
      "Iteration 241, loss = 0.09380828\n",
      "Iteration 28, loss = 0.49238431\n",
      "Iteration 124, loss = 0.11759301\n",
      "Iteration 114, loss = 0.21246153\n",
      "Iteration 138, loss = 0.19541408\n",
      "Iteration 125, loss = 0.11711465\n",
      "Iteration 126, loss = 0.11666032\n",
      "Iteration 127, loss = 0.11621586\n",
      "Iteration 128, loss = 0.11574706\n",
      "Iteration 199, loss = 0.15800797\n",
      "Iteration 129, loss = 0.11531073\n",
      "Iteration 130, loss = 0.11487376\n",
      "Iteration 131, loss = 0.11443886\n",
      "Iteration 132, loss = 0.11400633\n",
      "Iteration 133, loss = 0.11358481\n",
      "Iteration 139, loss = 0.19457816\n",
      "Iteration 242, loss = 0.09363130\n",
      "Iteration 243, loss = 0.09344496\n",
      "Iteration 244, loss = 0.09326088\n",
      "Iteration 245, loss = 0.09308440\n",
      "Iteration 246, loss = 0.09290337\n",
      "Iteration 247, loss = 0.09272860\n",
      "Iteration 248, loss = 0.09255339\n",
      "Iteration 249, loss = 0.09237365\n",
      "Iteration 250, loss = 0.09220894\n",
      "Iteration 251, loss = 0.09202982\n",
      "Iteration 249, loss = 0.13111484\n",
      "Iteration 252, loss = 0.09185646\n",
      "Iteration 253, loss = 0.09168859\n",
      "Iteration 134, loss = 0.11316312\n",
      "Iteration 254, loss = 0.09151023\n",
      "Iteration 135, loss = 0.11275559\n",
      "Iteration 136, loss = 0.11233465\n",
      "Iteration 255, loss = 0.09134555\n",
      "Iteration 137, loss = 0.11193894\n",
      "Iteration 256, loss = 0.09116939\n",
      "Iteration 138, loss = 0.11153743\n",
      "Iteration 257, loss = 0.09101163\n",
      "Iteration 139, loss = 0.11114681\n",
      "Iteration 258, loss = 0.09083625\n",
      "Iteration 259, loss = 0.09067146\n",
      "Iteration 140, loss = 0.11074355\n",
      "Iteration 140, loss = 0.19373278\n",
      "Iteration 200, loss = 0.15783393\n",
      "Iteration 260, loss = 0.09050398\n",
      "Iteration 141, loss = 0.11036022\n",
      "Iteration 142, loss = 0.10997625\n",
      "Iteration 143, loss = 0.10959767\n",
      "Iteration 115, loss = 0.21198067\n",
      "Iteration 261, loss = 0.09033579\n",
      "Iteration 260, loss = 0.21717231\n",
      "Iteration 250, loss = 0.13100786\n",
      "Iteration 262, loss = 0.09016777\n",
      "Iteration 263, loss = 0.09000525\n",
      "Iteration 264, loss = 0.08984270\n",
      "Iteration 265, loss = 0.08967317\n",
      "Iteration 266, loss = 0.08951469\n",
      "Iteration 267, loss = 0.08935488\n",
      "Iteration 144, loss = 0.10923111\n",
      "Iteration 268, loss = 0.08919053\n",
      "Iteration 145, loss = 0.10885286\n",
      "Iteration 29, loss = 0.48686824\n",
      "Iteration 269, loss = 0.08903176\n",
      "Iteration 146, loss = 0.10849199\n",
      "Iteration 147, loss = 0.10812968\n",
      "Iteration 270, loss = 0.08886997\n",
      "Iteration 148, loss = 0.10777784\n",
      "Iteration 271, loss = 0.08871593\n",
      "Iteration 149, loss = 0.10741546\n",
      "Iteration 150, loss = 0.10707441\n",
      "Iteration 151, loss = 0.10671680\n",
      "Iteration 272, loss = 0.08855715\n",
      "Iteration 251, loss = 0.13089884\n",
      "Iteration 273, loss = 0.08839963\n",
      "Iteration 152, loss = 0.10637164\n",
      "Iteration 201, loss = 0.15761671\n",
      "Iteration 153, loss = 0.10603018\n",
      "Iteration 154, loss = 0.10569672\n",
      "Iteration 141, loss = 0.19294639\n",
      "Iteration 274, loss = 0.08823788\n",
      "Iteration 155, loss = 0.10536109\n",
      "Iteration 275, loss = 0.08808626\n",
      "Iteration 276, loss = 0.08792832\n",
      "Iteration 252, loss = 0.13075160\n",
      "Iteration 156, loss = 0.10502765\n",
      "Iteration 277, loss = 0.08777624\n",
      "Iteration 157, loss = 0.10470702\n",
      "Iteration 278, loss = 0.08762326\n",
      "Iteration 158, loss = 0.10438149\n",
      "Iteration 279, loss = 0.08746758\n",
      "Iteration 280, loss = 0.08731209\n",
      "Iteration 159, loss = 0.10406061\n",
      "Iteration 281, loss = 0.08716093\n",
      "Iteration 282, loss = 0.08701174\n",
      "Iteration 160, loss = 0.10374819\n",
      "Iteration 253, loss = 0.13066820\n",
      "Iteration 161, loss = 0.10342379\n",
      "Iteration 283, loss = 0.08686341\n",
      "Iteration 162, loss = 0.10312151\n",
      "Iteration 30, loss = 0.48119454\n",
      "Iteration 284, loss = 0.08671644\n",
      "Iteration 163, loss = 0.10280727\n",
      "Iteration 254, loss = 0.13056911\n",
      "Iteration 202, loss = 0.15743608\n",
      "Iteration 285, loss = 0.08656286\n",
      "Iteration 261, loss = 0.21714896\n",
      "Iteration 286, loss = 0.08641489\n",
      "Iteration 287, loss = 0.08626345\n",
      "Iteration 288, loss = 0.08611524\n",
      "Iteration 289, loss = 0.08597385\n",
      "Iteration 164, loss = 0.10249895\n",
      "Iteration 290, loss = 0.08582888\n",
      "Iteration 142, loss = 0.19212250\n",
      "Iteration 291, loss = 0.08568951\n",
      "Iteration 292, loss = 0.08553773\n",
      "Iteration 293, loss = 0.08539356\n",
      "Iteration 294, loss = 0.08525050\n",
      "Iteration 295, loss = 0.08511076\n",
      "Iteration 296, loss = 0.08496929\n",
      "Iteration 297, loss = 0.08483127\n",
      "Iteration 298, loss = 0.08468543\n",
      "Iteration 31, loss = 0.47568429\n",
      "Iteration 299, loss = 0.08454527\n",
      "Iteration 165, loss = 0.10220048\n",
      "Iteration 300, loss = 0.08440450\n",
      "Iteration 301, loss = 0.08426948\n",
      "Iteration 302, loss = 0.08413301\n",
      "Iteration 303, loss = 0.08399711\n",
      "Iteration 304, loss = 0.08385668\n",
      "Iteration 166, loss = 0.10189704\n",
      "Iteration 167, loss = 0.10159119\n",
      "Iteration 168, loss = 0.10130448\n",
      "Iteration 116, loss = 0.21147822\n",
      "Iteration 169, loss = 0.10100945\n",
      "Iteration 305, loss = 0.08372356\n",
      "Iteration 170, loss = 0.10072560\n",
      "Iteration 171, loss = 0.10043857\n",
      "Iteration 172, loss = 0.10014913\n",
      "Iteration 173, loss = 0.09986392\n",
      "Iteration 255, loss = 0.13043983\n",
      "Iteration 203, loss = 0.15725186\n",
      "Iteration 306, loss = 0.08358387\n",
      "Iteration 307, loss = 0.08344991\n",
      "Iteration 174, loss = 0.09959317\n",
      "Iteration 175, loss = 0.09931601\n",
      "Iteration 308, loss = 0.08331546\n",
      "Iteration 176, loss = 0.09904267\n",
      "Iteration 143, loss = 0.19134978\n",
      "Iteration 177, loss = 0.09877037\n",
      "Iteration 256, loss = 0.13031929\n",
      "Iteration 178, loss = 0.09849227\n",
      "Iteration 117, loss = 0.21093956\n",
      "Iteration 179, loss = 0.09824428\n",
      "Iteration 262, loss = 0.21699436\n",
      "Iteration 180, loss = 0.09797085\n",
      "Iteration 181, loss = 0.09770773\n",
      "Iteration 309, loss = 0.08318272\n",
      "Iteration 182, loss = 0.09744586\n",
      "Iteration 310, loss = 0.08305245\n",
      "Iteration 183, loss = 0.09718091\n",
      "Iteration 311, loss = 0.08291978\n",
      "Iteration 184, loss = 0.09693990\n",
      "Iteration 312, loss = 0.08278524\n",
      "Iteration 313, loss = 0.08265744\n",
      "Iteration 314, loss = 0.08252829\n",
      "Iteration 185, loss = 0.09667475\n",
      "Iteration 144, loss = 0.19061436\n",
      "Iteration 186, loss = 0.09641908\n",
      "Iteration 315, loss = 0.08239179\n",
      "Iteration 316, loss = 0.08226482\n",
      "Iteration 257, loss = 0.13019272\n",
      "Iteration 317, loss = 0.08213652\n",
      "Iteration 318, loss = 0.08201211\n",
      "Iteration 319, loss = 0.08188746\n",
      "Iteration 187, loss = 0.09617615\n",
      "Iteration 188, loss = 0.09592398\n",
      "Iteration 189, loss = 0.09567568\n",
      "Iteration 320, loss = 0.08175893\n",
      "Iteration 190, loss = 0.09543492\n",
      "Iteration 204, loss = 0.15704942\n",
      "Iteration 321, loss = 0.08162542\n",
      "Iteration 322, loss = 0.08150799\n",
      "Iteration 323, loss = 0.08138299\n",
      "Iteration 324, loss = 0.08125120\n",
      "Iteration 325, loss = 0.08113262Iteration 191, loss = 0.09518711\n",
      "\n",
      "Iteration 326, loss = 0.08100421\n",
      "Iteration 118, loss = 0.21047448\n",
      "Iteration 327, loss = 0.08087987\n",
      "Iteration 32, loss = 0.47013462\n",
      "Iteration 328, loss = 0.08075556\n",
      "Iteration 329, loss = 0.08063207\n",
      "Iteration 330, loss = 0.08051237\n",
      "Iteration 331, loss = 0.08038987\n",
      "Iteration 332, loss = 0.08027198\n",
      "Iteration 333, loss = 0.08015029\n",
      "Iteration 334, loss = 0.08002325\n",
      "Iteration 335, loss = 0.07990328\n",
      "Iteration 145, loss = 0.18983808\n",
      "Iteration 336, loss = 0.07978011\n",
      "Iteration 337, loss = 0.07965935\n",
      "Iteration 192, loss = 0.09495301\n",
      "Iteration 193, loss = 0.09470282\n",
      "Iteration 194, loss = 0.09447412\n",
      "Iteration 258, loss = 0.13011552\n",
      "Iteration 195, loss = 0.09423741\n",
      "Iteration 338, loss = 0.07954103\n",
      "Iteration 196, loss = 0.09400998\n",
      "Iteration 339, loss = 0.07941624\n",
      "Iteration 197, loss = 0.09377129\n",
      "Iteration 340, loss = 0.07930285\n",
      "Iteration 341, loss = 0.07917668\n",
      "Iteration 342, loss = 0.07905835\n",
      "Iteration 198, loss = 0.09354477\n",
      "Iteration 343, loss = 0.07894174\n",
      "Iteration 344, loss = 0.07882024\n",
      "Iteration 345, loss = 0.07870512\n",
      "Iteration 199, loss = 0.09332712\n",
      "Iteration 200, loss = 0.09309071\n",
      "Iteration 346, loss = 0.07858543\n",
      "Iteration 347, loss = 0.07846917\n",
      "Iteration 201, loss = 0.09285778\n",
      "Iteration 348, loss = 0.07835556\n",
      "Iteration 349, loss = 0.07823680\n",
      "Iteration 202, loss = 0.09264309\n",
      "Iteration 350, loss = 0.07812334\n",
      "Iteration 351, loss = 0.07800826\n",
      "Iteration 352, loss = 0.07789309\n",
      "Iteration 203, loss = 0.09242397\n",
      "Iteration 353, loss = 0.07777533\n",
      "Iteration 259, loss = 0.12999337\n",
      "Iteration 354, loss = 0.07766721\n",
      "Iteration 204, loss = 0.09219933\n",
      "Iteration 355, loss = 0.07755223\n",
      "Iteration 356, loss = 0.07743917\n",
      "Iteration 357, loss = 0.07733032\n",
      "Iteration 358, loss = 0.07721969\n",
      "Iteration 359, loss = 0.07711682\n",
      "Iteration 360, loss = 0.07700272\n",
      "Iteration 361, loss = 0.07688987\n",
      "Iteration 205, loss = 0.15687948\n",
      "Iteration 362, loss = 0.07677891\n",
      "Iteration 119, loss = 0.20996368\n",
      "Iteration 263, loss = 0.21690377\n",
      "Iteration 363, loss = 0.07666706\n",
      "Iteration 146, loss = 0.18915717\n",
      "Iteration 364, loss = 0.07656215\n",
      "Iteration 365, loss = 0.07645019\n",
      "Iteration 366, loss = 0.07633836\n",
      "Iteration 367, loss = 0.07623379\n",
      "Iteration 205, loss = 0.09198607\n",
      "Iteration 368, loss = 0.07612749\n",
      "Iteration 369, loss = 0.07601568\n",
      "Iteration 370, loss = 0.07590960\n",
      "Iteration 206, loss = 0.09176406\n",
      "Iteration 371, loss = 0.07580054\n",
      "Iteration 207, loss = 0.09154886\n",
      "Iteration 372, loss = 0.07569366\n",
      "Iteration 373, loss = 0.07558780\n",
      "Iteration 208, loss = 0.09133787\n",
      "Iteration 209, loss = 0.09113194\n",
      "Iteration 374, loss = 0.07547716\n",
      "Iteration 375, loss = 0.07537549\n",
      "Iteration 210, loss = 0.09091888\n",
      "Iteration 211, loss = 0.09071642\n",
      "Iteration 260, loss = 0.12988887\n",
      "Iteration 376, loss = 0.07526509Iteration 212, loss = 0.09049809\n",
      "Iteration 213, loss = 0.09029940\n",
      "\n",
      "Iteration 206, loss = 0.15670349\n",
      "Iteration 214, loss = 0.09008659\n",
      "Iteration 147, loss = 0.18841039\n",
      "Iteration 33, loss = 0.46439589\n",
      "Iteration 215, loss = 0.08988234\n",
      "Iteration 261, loss = 0.12979552\n",
      "Iteration 216, loss = 0.08968110\n",
      "Iteration 377, loss = 0.07515837\n",
      "Iteration 217, loss = 0.08948234\n",
      "Iteration 378, loss = 0.07506173\n",
      "Iteration 379, loss = 0.07495334\n",
      "Iteration 380, loss = 0.07484690\n",
      "Iteration 264, loss = 0.21686636\n",
      "Iteration 218, loss = 0.08927851\n",
      "Iteration 381, loss = 0.07474115\n",
      "Iteration 148, loss = 0.18771674\n",
      "Iteration 219, loss = 0.08908183\n",
      "Iteration 382, loss = 0.07463916\n",
      "Iteration 383, loss = 0.07452890\n",
      "Iteration 384, loss = 0.07442826\n",
      "Iteration 385, loss = 0.07432602\n",
      "Iteration 120, loss = 0.20948145\n",
      "Iteration 386, loss = 0.07422053\n",
      "Iteration 387, loss = 0.07411512\n",
      "Iteration 388, loss = 0.07401626\n",
      "Iteration 262, loss = 0.12967261\n",
      "Iteration 389, loss = 0.07391367\n",
      "Iteration 390, loss = 0.07381146\n",
      "Iteration 391, loss = 0.07371047\n",
      "Iteration 392, loss = 0.07360859\n",
      "Iteration 220, loss = 0.08889033\n",
      "Iteration 221, loss = 0.08871772\n",
      "Iteration 222, loss = 0.08849826\n",
      "Iteration 223, loss = 0.08830677\n",
      "Iteration 224, loss = 0.08811204\n",
      "Iteration 225, loss = 0.08792332\n",
      "Iteration 207, loss = 0.15649834\n",
      "Iteration 226, loss = 0.08772911\n",
      "Iteration 227, loss = 0.08754899\n",
      "Iteration 228, loss = 0.08735768\n",
      "Iteration 229, loss = 0.08717039\n",
      "Iteration 230, loss = 0.08698453\n",
      "Iteration 393, loss = 0.07351482\n",
      "Iteration 231, loss = 0.08680945\n",
      "Iteration 34, loss = 0.45885131\n",
      "Iteration 263, loss = 0.12955381\n",
      "Iteration 208, loss = 0.15633069\n",
      "Iteration 232, loss = 0.08662860\n",
      "Iteration 233, loss = 0.08644295\n",
      "Iteration 234, loss = 0.08626546\n",
      "Iteration 149, loss = 0.18705616\n",
      "Iteration 264, loss = 0.12946105\n",
      "Iteration 235, loss = 0.08608611\n",
      "Iteration 394, loss = 0.07340951\n",
      "Iteration 395, loss = 0.07330912\n",
      "Iteration 396, loss = 0.07321263\n",
      "Iteration 397, loss = 0.07311195\n",
      "Iteration 398, loss = 0.07301932\n",
      "Iteration 399, loss = 0.07292036\n",
      "Iteration 400, loss = 0.07282257\n",
      "Iteration 236, loss = 0.08591898\n",
      "Iteration 401, loss = 0.07272505\n",
      "Iteration 402, loss = 0.07263013\n",
      "Iteration 403, loss = 0.07253016\n",
      "Iteration 404, loss = 0.07243225\n",
      "Iteration 405, loss = 0.07233827\n",
      "Iteration 237, loss = 0.08574051\n",
      "Iteration 238, loss = 0.08556963\n",
      "Iteration 406, loss = 0.07223992\n",
      "Iteration 150, loss = 0.18635827\n",
      "Iteration 265, loss = 0.21675232\n",
      "Iteration 407, loss = 0.07214581\n",
      "Iteration 121, loss = 0.20901551\n",
      "Iteration 408, loss = 0.07204825\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 209, loss = 0.15614711\n",
      "Iteration 265, loss = 0.12935193\n",
      "Iteration 239, loss = 0.08539176\n",
      "Iteration 266, loss = 0.12926826\n",
      "Iteration 240, loss = 0.08522151\n",
      "Iteration 241, loss = 0.08505111\n",
      "Iteration 242, loss = 0.08488030\n",
      "Iteration 243, loss = 0.08471271\n",
      "Iteration 244, loss = 0.08454720\n",
      "Iteration 1, loss = 0.69836738\n",
      "Iteration 245, loss = 0.08438571\n",
      "Iteration 246, loss = 0.08422266\n",
      "Iteration 247, loss = 0.08405381\n",
      "Iteration 2, loss = 0.66264447\n",
      "Iteration 248, loss = 0.08389764\n",
      "Iteration 249, loss = 0.08372653\n",
      "Iteration 3, loss = 0.61602342\n",
      "Iteration 35, loss = 0.45334568\n",
      "Iteration 4, loss = 0.56904096\n",
      "Iteration 5, loss = 0.52681079\n",
      "Iteration 6, loss = 0.49013297\n",
      "Iteration 7, loss = 0.45912406\n",
      "Iteration 266, loss = 0.21664249\n",
      "Iteration 267, loss = 0.12917027\n",
      "Iteration 250, loss = 0.08356765\n",
      "Iteration 251, loss = 0.08341875\n",
      "Iteration 151, loss = 0.18577577\n",
      "Iteration 252, loss = 0.08325522\n",
      "Iteration 253, loss = 0.08309089\n",
      "Iteration 254, loss = 0.08293656\n",
      "Iteration 255, loss = 0.08278284\n",
      "Iteration 256, loss = 0.08262302\n",
      "Iteration 257, loss = 0.08246986\n",
      "Iteration 258, loss = 0.08231494\n",
      "Iteration 259, loss = 0.08216618\n",
      "Iteration 152, loss = 0.18508344\n",
      "Iteration 260, loss = 0.08201883\n",
      "Iteration 261, loss = 0.08186448\n",
      "Iteration 122, loss = 0.20856204\n",
      "Iteration 262, loss = 0.08171117\n",
      "Iteration 210, loss = 0.15600803\n",
      "Iteration 8, loss = 0.43368487\n",
      "Iteration 153, loss = 0.18444393\n",
      "Iteration 9, loss = 0.41155754\n",
      "Iteration 263, loss = 0.08156054\n",
      "Iteration 10, loss = 0.39305370\n",
      "Iteration 264, loss = 0.08141815\n",
      "Iteration 267, loss = 0.21655610\n",
      "Iteration 265, loss = 0.08126709\n",
      "Iteration 11, loss = 0.37702631\n",
      "Iteration 266, loss = 0.08112660\n",
      "Iteration 12, loss = 0.36306771\n",
      "Iteration 13, loss = 0.35042191\n",
      "Iteration 267, loss = 0.08098247\n",
      "Iteration 14, loss = 0.33928358\n",
      "Iteration 268, loss = 0.08083613\n",
      "Iteration 15, loss = 0.32930409\n",
      "Iteration 269, loss = 0.08069258\n",
      "Iteration 270, loss = 0.08054101Iteration 268, loss = 0.12902902\n",
      "\n",
      "Iteration 36, loss = 0.44776849\n",
      "Iteration 16, loss = 0.32009752\n",
      "Iteration 17, loss = 0.31158862\n",
      "Iteration 154, loss = 0.18382351\n",
      "Iteration 18, loss = 0.30382514\n",
      "Iteration 19, loss = 0.29659093\n",
      "Iteration 20, loss = 0.28981784\n",
      "Iteration 269, loss = 0.12893745\n",
      "Iteration 21, loss = 0.28357177\n",
      "Iteration 270, loss = 0.12882456\n",
      "Iteration 155, loss = 0.18322285\n",
      "Iteration 271, loss = 0.08041106\n",
      "Iteration 22, loss = 0.27763328\n",
      "Iteration 272, loss = 0.08026370\n",
      "Iteration 211, loss = 0.15577697\n",
      "Iteration 273, loss = 0.08011683\n",
      "Iteration 123, loss = 0.20814232\n",
      "Iteration 274, loss = 0.07997866\n",
      "Iteration 275, loss = 0.07984097\n",
      "Iteration 276, loss = 0.07969846\n",
      "Iteration 277, loss = 0.07956197\n",
      "Iteration 278, loss = 0.07942328\n",
      "Iteration 279, loss = 0.07927979\n",
      "Iteration 280, loss = 0.07915003\n",
      "Iteration 281, loss = 0.07901932\n",
      "Iteration 282, loss = 0.07887894\n",
      "Iteration 283, loss = 0.07874454\n",
      "Iteration 284, loss = 0.07861284\n",
      "Iteration 23, loss = 0.27192285\n",
      "Iteration 24, loss = 0.26671236\n",
      "Iteration 25, loss = 0.26169744\n",
      "Iteration 212, loss = 0.15563631\n",
      "Iteration 26, loss = 0.25686549\n",
      "Iteration 27, loss = 0.25236636\n",
      "Iteration 271, loss = 0.12874445\n",
      "Iteration 285, loss = 0.07847795\n",
      "Iteration 28, loss = 0.24803678\n",
      "Iteration 124, loss = 0.20771722\n",
      "Iteration 156, loss = 0.18265428\n",
      "Iteration 29, loss = 0.24393288\n",
      "Iteration 286, loss = 0.07834911\n",
      "Iteration 30, loss = 0.23996380\n",
      "Iteration 287, loss = 0.07821881\n",
      "Iteration 288, loss = 0.07808370\n",
      "Iteration 289, loss = 0.07796033\n",
      "Iteration 37, loss = 0.44229014\n",
      "Iteration 290, loss = 0.07783256\n",
      "Iteration 291, loss = 0.07770309\n",
      "Iteration 292, loss = 0.07757768\n",
      "Iteration 293, loss = 0.07745117\n",
      "Iteration 31, loss = 0.23621328\n",
      "Iteration 294, loss = 0.07732216\n",
      "Iteration 32, loss = 0.23256242\n",
      "Iteration 33, loss = 0.22911415\n",
      "Iteration 34, loss = 0.22573380\n",
      "Iteration 35, loss = 0.22255386\n",
      "Iteration 157, loss = 0.18211362\n",
      "Iteration 36, loss = 0.21943233\n",
      "Iteration 37, loss = 0.21647342\n",
      "Iteration 295, loss = 0.07720027\n",
      "Iteration 268, loss = 0.21647192\n",
      "Iteration 272, loss = 0.12863739\n",
      "Iteration 296, loss = 0.07707131\n",
      "Iteration 38, loss = 0.21357922\n",
      "Iteration 297, loss = 0.07695184\n",
      "Iteration 39, loss = 0.21076428\n",
      "Iteration 298, loss = 0.07682212\n",
      "Iteration 40, loss = 0.20807337\n",
      "Iteration 299, loss = 0.07670195\n",
      "Iteration 213, loss = 0.15543688\n",
      "Iteration 273, loss = 0.12853163\n",
      "Iteration 300, loss = 0.07658379\n",
      "Iteration 301, loss = 0.07645921\n",
      "Iteration 302, loss = 0.07634038\n",
      "Iteration 269, loss = 0.21640660\n",
      "Iteration 303, loss = 0.07622182\n",
      "Iteration 41, loss = 0.20549462\n",
      "Iteration 304, loss = 0.07610355\n",
      "Iteration 42, loss = 0.20295341\n",
      "Iteration 274, loss = 0.12843623\n",
      "Iteration 305, loss = 0.07597785\n",
      "Iteration 306, loss = 0.07585761\n",
      "Iteration 307, loss = 0.07574562\n",
      "Iteration 308, loss = 0.07562562\n",
      "Iteration 158, loss = 0.18153090\n",
      "Iteration 309, loss = 0.07551040\n",
      "Iteration 310, loss = 0.07539270\n",
      "Iteration 43, loss = 0.20051357\n",
      "Iteration 44, loss = 0.19816774\n",
      "Iteration 45, loss = 0.19585152\n",
      "Iteration 46, loss = 0.19362201\n",
      "Iteration 47, loss = 0.19147915\n",
      "Iteration 125, loss = 0.20723247\n",
      "Iteration 48, loss = 0.18934720\n",
      "Iteration 311, loss = 0.07527811\n",
      "Iteration 49, loss = 0.18732480\n",
      "Iteration 50, loss = 0.18534010\n",
      "Iteration 38, loss = 0.43655698\n",
      "Iteration 51, loss = 0.18341286\n",
      "Iteration 312, loss = 0.07516168\n",
      "Iteration 313, loss = 0.07505346\n",
      "Iteration 314, loss = 0.07493659\n",
      "Iteration 214, loss = 0.15526521\n",
      "Iteration 52, loss = 0.18156581\n",
      "Iteration 315, loss = 0.07482259\n",
      "Iteration 316, loss = 0.07471204\n",
      "Iteration 317, loss = 0.07460065\n",
      "Iteration 318, loss = 0.07448607\n",
      "Iteration 159, loss = 0.18098068\n",
      "Iteration 319, loss = 0.07437736\n",
      "Iteration 53, loss = 0.17966650\n",
      "Iteration 320, loss = 0.07426611\n",
      "Iteration 321, loss = 0.07415810\n",
      "Iteration 215, loss = 0.15514182\n",
      "Iteration 322, loss = 0.07405270\n",
      "Iteration 275, loss = 0.12833925\n",
      "Iteration 323, loss = 0.07393849\n",
      "Iteration 324, loss = 0.07383362\n",
      "Iteration 126, loss = 0.20682149\n",
      "Iteration 325, loss = 0.07372060\n",
      "Iteration 326, loss = 0.07361779\n",
      "Iteration 54, loss = 0.17788470\n",
      "Iteration 55, loss = 0.17616908\n",
      "Iteration 56, loss = 0.17446519\n",
      "Iteration 57, loss = 0.17286537\n",
      "Iteration 276, loss = 0.12823654\n",
      "Iteration 39, loss = 0.43109920\n",
      "Iteration 160, loss = 0.18046216\n",
      "Iteration 270, loss = 0.21629664\n",
      "Iteration 327, loss = 0.07350991\n",
      "Iteration 277, loss = 0.12814637\n",
      "Iteration 328, loss = 0.07340449\n",
      "Iteration 58, loss = 0.17121092\n",
      "Iteration 329, loss = 0.07329762\n",
      "Iteration 59, loss = 0.16964566\n",
      "Iteration 60, loss = 0.16814643\n",
      "Iteration 61, loss = 0.16661467\n",
      "Iteration 161, loss = 0.17998119\n",
      "Iteration 62, loss = 0.16515693\n",
      "Iteration 330, loss = 0.07319227\n",
      "Iteration 216, loss = 0.15493992\n",
      "Iteration 331, loss = 0.07308724\n",
      "Iteration 332, loss = 0.07298329\n",
      "Iteration 333, loss = 0.07288171\n",
      "Iteration 334, loss = 0.07277962\n",
      "Iteration 335, loss = 0.07267299\n",
      "Iteration 336, loss = 0.07257381\n",
      "Iteration 63, loss = 0.16372468\n",
      "Iteration 278, loss = 0.12806118\n",
      "Iteration 337, loss = 0.07247033\n",
      "Iteration 338, loss = 0.07236747\n",
      "Iteration 339, loss = 0.07226773\n",
      "Iteration 340, loss = 0.07216924\n",
      "Iteration 341, loss = 0.07206862\n",
      "Iteration 342, loss = 0.07197143\n",
      "Iteration 279, loss = 0.12796318\n",
      "Iteration 64, loss = 0.16231968\n",
      "Iteration 127, loss = 0.20639610\n",
      "Iteration 65, loss = 0.16095901\n",
      "Iteration 66, loss = 0.15968050\n",
      "Iteration 343, loss = 0.07186497\n",
      "Iteration 67, loss = 0.15829378\n",
      "Iteration 40, loss = 0.42579614\n",
      "Iteration 271, loss = 0.21622804\n",
      "Iteration 344, loss = 0.07176610\n",
      "Iteration 68, loss = 0.15703979\n",
      "Iteration 162, loss = 0.17944101\n",
      "Iteration 345, loss = 0.07166811\n",
      "Iteration 346, loss = 0.07156880\n",
      "Iteration 163, loss = 0.17895139\n",
      "Iteration 347, loss = 0.07147631\n",
      "Iteration 348, loss = 0.07138129\n",
      "Iteration 349, loss = 0.07128143\n",
      "Iteration 217, loss = 0.15476431\n",
      "Iteration 69, loss = 0.15581754\n",
      "Iteration 350, loss = 0.07118660\n",
      "Iteration 280, loss = 0.12786455\n",
      "Iteration 351, loss = 0.07109265\n",
      "Iteration 70, loss = 0.15458614\n",
      "Iteration 352, loss = 0.07099642\n",
      "Iteration 71, loss = 0.15337952\n",
      "Iteration 72, loss = 0.15219627\n",
      "Iteration 353, loss = 0.07089969\n",
      "Iteration 354, loss = 0.07080737\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 73, loss = 0.15104951\n",
      "Iteration 272, loss = 0.21614392\n",
      "Iteration 74, loss = 0.14994806\n",
      "Iteration 281, loss = 0.12778436\n",
      "Iteration 164, loss = 0.17847890\n",
      "Iteration 75, loss = 0.14884486\n",
      "Iteration 76, loss = 0.14776801\n",
      "Iteration 1, loss = 0.70816119\n",
      "Iteration 2, loss = 0.67609766\n",
      "Iteration 3, loss = 0.63398686\n",
      "Iteration 4, loss = 0.59100361\n",
      "Iteration 5, loss = 0.55245477\n",
      "Iteration 218, loss = 0.15459360\n",
      "Iteration 6, loss = 0.51854901\n",
      "Iteration 128, loss = 0.20600286\n",
      "Iteration 77, loss = 0.14670525\n",
      "Iteration 41, loss = 0.42036114\n",
      "Iteration 78, loss = 0.14567025\n",
      "Iteration 79, loss = 0.14465255\n",
      "Iteration 80, loss = 0.14365790\n",
      "Iteration 81, loss = 0.14268915\n",
      "Iteration 7, loss = 0.48908265\n",
      "Iteration 82, loss = 0.14175191\n",
      "Iteration 282, loss = 0.12772083\n",
      "Iteration 165, loss = 0.17798051\n",
      "Iteration 8, loss = 0.46435874\n",
      "Iteration 283, loss = 0.12758424\n",
      "Iteration 129, loss = 0.20558728\n",
      "Iteration 83, loss = 0.14079259\n",
      "Iteration 9, loss = 0.44373236\n",
      "Iteration 10, loss = 0.42545470\n",
      "Iteration 42, loss = 0.41507345\n",
      "Iteration 84, loss = 0.13988931\n",
      "Iteration 85, loss = 0.13897687\n",
      "Iteration 86, loss = 0.13807930\n",
      "Iteration 87, loss = 0.13724519\n",
      "Iteration 88, loss = 0.13636867\n",
      "Iteration 11, loss = 0.40946850\n",
      "Iteration 273, loss = 0.21602430\n",
      "Iteration 12, loss = 0.39574175\n",
      "Iteration 89, loss = 0.13551978\n",
      "Iteration 13, loss = 0.38309888\n",
      "Iteration 14, loss = 0.37203415\n",
      "Iteration 90, loss = 0.13472394\n",
      "Iteration 15, loss = 0.36188722\n",
      "Iteration 166, loss = 0.17753144\n",
      "Iteration 91, loss = 0.13389057\n",
      "Iteration 92, loss = 0.13309015\n",
      "Iteration 219, loss = 0.15442780\n",
      "Iteration 16, loss = 0.35234978\n",
      "Iteration 93, loss = 0.13232163\n",
      "Iteration 94, loss = 0.13155023\n",
      "Iteration 95, loss = 0.13080538\n",
      "Iteration 96, loss = 0.13006544\n",
      "Iteration 220, loss = 0.15427014\n",
      "Iteration 17, loss = 0.34367717\n",
      "Iteration 18, loss = 0.33558489\n",
      "Iteration 97, loss = 0.12934445\n",
      "Iteration 19, loss = 0.32809423\n",
      "Iteration 98, loss = 0.12862095\n",
      "Iteration 20, loss = 0.32093912\n",
      "Iteration 284, loss = 0.12751224\n",
      "Iteration 99, loss = 0.12790561\n",
      "Iteration 100, loss = 0.12721010\n",
      "Iteration 21, loss = 0.31438070\n",
      "Iteration 101, loss = 0.12654503\n",
      "Iteration 167, loss = 0.17709099\n",
      "Iteration 22, loss = 0.30785812\n",
      "Iteration 102, loss = 0.12588103\n",
      "Iteration 23, loss = 0.30177194\n",
      "Iteration 103, loss = 0.12520188\n",
      "Iteration 24, loss = 0.29602188\n",
      "Iteration 104, loss = 0.12458061\n",
      "Iteration 25, loss = 0.29030142\n",
      "Iteration 105, loss = 0.12392021\n",
      "Iteration 106, loss = 0.12328710\n",
      "Iteration 26, loss = 0.28502462\n",
      "Iteration 27, loss = 0.27994764\n",
      "Iteration 28, loss = 0.27487735\n",
      "Iteration 29, loss = 0.27020862\n",
      "Iteration 30, loss = 0.26574549\n",
      "Iteration 31, loss = 0.26120033\n",
      "Iteration 285, loss = 0.12741186\n",
      "Iteration 107, loss = 0.12267419\n",
      "Iteration 108, loss = 0.12208474\n",
      "Iteration 32, loss = 0.25694128\n",
      "Iteration 274, loss = 0.21595889\n",
      "Iteration 109, loss = 0.12145422\n",
      "Iteration 43, loss = 0.40988282\n",
      "Iteration 110, loss = 0.12088213\n",
      "Iteration 33, loss = 0.25278718\n",
      "Iteration 286, loss = 0.12732445\n",
      "Iteration 111, loss = 0.12029366\n",
      "Iteration 168, loss = 0.17662372\n",
      "Iteration 112, loss = 0.11971256\n",
      "Iteration 113, loss = 0.11915831\n",
      "Iteration 287, loss = 0.12725763\n",
      "Iteration 114, loss = 0.11858907\n",
      "Iteration 115, loss = 0.11805960\n",
      "Iteration 221, loss = 0.15411590\n",
      "Iteration 116, loss = 0.11750539\n",
      "Iteration 34, loss = 0.24887667\n",
      "Iteration 117, loss = 0.11698124\n",
      "Iteration 35, loss = 0.24502693\n",
      "Iteration 118, loss = 0.11644175\n",
      "Iteration 119, loss = 0.11594507\n",
      "Iteration 36, loss = 0.24121221\n",
      "Iteration 37, loss = 0.23759140\n",
      "Iteration 169, loss = 0.17623141\n",
      "Iteration 38, loss = 0.23412130\n",
      "Iteration 222, loss = 0.15394441\n",
      "Iteration 39, loss = 0.23065120\n",
      "Iteration 130, loss = 0.20516930\n",
      "Iteration 120, loss = 0.11542407\n",
      "Iteration 121, loss = 0.11491912\n",
      "Iteration 40, loss = 0.22733699\n",
      "Iteration 122, loss = 0.11442657\n",
      "Iteration 123, loss = 0.11395051\n",
      "Iteration 44, loss = 0.40473482\n",
      "Iteration 41, loss = 0.22410548\n",
      "Iteration 124, loss = 0.11345964\n",
      "Iteration 42, loss = 0.22111837\n",
      "Iteration 43, loss = 0.21797063\n",
      "Iteration 44, loss = 0.21508273\n",
      "Iteration 288, loss = 0.12714408\n",
      "Iteration 125, loss = 0.11297936\n",
      "Iteration 131, loss = 0.20479623\n",
      "Iteration 45, loss = 0.21223539\n",
      "Iteration 46, loss = 0.20950464\n",
      "Iteration 47, loss = 0.20681714\n",
      "Iteration 48, loss = 0.20419933\n",
      "Iteration 49, loss = 0.20163803\n",
      "Iteration 50, loss = 0.19919219\n",
      "Iteration 51, loss = 0.19677968\n",
      "Iteration 223, loss = 0.15378976\n",
      "Iteration 126, loss = 0.11251900\n",
      "Iteration 170, loss = 0.17579392\n",
      "Iteration 52, loss = 0.19442871\n",
      "Iteration 53, loss = 0.19216317\n",
      "Iteration 127, loss = 0.11205017\n",
      "Iteration 275, loss = 0.21585957\n",
      "Iteration 128, loss = 0.11158163\n",
      "Iteration 289, loss = 0.12709946\n",
      "Iteration 129, loss = 0.11113675Iteration 54, loss = 0.18995814\n",
      "\n",
      "Iteration 130, loss = 0.11068158\n",
      "Iteration 55, loss = 0.18782294\n",
      "Iteration 56, loss = 0.18566622\n",
      "Iteration 131, loss = 0.11025648\n",
      "Iteration 57, loss = 0.18363970\n",
      "Iteration 132, loss = 0.20444587\n",
      "Iteration 290, loss = 0.12699817\n",
      "Iteration 171, loss = 0.17539201\n",
      "Iteration 45, loss = 0.39960297\n",
      "Iteration 132, loss = 0.10982648\n",
      "Iteration 291, loss = 0.12688686\n",
      "Iteration 58, loss = 0.18167586\n",
      "Iteration 59, loss = 0.17971928\n",
      "Iteration 133, loss = 0.10939692\n",
      "Iteration 292, loss = 0.12680999\n",
      "Iteration 276, loss = 0.21581238\n",
      "Iteration 60, loss = 0.17782877\n",
      "Iteration 61, loss = 0.17599697\n",
      "Iteration 224, loss = 0.15363611\n",
      "Iteration 293, loss = 0.12671961\n",
      "Iteration 62, loss = 0.17416952\n",
      "Iteration 134, loss = 0.10896860\n",
      "Iteration 135, loss = 0.10854749\n",
      "Iteration 136, loss = 0.10814469\n",
      "Iteration 137, loss = 0.10773366\n",
      "Iteration 133, loss = 0.20405410\n",
      "Iteration 172, loss = 0.17497477\n",
      "Iteration 138, loss = 0.10732052\n",
      "Iteration 139, loss = 0.10693238\n",
      "Iteration 140, loss = 0.10654198\n",
      "Iteration 141, loss = 0.10615019\n",
      "Iteration 142, loss = 0.10576922\n",
      "Iteration 143, loss = 0.10540095\n",
      "Iteration 144, loss = 0.10501311\n",
      "Iteration 145, loss = 0.10465358\n",
      "Iteration 146, loss = 0.10428106\n",
      "Iteration 225, loss = 0.15347997\n",
      "Iteration 147, loss = 0.10391595\n",
      "Iteration 148, loss = 0.10356226\n",
      "Iteration 63, loss = 0.17244550\n",
      "Iteration 149, loss = 0.10320641\n",
      "Iteration 64, loss = 0.17070648\n",
      "Iteration 65, loss = 0.16904229\n",
      "Iteration 150, loss = 0.10286159\n",
      "Iteration 66, loss = 0.16742768\n",
      "Iteration 173, loss = 0.17459119\n",
      "Iteration 67, loss = 0.16580500\n",
      "Iteration 68, loss = 0.16426548\n",
      "Iteration 69, loss = 0.16275070\n",
      "Iteration 70, loss = 0.16124570\n",
      "Iteration 294, loss = 0.12665900\n",
      "Iteration 151, loss = 0.10251798\n",
      "Iteration 71, loss = 0.15979697\n",
      "Iteration 46, loss = 0.39467717\n",
      "Iteration 72, loss = 0.15837371\n",
      "Iteration 152, loss = 0.10217052\n",
      "Iteration 277, loss = 0.21568130\n",
      "Iteration 73, loss = 0.15702884\n",
      "Iteration 174, loss = 0.17419049\n",
      "Iteration 74, loss = 0.15563299\n",
      "Iteration 75, loss = 0.15429732\n",
      "Iteration 76, loss = 0.15296941\n",
      "Iteration 153, loss = 0.10183446\n",
      "Iteration 77, loss = 0.15170724\n",
      "Iteration 78, loss = 0.15042767\n",
      "Iteration 79, loss = 0.14920509\n",
      "Iteration 154, loss = 0.10151047\n",
      "Iteration 80, loss = 0.14800812\n",
      "Iteration 81, loss = 0.14680581\n",
      "Iteration 155, loss = 0.10117671\n",
      "Iteration 134, loss = 0.20370488\n",
      "Iteration 82, loss = 0.14566495\n",
      "Iteration 226, loss = 0.15333883\n",
      "Iteration 83, loss = 0.14451757\n",
      "Iteration 84, loss = 0.14343970\n",
      "Iteration 156, loss = 0.10084733\n",
      "Iteration 295, loss = 0.12655906\n",
      "Iteration 157, loss = 0.10052860\n",
      "Iteration 158, loss = 0.10021060\n",
      "Iteration 175, loss = 0.17380197\n",
      "Iteration 159, loss = 0.09988729\n",
      "Iteration 85, loss = 0.14232362\n",
      "Iteration 278, loss = 0.21563244\n",
      "Iteration 86, loss = 0.14125761\n",
      "Iteration 87, loss = 0.14018967\n",
      "Iteration 160, loss = 0.09957326\n",
      "Iteration 161, loss = 0.09927320\n",
      "Iteration 162, loss = 0.09896331\n",
      "Iteration 88, loss = 0.13916079\n",
      "Iteration 163, loss = 0.09865655\n",
      "Iteration 89, loss = 0.13814839\n",
      "Iteration 90, loss = 0.13714591\n",
      "Iteration 91, loss = 0.13621668\n",
      "Iteration 92, loss = 0.13527355\n",
      "Iteration 93, loss = 0.13430760\n",
      "Iteration 296, loss = 0.12647850\n",
      "Iteration 94, loss = 0.13337765\n",
      "Iteration 95, loss = 0.13248079\n",
      "Iteration 164, loss = 0.09836093\n",
      "Iteration 47, loss = 0.38959797\n",
      "Iteration 165, loss = 0.09806169\n",
      "Iteration 96, loss = 0.13158501\n",
      "Iteration 166, loss = 0.09777155\n",
      "Iteration 97, loss = 0.13075802\n",
      "Iteration 98, loss = 0.12987639\n",
      "Iteration 167, loss = 0.09748164\n",
      "Iteration 176, loss = 0.17344605Iteration 99, loss = 0.12904431\n",
      "Iteration 168, loss = 0.09719205\n",
      "Iteration 297, loss = 0.12637946\n",
      "Iteration 100, loss = 0.12821418\n",
      "Iteration 101, loss = 0.12743261\n",
      "\n",
      "Iteration 102, loss = 0.12663240\n",
      "Iteration 135, loss = 0.20335334\n",
      "Iteration 177, loss = 0.17308195\n",
      "Iteration 227, loss = 0.15315961\n",
      "Iteration 169, loss = 0.09689622\n",
      "Iteration 170, loss = 0.09662053\n",
      "Iteration 171, loss = 0.09634603\n",
      "Iteration 172, loss = 0.09606117\n",
      "Iteration 173, loss = 0.09579215\n",
      "Iteration 174, loss = 0.09551510\n",
      "Iteration 103, loss = 0.12584779\n",
      "Iteration 175, loss = 0.09525104\n",
      "Iteration 228, loss = 0.15300503\n",
      "Iteration 298, loss = 0.12630553\n",
      "Iteration 176, loss = 0.09498369\n",
      "Iteration 177, loss = 0.09472301\n",
      "Iteration 279, loss = 0.21557285\n",
      "Iteration 48, loss = 0.38479593\n",
      "Iteration 178, loss = 0.09445406\n",
      "Iteration 179, loss = 0.09420127\n",
      "Iteration 180, loss = 0.09394716\n",
      "Iteration 181, loss = 0.09369620\n",
      "Iteration 104, loss = 0.12510516\n",
      "Iteration 178, loss = 0.17270836\n",
      "Iteration 105, loss = 0.12432798\n",
      "Iteration 106, loss = 0.12359380\n",
      "Iteration 107, loss = 0.12286552\n",
      "Iteration 182, loss = 0.09343419\n",
      "Iteration 183, loss = 0.09319608\n",
      "Iteration 184, loss = 0.09294102\n",
      "Iteration 299, loss = 0.12623273\n",
      "Iteration 229, loss = 0.15287597\n",
      "Iteration 136, loss = 0.20293298\n",
      "Iteration 108, loss = 0.12213941\n",
      "Iteration 109, loss = 0.12145749\n",
      "Iteration 110, loss = 0.12074367\n",
      "Iteration 111, loss = 0.12010133\n",
      "Iteration 112, loss = 0.11939750\n",
      "Iteration 113, loss = 0.11875403\n",
      "Iteration 185, loss = 0.09269326\n",
      "Iteration 114, loss = 0.11808771\n",
      "Iteration 186, loss = 0.09245084Iteration 179, loss = 0.17236628\n",
      "\n",
      "Iteration 115, loss = 0.11746559\n",
      "Iteration 300, loss = 0.12616096\n",
      "Iteration 187, loss = 0.09220890\n",
      "Iteration 188, loss = 0.09197975\n",
      "Iteration 189, loss = 0.09173540\n",
      "Iteration 190, loss = 0.09149534\n",
      "Iteration 116, loss = 0.11684095\n",
      "Iteration 230, loss = 0.15271311\n",
      "Iteration 117, loss = 0.11620731\n",
      "Iteration 118, loss = 0.11560010\n",
      "Iteration 119, loss = 0.11500654\n",
      "Iteration 120, loss = 0.11442888\n",
      "Iteration 301, loss = 0.12610300\n",
      "Iteration 280, loss = 0.21551000\n",
      "Iteration 121, loss = 0.11383479\n",
      "Iteration 191, loss = 0.09125969\n",
      "Iteration 192, loss = 0.09102690\n",
      "Iteration 122, loss = 0.11325906\n",
      "Iteration 193, loss = 0.09079807\n",
      "Iteration 194, loss = 0.09057740\n",
      "Iteration 231, loss = 0.15257236\n",
      "Iteration 180, loss = 0.17208203\n",
      "Iteration 123, loss = 0.11271429\n",
      "Iteration 195, loss = 0.09035232\n",
      "Iteration 124, loss = 0.11214823\n",
      "Iteration 125, loss = 0.11159001\n",
      "Iteration 137, loss = 0.20259058\n",
      "Iteration 49, loss = 0.37996452\n",
      "Iteration 232, loss = 0.15242121\n",
      "Iteration 126, loss = 0.11108073\n",
      "Iteration 196, loss = 0.09012430\n",
      "Iteration 127, loss = 0.11052857\n",
      "Iteration 181, loss = 0.17165144\n",
      "Iteration 302, loss = 0.12599810\n",
      "Iteration 197, loss = 0.08990095\n",
      "Iteration 128, loss = 0.11000187\n",
      "Iteration 129, loss = 0.10947611\n",
      "Iteration 130, loss = 0.10895644\n",
      "Iteration 198, loss = 0.08968056\n",
      "Iteration 199, loss = 0.08946728\n",
      "Iteration 303, loss = 0.12591363\n",
      "Iteration 200, loss = 0.08924261\n",
      "Iteration 201, loss = 0.08903594\n",
      "Iteration 281, loss = 0.21535797\n",
      "Iteration 202, loss = 0.08881167\n",
      "Iteration 203, loss = 0.08860455\n",
      "Iteration 204, loss = 0.08839650\n",
      "Iteration 205, loss = 0.08818651\n",
      "Iteration 206, loss = 0.08798093\n",
      "Iteration 207, loss = 0.08777361\n",
      "Iteration 50, loss = 0.37533008\n",
      "Iteration 131, loss = 0.10845367\n",
      "Iteration 138, loss = 0.20225226\n",
      "Iteration 132, loss = 0.10797035\n",
      "Iteration 182, loss = 0.17131659\n",
      "Iteration 208, loss = 0.08757040\n",
      "Iteration 133, loss = 0.10749340\n",
      "Iteration 209, loss = 0.08737795\n",
      "Iteration 134, loss = 0.10699165\n",
      "Iteration 135, loss = 0.10652370Iteration 210, loss = 0.08717204\n",
      "\n",
      "Iteration 136, loss = 0.10604735\n",
      "Iteration 304, loss = 0.12583340\n",
      "Iteration 211, loss = 0.08697190\n",
      "Iteration 212, loss = 0.08677336\n",
      "Iteration 213, loss = 0.08657976\n",
      "Iteration 137, loss = 0.10560683\n",
      "Iteration 138, loss = 0.10513154\n",
      "Iteration 139, loss = 0.10469556\n",
      "Iteration 214, loss = 0.08638851\n",
      "Iteration 183, loss = 0.17100664\n",
      "Iteration 140, loss = 0.10426438\n",
      "Iteration 282, loss = 0.21534339\n",
      "Iteration 141, loss = 0.10382201\n",
      "Iteration 142, loss = 0.10338507\n",
      "Iteration 233, loss = 0.15231424\n",
      "Iteration 143, loss = 0.10296537\n",
      "Iteration 215, loss = 0.08619461\n",
      "Iteration 305, loss = 0.12577573\n",
      "Iteration 216, loss = 0.08600655\n",
      "Iteration 139, loss = 0.20190519\n",
      "Iteration 217, loss = 0.08580819\n",
      "Iteration 218, loss = 0.08562163\n",
      "Iteration 219, loss = 0.08543655\n",
      "Iteration 220, loss = 0.08525495\n",
      "Iteration 221, loss = 0.08506268\n",
      "Iteration 234, loss = 0.15212711\n",
      "Iteration 144, loss = 0.10256750\n",
      "Iteration 222, loss = 0.08488294\n",
      "Iteration 223, loss = 0.08471230\n",
      "Iteration 224, loss = 0.08452899\n",
      "Iteration 51, loss = 0.37081639\n",
      "Iteration 145, loss = 0.10215987\n",
      "Iteration 146, loss = 0.10174519\n",
      "Iteration 225, loss = 0.08434269\n",
      "Iteration 306, loss = 0.12569175\n",
      "Iteration 147, loss = 0.10133610\n",
      "Iteration 148, loss = 0.10093481\n",
      "Iteration 149, loss = 0.10055124\n",
      "Iteration 150, loss = 0.10016133\n",
      "Iteration 151, loss = 0.09978971\n",
      "Iteration 152, loss = 0.09939945\n",
      "Iteration 226, loss = 0.08416836\n",
      "Iteration 184, loss = 0.17068312\n",
      "Iteration 227, loss = 0.08398926\n",
      "Iteration 228, loss = 0.08381981\n",
      "Iteration 229, loss = 0.08364857\n",
      "Iteration 230, loss = 0.08347155\n",
      "Iteration 231, loss = 0.08330402\n",
      "Iteration 153, loss = 0.09901932\n",
      "Iteration 154, loss = 0.09865552\n",
      "Iteration 155, loss = 0.09829341\n",
      "Iteration 156, loss = 0.09794359\n",
      "Iteration 157, loss = 0.09759311\n",
      "Iteration 158, loss = 0.09722817\n",
      "Iteration 159, loss = 0.09689046\n",
      "Iteration 235, loss = 0.15198991\n",
      "Iteration 140, loss = 0.20161505\n",
      "Iteration 307, loss = 0.12560006\n",
      "Iteration 52, loss = 0.36621044\n",
      "Iteration 232, loss = 0.08313503\n",
      "Iteration 283, loss = 0.21524634\n",
      "Iteration 233, loss = 0.08296281\n",
      "Iteration 234, loss = 0.08279870\n",
      "Iteration 185, loss = 0.17034044\n",
      "Iteration 235, loss = 0.08262735\n",
      "Iteration 308, loss = 0.12552847\n",
      "Iteration 160, loss = 0.09655609\n",
      "Iteration 236, loss = 0.08246869\n",
      "Iteration 237, loss = 0.08230565\n",
      "Iteration 238, loss = 0.08213599\n",
      "Iteration 239, loss = 0.08197168\n",
      "Iteration 240, loss = 0.08180439\n",
      "Iteration 241, loss = 0.08165170\n",
      "Iteration 161, loss = 0.09621734\n",
      "Iteration 242, loss = 0.08149087\n",
      "Iteration 162, loss = 0.09587919\n",
      "Iteration 236, loss = 0.15184533\n",
      "Iteration 243, loss = 0.08133484\n",
      "Iteration 163, loss = 0.09553434\n",
      "Iteration 164, loss = 0.09523300\n",
      "Iteration 165, loss = 0.09488651\n",
      "Iteration 141, loss = 0.20122427\n",
      "Iteration 309, loss = 0.12548231\n",
      "Iteration 166, loss = 0.09458083\n",
      "Iteration 244, loss = 0.08117791Iteration 167, loss = 0.09424245\n",
      "Iteration 186, loss = 0.17004089\n",
      "\n",
      "Iteration 245, loss = 0.08101893\n",
      "Iteration 168, loss = 0.09393178\n",
      "Iteration 237, loss = 0.15169300\n",
      "Iteration 246, loss = 0.08087053\n",
      "Iteration 169, loss = 0.09362274\n",
      "Iteration 53, loss = 0.36173653\n",
      "Iteration 247, loss = 0.08071947\n",
      "Iteration 310, loss = 0.12537305\n",
      "Iteration 248, loss = 0.08055876\n",
      "Iteration 170, loss = 0.09332668\n",
      "Iteration 171, loss = 0.09301098\n",
      "Iteration 187, loss = 0.16972444\n",
      "Iteration 172, loss = 0.09270206\n",
      "Iteration 311, loss = 0.12530867\n",
      "Iteration 173, loss = 0.09239699\n",
      "Iteration 249, loss = 0.08040535\n",
      "Iteration 174, loss = 0.09209533\n",
      "Iteration 175, loss = 0.09181340\n",
      "Iteration 250, loss = 0.08025376\n",
      "Iteration 284, loss = 0.21514972\n",
      "Iteration 176, loss = 0.09151508\n",
      "Iteration 177, loss = 0.09124038\n",
      "Iteration 178, loss = 0.09094916\n",
      "Iteration 179, loss = 0.09066134\n",
      "Iteration 180, loss = 0.09038550\n",
      "Iteration 251, loss = 0.08010642\n",
      "Iteration 188, loss = 0.16941074\n",
      "Iteration 142, loss = 0.20093144\n",
      "Iteration 181, loss = 0.09010728\n",
      "Iteration 252, loss = 0.07996083\n",
      "Iteration 253, loss = 0.07981026\n",
      "Iteration 312, loss = 0.12523892\n",
      "Iteration 254, loss = 0.07966482\n",
      "Iteration 238, loss = 0.15156641\n",
      "Iteration 255, loss = 0.07952219\n",
      "Iteration 256, loss = 0.07937041\n",
      "Iteration 285, loss = 0.21505633\n",
      "Iteration 257, loss = 0.07923076\n",
      "Iteration 258, loss = 0.07908229\n",
      "Iteration 182, loss = 0.08984427\n",
      "Iteration 259, loss = 0.07893657\n",
      "Iteration 183, loss = 0.08957767\n",
      "Iteration 54, loss = 0.35748939\n",
      "Iteration 189, loss = 0.16913037\n",
      "Iteration 260, loss = 0.07879848\n",
      "Iteration 313, loss = 0.12517427\n",
      "Iteration 239, loss = 0.15145173\n",
      "Iteration 261, loss = 0.07866484\n",
      "Iteration 184, loss = 0.08930259\n",
      "Iteration 262, loss = 0.07852288\n",
      "Iteration 185, loss = 0.08902829\n",
      "Iteration 186, loss = 0.08877035\n",
      "Iteration 263, loss = 0.07837603\n",
      "Iteration 187, loss = 0.08851329\n",
      "Iteration 188, loss = 0.08825609\n",
      "Iteration 264, loss = 0.07823948\n",
      "Iteration 189, loss = 0.08800498\n",
      "Iteration 265, loss = 0.07809847\n",
      "Iteration 143, loss = 0.20059895\n",
      "Iteration 190, loss = 0.08774949\n",
      "Iteration 314, loss = 0.12508594\n",
      "Iteration 266, loss = 0.07796496\n",
      "Iteration 267, loss = 0.07782783\n",
      "Iteration 268, loss = 0.07769442\n",
      "Iteration 191, loss = 0.08750054\n",
      "Iteration 269, loss = 0.07756041\n",
      "Iteration 270, loss = 0.07742697\n",
      "Iteration 192, loss = 0.08725628\n",
      "Iteration 193, loss = 0.08700724\n",
      "Iteration 190, loss = 0.16881379\n",
      "Iteration 271, loss = 0.07729421\n",
      "Iteration 272, loss = 0.07716053\n",
      "Iteration 273, loss = 0.07703185\n",
      "Iteration 274, loss = 0.07689677\n",
      "Iteration 275, loss = 0.07676841\n",
      "Iteration 276, loss = 0.07664255\n",
      "Iteration 277, loss = 0.07652008\n",
      "Iteration 194, loss = 0.08675655\n",
      "Iteration 195, loss = 0.08651368\n",
      "Iteration 315, loss = 0.12503760\n",
      "Iteration 196, loss = 0.08628167\n",
      "Iteration 278, loss = 0.07638303\n",
      "Iteration 240, loss = 0.15127580\n",
      "Iteration 197, loss = 0.08602613\n",
      "Iteration 286, loss = 0.21498343\n",
      "Iteration 144, loss = 0.20029646\n",
      "Iteration 191, loss = 0.16852088\n",
      "Iteration 55, loss = 0.35331456\n",
      "Iteration 198, loss = 0.08580017\n",
      "Iteration 199, loss = 0.08556453\n",
      "Iteration 279, loss = 0.07625490\n",
      "Iteration 280, loss = 0.07614443\n",
      "Iteration 281, loss = 0.07600526\n",
      "Iteration 282, loss = 0.07588100\n",
      "Iteration 283, loss = 0.07575597\n",
      "Iteration 284, loss = 0.07563167\n",
      "Iteration 285, loss = 0.07551445\n",
      "Iteration 200, loss = 0.08533640\n",
      "Iteration 286, loss = 0.07539456\n",
      "Iteration 316, loss = 0.12494384\n",
      "Iteration 201, loss = 0.08509830\n",
      "Iteration 287, loss = 0.07527113\n",
      "Iteration 288, loss = 0.07515054\n",
      "Iteration 202, loss = 0.08486939\n",
      "Iteration 289, loss = 0.07502926\n",
      "Iteration 241, loss = 0.15116505\n",
      "Iteration 290, loss = 0.07490641\n",
      "Iteration 291, loss = 0.07479264\n",
      "Iteration 203, loss = 0.08466285\n",
      "Iteration 292, loss = 0.07467509\n",
      "Iteration 293, loss = 0.07455537\n",
      "Iteration 294, loss = 0.07443557\n",
      "Iteration 317, loss = 0.12488959\n",
      "Iteration 204, loss = 0.08443299\n",
      "Iteration 205, loss = 0.08420709\n",
      "Iteration 192, loss = 0.16824636\n",
      "Iteration 295, loss = 0.07431817\n",
      "Iteration 206, loss = 0.08398020\n",
      "Iteration 287, loss = 0.21494262\n",
      "Iteration 207, loss = 0.08376912\n",
      "Iteration 318, loss = 0.12482072\n",
      "Iteration 193, loss = 0.16795901\n",
      "Iteration 145, loss = 0.19998534\n",
      "Iteration 208, loss = 0.08353598\n",
      "Iteration 209, loss = 0.08333423\n",
      "Iteration 210, loss = 0.08311751\n",
      "Iteration 211, loss = 0.08289763\n",
      "Iteration 242, loss = 0.15099161Iteration 56, loss = 0.34925854\n",
      "Iteration 212, loss = 0.08268982\n",
      "\n",
      "Iteration 213, loss = 0.08248349\n",
      "Iteration 214, loss = 0.08227796\n",
      "Iteration 296, loss = 0.07420038\n",
      "Iteration 297, loss = 0.07408934\n",
      "Iteration 298, loss = 0.07397436\n",
      "Iteration 319, loss = 0.12473735\n",
      "Iteration 243, loss = 0.15088346\n",
      "Iteration 299, loss = 0.07386231\n",
      "Iteration 288, loss = 0.21489862\n",
      "Iteration 300, loss = 0.07374644\n",
      "Iteration 215, loss = 0.08206242\n",
      "Iteration 216, loss = 0.08186930\n",
      "Iteration 217, loss = 0.08166902\n",
      "Iteration 218, loss = 0.08145913\n",
      "Iteration 219, loss = 0.08126635\n",
      "Iteration 146, loss = 0.19967429\n",
      "Iteration 220, loss = 0.08107198\n",
      "Iteration 301, loss = 0.07363256\n",
      "Iteration 194, loss = 0.16767967\n",
      "Iteration 302, loss = 0.07351451\n",
      "Iteration 303, loss = 0.07340698\n",
      "Iteration 221, loss = 0.08087078\n",
      "Iteration 304, loss = 0.07330183\n",
      "Iteration 222, loss = 0.08066855\n",
      "Iteration 320, loss = 0.12466341\n",
      "Iteration 57, loss = 0.34537963\n",
      "Iteration 223, loss = 0.08047474\n",
      "Iteration 224, loss = 0.08030134\n",
      "Iteration 305, loss = 0.07318682\n",
      "Iteration 225, loss = 0.08009291\n",
      "Iteration 195, loss = 0.16743274\n",
      "Iteration 226, loss = 0.07990176\n",
      "Iteration 306, loss = 0.07307319\n",
      "Iteration 227, loss = 0.07971239\n",
      "Iteration 228, loss = 0.07952275\n",
      "Iteration 229, loss = 0.07934974\n",
      "Iteration 307, loss = 0.07296328\n",
      "Iteration 308, loss = 0.07285090\n",
      "Iteration 309, loss = 0.07275252\n",
      "Iteration 230, loss = 0.07916119\n",
      "Iteration 310, loss = 0.07264005\n",
      "Iteration 321, loss = 0.12460560\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 231, loss = 0.07897229\n",
      "Iteration 232, loss = 0.07880405\n",
      "Iteration 244, loss = 0.15076905\n",
      "Iteration 311, loss = 0.07253220\n",
      "Iteration 233, loss = 0.07861781\n",
      "Iteration 196, loss = 0.16716366\n",
      "Iteration 312, loss = 0.07242734\n",
      "Iteration 234, loss = 0.07845689\n",
      "Iteration 235, loss = 0.07825355\n",
      "Iteration 236, loss = 0.07808554\n",
      "Iteration 313, loss = 0.07232238\n",
      "Iteration 237, loss = 0.07791348\n",
      "Iteration 197, loss = 0.16689642\n",
      "Iteration 1, loss = 0.57286976\n",
      "Iteration 238, loss = 0.07774518\n",
      "Iteration 147, loss = 0.19934676\n",
      "Iteration 58, loss = 0.34153186\n",
      "Iteration 314, loss = 0.07221171\n",
      "Iteration 289, loss = 0.21477931\n",
      "Iteration 239, loss = 0.07756074\n",
      "Iteration 240, loss = 0.07739640\n",
      "Iteration 315, loss = 0.07211365\n",
      "Iteration 2, loss = 0.55879844\n",
      "Iteration 316, loss = 0.07200514\n",
      "Iteration 317, loss = 0.07190233\n",
      "Iteration 318, loss = 0.07179900\n",
      "Iteration 319, loss = 0.07169475\n",
      "Iteration 320, loss = 0.07159588\n",
      "Iteration 241, loss = 0.07722645\n",
      "Iteration 3, loss = 0.53954590\n",
      "Iteration 242, loss = 0.07705744\n",
      "Iteration 243, loss = 0.07689766\n",
      "Iteration 321, loss = 0.07149086\n",
      "Iteration 244, loss = 0.07672310\n",
      "Iteration 322, loss = 0.07139062\n",
      "Iteration 4, loss = 0.51901610\n",
      "Iteration 245, loss = 0.07656565\n",
      "Iteration 5, loss = 0.49888721\n",
      "Iteration 245, loss = 0.15060702\n",
      "Iteration 323, loss = 0.07129601\n",
      "Iteration 246, loss = 0.07640481\n",
      "Iteration 6, loss = 0.47981940\n",
      "Iteration 247, loss = 0.07623947\n",
      "Iteration 7, loss = 0.46232084\n",
      "Iteration 248, loss = 0.07606969\n",
      "Iteration 249, loss = 0.07590372\n",
      "Iteration 8, loss = 0.44647254\n",
      "Iteration 250, loss = 0.07577049\n",
      "Iteration 324, loss = 0.07118494\n",
      "Iteration 251, loss = 0.07559521\n",
      "Iteration 325, loss = 0.07108883\n",
      "Iteration 252, loss = 0.07542802\n",
      "Iteration 326, loss = 0.07098650\n",
      "Iteration 253, loss = 0.07527895\n",
      "Iteration 254, loss = 0.07511370\n",
      "Iteration 327, loss = 0.07088739\n",
      "Iteration 255, loss = 0.07496859\n",
      "Iteration 256, loss = 0.07481557\n",
      "Iteration 257, loss = 0.07466822\n",
      "Iteration 258, loss = 0.07450183\n",
      "Iteration 259, loss = 0.07438036\n",
      "Iteration 260, loss = 0.07421194\n",
      "Iteration 59, loss = 0.33787411\n",
      "Iteration 261, loss = 0.07406527\n",
      "Iteration 262, loss = 0.07391479\n",
      "Iteration 148, loss = 0.19907493\n",
      "Iteration 263, loss = 0.07376777\n",
      "Iteration 264, loss = 0.07362076\n",
      "Iteration 265, loss = 0.07346830\n",
      "Iteration 9, loss = 0.43161440\n",
      "Iteration 266, loss = 0.07332934\n",
      "Iteration 198, loss = 0.16662913\n",
      "Iteration 328, loss = 0.07079539\n",
      "Iteration 267, loss = 0.07318181\n",
      "Iteration 246, loss = 0.15046512\n",
      "Iteration 10, loss = 0.41832463\n",
      "Iteration 11, loss = 0.40559014\n",
      "Iteration 12, loss = 0.39386880\n",
      "Iteration 268, loss = 0.07304225\n",
      "Iteration 13, loss = 0.38309090\n",
      "Iteration 14, loss = 0.37265641\n",
      "Iteration 15, loss = 0.36292661\n",
      "Iteration 16, loss = 0.35370273\n",
      "Iteration 329, loss = 0.07069042\n",
      "Iteration 290, loss = 0.21467057\n",
      "Iteration 247, loss = 0.15033506\n",
      "Iteration 17, loss = 0.34493527\n",
      "Iteration 330, loss = 0.07060030\n",
      "Iteration 331, loss = 0.07049456\n",
      "Iteration 199, loss = 0.16637152\n",
      "Iteration 332, loss = 0.07039969\n",
      "Iteration 333, loss = 0.07030320\n",
      "Iteration 334, loss = 0.07020853\n",
      "Iteration 269, loss = 0.07289557\n",
      "Iteration 18, loss = 0.33663918\n",
      "Iteration 335, loss = 0.07011438\n",
      "Iteration 19, loss = 0.32860465\n",
      "Iteration 336, loss = 0.07002066\n",
      "Iteration 149, loss = 0.19876013\n",
      "Iteration 200, loss = 0.16614975\n",
      "Iteration 20, loss = 0.32080405\n",
      "Iteration 270, loss = 0.07275124\n",
      "Iteration 271, loss = 0.07260914\n",
      "Iteration 272, loss = 0.07247292\n",
      "Iteration 273, loss = 0.07233637\n",
      "Iteration 274, loss = 0.07219429\n",
      "Iteration 275, loss = 0.07206382\n",
      "Iteration 276, loss = 0.07192548\n",
      "Iteration 277, loss = 0.07178448\n",
      "Iteration 278, loss = 0.07166247\n",
      "Iteration 337, loss = 0.06991824Iteration 291, loss = 0.21458584\n",
      "Iteration 21, loss = 0.31360576\n",
      "\n",
      "Iteration 338, loss = 0.06982795\n",
      "Iteration 60, loss = 0.33431799\n",
      "Iteration 339, loss = 0.06973181\n",
      "Iteration 340, loss = 0.06964024\n",
      "Iteration 22, loss = 0.30654816\n",
      "Iteration 341, loss = 0.06954511\n",
      "Iteration 342, loss = 0.06945612\n",
      "Iteration 343, loss = 0.06935965\n",
      "Iteration 201, loss = 0.16592603\n",
      "Iteration 344, loss = 0.06927474\n",
      "Iteration 150, loss = 0.19846319\n",
      "Iteration 23, loss = 0.29960323\n",
      "Iteration 24, loss = 0.29321578\n",
      "Iteration 248, loss = 0.15021669\n",
      "Iteration 345, loss = 0.06917445\n",
      "Iteration 279, loss = 0.07151970\n",
      "Iteration 346, loss = 0.06908555\n",
      "Iteration 347, loss = 0.06899618\n",
      "Iteration 25, loss = 0.28683050\n",
      "Iteration 280, loss = 0.07138392\n",
      "Iteration 281, loss = 0.07126505\n",
      "Iteration 282, loss = 0.07112709\n",
      "Iteration 26, loss = 0.28086001\n",
      "Iteration 27, loss = 0.27498777\n",
      "Iteration 28, loss = 0.26944107\n",
      "Iteration 283, loss = 0.07100292\n",
      "Iteration 284, loss = 0.07086419\n",
      "Iteration 348, loss = 0.06889887\n",
      "Iteration 285, loss = 0.07074000\n",
      "Iteration 286, loss = 0.07061215\n",
      "Iteration 287, loss = 0.07048988\n",
      "Iteration 349, loss = 0.06881136\n",
      "Iteration 288, loss = 0.07035759\n",
      "Iteration 292, loss = 0.21453751\n",
      "Iteration 289, loss = 0.07023724\n",
      "Iteration 290, loss = 0.07012969\n",
      "Iteration 350, loss = 0.06872009\n",
      "Iteration 249, loss = 0.15008977\n",
      "Iteration 202, loss = 0.16568710\n",
      "Iteration 29, loss = 0.26416979\n",
      "Iteration 351, loss = 0.06863779\n",
      "Iteration 352, loss = 0.06854175\n",
      "Iteration 30, loss = 0.25889867\n",
      "Iteration 353, loss = 0.06846314\n",
      "Iteration 291, loss = 0.06998726\n",
      "Iteration 354, loss = 0.06836984\n",
      "Iteration 355, loss = 0.06828059\n",
      "Iteration 61, loss = 0.33091175\n",
      "Iteration 31, loss = 0.25404814\n",
      "Iteration 356, loss = 0.06820005\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 292, loss = 0.06986386\n",
      "Iteration 32, loss = 0.24934434\n",
      "Iteration 33, loss = 0.24479266\n",
      "Iteration 34, loss = 0.24037831\n",
      "Iteration 250, loss = 0.14996778\n",
      "Iteration 35, loss = 0.23620031\n",
      "Iteration 36, loss = 0.23208945\n",
      "Iteration 203, loss = 0.16540601\n",
      "Iteration 37, loss = 0.22816012\n",
      "Iteration 38, loss = 0.22438499\n",
      "Iteration 293, loss = 0.06974859\n",
      "Iteration 151, loss = 0.19817594\n",
      "Iteration 294, loss = 0.06963159\n",
      "Iteration 1, loss = 0.74408102\n",
      "Iteration 204, loss = 0.16516943\n",
      "Iteration 295, loss = 0.06951782\n",
      "Iteration 296, loss = 0.06939955\n",
      "Iteration 297, loss = 0.06929670\n",
      "Iteration 298, loss = 0.06916898\n",
      "Iteration 39, loss = 0.22079697\n",
      "Iteration 299, loss = 0.06905915\n",
      "Iteration 300, loss = 0.06895039\n",
      "Iteration 301, loss = 0.06884963\n",
      "Iteration 205, loss = 0.16495783\n",
      "Iteration 2, loss = 0.70875881\n",
      "Iteration 302, loss = 0.06873243\n",
      "Iteration 293, loss = 0.21445876\n",
      "Iteration 3, loss = 0.66247621\n",
      "Iteration 303, loss = 0.06862950\n",
      "Iteration 40, loss = 0.21725613\n",
      "Iteration 304, loss = 0.06851559\n",
      "Iteration 4, loss = 0.61513241\n",
      "Iteration 305, loss = 0.06841394\n",
      "Iteration 62, loss = 0.32762742\n",
      "Iteration 306, loss = 0.06831353\n",
      "Iteration 152, loss = 0.19788850\n",
      "Iteration 5, loss = 0.57264468\n",
      "Iteration 6, loss = 0.53585873\n",
      "Iteration 7, loss = 0.50496928\n",
      "Iteration 307, loss = 0.06821021\n",
      "Iteration 251, loss = 0.14983129\n",
      "Iteration 308, loss = 0.06809688\n",
      "Iteration 309, loss = 0.06799015\n",
      "Iteration 41, loss = 0.21394422\n",
      "Iteration 42, loss = 0.21066375\n",
      "Iteration 310, loss = 0.06789240\n",
      "Iteration 311, loss = 0.06777982\n",
      "Iteration 43, loss = 0.20749479Iteration 312, loss = 0.06768006\n",
      "\n",
      "Iteration 313, loss = 0.06758282\n",
      "Iteration 314, loss = 0.06748941\n",
      "Iteration 315, loss = 0.06738247\n",
      "Iteration 316, loss = 0.06727804\n",
      "Iteration 317, loss = 0.06717942\n",
      "Iteration 318, loss = 0.06708302\n",
      "Iteration 319, loss = 0.06698098\n",
      "Iteration 320, loss = 0.06688758\n",
      "Iteration 321, loss = 0.06678424\n",
      "Iteration 252, loss = 0.14969395\n",
      "Iteration 322, loss = 0.06668822\n",
      "Iteration 44, loss = 0.20453610\n",
      "Iteration 323, loss = 0.06659531\n",
      "Iteration 324, loss = 0.06649769\n",
      "Iteration 325, loss = 0.06640251\n",
      "Iteration 45, loss = 0.20156537\n",
      "Iteration 46, loss = 0.19877453\n",
      "Iteration 47, loss = 0.19606149\n",
      "Iteration 48, loss = 0.19344495\n",
      "Iteration 49, loss = 0.19087691\n",
      "Iteration 63, loss = 0.32446628\n",
      "Iteration 206, loss = 0.16469565\n",
      "Iteration 8, loss = 0.47798285\n",
      "Iteration 50, loss = 0.18845706\n",
      "Iteration 9, loss = 0.45512563\n",
      "Iteration 153, loss = 0.19764347\n",
      "Iteration 326, loss = 0.06630500\n",
      "Iteration 51, loss = 0.18603585\n",
      "Iteration 10, loss = 0.43608087\n",
      "Iteration 11, loss = 0.41881611\n",
      "Iteration 294, loss = 0.21437507\n",
      "Iteration 207, loss = 0.16448446\n",
      "Iteration 12, loss = 0.40405006\n",
      "Iteration 52, loss = 0.18371282\n",
      "Iteration 53, loss = 0.18149233\n",
      "Iteration 13, loss = 0.39096744\n",
      "Iteration 327, loss = 0.06621396\n",
      "Iteration 54, loss = 0.17933817\n",
      "Iteration 14, loss = 0.37917772\n",
      "Iteration 328, loss = 0.06613328\n",
      "Iteration 15, loss = 0.36813204\n",
      "Iteration 16, loss = 0.35833636\n",
      "Iteration 64, loss = 0.32149317\n",
      "Iteration 55, loss = 0.17720731\n",
      "Iteration 154, loss = 0.19734718\n",
      "Iteration 208, loss = 0.16426645\n",
      "Iteration 65, loss = 0.31854080\n",
      "Iteration 253, loss = 0.14960216\n",
      "Iteration 56, loss = 0.17517227\n",
      "Iteration 329, loss = 0.06602157\n",
      "Iteration 57, loss = 0.17312982\n",
      "Iteration 17, loss = 0.34917530\n",
      "Iteration 330, loss = 0.06593300\n",
      "Iteration 58, loss = 0.17131125\n",
      "Iteration 331, loss = 0.06583463\n",
      "Iteration 332, loss = 0.06574904\n",
      "Iteration 333, loss = 0.06565583\n",
      "Iteration 18, loss = 0.34058165\n",
      "Iteration 334, loss = 0.06556657\n",
      "Iteration 59, loss = 0.16936026\n",
      "Iteration 335, loss = 0.06547371\n",
      "Iteration 60, loss = 0.16761020\n",
      "Iteration 209, loss = 0.16405502\n",
      "Iteration 336, loss = 0.06537905\n",
      "Iteration 61, loss = 0.16587645\n",
      "Iteration 337, loss = 0.06530398\n",
      "Iteration 19, loss = 0.33264941\n",
      "Iteration 338, loss = 0.06521656\n",
      "Iteration 62, loss = 0.16407851\n",
      "Iteration 339, loss = 0.06512203\n",
      "Iteration 254, loss = 0.14943891\n",
      "Iteration 63, loss = 0.16245816\n",
      "Iteration 340, loss = 0.06503383\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 64, loss = 0.16084362\n",
      "Iteration 155, loss = 0.19707791\n",
      "Iteration 1, loss = 0.79757222\n",
      "Iteration 65, loss = 0.15930980\n",
      "Iteration 20, loss = 0.32506141\n",
      "Iteration 66, loss = 0.15775592\n",
      "Iteration 2, loss = 0.75820695\n",
      "Iteration 67, loss = 0.15623992\n",
      "Iteration 68, loss = 0.15481235\n",
      "Iteration 69, loss = 0.15337467\n",
      "Iteration 295, loss = 0.21433900\n",
      "Iteration 21, loss = 0.31802098\n",
      "Iteration 22, loss = 0.31132475\n",
      "Iteration 210, loss = 0.16382003\n",
      "Iteration 255, loss = 0.14934962Iteration 70, loss = 0.15201221\n",
      "Iteration 23, loss = 0.30482036\n",
      "\n",
      "Iteration 3, loss = 0.70695470\n",
      "Iteration 24, loss = 0.29862500\n",
      "Iteration 71, loss = 0.15064880\n",
      "Iteration 211, loss = 0.16362363\n",
      "Iteration 25, loss = 0.29269237\n",
      "Iteration 296, loss = 0.21422107\n",
      "Iteration 72, loss = 0.14936059\n",
      "Iteration 26, loss = 0.28702498\n",
      "Iteration 27, loss = 0.28157985\n",
      "Iteration 73, loss = 0.14806620\n",
      "Iteration 4, loss = 0.65596707\n",
      "Iteration 28, loss = 0.27631148\n",
      "Iteration 29, loss = 0.27121143\n",
      "Iteration 256, loss = 0.14925103\n",
      "Iteration 156, loss = 0.19681790\n",
      "Iteration 74, loss = 0.14677242\n",
      "Iteration 66, loss = 0.31574545\n",
      "Iteration 30, loss = 0.26634698\n",
      "Iteration 212, loss = 0.16340685\n",
      "Iteration 5, loss = 0.60910388\n",
      "Iteration 75, loss = 0.14557840\n",
      "Iteration 31, loss = 0.26149724\n",
      "Iteration 76, loss = 0.14439944\n",
      "Iteration 6, loss = 0.56882244\n",
      "Iteration 77, loss = 0.14321331\n",
      "Iteration 78, loss = 0.14211024\n",
      "Iteration 7, loss = 0.53466357\n",
      "Iteration 32, loss = 0.25691919\n",
      "Iteration 8, loss = 0.50451868\n",
      "Iteration 297, loss = 0.21414953\n",
      "Iteration 67, loss = 0.31308460\n",
      "Iteration 157, loss = 0.19654980\n",
      "Iteration 257, loss = 0.14906765\n",
      "Iteration 9, loss = 0.47979643\n",
      "Iteration 213, loss = 0.16323213\n",
      "Iteration 10, loss = 0.45780704\n",
      "Iteration 79, loss = 0.14096370\n",
      "Iteration 33, loss = 0.25257474\n",
      "Iteration 34, loss = 0.24831979\n",
      "Iteration 35, loss = 0.24421388\n",
      "Iteration 80, loss = 0.13991795\n",
      "Iteration 36, loss = 0.24015397\n",
      "Iteration 11, loss = 0.43886899\n",
      "Iteration 258, loss = 0.14895572\n",
      "Iteration 81, loss = 0.13884766Iteration 37, loss = 0.23631903\n",
      "\n",
      "Iteration 38, loss = 0.23273040\n",
      "Iteration 82, loss = 0.13779039\n",
      "Iteration 214, loss = 0.16298478\n",
      "Iteration 83, loss = 0.13681054\n",
      "Iteration 84, loss = 0.13580980\n",
      "Iteration 12, loss = 0.42149768\n",
      "Iteration 13, loss = 0.40619122\n",
      "Iteration 14, loss = 0.39215213\n",
      "Iteration 85, loss = 0.13488187\n",
      "Iteration 86, loss = 0.13389900\n",
      "Iteration 87, loss = 0.13296535\n",
      "Iteration 88, loss = 0.13209710\n",
      "Iteration 259, loss = 0.14881483\n",
      "Iteration 158, loss = 0.19629433\n",
      "Iteration 39, loss = 0.22904357\n",
      "Iteration 298, loss = 0.21406447\n",
      "Iteration 68, loss = 0.31052343\n",
      "Iteration 215, loss = 0.16278795\n",
      "Iteration 89, loss = 0.13119116\n",
      "Iteration 40, loss = 0.22563919\n",
      "Iteration 90, loss = 0.13032921\n",
      "Iteration 91, loss = 0.12949081\n",
      "Iteration 15, loss = 0.37915633\n",
      "Iteration 92, loss = 0.12867889\n",
      "Iteration 41, loss = 0.22224488\n",
      "Iteration 16, loss = 0.36750984\n",
      "Iteration 17, loss = 0.35629740\n",
      "Iteration 42, loss = 0.21895399\n",
      "Iteration 260, loss = 0.14870539\n",
      "Iteration 93, loss = 0.12787908\n",
      "Iteration 43, loss = 0.21582762\n",
      "Iteration 94, loss = 0.12709642\n",
      "Iteration 216, loss = 0.16258899\n",
      "Iteration 95, loss = 0.12635108\n",
      "Iteration 44, loss = 0.21270985\n",
      "Iteration 96, loss = 0.12560488\n",
      "Iteration 159, loss = 0.19604469\n",
      "Iteration 18, loss = 0.34598528\n",
      "Iteration 69, loss = 0.30804821\n",
      "Iteration 97, loss = 0.12485476\n",
      "Iteration 45, loss = 0.20987948\n",
      "Iteration 98, loss = 0.12414998Iteration 299, loss = 0.21405240\n",
      "\n",
      "Iteration 46, loss = 0.20688380\n",
      "Iteration 217, loss = 0.16236558\n",
      "Iteration 47, loss = 0.20415467\n",
      "Iteration 99, loss = 0.12343945\n",
      "Iteration 261, loss = 0.14859343\n",
      "Iteration 19, loss = 0.33633257\n",
      "Iteration 48, loss = 0.20150590\n",
      "Iteration 20, loss = 0.32723008\n",
      "Iteration 160, loss = 0.19576150\n",
      "Iteration 100, loss = 0.12275795\n",
      "Iteration 49, loss = 0.19881932\n",
      "Iteration 101, loss = 0.12209163\n",
      "Iteration 50, loss = 0.19634037\n",
      "Iteration 102, loss = 0.12143216\n",
      "Iteration 218, loss = 0.16219583\n",
      "Iteration 21, loss = 0.31866192\n",
      "Iteration 262, loss = 0.14846340\n",
      "Iteration 103, loss = 0.12077870\n",
      "Iteration 22, loss = 0.31047517\n",
      "Iteration 104, loss = 0.12012480\n",
      "Iteration 51, loss = 0.19393942\n",
      "Iteration 52, loss = 0.19152106\n",
      "Iteration 70, loss = 0.30562172\n",
      "Iteration 53, loss = 0.18920602\n",
      "Iteration 54, loss = 0.18698258\n",
      "Iteration 219, loss = 0.16198492\n",
      "Iteration 105, loss = 0.11950667\n",
      "Iteration 23, loss = 0.30280303\n",
      "Iteration 106, loss = 0.11890299\n",
      "Iteration 107, loss = 0.11830916\n",
      "Iteration 24, loss = 0.29549577\n",
      "Iteration 263, loss = 0.14835284\n",
      "Iteration 25, loss = 0.28838221\n",
      "Iteration 55, loss = 0.18481732\n",
      "Iteration 108, loss = 0.11771238\n",
      "Iteration 109, loss = 0.11710489\n",
      "Iteration 110, loss = 0.11655552\n",
      "Iteration 161, loss = 0.19552135\n",
      "Iteration 111, loss = 0.11598044\n",
      "Iteration 300, loss = 0.21398252\n",
      "Iteration 112, loss = 0.11541315\n",
      "Iteration 26, loss = 0.28189422\n",
      "Iteration 113, loss = 0.11488248\n",
      "Iteration 56, loss = 0.18269219\n",
      "Iteration 57, loss = 0.18063428\n",
      "Iteration 220, loss = 0.16181340\n",
      "Iteration 114, loss = 0.11434305\n",
      "Iteration 58, loss = 0.17862747\n",
      "Iteration 71, loss = 0.30339559\n",
      "Iteration 27, loss = 0.27548819\n",
      "Iteration 59, loss = 0.17672988\n",
      "Iteration 162, loss = 0.19527331\n",
      "Iteration 264, loss = 0.14824923\n",
      "Iteration 28, loss = 0.26952935\n",
      "Iteration 60, loss = 0.17488239\n",
      "Iteration 115, loss = 0.11381777\n",
      "Iteration 61, loss = 0.17304444\n",
      "Iteration 265, loss = 0.14811712\n",
      "Iteration 116, loss = 0.11328761\n",
      "Iteration 29, loss = 0.26372195\n",
      "Iteration 62, loss = 0.17126522\n",
      "Iteration 117, loss = 0.11276707\n",
      "Iteration 118, loss = 0.11227249Iteration 30, loss = 0.25840669\n",
      "Iteration 221, loss = 0.16162257\n",
      "\n",
      "Iteration 31, loss = 0.25308603\n",
      "Iteration 63, loss = 0.16955697\n",
      "Iteration 32, loss = 0.24811204\n",
      "Iteration 301, loss = 0.21384885\n",
      "Iteration 119, loss = 0.11177081\n",
      "Iteration 64, loss = 0.16791530\n",
      "Iteration 120, loss = 0.11127098\n",
      "Iteration 121, loss = 0.11078895\n",
      "Iteration 65, loss = 0.16622987\n",
      "Iteration 122, loss = 0.11031282\n",
      "Iteration 123, loss = 0.10984875\n",
      "Iteration 72, loss = 0.30118394\n",
      "Iteration 124, loss = 0.10938973\n",
      "Iteration 33, loss = 0.24332821\n",
      "Iteration 66, loss = 0.16470389\n",
      "Iteration 266, loss = 0.14799265\n",
      "Iteration 125, loss = 0.10893650\n",
      "Iteration 222, loss = 0.16141050\n",
      "Iteration 126, loss = 0.10850175\n",
      "Iteration 163, loss = 0.19504879\n",
      "Iteration 67, loss = 0.16312651\n",
      "Iteration 34, loss = 0.23878184\n",
      "Iteration 127, loss = 0.10804136\n",
      "Iteration 68, loss = 0.16164661\n",
      "Iteration 35, loss = 0.23438376\n",
      "Iteration 69, loss = 0.16014998\n",
      "Iteration 70, loss = 0.15875780\n",
      "Iteration 36, loss = 0.23023853\n",
      "Iteration 37, loss = 0.22616922\n",
      "Iteration 38, loss = 0.22224919\n",
      "Iteration 267, loss = 0.14789857\n",
      "Iteration 302, loss = 0.21382724\n",
      "Iteration 223, loss = 0.16122603\n",
      "Iteration 128, loss = 0.10761639\n",
      "Iteration 71, loss = 0.15737336\n",
      "Iteration 129, loss = 0.10719472\n",
      "Iteration 72, loss = 0.15604243\n",
      "Iteration 73, loss = 0.15470698\n",
      "Iteration 39, loss = 0.21854908\n",
      "Iteration 73, loss = 0.29904595\n",
      "Iteration 74, loss = 0.15349977\n",
      "Iteration 75, loss = 0.15218409\n",
      "Iteration 130, loss = 0.10675960\n",
      "Iteration 40, loss = 0.21491053\n",
      "Iteration 76, loss = 0.15101978\n",
      "Iteration 164, loss = 0.19478230\n",
      "Iteration 131, loss = 0.10636215\n",
      "Iteration 132, loss = 0.10596043Iteration 268, loss = 0.14777606\n",
      "\n",
      "Iteration 224, loss = 0.16104334\n",
      "Iteration 133, loss = 0.10554359\n",
      "Iteration 41, loss = 0.21153832\n",
      "Iteration 77, loss = 0.14980819\n",
      "Iteration 134, loss = 0.10516725\n",
      "Iteration 135, loss = 0.10476734\n",
      "Iteration 136, loss = 0.10438707\n",
      "Iteration 137, loss = 0.10399137\n",
      "Iteration 303, loss = 0.21372274\n",
      "Iteration 42, loss = 0.20817957\n",
      "Iteration 74, loss = 0.29697021\n",
      "Iteration 269, loss = 0.14765826\n",
      "Iteration 43, loss = 0.20500722\n",
      "Iteration 78, loss = 0.14863553\n",
      "Iteration 79, loss = 0.14749994\n",
      "Iteration 138, loss = 0.10362463\n",
      "Iteration 80, loss = 0.14642593\n",
      "Iteration 225, loss = 0.16086573\n",
      "Iteration 81, loss = 0.14536732\n",
      "Iteration 139, loss = 0.10324733\n",
      "Iteration 140, loss = 0.10287507\n",
      "Iteration 165, loss = 0.19455279\n",
      "Iteration 141, loss = 0.10251881\n",
      "Iteration 82, loss = 0.14430049\n",
      "Iteration 44, loss = 0.20188411\n",
      "Iteration 83, loss = 0.14326454\n",
      "Iteration 45, loss = 0.19897337\n",
      "Iteration 46, loss = 0.19612136\n",
      "Iteration 84, loss = 0.14227444\n",
      "Iteration 142, loss = 0.10215862\n",
      "Iteration 47, loss = 0.19332829\n",
      "Iteration 48, loss = 0.19072273\n",
      "Iteration 75, loss = 0.29501030\n",
      "Iteration 143, loss = 0.10179918\n",
      "Iteration 270, loss = 0.14754388\n",
      "Iteration 166, loss = 0.19433304\n",
      "Iteration 85, loss = 0.14129281\n",
      "Iteration 226, loss = 0.16070054\n",
      "Iteration 144, loss = 0.10144571\n",
      "Iteration 86, loss = 0.14035545\n",
      "Iteration 145, loss = 0.10109401\n",
      "Iteration 87, loss = 0.13940944\n",
      "Iteration 146, loss = 0.10075712\n",
      "Iteration 49, loss = 0.18818773\n",
      "Iteration 147, loss = 0.10041654\n",
      "Iteration 88, loss = 0.13849260\n",
      "Iteration 50, loss = 0.18567269\n",
      "Iteration 148, loss = 0.10007861\n",
      "Iteration 89, loss = 0.13760162\n",
      "Iteration 149, loss = 0.09975196\n",
      "Iteration 90, loss = 0.13675246\n",
      "Iteration 150, loss = 0.09942138\n",
      "Iteration 304, loss = 0.21370671\n",
      "Iteration 151, loss = 0.09910129\n",
      "Iteration 51, loss = 0.18335117\n",
      "Iteration 91, loss = 0.13586757\n",
      "Iteration 152, loss = 0.09879558\n",
      "Iteration 92, loss = 0.13504745\n",
      "Iteration 153, loss = 0.09846776\n",
      "Iteration 93, loss = 0.13423794\n",
      "Iteration 227, loss = 0.16048805\n",
      "Iteration 94, loss = 0.13343247\n",
      "Iteration 52, loss = 0.18103247\n",
      "Iteration 271, loss = 0.14745836\n",
      "Iteration 154, loss = 0.09815472\n",
      "Iteration 53, loss = 0.17881368\n",
      "Iteration 95, loss = 0.13263896\n",
      "Iteration 155, loss = 0.09785127\n",
      "Iteration 167, loss = 0.19407144\n",
      "Iteration 228, loss = 0.16031150\n",
      "Iteration 156, loss = 0.09754410\n",
      "Iteration 157, loss = 0.09724651\n",
      "Iteration 76, loss = 0.29303309\n",
      "Iteration 158, loss = 0.09695326\n",
      "Iteration 54, loss = 0.17670265\n",
      "Iteration 159, loss = 0.09665377\n",
      "Iteration 55, loss = 0.17462955\n",
      "Iteration 56, loss = 0.17262590\n",
      "Iteration 96, loss = 0.13190031\n",
      "Iteration 160, loss = 0.09636438\n",
      "Iteration 272, loss = 0.14734027\n",
      "Iteration 97, loss = 0.13112937\n",
      "Iteration 161, loss = 0.09607897\n",
      "Iteration 98, loss = 0.13040447\n",
      "Iteration 162, loss = 0.09579125\n",
      "Iteration 99, loss = 0.12969208\n",
      "Iteration 229, loss = 0.16020671\n",
      "Iteration 163, loss = 0.09550891\n",
      "Iteration 164, loss = 0.09522788\n",
      "Iteration 305, loss = 0.21358848\n",
      "Iteration 100, loss = 0.12898330\n",
      "Iteration 57, loss = 0.17070567\n",
      "Iteration 58, loss = 0.16887216\n",
      "Iteration 168, loss = 0.19383878\n",
      "Iteration 273, loss = 0.14723853\n",
      "Iteration 165, loss = 0.09494736\n",
      "Iteration 77, loss = 0.29127750\n",
      "Iteration 101, loss = 0.12829298\n",
      "Iteration 166, loss = 0.09468105\n",
      "Iteration 102, loss = 0.12760802\n",
      "Iteration 274, loss = 0.14710720\n",
      "Iteration 59, loss = 0.16702041\n",
      "Iteration 103, loss = 0.12696406\n",
      "Iteration 167, loss = 0.09441645\n",
      "Iteration 230, loss = 0.15998893\n",
      "Iteration 168, loss = 0.09413419\n",
      "Iteration 60, loss = 0.16525511\n",
      "Iteration 61, loss = 0.16358992\n",
      "Iteration 104, loss = 0.12628838\n",
      "Iteration 169, loss = 0.09388491\n",
      "Iteration 170, loss = 0.09361019\n",
      "Iteration 62, loss = 0.16189776\n",
      "Iteration 171, loss = 0.09336565\n",
      "Iteration 231, loss = 0.15983857\n",
      "Iteration 63, loss = 0.16031906\n",
      "Iteration 172, loss = 0.09309292\n",
      "Iteration 169, loss = 0.19364123\n",
      "Iteration 105, loss = 0.12565332\n",
      "Iteration 173, loss = 0.09285702Iteration 64, loss = 0.15879352\n",
      "\n",
      "Iteration 174, loss = 0.09257997\n",
      "Iteration 275, loss = 0.14699382\n",
      "Iteration 306, loss = 0.21353023\n",
      "Iteration 106, loss = 0.12503091\n",
      "Iteration 175, loss = 0.09234491\n",
      "Iteration 65, loss = 0.15729386\n",
      "Iteration 176, loss = 0.09208506\n",
      "Iteration 66, loss = 0.15583242\n",
      "Iteration 78, loss = 0.28947230\n",
      "Iteration 232, loss = 0.15964372\n",
      "Iteration 107, loss = 0.12439344\n",
      "Iteration 177, loss = 0.09183385\n",
      "Iteration 108, loss = 0.12378840\n",
      "Iteration 109, loss = 0.12317968\n",
      "Iteration 178, loss = 0.09158898\n",
      "Iteration 67, loss = 0.15441044\n",
      "Iteration 110, loss = 0.12263994\n",
      "Iteration 179, loss = 0.09135256\n",
      "Iteration 68, loss = 0.15304148\n",
      "Iteration 111, loss = 0.12202406\n",
      "Iteration 233, loss = 0.15946479\n",
      "Iteration 69, loss = 0.15172472\n",
      "Iteration 170, loss = 0.19339570\n",
      "Iteration 180, loss = 0.09110976\n",
      "Iteration 181, loss = 0.09087460\n",
      "Iteration 182, loss = 0.09062892\n",
      "Iteration 112, loss = 0.12145484\n",
      "Iteration 276, loss = 0.14690326\n",
      "Iteration 307, loss = 0.21348498\n",
      "Iteration 113, loss = 0.12089761\n",
      "Iteration 183, loss = 0.09040095\n",
      "Iteration 70, loss = 0.15043846\n",
      "Iteration 184, loss = 0.09016323\n",
      "Iteration 71, loss = 0.14915207\n",
      "Iteration 79, loss = 0.28781970\n",
      "Iteration 185, loss = 0.08994065\n",
      "Iteration 234, loss = 0.15934315\n",
      "Iteration 186, loss = 0.08971185\n",
      "Iteration 114, loss = 0.12035769\n",
      "Iteration 187, loss = 0.08947864\n",
      "Iteration 115, loss = 0.11982713\n",
      "Iteration 171, loss = 0.19320348\n",
      "Iteration 188, loss = 0.08924751\n",
      "Iteration 308, loss = 0.21338588\n",
      "Iteration 277, loss = 0.14677951\n",
      "Iteration 72, loss = 0.14795804\n",
      "Iteration 189, loss = 0.08903457\n",
      "Iteration 73, loss = 0.14676560\n",
      "Iteration 116, loss = 0.11929101\n",
      "Iteration 117, loss = 0.11876938\n",
      "Iteration 74, loss = 0.14560040\n",
      "Iteration 118, loss = 0.11826821\n",
      "Iteration 119, loss = 0.11775783\n",
      "Iteration 120, loss = 0.11726172Iteration 190, loss = 0.08881676\n",
      "\n",
      "Iteration 191, loss = 0.08858872\n",
      "Iteration 172, loss = 0.19298788\n",
      "Iteration 121, loss = 0.11676526\n",
      "Iteration 75, loss = 0.14448862\n",
      "Iteration 235, loss = 0.15914148\n",
      "Iteration 192, loss = 0.08837768\n",
      "Iteration 122, loss = 0.11628412\n",
      "Iteration 278, loss = 0.14667264\n",
      "Iteration 193, loss = 0.08815655\n",
      "Iteration 194, loss = 0.08793983\n",
      "Iteration 195, loss = 0.08772605\n",
      "Iteration 80, loss = 0.28614756\n",
      "Iteration 76, loss = 0.14339778\n",
      "Iteration 196, loss = 0.08751795\n",
      "Iteration 279, loss = 0.14659618\n",
      "Iteration 77, loss = 0.14232816\n",
      "Iteration 236, loss = 0.15897979\n",
      "Iteration 309, loss = 0.21336384\n",
      "Iteration 197, loss = 0.08729729\n",
      "Iteration 123, loss = 0.11581760\n",
      "Iteration 124, loss = 0.11534104\n",
      "Iteration 125, loss = 0.11489346Iteration 78, loss = 0.14132840\n",
      "Iteration 237, loss = 0.15881560\n",
      "Iteration 198, loss = 0.08711122\n",
      "\n",
      "Iteration 199, loss = 0.08688295\n",
      "Iteration 200, loss = 0.08669806\n",
      "Iteration 79, loss = 0.14030191\n",
      "Iteration 173, loss = 0.19278311\n",
      "Iteration 201, loss = 0.08648416\n",
      "Iteration 126, loss = 0.11443009\n",
      "Iteration 238, loss = 0.15868193\n",
      "Iteration 280, loss = 0.14646186\n",
      "Iteration 127, loss = 0.11398297\n",
      "Iteration 80, loss = 0.13929913\n",
      "Iteration 239, loss = 0.15853028\n",
      "Iteration 81, loss = 0.28461132\n",
      "Iteration 202, loss = 0.08628418\n",
      "Iteration 81, loss = 0.13835688\n",
      "Iteration 128, loss = 0.11354572\n",
      "Iteration 240, loss = 0.15837578\n",
      "Iteration 203, loss = 0.08607586\n",
      "Iteration 174, loss = 0.19255178\n",
      "Iteration 129, loss = 0.11310907\n",
      "Iteration 310, loss = 0.21331739\n",
      "Iteration 204, loss = 0.08588768\n",
      "Iteration 82, loss = 0.13744226\n",
      "Iteration 205, loss = 0.08569039\n",
      "Iteration 206, loss = 0.08549011\n",
      "Iteration 83, loss = 0.13651098\n",
      "Iteration 130, loss = 0.11269633\n",
      "Iteration 207, loss = 0.08529326\n",
      "Iteration 208, loss = 0.08511318\n",
      "Iteration 131, loss = 0.11226297\n",
      "Iteration 209, loss = 0.08491231\n",
      "Iteration 281, loss = 0.14638547\n",
      "Iteration 84, loss = 0.13566058\n",
      "Iteration 210, loss = 0.08474246\n",
      "Iteration 211, loss = 0.08454394\n",
      "Iteration 212, loss = 0.08435075\n",
      "Iteration 132, loss = 0.11183947\n",
      "Iteration 213, loss = 0.08419864\n",
      "Iteration 214, loss = 0.08400338\n",
      "Iteration 133, loss = 0.11143894\n",
      "Iteration 85, loss = 0.13477134\n",
      "Iteration 82, loss = 0.28302865\n",
      "Iteration 175, loss = 0.19234211\n",
      "Iteration 134, loss = 0.11103010\n",
      "Iteration 241, loss = 0.15820969\n",
      "Iteration 86, loss = 0.13392279\n",
      "Iteration 215, loss = 0.08381464\n",
      "Iteration 282, loss = 0.14626059\n",
      "Iteration 87, loss = 0.13312506\n",
      "Iteration 135, loss = 0.11063289\n",
      "Iteration 216, loss = 0.08363395\n",
      "Iteration 311, loss = 0.21319314\n",
      "Iteration 136, loss = 0.11022581\n",
      "Iteration 88, loss = 0.13231414\n",
      "Iteration 137, loss = 0.10985733\n",
      "Iteration 217, loss = 0.08345425\n",
      "Iteration 138, loss = 0.10946237\n",
      "Iteration 89, loss = 0.13153061\n",
      "Iteration 139, loss = 0.10907654\n",
      "Iteration 90, loss = 0.13075260\n",
      "Iteration 91, loss = 0.12999414\n",
      "Iteration 218, loss = 0.08328148\n",
      "Iteration 219, loss = 0.08310456\n",
      "Iteration 242, loss = 0.15806914\n",
      "Iteration 220, loss = 0.08293046\n",
      "Iteration 176, loss = 0.19211164\n",
      "Iteration 283, loss = 0.14615830\n",
      "Iteration 92, loss = 0.12927212\n",
      "Iteration 83, loss = 0.28160160\n",
      "Iteration 140, loss = 0.10870398\n",
      "Iteration 221, loss = 0.08276406\n",
      "Iteration 312, loss = 0.21311958\n",
      "Iteration 141, loss = 0.10833379\n",
      "Iteration 243, loss = 0.15792346\n",
      "Iteration 222, loss = 0.08259377\n",
      "Iteration 223, loss = 0.08242625\n",
      "Iteration 224, loss = 0.08225472\n",
      "Iteration 225, loss = 0.08209082\n",
      "Iteration 142, loss = 0.10796422\n",
      "Iteration 143, loss = 0.10762479\n",
      "Iteration 226, loss = 0.08192040\n",
      "Iteration 227, loss = 0.08175603\n",
      "Iteration 144, loss = 0.10724456\n",
      "Iteration 228, loss = 0.08159445\n",
      "Iteration 145, loss = 0.10691502\n",
      "Iteration 93, loss = 0.12854432\n",
      "Iteration 229, loss = 0.08144413\n",
      "Iteration 244, loss = 0.15776146\n",
      "Iteration 94, loss = 0.12783635\n",
      "Iteration 146, loss = 0.10656517\n",
      "Iteration 147, loss = 0.10621264\n",
      "Iteration 177, loss = 0.19191267\n",
      "Iteration 230, loss = 0.08128148\n",
      "Iteration 284, loss = 0.14605148\n",
      "Iteration 231, loss = 0.08111525\n",
      "Iteration 84, loss = 0.28014612\n",
      "Iteration 95, loss = 0.12713284\n",
      "Iteration 232, loss = 0.08096885\n",
      "Iteration 96, loss = 0.12647080\n",
      "Iteration 233, loss = 0.08081455\n",
      "Iteration 245, loss = 0.15762296\n",
      "Iteration 97, loss = 0.12580951\n",
      "Iteration 313, loss = 0.21304181\n",
      "Iteration 148, loss = 0.10587734\n",
      "Iteration 285, loss = 0.14596628\n",
      "Iteration 234, loss = 0.08065727Iteration 98, loss = 0.12518431\n",
      "\n",
      "Iteration 149, loss = 0.10553311\n",
      "Iteration 150, loss = 0.10521285\n",
      "Iteration 235, loss = 0.08050216\n",
      "Iteration 99, loss = 0.12453029\n",
      "Iteration 236, loss = 0.08036759\n",
      "Iteration 237, loss = 0.08021253\n",
      "Iteration 238, loss = 0.08005273\n",
      "Iteration 100, loss = 0.12391564\n",
      "Iteration 151, loss = 0.10487920\n",
      "Iteration 178, loss = 0.19170911\n",
      "Iteration 239, loss = 0.07992053\n",
      "Iteration 152, loss = 0.10456013\n",
      "Iteration 240, loss = 0.07976935\n",
      "Iteration 153, loss = 0.10425013\n",
      "Iteration 241, loss = 0.07961300\n",
      "Iteration 242, loss = 0.07946599\n",
      "Iteration 246, loss = 0.15746352\n",
      "Iteration 243, loss = 0.07933063\n",
      "Iteration 154, loss = 0.10394661\n",
      "Iteration 155, loss = 0.10361910\n",
      "Iteration 101, loss = 0.12330387\n",
      "Iteration 102, loss = 0.12271569\n",
      "Iteration 85, loss = 0.27879240\n",
      "Iteration 244, loss = 0.07917284\n",
      "Iteration 103, loss = 0.12212091\n",
      "Iteration 245, loss = 0.07904960\n",
      "Iteration 286, loss = 0.14586265\n",
      "Iteration 314, loss = 0.21297561\n",
      "Iteration 156, loss = 0.10331639\n",
      "Iteration 157, loss = 0.10301974\n",
      "Iteration 246, loss = 0.07889301\n",
      "Iteration 158, loss = 0.10270955\n",
      "Iteration 104, loss = 0.12155636\n",
      "Iteration 247, loss = 0.15734496\n",
      "Iteration 287, loss = 0.14577789\n",
      "Iteration 248, loss = 0.15720375\n",
      "Iteration 247, loss = 0.07874958\n",
      "Iteration 248, loss = 0.07861203\n",
      "Iteration 179, loss = 0.19153691\n",
      "Iteration 249, loss = 0.07847977\n",
      "Iteration 105, loss = 0.12103167\n",
      "Iteration 250, loss = 0.07833483\n",
      "Iteration 106, loss = 0.12045459\n",
      "Iteration 107, loss = 0.11991114\n",
      "Iteration 159, loss = 0.10240960\n",
      "Iteration 160, loss = 0.10212384\n",
      "Iteration 251, loss = 0.07819792\n",
      "Iteration 161, loss = 0.10183759\n",
      "Iteration 252, loss = 0.07807288\n",
      "Iteration 162, loss = 0.10153226\n",
      "Iteration 253, loss = 0.07792088\n",
      "Iteration 86, loss = 0.27749597\n",
      "Iteration 254, loss = 0.07778650\n",
      "Iteration 108, loss = 0.11938565\n",
      "Iteration 163, loss = 0.10126124\n",
      "Iteration 255, loss = 0.07766635\n",
      "Iteration 109, loss = 0.11886228\n",
      "Iteration 164, loss = 0.10097753\n",
      "Iteration 165, loss = 0.10070800\n",
      "Iteration 256, loss = 0.07751733\n",
      "Iteration 110, loss = 0.11835800Iteration 166, loss = 0.10042977\n",
      "Iteration 257, loss = 0.07740325\n",
      "\n",
      "Iteration 167, loss = 0.10015396\n",
      "Iteration 249, loss = 0.15702564\n",
      "Iteration 288, loss = 0.14565940\n",
      "Iteration 168, loss = 0.09988885\n",
      "Iteration 180, loss = 0.19131691\n",
      "Iteration 111, loss = 0.11785701\n",
      "Iteration 112, loss = 0.11736703\n",
      "Iteration 250, loss = 0.15696285\n",
      "Iteration 258, loss = 0.07726280\n",
      "Iteration 315, loss = 0.21294837\n",
      "Iteration 259, loss = 0.07712469\n",
      "Iteration 113, loss = 0.11689209\n",
      "Iteration 289, loss = 0.14558220\n",
      "Iteration 260, loss = 0.07700156\n",
      "Iteration 261, loss = 0.07686337\n",
      "Iteration 262, loss = 0.07673036\n",
      "Iteration 114, loss = 0.11641006\n",
      "Iteration 115, loss = 0.11593245\n",
      "Iteration 251, loss = 0.15675131\n",
      "Iteration 169, loss = 0.09962919\n",
      "Iteration 263, loss = 0.07662043\n",
      "Iteration 290, loss = 0.14547596\n",
      "Iteration 264, loss = 0.07649420\n",
      "Iteration 265, loss = 0.07636162\n",
      "Iteration 266, loss = 0.07623771\n",
      "Iteration 170, loss = 0.09934920\n",
      "Iteration 267, loss = 0.07610228\n",
      "Iteration 171, loss = 0.09909361\n",
      "Iteration 116, loss = 0.11546830\n",
      "Iteration 316, loss = 0.21289737\n",
      "Iteration 87, loss = 0.27615044\n",
      "Iteration 172, loss = 0.09884356\n",
      "Iteration 181, loss = 0.19113766\n",
      "Iteration 252, loss = 0.15662390\n",
      "Iteration 268, loss = 0.07598631\n",
      "Iteration 173, loss = 0.09857937\n",
      "Iteration 291, loss = 0.14537817\n",
      "Iteration 174, loss = 0.09832692\n",
      "Iteration 269, loss = 0.07586237\n",
      "Iteration 270, loss = 0.07574221\n",
      "Iteration 175, loss = 0.09808531\n",
      "Iteration 271, loss = 0.07561449\n",
      "Iteration 253, loss = 0.15646750\n",
      "Iteration 272, loss = 0.07549804\n",
      "Iteration 176, loss = 0.09782523\n",
      "Iteration 117, loss = 0.11501450\n",
      "Iteration 273, loss = 0.07539358\n",
      "Iteration 274, loss = 0.07525490\n",
      "Iteration 177, loss = 0.09758920\n",
      "Iteration 317, loss = 0.21288909\n",
      "Iteration 118, loss = 0.11456883\n",
      "Iteration 275, loss = 0.07513080\n",
      "Iteration 119, loss = 0.11413221\n",
      "Iteration 178, loss = 0.09734085\n",
      "Iteration 276, loss = 0.07501854\n",
      "Iteration 88, loss = 0.27491657\n",
      "Iteration 182, loss = 0.19091349\n",
      "Iteration 277, loss = 0.07490766\n",
      "Iteration 179, loss = 0.09710177\n",
      "Iteration 254, loss = 0.15635455\n",
      "Iteration 120, loss = 0.11371759\n",
      "Iteration 292, loss = 0.14528514\n",
      "Iteration 278, loss = 0.07480637\n",
      "Iteration 279, loss = 0.07467225\n",
      "Iteration 255, loss = 0.15620778\n",
      "Iteration 121, loss = 0.11327840\n",
      "Iteration 280, loss = 0.07454889\n",
      "Iteration 281, loss = 0.07443904\n",
      "Iteration 282, loss = 0.07433713\n",
      "Iteration 283, loss = 0.07421843\n",
      "Iteration 284, loss = 0.07409088\n",
      "Iteration 285, loss = 0.07398619\n",
      "Iteration 180, loss = 0.09685538\n",
      "Iteration 181, loss = 0.09661848\n",
      "Iteration 122, loss = 0.11286812\n",
      "Iteration 286, loss = 0.07387839\n",
      "Iteration 89, loss = 0.27370232\n",
      "Iteration 123, loss = 0.11244093\n",
      "Iteration 293, loss = 0.14520582\n",
      "Iteration 287, loss = 0.07376406\n",
      "Iteration 182, loss = 0.09639499\n",
      "Iteration 124, loss = 0.11204276\n",
      "Iteration 183, loss = 0.09616072\n",
      "Iteration 125, loss = 0.11164643\n",
      "Iteration 184, loss = 0.09593465\n",
      "Iteration 183, loss = 0.19070445\n",
      "Iteration 126, loss = 0.11125004\n",
      "Iteration 185, loss = 0.09570447\n",
      "Iteration 186, loss = 0.09548797\n",
      "Iteration 288, loss = 0.07365824\n",
      "Iteration 256, loss = 0.15607113\n",
      "Iteration 318, loss = 0.21272680\n",
      "Iteration 289, loss = 0.07354086\n",
      "Iteration 187, loss = 0.09526309\n",
      "Iteration 290, loss = 0.07344967\n",
      "Iteration 294, loss = 0.14510960\n",
      "Iteration 127, loss = 0.11085563\n",
      "Iteration 188, loss = 0.09503080\n",
      "Iteration 189, loss = 0.09481987\n",
      "Iteration 90, loss = 0.27253830\n",
      "Iteration 190, loss = 0.09461283\n",
      "Iteration 291, loss = 0.07332632\n",
      "Iteration 128, loss = 0.11047675\n",
      "Iteration 257, loss = 0.15596736\n",
      "Iteration 292, loss = 0.07322128\n",
      "Iteration 293, loss = 0.07311372\n",
      "Iteration 294, loss = 0.07301238\n",
      "Iteration 295, loss = 0.07290031\n",
      "Iteration 191, loss = 0.09439742\n",
      "Iteration 296, loss = 0.07280603\n",
      "Iteration 192, loss = 0.09419399\n",
      "Iteration 184, loss = 0.19051406\n",
      "Iteration 193, loss = 0.09397426\n",
      "Iteration 194, loss = 0.09376167\n",
      "Iteration 258, loss = 0.15586015\n",
      "Iteration 195, loss = 0.09356350\n",
      "Iteration 295, loss = 0.14501167\n",
      "Iteration 297, loss = 0.07270475\n",
      "Iteration 298, loss = 0.07259159\n",
      "Iteration 319, loss = 0.21264857\n",
      "Iteration 299, loss = 0.07248438\n",
      "Iteration 129, loss = 0.11010495\n",
      "Iteration 300, loss = 0.07238235\n",
      "Iteration 301, loss = 0.07227520\n",
      "Iteration 302, loss = 0.07218784\n",
      "Iteration 303, loss = 0.07207268\n",
      "Iteration 196, loss = 0.09336528\n",
      "Iteration 197, loss = 0.09316151\n",
      "Iteration 198, loss = 0.09295716\n",
      "Iteration 130, loss = 0.10973075\n",
      "Iteration 131, loss = 0.10936940\n",
      "Iteration 185, loss = 0.19033533Iteration 304, loss = 0.07196877\n",
      "\n",
      "Iteration 296, loss = 0.14492531\n",
      "Iteration 132, loss = 0.10901035\n",
      "Iteration 199, loss = 0.09277054\n",
      "Iteration 305, loss = 0.07187655\n",
      "Iteration 320, loss = 0.21263060\n",
      "Iteration 91, loss = 0.27139080\n",
      "Iteration 306, loss = 0.07177237\n",
      "Iteration 259, loss = 0.15567262\n",
      "Iteration 200, loss = 0.09255898\n",
      "Iteration 133, loss = 0.10865718\n",
      "Iteration 201, loss = 0.09235900\n",
      "Iteration 307, loss = 0.07166935\n",
      "Iteration 308, loss = 0.07156763\n",
      "Iteration 309, loss = 0.07147541\n",
      "Iteration 202, loss = 0.09217399\n",
      "Iteration 134, loss = 0.10831044\n",
      "Iteration 310, loss = 0.07136445\n",
      "Iteration 297, loss = 0.14483507\n",
      "Iteration 135, loss = 0.10796461\n",
      "Iteration 260, loss = 0.15556657\n",
      "Iteration 136, loss = 0.10762331\n",
      "Iteration 311, loss = 0.07127537\n",
      "Iteration 203, loss = 0.09197394\n",
      "Iteration 312, loss = 0.07117865\n",
      "Iteration 186, loss = 0.19019034\n",
      "Iteration 313, loss = 0.07107405\n",
      "Iteration 204, loss = 0.09179011\n",
      "Iteration 205, loss = 0.09160478\n",
      "Iteration 314, loss = 0.07098056\n",
      "Iteration 92, loss = 0.27032902\n",
      "Iteration 137, loss = 0.10728872\n",
      "Iteration 206, loss = 0.09141705\n",
      "Iteration 138, loss = 0.10695144\n",
      "Iteration 315, loss = 0.07088388\n",
      "Iteration 316, loss = 0.07079780\n",
      "Iteration 139, loss = 0.10663098\n",
      "Iteration 207, loss = 0.09121799\n",
      "Iteration 317, loss = 0.07069113\n",
      "Iteration 318, loss = 0.07059225\n",
      "Iteration 321, loss = 0.21258561\n",
      "Iteration 140, loss = 0.10629858\n",
      "Iteration 319, loss = 0.07049813\n",
      "Iteration 261, loss = 0.15544155\n",
      "Iteration 320, loss = 0.07040617\n",
      "Iteration 321, loss = 0.07031542\n",
      "Iteration 298, loss = 0.14475959\n",
      "Iteration 322, loss = 0.07022716\n",
      "Iteration 187, loss = 0.19000542\n",
      "Iteration 208, loss = 0.09104009\n",
      "Iteration 262, loss = 0.15532001\n",
      "Iteration 323, loss = 0.07011975\n",
      "Iteration 324, loss = 0.07004141\n",
      "Iteration 325, loss = 0.06993201\n",
      "Iteration 141, loss = 0.10598901\n",
      "Iteration 326, loss = 0.06984339\n",
      "Iteration 209, loss = 0.09085928\n",
      "Iteration 210, loss = 0.09067309\n",
      "Iteration 211, loss = 0.09049708\n",
      "Iteration 212, loss = 0.09034017\n",
      "Iteration 263, loss = 0.15519043\n",
      "Iteration 213, loss = 0.09013291\n",
      "Iteration 299, loss = 0.14465223\n",
      "Iteration 214, loss = 0.08996362\n",
      "Iteration 142, loss = 0.10567656\n",
      "Iteration 215, loss = 0.08979508\n",
      "Iteration 327, loss = 0.06975021\n",
      "Iteration 216, loss = 0.08962268\n",
      "Iteration 143, loss = 0.10536120\n",
      "Iteration 217, loss = 0.08945193\n",
      "Iteration 218, loss = 0.08928097\n",
      "Iteration 144, loss = 0.10503603\n",
      "Iteration 219, loss = 0.08910289\n",
      "Iteration 322, loss = 0.21248048\n",
      "Iteration 145, loss = 0.10473165\n",
      "Iteration 220, loss = 0.08893617\n",
      "Iteration 93, loss = 0.26925660\n",
      "Iteration 221, loss = 0.08878622\n",
      "Iteration 146, loss = 0.10442805\n",
      "Iteration 328, loss = 0.06966790\n",
      "Iteration 329, loss = 0.06956344\n",
      "Iteration 330, loss = 0.06947415\n",
      "Iteration 222, loss = 0.08860393\n",
      "Iteration 331, loss = 0.06939047\n",
      "Iteration 332, loss = 0.06929453\n",
      "Iteration 300, loss = 0.14456783\n",
      "Iteration 333, loss = 0.06921044\n",
      "Iteration 264, loss = 0.15504473\n",
      "Iteration 334, loss = 0.06911284\n",
      "Iteration 188, loss = 0.18978950\n",
      "Iteration 335, loss = 0.06902564\n",
      "Iteration 336, loss = 0.06893650\n",
      "Iteration 337, loss = 0.06885409\n",
      "Iteration 338, loss = 0.06875644\n",
      "Iteration 147, loss = 0.10414286\n",
      "Iteration 323, loss = 0.21247110\n",
      "Iteration 94, loss = 0.26821627\n",
      "Iteration 301, loss = 0.14449265\n",
      "Iteration 148, loss = 0.10383973\n",
      "Iteration 339, loss = 0.06866857\n",
      "Iteration 324, loss = 0.21233875\n",
      "Iteration 223, loss = 0.08843317\n",
      "Iteration 189, loss = 0.18959848\n",
      "Iteration 265, loss = 0.15491772\n",
      "Iteration 149, loss = 0.10355607\n",
      "Iteration 224, loss = 0.08828084\n",
      "Iteration 340, loss = 0.06858835\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 225, loss = 0.08810404\n",
      "Iteration 150, loss = 0.10327016\n",
      "Iteration 1, loss = 0.68894537\n",
      "Iteration 302, loss = 0.14439136\n",
      "Iteration 2, loss = 0.66721192\n",
      "Iteration 266, loss = 0.15480886\n",
      "Iteration 151, loss = 0.10298371\n",
      "Iteration 3, loss = 0.63786126\n",
      "Iteration 226, loss = 0.08795129\n",
      "Iteration 4, loss = 0.60723349\n",
      "Iteration 227, loss = 0.08778108\n",
      "Iteration 5, loss = 0.57885064\n",
      "Iteration 6, loss = 0.55300786\n",
      "Iteration 152, loss = 0.10272078\n",
      "Iteration 228, loss = 0.08762409\n",
      "Iteration 95, loss = 0.26723511\n",
      "Iteration 153, loss = 0.10243232\n",
      "Iteration 154, loss = 0.10217438\n",
      "Iteration 7, loss = 0.53022320\n",
      "Iteration 155, loss = 0.10188824\n",
      "Iteration 229, loss = 0.08749321\n",
      "Iteration 8, loss = 0.51013852\n",
      "Iteration 267, loss = 0.15468483\n",
      "Iteration 9, loss = 0.49217147\n",
      "Iteration 10, loss = 0.47644533\n",
      "Iteration 303, loss = 0.14432386\n",
      "Iteration 11, loss = 0.46280511\n",
      "Iteration 12, loss = 0.45000350\n",
      "Iteration 156, loss = 0.10164233\n",
      "Iteration 190, loss = 0.18943656\n",
      "Iteration 304, loss = 0.14422636\n",
      "Iteration 230, loss = 0.08731650\n",
      "Iteration 157, loss = 0.10137615\n",
      "Iteration 268, loss = 0.15457213\n",
      "Iteration 158, loss = 0.10112441\n",
      "Iteration 231, loss = 0.08717368\n",
      "Iteration 232, loss = 0.08701205\n",
      "Iteration 13, loss = 0.43908088\n",
      "Iteration 233, loss = 0.08684913\n",
      "Iteration 234, loss = 0.08670960\n",
      "Iteration 325, loss = 0.21227384\n",
      "Iteration 14, loss = 0.42885185\n",
      "Iteration 15, loss = 0.41958269\n",
      "Iteration 16, loss = 0.41099844\n",
      "Iteration 96, loss = 0.26625476\n",
      "Iteration 269, loss = 0.15445423\n",
      "Iteration 159, loss = 0.10087859\n",
      "Iteration 305, loss = 0.14415023\n",
      "Iteration 17, loss = 0.40318464\n",
      "Iteration 18, loss = 0.39578669\n",
      "Iteration 160, loss = 0.10061399\n",
      "Iteration 235, loss = 0.08654948\n",
      "Iteration 191, loss = 0.18927655\n",
      "Iteration 236, loss = 0.08639890\n",
      "Iteration 237, loss = 0.08624854\n",
      "Iteration 238, loss = 0.08610453\n",
      "Iteration 19, loss = 0.38890001\n",
      "Iteration 161, loss = 0.10035827\n",
      "Iteration 20, loss = 0.38255181\n",
      "Iteration 270, loss = 0.15434203\n",
      "Iteration 239, loss = 0.08595251\n",
      "Iteration 21, loss = 0.37650127\n",
      "Iteration 162, loss = 0.10011278\n",
      "Iteration 22, loss = 0.37078080\n",
      "Iteration 326, loss = 0.21220336\n",
      "Iteration 240, loss = 0.08582046\n",
      "Iteration 23, loss = 0.36528040\n",
      "Iteration 24, loss = 0.36017715\n",
      "Iteration 306, loss = 0.14404354\n",
      "Iteration 241, loss = 0.08567339\n",
      "Iteration 163, loss = 0.09986565\n",
      "Iteration 97, loss = 0.26528160\n",
      "Iteration 164, loss = 0.09963339\n",
      "Iteration 271, loss = 0.15418830\n",
      "Iteration 165, loss = 0.09937815\n",
      "Iteration 242, loss = 0.08552292\n",
      "Iteration 166, loss = 0.09915502\n",
      "Iteration 25, loss = 0.35515453\n",
      "Iteration 192, loss = 0.18909885\n",
      "Iteration 26, loss = 0.35027074\n",
      "Iteration 27, loss = 0.34567389\n",
      "Iteration 28, loss = 0.34122877\n",
      "Iteration 29, loss = 0.33682293\n",
      "Iteration 243, loss = 0.08538252\n",
      "Iteration 30, loss = 0.33249670\n",
      "Iteration 167, loss = 0.09894375\n",
      "Iteration 168, loss = 0.09868533\n",
      "Iteration 244, loss = 0.08523692\n",
      "Iteration 272, loss = 0.15406210\n",
      "Iteration 245, loss = 0.08510014\n",
      "Iteration 31, loss = 0.32833275\n",
      "Iteration 246, loss = 0.08495916\n",
      "Iteration 247, loss = 0.08482278\n",
      "Iteration 32, loss = 0.32421265\n",
      "Iteration 33, loss = 0.32028330\n",
      "Iteration 248, loss = 0.08467411\n",
      "Iteration 98, loss = 0.26450490\n",
      "Iteration 193, loss = 0.18890974\n",
      "Iteration 34, loss = 0.31629327\n",
      "Iteration 169, loss = 0.09845516\n",
      "Iteration 307, loss = 0.14397267\n",
      "Iteration 327, loss = 0.21214213\n",
      "Iteration 35, loss = 0.31225010\n",
      "Iteration 249, loss = 0.08454055\n",
      "Iteration 250, loss = 0.08439965\n",
      "Iteration 36, loss = 0.30841597\n",
      "Iteration 251, loss = 0.08425945\n",
      "Iteration 37, loss = 0.30463320\n",
      "Iteration 170, loss = 0.09822642\n",
      "Iteration 273, loss = 0.15401000\n",
      "Iteration 38, loss = 0.30085486\n",
      "Iteration 39, loss = 0.29700087\n",
      "Iteration 308, loss = 0.14388323\n",
      "Iteration 40, loss = 0.29322011\n",
      "Iteration 41, loss = 0.28951743\n",
      "Iteration 252, loss = 0.08412777\n",
      "Iteration 171, loss = 0.09800424\n",
      "Iteration 253, loss = 0.08399450\n",
      "Iteration 274, loss = 0.15384865\n",
      "Iteration 172, loss = 0.09777691\n",
      "Iteration 99, loss = 0.26342898\n",
      "Iteration 173, loss = 0.09755308\n",
      "Iteration 42, loss = 0.28582850\n",
      "Iteration 328, loss = 0.21209524\n",
      "Iteration 174, loss = 0.09733824\n",
      "Iteration 43, loss = 0.28216127\n",
      "Iteration 254, loss = 0.08385266\n",
      "Iteration 44, loss = 0.27850407\n",
      "Iteration 45, loss = 0.27488415\n",
      "Iteration 194, loss = 0.18872628\n",
      "Iteration 309, loss = 0.14381063\n",
      "Iteration 255, loss = 0.08371874\n",
      "Iteration 46, loss = 0.27139996\n",
      "Iteration 275, loss = 0.15374079\n",
      "Iteration 47, loss = 0.26789521\n",
      "Iteration 175, loss = 0.09712281\n",
      "Iteration 256, loss = 0.08358314\n",
      "Iteration 48, loss = 0.26442223\n",
      "Iteration 49, loss = 0.26087265\n",
      "Iteration 257, loss = 0.08346020\n",
      "Iteration 50, loss = 0.25757021\n",
      "Iteration 176, loss = 0.09690806\n",
      "Iteration 51, loss = 0.25416827\n",
      "Iteration 177, loss = 0.09670561\n",
      "Iteration 258, loss = 0.08331887\n",
      "Iteration 178, loss = 0.09650798\n",
      "Iteration 259, loss = 0.08320171\n",
      "Iteration 276, loss = 0.15361389\n",
      "Iteration 52, loss = 0.25089136\n",
      "Iteration 260, loss = 0.08307008\n",
      "Iteration 329, loss = 0.21202686\n",
      "Iteration 53, loss = 0.24765678\n",
      "Iteration 179, loss = 0.09628649\n",
      "Iteration 195, loss = 0.18856902\n",
      "Iteration 54, loss = 0.24448922\n",
      "Iteration 55, loss = 0.24135243\n",
      "Iteration 310, loss = 0.14374770\n",
      "Iteration 261, loss = 0.08293512\n",
      "Iteration 56, loss = 0.23814712\n",
      "Iteration 180, loss = 0.09608066\n",
      "Iteration 100, loss = 0.26256647\n",
      "Iteration 57, loss = 0.23511723\n",
      "Iteration 262, loss = 0.08281728\n",
      "Iteration 181, loss = 0.09589627\n",
      "Iteration 311, loss = 0.14364206\n",
      "Iteration 263, loss = 0.08268442\n",
      "Iteration 264, loss = 0.08256253\n",
      "Iteration 58, loss = 0.23209487\n",
      "Iteration 312, loss = 0.14356747\n",
      "Iteration 277, loss = 0.15349955\n",
      "Iteration 265, loss = 0.08243364\n",
      "Iteration 196, loss = 0.18839790\n",
      "Iteration 59, loss = 0.22906165\n",
      "Iteration 182, loss = 0.09569534\n",
      "Iteration 183, loss = 0.09549598\n",
      "Iteration 184, loss = 0.09529736\n",
      "Iteration 278, loss = 0.15338366\n",
      "Iteration 185, loss = 0.09511680\n",
      "Iteration 60, loss = 0.22609295\n",
      "Iteration 61, loss = 0.22314626\n",
      "Iteration 62, loss = 0.22026195\n",
      "Iteration 266, loss = 0.08230679\n",
      "Iteration 101, loss = 0.26175453\n",
      "Iteration 63, loss = 0.21745328Iteration 267, loss = 0.08219161\n",
      "\n",
      "Iteration 268, loss = 0.08206203\n",
      "Iteration 269, loss = 0.08195863\n",
      "Iteration 330, loss = 0.21198391\n",
      "Iteration 270, loss = 0.08182601\n",
      "Iteration 186, loss = 0.09491620\n",
      "Iteration 279, loss = 0.15329869\n",
      "Iteration 64, loss = 0.21466454\n",
      "Iteration 65, loss = 0.21197386\n",
      "Iteration 187, loss = 0.09473353\n",
      "Iteration 313, loss = 0.14347887\n",
      "Iteration 66, loss = 0.20928398\n",
      "Iteration 188, loss = 0.09455746\n",
      "Iteration 67, loss = 0.20666468\n",
      "Iteration 331, loss = 0.21193459\n",
      "Iteration 68, loss = 0.20422422\n",
      "Iteration 280, loss = 0.15315601\n",
      "Iteration 69, loss = 0.20154926\n",
      "Iteration 189, loss = 0.09435860\n",
      "Iteration 271, loss = 0.08170494\n",
      "Iteration 102, loss = 0.26091944\n",
      "Iteration 272, loss = 0.08160086\n",
      "Iteration 70, loss = 0.19914171\n",
      "Iteration 197, loss = 0.18822704\n",
      "Iteration 273, loss = 0.08147085\n",
      "Iteration 274, loss = 0.08136189\n",
      "Iteration 275, loss = 0.08123537\n",
      "Iteration 276, loss = 0.08113246\n",
      "Iteration 277, loss = 0.08101007\n",
      "Iteration 190, loss = 0.09418353\n",
      "Iteration 314, loss = 0.14339807\n",
      "Iteration 71, loss = 0.19679023\n",
      "Iteration 72, loss = 0.19439218\n",
      "Iteration 198, loss = 0.18806832\n",
      "Iteration 281, loss = 0.15304441\n",
      "Iteration 73, loss = 0.19214485\n",
      "Iteration 191, loss = 0.09400224\n",
      "Iteration 278, loss = 0.08089234\n",
      "Iteration 74, loss = 0.18997196\n",
      "Iteration 75, loss = 0.18783207\n",
      "Iteration 192, loss = 0.09382444\n",
      "Iteration 279, loss = 0.08078332\n",
      "Iteration 193, loss = 0.09365427\n",
      "Iteration 76, loss = 0.18575813\n",
      "Iteration 77, loss = 0.18371722\n",
      "Iteration 103, loss = 0.26013785\n",
      "Iteration 78, loss = 0.18169732\n",
      "Iteration 194, loss = 0.09348318\n",
      "Iteration 332, loss = 0.21186438\n",
      "Iteration 280, loss = 0.08067998\n",
      "Iteration 282, loss = 0.15299493\n",
      "Iteration 79, loss = 0.17972508\n",
      "Iteration 195, loss = 0.09330865\n",
      "Iteration 315, loss = 0.14332747\n",
      "Iteration 80, loss = 0.17780034\n",
      "Iteration 281, loss = 0.08055896\n",
      "Iteration 282, loss = 0.08044802\n",
      "Iteration 199, loss = 0.18793584\n",
      "Iteration 283, loss = 0.08033324\n",
      "Iteration 283, loss = 0.15283199\n",
      "Iteration 196, loss = 0.09312888\n",
      "Iteration 81, loss = 0.17594581\n",
      "Iteration 284, loss = 0.08022510\n",
      "Iteration 284, loss = 0.15272644\n",
      "Iteration 82, loss = 0.17416347\n",
      "Iteration 197, loss = 0.09295999\n",
      "Iteration 83, loss = 0.17231159\n",
      "Iteration 84, loss = 0.17055700\n",
      "Iteration 285, loss = 0.08011460\n",
      "Iteration 85, loss = 0.16888059\n",
      "Iteration 316, loss = 0.14325239\n",
      "Iteration 286, loss = 0.08000936\n",
      "Iteration 198, loss = 0.09279864\n",
      "Iteration 333, loss = 0.21177793\n",
      "Iteration 199, loss = 0.09262870\n",
      "Iteration 287, loss = 0.07990969\n",
      "Iteration 86, loss = 0.16719952\n",
      "Iteration 87, loss = 0.16557099\n",
      "Iteration 288, loss = 0.07978994\n",
      "Iteration 88, loss = 0.16392784\n",
      "Iteration 200, loss = 0.18775550\n",
      "Iteration 89, loss = 0.16240209\n",
      "Iteration 90, loss = 0.16088680\n",
      "Iteration 91, loss = 0.15939074\n",
      "Iteration 289, loss = 0.07968508\n",
      "Iteration 92, loss = 0.15788140\n",
      "Iteration 200, loss = 0.09245889\n",
      "Iteration 104, loss = 0.25927368\n",
      "Iteration 93, loss = 0.15650164\n",
      "Iteration 285, loss = 0.15266182\n",
      "Iteration 94, loss = 0.15508891\n",
      "Iteration 290, loss = 0.07957835\n",
      "Iteration 95, loss = 0.15373426\n",
      "Iteration 201, loss = 0.09229281\n",
      "Iteration 291, loss = 0.07947044\n",
      "Iteration 96, loss = 0.15238034\n",
      "Iteration 292, loss = 0.07937373\n",
      "Iteration 201, loss = 0.18756755\n",
      "Iteration 293, loss = 0.07925738\n",
      "Iteration 294, loss = 0.07915493\n",
      "Iteration 286, loss = 0.15253749\n",
      "Iteration 334, loss = 0.21171955\n",
      "Iteration 97, loss = 0.15107619\n",
      "Iteration 202, loss = 0.09213817\n",
      "Iteration 295, loss = 0.07906649\n",
      "Iteration 317, loss = 0.14320533\n",
      "Iteration 98, loss = 0.14981277\n",
      "Iteration 203, loss = 0.09197853\n",
      "Iteration 99, loss = 0.14855702\n",
      "Iteration 100, loss = 0.14729087\n",
      "Iteration 202, loss = 0.18743366\n",
      "Iteration 101, loss = 0.14608586\n",
      "Iteration 102, loss = 0.14491166\n",
      "Iteration 204, loss = 0.09181271\n",
      "Iteration 318, loss = 0.14309028\n",
      "Iteration 103, loss = 0.14381296\n",
      "Iteration 287, loss = 0.15244202\n",
      "Iteration 205, loss = 0.09166397\n",
      "Iteration 104, loss = 0.14266582Iteration 206, loss = 0.09149991\n",
      "Iteration 296, loss = 0.07894448\n",
      "Iteration 288, loss = 0.15233284\n",
      "Iteration 207, loss = 0.09135098\n",
      "\n",
      "Iteration 105, loss = 0.25857384\n",
      "Iteration 297, loss = 0.07884590\n",
      "Iteration 208, loss = 0.09118806\n",
      "Iteration 105, loss = 0.14156637\n",
      "Iteration 298, loss = 0.07875248\n",
      "Iteration 106, loss = 0.14052435\n",
      "Iteration 107, loss = 0.13944465\n",
      "Iteration 335, loss = 0.21165439\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 209, loss = 0.09103751\n",
      "Iteration 319, loss = 0.14300259\n",
      "Iteration 108, loss = 0.13842872\n",
      "Iteration 299, loss = 0.07864443\n",
      "Iteration 106, loss = 0.25784933\n",
      "Iteration 109, loss = 0.13737973\n",
      "Iteration 289, loss = 0.15224462\n",
      "Iteration 320, loss = 0.14292154\n",
      "Iteration 110, loss = 0.13645177\n",
      "Iteration 300, loss = 0.07855005\n",
      "Iteration 111, loss = 0.13546819\n",
      "Iteration 210, loss = 0.09090480\n",
      "Iteration 112, loss = 0.13453750\n",
      "Iteration 203, loss = 0.18725786\n",
      "Iteration 321, loss = 0.14285996\n",
      "Iteration 211, loss = 0.09074960\n",
      "Iteration 113, loss = 0.13365652\n",
      "Iteration 301, loss = 0.07843417\n",
      "Iteration 114, loss = 0.13270845\n",
      "Iteration 115, loss = 0.13184869\n",
      "Iteration 302, loss = 0.07833546\n",
      "Iteration 107, loss = 0.25703268\n",
      "Iteration 116, loss = 0.13097236Iteration 303, loss = 0.07826327\n",
      "\n",
      "Iteration 290, loss = 0.15213037\n",
      "Iteration 212, loss = 0.09059339\n",
      "Iteration 117, loss = 0.13013935\n",
      "Iteration 213, loss = 0.09045426\n",
      "Iteration 118, loss = 0.12931853\n",
      "Iteration 119, loss = 0.12850953\n",
      "Iteration 214, loss = 0.09029706\n",
      "Iteration 120, loss = 0.12769151\n",
      "Iteration 291, loss = 0.15205172\n",
      "Iteration 304, loss = 0.07814954\n",
      "Iteration 121, loss = 0.12692950\n",
      "Iteration 122, loss = 0.12615337\n",
      "Iteration 123, loss = 0.12539826\n",
      "Iteration 305, loss = 0.07804108\n",
      "Iteration 1, loss = 0.87114604\n",
      "Iteration 124, loss = 0.12467751\n",
      "Iteration 306, loss = 0.07794815\n",
      "Iteration 125, loss = 0.12393111\n",
      "Iteration 2, loss = 0.80411033\n",
      "Iteration 307, loss = 0.07784832\n",
      "Iteration 126, loss = 0.12325901\n",
      "Iteration 308, loss = 0.07775262\n",
      "Iteration 3, loss = 0.72246672\n",
      "Iteration 215, loss = 0.09017351\n",
      "Iteration 216, loss = 0.09002239\n",
      "Iteration 217, loss = 0.08987536\n",
      "Iteration 218, loss = 0.08973286\n",
      "Iteration 219, loss = 0.08958862\n",
      "Iteration 204, loss = 0.18710695\n",
      "Iteration 108, loss = 0.25630130\n",
      "Iteration 220, loss = 0.08946025\n",
      "Iteration 4, loss = 0.64796127\n",
      "Iteration 127, loss = 0.12253400\n",
      "Iteration 309, loss = 0.07765626\n",
      "Iteration 128, loss = 0.12187371\n",
      "Iteration 129, loss = 0.12117084\n",
      "Iteration 310, loss = 0.07756292\n",
      "Iteration 221, loss = 0.08931686\n",
      "Iteration 130, loss = 0.12050379\n",
      "Iteration 222, loss = 0.08917891\n",
      "Iteration 311, loss = 0.07746964\n",
      "Iteration 322, loss = 0.14279147\n",
      "Iteration 312, loss = 0.07737290\n",
      "Iteration 292, loss = 0.15193583\n",
      "Iteration 131, loss = 0.11985937\n",
      "Iteration 313, loss = 0.07728311\n",
      "Iteration 132, loss = 0.11924108\n",
      "Iteration 205, loss = 0.18695016\n",
      "Iteration 314, loss = 0.07718435\n",
      "Iteration 315, loss = 0.07709388\n",
      "Iteration 5, loss = 0.58559311\n",
      "Iteration 6, loss = 0.53847692\n",
      "Iteration 316, loss = 0.07700522\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 133, loss = 0.11858603\n",
      "Iteration 293, loss = 0.15181956\n",
      "Iteration 323, loss = 0.14269732\n",
      "Iteration 134, loss = 0.11796878\n",
      "Iteration 223, loss = 0.08904168\n",
      "Iteration 135, loss = 0.11738256\n",
      "Iteration 136, loss = 0.11675854\n",
      "Iteration 137, loss = 0.11619222\n",
      "Iteration 7, loss = 0.50202138\n",
      "Iteration 138, loss = 0.11559321\n",
      "Iteration 294, loss = 0.15176592\n",
      "Iteration 139, loss = 0.11503678\n",
      "Iteration 140, loss = 0.11447302\n",
      "Iteration 8, loss = 0.47372573\n",
      "Iteration 295, loss = 0.15163965\n",
      "Iteration 9, loss = 0.45140651\n",
      "Iteration 224, loss = 0.08890382\n",
      "Iteration 10, loss = 0.43346952Iteration 225, loss = 0.08877144\n",
      "Iteration 141, loss = 0.11390961\n",
      "\n",
      "Iteration 142, loss = 0.11336822\n",
      "Iteration 206, loss = 0.18677693\n",
      "Iteration 143, loss = 0.11284417\n",
      "Iteration 144, loss = 0.11232295\n",
      "Iteration 145, loss = 0.11180210\n",
      "Iteration 146, loss = 0.11127404\n",
      "Iteration 147, loss = 0.11079900\n",
      "Iteration 148, loss = 0.11028123\n",
      "Iteration 109, loss = 0.25565249\n",
      "Iteration 11, loss = 0.41864523\n",
      "Iteration 1, loss = 0.86186507\n",
      "Iteration 324, loss = 0.14261706\n",
      "Iteration 226, loss = 0.08864099\n",
      "Iteration 12, loss = 0.40575843\n",
      "Iteration 296, loss = 0.15155102\n",
      "Iteration 227, loss = 0.08850590\n",
      "Iteration 149, loss = 0.10979629\n",
      "Iteration 13, loss = 0.39460894\n",
      "Iteration 228, loss = 0.08837617\n",
      "Iteration 14, loss = 0.38448844\n",
      "Iteration 325, loss = 0.14257041\n",
      "Iteration 229, loss = 0.08824650\n",
      "Iteration 110, loss = 0.25499319\n",
      "Iteration 2, loss = 0.81221717\n",
      "Iteration 150, loss = 0.10932821\n",
      "Iteration 151, loss = 0.10885196\n",
      "Iteration 152, loss = 0.10838823\n",
      "Iteration 297, loss = 0.15142672\n",
      "Iteration 15, loss = 0.37512794\n",
      "Iteration 3, loss = 0.74977676\n",
      "Iteration 207, loss = 0.18666400\n",
      "Iteration 230, loss = 0.08811476\n",
      "Iteration 298, loss = 0.15134054\n",
      "Iteration 16, loss = 0.36634598\n",
      "Iteration 153, loss = 0.10792035\n",
      "Iteration 231, loss = 0.08797906\n",
      "Iteration 154, loss = 0.10746677\n",
      "Iteration 326, loss = 0.14247866\n",
      "Iteration 155, loss = 0.10701997\n",
      "Iteration 156, loss = 0.10659377\n",
      "Iteration 299, loss = 0.15124030\n",
      "Iteration 157, loss = 0.10615151\n",
      "Iteration 158, loss = 0.10572599\n",
      "Iteration 232, loss = 0.08787002\n",
      "Iteration 159, loss = 0.10527792\n",
      "Iteration 160, loss = 0.10486491\n",
      "Iteration 17, loss = 0.35807391\n",
      "Iteration 161, loss = 0.10445670\n",
      "Iteration 162, loss = 0.10403854\n",
      "Iteration 233, loss = 0.08772488\n",
      "Iteration 111, loss = 0.25428572\n",
      "Iteration 163, loss = 0.10362864\n",
      "Iteration 4, loss = 0.68952109\n",
      "Iteration 164, loss = 0.10322352\n",
      "Iteration 18, loss = 0.35019546\n",
      "Iteration 165, loss = 0.10281764\n",
      "Iteration 208, loss = 0.18649198\n",
      "Iteration 166, loss = 0.10242322\n",
      "Iteration 234, loss = 0.08759742\n",
      "Iteration 5, loss = 0.63723511\n",
      "Iteration 235, loss = 0.08747155\n",
      "Iteration 19, loss = 0.34265045\n",
      "Iteration 6, loss = 0.59454470\n",
      "Iteration 327, loss = 0.14240142\n",
      "Iteration 167, loss = 0.10204798\n",
      "Iteration 168, loss = 0.10164845\n",
      "Iteration 7, loss = 0.55963916\n",
      "Iteration 169, loss = 0.10128756\n",
      "Iteration 170, loss = 0.10088924\n",
      "Iteration 20, loss = 0.33530724\n",
      "Iteration 236, loss = 0.08735732\n",
      "Iteration 112, loss = 0.25368727\n",
      "Iteration 21, loss = 0.32828211\n",
      "Iteration 171, loss = 0.10050658\n",
      "Iteration 172, loss = 0.10013517\n",
      "Iteration 237, loss = 0.08724942\n",
      "Iteration 173, loss = 0.09976651\n",
      "Iteration 22, loss = 0.32136685Iteration 174, loss = 0.09939842\n",
      "Iteration 300, loss = 0.15117100\n",
      "Iteration 238, loss = 0.08711176\n",
      "\n",
      "Iteration 175, loss = 0.09905678\n",
      "Iteration 328, loss = 0.14232017\n",
      "Iteration 176, loss = 0.09870499\n",
      "Iteration 177, loss = 0.09832999\n",
      "Iteration 239, loss = 0.08698647\n",
      "Iteration 23, loss = 0.31474479\n",
      "Iteration 178, loss = 0.09799999\n",
      "Iteration 209, loss = 0.18637202\n",
      "Iteration 301, loss = 0.15106357\n",
      "Iteration 24, loss = 0.30842082\n",
      "Iteration 240, loss = 0.08686328\n",
      "Iteration 179, loss = 0.09764879\n",
      "Iteration 180, loss = 0.09733064\n",
      "Iteration 210, loss = 0.18619885\n",
      "Iteration 8, loss = 0.53116111\n",
      "Iteration 241, loss = 0.08674222\n",
      "Iteration 181, loss = 0.09696728\n",
      "Iteration 242, loss = 0.08662699\n",
      "Iteration 182, loss = 0.09664622\n",
      "Iteration 183, loss = 0.09635190Iteration 25, loss = 0.30227150\n",
      "\n",
      "Iteration 329, loss = 0.14225932\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 184, loss = 0.09602033\n",
      "Iteration 185, loss = 0.09570459\n",
      "Iteration 113, loss = 0.25304600\n",
      "Iteration 186, loss = 0.09539146\n",
      "Iteration 187, loss = 0.09509481\n",
      "Iteration 188, loss = 0.09477889\n",
      "Iteration 189, loss = 0.09448676\n",
      "Iteration 243, loss = 0.08649425\n",
      "Iteration 190, loss = 0.09419139\n",
      "Iteration 302, loss = 0.15098916\n",
      "Iteration 26, loss = 0.29626095\n",
      "Iteration 9, loss = 0.50719891\n",
      "Iteration 1, loss = 0.81442479\n",
      "Iteration 244, loss = 0.08639229\n",
      "Iteration 27, loss = 0.29035736\n",
      "Iteration 303, loss = 0.15088067\n",
      "Iteration 245, loss = 0.08626784\n",
      "Iteration 10, loss = 0.48740623\n",
      "Iteration 211, loss = 0.18604123\n",
      "Iteration 28, loss = 0.28472407\n",
      "Iteration 246, loss = 0.08614622\n",
      "Iteration 191, loss = 0.09390177\n",
      "Iteration 29, loss = 0.27948014\n",
      "Iteration 2, loss = 0.78664112\n",
      "Iteration 247, loss = 0.08604113\n",
      "Iteration 192, loss = 0.09361401\n",
      "Iteration 3, loss = 0.74891665\n",
      "Iteration 30, loss = 0.27412907\n",
      "Iteration 114, loss = 0.25243212\n",
      "Iteration 193, loss = 0.09333501\n",
      "Iteration 304, loss = 0.15076723\n",
      "Iteration 4, loss = 0.71036530\n",
      "Iteration 5, loss = 0.67353350\n",
      "Iteration 6, loss = 0.64055493\n",
      "Iteration 194, loss = 0.09306616\n",
      "Iteration 248, loss = 0.08592801\n",
      "Iteration 7, loss = 0.61122675\n",
      "Iteration 8, loss = 0.58486604\n",
      "Iteration 249, loss = 0.08580091\n",
      "Iteration 11, loss = 0.47011206\n",
      "Iteration 250, loss = 0.08570594\n",
      "Iteration 305, loss = 0.15068521\n",
      "Iteration 9, loss = 0.56194644\n",
      "Iteration 195, loss = 0.09278267\n",
      "Iteration 10, loss = 0.54031695\n",
      "Iteration 196, loss = 0.09252592\n",
      "Iteration 197, loss = 0.09224729\n",
      "Iteration 31, loss = 0.26894879\n",
      "Iteration 198, loss = 0.09199211\n",
      "Iteration 199, loss = 0.09171953\n",
      "Iteration 212, loss = 0.18589628\n",
      "Iteration 251, loss = 0.08557959\n",
      "Iteration 200, loss = 0.09146530\n",
      "Iteration 12, loss = 0.45514903\n",
      "Iteration 32, loss = 0.26414253\n",
      "Iteration 115, loss = 0.25183557\n",
      "Iteration 201, loss = 0.09120742\n",
      "Iteration 11, loss = 0.52185244\n",
      "Iteration 202, loss = 0.09096294\n",
      "Iteration 203, loss = 0.09070369\n",
      "Iteration 252, loss = 0.08546581\n",
      "Iteration 306, loss = 0.15060887\n",
      "Iteration 12, loss = 0.50460818\n",
      "Iteration 204, loss = 0.09046293\n",
      "Iteration 13, loss = 0.48944612Iteration 33, loss = 0.25938117\n",
      "\n",
      "Iteration 13, loss = 0.44181222\n",
      "Iteration 253, loss = 0.08536757\n",
      "Iteration 254, loss = 0.08525030\n",
      "Iteration 205, loss = 0.09020649\n",
      "Iteration 206, loss = 0.08996900Iteration 14, loss = 0.42987366\n",
      "Iteration 255, loss = 0.08513434\n",
      "Iteration 307, loss = 0.15050497\n",
      "\n",
      "Iteration 256, loss = 0.08504017\n",
      "Iteration 14, loss = 0.47531848\n",
      "Iteration 34, loss = 0.25460876\n",
      "Iteration 15, loss = 0.46290975\n",
      "Iteration 207, loss = 0.08972473\n",
      "Iteration 16, loss = 0.45144345\n",
      "Iteration 35, loss = 0.25016004\n",
      "Iteration 17, loss = 0.44122775\n",
      "Iteration 208, loss = 0.08948450\n",
      "Iteration 209, loss = 0.08926956\n",
      "Iteration 210, loss = 0.08901633\n",
      "Iteration 211, loss = 0.08877256\n",
      "Iteration 257, loss = 0.08491829\n",
      "Iteration 36, loss = 0.24590861\n",
      "Iteration 18, loss = 0.43188392\n",
      "Iteration 212, loss = 0.08855381\n",
      "Iteration 213, loss = 0.18576429\n",
      "Iteration 37, loss = 0.24174499\n",
      "Iteration 19, loss = 0.42354596\n",
      "Iteration 15, loss = 0.41904445\n",
      "Iteration 116, loss = 0.25116295\n",
      "Iteration 308, loss = 0.15041298\n",
      "Iteration 213, loss = 0.08831739\n",
      "Iteration 20, loss = 0.41579994\n",
      "Iteration 214, loss = 0.08810478\n",
      "Iteration 21, loss = 0.40857381\n",
      "Iteration 214, loss = 0.18563449\n",
      "Iteration 22, loss = 0.40198419\n",
      "Iteration 23, loss = 0.39596354\n",
      "Iteration 258, loss = 0.08481868\n",
      "Iteration 24, loss = 0.39028748\n",
      "Iteration 25, loss = 0.38503029\n",
      "Iteration 259, loss = 0.08470928\n",
      "Iteration 26, loss = 0.37994002\n",
      "Iteration 27, loss = 0.37526475\n",
      "Iteration 260, loss = 0.08460542\n",
      "Iteration 16, loss = 0.40901279\n",
      "Iteration 215, loss = 0.08786248\n",
      "Iteration 38, loss = 0.23777924\n",
      "Iteration 216, loss = 0.08763932\n",
      "Iteration 39, loss = 0.23387886\n",
      "Iteration 217, loss = 0.08742863\n",
      "Iteration 218, loss = 0.08720465\n",
      "Iteration 219, loss = 0.08697362\n",
      "Iteration 28, loss = 0.37077562\n",
      "Iteration 220, loss = 0.08677160\n",
      "Iteration 309, loss = 0.15032258\n",
      "Iteration 29, loss = 0.36633827\n",
      "Iteration 221, loss = 0.08656433\n",
      "Iteration 30, loss = 0.36218443\n",
      "Iteration 222, loss = 0.08634223\n",
      "Iteration 117, loss = 0.25060860\n",
      "Iteration 223, loss = 0.08612883\n",
      "Iteration 31, loss = 0.35811290\n",
      "Iteration 40, loss = 0.23015758\n",
      "Iteration 17, loss = 0.39951256\n",
      "Iteration 261, loss = 0.08450866\n",
      "Iteration 310, loss = 0.15025236\n",
      "Iteration 224, loss = 0.08592210\n",
      "Iteration 32, loss = 0.35430271\n",
      "Iteration 262, loss = 0.08439843\n",
      "Iteration 33, loss = 0.35039822\n",
      "Iteration 34, loss = 0.34665702\n",
      "Iteration 18, loss = 0.39080510\n",
      "Iteration 263, loss = 0.08428679\n",
      "Iteration 35, loss = 0.34299057\n",
      "Iteration 41, loss = 0.22654820\n",
      "Iteration 264, loss = 0.08419036\n",
      "Iteration 311, loss = 0.15014780\n",
      "Iteration 225, loss = 0.08571997\n",
      "Iteration 265, loss = 0.08407913\n",
      "Iteration 42, loss = 0.22308883\n",
      "Iteration 226, loss = 0.08551324\n",
      "Iteration 227, loss = 0.08530207\n",
      "Iteration 43, loss = 0.21971692\n",
      "Iteration 215, loss = 0.18546949\n",
      "Iteration 312, loss = 0.15007326\n",
      "Iteration 228, loss = 0.08509885\n",
      "Iteration 229, loss = 0.08490708\n",
      "Iteration 230, loss = 0.08469133\n",
      "Iteration 231, loss = 0.08450818\n",
      "Iteration 118, loss = 0.25001413\n",
      "Iteration 313, loss = 0.15001877\n",
      "Iteration 266, loss = 0.08398202\n",
      "Iteration 19, loss = 0.38239772\n",
      "Iteration 36, loss = 0.33940613\n",
      "Iteration 44, loss = 0.21653631\n",
      "Iteration 37, loss = 0.33582809\n",
      "Iteration 45, loss = 0.21323074\n",
      "Iteration 38, loss = 0.33237423\n",
      "Iteration 232, loss = 0.08430193\n",
      "Iteration 20, loss = 0.37446166\n",
      "Iteration 233, loss = 0.08411442\n",
      "Iteration 234, loss = 0.08391842\n",
      "Iteration 216, loss = 0.18533552\n",
      "Iteration 46, loss = 0.21021822\n",
      "Iteration 39, loss = 0.32889231\n",
      "Iteration 235, loss = 0.08372957\n",
      "Iteration 40, loss = 0.32545565\n",
      "Iteration 267, loss = 0.08388662\n",
      "Iteration 236, loss = 0.08353582\n",
      "Iteration 41, loss = 0.32214803\n",
      "Iteration 217, loss = 0.18521056\n",
      "Iteration 47, loss = 0.20734559\n",
      "Iteration 21, loss = 0.36705538\n",
      "Iteration 268, loss = 0.08378169\n",
      "Iteration 269, loss = 0.08368182\n",
      "Iteration 237, loss = 0.08334946\n",
      "Iteration 119, loss = 0.24945453\n",
      "Iteration 238, loss = 0.08315890\n",
      "Iteration 239, loss = 0.08298982\n",
      "Iteration 42, loss = 0.31874319\n",
      "Iteration 43, loss = 0.31552779\n",
      "Iteration 270, loss = 0.08357950\n",
      "Iteration 44, loss = 0.31221532\n",
      "Iteration 271, loss = 0.08349086\n",
      "Iteration 240, loss = 0.08279674\n",
      "Iteration 48, loss = 0.20449097\n",
      "Iteration 314, loss = 0.14989737\n",
      "Iteration 241, loss = 0.08261925\n",
      "Iteration 22, loss = 0.35974496\n",
      "Iteration 242, loss = 0.08241811\n",
      "Iteration 45, loss = 0.30883308\n",
      "Iteration 272, loss = 0.08338343\n",
      "Iteration 46, loss = 0.30564095\n",
      "Iteration 243, loss = 0.08223240\n",
      "Iteration 244, loss = 0.08205547\n",
      "Iteration 49, loss = 0.20177366\n",
      "Iteration 245, loss = 0.08187962\n",
      "Iteration 218, loss = 0.18510305\n",
      "Iteration 47, loss = 0.30248663\n",
      "Iteration 50, loss = 0.19909702\n",
      "Iteration 315, loss = 0.14979469\n",
      "Iteration 246, loss = 0.08169998\n",
      "Iteration 48, loss = 0.29917802\n",
      "Iteration 49, loss = 0.29600717\n",
      "Iteration 247, loss = 0.08153513\n",
      "Iteration 120, loss = 0.24891253\n",
      "Iteration 23, loss = 0.35304870\n",
      "Iteration 248, loss = 0.08135836\n",
      "Iteration 273, loss = 0.08328234\n",
      "Iteration 249, loss = 0.08119066\n",
      "Iteration 274, loss = 0.08318378\n",
      "Iteration 316, loss = 0.14971511\n",
      "Iteration 275, loss = 0.08311758\n",
      "Iteration 24, loss = 0.34641181\n",
      "Iteration 276, loss = 0.08300803\n",
      "Iteration 51, loss = 0.19658751\n",
      "Iteration 277, loss = 0.08289549\n",
      "Iteration 52, loss = 0.19407223\n",
      "Iteration 317, loss = 0.14965800\n",
      "Iteration 50, loss = 0.29281918\n",
      "Iteration 51, loss = 0.28970133\n",
      "Iteration 250, loss = 0.08103044\n",
      "Iteration 251, loss = 0.08088545\n",
      "Iteration 252, loss = 0.08068255\n",
      "Iteration 52, loss = 0.28653223\n",
      "Iteration 53, loss = 0.28332759\n",
      "Iteration 25, loss = 0.34003677\n",
      "Iteration 54, loss = 0.28031771\n",
      "Iteration 53, loss = 0.19170462\n",
      "Iteration 278, loss = 0.08279289\n",
      "Iteration 279, loss = 0.08271321\n",
      "Iteration 54, loss = 0.18944230\n",
      "Iteration 253, loss = 0.08052355\n",
      "Iteration 254, loss = 0.08035516\n",
      "Iteration 219, loss = 0.18494284\n",
      "Iteration 255, loss = 0.08018677\n",
      "Iteration 256, loss = 0.08005666\n",
      "Iteration 55, loss = 0.27712947\n",
      "Iteration 257, loss = 0.07986267\n",
      "Iteration 280, loss = 0.08260438\n",
      "Iteration 258, loss = 0.07970487\n",
      "Iteration 56, loss = 0.27400463\n",
      "Iteration 55, loss = 0.18720588\n",
      "Iteration 281, loss = 0.08252650\n",
      "Iteration 259, loss = 0.07953896\n",
      "Iteration 57, loss = 0.27100191\n",
      "Iteration 282, loss = 0.08242242\n",
      "Iteration 121, loss = 0.24836854\n",
      "Iteration 58, loss = 0.26800882\n",
      "Iteration 283, loss = 0.08232945\n",
      "Iteration 59, loss = 0.26497754\n",
      "Iteration 26, loss = 0.33411898\n",
      "Iteration 56, loss = 0.18506391\n",
      "Iteration 260, loss = 0.07937959\n",
      "Iteration 220, loss = 0.18478294\n",
      "Iteration 284, loss = 0.08224788\n",
      "Iteration 60, loss = 0.26205832\n",
      "Iteration 261, loss = 0.07923269\n",
      "Iteration 262, loss = 0.07906438\n",
      "Iteration 318, loss = 0.14954548\n",
      "Iteration 263, loss = 0.07889944\n",
      "Iteration 264, loss = 0.07875311\n",
      "Iteration 27, loss = 0.32830530\n",
      "Iteration 61, loss = 0.25908040\n",
      "Iteration 57, loss = 0.18305864\n",
      "Iteration 62, loss = 0.25614289\n",
      "Iteration 265, loss = 0.07860097\n",
      "Iteration 63, loss = 0.25325568\n",
      "Iteration 58, loss = 0.18110149\n",
      "Iteration 64, loss = 0.25043287\n",
      "Iteration 285, loss = 0.08215578\n",
      "Iteration 319, loss = 0.14949187\n",
      "Iteration 266, loss = 0.07843595\n",
      "Iteration 267, loss = 0.07828140\n",
      "Iteration 268, loss = 0.07815673\n",
      "Iteration 320, loss = 0.14937294\n",
      "Iteration 122, loss = 0.24788035\n",
      "Iteration 269, loss = 0.07799630\n",
      "Iteration 28, loss = 0.32295830\n",
      "Iteration 270, loss = 0.07783370\n",
      "Iteration 59, loss = 0.17912781\n",
      "Iteration 29, loss = 0.31753773\n",
      "Iteration 65, loss = 0.24762428\n",
      "Iteration 286, loss = 0.08205409\n",
      "Iteration 271, loss = 0.07767999\n",
      "Iteration 221, loss = 0.18465917\n",
      "Iteration 272, loss = 0.07756179\n",
      "Iteration 66, loss = 0.24485226\n",
      "Iteration 60, loss = 0.17728417\n",
      "Iteration 67, loss = 0.24208850\n",
      "Iteration 68, loss = 0.23940452\n",
      "Iteration 273, loss = 0.07738686\n",
      "Iteration 287, loss = 0.08197843\n",
      "Iteration 69, loss = 0.23664054\n",
      "Iteration 70, loss = 0.23393816\n",
      "Iteration 274, loss = 0.07723500\n",
      "Iteration 30, loss = 0.31249301\n",
      "Iteration 71, loss = 0.23128078\n",
      "Iteration 288, loss = 0.08187304\n",
      "Iteration 289, loss = 0.08179327\n",
      "Iteration 31, loss = 0.30755129\n",
      "Iteration 290, loss = 0.08170352\n",
      "Iteration 61, loss = 0.17551243\n",
      "Iteration 72, loss = 0.22864335\n",
      "Iteration 275, loss = 0.07708939\n",
      "Iteration 73, loss = 0.22597759\n",
      "Iteration 62, loss = 0.17375868\n",
      "Iteration 291, loss = 0.08161156\n",
      "Iteration 276, loss = 0.07697564\n",
      "Iteration 123, loss = 0.24736066Iteration 74, loss = 0.22336021Iteration 222, loss = 0.18452456\n",
      "Iteration 63, loss = 0.17211262\n",
      "Iteration 277, loss = 0.07679618\n",
      "\n",
      "\n",
      "Iteration 64, loss = 0.17053306\n",
      "Iteration 75, loss = 0.22081827\n",
      "Iteration 278, loss = 0.07666380\n",
      "Iteration 292, loss = 0.08152213\n",
      "Iteration 279, loss = 0.07652511\n",
      "Iteration 321, loss = 0.14930384\n",
      "Iteration 76, loss = 0.21809515\n",
      "Iteration 32, loss = 0.30286843\n",
      "Iteration 280, loss = 0.07637943\n",
      "Iteration 293, loss = 0.08142798\n",
      "Iteration 65, loss = 0.16896446\n",
      "Iteration 294, loss = 0.08134852\n",
      "Iteration 322, loss = 0.14921345\n",
      "Iteration 77, loss = 0.21561748\n",
      "Iteration 295, loss = 0.08125387\n",
      "Iteration 281, loss = 0.07623076\n",
      "Iteration 223, loss = 0.18440442\n",
      "Iteration 282, loss = 0.07608636\n",
      "Iteration 66, loss = 0.16745735\n",
      "Iteration 78, loss = 0.21308365\n",
      "Iteration 283, loss = 0.07595529\n",
      "Iteration 79, loss = 0.21048958\n",
      "Iteration 284, loss = 0.07582152\n",
      "Iteration 67, loss = 0.16601034Iteration 285, loss = 0.07569174\n",
      "\n",
      "Iteration 33, loss = 0.29832132\n",
      "Iteration 296, loss = 0.08117423\n",
      "Iteration 323, loss = 0.14920559\n",
      "Iteration 80, loss = 0.20793017\n",
      "Iteration 81, loss = 0.20539853\n",
      "Iteration 286, loss = 0.07555397\n",
      "Iteration 297, loss = 0.08108008\n",
      "Iteration 124, loss = 0.24681225\n",
      "Iteration 82, loss = 0.20286950\n",
      "Iteration 287, loss = 0.07541952\n",
      "Iteration 298, loss = 0.08099798\n",
      "Iteration 68, loss = 0.16459664\n",
      "Iteration 324, loss = 0.14906315\n",
      "Iteration 224, loss = 0.18431257\n",
      "Iteration 299, loss = 0.08091483\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 34, loss = 0.29395203\n",
      "Iteration 83, loss = 0.20040435\n",
      "Iteration 69, loss = 0.16325686\n",
      "Iteration 288, loss = 0.07528887\n",
      "Iteration 84, loss = 0.19807986\n",
      "Iteration 85, loss = 0.19563922\n",
      "Iteration 325, loss = 0.14897248\n",
      "Iteration 289, loss = 0.07515807\n",
      "Iteration 86, loss = 0.19326938\n",
      "Iteration 70, loss = 0.16190151\n",
      "Iteration 125, loss = 0.24631873\n",
      "Iteration 290, loss = 0.07502641\n",
      "Iteration 87, loss = 0.19104056\n",
      "Iteration 1, loss = 0.79342065\n",
      "Iteration 291, loss = 0.07489539\n",
      "Iteration 88, loss = 0.18871875\n",
      "Iteration 326, loss = 0.14890360\n",
      "Iteration 292, loss = 0.07476317\n",
      "Iteration 89, loss = 0.18652687\n",
      "Iteration 71, loss = 0.16066032\n",
      "Iteration 2, loss = 0.75351562\n",
      "Iteration 90, loss = 0.18438102\n",
      "Iteration 91, loss = 0.18217453\n",
      "Iteration 293, loss = 0.07463216\n",
      "Iteration 35, loss = 0.28965821\n",
      "Iteration 294, loss = 0.07449346\n",
      "Iteration 295, loss = 0.07437882\n",
      "Iteration 296, loss = 0.07425046\n",
      "Iteration 225, loss = 0.18419606\n",
      "Iteration 72, loss = 0.15941412\n",
      "Iteration 36, loss = 0.28556424\n",
      "Iteration 73, loss = 0.15822159\n",
      "Iteration 92, loss = 0.18008750\n",
      "Iteration 297, loss = 0.07411929\n",
      "Iteration 298, loss = 0.07398678\n",
      "Iteration 299, loss = 0.07386320\n",
      "Iteration 37, loss = 0.28169440\n",
      "Iteration 300, loss = 0.07375215\n",
      "Iteration 301, loss = 0.07362448\n",
      "Iteration 126, loss = 0.24584345\n",
      "Iteration 302, loss = 0.07349654\n",
      "Iteration 303, loss = 0.07339242\n",
      "Iteration 93, loss = 0.17811924\n",
      "Iteration 94, loss = 0.17610260\n",
      "Iteration 3, loss = 0.70228638\n",
      "Iteration 74, loss = 0.15707911\n",
      "Iteration 95, loss = 0.17413307\n",
      "Iteration 304, loss = 0.07324888\n",
      "Iteration 96, loss = 0.17222257\n",
      "Iteration 97, loss = 0.17040604\n",
      "Iteration 38, loss = 0.27789508\n",
      "Iteration 327, loss = 0.14884986\n",
      "Iteration 75, loss = 0.15596910\n",
      "Iteration 98, loss = 0.16861311\n",
      "Iteration 305, loss = 0.07315334\n",
      "Iteration 306, loss = 0.07300588\n",
      "Iteration 307, loss = 0.07290064\n",
      "Iteration 328, loss = 0.14873308\n",
      "Iteration 226, loss = 0.18401418\n",
      "Iteration 308, loss = 0.07279053\n",
      "Iteration 4, loss = 0.65242248\n",
      "Iteration 76, loss = 0.15487772\n",
      "Iteration 309, loss = 0.07265825\n",
      "Iteration 99, loss = 0.16680140\n",
      "Iteration 100, loss = 0.16512645\n",
      "Iteration 101, loss = 0.16349100\n",
      "Iteration 77, loss = 0.15383648\n",
      "Iteration 102, loss = 0.16179917\n",
      "Iteration 310, loss = 0.07253877\n",
      "Iteration 103, loss = 0.16017408\n",
      "Iteration 39, loss = 0.27412621\n",
      "Iteration 311, loss = 0.07241833\n",
      "Iteration 5, loss = 0.60809278\n",
      "Iteration 78, loss = 0.15284148\n",
      "Iteration 6, loss = 0.57084499\n",
      "Iteration 104, loss = 0.15866720\n",
      "Iteration 105, loss = 0.15708344\n",
      "Iteration 7, loss = 0.53969414\n",
      "Iteration 312, loss = 0.07229933\n",
      "Iteration 79, loss = 0.15182516\n",
      "Iteration 40, loss = 0.27066701\n",
      "Iteration 127, loss = 0.24538053\n",
      "Iteration 329, loss = 0.14869197\n",
      "Iteration 227, loss = 0.18398482\n",
      "Iteration 106, loss = 0.15562807\n",
      "Iteration 313, loss = 0.07219515\n",
      "Iteration 314, loss = 0.07206594\n",
      "Iteration 80, loss = 0.15087808\n",
      "Iteration 107, loss = 0.15417436\n",
      "Iteration 315, loss = 0.07195132\n",
      "Iteration 108, loss = 0.15270743\n",
      "Iteration 316, loss = 0.07184195\n",
      "Iteration 109, loss = 0.15141494\n",
      "Iteration 317, loss = 0.07173388\n",
      "Iteration 330, loss = 0.14859452\n",
      "Iteration 110, loss = 0.14992457\n",
      "Iteration 41, loss = 0.26710179\n",
      "Iteration 81, loss = 0.14996893\n",
      "Iteration 82, loss = 0.14903074\n",
      "Iteration 331, loss = 0.14849949\n",
      "Iteration 42, loss = 0.26374313\n",
      "Iteration 228, loss = 0.18379870\n",
      "Iteration 83, loss = 0.14816122\n",
      "Iteration 128, loss = 0.24493366\n",
      "Iteration 8, loss = 0.51410461\n",
      "Iteration 318, loss = 0.07160655\n",
      "Iteration 319, loss = 0.07150672\n",
      "Iteration 111, loss = 0.14856753\n",
      "Iteration 320, loss = 0.07139658\n",
      "Iteration 84, loss = 0.14728090\n",
      "Iteration 112, loss = 0.14725219\n",
      "Iteration 321, loss = 0.07127944\n",
      "Iteration 322, loss = 0.07119219\n",
      "Iteration 323, loss = 0.07106044\n",
      "Iteration 43, loss = 0.26052678\n",
      "Iteration 324, loss = 0.07097242\n",
      "Iteration 9, loss = 0.49286356\n",
      "Iteration 113, loss = 0.14597489\n",
      "Iteration 85, loss = 0.14647033\n",
      "Iteration 325, loss = 0.07085044\n",
      "Iteration 326, loss = 0.07074055\n",
      "Iteration 332, loss = 0.14844398\n",
      "Iteration 86, loss = 0.14564069\n",
      "Iteration 114, loss = 0.14469562\n",
      "Iteration 10, loss = 0.47536869\n",
      "Iteration 87, loss = 0.14486941\n",
      "Iteration 44, loss = 0.25730756\n",
      "Iteration 327, loss = 0.07063661\n",
      "Iteration 129, loss = 0.24441976\n",
      "Iteration 115, loss = 0.14347088\n",
      "Iteration 229, loss = 0.18365597\n",
      "Iteration 333, loss = 0.14839525\n",
      "Iteration 328, loss = 0.07052574\n",
      "Iteration 116, loss = 0.14219264\n",
      "Iteration 329, loss = 0.07042523\n",
      "Iteration 330, loss = 0.07034002\n",
      "Iteration 117, loss = 0.14100632\n",
      "Iteration 331, loss = 0.07024210\n",
      "Iteration 118, loss = 0.13982017\n",
      "Iteration 332, loss = 0.07011591\n",
      "Iteration 333, loss = 0.07001169\n",
      "Iteration 119, loss = 0.13871629\n",
      "Iteration 334, loss = 0.06991380\n",
      "Iteration 120, loss = 0.13751678\n",
      "Iteration 335, loss = 0.06980271\n",
      "Iteration 121, loss = 0.13641108\n",
      "Iteration 336, loss = 0.06970751\n",
      "Iteration 337, loss = 0.06961838\n",
      "Iteration 11, loss = 0.46042828\n",
      "Iteration 122, loss = 0.13530266\n",
      "Iteration 338, loss = 0.06951303\n",
      "Iteration 45, loss = 0.25425743\n",
      "Iteration 88, loss = 0.14410870\n",
      "Iteration 123, loss = 0.13428986\n",
      "Iteration 12, loss = 0.44781534\n",
      "Iteration 124, loss = 0.13318403\n",
      "Iteration 334, loss = 0.14830026\n",
      "Iteration 89, loss = 0.14332551\n",
      "Iteration 230, loss = 0.18355300\n",
      "Iteration 125, loss = 0.13222566\n",
      "Iteration 126, loss = 0.13119539\n",
      "Iteration 339, loss = 0.06941402\n",
      "Iteration 13, loss = 0.43682447\n",
      "Iteration 340, loss = 0.06931166\n",
      "Iteration 90, loss = 0.14261158\n",
      "Iteration 335, loss = 0.14820327\n",
      "Iteration 341, loss = 0.06921570\n",
      "Iteration 91, loss = 0.14190466\n",
      "Iteration 127, loss = 0.13019261\n",
      "Iteration 130, loss = 0.24398735\n",
      "Iteration 46, loss = 0.25122942\n",
      "Iteration 14, loss = 0.42706551\n",
      "Iteration 342, loss = 0.06912160\n",
      "Iteration 343, loss = 0.06902070\n",
      "Iteration 15, loss = 0.41833888\n",
      "Iteration 92, loss = 0.14123586\n",
      "Iteration 128, loss = 0.12927336\n",
      "Iteration 344, loss = 0.06892043\n",
      "Iteration 345, loss = 0.06882526\n",
      "Iteration 129, loss = 0.12828771\n",
      "Iteration 130, loss = 0.12736713\n",
      "Iteration 231, loss = 0.18342776\n",
      "Iteration 131, loss = 0.12649787\n",
      "Iteration 132, loss = 0.12562753\n",
      "Iteration 93, loss = 0.14054029\n",
      "Iteration 336, loss = 0.14815653\n",
      "Iteration 131, loss = 0.24358265\n",
      "Iteration 94, loss = 0.13987809\n",
      "Iteration 47, loss = 0.24834764\n",
      "Iteration 346, loss = 0.06872309\n",
      "Iteration 133, loss = 0.12476575\n",
      "Iteration 95, loss = 0.13921812Iteration 16, loss = 0.41045685\n",
      "\n",
      "Iteration 347, loss = 0.06863291\n",
      "Iteration 348, loss = 0.06853333\n",
      "Iteration 48, loss = 0.24559184\n",
      "Iteration 349, loss = 0.06847158\n",
      "Iteration 17, loss = 0.40304289\n",
      "Iteration 96, loss = 0.13862324\n",
      "Iteration 18, loss = 0.39609457\n",
      "Iteration 337, loss = 0.14805300\n",
      "Iteration 134, loss = 0.12392844\n",
      "Iteration 350, loss = 0.06834565\n",
      "Iteration 135, loss = 0.12308460\n",
      "Iteration 351, loss = 0.06825771\n",
      "Iteration 136, loss = 0.12234309\n",
      "Iteration 97, loss = 0.13797952\n",
      "Iteration 352, loss = 0.06815859\n",
      "Iteration 353, loss = 0.06806686\n",
      "Iteration 137, loss = 0.12149907\n",
      "Iteration 232, loss = 0.18328358\n",
      "Iteration 49, loss = 0.24279454\n",
      "Iteration 354, loss = 0.06796283\n",
      "Iteration 338, loss = 0.14800294\n",
      "Iteration 138, loss = 0.12069503\n",
      "Iteration 139, loss = 0.11995580\n",
      "Iteration 19, loss = 0.38950267\n",
      "Iteration 20, loss = 0.38328763\n",
      "Iteration 339, loss = 0.14794484\n",
      "Iteration 98, loss = 0.13737458\n",
      "Iteration 355, loss = 0.06787536\n",
      "Iteration 50, loss = 0.24011806\n",
      "Iteration 356, loss = 0.06778902\n",
      "Iteration 140, loss = 0.11922884\n",
      "Iteration 132, loss = 0.24313057\n",
      "Iteration 99, loss = 0.13678531\n",
      "Iteration 357, loss = 0.06769601\n",
      "Iteration 100, loss = 0.13621569\n",
      "Iteration 358, loss = 0.06760010\n",
      "Iteration 359, loss = 0.06750814\n",
      "Iteration 101, loss = 0.13564970\n",
      "Iteration 233, loss = 0.18317816\n",
      "Iteration 141, loss = 0.11850664\n",
      "Iteration 142, loss = 0.11779034\n",
      "Iteration 21, loss = 0.37716182\n",
      "Iteration 360, loss = 0.06741017\n",
      "Iteration 143, loss = 0.11707411\n",
      "Iteration 144, loss = 0.11637856\n",
      "Iteration 22, loss = 0.37117344\n",
      "Iteration 145, loss = 0.11571541\n",
      "Iteration 361, loss = 0.06731937\n",
      "Iteration 362, loss = 0.06722347\n",
      "Iteration 51, loss = 0.23746524\n",
      "Iteration 363, loss = 0.06713351\n",
      "Iteration 364, loss = 0.06704358\n",
      "Iteration 146, loss = 0.11505451\n",
      "Iteration 365, loss = 0.06695338\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 0.36536705\n",
      "Iteration 24, loss = 0.35975534\n",
      "Iteration 340, loss = 0.14784933\n",
      "Iteration 102, loss = 0.13509364\n",
      "Iteration 341, loss = 0.14778573\n",
      "Iteration 103, loss = 0.13453773\n",
      "Iteration 1, loss = 0.87542783\n",
      "Iteration 104, loss = 0.13402947\n",
      "Iteration 147, loss = 0.11441330\n",
      "Iteration 234, loss = 0.18308555\n",
      "Iteration 52, loss = 0.23511485\n",
      "Iteration 148, loss = 0.11379162\n",
      "Iteration 53, loss = 0.23251052\n",
      "Iteration 133, loss = 0.24270634\n",
      "Iteration 105, loss = 0.13351219\n",
      "Iteration 149, loss = 0.11318060\n",
      "Iteration 150, loss = 0.11259272\n",
      "Iteration 25, loss = 0.35436246\n",
      "Iteration 151, loss = 0.11197213\n",
      "Iteration 26, loss = 0.34894184\n",
      "Iteration 152, loss = 0.11138533\n",
      "Iteration 235, loss = 0.18292509\n",
      "Iteration 27, loss = 0.34368967\n",
      "Iteration 342, loss = 0.14770753\n",
      "Iteration 106, loss = 0.13298061\n",
      "Iteration 2, loss = 0.83369358\n",
      "Iteration 28, loss = 0.33850630\n",
      "Iteration 134, loss = 0.24224842\n",
      "Iteration 153, loss = 0.11085320\n",
      "Iteration 107, loss = 0.13249084\n",
      "Iteration 54, loss = 0.23011800\n",
      "Iteration 343, loss = 0.14763907\n",
      "Iteration 154, loss = 0.11023132\n",
      "Iteration 55, loss = 0.22784187\n",
      "Iteration 155, loss = 0.10969702\n",
      "Iteration 156, loss = 0.10917044\n",
      "Iteration 157, loss = 0.10862211\n",
      "Iteration 158, loss = 0.10813848\n",
      "Iteration 56, loss = 0.22548469\n",
      "Iteration 159, loss = 0.10760912\n",
      "Iteration 108, loss = 0.13201473\n",
      "Iteration 160, loss = 0.10709854\n",
      "Iteration 344, loss = 0.14761393\n",
      "Iteration 161, loss = 0.10660802\n",
      "Iteration 3, loss = 0.77958255\n",
      "Iteration 236, loss = 0.18281408\n",
      "Iteration 135, loss = 0.24184282\n",
      "Iteration 109, loss = 0.13151866\n",
      "Iteration 29, loss = 0.33349481\n",
      "Iteration 4, loss = 0.72884085\n",
      "Iteration 30, loss = 0.32840414\n",
      "Iteration 162, loss = 0.10613502\n",
      "Iteration 110, loss = 0.13105201\n",
      "Iteration 163, loss = 0.10567042\n",
      "Iteration 164, loss = 0.10518353\n",
      "Iteration 111, loss = 0.13059675\n",
      "Iteration 165, loss = 0.10477418\n",
      "Iteration 57, loss = 0.22333305\n",
      "Iteration 166, loss = 0.10436067\n",
      "Iteration 345, loss = 0.14749656\n",
      "Iteration 112, loss = 0.13015341\n",
      "Iteration 113, loss = 0.12970457\n",
      "Iteration 31, loss = 0.32339177\n",
      "Iteration 5, loss = 0.68383493\n",
      "Iteration 167, loss = 0.10388475\n",
      "Iteration 32, loss = 0.31869304\n",
      "Iteration 58, loss = 0.22117001\n",
      "Iteration 237, loss = 0.18269432\n",
      "Iteration 168, loss = 0.10346904\n",
      "Iteration 346, loss = 0.14740665\n",
      "Iteration 114, loss = 0.12925222\n",
      "Iteration 33, loss = 0.31385549\n",
      "Iteration 169, loss = 0.10306494\n",
      "Iteration 136, loss = 0.24163699\n",
      "Iteration 115, loss = 0.12884647\n",
      "Iteration 170, loss = 0.10265667\n",
      "Iteration 59, loss = 0.21910208\n",
      "Iteration 171, loss = 0.10228211\n",
      "Iteration 172, loss = 0.10187231\n",
      "Iteration 34, loss = 0.30929148\n",
      "Iteration 116, loss = 0.12839609\n",
      "Iteration 173, loss = 0.10149178\n",
      "Iteration 174, loss = 0.10110598\n",
      "Iteration 347, loss = 0.14740643\n",
      "Iteration 6, loss = 0.64704798\n",
      "Iteration 117, loss = 0.12799201\n",
      "Iteration 238, loss = 0.18257954\n",
      "Iteration 348, loss = 0.14726328\n",
      "Iteration 175, loss = 0.10077071\n",
      "Iteration 176, loss = 0.10039450\n",
      "Iteration 35, loss = 0.30468336\n",
      "Iteration 60, loss = 0.21711966\n",
      "Iteration 177, loss = 0.10004354\n",
      "Iteration 178, loss = 0.09968362\n",
      "Iteration 118, loss = 0.12758600\n",
      "Iteration 36, loss = 0.30031154\n",
      "Iteration 137, loss = 0.24105843\n",
      "Iteration 61, loss = 0.21512572\n",
      "Iteration 37, loss = 0.29589634\n",
      "Iteration 7, loss = 0.61776081\n",
      "Iteration 349, loss = 0.14719152\n",
      "Iteration 179, loss = 0.09934013\n",
      "Iteration 119, loss = 0.12716428\n",
      "Iteration 180, loss = 0.09901661\n",
      "Iteration 181, loss = 0.09866116\n",
      "Iteration 38, loss = 0.29142246\n",
      "Iteration 120, loss = 0.12676998\n",
      "Iteration 182, loss = 0.09834261\n",
      "Iteration 62, loss = 0.21317556\n",
      "Iteration 239, loss = 0.18245307\n",
      "Iteration 183, loss = 0.09802654\n",
      "Iteration 138, loss = 0.24062787\n",
      "Iteration 184, loss = 0.09769645\n",
      "Iteration 63, loss = 0.21135087\n",
      "Iteration 185, loss = 0.09741524\n",
      "Iteration 186, loss = 0.09709606\n",
      "Iteration 121, loss = 0.12639993\n",
      "Iteration 187, loss = 0.09678798\n",
      "Iteration 39, loss = 0.28715078\n",
      "Iteration 188, loss = 0.09648720\n",
      "Iteration 8, loss = 0.59180374\n",
      "Iteration 64, loss = 0.20951688\n",
      "Iteration 122, loss = 0.12599516\n",
      "Iteration 189, loss = 0.09619983\n",
      "Iteration 123, loss = 0.12561321\n",
      "Iteration 40, loss = 0.28300818\n",
      "Iteration 190, loss = 0.09589303\n",
      "Iteration 124, loss = 0.12525673\n",
      "Iteration 65, loss = 0.20775823\n",
      "Iteration 191, loss = 0.09562553\n",
      "Iteration 125, loss = 0.12489848\n",
      "Iteration 192, loss = 0.09532974\n",
      "Iteration 350, loss = 0.14713283\n",
      "Iteration 193, loss = 0.09504008\n",
      "Iteration 194, loss = 0.09477697\n",
      "Iteration 195, loss = 0.09449314\n",
      "Iteration 351, loss = 0.14705427\n",
      "Iteration 139, loss = 0.24034684\n",
      "Iteration 240, loss = 0.18235773\n",
      "Iteration 41, loss = 0.27884734\n",
      "Iteration 9, loss = 0.57120761\n",
      "Iteration 352, loss = 0.14698030\n",
      "Iteration 126, loss = 0.12452425\n",
      "Iteration 196, loss = 0.09424527\n",
      "Iteration 42, loss = 0.27472263\n",
      "Iteration 197, loss = 0.09396340\n",
      "Iteration 198, loss = 0.09372167\n",
      "Iteration 127, loss = 0.12416561\n",
      "Iteration 140, loss = 0.23990673\n",
      "Iteration 128, loss = 0.12382708\n",
      "Iteration 129, loss = 0.12346522\n",
      "Iteration 241, loss = 0.18227237\n",
      "Iteration 66, loss = 0.20601336\n",
      "Iteration 199, loss = 0.09348334\n",
      "Iteration 43, loss = 0.27063795\n",
      "Iteration 130, loss = 0.12313733\n",
      "Iteration 10, loss = 0.55397837\n",
      "Iteration 131, loss = 0.12280087\n",
      "Iteration 353, loss = 0.14691231\n",
      "Iteration 200, loss = 0.09321912\n",
      "Iteration 201, loss = 0.09297539\n",
      "Iteration 202, loss = 0.09272468\n",
      "Iteration 203, loss = 0.09248728\n",
      "Iteration 132, loss = 0.12247409\n",
      "Iteration 44, loss = 0.26680794\n",
      "Iteration 133, loss = 0.12213311\n",
      "Iteration 11, loss = 0.53958384\n",
      "Iteration 45, loss = 0.26289121\n",
      "Iteration 67, loss = 0.20436246\n",
      "Iteration 134, loss = 0.12182227\n",
      "Iteration 135, loss = 0.12150322\n",
      "Iteration 354, loss = 0.14686302\n",
      "Iteration 204, loss = 0.09223841\n",
      "Iteration 242, loss = 0.18213518\n",
      "Iteration 355, loss = 0.14679653\n",
      "Iteration 205, loss = 0.09203062\n",
      "Iteration 68, loss = 0.20269517\n",
      "Iteration 46, loss = 0.25898887\n",
      "Iteration 141, loss = 0.23952987\n",
      "Iteration 206, loss = 0.09178704\n",
      "Iteration 207, loss = 0.09155127\n",
      "Iteration 69, loss = 0.20112176\n",
      "Iteration 208, loss = 0.09132239\n",
      "Iteration 12, loss = 0.52690046\n",
      "Iteration 136, loss = 0.12118522\n",
      "Iteration 243, loss = 0.18204738\n",
      "Iteration 209, loss = 0.09108474\n",
      "Iteration 47, loss = 0.25522190\n",
      "Iteration 210, loss = 0.09087807\n",
      "Iteration 48, loss = 0.25159233\n",
      "Iteration 13, loss = 0.51564072\n",
      "Iteration 137, loss = 0.12087474\n",
      "Iteration 70, loss = 0.19953485\n",
      "Iteration 211, loss = 0.09065997\n",
      "Iteration 356, loss = 0.14672182\n",
      "Iteration 138, loss = 0.12056580\n",
      "Iteration 142, loss = 0.23916700\n",
      "Iteration 212, loss = 0.09044808\n",
      "Iteration 139, loss = 0.12027725\n",
      "Iteration 49, loss = 0.24802373Iteration 213, loss = 0.09021358\n",
      "Iteration 214, loss = 0.09000135\n",
      "\n",
      "Iteration 357, loss = 0.14664789\n",
      "Iteration 71, loss = 0.19801250\n",
      "Iteration 244, loss = 0.18191905\n",
      "Iteration 14, loss = 0.50575517\n",
      "Iteration 358, loss = 0.14658378\n",
      "Iteration 215, loss = 0.08979667\n",
      "Iteration 140, loss = 0.11998660\n",
      "Iteration 72, loss = 0.19656208\n",
      "Iteration 143, loss = 0.23879343\n",
      "Iteration 216, loss = 0.08960851\n",
      "Iteration 217, loss = 0.08939080\n",
      "Iteration 141, loss = 0.11967757\n",
      "Iteration 50, loss = 0.24444021\n",
      "Iteration 218, loss = 0.08917921\n",
      "Iteration 73, loss = 0.19516290\n",
      "Iteration 219, loss = 0.08897895\n",
      "Iteration 15, loss = 0.49644003\n",
      "Iteration 220, loss = 0.08878461\n",
      "Iteration 359, loss = 0.14653791\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 51, loss = 0.24118133\n",
      "Iteration 245, loss = 0.18180215\n",
      "Iteration 74, loss = 0.19373312\n",
      "Iteration 221, loss = 0.08857815\n",
      "Iteration 222, loss = 0.08839131\n",
      "Iteration 223, loss = 0.08820842\n",
      "Iteration 52, loss = 0.23784516\n",
      "Iteration 144, loss = 0.23851118\n",
      "Iteration 53, loss = 0.23465496\n",
      "Iteration 142, loss = 0.11938944\n",
      "Iteration 143, loss = 0.11909978\n",
      "Iteration 224, loss = 0.08800780\n",
      "Iteration 144, loss = 0.11882137\n",
      "Iteration 1, loss = 0.63054300\n",
      "Iteration 2, loss = 0.61305080\n",
      "Iteration 225, loss = 0.08782053\n",
      "Iteration 16, loss = 0.48810471\n",
      "Iteration 3, loss = 0.58888703\n",
      "Iteration 145, loss = 0.11853640\n",
      "Iteration 226, loss = 0.08764413\n",
      "Iteration 54, loss = 0.23145915\n",
      "Iteration 75, loss = 0.19238012\n",
      "Iteration 227, loss = 0.08744430\n",
      "Iteration 246, loss = 0.18168856\n",
      "Iteration 146, loss = 0.11825705\n",
      "Iteration 76, loss = 0.19105909\n",
      "Iteration 4, loss = 0.56230198\n",
      "Iteration 228, loss = 0.08726594\n",
      "Iteration 145, loss = 0.23818704\n",
      "Iteration 229, loss = 0.08707776\n",
      "Iteration 147, loss = 0.11799082\n",
      "Iteration 55, loss = 0.22838944\n",
      "Iteration 5, loss = 0.53681345\n",
      "Iteration 148, loss = 0.11772110\n",
      "Iteration 230, loss = 0.08688952\n",
      "Iteration 17, loss = 0.48023155\n",
      "Iteration 6, loss = 0.51288117\n",
      "Iteration 231, loss = 0.08671755\n",
      "Iteration 232, loss = 0.08655691\n",
      "Iteration 247, loss = 0.18158659\n",
      "Iteration 7, loss = 0.49155249\n",
      "Iteration 77, loss = 0.18975765\n",
      "Iteration 149, loss = 0.11744487\n",
      "Iteration 8, loss = 0.47249094\n",
      "Iteration 18, loss = 0.47275880\n",
      "Iteration 9, loss = 0.45571828\n",
      "Iteration 78, loss = 0.18852532\n",
      "Iteration 233, loss = 0.08636549\n",
      "Iteration 150, loss = 0.11718600\n",
      "Iteration 56, loss = 0.22558446\n",
      "Iteration 234, loss = 0.08618792\n",
      "Iteration 10, loss = 0.44128787\n",
      "Iteration 235, loss = 0.08602794\n",
      "Iteration 11, loss = 0.42879278\n",
      "Iteration 236, loss = 0.08585257\n",
      "Iteration 57, loss = 0.22271224\n",
      "Iteration 12, loss = 0.41841443\n",
      "Iteration 237, loss = 0.08569063\n",
      "Iteration 238, loss = 0.08549425\n",
      "Iteration 13, loss = 0.40932541\n",
      "Iteration 239, loss = 0.08531981\n",
      "Iteration 151, loss = 0.11692627\n",
      "Iteration 146, loss = 0.23783440\n",
      "Iteration 240, loss = 0.08517823\n",
      "Iteration 248, loss = 0.18148920\n",
      "Iteration 79, loss = 0.18725646\n",
      "Iteration 152, loss = 0.11665442\n",
      "Iteration 14, loss = 0.40176117\n",
      "Iteration 19, loss = 0.46564744\n",
      "Iteration 58, loss = 0.21988957\n",
      "Iteration 80, loss = 0.18610416\n",
      "Iteration 241, loss = 0.08501538\n",
      "Iteration 242, loss = 0.08484395\n",
      "Iteration 59, loss = 0.21721210\n",
      "Iteration 15, loss = 0.39475047\n",
      "Iteration 153, loss = 0.11640995\n",
      "Iteration 16, loss = 0.38896365\n",
      "Iteration 243, loss = 0.08468799\n",
      "Iteration 244, loss = 0.08452487\n",
      "Iteration 17, loss = 0.38374656\n",
      "Iteration 18, loss = 0.37888948\n",
      "Iteration 20, loss = 0.45873201\n",
      "Iteration 60, loss = 0.21456283\n",
      "Iteration 19, loss = 0.37442408\n",
      "Iteration 154, loss = 0.11615927\n",
      "Iteration 20, loss = 0.37032267\n",
      "Iteration 21, loss = 0.36634785\n",
      "Iteration 22, loss = 0.36256588\n",
      "Iteration 81, loss = 0.18491630\n",
      "Iteration 147, loss = 0.23744555\n",
      "Iteration 245, loss = 0.08438632\n",
      "Iteration 21, loss = 0.45202694\n",
      "Iteration 246, loss = 0.08418733\n",
      "Iteration 249, loss = 0.18139904\n",
      "Iteration 155, loss = 0.11589815\n",
      "Iteration 23, loss = 0.35893446\n",
      "Iteration 247, loss = 0.08404768\n",
      "Iteration 61, loss = 0.21212095\n",
      "Iteration 156, loss = 0.11565607\n",
      "Iteration 248, loss = 0.08390851\n",
      "Iteration 249, loss = 0.08373975\n",
      "Iteration 82, loss = 0.18379311\n",
      "Iteration 157, loss = 0.11539588\n",
      "Iteration 62, loss = 0.20964148\n",
      "Iteration 250, loss = 0.08360033\n",
      "Iteration 24, loss = 0.35530880\n",
      "Iteration 25, loss = 0.35187768\n",
      "Iteration 251, loss = 0.08343959\n",
      "Iteration 158, loss = 0.11515388\n",
      "Iteration 63, loss = 0.20736558\n",
      "Iteration 250, loss = 0.18127585\n",
      "Iteration 252, loss = 0.08327582\n",
      "Iteration 253, loss = 0.08314474\n",
      "Iteration 83, loss = 0.18272226\n",
      "Iteration 26, loss = 0.34841738\n",
      "Iteration 159, loss = 0.11492258\n",
      "Iteration 22, loss = 0.44565963\n",
      "Iteration 254, loss = 0.08296306\n",
      "Iteration 27, loss = 0.34503733\n",
      "Iteration 160, loss = 0.11467255\n",
      "Iteration 28, loss = 0.34162398\n",
      "Iteration 148, loss = 0.23716664\n",
      "Iteration 255, loss = 0.08282780\n",
      "Iteration 84, loss = 0.18161946\n",
      "Iteration 256, loss = 0.08272293\n",
      "Iteration 64, loss = 0.20519550\n",
      "Iteration 257, loss = 0.08255828\n",
      "Iteration 161, loss = 0.11443310\n",
      "Iteration 258, loss = 0.08239485\n",
      "Iteration 29, loss = 0.33825931\n",
      "Iteration 85, loss = 0.18064419\n",
      "Iteration 162, loss = 0.11419538\n",
      "Iteration 65, loss = 0.20292541\n",
      "Iteration 163, loss = 0.11396511\n",
      "Iteration 30, loss = 0.33489067\n",
      "Iteration 23, loss = 0.43938078\n",
      "Iteration 31, loss = 0.33153626\n",
      "Iteration 66, loss = 0.20085450\n",
      "Iteration 259, loss = 0.08225855\n",
      "Iteration 260, loss = 0.08210200\n",
      "Iteration 164, loss = 0.11373978\n",
      "Iteration 261, loss = 0.08197078\n",
      "Iteration 251, loss = 0.18118132\n",
      "Iteration 86, loss = 0.17959762\n",
      "Iteration 262, loss = 0.08182056\n",
      "Iteration 263, loss = 0.08168615\n",
      "Iteration 149, loss = 0.23680506\n",
      "Iteration 165, loss = 0.11350254\n",
      "Iteration 32, loss = 0.32815087\n",
      "Iteration 33, loss = 0.32475376\n",
      "Iteration 252, loss = 0.18105813\n",
      "Iteration 264, loss = 0.08154984\n",
      "Iteration 67, loss = 0.19874882\n",
      "Iteration 34, loss = 0.32150203\n",
      "Iteration 166, loss = 0.11328670\n",
      "Iteration 35, loss = 0.31823556\n",
      "Iteration 167, loss = 0.11304865\n",
      "Iteration 24, loss = 0.43319706\n",
      "Iteration 265, loss = 0.08141948\n",
      "Iteration 68, loss = 0.19690787\n",
      "Iteration 87, loss = 0.17860408\n",
      "Iteration 36, loss = 0.31493996\n",
      "Iteration 266, loss = 0.08126652\n",
      "Iteration 267, loss = 0.08114685\n",
      "Iteration 168, loss = 0.11284254\n",
      "Iteration 268, loss = 0.08100092\n",
      "Iteration 37, loss = 0.31155103\n",
      "Iteration 69, loss = 0.19499600\n",
      "Iteration 38, loss = 0.30831933\n",
      "Iteration 169, loss = 0.11262321\n",
      "Iteration 88, loss = 0.17767505\n",
      "Iteration 39, loss = 0.30506048\n",
      "Iteration 150, loss = 0.23650125\n",
      "Iteration 40, loss = 0.30167680\n",
      "Iteration 25, loss = 0.42719926\n",
      "Iteration 41, loss = 0.29830245\n",
      "Iteration 42, loss = 0.29500132\n",
      "Iteration 43, loss = 0.29155645\n",
      "Iteration 170, loss = 0.11239711\n",
      "Iteration 44, loss = 0.28831609\n",
      "Iteration 45, loss = 0.28495650\n",
      "Iteration 171, loss = 0.11219628\n",
      "Iteration 46, loss = 0.28179782\n",
      "Iteration 269, loss = 0.08087042\n",
      "Iteration 172, loss = 0.11195640\n",
      "Iteration 270, loss = 0.08074914\n",
      "Iteration 89, loss = 0.17671158\n",
      "Iteration 271, loss = 0.08060019\n",
      "Iteration 272, loss = 0.08049100\n",
      "Iteration 151, loss = 0.23613530\n",
      "Iteration 70, loss = 0.19318114\n",
      "Iteration 273, loss = 0.08036169\n",
      "Iteration 47, loss = 0.27859708Iteration 274, loss = 0.08021736\n",
      "Iteration 90, loss = 0.17582849\n",
      "Iteration 253, loss = 0.18097936\n",
      "Iteration 275, loss = 0.08012122\n",
      "Iteration 71, loss = 0.19141497\n",
      "Iteration 276, loss = 0.07998127\n",
      "\n",
      "Iteration 277, loss = 0.07986104\n",
      "Iteration 26, loss = 0.42121807\n",
      "Iteration 173, loss = 0.11175716\n",
      "Iteration 278, loss = 0.07971986\n",
      "Iteration 279, loss = 0.07958627\n",
      "Iteration 72, loss = 0.18971595\n",
      "Iteration 280, loss = 0.07947277\n",
      "Iteration 174, loss = 0.11153543\n",
      "Iteration 48, loss = 0.27545206\n",
      "Iteration 175, loss = 0.11135486\n",
      "Iteration 254, loss = 0.18085567\n",
      "Iteration 27, loss = 0.41565704\n",
      "Iteration 176, loss = 0.11112687\n",
      "Iteration 91, loss = 0.17489923\n",
      "Iteration 177, loss = 0.11093387\n",
      "Iteration 73, loss = 0.18809587\n",
      "Iteration 92, loss = 0.17404115\n",
      "Iteration 281, loss = 0.07934858\n",
      "Iteration 178, loss = 0.11072249\n",
      "Iteration 282, loss = 0.07922124\n",
      "Iteration 49, loss = 0.27234448\n",
      "Iteration 283, loss = 0.07910699\n",
      "Iteration 50, loss = 0.26937274\n",
      "Iteration 284, loss = 0.07899513\n",
      "Iteration 51, loss = 0.26642577\n",
      "Iteration 285, loss = 0.07887932\n",
      "Iteration 52, loss = 0.26343385Iteration 74, loss = 0.18655935\n",
      "\n",
      "Iteration 286, loss = 0.07874757\n",
      "Iteration 152, loss = 0.23581597\n",
      "Iteration 93, loss = 0.17323054\n",
      "Iteration 287, loss = 0.07865444\n",
      "Iteration 53, loss = 0.26056980\n",
      "Iteration 288, loss = 0.07850388\n",
      "Iteration 75, loss = 0.18508389\n",
      "Iteration 54, loss = 0.25775451\n",
      "Iteration 55, loss = 0.25493658\n",
      "Iteration 28, loss = 0.40976959\n",
      "Iteration 255, loss = 0.18074229\n",
      "Iteration 76, loss = 0.18355621\n",
      "Iteration 179, loss = 0.11052247\n",
      "Iteration 289, loss = 0.07843789\n",
      "Iteration 56, loss = 0.25217580\n",
      "Iteration 180, loss = 0.11031328\n",
      "Iteration 290, loss = 0.07829725\n",
      "Iteration 181, loss = 0.11011152\n",
      "Iteration 94, loss = 0.17236566\n",
      "Iteration 182, loss = 0.10991760\n",
      "Iteration 29, loss = 0.40411374\n",
      "Iteration 291, loss = 0.07820551\n",
      "Iteration 292, loss = 0.07807136\n",
      "Iteration 57, loss = 0.24947254\n",
      "Iteration 293, loss = 0.07795697\n",
      "Iteration 294, loss = 0.07782575\n",
      "Iteration 77, loss = 0.18220058\n",
      "Iteration 153, loss = 0.23557854\n",
      "Iteration 95, loss = 0.17158775\n",
      "Iteration 78, loss = 0.18091051\n",
      "Iteration 58, loss = 0.24684274\n",
      "Iteration 183, loss = 0.10973154\n",
      "Iteration 59, loss = 0.24420783\n",
      "Iteration 30, loss = 0.39874271\n",
      "Iteration 60, loss = 0.24156197\n",
      "Iteration 295, loss = 0.07772913\n",
      "Iteration 256, loss = 0.18067616\n",
      "Iteration 96, loss = 0.17074787\n",
      "Iteration 296, loss = 0.07761702\n",
      "Iteration 79, loss = 0.17956115\n",
      "Iteration 154, loss = 0.23523309\n",
      "Iteration 297, loss = 0.07749636\n",
      "Iteration 61, loss = 0.23897196\n",
      "Iteration 184, loss = 0.10953943\n",
      "Iteration 80, loss = 0.17834368\n",
      "Iteration 185, loss = 0.10934971\n",
      "Iteration 62, loss = 0.23639355\n",
      "Iteration 186, loss = 0.10916395\n",
      "Iteration 298, loss = 0.07741399\n",
      "Iteration 299, loss = 0.07728019\n",
      "Iteration 63, loss = 0.23398065\n",
      "Iteration 187, loss = 0.10895934\n",
      "Iteration 81, loss = 0.17712056\n",
      "Iteration 64, loss = 0.23142891\n",
      "Iteration 300, loss = 0.07719458\n",
      "Iteration 188, loss = 0.10878512\n",
      "Iteration 82, loss = 0.17601190\n",
      "Iteration 301, loss = 0.07708312\n",
      "Iteration 97, loss = 0.17000458\n",
      "Iteration 31, loss = 0.39310381\n",
      "Iteration 302, loss = 0.07697317\n",
      "Iteration 65, loss = 0.22904080\n",
      "Iteration 257, loss = 0.18055829\n",
      "Iteration 83, loss = 0.17482303\n",
      "Iteration 303, loss = 0.07690554\n",
      "Iteration 304, loss = 0.07678426\n",
      "Iteration 305, loss = 0.07665918\n",
      "Iteration 189, loss = 0.10860565\n",
      "Iteration 306, loss = 0.07656455\n",
      "Iteration 98, loss = 0.16928975\n",
      "Iteration 307, loss = 0.07646853\n",
      "Iteration 66, loss = 0.22666197\n",
      "Iteration 308, loss = 0.07633923\n",
      "Iteration 155, loss = 0.23495113\n",
      "Iteration 309, loss = 0.07625020\n",
      "Iteration 99, loss = 0.16848604\n",
      "Iteration 310, loss = 0.07612084\n",
      "Iteration 190, loss = 0.10841261\n",
      "Iteration 67, loss = 0.22425993\n",
      "Iteration 32, loss = 0.38769437\n",
      "Iteration 84, loss = 0.17376384\n",
      "Iteration 191, loss = 0.10821979\n",
      "Iteration 68, loss = 0.22198384\n",
      "Iteration 69, loss = 0.21970695\n",
      "Iteration 33, loss = 0.38232230\n",
      "Iteration 100, loss = 0.16777346\n",
      "Iteration 85, loss = 0.17267561\n",
      "Iteration 192, loss = 0.10805535\n",
      "Iteration 70, loss = 0.21742795\n",
      "Iteration 258, loss = 0.18047889\n",
      "Iteration 71, loss = 0.21520783\n",
      "Iteration 193, loss = 0.10787664\n",
      "Iteration 311, loss = 0.07603817\n",
      "Iteration 86, loss = 0.17165456\n",
      "Iteration 312, loss = 0.07591202\n",
      "Iteration 72, loss = 0.21300701\n",
      "Iteration 313, loss = 0.07581217\n",
      "Iteration 73, loss = 0.21083322\n",
      "Iteration 314, loss = 0.07572426\n",
      "Iteration 74, loss = 0.20871781\n",
      "Iteration 315, loss = 0.07561244\n",
      "Iteration 316, loss = 0.07556829\n",
      "Iteration 75, loss = 0.20663958\n",
      "Iteration 87, loss = 0.17068812\n",
      "Iteration 317, loss = 0.07541676\n",
      "Iteration 156, loss = 0.23466956\n",
      "Iteration 194, loss = 0.10768723\n",
      "Iteration 195, loss = 0.10752458\n",
      "Iteration 101, loss = 0.16706545\n",
      "Iteration 34, loss = 0.37699384\n",
      "Iteration 88, loss = 0.16973854\n",
      "Iteration 76, loss = 0.20453118\n",
      "Iteration 259, loss = 0.18034879\n",
      "Iteration 77, loss = 0.20248513\n",
      "Iteration 102, loss = 0.16638904\n",
      "Iteration 89, loss = 0.16880491\n",
      "Iteration 318, loss = 0.07530685\n",
      "Iteration 90, loss = 0.16789355\n",
      "Iteration 196, loss = 0.10734971\n",
      "Iteration 319, loss = 0.07521685\n",
      "Iteration 197, loss = 0.10717488\n",
      "Iteration 91, loss = 0.16704357\n",
      "Iteration 320, loss = 0.07513071\n",
      "Iteration 35, loss = 0.37175675\n",
      "Iteration 198, loss = 0.10698529\n",
      "Iteration 78, loss = 0.20059685\n",
      "Iteration 79, loss = 0.19859697\n",
      "Iteration 321, loss = 0.07502337\n",
      "Iteration 199, loss = 0.10683641\n",
      "Iteration 103, loss = 0.16569692\n",
      "Iteration 200, loss = 0.10665183\n",
      "Iteration 157, loss = 0.23441783\n",
      "Iteration 322, loss = 0.07492964\n",
      "Iteration 323, loss = 0.07481653\n",
      "Iteration 80, loss = 0.19663949\n",
      "Iteration 324, loss = 0.07472935\n",
      "Iteration 201, loss = 0.10648781\n",
      "Iteration 36, loss = 0.36656085\n",
      "Iteration 104, loss = 0.16503941\n",
      "Iteration 92, loss = 0.16619950\n",
      "Iteration 260, loss = 0.18035124\n",
      "Iteration 325, loss = 0.07463322\n",
      "Iteration 81, loss = 0.19478002\n",
      "Iteration 82, loss = 0.19298326\n",
      "Iteration 202, loss = 0.10631294\n",
      "Iteration 83, loss = 0.19109277\n",
      "Iteration 93, loss = 0.16534879\n",
      "Iteration 84, loss = 0.18931454\n",
      "Iteration 105, loss = 0.16438258\n",
      "Iteration 326, loss = 0.07452007\n",
      "Iteration 327, loss = 0.07443444\n",
      "Iteration 85, loss = 0.18752917\n",
      "Iteration 328, loss = 0.07433771\n",
      "Iteration 86, loss = 0.18582272\n",
      "Iteration 158, loss = 0.23404855\n",
      "Iteration 203, loss = 0.10616860\n",
      "Iteration 94, loss = 0.16455711\n",
      "Iteration 261, loss = 0.18017537\n",
      "Iteration 37, loss = 0.36140597\n",
      "Iteration 95, loss = 0.16376696\n",
      "Iteration 329, loss = 0.07424137\n",
      "Iteration 87, loss = 0.18407364\n",
      "Iteration 330, loss = 0.07415743\n",
      "Iteration 331, loss = 0.07406759\n",
      "Iteration 332, loss = 0.07397655\n",
      "Iteration 204, loss = 0.10597963\n",
      "Iteration 106, loss = 0.16373959\n",
      "Iteration 205, loss = 0.10582655\n",
      "Iteration 88, loss = 0.18242616\n",
      "Iteration 333, loss = 0.07387255\n",
      "Iteration 96, loss = 0.16300117\n",
      "Iteration 107, loss = 0.16316055\n",
      "Iteration 206, loss = 0.10565189\n",
      "Iteration 262, loss = 0.18004424\n",
      "Iteration 207, loss = 0.10549821\n",
      "Iteration 97, loss = 0.16226497\n",
      "Iteration 334, loss = 0.07379932\n",
      "Iteration 335, loss = 0.07369530\n",
      "Iteration 208, loss = 0.10533285\n",
      "Iteration 209, loss = 0.10518819\n",
      "Iteration 89, loss = 0.18069994\n",
      "Iteration 210, loss = 0.10501856\n",
      "Iteration 38, loss = 0.35637584\n",
      "Iteration 211, loss = 0.10485305\n",
      "Iteration 336, loss = 0.07360863\n",
      "Iteration 337, loss = 0.07351753\n",
      "Iteration 90, loss = 0.17914555\n",
      "Iteration 159, loss = 0.23379644\n",
      "Iteration 91, loss = 0.17742721\n",
      "Iteration 98, loss = 0.16155526\n",
      "Iteration 338, loss = 0.07342952\n",
      "Iteration 92, loss = 0.17582991\n",
      "Iteration 339, loss = 0.07333319\n",
      "Iteration 108, loss = 0.16252921\n",
      "Iteration 109, loss = 0.16193205\n",
      "Iteration 340, loss = 0.07326296\n",
      "Iteration 39, loss = 0.35124656\n",
      "Iteration 341, loss = 0.07320823\n",
      "Iteration 99, loss = 0.16082339\n",
      "Iteration 93, loss = 0.17431682\n",
      "Iteration 212, loss = 0.10472171\n",
      "Iteration 110, loss = 0.16137411\n",
      "Iteration 342, loss = 0.07309539\n",
      "Iteration 94, loss = 0.17271998Iteration 263, loss = 0.18000970\n",
      "\n",
      "Iteration 100, loss = 0.16015174\n",
      "Iteration 343, loss = 0.07303282\n",
      "Iteration 344, loss = 0.07291986\n",
      "Iteration 213, loss = 0.10455799\n",
      "Iteration 160, loss = 0.23346079\n",
      "Iteration 345, loss = 0.07282006\n",
      "Iteration 346, loss = 0.07273997\n",
      "Iteration 95, loss = 0.17114733\n",
      "Iteration 40, loss = 0.34641484\n",
      "Iteration 96, loss = 0.16964471\n",
      "Iteration 97, loss = 0.16819291\n",
      "Iteration 98, loss = 0.16670323\n",
      "Iteration 111, loss = 0.16078634\n",
      "Iteration 214, loss = 0.10440586\n",
      "Iteration 347, loss = 0.07269160\n",
      "Iteration 264, loss = 0.17997187\n",
      "Iteration 99, loss = 0.16514916\n",
      "Iteration 101, loss = 0.15947104\n",
      "Iteration 348, loss = 0.07259491\n",
      "Iteration 349, loss = 0.07253037\n",
      "Iteration 41, loss = 0.34158680\n",
      "Iteration 100, loss = 0.16368639\n",
      "Iteration 350, loss = 0.07243305\n",
      "Iteration 215, loss = 0.10425228\n",
      "Iteration 351, loss = 0.07234381\n",
      "Iteration 352, loss = 0.07226866\n",
      "Iteration 102, loss = 0.15882735\n",
      "Iteration 353, loss = 0.07220574\n",
      "Iteration 216, loss = 0.10409601\n",
      "Iteration 354, loss = 0.07209528\n",
      "Iteration 101, loss = 0.16228424\n",
      "Iteration 42, loss = 0.33676235\n",
      "Iteration 355, loss = 0.07201092\n",
      "Iteration 217, loss = 0.10394201\n",
      "Iteration 356, loss = 0.07194558\n",
      "Iteration 112, loss = 0.16022885\n",
      "Iteration 102, loss = 0.16078196\n",
      "Iteration 103, loss = 0.15819005\n",
      "Iteration 103, loss = 0.15949638\n",
      "Iteration 104, loss = 0.15753811\n",
      "Iteration 161, loss = 0.23325434\n",
      "Iteration 218, loss = 0.10379029\n",
      "Iteration 265, loss = 0.17980466\n",
      "Iteration 104, loss = 0.15803476\n",
      "Iteration 357, loss = 0.07187025\n",
      "Iteration 113, loss = 0.15966000\n",
      "Iteration 105, loss = 0.15665921\n",
      "Iteration 43, loss = 0.33204379\n",
      "Iteration 105, loss = 0.15692574\n",
      "Iteration 358, loss = 0.07177008\n",
      "Iteration 114, loss = 0.15915119\n",
      "Iteration 219, loss = 0.10364325\n",
      "Iteration 106, loss = 0.15532163\n",
      "Iteration 107, loss = 0.15403970\n",
      "Iteration 359, loss = 0.07171574\n",
      "Iteration 106, loss = 0.15632004\n",
      "Iteration 107, loss = 0.15573651\n",
      "Iteration 108, loss = 0.15272699\n",
      "Iteration 162, loss = 0.23296171\n",
      "Iteration 108, loss = 0.15514490\n",
      "Iteration 220, loss = 0.10350617\n",
      "Iteration 266, loss = 0.17968493\n",
      "Iteration 109, loss = 0.15143121\n",
      "Iteration 115, loss = 0.15860326\n",
      "Iteration 360, loss = 0.07162227\n",
      "Iteration 109, loss = 0.15455487\n",
      "Iteration 44, loss = 0.32734376\n",
      "Iteration 110, loss = 0.15021938\n",
      "Iteration 111, loss = 0.14895371\n",
      "Iteration 361, loss = 0.07154789\n",
      "Iteration 221, loss = 0.10335133\n",
      "Iteration 112, loss = 0.14776651\n",
      "Iteration 362, loss = 0.07146697\n",
      "Iteration 222, loss = 0.10321393\n",
      "Iteration 363, loss = 0.07140787\n",
      "Iteration 116, loss = 0.15809325\n",
      "Iteration 113, loss = 0.14658517\n",
      "Iteration 364, loss = 0.07133070\n",
      "Iteration 223, loss = 0.10309070\n",
      "Iteration 163, loss = 0.23268169\n",
      "Iteration 110, loss = 0.15401130\n",
      "Iteration 114, loss = 0.14541642\n",
      "Iteration 365, loss = 0.07123535\n",
      "Iteration 115, loss = 0.14423013\n",
      "Iteration 45, loss = 0.32290054\n",
      "Iteration 267, loss = 0.17961879\n",
      "Iteration 224, loss = 0.10292329\n",
      "Iteration 117, loss = 0.15756748\n",
      "Iteration 366, loss = 0.07117190\n",
      "Iteration 367, loss = 0.07107851\n",
      "Iteration 368, loss = 0.07101473\n",
      "Iteration 116, loss = 0.14312332\n",
      "Iteration 225, loss = 0.10278230Iteration 46, loss = 0.31848109\n",
      "\n",
      "Iteration 111, loss = 0.15346808\n",
      "Iteration 117, loss = 0.14199374\n",
      "Iteration 369, loss = 0.07094498\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 268, loss = 0.17949323\n",
      "Iteration 164, loss = 0.23238790\n",
      "Iteration 118, loss = 0.15707484\n",
      "Iteration 1, loss = 0.83085875\n",
      "Iteration 47, loss = 0.31405787\n",
      "Iteration 118, loss = 0.14090562\n",
      "Iteration 226, loss = 0.10265509\n",
      "Iteration 112, loss = 0.15290638\n",
      "Iteration 113, loss = 0.15238951\n",
      "Iteration 227, loss = 0.10250634\n",
      "Iteration 119, loss = 0.15659429\n",
      "Iteration 119, loss = 0.13984523\n",
      "Iteration 120, loss = 0.15607987\n",
      "Iteration 228, loss = 0.10237120\n",
      "Iteration 120, loss = 0.13878265\n",
      "Iteration 269, loss = 0.17942240\n",
      "Iteration 2, loss = 0.79184595\n",
      "Iteration 121, loss = 0.13777544\n",
      "Iteration 114, loss = 0.15186575\n",
      "Iteration 122, loss = 0.13678759\n",
      "Iteration 229, loss = 0.10224524\n",
      "Iteration 123, loss = 0.13576918\n",
      "Iteration 230, loss = 0.10209356\n",
      "Iteration 3, loss = 0.74086146\n",
      "Iteration 165, loss = 0.23211661\n",
      "Iteration 231, loss = 0.10196027\n",
      "Iteration 232, loss = 0.10182796\n",
      "Iteration 48, loss = 0.30973383\n",
      "Iteration 121, loss = 0.15561582\n",
      "Iteration 233, loss = 0.10169330\n",
      "Iteration 115, loss = 0.15135106\n",
      "Iteration 124, loss = 0.13477383\n",
      "Iteration 116, loss = 0.15082466\n",
      "Iteration 125, loss = 0.13383356\n",
      "Iteration 126, loss = 0.13290752\n",
      "Iteration 117, loss = 0.15034846\n",
      "Iteration 234, loss = 0.10158516\n",
      "Iteration 4, loss = 0.68973402\n",
      "Iteration 270, loss = 0.17933124\n",
      "Iteration 127, loss = 0.13198008\n",
      "Iteration 166, loss = 0.23187246\n",
      "Iteration 128, loss = 0.13108306\n",
      "Iteration 118, loss = 0.14983602\n",
      "Iteration 129, loss = 0.13024102\n",
      "Iteration 5, loss = 0.64363723\n",
      "Iteration 122, loss = 0.15513853\n",
      "Iteration 119, loss = 0.14933790\n",
      "Iteration 271, loss = 0.17925603\n",
      "Iteration 49, loss = 0.30560862\n",
      "Iteration 235, loss = 0.10143268\n",
      "Iteration 236, loss = 0.10129748\n",
      "Iteration 130, loss = 0.12939297\n",
      "Iteration 123, loss = 0.15466093\n",
      "Iteration 131, loss = 0.12853587\n",
      "Iteration 120, loss = 0.14885377\n",
      "Iteration 50, loss = 0.30142015\n",
      "Iteration 132, loss = 0.12776711\n",
      "Iteration 121, loss = 0.14840983\n",
      "Iteration 237, loss = 0.10116335\n",
      "Iteration 133, loss = 0.12691346\n",
      "Iteration 122, loss = 0.14792423\n",
      "Iteration 124, loss = 0.15422383\n",
      "Iteration 6, loss = 0.60352905\n",
      "Iteration 134, loss = 0.12612324\n",
      "Iteration 272, loss = 0.17913855\n",
      "Iteration 7, loss = 0.57017770\n",
      "Iteration 135, loss = 0.12535208Iteration 167, loss = 0.23162387\n",
      "\n",
      "Iteration 238, loss = 0.10102340\n",
      "Iteration 273, loss = 0.17906776\n",
      "Iteration 239, loss = 0.10089858\n",
      "Iteration 125, loss = 0.15375194\n",
      "Iteration 136, loss = 0.12459667\n",
      "Iteration 51, loss = 0.29749810\n",
      "Iteration 137, loss = 0.12384757\n",
      "Iteration 8, loss = 0.54304799\n",
      "Iteration 240, loss = 0.10078322\n",
      "Iteration 138, loss = 0.12314761\n",
      "Iteration 123, loss = 0.14744021\n",
      "Iteration 241, loss = 0.10064612\n",
      "Iteration 126, loss = 0.15331913\n",
      "Iteration 9, loss = 0.51992669\n",
      "Iteration 242, loss = 0.10052896\n",
      "Iteration 52, loss = 0.29371959\n",
      "Iteration 139, loss = 0.12243689\n",
      "Iteration 243, loss = 0.10038586\n",
      "Iteration 127, loss = 0.15286872\n",
      "Iteration 124, loss = 0.14698476\n",
      "Iteration 168, loss = 0.23146069\n",
      "Iteration 140, loss = 0.12174882\n",
      "Iteration 141, loss = 0.12107246\n",
      "Iteration 142, loss = 0.12043875\n",
      "Iteration 244, loss = 0.10026400\n",
      "Iteration 143, loss = 0.11978855\n",
      "Iteration 125, loss = 0.14652879\n",
      "Iteration 144, loss = 0.11914206\n",
      "Iteration 145, loss = 0.11853724\n",
      "Iteration 128, loss = 0.15243042\n",
      "Iteration 10, loss = 0.50096427\n",
      "Iteration 53, loss = 0.28987822\n",
      "Iteration 11, loss = 0.48522324\n",
      "Iteration 126, loss = 0.14610740\n",
      "Iteration 129, loss = 0.15201752\n",
      "Iteration 245, loss = 0.10013274\n",
      "Iteration 169, loss = 0.23115537\n",
      "Iteration 274, loss = 0.17894440\n",
      "Iteration 127, loss = 0.14564392\n",
      "Iteration 146, loss = 0.11796956\n",
      "Iteration 12, loss = 0.47223284\n",
      "Iteration 54, loss = 0.28610881\n",
      "Iteration 147, loss = 0.11736074\n",
      "Iteration 128, loss = 0.14521763\n",
      "Iteration 246, loss = 0.10005191\n",
      "Iteration 13, loss = 0.46111836\n",
      "Iteration 148, loss = 0.11676218\n",
      "Iteration 247, loss = 0.09988688\n",
      "Iteration 14, loss = 0.45132173\n",
      "Iteration 130, loss = 0.15161083\n",
      "Iteration 248, loss = 0.09976694\n",
      "Iteration 129, loss = 0.14478707\n",
      "Iteration 15, loss = 0.44306305\n",
      "Iteration 131, loss = 0.15119651\n",
      "Iteration 275, loss = 0.17888841\n",
      "Iteration 149, loss = 0.11623233\n",
      "Iteration 55, loss = 0.28253495\n",
      "Iteration 130, loss = 0.14437152\n",
      "Iteration 170, loss = 0.23087203\n",
      "Iteration 150, loss = 0.11563776\n",
      "Iteration 249, loss = 0.09964947\n",
      "Iteration 131, loss = 0.14396299\n",
      "Iteration 151, loss = 0.11510521\n",
      "Iteration 250, loss = 0.09953451\n",
      "Iteration 132, loss = 0.15077614\n",
      "Iteration 152, loss = 0.11456946\n",
      "Iteration 153, loss = 0.11407276\n",
      "Iteration 132, loss = 0.14350784\n",
      "Iteration 56, loss = 0.27909965\n",
      "Iteration 276, loss = 0.17878764\n",
      "Iteration 154, loss = 0.11356303\n",
      "Iteration 251, loss = 0.09942245\n",
      "Iteration 133, loss = 0.15038891\n",
      "Iteration 155, loss = 0.11307306Iteration 16, loss = 0.43556866\n",
      "\n",
      "Iteration 133, loss = 0.14310674\n",
      "Iteration 252, loss = 0.09931213\n",
      "Iteration 57, loss = 0.27573871\n",
      "Iteration 156, loss = 0.11258605\n",
      "Iteration 171, loss = 0.23063757\n",
      "Iteration 134, loss = 0.14271233\n",
      "Iteration 253, loss = 0.09917676\n",
      "Iteration 157, loss = 0.11208349\n",
      "Iteration 17, loss = 0.42888549\n",
      "Iteration 254, loss = 0.09905351\n",
      "Iteration 134, loss = 0.14998946\n",
      "Iteration 18, loss = 0.42256752\n",
      "Iteration 158, loss = 0.11161507\n",
      "Iteration 255, loss = 0.09894038\n",
      "Iteration 277, loss = 0.17873734\n",
      "Iteration 135, loss = 0.14231018\n",
      "Iteration 58, loss = 0.27246195\n",
      "Iteration 135, loss = 0.14962283\n",
      "Iteration 136, loss = 0.14193631\n",
      "Iteration 256, loss = 0.09883671\n",
      "Iteration 159, loss = 0.11112315\n",
      "Iteration 172, loss = 0.23039143\n",
      "Iteration 137, loss = 0.14153593\n",
      "Iteration 160, loss = 0.11068505\n",
      "Iteration 19, loss = 0.41670375\n",
      "Iteration 136, loss = 0.14924382\n",
      "Iteration 161, loss = 0.11024005\n",
      "Iteration 162, loss = 0.10979334\n",
      "Iteration 163, loss = 0.10933512\n",
      "Iteration 138, loss = 0.14114604\n",
      "Iteration 257, loss = 0.09871156\n",
      "Iteration 59, loss = 0.26930629\n",
      "Iteration 139, loss = 0.14076124\n",
      "Iteration 258, loss = 0.09860484\n",
      "Iteration 137, loss = 0.14887809\n",
      "Iteration 164, loss = 0.10890116\n",
      "Iteration 259, loss = 0.09848520\n",
      "Iteration 20, loss = 0.41120768\n",
      "Iteration 278, loss = 0.17861933\n",
      "Iteration 165, loss = 0.10851183\n",
      "Iteration 260, loss = 0.09837619\n",
      "Iteration 166, loss = 0.10807570\n",
      "Iteration 140, loss = 0.14040432Iteration 21, loss = 0.40579165\n",
      "\n",
      "Iteration 167, loss = 0.10766254\n",
      "Iteration 173, loss = 0.23017160\n",
      "Iteration 261, loss = 0.09825290\n",
      "Iteration 138, loss = 0.14851492\n",
      "Iteration 168, loss = 0.10724221\n",
      "Iteration 22, loss = 0.40055630\n",
      "Iteration 169, loss = 0.10686813\n",
      "Iteration 170, loss = 0.10645570\n",
      "Iteration 262, loss = 0.09814600\n",
      "Iteration 60, loss = 0.26616803\n",
      "Iteration 171, loss = 0.10610972\n",
      "Iteration 23, loss = 0.39536638\n",
      "Iteration 279, loss = 0.17855499\n",
      "Iteration 141, loss = 0.14003656\n",
      "Iteration 139, loss = 0.14814626\n",
      "Iteration 140, loss = 0.14779349\n",
      "Iteration 263, loss = 0.09802583\n",
      "Iteration 172, loss = 0.10572571\n",
      "Iteration 61, loss = 0.26328457\n",
      "Iteration 142, loss = 0.13965968\n",
      "Iteration 174, loss = 0.22991340\n",
      "Iteration 143, loss = 0.13927303\n",
      "Iteration 173, loss = 0.10531352\n",
      "Iteration 264, loss = 0.09793357\n",
      "Iteration 24, loss = 0.39027561\n",
      "Iteration 174, loss = 0.10496378\n",
      "Iteration 144, loss = 0.13890595\n",
      "Iteration 265, loss = 0.09781381\n",
      "Iteration 280, loss = 0.17845557\n",
      "Iteration 25, loss = 0.38547291\n",
      "Iteration 266, loss = 0.09770222\n",
      "Iteration 267, loss = 0.09759172\n",
      "Iteration 26, loss = 0.38044562\n",
      "Iteration 141, loss = 0.14740262\n",
      "Iteration 268, loss = 0.09749152\n",
      "Iteration 62, loss = 0.26042385\n",
      "Iteration 281, loss = 0.17837142\n",
      "Iteration 175, loss = 0.10460684\n",
      "Iteration 269, loss = 0.09737939\n",
      "Iteration 27, loss = 0.37574521\n",
      "Iteration 176, loss = 0.10425623\n",
      "Iteration 270, loss = 0.09729286\n",
      "Iteration 177, loss = 0.10391666\n",
      "Iteration 271, loss = 0.09715716\n",
      "Iteration 178, loss = 0.10356817\n",
      "Iteration 179, loss = 0.10321794\n",
      "Iteration 272, loss = 0.09705333\n",
      "Iteration 142, loss = 0.14706946\n",
      "Iteration 180, loss = 0.10287710\n",
      "Iteration 145, loss = 0.13859994\n",
      "Iteration 181, loss = 0.10253350\n",
      "Iteration 28, loss = 0.37083112\n",
      "Iteration 63, loss = 0.25768472\n",
      "Iteration 175, loss = 0.22976846\n",
      "Iteration 182, loss = 0.10221258\n",
      "Iteration 143, loss = 0.14670493\n",
      "Iteration 273, loss = 0.09694281\n",
      "Iteration 274, loss = 0.09684501\n",
      "Iteration 29, loss = 0.36619110\n",
      "Iteration 146, loss = 0.13818744\n",
      "Iteration 183, loss = 0.10188152\n",
      "Iteration 184, loss = 0.10157561\n",
      "Iteration 185, loss = 0.10124031\n",
      "Iteration 147, loss = 0.13786731\n",
      "Iteration 144, loss = 0.14637667\n",
      "Iteration 282, loss = 0.17830198\n",
      "Iteration 186, loss = 0.10092077\n",
      "Iteration 64, loss = 0.25500306\n",
      "Iteration 145, loss = 0.14602124\n",
      "Iteration 275, loss = 0.09674935\n",
      "Iteration 148, loss = 0.13751067\n",
      "Iteration 187, loss = 0.10059250\n",
      "Iteration 30, loss = 0.36147871\n",
      "Iteration 188, loss = 0.10028952\n",
      "Iteration 176, loss = 0.22945365\n",
      "Iteration 276, loss = 0.09663188\n",
      "Iteration 31, loss = 0.35685795\n",
      "Iteration 146, loss = 0.14568532\n",
      "Iteration 149, loss = 0.13714859\n",
      "Iteration 277, loss = 0.09651988\n",
      "Iteration 283, loss = 0.17819361\n",
      "Iteration 147, loss = 0.14537132\n",
      "Iteration 32, loss = 0.35229321\n",
      "Iteration 150, loss = 0.13681725\n",
      "Iteration 189, loss = 0.09999418\n",
      "Iteration 278, loss = 0.09643081\n",
      "Iteration 65, loss = 0.25244660\n",
      "Iteration 279, loss = 0.09632832\n",
      "Iteration 280, loss = 0.09621414\n",
      "Iteration 281, loss = 0.09612193\n",
      "Iteration 282, loss = 0.09603251\n",
      "Iteration 190, loss = 0.09969728\n",
      "Iteration 191, loss = 0.09940667\n",
      "Iteration 192, loss = 0.09913152\n",
      "Iteration 193, loss = 0.09880888\n",
      "Iteration 151, loss = 0.13650192\n",
      "Iteration 148, loss = 0.14503288\n",
      "Iteration 152, loss = 0.13616755\n",
      "Iteration 33, loss = 0.34780964\n",
      "Iteration 194, loss = 0.09857815\n",
      "Iteration 283, loss = 0.09590729\n",
      "Iteration 195, loss = 0.09830968\n",
      "Iteration 34, loss = 0.34358826\n",
      "Iteration 196, loss = 0.09803583\n",
      "Iteration 284, loss = 0.09583062\n",
      "Iteration 177, loss = 0.22931303\n",
      "Iteration 285, loss = 0.09571363\n",
      "Iteration 35, loss = 0.33922828\n",
      "Iteration 153, loss = 0.13582616\n",
      "Iteration 286, loss = 0.09561526\n",
      "Iteration 66, loss = 0.24995188\n",
      "Iteration 154, loss = 0.13552223\n",
      "Iteration 284, loss = 0.17811980\n",
      "Iteration 149, loss = 0.14472746\n",
      "Iteration 150, loss = 0.14439546\n",
      "Iteration 36, loss = 0.33492072\n",
      "Iteration 197, loss = 0.09771432\n",
      "Iteration 67, loss = 0.24758043\n",
      "Iteration 155, loss = 0.13518844\n",
      "Iteration 198, loss = 0.09748521\n",
      "Iteration 287, loss = 0.09551322\n",
      "Iteration 199, loss = 0.09719018\n",
      "Iteration 200, loss = 0.09694373\n",
      "Iteration 68, loss = 0.24527476\n",
      "Iteration 288, loss = 0.09541013\n",
      "Iteration 201, loss = 0.09667231\n",
      "Iteration 151, loss = 0.14408687\n",
      "Iteration 285, loss = 0.17803885\n",
      "Iteration 202, loss = 0.09648049\n",
      "Iteration 289, loss = 0.09532855\n",
      "Iteration 290, loss = 0.09522249\n",
      "Iteration 291, loss = 0.09511873\n",
      "Iteration 156, loss = 0.13486490\n",
      "Iteration 203, loss = 0.09619423\n",
      "Iteration 152, loss = 0.14377097\n",
      "Iteration 178, loss = 0.22900717\n",
      "Iteration 157, loss = 0.13453803\n",
      "Iteration 37, loss = 0.33059899\n",
      "Iteration 204, loss = 0.09591448\n",
      "Iteration 153, loss = 0.14346659\n",
      "Iteration 286, loss = 0.17796450\n",
      "Iteration 205, loss = 0.09567206\n",
      "Iteration 206, loss = 0.09545585\n",
      "Iteration 154, loss = 0.14316212\n",
      "Iteration 207, loss = 0.09519776\n",
      "Iteration 38, loss = 0.32657146\n",
      "Iteration 292, loss = 0.09502865\n",
      "Iteration 158, loss = 0.13422908\n",
      "Iteration 69, loss = 0.24297275\n",
      "Iteration 293, loss = 0.09495071\n",
      "Iteration 159, loss = 0.13392989\n",
      "Iteration 208, loss = 0.09495638\n",
      "Iteration 209, loss = 0.09473189\n",
      "Iteration 160, loss = 0.13360950\n",
      "Iteration 294, loss = 0.09483925\n",
      "Iteration 39, loss = 0.32232457\n",
      "Iteration 210, loss = 0.09450503\n",
      "Iteration 179, loss = 0.22880064Iteration 155, loss = 0.14286665\n",
      "\n",
      "Iteration 161, loss = 0.13330561\n",
      "Iteration 295, loss = 0.09474168\n",
      "Iteration 287, loss = 0.17789579\n",
      "Iteration 211, loss = 0.09426973\n",
      "Iteration 70, loss = 0.24088403\n",
      "Iteration 212, loss = 0.09405519\n",
      "Iteration 40, loss = 0.31815547\n",
      "Iteration 41, loss = 0.31408680\n",
      "Iteration 296, loss = 0.09464879\n",
      "Iteration 156, loss = 0.14255361\n",
      "Iteration 213, loss = 0.09383027\n",
      "Iteration 214, loss = 0.09360477\n",
      "Iteration 297, loss = 0.09455138\n",
      "Iteration 162, loss = 0.13301518\n",
      "Iteration 298, loss = 0.09447571\n",
      "Iteration 215, loss = 0.09341898\n",
      "Iteration 216, loss = 0.09316619\n",
      "Iteration 180, loss = 0.22857910\n",
      "Iteration 163, loss = 0.13272887\n",
      "Iteration 217, loss = 0.09296914\n",
      "Iteration 71, loss = 0.23875811\n",
      "Iteration 157, loss = 0.14228014\n",
      "Iteration 288, loss = 0.17777483\n",
      "Iteration 42, loss = 0.31008739\n",
      "Iteration 299, loss = 0.09438994\n",
      "Iteration 164, loss = 0.13240310\n",
      "Iteration 300, loss = 0.09427824\n",
      "Iteration 165, loss = 0.13210949\n",
      "Iteration 289, loss = 0.17770536\n",
      "Iteration 301, loss = 0.09417632\n",
      "Iteration 158, loss = 0.14198666\n",
      "Iteration 166, loss = 0.13184343\n",
      "Iteration 218, loss = 0.09278873\n",
      "Iteration 72, loss = 0.23682630\n",
      "Iteration 43, loss = 0.30616339\n",
      "Iteration 219, loss = 0.09257227\n",
      "Iteration 159, loss = 0.14169181\n",
      "Iteration 220, loss = 0.09233508\n",
      "Iteration 221, loss = 0.09215992\n",
      "Iteration 181, loss = 0.22839502\n",
      "Iteration 222, loss = 0.09200228\n",
      "Iteration 302, loss = 0.09409022\n",
      "Iteration 223, loss = 0.09180473\n",
      "Iteration 73, loss = 0.23484484\n",
      "Iteration 224, loss = 0.09156485\n",
      "Iteration 44, loss = 0.30224982\n",
      "Iteration 290, loss = 0.17766435\n",
      "Iteration 45, loss = 0.29857549\n",
      "Iteration 303, loss = 0.09399673\n",
      "Iteration 160, loss = 0.14142532\n",
      "Iteration 167, loss = 0.13152104\n",
      "Iteration 161, loss = 0.14114797\n",
      "Iteration 46, loss = 0.29454803\n",
      "Iteration 168, loss = 0.13123856\n",
      "Iteration 225, loss = 0.09136675\n",
      "Iteration 226, loss = 0.09117816\n",
      "Iteration 304, loss = 0.09391106\n",
      "Iteration 227, loss = 0.09098580\n",
      "Iteration 74, loss = 0.23302994\n",
      "Iteration 228, loss = 0.09081862\n",
      "Iteration 305, loss = 0.09381177\n",
      "Iteration 229, loss = 0.09061031\n",
      "Iteration 306, loss = 0.09372541\n",
      "Iteration 291, loss = 0.17756202\n",
      "Iteration 307, loss = 0.09364672\n",
      "Iteration 162, loss = 0.14087821\n",
      "Iteration 169, loss = 0.13098296\n",
      "Iteration 47, loss = 0.29088624\n",
      "Iteration 75, loss = 0.23118430\n",
      "Iteration 170, loss = 0.13069380\n",
      "Iteration 230, loss = 0.09041149\n",
      "Iteration 182, loss = 0.22818357\n",
      "Iteration 231, loss = 0.09026653\n",
      "Iteration 232, loss = 0.09007606\n",
      "Iteration 233, loss = 0.08990004\n",
      "Iteration 163, loss = 0.14058357\n",
      "Iteration 48, loss = 0.28714475\n",
      "Iteration 234, loss = 0.08973539\n",
      "Iteration 308, loss = 0.09355310\n",
      "Iteration 235, loss = 0.08957648\n",
      "Iteration 76, loss = 0.22950243\n",
      "Iteration 183, loss = 0.22788404\n",
      "Iteration 171, loss = 0.13038026\n",
      "Iteration 309, loss = 0.09348553\n",
      "Iteration 292, loss = 0.17749544\n",
      "Iteration 310, loss = 0.09337134\n",
      "Iteration 164, loss = 0.14032595\n",
      "Iteration 311, loss = 0.09327218\n",
      "Iteration 236, loss = 0.08937350\n",
      "Iteration 172, loss = 0.13013195\n",
      "Iteration 237, loss = 0.08920003\n",
      "Iteration 49, loss = 0.28352253\n",
      "Iteration 238, loss = 0.08904591\n",
      "Iteration 239, loss = 0.08888452\n",
      "Iteration 77, loss = 0.22779849\n",
      "Iteration 240, loss = 0.08873503\n",
      "Iteration 241, loss = 0.08853369\n",
      "Iteration 312, loss = 0.09318917\n",
      "Iteration 313, loss = 0.09310416\n",
      "Iteration 50, loss = 0.28000017\n",
      "Iteration 165, loss = 0.14003835\n",
      "Iteration 173, loss = 0.12985987\n",
      "Iteration 314, loss = 0.09302487\n",
      "Iteration 166, loss = 0.13976826\n",
      "Iteration 242, loss = 0.08838283Iteration 293, loss = 0.17739767\n",
      "\n",
      "Iteration 243, loss = 0.08821044\n",
      "Iteration 174, loss = 0.12957580\n",
      "Iteration 51, loss = 0.27663224\n",
      "Iteration 78, loss = 0.22615406\n",
      "Iteration 184, loss = 0.22768818\n",
      "Iteration 244, loss = 0.08807334\n",
      "Iteration 245, loss = 0.08789778\n",
      "Iteration 246, loss = 0.08775988\n",
      "Iteration 167, loss = 0.13952093\n",
      "Iteration 52, loss = 0.27309702\n",
      "Iteration 175, loss = 0.12932380\n",
      "Iteration 247, loss = 0.08761111\n",
      "Iteration 294, loss = 0.17732201\n",
      "Iteration 248, loss = 0.08745105\n",
      "Iteration 315, loss = 0.09293203\n",
      "Iteration 168, loss = 0.13926632\n",
      "Iteration 79, loss = 0.22467306\n",
      "Iteration 316, loss = 0.09283477\n",
      "Iteration 317, loss = 0.09276648\n",
      "Iteration 176, loss = 0.12902732\n",
      "Iteration 318, loss = 0.09267274\n",
      "Iteration 185, loss = 0.22749849\n",
      "Iteration 249, loss = 0.08727136\n",
      "Iteration 177, loss = 0.12879521\n",
      "Iteration 53, loss = 0.26976123\n",
      "Iteration 295, loss = 0.17723679\n",
      "Iteration 250, loss = 0.08715127\n",
      "Iteration 178, loss = 0.12851658\n",
      "Iteration 251, loss = 0.08698592\n",
      "Iteration 319, loss = 0.09258249\n",
      "Iteration 320, loss = 0.09249434\n",
      "Iteration 80, loss = 0.22306222\n",
      "Iteration 54, loss = 0.26642159\n",
      "Iteration 186, loss = 0.22725073\n",
      "Iteration 321, loss = 0.09240932\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 169, loss = 0.13899705\n",
      "Iteration 252, loss = 0.08684135\n",
      "Iteration 179, loss = 0.12829575\n",
      "Iteration 296, loss = 0.17717436\n",
      "Iteration 170, loss = 0.13873613\n",
      "Iteration 180, loss = 0.12799131Iteration 81, loss = 0.22162129\n",
      "Iteration 253, loss = 0.08667318\n",
      "\n",
      "Iteration 254, loss = 0.08655400\n",
      "Iteration 55, loss = 0.26326598\n",
      "Iteration 255, loss = 0.08639697\n",
      "Iteration 256, loss = 0.08623781\n",
      "Iteration 56, loss = 0.26013125\n",
      "Iteration 181, loss = 0.12775458\n",
      "Iteration 171, loss = 0.13850197\n",
      "Iteration 257, loss = 0.08610261\n",
      "Iteration 172, loss = 0.13825421\n",
      "Iteration 258, loss = 0.08594144\n",
      "Iteration 259, loss = 0.08579935\n",
      "Iteration 260, loss = 0.08566242\n",
      "Iteration 182, loss = 0.12752280\n",
      "Iteration 57, loss = 0.25697391\n",
      "Iteration 1, loss = 0.93377773\n",
      "Iteration 183, loss = 0.12723976\n",
      "Iteration 297, loss = 0.17709800\n",
      "Iteration 58, loss = 0.25409651\n",
      "Iteration 187, loss = 0.22707503\n",
      "Iteration 261, loss = 0.08553888\n",
      "Iteration 262, loss = 0.08540665\n",
      "Iteration 263, loss = 0.08526824\n",
      "Iteration 173, loss = 0.13800515\n",
      "Iteration 59, loss = 0.25108236\n",
      "Iteration 184, loss = 0.12699624\n",
      "Iteration 264, loss = 0.08516895\n",
      "Iteration 265, loss = 0.08500647\n",
      "Iteration 174, loss = 0.13776821\n",
      "Iteration 2, loss = 0.88909779\n",
      "Iteration 266, loss = 0.08484226\n",
      "Iteration 267, loss = 0.08473696\n",
      "Iteration 298, loss = 0.17702741\n",
      "Iteration 268, loss = 0.08463678\n",
      "Iteration 185, loss = 0.12676106\n",
      "Iteration 175, loss = 0.13755031\n",
      "Iteration 82, loss = 0.22018434\n",
      "Iteration 188, loss = 0.22688384\n",
      "Iteration 60, loss = 0.24819124\n",
      "Iteration 269, loss = 0.08448433\n",
      "Iteration 186, loss = 0.12650908\n",
      "Iteration 83, loss = 0.21879083\n",
      "Iteration 61, loss = 0.24541120\n",
      "Iteration 270, loss = 0.08434596\n",
      "Iteration 271, loss = 0.08422222\n",
      "Iteration 272, loss = 0.08411626\n",
      "Iteration 62, loss = 0.24273738\n",
      "Iteration 84, loss = 0.21748147\n",
      "Iteration 187, loss = 0.12627625\n",
      "Iteration 299, loss = 0.17696504\n",
      "Iteration 176, loss = 0.13730435\n",
      "Iteration 3, loss = 0.83058944\n",
      "Iteration 273, loss = 0.08396684Iteration 188, loss = 0.12604961\n",
      "\n",
      "Iteration 274, loss = 0.08385027\n",
      "Iteration 177, loss = 0.13706954\n",
      "Iteration 63, loss = 0.24002053\n",
      "Iteration 275, loss = 0.08372217\n",
      "Iteration 189, loss = 0.12582313\n",
      "Iteration 4, loss = 0.77425687\n",
      "Iteration 276, loss = 0.08363711\n",
      "Iteration 300, loss = 0.17691086\n",
      "Iteration 189, loss = 0.22665469\n",
      "Iteration 64, loss = 0.23753586\n",
      "Iteration 190, loss = 0.12562734\n",
      "Iteration 178, loss = 0.13684381\n",
      "Iteration 277, loss = 0.08351774\n",
      "Iteration 65, loss = 0.23499174\n",
      "Iteration 191, loss = 0.12533861\n",
      "Iteration 85, loss = 0.21616695\n",
      "Iteration 179, loss = 0.13662370\n",
      "Iteration 192, loss = 0.12514341\n",
      "Iteration 5, loss = 0.72383444\n",
      "Iteration 86, loss = 0.21492784\n",
      "Iteration 180, loss = 0.13640597\n",
      "Iteration 6, loss = 0.68086648\n",
      "Iteration 278, loss = 0.08335734\n",
      "Iteration 279, loss = 0.08329857\n",
      "Iteration 280, loss = 0.08316176\n",
      "Iteration 281, loss = 0.08305268\n",
      "Iteration 66, loss = 0.23252774\n",
      "Iteration 301, loss = 0.17679371\n",
      "Iteration 282, loss = 0.08290938\n",
      "Iteration 283, loss = 0.08279182\n",
      "Iteration 181, loss = 0.13616843Iteration 284, loss = 0.08267555\n",
      "\n",
      "Iteration 193, loss = 0.12489542\n",
      "Iteration 190, loss = 0.22648035\n",
      "Iteration 87, loss = 0.21370859\n",
      "Iteration 194, loss = 0.12467867\n",
      "Iteration 302, loss = 0.17674285\n",
      "Iteration 285, loss = 0.08257203\n",
      "Iteration 67, loss = 0.23015439\n",
      "Iteration 286, loss = 0.08245830\n",
      "Iteration 287, loss = 0.08234325\n",
      "Iteration 195, loss = 0.12445090\n",
      "Iteration 191, loss = 0.22629271Iteration 182, loss = 0.13595246\n",
      "\n",
      "Iteration 288, loss = 0.08224618\n",
      "Iteration 68, loss = 0.22781244\n",
      "Iteration 289, loss = 0.08213616\n",
      "Iteration 7, loss = 0.64590150\n",
      "Iteration 196, loss = 0.12425094\n",
      "Iteration 88, loss = 0.21254598\n",
      "Iteration 290, loss = 0.08204549\n",
      "Iteration 183, loss = 0.13572464\n",
      "Iteration 291, loss = 0.08190511\n",
      "Iteration 197, loss = 0.12401701\n",
      "Iteration 69, loss = 0.22557001\n",
      "Iteration 303, loss = 0.17665444\n",
      "Iteration 184, loss = 0.13551118\n",
      "Iteration 198, loss = 0.12380296Iteration 292, loss = 0.08181091\n",
      "\n",
      "Iteration 8, loss = 0.61679141\n",
      "Iteration 192, loss = 0.22604948\n",
      "Iteration 304, loss = 0.17659566\n",
      "Iteration 293, loss = 0.08175705Iteration 70, loss = 0.22343745\n",
      "\n",
      "Iteration 89, loss = 0.21140912\n",
      "Iteration 199, loss = 0.12360053\n",
      "Iteration 185, loss = 0.13529855\n",
      "Iteration 294, loss = 0.08160183\n",
      "Iteration 186, loss = 0.13507221\n",
      "Iteration 200, loss = 0.12338880\n",
      "Iteration 295, loss = 0.08150172\n",
      "Iteration 296, loss = 0.08139656\n",
      "Iteration 9, loss = 0.59359522\n",
      "Iteration 71, loss = 0.22133407\n",
      "Iteration 72, loss = 0.21934639\n",
      "Iteration 297, loss = 0.08127865\n",
      "Iteration 201, loss = 0.12316811\n",
      "Iteration 298, loss = 0.08117865\n",
      "Iteration 202, loss = 0.12297197\n",
      "Iteration 193, loss = 0.22591898\n",
      "Iteration 299, loss = 0.08108777\n",
      "Iteration 305, loss = 0.17652481\n",
      "Iteration 187, loss = 0.13486487\n",
      "Iteration 90, loss = 0.21032721\n",
      "Iteration 300, loss = 0.08100039\n",
      "Iteration 188, loss = 0.13466375\n",
      "Iteration 203, loss = 0.12277844\n",
      "Iteration 73, loss = 0.21735037\n",
      "Iteration 301, loss = 0.08089908\n",
      "Iteration 302, loss = 0.08077567\n",
      "Iteration 204, loss = 0.12256450\n",
      "Iteration 91, loss = 0.20920301\n",
      "Iteration 205, loss = 0.12237617\n",
      "Iteration 189, loss = 0.13443042\n",
      "Iteration 10, loss = 0.57427707\n",
      "Iteration 74, loss = 0.21551617\n",
      "Iteration 303, loss = 0.08067848\n",
      "Iteration 306, loss = 0.17645018\n",
      "Iteration 206, loss = 0.12216681\n",
      "Iteration 304, loss = 0.08058154\n",
      "Iteration 194, loss = 0.22580372\n",
      "Iteration 305, loss = 0.08052696\n",
      "Iteration 190, loss = 0.13421732\n",
      "Iteration 75, loss = 0.21366801\n",
      "Iteration 92, loss = 0.20818918\n",
      "Iteration 191, loss = 0.13402780\n",
      "Iteration 306, loss = 0.08041390\n",
      "Iteration 76, loss = 0.21188405\n",
      "Iteration 307, loss = 0.17639474\n",
      "Iteration 93, loss = 0.20714872\n",
      "Iteration 307, loss = 0.08029862\n",
      "Iteration 308, loss = 0.08025903\n",
      "Iteration 192, loss = 0.13382411\n",
      "Iteration 308, loss = 0.17630147\n",
      "Iteration 309, loss = 0.08014667\n",
      "Iteration 11, loss = 0.55745294\n",
      "Iteration 310, loss = 0.08005285\n",
      "Iteration 94, loss = 0.20618247\n",
      "Iteration 207, loss = 0.12198875\n",
      "Iteration 311, loss = 0.07992418\n",
      "Iteration 195, loss = 0.22553054\n",
      "Iteration 312, loss = 0.07986782\n",
      "Iteration 313, loss = 0.07975528\n",
      "Iteration 208, loss = 0.12178940\n",
      "Iteration 77, loss = 0.21019925\n",
      "Iteration 209, loss = 0.12158604\n",
      "Iteration 12, loss = 0.54352558\n",
      "Iteration 78, loss = 0.20859534\n",
      "Iteration 210, loss = 0.12142306\n",
      "Iteration 193, loss = 0.13361663\n",
      "Iteration 95, loss = 0.20522474\n",
      "Iteration 314, loss = 0.07966189\n",
      "Iteration 211, loss = 0.12121128\n",
      "Iteration 194, loss = 0.13342054\n",
      "Iteration 79, loss = 0.20696628\n",
      "Iteration 80, loss = 0.20543558\n",
      "Iteration 309, loss = 0.17623762\n",
      "Iteration 315, loss = 0.07958745\n",
      "Iteration 212, loss = 0.12105638\n",
      "Iteration 13, loss = 0.53129291\n",
      "Iteration 196, loss = 0.22535818\n",
      "Iteration 316, loss = 0.07948894\n",
      "Iteration 195, loss = 0.13321429\n",
      "Iteration 213, loss = 0.12084670\n",
      "Iteration 317, loss = 0.07937725\n",
      "Iteration 96, loss = 0.20430204\n",
      "Iteration 318, loss = 0.07932270\n",
      "Iteration 319, loss = 0.07922794\n",
      "Iteration 310, loss = 0.17620029\n",
      "Iteration 320, loss = 0.07913105\n",
      "Iteration 214, loss = 0.12067467\n",
      "Iteration 196, loss = 0.13302687\n",
      "Iteration 81, loss = 0.20395716\n",
      "Iteration 215, loss = 0.12049256\n",
      "Iteration 216, loss = 0.12031896\n",
      "Iteration 197, loss = 0.13285211\n",
      "Iteration 82, loss = 0.20247140\n",
      "Iteration 14, loss = 0.52010917\n",
      "Iteration 321, loss = 0.07904145\n",
      "Iteration 322, loss = 0.07897057\n",
      "Iteration 97, loss = 0.20337931\n",
      "Iteration 323, loss = 0.07889850\n",
      "Iteration 324, loss = 0.07878867\n",
      "Iteration 83, loss = 0.20108087\n",
      "Iteration 84, loss = 0.19974824\n",
      "Iteration 217, loss = 0.12013361\n",
      "Iteration 198, loss = 0.13263369\n",
      "Iteration 325, loss = 0.07871859\n",
      "Iteration 311, loss = 0.17612507\n",
      "Iteration 98, loss = 0.20254279\n",
      "Iteration 85, loss = 0.19841148\n",
      "Iteration 197, loss = 0.22513708\n",
      "Iteration 326, loss = 0.07862112\n",
      "Iteration 327, loss = 0.07855572\n",
      "Iteration 15, loss = 0.50978576\n",
      "Iteration 218, loss = 0.11995582\n",
      "Iteration 199, loss = 0.13244662\n",
      "Iteration 198, loss = 0.22494994\n",
      "Iteration 312, loss = 0.17603516\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 328, loss = 0.07847269\n",
      "Iteration 219, loss = 0.11979680\n",
      "Iteration 329, loss = 0.07841244\n",
      "Iteration 86, loss = 0.19711105\n",
      "Iteration 330, loss = 0.07831788\n",
      "Iteration 200, loss = 0.13225629\n",
      "Iteration 331, loss = 0.07824893\n",
      "Iteration 99, loss = 0.20165643\n",
      "Iteration 332, loss = 0.07816641\n",
      "Iteration 87, loss = 0.19590324\n",
      "Iteration 220, loss = 0.11964787\n",
      "Iteration 16, loss = 0.50037397\n",
      "Iteration 221, loss = 0.11946466\n",
      "Iteration 201, loss = 0.13206637\n",
      "Iteration 333, loss = 0.07810419\n",
      "Iteration 1, loss = 0.74152011\n",
      "Iteration 222, loss = 0.11930910\n",
      "Iteration 2, loss = 0.72046001\n",
      "Iteration 3, loss = 0.69213222\n",
      "Iteration 334, loss = 0.07801096\n",
      "Iteration 335, loss = 0.07795146\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 0.66314874\n",
      "Iteration 5, loss = 0.63511291\n",
      "Iteration 202, loss = 0.13187245\n",
      "Iteration 199, loss = 0.22477369\n",
      "Iteration 6, loss = 0.60984667\n",
      "Iteration 100, loss = 0.20082688\n",
      "Iteration 7, loss = 0.58707688\n",
      "Iteration 88, loss = 0.19470981\n",
      "Iteration 89, loss = 0.19355873\n",
      "Iteration 223, loss = 0.11913940\n",
      "Iteration 1, loss = 0.79719631\n",
      "Iteration 203, loss = 0.13171915\n",
      "Iteration 8, loss = 0.56743568\n",
      "Iteration 17, loss = 0.49131983\n",
      "Iteration 2, loss = 0.77330540\n",
      "Iteration 101, loss = 0.20002757\n",
      "Iteration 3, loss = 0.74112450\n",
      "Iteration 224, loss = 0.11896794\n",
      "Iteration 90, loss = 0.19240422\n",
      "Iteration 204, loss = 0.13151221\n",
      "Iteration 102, loss = 0.19922690\n",
      "Iteration 225, loss = 0.11885015\n",
      "Iteration 200, loss = 0.22458890\n",
      "Iteration 9, loss = 0.54978862\n",
      "Iteration 91, loss = 0.19146475\n",
      "Iteration 226, loss = 0.11865616\n",
      "Iteration 18, loss = 0.48255932\n",
      "Iteration 4, loss = 0.70723155Iteration 10, loss = 0.53420723\n",
      "\n",
      "Iteration 92, loss = 0.19024339\n",
      "Iteration 11, loss = 0.51978180\n",
      "Iteration 205, loss = 0.13132881\n",
      "Iteration 206, loss = 0.13114574\n",
      "Iteration 93, loss = 0.18922455\n",
      "Iteration 103, loss = 0.19846611\n",
      "Iteration 227, loss = 0.11847549\n",
      "Iteration 12, loss = 0.50728419\n",
      "Iteration 13, loss = 0.49575949Iteration 201, loss = 0.22440152\n",
      "Iteration 228, loss = 0.11834926\n",
      "\n",
      "Iteration 14, loss = 0.48553443\n",
      "Iteration 104, loss = 0.19772990\n",
      "Iteration 207, loss = 0.13097480\n",
      "Iteration 229, loss = 0.11818145\n",
      "Iteration 19, loss = 0.47448890\n",
      "Iteration 15, loss = 0.47610512Iteration 208, loss = 0.13080716\n",
      "\n",
      "Iteration 94, loss = 0.18821164\n",
      "Iteration 5, loss = 0.67507421\n",
      "Iteration 209, loss = 0.13062926\n",
      "Iteration 105, loss = 0.19700532\n",
      "Iteration 230, loss = 0.11803431\n",
      "Iteration 16, loss = 0.46771019\n",
      "Iteration 17, loss = 0.46009242\n",
      "Iteration 20, loss = 0.46639348\n",
      "Iteration 231, loss = 0.11785277\n",
      "Iteration 18, loss = 0.45279687\n",
      "Iteration 95, loss = 0.18725425\n",
      "Iteration 19, loss = 0.44627171\n",
      "Iteration 6, loss = 0.64599061\n",
      "Iteration 232, loss = 0.11773017\n",
      "Iteration 20, loss = 0.44045782\n",
      "Iteration 106, loss = 0.19627107\n",
      "Iteration 96, loss = 0.18628615\n",
      "Iteration 21, loss = 0.43506911\n",
      "Iteration 202, loss = 0.22419806\n",
      "Iteration 210, loss = 0.13045088\n",
      "Iteration 211, loss = 0.13027197\n",
      "Iteration 107, loss = 0.19558199\n",
      "Iteration 7, loss = 0.61984550\n",
      "Iteration 97, loss = 0.18533321\n",
      "Iteration 233, loss = 0.11755798\n",
      "Iteration 203, loss = 0.22407989\n",
      "Iteration 22, loss = 0.43016839Iteration 234, loss = 0.11742733\n",
      "\n",
      "Iteration 21, loss = 0.45881908\n",
      "Iteration 235, loss = 0.11728865\n",
      "Iteration 23, loss = 0.42571609\n",
      "Iteration 8, loss = 0.59701011\n",
      "Iteration 24, loss = 0.42165112\n",
      "Iteration 25, loss = 0.41798560\n",
      "Iteration 26, loss = 0.41476317\n",
      "Iteration 98, loss = 0.18444952\n",
      "Iteration 212, loss = 0.13011274\n",
      "Iteration 213, loss = 0.12994751\n",
      "Iteration 236, loss = 0.11712334\n",
      "Iteration 27, loss = 0.41167038\n",
      "Iteration 99, loss = 0.18356759\n",
      "Iteration 108, loss = 0.19490957\n",
      "Iteration 22, loss = 0.45106366\n",
      "Iteration 9, loss = 0.57673563\n",
      "Iteration 204, loss = 0.22396949\n",
      "Iteration 237, loss = 0.11702557\n",
      "Iteration 28, loss = 0.40896261\n",
      "Iteration 214, loss = 0.12976944\n",
      "Iteration 29, loss = 0.40639474\n",
      "Iteration 30, loss = 0.40395771\n",
      "Iteration 238, loss = 0.11682503\n",
      "Iteration 100, loss = 0.18267413\n",
      "Iteration 109, loss = 0.19421883\n",
      "Iteration 31, loss = 0.40179336\n",
      "Iteration 101, loss = 0.18181581\n",
      "Iteration 239, loss = 0.11669345\n",
      "Iteration 102, loss = 0.18102909\n",
      "Iteration 23, loss = 0.44382658\n",
      "Iteration 205, loss = 0.22368911\n",
      "Iteration 10, loss = 0.55879818\n",
      "Iteration 32, loss = 0.39989713\n",
      "Iteration 215, loss = 0.12960528\n",
      "Iteration 33, loss = 0.39795406\n",
      "Iteration 34, loss = 0.39612905\n",
      "Iteration 35, loss = 0.39451242\n",
      "Iteration 110, loss = 0.19357927\n",
      "Iteration 240, loss = 0.11653588\n",
      "Iteration 11, loss = 0.54304922\n",
      "Iteration 216, loss = 0.12944305\n",
      "Iteration 241, loss = 0.11639556\n",
      "Iteration 24, loss = 0.43636631\n",
      "Iteration 111, loss = 0.19300635\n",
      "Iteration 36, loss = 0.39287520\n",
      "Iteration 242, loss = 0.11626079\n",
      "Iteration 12, loss = 0.52881304\n",
      "Iteration 103, loss = 0.18023009\n",
      "Iteration 37, loss = 0.39132540\n",
      "Iteration 104, loss = 0.17938262\n",
      "Iteration 217, loss = 0.12926731\n",
      "Iteration 206, loss = 0.22349725\n",
      "Iteration 243, loss = 0.11611709\n",
      "Iteration 13, loss = 0.51646233\n",
      "Iteration 112, loss = 0.19235815\n",
      "Iteration 244, loss = 0.11599440\n",
      "Iteration 38, loss = 0.38982913\n",
      "Iteration 39, loss = 0.38841322\n",
      "Iteration 25, loss = 0.42947993\n",
      "Iteration 218, loss = 0.12910689\n",
      "Iteration 113, loss = 0.19172344\n",
      "Iteration 105, loss = 0.17862439Iteration 40, loss = 0.38699001\n",
      "\n",
      "Iteration 41, loss = 0.38558194\n",
      "Iteration 219, loss = 0.12896518\n",
      "Iteration 207, loss = 0.22338395\n",
      "Iteration 245, loss = 0.11589200\n",
      "Iteration 106, loss = 0.17783627\n",
      "Iteration 42, loss = 0.38426223\n",
      "Iteration 246, loss = 0.11570878\n",
      "Iteration 14, loss = 0.50539581\n",
      "Iteration 107, loss = 0.17709018\n",
      "Iteration 15, loss = 0.49580170\n",
      "Iteration 43, loss = 0.38287150\n",
      "Iteration 114, loss = 0.19117923\n",
      "Iteration 26, loss = 0.42250649\n",
      "Iteration 44, loss = 0.38150645\n",
      "Iteration 220, loss = 0.12878983\n",
      "Iteration 45, loss = 0.38015491\n",
      "Iteration 247, loss = 0.11556679\n",
      "Iteration 108, loss = 0.17634410\n",
      "Iteration 248, loss = 0.11543572\n",
      "Iteration 208, loss = 0.22319036\n",
      "Iteration 46, loss = 0.37881315\n",
      "Iteration 47, loss = 0.37738217\n",
      "Iteration 221, loss = 0.12862553\n",
      "Iteration 48, loss = 0.37601091\n",
      "Iteration 27, loss = 0.41550899\n",
      "Iteration 115, loss = 0.19057567\n",
      "Iteration 49, loss = 0.37460918\n",
      "Iteration 249, loss = 0.11532249\n",
      "Iteration 16, loss = 0.48717751\n",
      "Iteration 50, loss = 0.37318518\n",
      "Iteration 250, loss = 0.11517627Iteration 109, loss = 0.17562614\n",
      "\n",
      "Iteration 222, loss = 0.12848135\n",
      "Iteration 251, loss = 0.11504414\n",
      "Iteration 110, loss = 0.17492621\n",
      "Iteration 209, loss = 0.22299734\n",
      "Iteration 17, loss = 0.47907247\n",
      "Iteration 116, loss = 0.19003076\n",
      "Iteration 51, loss = 0.37179794\n",
      "Iteration 28, loss = 0.40881205\n",
      "Iteration 52, loss = 0.37029792\n",
      "Iteration 252, loss = 0.11493353\n",
      "Iteration 18, loss = 0.47244600\n",
      "Iteration 111, loss = 0.17424035\n",
      "Iteration 223, loss = 0.12831908\n",
      "Iteration 53, loss = 0.36884879\n",
      "Iteration 117, loss = 0.18943940\n",
      "Iteration 224, loss = 0.12816390\n",
      "Iteration 19, loss = 0.46594676\n",
      "Iteration 29, loss = 0.40207336\n",
      "Iteration 253, loss = 0.11478171\n",
      "Iteration 54, loss = 0.36725078\n",
      "Iteration 55, loss = 0.36570782\n",
      "Iteration 56, loss = 0.36408342\n",
      "Iteration 118, loss = 0.18891733\n",
      "Iteration 112, loss = 0.17356038\n",
      "Iteration 254, loss = 0.11465469\n",
      "Iteration 225, loss = 0.12801137\n",
      "Iteration 210, loss = 0.22284660\n",
      "Iteration 20, loss = 0.46054057\n",
      "Iteration 113, loss = 0.17293613Iteration 57, loss = 0.36239947\n",
      "\n",
      "Iteration 58, loss = 0.36071559\n",
      "Iteration 226, loss = 0.12785643\n",
      "Iteration 255, loss = 0.11452615\n",
      "Iteration 211, loss = 0.22268479\n",
      "Iteration 119, loss = 0.18836225\n",
      "Iteration 30, loss = 0.39582912\n",
      "Iteration 59, loss = 0.35901989\n",
      "Iteration 60, loss = 0.35725651\n",
      "Iteration 256, loss = 0.11441803\n",
      "Iteration 61, loss = 0.35545679\n",
      "Iteration 62, loss = 0.35354880\n",
      "Iteration 257, loss = 0.11431197\n",
      "Iteration 21, loss = 0.45512699\n",
      "Iteration 114, loss = 0.17228373\n",
      "Iteration 227, loss = 0.12770437\n",
      "Iteration 115, loss = 0.17160300\n",
      "Iteration 120, loss = 0.18785215\n",
      "Iteration 258, loss = 0.11416276\n",
      "Iteration 22, loss = 0.45031438\n",
      "Iteration 228, loss = 0.12755254\n",
      "Iteration 121, loss = 0.18737028\n",
      "Iteration 31, loss = 0.38937113\n",
      "Iteration 63, loss = 0.35171374\n",
      "Iteration 64, loss = 0.34989879\n",
      "Iteration 65, loss = 0.34804788\n",
      "Iteration 23, loss = 0.44576299\n",
      "Iteration 24, loss = 0.44158162\n",
      "Iteration 116, loss = 0.17098374\n",
      "Iteration 212, loss = 0.22253539\n",
      "Iteration 259, loss = 0.11405455\n",
      "Iteration 66, loss = 0.34617688\n",
      "Iteration 260, loss = 0.11392768\n",
      "Iteration 229, loss = 0.12741660\n",
      "Iteration 67, loss = 0.34424499\n",
      "Iteration 32, loss = 0.38341416\n",
      "Iteration 122, loss = 0.18684512\n",
      "Iteration 261, loss = 0.11381937\n",
      "Iteration 117, loss = 0.17040594\n",
      "Iteration 230, loss = 0.12727092\n",
      "Iteration 118, loss = 0.16981347\n",
      "Iteration 231, loss = 0.12711062\n",
      "Iteration 123, loss = 0.18631838\n",
      "Iteration 213, loss = 0.22238332\n",
      "Iteration 68, loss = 0.34230286\n",
      "Iteration 262, loss = 0.11368039\n",
      "Iteration 69, loss = 0.34031584\n",
      "Iteration 70, loss = 0.33830708\n",
      "Iteration 263, loss = 0.11356952\n",
      "Iteration 71, loss = 0.33625675\n",
      "Iteration 72, loss = 0.33423142\n",
      "Iteration 25, loss = 0.43753070\n",
      "Iteration 33, loss = 0.37751106\n",
      "Iteration 119, loss = 0.16920390\n",
      "Iteration 73, loss = 0.33200987\n",
      "Iteration 232, loss = 0.12696164\n",
      "Iteration 264, loss = 0.11345429\n",
      "Iteration 214, loss = 0.22221445\n",
      "Iteration 26, loss = 0.43368469\n",
      "Iteration 120, loss = 0.16864570\n",
      "Iteration 27, loss = 0.42987774\n",
      "Iteration 121, loss = 0.16803791\n",
      "Iteration 74, loss = 0.33002477\n",
      "Iteration 265, loss = 0.11333380\n",
      "Iteration 75, loss = 0.32782459\n",
      "Iteration 76, loss = 0.32569654\n",
      "Iteration 124, loss = 0.18584904\n",
      "Iteration 77, loss = 0.32365647\n",
      "Iteration 78, loss = 0.32138644\n",
      "Iteration 79, loss = 0.31939116\n",
      "Iteration 233, loss = 0.12681032\n",
      "Iteration 125, loss = 0.18535300\n",
      "Iteration 266, loss = 0.11323120\n",
      "Iteration 122, loss = 0.16749977\n",
      "Iteration 34, loss = 0.37172509\n",
      "Iteration 267, loss = 0.11310552\n",
      "Iteration 234, loss = 0.12666707\n",
      "Iteration 28, loss = 0.42639609\n",
      "Iteration 123, loss = 0.16700617Iteration 126, loss = 0.18488873\n",
      "Iteration 215, loss = 0.22204897\n",
      "Iteration 29, loss = 0.42283616\n",
      "\n",
      "Iteration 80, loss = 0.31714086\n",
      "Iteration 268, loss = 0.11299719\n",
      "Iteration 124, loss = 0.16645341\n",
      "Iteration 235, loss = 0.12652663\n",
      "Iteration 81, loss = 0.31509524\n",
      "Iteration 269, loss = 0.11288937\n",
      "Iteration 82, loss = 0.31275767\n",
      "Iteration 83, loss = 0.31061519\n",
      "Iteration 125, loss = 0.16589791\n",
      "Iteration 84, loss = 0.30843493\n",
      "Iteration 270, loss = 0.11276498\n",
      "Iteration 85, loss = 0.30604930\n",
      "Iteration 35, loss = 0.36611790\n",
      "Iteration 86, loss = 0.30375446\n",
      "Iteration 30, loss = 0.41939766\n",
      "Iteration 127, loss = 0.18443843\n",
      "Iteration 236, loss = 0.12638960\n",
      "Iteration 216, loss = 0.22188435\n",
      "Iteration 126, loss = 0.16539273\n",
      "Iteration 271, loss = 0.11266656\n",
      "Iteration 87, loss = 0.30149523\n",
      "Iteration 31, loss = 0.41602096\n",
      "Iteration 127, loss = 0.16488526\n",
      "Iteration 36, loss = 0.36077622\n",
      "Iteration 237, loss = 0.12623728\n",
      "Iteration 238, loss = 0.12609282\n",
      "Iteration 272, loss = 0.11257090\n",
      "Iteration 239, loss = 0.12595474\n",
      "Iteration 128, loss = 0.18395684\n",
      "Iteration 273, loss = 0.11243295Iteration 88, loss = 0.29909958\n",
      "Iteration 89, loss = 0.29673633\n",
      "\n",
      "Iteration 90, loss = 0.29438671\n",
      "Iteration 128, loss = 0.16440325\n",
      "Iteration 274, loss = 0.11233732\n",
      "Iteration 91, loss = 0.29199075\n",
      "Iteration 129, loss = 0.18353154\n",
      "Iteration 129, loss = 0.16392167\n",
      "Iteration 92, loss = 0.28952678\n",
      "Iteration 217, loss = 0.22179617\n",
      "Iteration 32, loss = 0.41267477\n",
      "Iteration 33, loss = 0.40943986\n",
      "Iteration 275, loss = 0.11222903\n",
      "Iteration 93, loss = 0.28719152\n",
      "Iteration 37, loss = 0.35558459\n",
      "Iteration 276, loss = 0.11212935\n",
      "Iteration 130, loss = 0.18307967\n",
      "Iteration 94, loss = 0.28455438\n",
      "Iteration 95, loss = 0.28202847\n",
      "Iteration 218, loss = 0.22154810\n",
      "Iteration 240, loss = 0.12584613\n",
      "Iteration 130, loss = 0.16341823\n",
      "Iteration 96, loss = 0.27945678\n",
      "Iteration 97, loss = 0.27692469\n",
      "Iteration 241, loss = 0.12570662\n",
      "Iteration 98, loss = 0.27425163\n",
      "Iteration 34, loss = 0.40622125\n",
      "Iteration 131, loss = 0.16295760\n",
      "Iteration 277, loss = 0.11204880\n",
      "Iteration 242, loss = 0.12553252\n",
      "Iteration 35, loss = 0.40302213\n",
      "Iteration 38, loss = 0.35051529\n",
      "Iteration 278, loss = 0.11193051\n",
      "Iteration 131, loss = 0.18268760\n",
      "Iteration 243, loss = 0.12539861\n",
      "Iteration 99, loss = 0.27152683\n",
      "Iteration 100, loss = 0.26888141\n",
      "Iteration 244, loss = 0.12528177\n",
      "Iteration 132, loss = 0.16252538\n",
      "Iteration 219, loss = 0.22147986\n",
      "Iteration 132, loss = 0.18223001\n",
      "Iteration 101, loss = 0.26608230\n",
      "Iteration 102, loss = 0.26334290\n",
      "Iteration 103, loss = 0.26059237\n",
      "Iteration 279, loss = 0.11182723\n",
      "Iteration 245, loss = 0.12516084\n",
      "Iteration 133, loss = 0.16205638\n",
      "Iteration 36, loss = 0.39990124\n",
      "Iteration 280, loss = 0.11171268\n",
      "Iteration 246, loss = 0.12500507\n",
      "Iteration 281, loss = 0.11162127\n",
      "Iteration 37, loss = 0.39679300\n",
      "Iteration 282, loss = 0.11149639\n",
      "Iteration 134, loss = 0.16159615Iteration 220, loss = 0.22128659\n",
      "Iteration 38, loss = 0.39370898\n",
      "\n",
      "Iteration 104, loss = 0.25777213\n",
      "Iteration 39, loss = 0.34575170\n",
      "Iteration 105, loss = 0.25499432\n",
      "Iteration 133, loss = 0.18182805\n",
      "Iteration 106, loss = 0.25221466\n",
      "Iteration 107, loss = 0.24947748\n",
      "Iteration 283, loss = 0.11140546\n",
      "Iteration 247, loss = 0.12487263\n",
      "Iteration 135, loss = 0.16118033\n",
      "Iteration 134, loss = 0.18140683\n",
      "Iteration 248, loss = 0.12474848\n",
      "Iteration 39, loss = 0.39068873\n",
      "Iteration 284, loss = 0.11131391\n",
      "Iteration 108, loss = 0.24656522\n",
      "Iteration 40, loss = 0.34085506\n",
      "Iteration 109, loss = 0.24375365\n",
      "Iteration 136, loss = 0.16072766\n",
      "Iteration 285, loss = 0.11121869\n",
      "Iteration 110, loss = 0.24090038\n",
      "Iteration 249, loss = 0.12461191\n",
      "Iteration 286, loss = 0.11110762\n",
      "Iteration 221, loss = 0.22113036\n",
      "Iteration 287, loss = 0.11101928\n",
      "Iteration 135, loss = 0.18098229\n",
      "Iteration 41, loss = 0.33629891\n",
      "Iteration 40, loss = 0.38774092\n",
      "Iteration 136, loss = 0.18060001\n",
      "Iteration 137, loss = 0.16032761\n",
      "Iteration 111, loss = 0.23808802\n",
      "Iteration 112, loss = 0.23529697\n",
      "Iteration 288, loss = 0.11094265\n",
      "Iteration 113, loss = 0.23251484\n",
      "Iteration 138, loss = 0.15990906\n",
      "Iteration 222, loss = 0.22104186\n",
      "Iteration 114, loss = 0.22978868\n",
      "Iteration 115, loss = 0.22692481\n",
      "Iteration 116, loss = 0.22422451\n",
      "Iteration 117, loss = 0.22157519\n",
      "Iteration 41, loss = 0.38470059\n",
      "Iteration 139, loss = 0.15952111Iteration 250, loss = 0.12449393\n",
      "\n",
      "Iteration 289, loss = 0.11083465\n",
      "Iteration 118, loss = 0.21882034\n",
      "Iteration 119, loss = 0.21625408\n",
      "Iteration 120, loss = 0.21365187\n",
      "Iteration 251, loss = 0.12437380\n",
      "Iteration 290, loss = 0.11074342\n",
      "Iteration 121, loss = 0.21112963\n",
      "Iteration 42, loss = 0.33190557\n",
      "Iteration 137, loss = 0.18022919\n",
      "Iteration 291, loss = 0.11064397\n",
      "Iteration 140, loss = 0.15910245\n",
      "Iteration 42, loss = 0.38170537\n",
      "Iteration 292, loss = 0.11055990\n",
      "Iteration 122, loss = 0.20861420\n",
      "Iteration 252, loss = 0.12422132\n",
      "Iteration 43, loss = 0.32754553\n",
      "Iteration 43, loss = 0.37866517\n",
      "Iteration 141, loss = 0.15872294\n",
      "Iteration 253, loss = 0.12408356\n",
      "Iteration 223, loss = 0.22084218\n",
      "Iteration 293, loss = 0.11043298\n",
      "Iteration 123, loss = 0.20620961\n",
      "Iteration 124, loss = 0.20375443\n",
      "Iteration 125, loss = 0.20146129\n",
      "Iteration 138, loss = 0.17981987\n",
      "Iteration 126, loss = 0.19908525\n",
      "Iteration 254, loss = 0.12398248\n",
      "Iteration 127, loss = 0.19687280\n",
      "Iteration 128, loss = 0.19468020\n",
      "Iteration 142, loss = 0.15830559\n",
      "Iteration 143, loss = 0.15796478\n",
      "Iteration 44, loss = 0.37569843\n",
      "Iteration 294, loss = 0.11035754\n",
      "Iteration 295, loss = 0.11028570\n",
      "Iteration 255, loss = 0.12384332\n",
      "Iteration 224, loss = 0.22072868\n",
      "Iteration 144, loss = 0.15757921\n",
      "Iteration 296, loss = 0.11016394\n",
      "Iteration 139, loss = 0.17947558\n",
      "Iteration 44, loss = 0.32336895\n",
      "Iteration 129, loss = 0.19249045\n",
      "Iteration 145, loss = 0.15718692\n",
      "Iteration 45, loss = 0.37264689\n",
      "Iteration 130, loss = 0.19043824\n",
      "Iteration 297, loss = 0.11009244\n",
      "Iteration 256, loss = 0.12372223\n",
      "Iteration 131, loss = 0.18837886\n",
      "Iteration 132, loss = 0.18649968\n",
      "Iteration 46, loss = 0.36967513\n",
      "Iteration 133, loss = 0.18453332\n",
      "Iteration 257, loss = 0.12357731\n",
      "Iteration 140, loss = 0.17907515\n",
      "Iteration 298, loss = 0.10998895\n",
      "Iteration 45, loss = 0.31932831\n",
      "Iteration 146, loss = 0.15682522\n",
      "Iteration 134, loss = 0.18268290\n",
      "Iteration 135, loss = 0.18095364\n",
      "Iteration 136, loss = 0.17920278\n",
      "Iteration 137, loss = 0.17751576\n",
      "Iteration 47, loss = 0.36675897\n",
      "Iteration 138, loss = 0.17586063\n",
      "Iteration 299, loss = 0.10988817\n",
      "Iteration 147, loss = 0.15649136\n",
      "Iteration 225, loss = 0.22059911\n",
      "Iteration 300, loss = 0.10980800\n",
      "Iteration 141, loss = 0.17870080\n",
      "Iteration 139, loss = 0.17438001\n",
      "Iteration 148, loss = 0.15610835\n",
      "Iteration 258, loss = 0.12346187\n",
      "Iteration 140, loss = 0.17281200\n",
      "Iteration 141, loss = 0.17127596\n",
      "Iteration 48, loss = 0.36372880\n",
      "Iteration 301, loss = 0.10974752\n",
      "Iteration 46, loss = 0.31550306\n",
      "Iteration 259, loss = 0.12333535\n",
      "Iteration 302, loss = 0.10966865\n",
      "Iteration 142, loss = 0.17832721\n",
      "Iteration 260, loss = 0.12320822\n",
      "Iteration 303, loss = 0.10953485\n",
      "Iteration 49, loss = 0.36077890\n",
      "Iteration 149, loss = 0.15577576\n",
      "Iteration 142, loss = 0.16987038\n",
      "Iteration 226, loss = 0.22045628\n",
      "Iteration 150, loss = 0.15541747\n",
      "Iteration 47, loss = 0.31165376\n",
      "Iteration 151, loss = 0.15510416\n",
      "Iteration 143, loss = 0.16843196\n",
      "Iteration 143, loss = 0.17797799\n",
      "Iteration 144, loss = 0.16702238\n",
      "Iteration 304, loss = 0.10946961\n",
      "Iteration 145, loss = 0.16571689\n",
      "Iteration 261, loss = 0.12308314\n",
      "Iteration 50, loss = 0.35776556\n",
      "Iteration 305, loss = 0.10941060\n",
      "Iteration 146, loss = 0.16438760\n",
      "Iteration 306, loss = 0.10929338\n",
      "Iteration 48, loss = 0.30793058\n",
      "Iteration 147, loss = 0.16318436\n",
      "Iteration 148, loss = 0.16193581\n",
      "Iteration 152, loss = 0.15474916\n",
      "Iteration 51, loss = 0.35482157\n",
      "Iteration 144, loss = 0.17760515\n",
      "Iteration 307, loss = 0.10920920\n",
      "Iteration 262, loss = 0.12296275\n",
      "Iteration 227, loss = 0.22029197\n",
      "Iteration 153, loss = 0.15445073\n",
      "Iteration 308, loss = 0.10912763\n",
      "Iteration 263, loss = 0.12285446\n",
      "Iteration 154, loss = 0.15412923\n",
      "Iteration 145, loss = 0.17725884\n",
      "Iteration 49, loss = 0.30449368\n",
      "Iteration 149, loss = 0.16071049\n",
      "Iteration 52, loss = 0.35193150\n",
      "Iteration 150, loss = 0.15953451\n",
      "Iteration 264, loss = 0.12274228\n",
      "Iteration 53, loss = 0.34905458\n",
      "Iteration 151, loss = 0.15844124\n",
      "Iteration 265, loss = 0.12260865\n",
      "Iteration 152, loss = 0.15733332\n",
      "Iteration 153, loss = 0.15627442\n",
      "Iteration 228, loss = 0.22012616\n",
      "Iteration 309, loss = 0.10903892\n",
      "Iteration 54, loss = 0.34620207\n",
      "Iteration 154, loss = 0.15520560\n",
      "Iteration 310, loss = 0.10895845\n",
      "Iteration 155, loss = 0.15414154\n",
      "Iteration 266, loss = 0.12247376Iteration 146, loss = 0.17692550\n",
      "Iteration 155, loss = 0.15377524\n",
      "\n",
      "Iteration 50, loss = 0.30107845\n",
      "Iteration 156, loss = 0.15344721\n",
      "Iteration 311, loss = 0.10888822\n",
      "Iteration 312, loss = 0.10881833\n",
      "Iteration 157, loss = 0.15317191\n",
      "Iteration 156, loss = 0.15321873\n",
      "Iteration 267, loss = 0.12237499\n",
      "Iteration 147, loss = 0.17657649\n",
      "Iteration 158, loss = 0.15283192\n",
      "Iteration 313, loss = 0.10871103\n",
      "Iteration 229, loss = 0.21999645\n",
      "Iteration 148, loss = 0.17624613\n",
      "Iteration 51, loss = 0.29781981\n",
      "Iteration 314, loss = 0.10868428\n",
      "Iteration 157, loss = 0.15226220\n",
      "Iteration 55, loss = 0.34337449\n",
      "Iteration 315, loss = 0.10857570\n",
      "Iteration 52, loss = 0.29479068\n",
      "Iteration 158, loss = 0.15124788\n",
      "Iteration 159, loss = 0.15249900\n",
      "Iteration 268, loss = 0.12225754\n",
      "Iteration 159, loss = 0.15037161\n",
      "Iteration 160, loss = 0.14940492\n",
      "Iteration 56, loss = 0.34054505\n",
      "Iteration 160, loss = 0.15225650\n",
      "Iteration 149, loss = 0.17590332\n",
      "Iteration 269, loss = 0.12214488\n",
      "Iteration 161, loss = 0.14850488\n",
      "Iteration 162, loss = 0.14762647\n",
      "Iteration 230, loss = 0.21987563\n",
      "Iteration 270, loss = 0.12201983\n",
      "Iteration 316, loss = 0.10847080\n",
      "Iteration 150, loss = 0.17560380\n",
      "Iteration 163, loss = 0.14673213\n",
      "Iteration 57, loss = 0.33775868\n",
      "Iteration 164, loss = 0.14594369\n",
      "Iteration 161, loss = 0.15192980\n",
      "Iteration 271, loss = 0.12189927\n",
      "Iteration 317, loss = 0.10840229\n",
      "Iteration 58, loss = 0.33500652\n",
      "Iteration 165, loss = 0.14507167\n",
      "Iteration 318, loss = 0.10833757\n",
      "Iteration 166, loss = 0.14424777\n",
      "Iteration 53, loss = 0.29163531\n",
      "Iteration 151, loss = 0.17526020\n",
      "Iteration 231, loss = 0.21972199\n",
      "Iteration 319, loss = 0.10823840\n",
      "Iteration 272, loss = 0.12178861\n",
      "Iteration 162, loss = 0.15162044\n",
      "Iteration 59, loss = 0.33228605\n",
      "Iteration 167, loss = 0.14346986\n",
      "Iteration 163, loss = 0.15130174\n",
      "Iteration 273, loss = 0.12167424\n",
      "Iteration 168, loss = 0.14274164\n",
      "Iteration 320, loss = 0.10817445\n",
      "Iteration 169, loss = 0.14195623\n",
      "Iteration 170, loss = 0.14122184\n",
      "Iteration 152, loss = 0.17493772\n",
      "Iteration 321, loss = 0.10809601\n",
      "Iteration 54, loss = 0.28873548\n",
      "Iteration 60, loss = 0.32964133\n",
      "Iteration 171, loss = 0.14050343\n",
      "Iteration 322, loss = 0.10800539\n",
      "Iteration 164, loss = 0.15104566\n",
      "Iteration 61, loss = 0.32686255\n",
      "Iteration 232, loss = 0.21960004\n",
      "Iteration 172, loss = 0.13977668\n",
      "Iteration 173, loss = 0.13913168\n",
      "Iteration 62, loss = 0.32430675\n",
      "Iteration 274, loss = 0.12155869\n",
      "Iteration 165, loss = 0.15074381\n",
      "Iteration 174, loss = 0.13841941\n",
      "Iteration 153, loss = 0.17464021\n",
      "Iteration 323, loss = 0.10796136\n",
      "Iteration 275, loss = 0.12145987\n",
      "Iteration 324, loss = 0.10787502\n",
      "Iteration 166, loss = 0.15044515\n",
      "Iteration 175, loss = 0.13773554\n",
      "Iteration 55, loss = 0.28605337\n",
      "Iteration 176, loss = 0.13718198\n",
      "Iteration 167, loss = 0.15016455\n",
      "Iteration 154, loss = 0.17434042\n",
      "Iteration 177, loss = 0.13643485\n",
      "Iteration 233, loss = 0.21945726\n",
      "Iteration 63, loss = 0.32161234\n",
      "Iteration 178, loss = 0.13582734\n",
      "Iteration 276, loss = 0.12133655\n",
      "Iteration 325, loss = 0.10785276\n",
      "Iteration 155, loss = 0.17402001\n",
      "Iteration 326, loss = 0.10773066\n",
      "Iteration 56, loss = 0.28333355\n",
      "Iteration 64, loss = 0.31903298\n",
      "Iteration 179, loss = 0.13517507\n",
      "Iteration 168, loss = 0.14992173\n",
      "Iteration 327, loss = 0.10765288\n",
      "Iteration 180, loss = 0.13453107\n",
      "Iteration 328, loss = 0.10756968\n",
      "Iteration 277, loss = 0.12123964\n",
      "Iteration 57, loss = 0.28063375\n",
      "Iteration 181, loss = 0.13396061\n",
      "Iteration 156, loss = 0.17371875\n",
      "Iteration 182, loss = 0.13333688\n",
      "Iteration 169, loss = 0.14960962\n",
      "Iteration 183, loss = 0.13274082\n",
      "Iteration 65, loss = 0.31646984\n",
      "Iteration 234, loss = 0.21937055\n",
      "Iteration 184, loss = 0.13218830\n",
      "Iteration 278, loss = 0.12111989\n",
      "Iteration 329, loss = 0.10749893\n",
      "Iteration 279, loss = 0.12102132\n",
      "Iteration 157, loss = 0.17341245\n",
      "Iteration 66, loss = 0.31391080\n",
      "Iteration 185, loss = 0.13160019\n",
      "Iteration 330, loss = 0.10742622\n",
      "Iteration 170, loss = 0.14930737\n",
      "Iteration 186, loss = 0.13102830\n",
      "Iteration 58, loss = 0.27828507\n",
      "Iteration 171, loss = 0.14904394\n",
      "Iteration 67, loss = 0.31137492\n",
      "Iteration 172, loss = 0.14879495\n",
      "Iteration 59, loss = 0.27587458\n",
      "Iteration 235, loss = 0.21920049\n",
      "Iteration 280, loss = 0.12089510\n",
      "Iteration 173, loss = 0.14848540\n",
      "Iteration 187, loss = 0.13050715\n",
      "Iteration 331, loss = 0.10734185\n",
      "Iteration 188, loss = 0.12992838Iteration 174, loss = 0.14820406\n",
      "Iteration 332, loss = 0.10728231\n",
      "Iteration 68, loss = 0.30899348\n",
      "\n",
      "Iteration 158, loss = 0.17311425\n",
      "Iteration 189, loss = 0.12949591\n",
      "Iteration 281, loss = 0.12080461\n",
      "Iteration 333, loss = 0.10720771\n",
      "Iteration 190, loss = 0.12889971\n",
      "Iteration 282, loss = 0.12069652\n",
      "Iteration 69, loss = 0.30640357\n",
      "Iteration 334, loss = 0.10711890\n",
      "Iteration 191, loss = 0.12839476\n",
      "Iteration 159, loss = 0.17283451\n",
      "Iteration 283, loss = 0.12058645\n",
      "Iteration 236, loss = 0.21905249\n",
      "Iteration 70, loss = 0.30398968\n",
      "Iteration 175, loss = 0.14796906Iteration 60, loss = 0.27358772\n",
      "Iteration 192, loss = 0.12790141\n",
      "\n",
      "Iteration 193, loss = 0.12737416\n",
      "Iteration 335, loss = 0.10708501\n",
      "Iteration 194, loss = 0.12688974\n",
      "Iteration 195, loss = 0.12647802\n",
      "Iteration 71, loss = 0.30154725\n",
      "Iteration 196, loss = 0.12597741\n",
      "Iteration 336, loss = 0.10698326\n",
      "Iteration 284, loss = 0.12046744\n",
      "Iteration 160, loss = 0.17257597\n",
      "Iteration 61, loss = 0.27147809\n",
      "Iteration 337, loss = 0.10691497\n",
      "Iteration 176, loss = 0.14770778\n",
      "Iteration 72, loss = 0.29908096\n",
      "Iteration 197, loss = 0.12559493Iteration 285, loss = 0.12036190\n",
      "\n",
      "Iteration 177, loss = 0.14744589\n",
      "Iteration 62, loss = 0.26931443\n",
      "Iteration 286, loss = 0.12026782\n",
      "Iteration 161, loss = 0.17226317\n",
      "Iteration 178, loss = 0.14717678\n",
      "Iteration 198, loss = 0.12512484\n",
      "Iteration 338, loss = 0.10684844\n",
      "Iteration 237, loss = 0.21897636\n",
      "Iteration 199, loss = 0.12466915\n",
      "Iteration 200, loss = 0.12423153\n",
      "Iteration 162, loss = 0.17200077\n",
      "Iteration 73, loss = 0.29673412\n",
      "Iteration 201, loss = 0.12382268\n",
      "Iteration 179, loss = 0.14694454\n",
      "Iteration 287, loss = 0.12015725\n",
      "Iteration 202, loss = 0.12345865\n",
      "Iteration 339, loss = 0.10678503\n",
      "Iteration 63, loss = 0.26732983\n",
      "Iteration 74, loss = 0.29433755\n",
      "Iteration 180, loss = 0.14667892\n",
      "Iteration 238, loss = 0.21880635\n",
      "Iteration 203, loss = 0.12307848\n",
      "Iteration 340, loss = 0.10671954\n",
      "Iteration 204, loss = 0.12262733\n",
      "Iteration 181, loss = 0.14644912\n",
      "Iteration 205, loss = 0.12222290\n",
      "Iteration 341, loss = 0.10666729\n",
      "Iteration 206, loss = 0.12179587\n",
      "Iteration 163, loss = 0.17172798\n",
      "Iteration 75, loss = 0.29203225\n",
      "Iteration 288, loss = 0.12004566\n",
      "Iteration 182, loss = 0.14623665\n",
      "Iteration 342, loss = 0.10656721\n",
      "Iteration 207, loss = 0.12144334\n",
      "Iteration 289, loss = 0.11996368\n",
      "Iteration 239, loss = 0.21870446\n",
      "Iteration 183, loss = 0.14591589\n",
      "Iteration 76, loss = 0.28970618\n",
      "Iteration 64, loss = 0.26539348\n",
      "Iteration 290, loss = 0.11983379\n",
      "Iteration 208, loss = 0.12106046\n",
      "Iteration 343, loss = 0.10651472\n",
      "Iteration 164, loss = 0.17146196Iteration 209, loss = 0.12070166\n",
      "\n",
      "Iteration 210, loss = 0.12035249\n",
      "Iteration 211, loss = 0.11995825\n",
      "Iteration 212, loss = 0.11962487\n",
      "Iteration 344, loss = 0.10645871\n",
      "Iteration 213, loss = 0.11927209\n",
      "Iteration 291, loss = 0.11974487\n",
      "Iteration 77, loss = 0.28740716\n",
      "Iteration 345, loss = 0.10638571\n",
      "Iteration 184, loss = 0.14566103\n",
      "Iteration 65, loss = 0.26354875\n",
      "Iteration 346, loss = 0.10630728\n",
      "Iteration 78, loss = 0.28507009\n",
      "Iteration 240, loss = 0.21861926\n",
      "Iteration 292, loss = 0.11963256\n",
      "Iteration 165, loss = 0.17115917\n",
      "Iteration 214, loss = 0.11892533\n",
      "Iteration 293, loss = 0.11952771\n",
      "Iteration 215, loss = 0.11856881\n",
      "Iteration 347, loss = 0.10623748\n",
      "Iteration 216, loss = 0.11824657\n",
      "Iteration 185, loss = 0.14542967\n",
      "Iteration 294, loss = 0.11943487\n",
      "Iteration 217, loss = 0.11793885\n",
      "Iteration 348, loss = 0.10618158\n",
      "Iteration 66, loss = 0.26178003\n",
      "Iteration 218, loss = 0.11760016\n",
      "Iteration 241, loss = 0.21844246\n",
      "Iteration 219, loss = 0.11730286\n",
      "Iteration 186, loss = 0.14519892\n",
      "Iteration 349, loss = 0.10610499\n",
      "Iteration 166, loss = 0.17091288\n",
      "Iteration 79, loss = 0.28280734\n",
      "Iteration 187, loss = 0.14495165\n",
      "Iteration 220, loss = 0.11695854\n",
      "Iteration 350, loss = 0.10604598\n",
      "Iteration 188, loss = 0.14471155\n",
      "Iteration 295, loss = 0.11933206\n",
      "Iteration 167, loss = 0.17062959\n",
      "Iteration 189, loss = 0.14454810\n",
      "Iteration 67, loss = 0.26006412\n",
      "Iteration 80, loss = 0.28056580\n",
      "Iteration 221, loss = 0.11667185\n",
      "Iteration 296, loss = 0.11923199\n",
      "Iteration 351, loss = 0.10598941\n",
      "Iteration 242, loss = 0.21836225\n",
      "Iteration 81, loss = 0.27832170\n",
      "Iteration 168, loss = 0.17037177\n",
      "Iteration 222, loss = 0.11635720\n",
      "Iteration 352, loss = 0.10592930\n",
      "Iteration 223, loss = 0.11604628\n",
      "Iteration 190, loss = 0.14425031\n",
      "Iteration 224, loss = 0.11573332\n",
      "Iteration 82, loss = 0.27612196\n",
      "Iteration 191, loss = 0.14401612\n",
      "Iteration 353, loss = 0.10585904\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 243, loss = 0.21818826\n",
      "Iteration 297, loss = 0.11915172\n",
      "Iteration 83, loss = 0.27397510\n",
      "Iteration 68, loss = 0.25850458\n",
      "Iteration 192, loss = 0.14379795\n",
      "Iteration 169, loss = 0.17013203\n",
      "Iteration 84, loss = 0.27169859\n",
      "Iteration 298, loss = 0.11905362\n",
      "Iteration 225, loss = 0.11544465\n",
      "Iteration 299, loss = 0.11894416\n",
      "Iteration 193, loss = 0.14360146\n",
      "Iteration 300, loss = 0.11887148\n",
      "Iteration 226, loss = 0.11516158\n",
      "Iteration 227, loss = 0.11489527\n",
      "Iteration 69, loss = 0.25683971\n",
      "Iteration 1, loss = 0.97473958\n",
      "Iteration 228, loss = 0.11466268\n",
      "Iteration 229, loss = 0.11433128\n",
      "Iteration 170, loss = 0.16989141\n",
      "Iteration 194, loss = 0.14335721\n",
      "Iteration 230, loss = 0.11403491\n",
      "Iteration 85, loss = 0.26956164\n",
      "Iteration 231, loss = 0.11379329\n",
      "Iteration 232, loss = 0.11354132\n",
      "Iteration 171, loss = 0.16961348\n",
      "Iteration 233, loss = 0.11323064\n",
      "Iteration 244, loss = 0.21810535\n",
      "Iteration 86, loss = 0.26746088\n",
      "Iteration 301, loss = 0.11874245\n",
      "Iteration 195, loss = 0.14312426\n",
      "Iteration 70, loss = 0.25531006\n",
      "Iteration 234, loss = 0.11295195\n",
      "Iteration 302, loss = 0.11864899\n",
      "Iteration 87, loss = 0.26525694\n",
      "Iteration 235, loss = 0.11272495\n",
      "Iteration 196, loss = 0.14291235\n",
      "Iteration 236, loss = 0.11250586\n",
      "Iteration 2, loss = 0.93461082\n",
      "Iteration 88, loss = 0.26325252\n",
      "Iteration 172, loss = 0.16936350\n",
      "Iteration 71, loss = 0.25383865\n",
      "Iteration 303, loss = 0.11856610\n",
      "Iteration 245, loss = 0.21799732\n",
      "Iteration 197, loss = 0.14270379\n",
      "Iteration 304, loss = 0.11847454\n",
      "Iteration 198, loss = 0.14250708\n",
      "Iteration 237, loss = 0.11219909\n",
      "Iteration 89, loss = 0.26115506\n",
      "Iteration 238, loss = 0.11196011\n",
      "Iteration 173, loss = 0.16912888\n",
      "Iteration 239, loss = 0.11166076\n",
      "Iteration 305, loss = 0.11836192\n",
      "Iteration 240, loss = 0.11146417\n",
      "Iteration 199, loss = 0.14229489\n",
      "Iteration 174, loss = 0.16886644\n",
      "Iteration 72, loss = 0.25243023\n",
      "Iteration 246, loss = 0.21791255\n",
      "Iteration 3, loss = 0.88347917\n",
      "Iteration 241, loss = 0.11116633\n",
      "Iteration 306, loss = 0.11827476\n",
      "Iteration 90, loss = 0.25908168Iteration 242, loss = 0.11094504\n",
      "\n",
      "Iteration 243, loss = 0.11069245\n",
      "Iteration 73, loss = 0.25107084\n",
      "Iteration 244, loss = 0.11040783\n",
      "Iteration 307, loss = 0.11817388\n",
      "Iteration 245, loss = 0.11018568\n",
      "Iteration 91, loss = 0.25708363\n",
      "Iteration 200, loss = 0.14208870\n",
      "Iteration 92, loss = 0.25507837\n",
      "Iteration 201, loss = 0.14190203\n",
      "Iteration 247, loss = 0.21775884\n",
      "Iteration 308, loss = 0.11809267\n",
      "Iteration 175, loss = 0.16865355\n",
      "Iteration 202, loss = 0.14173614\n",
      "Iteration 309, loss = 0.11799811\n",
      "Iteration 176, loss = 0.16842746\n",
      "Iteration 246, loss = 0.10994622\n",
      "Iteration 4, loss = 0.83416970\n",
      "Iteration 247, loss = 0.10973016\n",
      "Iteration 74, loss = 0.24975143\n",
      "Iteration 93, loss = 0.25312963\n",
      "Iteration 203, loss = 0.14149030\n",
      "Iteration 248, loss = 0.10957175\n",
      "Iteration 249, loss = 0.10923164\n",
      "Iteration 248, loss = 0.21764895\n",
      "Iteration 250, loss = 0.10907776\n",
      "Iteration 94, loss = 0.25121143\n",
      "Iteration 251, loss = 0.10882730\n",
      "Iteration 310, loss = 0.11789890\n",
      "Iteration 5, loss = 0.78937421\n",
      "Iteration 177, loss = 0.16817727\n",
      "Iteration 75, loss = 0.24849549\n",
      "Iteration 204, loss = 0.14136123\n",
      "Iteration 252, loss = 0.10856700\n",
      "Iteration 311, loss = 0.11782913\n",
      "Iteration 253, loss = 0.10833318\n",
      "Iteration 95, loss = 0.24924170\n",
      "Iteration 254, loss = 0.10811611\n",
      "Iteration 178, loss = 0.16793319\n",
      "Iteration 205, loss = 0.14110852\n",
      "Iteration 312, loss = 0.11772829\n",
      "Iteration 255, loss = 0.10793379\n",
      "Iteration 179, loss = 0.16772889\n",
      "Iteration 249, loss = 0.21751323\n",
      "Iteration 96, loss = 0.24740171\n",
      "Iteration 206, loss = 0.14091062\n",
      "Iteration 76, loss = 0.24722693\n",
      "Iteration 6, loss = 0.75136246\n",
      "Iteration 313, loss = 0.11762886\n",
      "Iteration 256, loss = 0.10767511\n",
      "Iteration 207, loss = 0.14071946\n",
      "Iteration 314, loss = 0.11753935Iteration 97, loss = 0.24555835\n",
      "Iteration 257, loss = 0.10748512\n",
      "\n",
      "Iteration 77, loss = 0.24601456\n",
      "Iteration 180, loss = 0.16749725\n",
      "Iteration 250, loss = 0.21740557\n",
      "Iteration 258, loss = 0.10724248\n",
      "Iteration 181, loss = 0.16724343\n",
      "Iteration 208, loss = 0.14052541\n",
      "Iteration 209, loss = 0.14042007\n",
      "Iteration 78, loss = 0.24485780\n",
      "Iteration 259, loss = 0.10707228\n",
      "Iteration 260, loss = 0.10688469\n",
      "Iteration 98, loss = 0.24379791\n",
      "Iteration 7, loss = 0.71812644\n",
      "Iteration 261, loss = 0.10663273\n",
      "Iteration 315, loss = 0.11745621\n",
      "Iteration 210, loss = 0.14014538\n",
      "Iteration 99, loss = 0.24201980\n",
      "Iteration 262, loss = 0.10645774\n",
      "Iteration 211, loss = 0.13994882\n",
      "Iteration 263, loss = 0.10625951\n",
      "Iteration 316, loss = 0.11739015\n",
      "Iteration 251, loss = 0.21729840\n",
      "Iteration 317, loss = 0.11726665\n",
      "Iteration 264, loss = 0.10603484\n",
      "Iteration 265, loss = 0.10584038\n",
      "Iteration 182, loss = 0.16706884\n",
      "Iteration 79, loss = 0.24377983\n",
      "Iteration 100, loss = 0.24025791\n",
      "Iteration 8, loss = 0.68926638\n",
      "Iteration 212, loss = 0.13978871\n",
      "Iteration 266, loss = 0.10567685\n",
      "Iteration 183, loss = 0.16681147\n",
      "Iteration 213, loss = 0.13959852\n",
      "Iteration 101, loss = 0.23853656\n",
      "Iteration 318, loss = 0.11719018\n",
      "Iteration 267, loss = 0.10543347\n",
      "Iteration 80, loss = 0.24260352\n",
      "Iteration 252, loss = 0.21737832\n",
      "Iteration 9, loss = 0.66496008\n",
      "Iteration 268, loss = 0.10527787\n",
      "Iteration 319, loss = 0.11709764\n",
      "Iteration 102, loss = 0.23693151\n",
      "Iteration 214, loss = 0.13948172\n",
      "Iteration 269, loss = 0.10503820\n",
      "Iteration 184, loss = 0.16662771\n",
      "Iteration 320, loss = 0.11701529\n",
      "Iteration 321, loss = 0.11691726\n",
      "Iteration 270, loss = 0.10485559\n",
      "Iteration 103, loss = 0.23529196\n",
      "Iteration 215, loss = 0.13926992\n",
      "Iteration 185, loss = 0.16636951\n",
      "Iteration 271, loss = 0.10469320\n",
      "Iteration 81, loss = 0.24153962\n",
      "Iteration 322, loss = 0.11683882\n",
      "Iteration 272, loss = 0.10449233\n",
      "Iteration 216, loss = 0.13907217\n",
      "Iteration 253, loss = 0.21705566\n",
      "Iteration 273, loss = 0.10430496\n",
      "Iteration 10, loss = 0.64372378\n",
      "Iteration 104, loss = 0.23368236\n",
      "Iteration 323, loss = 0.11676519\n",
      "Iteration 274, loss = 0.10411756\n",
      "Iteration 82, loss = 0.24051316Iteration 217, loss = 0.13890208\n",
      "Iteration 186, loss = 0.16614781\n",
      "\n",
      "Iteration 105, loss = 0.23216639\n",
      "Iteration 275, loss = 0.10393401\n",
      "Iteration 218, loss = 0.13874632\n",
      "Iteration 324, loss = 0.11666651\n",
      "Iteration 276, loss = 0.10373280\n",
      "Iteration 277, loss = 0.10357573\n",
      "Iteration 106, loss = 0.23061277\n",
      "Iteration 278, loss = 0.10349491\n",
      "Iteration 254, loss = 0.21697745\n",
      "Iteration 219, loss = 0.13856909\n",
      "Iteration 325, loss = 0.11657385\n",
      "Iteration 11, loss = 0.62513498\n",
      "Iteration 187, loss = 0.16593835\n",
      "Iteration 326, loss = 0.11649671\n",
      "Iteration 107, loss = 0.22912695\n",
      "Iteration 327, loss = 0.11641166\n",
      "Iteration 188, loss = 0.16574263\n",
      "Iteration 279, loss = 0.10322510\n",
      "Iteration 83, loss = 0.23947414\n",
      "Iteration 220, loss = 0.13842182\n",
      "Iteration 280, loss = 0.10311980\n",
      "Iteration 221, loss = 0.13826982\n",
      "Iteration 281, loss = 0.10284444\n",
      "Iteration 282, loss = 0.10268278\n",
      "Iteration 283, loss = 0.10250822\n",
      "Iteration 328, loss = 0.11632533\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 255, loss = 0.21686326\n",
      "Iteration 222, loss = 0.13807925\n",
      "Iteration 108, loss = 0.22769690\n",
      "Iteration 284, loss = 0.10231759\n",
      "Iteration 12, loss = 0.60884293\n",
      "Iteration 1, loss = 1.07918551\n",
      "Iteration 2, loss = 1.01016431\n",
      "Iteration 3, loss = 0.91851939\n",
      "Iteration 285, loss = 0.10217160\n",
      "Iteration 4, loss = 0.82574443\n",
      "Iteration 5, loss = 0.74202351\n",
      "Iteration 6, loss = 0.66904476\n",
      "Iteration 7, loss = 0.60813804\n",
      "Iteration 8, loss = 0.55794109\n",
      "Iteration 9, loss = 0.51647211\n",
      "Iteration 286, loss = 0.10199695\n",
      "Iteration 10, loss = 0.48171420\n",
      "Iteration 11, loss = 0.45262574\n",
      "Iteration 12, loss = 0.42801796\n",
      "Iteration 13, loss = 0.40688317\n",
      "Iteration 14, loss = 0.38848606\n",
      "Iteration 15, loss = 0.37216597\n",
      "Iteration 16, loss = 0.35773461\n",
      "Iteration 17, loss = 0.34477058\n",
      "Iteration 84, loss = 0.23852786\n",
      "Iteration 18, loss = 0.33328953\n",
      "Iteration 19, loss = 0.32289087\n",
      "Iteration 20, loss = 0.31312577\n",
      "Iteration 21, loss = 0.30431816\n",
      "Iteration 189, loss = 0.16552244\n",
      "Iteration 223, loss = 0.13790592\n",
      "Iteration 109, loss = 0.22625638\n",
      "Iteration 22, loss = 0.29618391\n",
      "Iteration 287, loss = 0.10184447\n",
      "Iteration 23, loss = 0.28866571\n",
      "Iteration 256, loss = 0.21677779\n",
      "Iteration 24, loss = 0.28167722\n",
      "Iteration 288, loss = 0.10165104\n",
      "Iteration 25, loss = 0.27512133\n",
      "Iteration 26, loss = 0.26906236\n",
      "Iteration 85, loss = 0.23759674\n",
      "Iteration 190, loss = 0.16531011\n",
      "Iteration 224, loss = 0.13784453\n",
      "Iteration 110, loss = 0.22482672Iteration 289, loss = 0.10146666\n",
      "\n",
      "Iteration 225, loss = 0.13758621\n",
      "Iteration 86, loss = 0.23663045\n",
      "Iteration 27, loss = 0.26344411\n",
      "Iteration 290, loss = 0.10131203\n",
      "Iteration 28, loss = 0.25815414\n",
      "Iteration 29, loss = 0.25313478\n",
      "Iteration 30, loss = 0.24846272\n",
      "Iteration 31, loss = 0.24396473\n",
      "Iteration 13, loss = 0.59509342\n",
      "Iteration 32, loss = 0.23979880\n",
      "Iteration 291, loss = 0.10114352\n",
      "Iteration 191, loss = 0.16511411\n",
      "Iteration 33, loss = 0.23582609\n",
      "Iteration 111, loss = 0.22348862\n",
      "Iteration 34, loss = 0.23203612\n",
      "Iteration 35, loss = 0.22848612\n",
      "Iteration 292, loss = 0.10100057\n",
      "Iteration 36, loss = 0.22508125\n",
      "Iteration 37, loss = 0.22184377\n",
      "Iteration 38, loss = 0.21872558\n",
      "Iteration 226, loss = 0.13748495\n",
      "Iteration 39, loss = 0.21576913\n",
      "Iteration 40, loss = 0.21290972\n",
      "Iteration 41, loss = 0.21022849\n",
      "Iteration 42, loss = 0.20762387\n",
      "Iteration 43, loss = 0.20512608\n",
      "Iteration 44, loss = 0.20273749\n",
      "Iteration 45, loss = 0.20046396\n",
      "Iteration 46, loss = 0.19820107\n",
      "Iteration 47, loss = 0.19610462\n",
      "Iteration 48, loss = 0.19400000\n",
      "Iteration 49, loss = 0.19199456\n",
      "Iteration 50, loss = 0.19010416\n",
      "Iteration 51, loss = 0.18825565\n",
      "Iteration 52, loss = 0.18640972\n",
      "Iteration 53, loss = 0.18466402\n",
      "Iteration 54, loss = 0.18299582\n",
      "Iteration 55, loss = 0.18133957\n",
      "Iteration 56, loss = 0.17976458\n",
      "Iteration 57, loss = 0.17820847\n",
      "Iteration 58, loss = 0.17673757\n",
      "Iteration 59, loss = 0.17528376\n",
      "Iteration 60, loss = 0.17386206\n",
      "Iteration 192, loss = 0.16494207\n",
      "Iteration 293, loss = 0.10082588\n",
      "Iteration 112, loss = 0.22217433\n",
      "Iteration 294, loss = 0.10065101\n",
      "Iteration 61, loss = 0.17248396Iteration 227, loss = 0.13730291\n",
      "Iteration 257, loss = 0.21671086\n",
      "\n",
      "Iteration 62, loss = 0.17115604\n",
      "Iteration 228, loss = 0.13715079\n",
      "Iteration 63, loss = 0.16984061\n",
      "Iteration 64, loss = 0.16857286\n",
      "Iteration 14, loss = 0.58269409\n",
      "Iteration 65, loss = 0.16730738\n",
      "Iteration 229, loss = 0.13698084\n",
      "Iteration 193, loss = 0.16469179\n",
      "Iteration 87, loss = 0.23573162\n",
      "Iteration 113, loss = 0.22087863\n",
      "Iteration 295, loss = 0.10049640\n",
      "Iteration 258, loss = 0.21652459\n",
      "Iteration 66, loss = 0.16610042\n",
      "Iteration 67, loss = 0.16492793\n",
      "Iteration 194, loss = 0.16450956\n",
      "Iteration 296, loss = 0.10035683\n",
      "Iteration 68, loss = 0.16376827\n",
      "Iteration 69, loss = 0.16261285\n",
      "Iteration 70, loss = 0.16150797\n",
      "Iteration 71, loss = 0.16045700\n",
      "Iteration 72, loss = 0.15937710\n",
      "Iteration 73, loss = 0.15835247\n",
      "Iteration 74, loss = 0.15731915\n",
      "Iteration 75, loss = 0.15634890\n",
      "Iteration 76, loss = 0.15536596\n",
      "Iteration 297, loss = 0.10017932\n",
      "Iteration 77, loss = 0.15441977\n",
      "Iteration 78, loss = 0.15347896\n",
      "Iteration 79, loss = 0.15258026\n",
      "Iteration 80, loss = 0.15166806\n",
      "Iteration 298, loss = 0.10002542\n",
      "Iteration 114, loss = 0.21958423\n",
      "Iteration 81, loss = 0.15081152\n",
      "Iteration 299, loss = 0.10001249\n",
      "Iteration 230, loss = 0.13686272\n",
      "Iteration 82, loss = 0.14996074\n",
      "Iteration 83, loss = 0.14911508\n",
      "Iteration 84, loss = 0.14830143\n",
      "Iteration 85, loss = 0.14748842\n",
      "Iteration 86, loss = 0.14668637\n",
      "Iteration 87, loss = 0.14592281\n",
      "Iteration 88, loss = 0.14515751\n",
      "Iteration 89, loss = 0.14443159\n",
      "Iteration 90, loss = 0.14367898\n",
      "Iteration 91, loss = 0.14294832\n",
      "Iteration 88, loss = 0.23484840Iteration 92, loss = 0.14225073\n",
      "Iteration 15, loss = 0.57207773\n",
      "Iteration 93, loss = 0.14155349\n",
      "Iteration 94, loss = 0.14085229\n",
      "\n",
      "Iteration 231, loss = 0.13667768\n",
      "Iteration 95, loss = 0.14019516\n",
      "Iteration 96, loss = 0.13952222\n",
      "Iteration 195, loss = 0.16432605\n",
      "Iteration 115, loss = 0.21834408\n",
      "Iteration 300, loss = 0.09974213\n",
      "Iteration 97, loss = 0.13888456\n",
      "Iteration 98, loss = 0.13821579\n",
      "Iteration 99, loss = 0.13761058\n",
      "Iteration 100, loss = 0.13696459\n",
      "Iteration 232, loss = 0.13655058\n",
      "Iteration 101, loss = 0.13635056\n",
      "Iteration 301, loss = 0.09956851\n",
      "Iteration 259, loss = 0.21646817\n",
      "Iteration 102, loss = 0.13574106\n",
      "Iteration 116, loss = 0.21708777\n",
      "Iteration 233, loss = 0.13637433\n",
      "Iteration 103, loss = 0.13517185\n",
      "Iteration 302, loss = 0.09942563\n",
      "Iteration 234, loss = 0.13625117\n",
      "Iteration 104, loss = 0.13457557\n",
      "Iteration 303, loss = 0.09926437\n",
      "Iteration 16, loss = 0.56252737Iteration 105, loss = 0.13398711\n",
      "\n",
      "Iteration 235, loss = 0.13610616\n",
      "Iteration 106, loss = 0.13340866\n",
      "Iteration 107, loss = 0.13285621\n",
      "Iteration 108, loss = 0.13231157\n",
      "Iteration 109, loss = 0.13176125\n",
      "Iteration 110, loss = 0.13123180\n",
      "Iteration 111, loss = 0.13070220\n",
      "Iteration 112, loss = 0.13017814\n",
      "Iteration 113, loss = 0.12967194\n",
      "Iteration 114, loss = 0.12915973\n",
      "Iteration 196, loss = 0.16411634\n",
      "Iteration 117, loss = 0.21590135\n",
      "Iteration 115, loss = 0.12865943\n",
      "Iteration 89, loss = 0.23399640\n",
      "Iteration 304, loss = 0.09911483\n",
      "Iteration 116, loss = 0.12816978\n",
      "Iteration 117, loss = 0.12767696\n",
      "Iteration 118, loss = 0.12718447\n",
      "Iteration 119, loss = 0.12671370\n",
      "Iteration 305, loss = 0.09898881\n",
      "Iteration 120, loss = 0.12625275\n",
      "Iteration 121, loss = 0.12577360\n",
      "Iteration 122, loss = 0.12531099\n",
      "Iteration 123, loss = 0.12486020\n",
      "Iteration 306, loss = 0.09883908\n",
      "Iteration 260, loss = 0.21633898\n",
      "Iteration 124, loss = 0.12441694\n",
      "Iteration 197, loss = 0.16393613\n",
      "Iteration 125, loss = 0.12396284\n",
      "Iteration 307, loss = 0.09872157\n",
      "Iteration 126, loss = 0.12353240\n",
      "Iteration 127, loss = 0.12310501\n",
      "Iteration 308, loss = 0.09853520\n",
      "Iteration 236, loss = 0.13598647\n",
      "Iteration 128, loss = 0.12265697\n",
      "Iteration 129, loss = 0.12223600\n",
      "Iteration 130, loss = 0.12182848\n",
      "Iteration 131, loss = 0.12140309\n",
      "Iteration 132, loss = 0.12100710\n",
      "Iteration 118, loss = 0.21473440\n",
      "Iteration 90, loss = 0.23316614\n",
      "Iteration 133, loss = 0.12059996\n",
      "Iteration 134, loss = 0.12019046\n",
      "Iteration 135, loss = 0.11980286\n",
      "Iteration 309, loss = 0.09838136\n",
      "Iteration 136, loss = 0.11941121\n",
      "Iteration 310, loss = 0.09825774\n",
      "Iteration 17, loss = 0.55354418\n",
      "Iteration 119, loss = 0.21356514\n",
      "Iteration 311, loss = 0.09811793\n",
      "Iteration 237, loss = 0.13582552\n",
      "Iteration 312, loss = 0.09796041\n",
      "Iteration 198, loss = 0.16371147\n",
      "Iteration 137, loss = 0.11903011\n",
      "Iteration 238, loss = 0.13570173\n",
      "Iteration 138, loss = 0.11864751\n",
      "Iteration 139, loss = 0.11826525\n",
      "Iteration 140, loss = 0.11789765\n",
      "Iteration 141, loss = 0.11751996\n",
      "Iteration 142, loss = 0.11715755\n",
      "Iteration 143, loss = 0.11678720\n",
      "Iteration 144, loss = 0.11642737\n",
      "Iteration 145, loss = 0.11608369\n",
      "Iteration 146, loss = 0.11572065\n",
      "Iteration 147, loss = 0.11538041\n",
      "Iteration 199, loss = 0.16353858\n",
      "Iteration 313, loss = 0.09781892\n",
      "Iteration 148, loss = 0.11503470\n",
      "Iteration 149, loss = 0.11469595\n",
      "Iteration 150, loss = 0.11435335\n",
      "Iteration 314, loss = 0.09768820\n",
      "Iteration 261, loss = 0.21624375\n",
      "Iteration 151, loss = 0.11402923\n",
      "Iteration 152, loss = 0.11368798\n",
      "Iteration 153, loss = 0.11336811\n",
      "Iteration 154, loss = 0.11303760\n",
      "Iteration 155, loss = 0.11272115\n",
      "Iteration 156, loss = 0.11239298\n",
      "Iteration 91, loss = 0.23232767\n",
      "Iteration 157, loss = 0.11207632\n",
      "Iteration 158, loss = 0.11176198\n",
      "Iteration 159, loss = 0.11144827\n",
      "Iteration 160, loss = 0.11114486\n",
      "Iteration 120, loss = 0.21244387\n",
      "Iteration 161, loss = 0.11084380\n",
      "Iteration 162, loss = 0.11053550\n",
      "Iteration 239, loss = 0.13557199\n",
      "Iteration 315, loss = 0.09759766\n",
      "Iteration 163, loss = 0.11024070\n",
      "Iteration 18, loss = 0.54577913\n",
      "Iteration 164, loss = 0.10993374\n",
      "Iteration 165, loss = 0.10964811\n",
      "Iteration 166, loss = 0.10935187\n",
      "Iteration 167, loss = 0.10905756\n",
      "Iteration 200, loss = 0.16334662\n",
      "Iteration 168, loss = 0.10876901\n",
      "Iteration 169, loss = 0.10848925\n",
      "Iteration 170, loss = 0.10819704\n",
      "Iteration 171, loss = 0.10792803\n",
      "Iteration 172, loss = 0.10765029\n",
      "Iteration 173, loss = 0.10736130\n",
      "Iteration 174, loss = 0.10709411\n",
      "Iteration 175, loss = 0.10682077\n",
      "Iteration 176, loss = 0.10655230\n",
      "Iteration 177, loss = 0.10629331\n",
      "Iteration 178, loss = 0.10602341\n",
      "Iteration 179, loss = 0.10576277\n",
      "Iteration 240, loss = 0.13542900\n",
      "Iteration 316, loss = 0.09746680\n",
      "Iteration 201, loss = 0.16314034\n",
      "Iteration 92, loss = 0.23153029\n",
      "Iteration 180, loss = 0.10549685\n",
      "Iteration 317, loss = 0.09730367\n",
      "Iteration 121, loss = 0.21135983\n",
      "Iteration 181, loss = 0.10524167\n",
      "Iteration 182, loss = 0.10498599\n",
      "Iteration 318, loss = 0.09716698\n",
      "Iteration 183, loss = 0.10472354\n",
      "Iteration 184, loss = 0.10446957\n",
      "Iteration 185, loss = 0.10422103\n",
      "Iteration 186, loss = 0.10396608\n",
      "Iteration 187, loss = 0.10372372\n",
      "Iteration 319, loss = 0.09705577\n",
      "Iteration 122, loss = 0.21026101\n",
      "Iteration 188, loss = 0.10347707\n",
      "Iteration 189, loss = 0.10323279\n",
      "Iteration 190, loss = 0.10298435Iteration 241, loss = 0.13527920\n",
      "\n",
      "Iteration 320, loss = 0.09686562\n",
      "Iteration 191, loss = 0.10275085\n",
      "Iteration 262, loss = 0.21614584\n",
      "Iteration 192, loss = 0.10250743\n",
      "Iteration 193, loss = 0.10227984\n",
      "Iteration 194, loss = 0.10203738\n",
      "Iteration 242, loss = 0.13515459\n",
      "Iteration 202, loss = 0.16296817\n",
      "Iteration 195, loss = 0.10179918\n",
      "Iteration 123, loss = 0.20925792\n",
      "Iteration 93, loss = 0.23077308\n",
      "Iteration 243, loss = 0.13501597\n",
      "Iteration 196, loss = 0.10157727\n",
      "Iteration 124, loss = 0.20816324\n",
      "Iteration 321, loss = 0.09679525\n",
      "Iteration 19, loss = 0.53838539\n",
      "Iteration 322, loss = 0.09664663\n",
      "Iteration 203, loss = 0.16278597\n",
      "Iteration 323, loss = 0.09650496\n",
      "Iteration 244, loss = 0.13489817\n",
      "Iteration 324, loss = 0.09636665\n",
      "Iteration 325, loss = 0.09627308\n",
      "Iteration 125, loss = 0.20720229\n",
      "Iteration 197, loss = 0.10133778\n",
      "Iteration 204, loss = 0.16257099\n",
      "Iteration 20, loss = 0.53140837\n",
      "Iteration 198, loss = 0.10111354Iteration 326, loss = 0.09612476\n",
      "\n",
      "Iteration 327, loss = 0.09601767\n",
      "Iteration 94, loss = 0.22999601\n",
      "Iteration 199, loss = 0.10088427\n",
      "Iteration 200, loss = 0.10065698\n",
      "Iteration 263, loss = 0.21602221\n",
      "Iteration 201, loss = 0.10043640\n",
      "Iteration 245, loss = 0.13477572\n",
      "Iteration 202, loss = 0.10020874\n",
      "Iteration 203, loss = 0.09999504\n",
      "Iteration 204, loss = 0.09976725\n",
      "Iteration 205, loss = 0.09954658\n",
      "Iteration 206, loss = 0.09933007\n",
      "Iteration 126, loss = 0.20619904\n",
      "Iteration 207, loss = 0.09911519\n",
      "Iteration 208, loss = 0.09890663\n",
      "Iteration 209, loss = 0.09868479\n",
      "Iteration 210, loss = 0.09846985\n",
      "Iteration 211, loss = 0.09826158\n",
      "Iteration 328, loss = 0.09589594\n",
      "Iteration 212, loss = 0.09805425\n",
      "Iteration 213, loss = 0.09784372\n",
      "Iteration 214, loss = 0.09764322\n",
      "Iteration 215, loss = 0.09743137\n",
      "Iteration 216, loss = 0.09722825\n",
      "Iteration 217, loss = 0.09702581\n",
      "Iteration 218, loss = 0.09682173\n",
      "Iteration 219, loss = 0.09663222\n",
      "Iteration 220, loss = 0.09642249\n",
      "Iteration 221, loss = 0.09621909\n",
      "Iteration 222, loss = 0.09602936\n",
      "Iteration 223, loss = 0.09582978\n",
      "Iteration 224, loss = 0.09563087\n",
      "Iteration 205, loss = 0.16240760\n",
      "Iteration 225, loss = 0.09543649\n",
      "Iteration 226, loss = 0.09524667\n",
      "Iteration 227, loss = 0.09504760\n",
      "Iteration 228, loss = 0.09485643\n",
      "Iteration 229, loss = 0.09465962\n",
      "Iteration 230, loss = 0.09447083\n",
      "Iteration 231, loss = 0.09428399\n",
      "Iteration 232, loss = 0.09409374\n",
      "Iteration 233, loss = 0.09391146\n",
      "Iteration 234, loss = 0.09371660\n",
      "Iteration 329, loss = 0.09572437\n",
      "Iteration 235, loss = 0.09353709\n",
      "Iteration 236, loss = 0.09335623\n",
      "Iteration 246, loss = 0.13462391\n",
      "Iteration 237, loss = 0.09316644\n",
      "Iteration 127, loss = 0.20522094\n",
      "Iteration 238, loss = 0.09298391\n",
      "Iteration 239, loss = 0.09281039\n",
      "Iteration 240, loss = 0.09262756\n",
      "Iteration 241, loss = 0.09244752\n",
      "Iteration 330, loss = 0.09565768\n",
      "Iteration 242, loss = 0.09227101\n",
      "Iteration 243, loss = 0.09209311\n",
      "Iteration 244, loss = 0.09191392\n",
      "Iteration 245, loss = 0.09173513\n",
      "Iteration 246, loss = 0.09156809\n",
      "Iteration 331, loss = 0.09548407\n",
      "Iteration 247, loss = 0.09139605\n",
      "Iteration 248, loss = 0.09122369\n",
      "Iteration 249, loss = 0.09104441\n",
      "Iteration 250, loss = 0.09087496\n",
      "Iteration 251, loss = 0.09070534\n",
      "Iteration 252, loss = 0.09054089\n",
      "Iteration 95, loss = 0.22924057\n",
      "Iteration 253, loss = 0.09037168\n",
      "Iteration 254, loss = 0.09020102\n",
      "Iteration 264, loss = 0.21591284\n",
      "Iteration 206, loss = 0.16225854\n",
      "Iteration 247, loss = 0.13451262\n",
      "Iteration 21, loss = 0.52473802\n",
      "Iteration 332, loss = 0.09537645\n",
      "Iteration 255, loss = 0.09003246\n",
      "Iteration 256, loss = 0.08987217\n",
      "Iteration 333, loss = 0.09527369\n",
      "Iteration 257, loss = 0.08971030\n",
      "Iteration 265, loss = 0.21584022\n",
      "Iteration 128, loss = 0.20429110\n",
      "Iteration 258, loss = 0.08953999\n",
      "Iteration 259, loss = 0.08938071\n",
      "Iteration 260, loss = 0.08921971\n",
      "Iteration 261, loss = 0.08905642\n",
      "Iteration 248, loss = 0.13439133\n",
      "Iteration 334, loss = 0.09515665\n",
      "Iteration 207, loss = 0.16203612\n",
      "Iteration 262, loss = 0.08889819\n",
      "Iteration 263, loss = 0.08874362\n",
      "Iteration 264, loss = 0.08858769\n",
      "Iteration 265, loss = 0.08842437\n",
      "Iteration 266, loss = 0.08826926\n",
      "Iteration 267, loss = 0.08811078\n",
      "Iteration 268, loss = 0.08795777\n",
      "Iteration 269, loss = 0.08780029\n",
      "Iteration 270, loss = 0.08764865\n",
      "Iteration 335, loss = 0.09500652\n",
      "Iteration 271, loss = 0.08749331\n",
      "Iteration 272, loss = 0.08734188\n",
      "Iteration 273, loss = 0.08718893\n",
      "Iteration 274, loss = 0.08703896\n",
      "Iteration 275, loss = 0.08689693\n",
      "Iteration 276, loss = 0.08673646\n",
      "Iteration 96, loss = 0.22853911\n",
      "Iteration 336, loss = 0.09489433\n",
      "Iteration 337, loss = 0.09477684\n",
      "Iteration 129, loss = 0.20333910\n",
      "Iteration 338, loss = 0.09471864\n",
      "Iteration 277, loss = 0.08659058\n",
      "Iteration 278, loss = 0.08643840\n",
      "Iteration 279, loss = 0.08629747\n",
      "Iteration 280, loss = 0.08614608\n",
      "Iteration 281, loss = 0.08599954\n",
      "Iteration 282, loss = 0.08584947\n",
      "Iteration 283, loss = 0.08570730\n",
      "Iteration 284, loss = 0.08556373\n",
      "Iteration 285, loss = 0.08542342\n",
      "Iteration 286, loss = 0.08526865\n",
      "Iteration 339, loss = 0.09455008\n",
      "Iteration 287, loss = 0.08512660\n",
      "Iteration 22, loss = 0.51819998\n",
      "Iteration 288, loss = 0.08498103\n",
      "Iteration 130, loss = 0.20246051\n",
      "Iteration 289, loss = 0.08483844\n",
      "Iteration 290, loss = 0.08469936\n",
      "Iteration 291, loss = 0.08455381\n",
      "Iteration 249, loss = 0.13424113\n",
      "Iteration 292, loss = 0.08441096\n",
      "Iteration 208, loss = 0.16185651\n",
      "Iteration 340, loss = 0.09443426\n",
      "Iteration 293, loss = 0.08427303\n",
      "Iteration 250, loss = 0.13412712\n",
      "Iteration 294, loss = 0.08412957\n",
      "Iteration 295, loss = 0.08399874\n",
      "Iteration 296, loss = 0.08384822\n",
      "Iteration 297, loss = 0.08371592\n",
      "Iteration 97, loss = 0.22778545\n",
      "Iteration 298, loss = 0.08357551\n",
      "Iteration 266, loss = 0.21577232\n",
      "Iteration 299, loss = 0.08344596\n",
      "Iteration 341, loss = 0.09432565\n",
      "Iteration 131, loss = 0.20156157\n",
      "Iteration 209, loss = 0.16167478\n",
      "Iteration 300, loss = 0.08330127\n",
      "Iteration 301, loss = 0.08316737\n",
      "Iteration 302, loss = 0.08302854\n",
      "Iteration 303, loss = 0.08290121\n",
      "Iteration 304, loss = 0.08276320\n",
      "Iteration 210, loss = 0.16153099\n",
      "Iteration 305, loss = 0.08263342\n",
      "Iteration 251, loss = 0.13402177\n",
      "Iteration 306, loss = 0.08249647\n",
      "Iteration 307, loss = 0.08236197\n",
      "Iteration 308, loss = 0.08223759\n",
      "Iteration 342, loss = 0.09420571\n",
      "Iteration 309, loss = 0.08209862\n",
      "Iteration 310, loss = 0.08197075\n",
      "Iteration 311, loss = 0.08183955\n",
      "Iteration 312, loss = 0.08170751\n",
      "Iteration 313, loss = 0.08157916\n",
      "Iteration 314, loss = 0.08145158\n",
      "Iteration 343, loss = 0.09408895\n",
      "Iteration 315, loss = 0.08132551\n",
      "Iteration 316, loss = 0.08119992\n",
      "Iteration 317, loss = 0.08106277\n",
      "Iteration 318, loss = 0.08093746\n",
      "Iteration 319, loss = 0.08081502\n",
      "Iteration 320, loss = 0.08068798\n",
      "Iteration 321, loss = 0.08055765\n",
      "Iteration 322, loss = 0.08044203\n",
      "Iteration 323, loss = 0.08031208\n",
      "Iteration 132, loss = 0.20070390\n",
      "Iteration 344, loss = 0.09399338\n",
      "Iteration 324, loss = 0.08019242\n",
      "Iteration 98, loss = 0.22711751\n",
      "Iteration 345, loss = 0.09387737\n",
      "Iteration 325, loss = 0.08007323\n",
      "Iteration 326, loss = 0.07994571\n",
      "Iteration 252, loss = 0.13388351\n",
      "Iteration 327, loss = 0.07982687\n",
      "Iteration 23, loss = 0.51183848\n",
      "Iteration 328, loss = 0.07970107\n",
      "Iteration 329, loss = 0.07958442\n",
      "Iteration 267, loss = 0.21568904\n",
      "Iteration 330, loss = 0.07946156\n",
      "Iteration 331, loss = 0.07934329\n",
      "Iteration 332, loss = 0.07922575\n",
      "Iteration 333, loss = 0.07910632\n",
      "Iteration 334, loss = 0.07898533\n",
      "Iteration 335, loss = 0.07886606\n",
      "Iteration 336, loss = 0.07875500\n",
      "Iteration 337, loss = 0.07863722\n",
      "Iteration 346, loss = 0.09376579\n",
      "Iteration 338, loss = 0.07852223\n",
      "Iteration 253, loss = 0.13378368\n",
      "Iteration 133, loss = 0.19986923\n",
      "Iteration 339, loss = 0.07840713\n",
      "Iteration 211, loss = 0.16133023\n",
      "Iteration 347, loss = 0.09364905\n",
      "Iteration 340, loss = 0.07828731\n",
      "Iteration 341, loss = 0.07817154\n",
      "Iteration 342, loss = 0.07805049\n",
      "Iteration 343, loss = 0.07794147\n",
      "Iteration 344, loss = 0.07782786\n",
      "Iteration 254, loss = 0.13364801\n",
      "Iteration 99, loss = 0.22645597\n",
      "Iteration 348, loss = 0.09355240\n",
      "Iteration 345, loss = 0.07771227\n",
      "Iteration 255, loss = 0.13351791\n",
      "Iteration 346, loss = 0.07760366\n",
      "Iteration 134, loss = 0.19900009\n",
      "Iteration 347, loss = 0.07748354\n",
      "Iteration 348, loss = 0.07737403\n",
      "Iteration 349, loss = 0.07725888\n",
      "Iteration 350, loss = 0.07714913\n",
      "Iteration 351, loss = 0.07703449\n",
      "Iteration 352, loss = 0.07692819\n",
      "Iteration 349, loss = 0.09342567\n",
      "Iteration 135, loss = 0.19817218\n",
      "Iteration 350, loss = 0.09335018\n",
      "Iteration 24, loss = 0.50574451\n",
      "Iteration 353, loss = 0.07681719\n",
      "Iteration 354, loss = 0.07670117\n",
      "Iteration 355, loss = 0.07658764\n",
      "Iteration 356, loss = 0.07648147\n",
      "Iteration 357, loss = 0.07637002\n",
      "Iteration 358, loss = 0.07626249\n",
      "Iteration 359, loss = 0.07616045\n",
      "Iteration 268, loss = 0.21557003\n",
      "Iteration 360, loss = 0.07604506\n",
      "Iteration 361, loss = 0.07594239\n",
      "Iteration 362, loss = 0.07583426\n",
      "Iteration 363, loss = 0.07572717\n",
      "Iteration 364, loss = 0.07562264\n",
      "Iteration 365, loss = 0.07551465\n",
      "Iteration 366, loss = 0.07540212\n",
      "Iteration 367, loss = 0.07530325\n",
      "Iteration 368, loss = 0.07519326\n",
      "Iteration 369, loss = 0.07508951\n",
      "Iteration 212, loss = 0.16115824\n",
      "Iteration 370, loss = 0.07498688\n",
      "Iteration 371, loss = 0.07488444\n",
      "Iteration 372, loss = 0.07477631\n",
      "Iteration 351, loss = 0.09323624\n",
      "Iteration 373, loss = 0.07467737\n",
      "Iteration 374, loss = 0.07457080\n",
      "Iteration 375, loss = 0.07446675\n",
      "Iteration 352, loss = 0.09311381\n",
      "Iteration 353, loss = 0.09304739\n",
      "Iteration 136, loss = 0.19741653\n",
      "Iteration 376, loss = 0.07436222\n",
      "Iteration 377, loss = 0.07426934\n",
      "Iteration 213, loss = 0.16098330\n",
      "Iteration 378, loss = 0.07416155\n",
      "Iteration 379, loss = 0.07405747\n",
      "Iteration 380, loss = 0.07395963\n",
      "Iteration 381, loss = 0.07385789\n",
      "Iteration 382, loss = 0.07376629\n",
      "Iteration 256, loss = 0.13343183\n",
      "Iteration 354, loss = 0.09292653\n",
      "Iteration 100, loss = 0.22579373\n",
      "Iteration 383, loss = 0.07366035\n",
      "Iteration 384, loss = 0.07356565\n",
      "Iteration 385, loss = 0.07346258\n",
      "Iteration 257, loss = 0.13327340\n",
      "Iteration 386, loss = 0.07337212\n",
      "Iteration 387, loss = 0.07327152\n",
      "Iteration 388, loss = 0.07316850\n",
      "Iteration 389, loss = 0.07307538\n",
      "Iteration 258, loss = 0.13315071\n",
      "Iteration 390, loss = 0.07298080\n",
      "Iteration 355, loss = 0.09279954\n",
      "Iteration 391, loss = 0.07288678\n",
      "Iteration 392, loss = 0.07278711\n",
      "Iteration 393, loss = 0.07269238\n",
      "Iteration 394, loss = 0.07259428\n",
      "Iteration 137, loss = 0.19663318\n",
      "Iteration 395, loss = 0.07249989\n",
      "Iteration 396, loss = 0.07240822\n",
      "Iteration 397, loss = 0.07231407\n",
      "Iteration 398, loss = 0.07222248\n",
      "Iteration 399, loss = 0.07212899\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 356, loss = 0.09271525Iteration 269, loss = 0.21544212\n",
      "\n",
      "Iteration 1, loss = 0.69293134\n",
      "Iteration 259, loss = 0.13303770\n",
      "Iteration 2, loss = 0.66094486\n",
      "Iteration 3, loss = 0.61860027\n",
      "Iteration 214, loss = 0.16081117\n",
      "Iteration 4, loss = 0.57590690\n",
      "Iteration 5, loss = 0.53645466\n",
      "Iteration 101, loss = 0.22514807\n",
      "Iteration 6, loss = 0.50075398\n",
      "Iteration 25, loss = 0.49948458\n",
      "Iteration 7, loss = 0.47074605\n",
      "Iteration 8, loss = 0.44479224\n",
      "Iteration 9, loss = 0.42243645\n",
      "Iteration 357, loss = 0.09259377\n",
      "Iteration 358, loss = 0.09250042\n",
      "Iteration 260, loss = 0.13295223\n",
      "Iteration 138, loss = 0.19587703\n",
      "Iteration 359, loss = 0.09241459\n",
      "Iteration 261, loss = 0.13281362\n",
      "Iteration 10, loss = 0.40307970\n",
      "Iteration 270, loss = 0.21536763\n",
      "Iteration 11, loss = 0.38619188\n",
      "Iteration 12, loss = 0.37102581\n",
      "Iteration 13, loss = 0.35783011\n",
      "Iteration 14, loss = 0.34586614\n",
      "Iteration 15, loss = 0.33502187\n",
      "Iteration 16, loss = 0.32505457\n",
      "Iteration 17, loss = 0.31595372\n",
      "Iteration 18, loss = 0.30761740\n",
      "Iteration 19, loss = 0.29973646\n",
      "Iteration 360, loss = 0.09230048\n",
      "Iteration 20, loss = 0.29256663\n",
      "Iteration 21, loss = 0.28575712\n",
      "Iteration 22, loss = 0.27933194\n",
      "Iteration 215, loss = 0.16064488\n",
      "Iteration 139, loss = 0.19513693\n",
      "Iteration 102, loss = 0.22449179\n",
      "Iteration 26, loss = 0.49356645\n",
      "Iteration 262, loss = 0.13268939\n",
      "Iteration 216, loss = 0.16048788\n",
      "Iteration 23, loss = 0.27331391\n",
      "Iteration 24, loss = 0.26767566\n",
      "Iteration 25, loss = 0.26238470\n",
      "Iteration 26, loss = 0.25734083\n",
      "Iteration 27, loss = 0.25259619\n",
      "Iteration 361, loss = 0.09217830\n",
      "Iteration 28, loss = 0.24812423\n",
      "Iteration 29, loss = 0.24374912\n",
      "Iteration 30, loss = 0.23964949\n",
      "Iteration 31, loss = 0.23575559\n",
      "Iteration 32, loss = 0.23198901\n",
      "Iteration 33, loss = 0.22848069\n",
      "Iteration 34, loss = 0.22499824\n",
      "Iteration 35, loss = 0.22177528\n",
      "Iteration 263, loss = 0.13257795\n",
      "Iteration 36, loss = 0.21861117\n",
      "Iteration 37, loss = 0.21552818\n",
      "Iteration 38, loss = 0.21268505\n",
      "Iteration 39, loss = 0.20988175\n",
      "Iteration 40, loss = 0.20717277\n",
      "Iteration 41, loss = 0.20455906\n",
      "Iteration 42, loss = 0.20203236\n",
      "Iteration 140, loss = 0.19440177\n",
      "Iteration 271, loss = 0.21525129\n",
      "Iteration 362, loss = 0.09208215\n",
      "Iteration 27, loss = 0.48749858\n",
      "Iteration 363, loss = 0.09203165\n",
      "Iteration 272, loss = 0.21520398\n",
      "Iteration 364, loss = 0.09191055\n",
      "Iteration 103, loss = 0.22389960\n",
      "Iteration 264, loss = 0.13247474\n",
      "Iteration 141, loss = 0.19375496\n",
      "Iteration 365, loss = 0.09179160\n",
      "Iteration 43, loss = 0.19963168\n",
      "Iteration 217, loss = 0.16030628\n",
      "Iteration 366, loss = 0.09167317\n",
      "Iteration 265, loss = 0.13235756\n",
      "Iteration 44, loss = 0.19726771\n",
      "Iteration 45, loss = 0.19499917\n",
      "Iteration 46, loss = 0.19281009\n",
      "Iteration 47, loss = 0.19070132\n",
      "Iteration 48, loss = 0.18860359\n",
      "Iteration 49, loss = 0.18662760\n",
      "Iteration 50, loss = 0.18464202\n",
      "Iteration 51, loss = 0.18280224\n",
      "Iteration 52, loss = 0.18092578\n",
      "Iteration 218, loss = 0.16015222\n",
      "Iteration 53, loss = 0.17916414\n",
      "Iteration 142, loss = 0.19301557\n",
      "Iteration 143, loss = 0.19229588\n",
      "Iteration 54, loss = 0.17746854\n",
      "Iteration 104, loss = 0.22324884\n",
      "Iteration 28, loss = 0.48149637\n",
      "Iteration 367, loss = 0.09162215\n",
      "Iteration 55, loss = 0.17571406\n",
      "Iteration 56, loss = 0.17413765\n",
      "Iteration 57, loss = 0.17252725\n",
      "Iteration 58, loss = 0.17095927\n",
      "Iteration 59, loss = 0.16945403\n",
      "Iteration 266, loss = 0.13228938\n",
      "Iteration 60, loss = 0.16799308\n",
      "Iteration 368, loss = 0.09154146\n",
      "Iteration 61, loss = 0.16653625\n",
      "Iteration 62, loss = 0.16513666\n",
      "Iteration 63, loss = 0.16375547\n",
      "Iteration 64, loss = 0.16242177\n",
      "Iteration 65, loss = 0.16111510\n",
      "Iteration 66, loss = 0.15983612\n",
      "Iteration 67, loss = 0.15856961\n",
      "Iteration 68, loss = 0.15737198\n",
      "Iteration 273, loss = 0.21506505\n",
      "Iteration 69, loss = 0.15617980\n",
      "Iteration 70, loss = 0.15498647\n",
      "Iteration 71, loss = 0.15385104\n",
      "Iteration 219, loss = 0.15996541\n",
      "Iteration 72, loss = 0.15274025\n",
      "Iteration 73, loss = 0.15163353\n",
      "Iteration 74, loss = 0.15056036\n",
      "Iteration 75, loss = 0.14951774\n",
      "Iteration 76, loss = 0.14848099\n",
      "Iteration 77, loss = 0.14745911\n",
      "Iteration 369, loss = 0.09139016Iteration 267, loss = 0.13217960\n",
      "\n",
      "Iteration 78, loss = 0.14649079\n",
      "Iteration 79, loss = 0.14551154\n",
      "Iteration 105, loss = 0.22265732\n",
      "Iteration 80, loss = 0.14454505\n",
      "Iteration 81, loss = 0.14362999\n",
      "Iteration 82, loss = 0.14273246\n",
      "Iteration 83, loss = 0.14182314\n",
      "Iteration 144, loss = 0.19165770\n",
      "Iteration 84, loss = 0.14096141\n",
      "Iteration 85, loss = 0.14008093\n",
      "Iteration 370, loss = 0.09128640\n",
      "Iteration 86, loss = 0.13926416\n",
      "Iteration 268, loss = 0.13202133\n",
      "Iteration 371, loss = 0.09117454\n",
      "Iteration 106, loss = 0.22208747\n",
      "Iteration 29, loss = 0.47573964\n",
      "Iteration 269, loss = 0.13191597\n",
      "Iteration 145, loss = 0.19097813\n",
      "Iteration 220, loss = 0.15980546\n",
      "Iteration 372, loss = 0.09109883\n",
      "Iteration 274, loss = 0.21498015\n",
      "Iteration 373, loss = 0.09103470\n",
      "Iteration 87, loss = 0.13841166\n",
      "Iteration 88, loss = 0.13760775\n",
      "Iteration 146, loss = 0.19038419\n",
      "Iteration 270, loss = 0.13179630\n",
      "Iteration 221, loss = 0.15966313\n",
      "Iteration 147, loss = 0.18978677\n",
      "Iteration 271, loss = 0.13170712\n",
      "Iteration 374, loss = 0.09096527\n",
      "Iteration 275, loss = 0.21492294\n",
      "Iteration 89, loss = 0.13677509\n",
      "Iteration 272, loss = 0.13162450\n",
      "Iteration 90, loss = 0.13602067\n",
      "Iteration 91, loss = 0.13523815\n",
      "Iteration 92, loss = 0.13447841\n",
      "Iteration 93, loss = 0.13373307\n",
      "Iteration 94, loss = 0.13299994\n",
      "Iteration 95, loss = 0.13228309\n",
      "Iteration 96, loss = 0.13156666\n",
      "Iteration 97, loss = 0.13086069\n",
      "Iteration 98, loss = 0.13016981\n",
      "Iteration 99, loss = 0.12949337\n",
      "Iteration 100, loss = 0.12882609\n",
      "Iteration 101, loss = 0.12815895\n",
      "Iteration 102, loss = 0.12752293\n",
      "Iteration 375, loss = 0.09087050\n",
      "Iteration 103, loss = 0.12687267\n",
      "Iteration 107, loss = 0.22153547\n",
      "Iteration 376, loss = 0.09070845\n",
      "Iteration 104, loss = 0.12625956\n",
      "Iteration 105, loss = 0.12563190\n",
      "Iteration 273, loss = 0.13150365\n",
      "Iteration 106, loss = 0.12502552\n",
      "Iteration 107, loss = 0.12442916\n",
      "Iteration 30, loss = 0.46979511\n",
      "Iteration 377, loss = 0.09064528\n",
      "Iteration 222, loss = 0.15947413\n",
      "Iteration 148, loss = 0.18911677\n",
      "Iteration 276, loss = 0.21480740\n",
      "Iteration 108, loss = 0.12382115\n",
      "Iteration 378, loss = 0.09059066\n",
      "Iteration 274, loss = 0.13138613\n",
      "Iteration 379, loss = 0.09049840\n",
      "Iteration 109, loss = 0.12323408\n",
      "Iteration 110, loss = 0.12267006\n",
      "Iteration 111, loss = 0.12211487\n",
      "Iteration 112, loss = 0.12155584\n",
      "Iteration 380, loss = 0.09039805\n",
      "Iteration 113, loss = 0.12100421\n",
      "Iteration 108, loss = 0.22097761\n",
      "Iteration 114, loss = 0.12045537\n",
      "Iteration 115, loss = 0.11993458\n",
      "Iteration 116, loss = 0.11940942\n",
      "Iteration 275, loss = 0.13130738\n",
      "Iteration 223, loss = 0.15932060\n",
      "Iteration 31, loss = 0.46403202\n",
      "Iteration 149, loss = 0.18856410\n",
      "Iteration 276, loss = 0.13118378\n",
      "Iteration 109, loss = 0.22041345\n",
      "Iteration 277, loss = 0.21470428\n",
      "Iteration 117, loss = 0.11887944\n",
      "Iteration 381, loss = 0.09031735\n",
      "Iteration 118, loss = 0.11836055\n",
      "Iteration 119, loss = 0.11786304\n",
      "Iteration 120, loss = 0.11736197\n",
      "Iteration 121, loss = 0.11686608\n",
      "Iteration 122, loss = 0.11637787\n",
      "Iteration 150, loss = 0.18793343\n",
      "Iteration 123, loss = 0.11589887\n",
      "Iteration 124, loss = 0.11542999\n",
      "Iteration 125, loss = 0.11495135\n",
      "Iteration 126, loss = 0.11449624\n",
      "Iteration 127, loss = 0.11404323\n",
      "Iteration 151, loss = 0.18741281\n",
      "Iteration 382, loss = 0.09020662\n",
      "Iteration 224, loss = 0.15916001\n",
      "Iteration 128, loss = 0.11360092\n",
      "Iteration 152, loss = 0.18682637\n",
      "Iteration 383, loss = 0.09015233\n",
      "Iteration 129, loss = 0.11314262\n",
      "Iteration 277, loss = 0.13110328\n",
      "Iteration 130, loss = 0.11271824\n",
      "Iteration 384, loss = 0.09005885\n",
      "Iteration 131, loss = 0.11228471\n",
      "Iteration 132, loss = 0.11185521\n",
      "Iteration 133, loss = 0.11143663\n",
      "Iteration 134, loss = 0.11102635\n",
      "Iteration 135, loss = 0.11060489\n",
      "Iteration 136, loss = 0.11019092\n",
      "Iteration 137, loss = 0.10978526\n",
      "Iteration 278, loss = 0.13099963\n",
      "Iteration 138, loss = 0.10939569\n",
      "Iteration 139, loss = 0.10900253\n",
      "Iteration 153, loss = 0.18626620\n",
      "Iteration 140, loss = 0.10861091\n",
      "Iteration 141, loss = 0.10823232\n",
      "Iteration 278, loss = 0.21461381\n",
      "Iteration 385, loss = 0.08999001\n",
      "Iteration 386, loss = 0.08990786\n",
      "Iteration 225, loss = 0.15898637\n",
      "Iteration 387, loss = 0.08981833\n",
      "Iteration 110, loss = 0.21988279\n",
      "Iteration 32, loss = 0.45802594\n",
      "Iteration 142, loss = 0.10784356\n",
      "Iteration 143, loss = 0.10746694\n",
      "Iteration 144, loss = 0.10709685\n",
      "Iteration 145, loss = 0.10672364\n",
      "Iteration 146, loss = 0.10635394\n",
      "Iteration 388, loss = 0.08975231\n",
      "Iteration 279, loss = 0.13089790\n",
      "Iteration 226, loss = 0.15885734\n",
      "Iteration 147, loss = 0.10599651\n",
      "Iteration 148, loss = 0.10564324\n",
      "Iteration 389, loss = 0.08965277\n",
      "Iteration 149, loss = 0.10528741\n",
      "Iteration 150, loss = 0.10492709\n",
      "Iteration 154, loss = 0.18575550\n",
      "Iteration 151, loss = 0.10458678\n",
      "Iteration 152, loss = 0.10424156\n",
      "Iteration 153, loss = 0.10389971\n",
      "Iteration 155, loss = 0.18519618\n",
      "Iteration 227, loss = 0.15866452\n",
      "Iteration 111, loss = 0.21935640\n",
      "Iteration 154, loss = 0.10356259\n",
      "Iteration 155, loss = 0.10323183\n",
      "Iteration 156, loss = 0.10289187\n",
      "Iteration 228, loss = 0.15850529\n",
      "Iteration 156, loss = 0.18468978Iteration 157, loss = 0.10257504\n",
      "\n",
      "Iteration 33, loss = 0.45225419\n",
      "Iteration 390, loss = 0.08955794\n",
      "Iteration 158, loss = 0.10225175\n",
      "Iteration 279, loss = 0.21451656\n",
      "Iteration 280, loss = 0.13079822\n",
      "Iteration 159, loss = 0.10194247\n",
      "Iteration 391, loss = 0.08948790\n",
      "Iteration 281, loss = 0.13072078\n",
      "Iteration 392, loss = 0.08941334\n",
      "Iteration 160, loss = 0.10161795\n",
      "Iteration 161, loss = 0.10130859\n",
      "Iteration 162, loss = 0.10099195\n",
      "Iteration 393, loss = 0.08937707\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 163, loss = 0.10068670\n",
      "Iteration 157, loss = 0.18419431\n",
      "Iteration 164, loss = 0.10038578\n",
      "Iteration 229, loss = 0.15837082\n",
      "Iteration 165, loss = 0.10007659\n",
      "Iteration 1, loss = 0.70130877\n",
      "Iteration 280, loss = 0.21443970\n",
      "Iteration 2, loss = 0.66590618\n",
      "Iteration 112, loss = 0.21883937\n",
      "Iteration 3, loss = 0.61924551\n",
      "Iteration 166, loss = 0.09977704\n",
      "Iteration 282, loss = 0.13062593\n",
      "Iteration 167, loss = 0.09948257\n",
      "Iteration 168, loss = 0.09918594\n",
      "Iteration 169, loss = 0.09889518\n",
      "Iteration 170, loss = 0.09860604\n",
      "Iteration 171, loss = 0.09832799\n",
      "Iteration 172, loss = 0.09804232\n",
      "Iteration 173, loss = 0.09776374\n",
      "Iteration 174, loss = 0.09747806\n",
      "Iteration 175, loss = 0.09719618\n",
      "Iteration 176, loss = 0.09692899\n",
      "Iteration 177, loss = 0.09665382\n",
      "Iteration 283, loss = 0.13050059\n",
      "Iteration 178, loss = 0.09639463\n",
      "Iteration 158, loss = 0.18370949\n",
      "Iteration 4, loss = 0.57255667\n",
      "Iteration 179, loss = 0.09611631\n",
      "Iteration 180, loss = 0.09584896\n",
      "Iteration 230, loss = 0.15818942\n",
      "Iteration 181, loss = 0.09559597\n",
      "Iteration 182, loss = 0.09533130\n",
      "Iteration 183, loss = 0.09506632\n",
      "Iteration 34, loss = 0.44643413\n",
      "Iteration 184, loss = 0.09481993\n",
      "Iteration 159, loss = 0.18321474\n",
      "Iteration 284, loss = 0.13043331\n",
      "Iteration 231, loss = 0.15805313\n",
      "Iteration 113, loss = 0.21833482\n",
      "Iteration 5, loss = 0.53043906\n",
      "Iteration 6, loss = 0.49407769\n",
      "Iteration 7, loss = 0.46372517\n",
      "Iteration 185, loss = 0.09455158\n",
      "Iteration 8, loss = 0.43755930\n",
      "Iteration 9, loss = 0.41591075\n",
      "Iteration 186, loss = 0.09430911\n",
      "Iteration 10, loss = 0.39735732\n",
      "Iteration 11, loss = 0.38101673\n",
      "Iteration 12, loss = 0.36661878\n",
      "Iteration 281, loss = 0.21442882\n",
      "Iteration 285, loss = 0.13032209\n",
      "Iteration 13, loss = 0.35426200\n",
      "Iteration 187, loss = 0.09406616\n",
      "Iteration 188, loss = 0.09381061\n",
      "Iteration 14, loss = 0.34289644\n",
      "Iteration 189, loss = 0.09356017\n",
      "Iteration 35, loss = 0.44091888\n",
      "Iteration 190, loss = 0.09331779\n",
      "Iteration 15, loss = 0.33249502\n",
      "Iteration 16, loss = 0.32305078\n",
      "Iteration 160, loss = 0.18276550\n",
      "Iteration 191, loss = 0.09307732\n",
      "Iteration 17, loss = 0.31435741\n",
      "Iteration 192, loss = 0.09283840\n",
      "Iteration 18, loss = 0.30635811\n",
      "Iteration 193, loss = 0.09260220\n",
      "Iteration 19, loss = 0.29877437\n",
      "Iteration 194, loss = 0.09236267\n",
      "Iteration 195, loss = 0.09212627\n",
      "Iteration 196, loss = 0.09189037\n",
      "Iteration 232, loss = 0.15789114\n",
      "Iteration 197, loss = 0.09166339\n",
      "Iteration 286, loss = 0.13022390\n",
      "Iteration 20, loss = 0.29165968\n",
      "Iteration 198, loss = 0.09143193\n",
      "Iteration 199, loss = 0.09121032\n",
      "Iteration 200, loss = 0.09098575\n",
      "Iteration 21, loss = 0.28510590\n",
      "Iteration 114, loss = 0.21780929\n",
      "Iteration 22, loss = 0.27890304\n",
      "Iteration 201, loss = 0.09075514\n",
      "Iteration 161, loss = 0.18228827\n",
      "Iteration 23, loss = 0.27292079\n",
      "Iteration 202, loss = 0.09053894\n",
      "Iteration 203, loss = 0.09031506\n",
      "Iteration 287, loss = 0.13014503\n",
      "Iteration 204, loss = 0.09009907\n",
      "Iteration 24, loss = 0.26735285\n",
      "Iteration 205, loss = 0.08987817\n",
      "Iteration 206, loss = 0.08966400\n",
      "Iteration 25, loss = 0.26207702\n",
      "Iteration 207, loss = 0.08944539\n",
      "Iteration 208, loss = 0.08923760\n",
      "Iteration 26, loss = 0.25709934\n",
      "Iteration 282, loss = 0.21425544\n",
      "Iteration 233, loss = 0.15778915\n",
      "Iteration 209, loss = 0.08902463\n",
      "Iteration 162, loss = 0.18183467\n",
      "Iteration 210, loss = 0.08881227\n",
      "Iteration 27, loss = 0.25227119\n",
      "Iteration 28, loss = 0.24768468\n",
      "Iteration 211, loss = 0.08860273\n",
      "Iteration 29, loss = 0.24337495\n",
      "Iteration 36, loss = 0.43522513\n",
      "Iteration 212, loss = 0.08839398\n",
      "Iteration 213, loss = 0.08819408\n",
      "Iteration 214, loss = 0.08798411\n",
      "Iteration 215, loss = 0.08777889\n",
      "Iteration 216, loss = 0.08757797\n",
      "Iteration 115, loss = 0.21735949\n",
      "Iteration 217, loss = 0.08738720\n",
      "Iteration 30, loss = 0.23922193\n",
      "Iteration 218, loss = 0.08718747\n",
      "Iteration 31, loss = 0.23520123\n",
      "Iteration 219, loss = 0.08698879\n",
      "Iteration 220, loss = 0.08678122\n",
      "Iteration 32, loss = 0.23136847\n",
      "Iteration 221, loss = 0.08658617\n",
      "Iteration 33, loss = 0.22774988\n",
      "Iteration 222, loss = 0.08639865\n",
      "Iteration 223, loss = 0.08620604\n",
      "Iteration 224, loss = 0.08601943\n",
      "Iteration 225, loss = 0.08582686\n",
      "Iteration 34, loss = 0.22423546\n",
      "Iteration 226, loss = 0.08563723\n",
      "Iteration 35, loss = 0.22083336\n",
      "Iteration 163, loss = 0.18140798\n",
      "Iteration 36, loss = 0.21754778\n",
      "Iteration 37, loss = 0.21444802\n",
      "Iteration 288, loss = 0.13003438\n",
      "Iteration 234, loss = 0.15756928\n",
      "Iteration 227, loss = 0.08545245\n",
      "Iteration 38, loss = 0.21142611\n",
      "Iteration 39, loss = 0.20855774\n",
      "Iteration 40, loss = 0.20569253\n",
      "Iteration 228, loss = 0.08526856\n",
      "Iteration 164, loss = 0.18095193\n",
      "Iteration 229, loss = 0.08508124\n",
      "Iteration 230, loss = 0.08490006\n",
      "Iteration 231, loss = 0.08472047\n",
      "Iteration 232, loss = 0.08453694\n",
      "Iteration 41, loss = 0.20298474\n",
      "Iteration 289, loss = 0.12996529\n",
      "Iteration 233, loss = 0.08435700\n",
      "Iteration 234, loss = 0.08417534\n",
      "Iteration 42, loss = 0.20036221\n",
      "Iteration 235, loss = 0.15743249\n",
      "Iteration 235, loss = 0.08400019\n",
      "Iteration 283, loss = 0.21422964\n",
      "Iteration 43, loss = 0.19785775\n",
      "Iteration 116, loss = 0.21687398\n",
      "Iteration 236, loss = 0.08382617\n",
      "Iteration 44, loss = 0.19540731\n",
      "Iteration 237, loss = 0.08364486\n",
      "Iteration 45, loss = 0.19305385\n",
      "Iteration 238, loss = 0.08347822\n",
      "Iteration 46, loss = 0.19074643\n",
      "Iteration 290, loss = 0.12986736\n",
      "Iteration 37, loss = 0.42985169\n",
      "Iteration 165, loss = 0.18051130\n",
      "Iteration 117, loss = 0.21638238\n",
      "Iteration 47, loss = 0.18850347\n",
      "Iteration 48, loss = 0.18637543\n",
      "Iteration 49, loss = 0.18426611\n",
      "Iteration 50, loss = 0.18223945\n",
      "Iteration 51, loss = 0.18026579\n",
      "Iteration 239, loss = 0.08330473\n",
      "Iteration 240, loss = 0.08312846\n",
      "Iteration 241, loss = 0.08296787\n",
      "Iteration 52, loss = 0.17834972\n",
      "Iteration 242, loss = 0.08280297\n",
      "Iteration 53, loss = 0.17648421\n",
      "Iteration 291, loss = 0.12975914\n",
      "Iteration 236, loss = 0.15727498\n",
      "Iteration 243, loss = 0.08262377\n",
      "Iteration 54, loss = 0.17466067\n",
      "Iteration 55, loss = 0.17292013\n",
      "Iteration 56, loss = 0.17122022\n",
      "Iteration 57, loss = 0.16958472\n",
      "Iteration 166, loss = 0.18011083\n",
      "Iteration 58, loss = 0.16790442\n",
      "Iteration 244, loss = 0.08246479\n",
      "Iteration 59, loss = 0.16633470\n",
      "Iteration 245, loss = 0.08229736\n",
      "Iteration 246, loss = 0.08212978\n",
      "Iteration 247, loss = 0.08196331\n",
      "Iteration 248, loss = 0.08180433\n",
      "Iteration 284, loss = 0.21411583\n",
      "Iteration 249, loss = 0.08163938\n",
      "Iteration 250, loss = 0.08147597\n",
      "Iteration 237, loss = 0.15711308\n",
      "Iteration 251, loss = 0.08132824\n",
      "Iteration 252, loss = 0.08116064\n",
      "Iteration 60, loss = 0.16481369\n",
      "Iteration 61, loss = 0.16328853\n",
      "Iteration 253, loss = 0.08100942\n",
      "Iteration 254, loss = 0.08085300Iteration 62, loss = 0.16185633\n",
      "\n",
      "Iteration 38, loss = 0.42426274\n",
      "Iteration 255, loss = 0.08069023\n",
      "Iteration 256, loss = 0.08053599\n",
      "Iteration 257, loss = 0.08038599\n",
      "Iteration 118, loss = 0.21594404\n",
      "Iteration 258, loss = 0.08023129\n",
      "Iteration 259, loss = 0.08007518\n",
      "Iteration 292, loss = 0.12966915Iteration 167, loss = 0.17970779\n",
      "Iteration 63, loss = 0.16039337\n",
      "Iteration 260, loss = 0.07992918\n",
      "\n",
      "Iteration 64, loss = 0.15900170\n",
      "Iteration 261, loss = 0.07977642\n",
      "Iteration 65, loss = 0.15766096\n",
      "Iteration 66, loss = 0.15633466\n",
      "Iteration 262, loss = 0.07962959\n",
      "Iteration 263, loss = 0.07948022\n",
      "Iteration 264, loss = 0.07932965\n",
      "Iteration 67, loss = 0.15502080\n",
      "Iteration 265, loss = 0.07918162\n",
      "Iteration 168, loss = 0.17924271\n",
      "Iteration 285, loss = 0.21403670\n",
      "Iteration 68, loss = 0.15377464\n",
      "Iteration 69, loss = 0.15253945\n",
      "Iteration 70, loss = 0.15131781\n",
      "Iteration 71, loss = 0.15015869\n",
      "Iteration 72, loss = 0.14899573\n",
      "Iteration 238, loss = 0.15696476\n",
      "Iteration 73, loss = 0.14785012\n",
      "Iteration 293, loss = 0.12957971\n",
      "Iteration 266, loss = 0.07903427\n",
      "Iteration 74, loss = 0.14675757\n",
      "Iteration 39, loss = 0.41885414\n",
      "Iteration 267, loss = 0.07889122\n",
      "Iteration 268, loss = 0.07874646\n",
      "Iteration 119, loss = 0.21546124\n",
      "Iteration 169, loss = 0.17892406\n",
      "Iteration 294, loss = 0.12951654\n",
      "Iteration 269, loss = 0.07860945\n",
      "Iteration 170, loss = 0.17848380\n",
      "Iteration 75, loss = 0.14567816\n",
      "Iteration 270, loss = 0.07845560\n",
      "Iteration 76, loss = 0.14463004\n",
      "Iteration 77, loss = 0.14359162\n",
      "Iteration 271, loss = 0.07831320\n",
      "Iteration 78, loss = 0.14255846\n",
      "Iteration 79, loss = 0.14156177\n",
      "Iteration 272, loss = 0.07817053\n",
      "Iteration 80, loss = 0.14060536\n",
      "Iteration 81, loss = 0.13965168\n",
      "Iteration 82, loss = 0.13869714\n",
      "Iteration 273, loss = 0.07803318\n",
      "Iteration 83, loss = 0.13778604\n",
      "Iteration 274, loss = 0.07789204\n",
      "Iteration 275, loss = 0.07775377\n",
      "Iteration 276, loss = 0.07761130\n",
      "Iteration 120, loss = 0.21503649\n",
      "Iteration 239, loss = 0.15685016\n",
      "Iteration 277, loss = 0.07747580\n",
      "Iteration 278, loss = 0.07733979\n",
      "Iteration 84, loss = 0.13689248\n",
      "Iteration 295, loss = 0.12939750\n",
      "Iteration 85, loss = 0.13600395\n",
      "Iteration 171, loss = 0.17812690\n",
      "Iteration 86, loss = 0.13515040\n",
      "Iteration 87, loss = 0.13428445\n",
      "Iteration 88, loss = 0.13346508\n",
      "Iteration 89, loss = 0.13264483\n",
      "Iteration 90, loss = 0.13183295\n",
      "Iteration 296, loss = 0.12932162\n",
      "Iteration 286, loss = 0.21394270\n",
      "Iteration 279, loss = 0.07719949\n",
      "Iteration 240, loss = 0.15668211\n",
      "Iteration 280, loss = 0.07706173\n",
      "Iteration 281, loss = 0.07692591\n",
      "Iteration 282, loss = 0.07679466\n",
      "Iteration 283, loss = 0.07666091\n",
      "Iteration 297, loss = 0.12920013\n",
      "Iteration 284, loss = 0.07653423\n",
      "Iteration 285, loss = 0.07639487\n",
      "Iteration 286, loss = 0.07626830\n",
      "Iteration 287, loss = 0.07613240\n",
      "Iteration 91, loss = 0.13105558\n",
      "Iteration 288, loss = 0.07600286\n",
      "Iteration 289, loss = 0.07587258\n",
      "Iteration 290, loss = 0.07574814\n",
      "Iteration 40, loss = 0.41380030\n",
      "Iteration 291, loss = 0.07561708\n",
      "Iteration 292, loss = 0.07548748\n",
      "Iteration 293, loss = 0.07536266\n",
      "Iteration 294, loss = 0.07523313\n",
      "Iteration 92, loss = 0.13026972\n",
      "Iteration 295, loss = 0.07510637\n",
      "Iteration 296, loss = 0.07498493\n",
      "Iteration 93, loss = 0.12951928\n",
      "Iteration 297, loss = 0.07486229\n",
      "Iteration 298, loss = 0.07474030\n",
      "Iteration 299, loss = 0.07461302\n",
      "Iteration 300, loss = 0.07449591\n",
      "Iteration 301, loss = 0.07437169\n",
      "Iteration 302, loss = 0.07424558\n",
      "Iteration 303, loss = 0.07412901\n",
      "Iteration 304, loss = 0.07400896\n",
      "Iteration 94, loss = 0.12876740\n",
      "Iteration 121, loss = 0.21459764\n",
      "Iteration 95, loss = 0.12802176\n",
      "Iteration 305, loss = 0.07389016\n",
      "Iteration 96, loss = 0.12731672\n",
      "Iteration 97, loss = 0.12660288\n",
      "Iteration 306, loss = 0.07377043\n",
      "Iteration 287, loss = 0.21387967\n",
      "Iteration 172, loss = 0.17772058\n",
      "Iteration 98, loss = 0.12590480\n",
      "Iteration 298, loss = 0.12912718\n",
      "Iteration 99, loss = 0.12522043\n",
      "Iteration 307, loss = 0.07365875\n",
      "Iteration 308, loss = 0.07353263Iteration 100, loss = 0.12454658\n",
      "\n",
      "Iteration 309, loss = 0.07342380\n",
      "Iteration 101, loss = 0.12389001\n",
      "Iteration 102, loss = 0.12324060\n",
      "Iteration 103, loss = 0.12260105\n",
      "Iteration 299, loss = 0.12903319\n",
      "Iteration 104, loss = 0.12196439\n",
      "Iteration 105, loss = 0.12135758\n",
      "Iteration 122, loss = 0.21415318\n",
      "Iteration 106, loss = 0.12074507\n",
      "Iteration 107, loss = 0.12015534\n",
      "Iteration 241, loss = 0.15653654\n",
      "Iteration 108, loss = 0.11955464\n",
      "Iteration 109, loss = 0.11897833\n",
      "Iteration 110, loss = 0.11840429\n",
      "Iteration 111, loss = 0.11784211\n",
      "Iteration 112, loss = 0.11728843\n",
      "Iteration 113, loss = 0.11673890\n",
      "Iteration 41, loss = 0.40853614\n",
      "Iteration 310, loss = 0.07330185\n",
      "Iteration 300, loss = 0.12896888\n",
      "Iteration 311, loss = 0.07319117\n",
      "Iteration 173, loss = 0.17733971\n",
      "Iteration 301, loss = 0.12888905\n",
      "Iteration 312, loss = 0.07308378\n",
      "Iteration 302, loss = 0.12877684\n",
      "Iteration 114, loss = 0.11621018\n",
      "Iteration 313, loss = 0.07296511\n",
      "Iteration 174, loss = 0.17697255\n",
      "Iteration 242, loss = 0.15635968\n",
      "Iteration 314, loss = 0.07284859\n",
      "Iteration 315, loss = 0.07274270\n",
      "Iteration 316, loss = 0.07262254\n",
      "Iteration 115, loss = 0.11568263\n",
      "Iteration 317, loss = 0.07251493\n",
      "Iteration 116, loss = 0.11516247\n",
      "Iteration 288, loss = 0.21379057\n",
      "Iteration 318, loss = 0.07239910\n",
      "Iteration 117, loss = 0.11464519\n",
      "Iteration 319, loss = 0.07229135\n",
      "Iteration 320, loss = 0.07218052\n",
      "Iteration 321, loss = 0.07206964\n",
      "Iteration 322, loss = 0.07196537\n",
      "Iteration 118, loss = 0.11412794\n",
      "Iteration 119, loss = 0.11363137\n",
      "Iteration 120, loss = 0.11313842\n",
      "Iteration 42, loss = 0.40353840\n",
      "Iteration 121, loss = 0.11265836\n",
      "Iteration 323, loss = 0.07185386\n",
      "Iteration 123, loss = 0.21374623\n",
      "Iteration 122, loss = 0.11217312\n",
      "Iteration 324, loss = 0.07174676\n",
      "Iteration 325, loss = 0.07163651\n",
      "Iteration 326, loss = 0.07153427\n",
      "Iteration 175, loss = 0.17659908\n",
      "Iteration 327, loss = 0.07142476\n",
      "Iteration 328, loss = 0.07132155\n",
      "Iteration 329, loss = 0.07120963\n",
      "Iteration 330, loss = 0.07110653\n",
      "Iteration 123, loss = 0.11169740\n",
      "Iteration 331, loss = 0.07100079\n",
      "Iteration 332, loss = 0.07089538\n",
      "Iteration 303, loss = 0.12869027\n",
      "Iteration 243, loss = 0.15625447\n",
      "Iteration 333, loss = 0.07080197\n",
      "Iteration 124, loss = 0.11123725\n",
      "Iteration 125, loss = 0.11079293\n",
      "Iteration 289, loss = 0.21368197\n",
      "Iteration 176, loss = 0.17625784\n",
      "Iteration 126, loss = 0.11031738\n",
      "Iteration 127, loss = 0.10987699\n",
      "Iteration 334, loss = 0.07069093\n",
      "Iteration 335, loss = 0.07058760\n",
      "Iteration 336, loss = 0.07049028\n",
      "Iteration 337, loss = 0.07038992\n",
      "Iteration 338, loss = 0.07028196\n",
      "Iteration 128, loss = 0.10942745\n",
      "Iteration 339, loss = 0.07018731\n",
      "Iteration 129, loss = 0.10899768\n",
      "Iteration 340, loss = 0.07008325\n",
      "Iteration 304, loss = 0.12860538\n",
      "Iteration 130, loss = 0.10855441\n",
      "Iteration 131, loss = 0.10815023\n",
      "Iteration 341, loss = 0.06998579\n",
      "Iteration 132, loss = 0.10772370\n",
      "Iteration 342, loss = 0.06988770\n",
      "Iteration 133, loss = 0.10731617\n",
      "Iteration 343, loss = 0.06978962\n",
      "Iteration 344, loss = 0.06969469\n",
      "Iteration 124, loss = 0.21336707\n",
      "Iteration 345, loss = 0.06960139\n",
      "Iteration 346, loss = 0.06949833\n",
      "Iteration 347, loss = 0.06940042\n",
      "Iteration 348, loss = 0.06930818\n",
      "Iteration 349, loss = 0.06921859\n",
      "Iteration 134, loss = 0.10689999\n",
      "Iteration 350, loss = 0.06911633\n",
      "Iteration 177, loss = 0.17591254\n",
      "Iteration 351, loss = 0.06902360\n",
      "Iteration 244, loss = 0.15610628\n",
      "Iteration 43, loss = 0.39850860\n",
      "Iteration 290, loss = 0.21361350\n",
      "Iteration 135, loss = 0.10650166\n",
      "Iteration 136, loss = 0.10609422\n",
      "Iteration 137, loss = 0.10570913\n",
      "Iteration 138, loss = 0.10531958\n",
      "Iteration 352, loss = 0.06892897\n",
      "Iteration 353, loss = 0.06883902\n",
      "Iteration 245, loss = 0.15596650\n",
      "Iteration 139, loss = 0.10494455\n",
      "Iteration 305, loss = 0.12852842\n",
      "Iteration 354, loss = 0.06874399\n",
      "Iteration 140, loss = 0.10455287\n",
      "Iteration 141, loss = 0.10418468\n",
      "Iteration 355, loss = 0.06864643\n",
      "Iteration 142, loss = 0.10382031\n",
      "Iteration 143, loss = 0.10345868\n",
      "Iteration 356, loss = 0.06855460\n",
      "Iteration 357, loss = 0.06846720\n",
      "Iteration 144, loss = 0.10308664\n",
      "Iteration 358, loss = 0.06837040\n",
      "Iteration 359, loss = 0.06828136\n",
      "Iteration 246, loss = 0.15580678\n",
      "Iteration 178, loss = 0.17554580\n",
      "Iteration 360, loss = 0.06819030\n",
      "Iteration 145, loss = 0.10273309\n",
      "Iteration 361, loss = 0.06809923\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 125, loss = 0.21286274\n",
      "Iteration 1, loss = 0.70445533\n",
      "Iteration 2, loss = 0.67336941\n",
      "Iteration 306, loss = 0.12843782\n",
      "Iteration 3, loss = 0.63126892\n",
      "Iteration 4, loss = 0.58949914\n",
      "Iteration 146, loss = 0.10237759\n",
      "Iteration 307, loss = 0.12833296\n",
      "Iteration 5, loss = 0.55095129\n",
      "Iteration 6, loss = 0.51776442\n",
      "Iteration 147, loss = 0.10203027\n",
      "Iteration 7, loss = 0.48918475\n",
      "Iteration 8, loss = 0.46507198\n",
      "Iteration 9, loss = 0.44363026\n",
      "Iteration 10, loss = 0.42600868Iteration 148, loss = 0.10169685\n",
      "Iteration 44, loss = 0.39360222\n",
      "Iteration 291, loss = 0.21353529\n",
      "\n",
      "Iteration 149, loss = 0.10134799\n",
      "Iteration 179, loss = 0.17522908\n",
      "Iteration 11, loss = 0.41037298\n",
      "Iteration 150, loss = 0.10100699\n",
      "Iteration 151, loss = 0.10067992\n",
      "Iteration 126, loss = 0.21247952\n",
      "Iteration 152, loss = 0.10034280\n",
      "Iteration 12, loss = 0.39643746\n",
      "Iteration 180, loss = 0.17492702\n",
      "Iteration 153, loss = 0.10000914\n",
      "Iteration 154, loss = 0.09968986\n",
      "Iteration 155, loss = 0.09937692\n",
      "Iteration 156, loss = 0.09904551\n",
      "Iteration 13, loss = 0.38398833\n",
      "Iteration 308, loss = 0.12826800\n",
      "Iteration 157, loss = 0.09873512\n",
      "Iteration 45, loss = 0.38875641\n",
      "Iteration 158, loss = 0.09842173\n",
      "Iteration 14, loss = 0.37285827\n",
      "Iteration 247, loss = 0.15566722\n",
      "Iteration 159, loss = 0.09811621\n",
      "Iteration 15, loss = 0.36286657\n",
      "Iteration 160, loss = 0.09780733\n",
      "Iteration 16, loss = 0.35349673\n",
      "Iteration 309, loss = 0.12818716\n",
      "Iteration 248, loss = 0.15554045\n",
      "Iteration 17, loss = 0.34485364\n",
      "Iteration 161, loss = 0.09750911\n",
      "Iteration 162, loss = 0.09721218\n",
      "Iteration 163, loss = 0.09691265\n",
      "Iteration 164, loss = 0.09662317\n",
      "Iteration 165, loss = 0.09633619\n",
      "Iteration 181, loss = 0.17458487\n",
      "Iteration 127, loss = 0.21210560\n",
      "Iteration 292, loss = 0.21344592\n",
      "Iteration 18, loss = 0.33672398\n",
      "Iteration 19, loss = 0.32924213\n",
      "Iteration 20, loss = 0.32193982\n",
      "Iteration 166, loss = 0.09603908\n",
      "Iteration 167, loss = 0.09576940\n",
      "Iteration 21, loss = 0.31534197\n",
      "Iteration 168, loss = 0.09547693\n",
      "Iteration 22, loss = 0.30876808\n",
      "Iteration 249, loss = 0.15539335\n",
      "Iteration 169, loss = 0.09519896\n",
      "Iteration 310, loss = 0.12809802\n",
      "Iteration 170, loss = 0.09492377\n",
      "Iteration 46, loss = 0.38409171\n",
      "Iteration 23, loss = 0.30271393\n",
      "Iteration 171, loss = 0.09466262\n",
      "Iteration 128, loss = 0.21174633\n",
      "Iteration 182, loss = 0.17423962\n",
      "Iteration 311, loss = 0.12803799\n",
      "Iteration 24, loss = 0.29676161\n",
      "Iteration 172, loss = 0.09438507\n",
      "Iteration 25, loss = 0.29118780\n",
      "Iteration 26, loss = 0.28576589\n",
      "Iteration 293, loss = 0.21338316\n",
      "Iteration 27, loss = 0.28052721\n",
      "Iteration 28, loss = 0.27549230\n",
      "Iteration 173, loss = 0.09411693\n",
      "Iteration 174, loss = 0.09384810\n",
      "Iteration 29, loss = 0.27059722\n",
      "Iteration 175, loss = 0.09358828\n",
      "Iteration 176, loss = 0.09332366\n",
      "Iteration 177, loss = 0.09306685\n",
      "Iteration 30, loss = 0.26594917\n",
      "Iteration 178, loss = 0.09281544\n",
      "Iteration 250, loss = 0.15528230\n",
      "Iteration 31, loss = 0.26144582\n",
      "Iteration 312, loss = 0.12794163\n",
      "Iteration 179, loss = 0.09255882\n",
      "Iteration 180, loss = 0.09231008\n",
      "Iteration 181, loss = 0.09205764\n",
      "Iteration 182, loss = 0.09181502\n",
      "Iteration 32, loss = 0.25695507\n",
      "Iteration 183, loss = 0.09157085\n",
      "Iteration 184, loss = 0.09132949\n",
      "Iteration 183, loss = 0.17392454\n",
      "Iteration 47, loss = 0.37938129\n",
      "Iteration 33, loss = 0.25281067\n",
      "Iteration 34, loss = 0.24867176\n",
      "Iteration 129, loss = 0.21132185\n",
      "Iteration 35, loss = 0.24461770\n",
      "Iteration 185, loss = 0.09108667\n",
      "Iteration 36, loss = 0.24079544\n",
      "Iteration 313, loss = 0.12786922\n",
      "Iteration 37, loss = 0.23706544\n",
      "Iteration 186, loss = 0.09084433\n",
      "Iteration 38, loss = 0.23341857\n",
      "Iteration 39, loss = 0.22988337\n",
      "Iteration 40, loss = 0.22648419\n",
      "Iteration 294, loss = 0.21330933\n",
      "Iteration 41, loss = 0.22312384\n",
      "Iteration 42, loss = 0.21979959\n",
      "Iteration 43, loss = 0.21672799\n",
      "Iteration 44, loss = 0.21358254\n",
      "Iteration 45, loss = 0.21063930\n",
      "Iteration 187, loss = 0.09062040\n",
      "Iteration 130, loss = 0.21092295\n",
      "Iteration 251, loss = 0.15513737\n",
      "Iteration 46, loss = 0.20778289\n",
      "Iteration 188, loss = 0.09038120\n",
      "Iteration 47, loss = 0.20493055\n",
      "Iteration 48, loss = 0.20222692\n",
      "Iteration 189, loss = 0.09014370\n",
      "Iteration 314, loss = 0.12778212\n",
      "Iteration 190, loss = 0.08992128\n",
      "Iteration 315, loss = 0.12772162\n",
      "Iteration 49, loss = 0.19957278\n",
      "Iteration 191, loss = 0.08968591\n",
      "Iteration 184, loss = 0.17361087\n",
      "Iteration 192, loss = 0.08946715\n",
      "Iteration 193, loss = 0.08924940\n",
      "Iteration 194, loss = 0.08901672\n",
      "Iteration 195, loss = 0.08881213\n",
      "Iteration 196, loss = 0.08857646\n",
      "Iteration 197, loss = 0.08836288\n",
      "Iteration 50, loss = 0.19702196\n",
      "Iteration 48, loss = 0.37482456\n",
      "Iteration 185, loss = 0.17328583\n",
      "Iteration 316, loss = 0.12763407\n",
      "Iteration 198, loss = 0.08815035\n",
      "Iteration 51, loss = 0.19446781\n",
      "Iteration 252, loss = 0.15497879\n",
      "Iteration 52, loss = 0.19207445\n",
      "Iteration 199, loss = 0.08793665\n",
      "Iteration 186, loss = 0.17297761\n",
      "Iteration 53, loss = 0.18973406\n",
      "Iteration 200, loss = 0.08772137\n",
      "Iteration 201, loss = 0.08751774\n",
      "Iteration 202, loss = 0.08730277\n",
      "Iteration 203, loss = 0.08710252\n",
      "Iteration 54, loss = 0.18745865\n",
      "Iteration 131, loss = 0.21056070\n",
      "Iteration 204, loss = 0.08689455\n",
      "Iteration 55, loss = 0.18522832\n",
      "Iteration 56, loss = 0.18304832\n",
      "Iteration 317, loss = 0.12755274Iteration 57, loss = 0.18099665\n",
      "Iteration 295, loss = 0.21324987\n",
      "\n",
      "Iteration 58, loss = 0.17893743\n",
      "Iteration 49, loss = 0.37036788\n",
      "Iteration 59, loss = 0.17689705\n",
      "Iteration 60, loss = 0.17498132\n",
      "Iteration 61, loss = 0.17313831\n",
      "Iteration 62, loss = 0.17128492\n",
      "Iteration 63, loss = 0.16949502\n",
      "Iteration 64, loss = 0.16773597\n",
      "Iteration 65, loss = 0.16606381\n",
      "Iteration 253, loss = 0.15486440\n",
      "Iteration 205, loss = 0.08668960\n",
      "Iteration 206, loss = 0.08649388\n",
      "Iteration 66, loss = 0.16443861\n",
      "Iteration 187, loss = 0.17267912\n",
      "Iteration 318, loss = 0.12748215\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 207, loss = 0.08628768\n",
      "Iteration 208, loss = 0.08609583\n",
      "Iteration 209, loss = 0.08590056\n",
      "Iteration 210, loss = 0.08569506\n",
      "Iteration 211, loss = 0.08550150\n",
      "Iteration 212, loss = 0.08531184\n",
      "Iteration 213, loss = 0.08512211\n",
      "Iteration 214, loss = 0.08492383\n",
      "Iteration 188, loss = 0.17239800\n",
      "Iteration 254, loss = 0.15472469\n",
      "Iteration 132, loss = 0.21019856\n",
      "Iteration 215, loss = 0.08474440\n",
      "Iteration 1, loss = 0.57736952\n",
      "Iteration 2, loss = 0.56334221\n",
      "Iteration 3, loss = 0.54412740\n",
      "Iteration 67, loss = 0.16280146\n",
      "Iteration 68, loss = 0.16122041\n",
      "Iteration 69, loss = 0.15971189\n",
      "Iteration 216, loss = 0.08455087\n",
      "Iteration 70, loss = 0.15820358\n",
      "Iteration 71, loss = 0.15676739\n",
      "Iteration 72, loss = 0.15532464\n",
      "Iteration 73, loss = 0.15395554\n",
      "Iteration 74, loss = 0.15258679\n",
      "Iteration 75, loss = 0.15130971\n",
      "Iteration 76, loss = 0.14997527\n",
      "Iteration 77, loss = 0.14871859\n",
      "Iteration 4, loss = 0.52373414\n",
      "Iteration 217, loss = 0.08437261\n",
      "Iteration 5, loss = 0.50329042\n",
      "Iteration 296, loss = 0.21315925\n",
      "Iteration 50, loss = 0.36597392\n",
      "Iteration 78, loss = 0.14749086\n",
      "Iteration 218, loss = 0.08418337\n",
      "Iteration 79, loss = 0.14628517\n",
      "Iteration 6, loss = 0.48425922\n",
      "Iteration 7, loss = 0.46702979\n",
      "Iteration 80, loss = 0.14508114\n",
      "Iteration 219, loss = 0.08400593\n",
      "Iteration 81, loss = 0.14391949\n",
      "Iteration 220, loss = 0.08381741\n",
      "Iteration 8, loss = 0.45081158\n",
      "Iteration 82, loss = 0.14279418\n",
      "Iteration 221, loss = 0.08364427\n",
      "Iteration 189, loss = 0.17211170\n",
      "Iteration 222, loss = 0.08345885\n",
      "Iteration 83, loss = 0.14168358\n",
      "Iteration 223, loss = 0.08328099\n",
      "Iteration 9, loss = 0.43616170\n",
      "Iteration 224, loss = 0.08310774\n",
      "Iteration 255, loss = 0.15463330\n",
      "Iteration 225, loss = 0.08294219\n",
      "Iteration 226, loss = 0.08276836\n",
      "Iteration 10, loss = 0.42260507\n",
      "Iteration 84, loss = 0.14059945\n",
      "Iteration 227, loss = 0.08258985\n",
      "Iteration 190, loss = 0.17178612\n",
      "Iteration 85, loss = 0.13954090\n",
      "Iteration 133, loss = 0.20982441\n",
      "Iteration 86, loss = 0.13847851\n",
      "Iteration 228, loss = 0.08241858\n",
      "Iteration 229, loss = 0.08224799\n",
      "Iteration 87, loss = 0.13745895\n",
      "Iteration 88, loss = 0.13647592\n",
      "Iteration 230, loss = 0.08208270\n",
      "Iteration 89, loss = 0.13549310\n",
      "Iteration 90, loss = 0.13453905\n",
      "Iteration 91, loss = 0.13361980\n",
      "Iteration 231, loss = 0.08191498\n",
      "Iteration 11, loss = 0.40999907\n",
      "Iteration 92, loss = 0.13267000\n",
      "Iteration 93, loss = 0.13176369\n",
      "Iteration 134, loss = 0.20947464\n",
      "Iteration 94, loss = 0.13091955\n",
      "Iteration 95, loss = 0.13001679\n",
      "Iteration 96, loss = 0.12917991\n",
      "Iteration 97, loss = 0.12834729\n",
      "Iteration 191, loss = 0.17151482\n",
      "Iteration 98, loss = 0.12753555\n",
      "Iteration 99, loss = 0.12673961\n",
      "Iteration 12, loss = 0.39816529\n",
      "Iteration 232, loss = 0.08174478\n",
      "Iteration 233, loss = 0.08158400\n",
      "Iteration 234, loss = 0.08142328\n",
      "Iteration 100, loss = 0.12592383\n",
      "Iteration 256, loss = 0.15448470\n",
      "Iteration 235, loss = 0.08125618\n",
      "Iteration 101, loss = 0.12513700\n",
      "Iteration 236, loss = 0.08110198\n",
      "Iteration 102, loss = 0.12441675\n",
      "Iteration 237, loss = 0.08093166\n",
      "Iteration 51, loss = 0.36194189\n",
      "Iteration 103, loss = 0.12366141\n",
      "Iteration 192, loss = 0.17123551\n",
      "Iteration 13, loss = 0.38725395\n",
      "Iteration 238, loss = 0.08077794\n",
      "Iteration 297, loss = 0.21306383\n",
      "Iteration 14, loss = 0.37688192\n",
      "Iteration 104, loss = 0.12291840\n",
      "Iteration 15, loss = 0.36705079\n",
      "Iteration 239, loss = 0.08062361\n",
      "Iteration 16, loss = 0.35767377\n",
      "Iteration 240, loss = 0.08046570\n",
      "Iteration 105, loss = 0.12219246\n",
      "Iteration 241, loss = 0.08030788\n",
      "Iteration 106, loss = 0.12147346\n",
      "Iteration 135, loss = 0.20907980\n",
      "Iteration 193, loss = 0.17096538\n",
      "Iteration 107, loss = 0.12081605\n",
      "Iteration 17, loss = 0.34873229\n",
      "Iteration 108, loss = 0.12009347\n",
      "Iteration 109, loss = 0.11940820\n",
      "Iteration 110, loss = 0.11874116\n",
      "Iteration 111, loss = 0.11807330\n",
      "Iteration 52, loss = 0.35762473\n",
      "Iteration 112, loss = 0.11745220\n",
      "Iteration 242, loss = 0.08014361Iteration 257, loss = 0.15432144\n",
      "\n",
      "Iteration 18, loss = 0.34037935\n",
      "Iteration 243, loss = 0.07999456\n",
      "Iteration 244, loss = 0.07984081\n",
      "Iteration 113, loss = 0.11681004\n",
      "Iteration 245, loss = 0.07968682\n",
      "Iteration 246, loss = 0.07953545\n",
      "Iteration 19, loss = 0.33219630\n",
      "Iteration 247, loss = 0.07938581\n",
      "Iteration 114, loss = 0.11617210\n",
      "Iteration 20, loss = 0.32446378\n",
      "Iteration 248, loss = 0.07923728\n",
      "Iteration 21, loss = 0.31685560\n",
      "Iteration 249, loss = 0.07908762\n",
      "Iteration 194, loss = 0.17067847\n",
      "Iteration 250, loss = 0.07894124\n",
      "Iteration 251, loss = 0.07879595\n",
      "Iteration 22, loss = 0.30967769\n",
      "Iteration 298, loss = 0.21308358\n",
      "Iteration 252, loss = 0.07865461\n",
      "Iteration 115, loss = 0.11556663\n",
      "Iteration 253, loss = 0.07850716\n",
      "Iteration 23, loss = 0.30242739\n",
      "Iteration 136, loss = 0.20875268\n",
      "Iteration 116, loss = 0.11495752\n",
      "Iteration 24, loss = 0.29586261\n",
      "Iteration 117, loss = 0.11438700\n",
      "Iteration 25, loss = 0.28913168\n",
      "Iteration 118, loss = 0.11379501\n",
      "Iteration 26, loss = 0.28305318\n",
      "Iteration 254, loss = 0.07835788\n",
      "Iteration 258, loss = 0.15419668\n",
      "Iteration 255, loss = 0.07821563\n",
      "Iteration 256, loss = 0.07806970\n",
      "Iteration 257, loss = 0.07793719\n",
      "Iteration 119, loss = 0.11320414\n",
      "Iteration 258, loss = 0.07778919\n",
      "Iteration 120, loss = 0.11264190\n",
      "Iteration 259, loss = 0.07764913\n",
      "Iteration 27, loss = 0.27675226\n",
      "Iteration 121, loss = 0.11208576\n",
      "Iteration 28, loss = 0.27101392\n",
      "Iteration 29, loss = 0.26529759\n",
      "Iteration 122, loss = 0.11153223\n",
      "Iteration 260, loss = 0.07750833\n",
      "Iteration 261, loss = 0.07737607\n",
      "Iteration 123, loss = 0.11099937\n",
      "Iteration 262, loss = 0.07723830\n",
      "Iteration 263, loss = 0.07709727\n",
      "Iteration 124, loss = 0.11045710\n",
      "Iteration 125, loss = 0.10991940\n",
      "Iteration 126, loss = 0.10940812\n",
      "Iteration 127, loss = 0.10890151\n",
      "Iteration 299, loss = 0.21294954\n",
      "Iteration 195, loss = 0.17047803\n",
      "Iteration 30, loss = 0.25989379\n",
      "Iteration 31, loss = 0.25456008\n",
      "Iteration 128, loss = 0.10838865\n",
      "Iteration 53, loss = 0.35344611\n",
      "Iteration 259, loss = 0.15404552\n",
      "Iteration 264, loss = 0.07696285\n",
      "Iteration 129, loss = 0.10787905\n",
      "Iteration 265, loss = 0.07683195\n",
      "Iteration 137, loss = 0.20840064\n",
      "Iteration 130, loss = 0.10741523\n",
      "Iteration 266, loss = 0.07670584\n",
      "Iteration 267, loss = 0.07656501Iteration 131, loss = 0.10692763\n",
      "Iteration 32, loss = 0.24949472\n",
      "\n",
      "Iteration 33, loss = 0.24454581\n",
      "Iteration 268, loss = 0.07642977\n",
      "Iteration 34, loss = 0.23995543\n",
      "Iteration 269, loss = 0.07629553\n",
      "Iteration 270, loss = 0.07616486\n",
      "Iteration 260, loss = 0.15393016\n",
      "Iteration 35, loss = 0.23536645\n",
      "Iteration 271, loss = 0.07604050\n",
      "Iteration 272, loss = 0.07590293\n",
      "Iteration 36, loss = 0.23108549\n",
      "Iteration 273, loss = 0.07577750\n",
      "Iteration 37, loss = 0.22683171\n",
      "Iteration 274, loss = 0.07565296\n",
      "Iteration 196, loss = 0.17016166\n",
      "Iteration 132, loss = 0.10644941\n",
      "Iteration 133, loss = 0.10596575\n",
      "Iteration 38, loss = 0.22297989\n",
      "Iteration 275, loss = 0.07552137\n",
      "Iteration 39, loss = 0.21902348\n",
      "Iteration 276, loss = 0.07540122\n",
      "Iteration 134, loss = 0.10554391\n",
      "Iteration 277, loss = 0.07527047\n",
      "Iteration 135, loss = 0.10504676\n",
      "Iteration 136, loss = 0.10462840\n",
      "Iteration 137, loss = 0.10416302\n",
      "Iteration 197, loss = 0.16988564\n",
      "Iteration 278, loss = 0.07514608\n",
      "Iteration 279, loss = 0.07502093\n",
      "Iteration 280, loss = 0.07489502\n",
      "Iteration 138, loss = 0.10372508\n",
      "Iteration 281, loss = 0.07477776\n",
      "Iteration 40, loss = 0.21542055\n",
      "Iteration 139, loss = 0.10329729\n",
      "Iteration 140, loss = 0.10286905\n",
      "Iteration 41, loss = 0.21183529\n",
      "Iteration 42, loss = 0.20829098\n",
      "Iteration 138, loss = 0.20805005Iteration 300, loss = 0.21286303\n",
      "\n",
      "Iteration 141, loss = 0.10243712\n",
      "Iteration 282, loss = 0.07464770\n",
      "Iteration 43, loss = 0.20500636\n",
      "Iteration 44, loss = 0.20194004\n",
      "Iteration 142, loss = 0.10204515\n",
      "Iteration 54, loss = 0.34961103\n",
      "Iteration 261, loss = 0.15381079\n",
      "Iteration 143, loss = 0.10161488\n",
      "Iteration 45, loss = 0.19880981\n",
      "Iteration 198, loss = 0.16963841\n",
      "Iteration 283, loss = 0.07453702\n",
      "Iteration 144, loss = 0.10122886\n",
      "Iteration 46, loss = 0.19581362\n",
      "Iteration 284, loss = 0.07440832\n",
      "Iteration 145, loss = 0.10082402\n",
      "Iteration 285, loss = 0.07428851\n",
      "Iteration 47, loss = 0.19301036\n",
      "Iteration 48, loss = 0.19033754\n",
      "Iteration 49, loss = 0.18767204\n",
      "Iteration 286, loss = 0.07416561\n",
      "Iteration 287, loss = 0.07404259\n",
      "Iteration 50, loss = 0.18515244\n",
      "Iteration 288, loss = 0.07392493\n",
      "Iteration 262, loss = 0.15367862\n",
      "Iteration 51, loss = 0.18273710\n",
      "Iteration 289, loss = 0.07380657\n",
      "Iteration 290, loss = 0.07368974\n",
      "Iteration 52, loss = 0.18035676\n",
      "Iteration 53, loss = 0.17813436\n",
      "Iteration 54, loss = 0.17591549\n",
      "Iteration 146, loss = 0.10042218\n",
      "Iteration 139, loss = 0.20774427\n",
      "Iteration 147, loss = 0.10004073\n",
      "Iteration 148, loss = 0.09965525\n",
      "Iteration 149, loss = 0.09928364\n",
      "Iteration 291, loss = 0.07357128\n",
      "Iteration 150, loss = 0.09890576\n",
      "Iteration 55, loss = 0.34569161Iteration 151, loss = 0.09852066\n",
      "Iteration 199, loss = 0.16940429\n",
      "Iteration 292, loss = 0.07345977\n",
      "Iteration 293, loss = 0.07333974\n",
      "Iteration 152, loss = 0.09817272\n",
      "Iteration 294, loss = 0.07322585\n",
      "Iteration 153, loss = 0.09779727\n",
      "\n",
      "Iteration 55, loss = 0.17381980\n",
      "Iteration 295, loss = 0.07310794\n",
      "Iteration 154, loss = 0.09745552\n",
      "Iteration 296, loss = 0.07299295\n",
      "Iteration 297, loss = 0.07287366\n",
      "Iteration 155, loss = 0.09709477Iteration 263, loss = 0.15354817\n",
      "Iteration 200, loss = 0.16912867\n",
      "\n",
      "Iteration 56, loss = 0.17174784\n",
      "Iteration 156, loss = 0.09673865\n",
      "Iteration 157, loss = 0.09641010\n",
      "Iteration 298, loss = 0.07276134\n",
      "Iteration 158, loss = 0.09605483\n",
      "Iteration 299, loss = 0.07265131\n",
      "Iteration 300, loss = 0.07254270\n",
      "Iteration 301, loss = 0.07242423\n",
      "Iteration 159, loss = 0.09572041\n",
      "Iteration 302, loss = 0.07231779\n",
      "Iteration 303, loss = 0.07220964\n",
      "Iteration 304, loss = 0.07209265\n",
      "Iteration 57, loss = 0.16976044\n",
      "Iteration 301, loss = 0.21278330\n",
      "Iteration 201, loss = 0.16891624\n",
      "Iteration 160, loss = 0.09539164\n",
      "Iteration 161, loss = 0.09505737\n",
      "Iteration 58, loss = 0.16786472\n",
      "Iteration 162, loss = 0.09474297\n",
      "Iteration 140, loss = 0.20740375\n",
      "Iteration 305, loss = 0.07198908\n",
      "Iteration 59, loss = 0.16600757\n",
      "Iteration 306, loss = 0.07188047\n",
      "Iteration 163, loss = 0.09440608\n",
      "Iteration 60, loss = 0.16422180\n",
      "Iteration 264, loss = 0.15342686\n",
      "Iteration 202, loss = 0.16866270\n",
      "Iteration 307, loss = 0.07176759\n",
      "Iteration 164, loss = 0.09410900\n",
      "Iteration 302, loss = 0.21270543\n",
      "Iteration 165, loss = 0.09376659\n",
      "Iteration 308, loss = 0.07166696\n",
      "Iteration 141, loss = 0.20704794\n",
      "Iteration 166, loss = 0.09346177\n",
      "Iteration 167, loss = 0.09316996\n",
      "Iteration 168, loss = 0.09285291\n",
      "Iteration 61, loss = 0.16246609\n",
      "Iteration 309, loss = 0.07155128\n",
      "Iteration 169, loss = 0.09255237\n",
      "Iteration 170, loss = 0.09226350\n",
      "Iteration 310, loss = 0.07144497\n",
      "Iteration 62, loss = 0.16082364\n",
      "Iteration 56, loss = 0.34201260\n",
      "Iteration 171, loss = 0.09197734\n",
      "Iteration 172, loss = 0.09168802\n",
      "Iteration 311, loss = 0.07134102\n",
      "Iteration 63, loss = 0.15914543\n",
      "Iteration 173, loss = 0.09138294\n",
      "Iteration 312, loss = 0.07123494Iteration 64, loss = 0.15753941\n",
      "\n",
      "Iteration 174, loss = 0.09109681\n",
      "Iteration 65, loss = 0.15601181\n",
      "Iteration 313, loss = 0.07112825\n",
      "Iteration 66, loss = 0.15449400\n",
      "Iteration 175, loss = 0.09081794\n",
      "Iteration 67, loss = 0.15301341\n",
      "Iteration 68, loss = 0.15158913\n",
      "Iteration 69, loss = 0.15026264\n",
      "Iteration 314, loss = 0.07103117\n",
      "Iteration 70, loss = 0.14883354\n",
      "Iteration 315, loss = 0.07092376\n",
      "Iteration 316, loss = 0.07081843\n",
      "Iteration 71, loss = 0.14755547\n",
      "Iteration 176, loss = 0.09055674\n",
      "Iteration 317, loss = 0.07071317\n",
      "Iteration 72, loss = 0.14626710\n",
      "Iteration 73, loss = 0.14504447\n",
      "Iteration 318, loss = 0.07061086\n",
      "Iteration 203, loss = 0.16841506\n",
      "Iteration 57, loss = 0.33838727\n",
      "Iteration 177, loss = 0.09027369\n",
      "Iteration 265, loss = 0.15331702\n",
      "Iteration 319, loss = 0.07051089\n",
      "Iteration 266, loss = 0.15317331\n",
      "Iteration 178, loss = 0.08999770\n",
      "Iteration 179, loss = 0.08973171\n",
      "Iteration 180, loss = 0.08945961\n",
      "Iteration 181, loss = 0.08920048\n",
      "Iteration 182, loss = 0.08893193\n",
      "Iteration 74, loss = 0.14384709\n",
      "Iteration 183, loss = 0.08867893Iteration 75, loss = 0.14266255\n",
      "\n",
      "Iteration 76, loss = 0.14152285\n",
      "Iteration 320, loss = 0.07040558\n",
      "Iteration 321, loss = 0.07030723\n",
      "Iteration 77, loss = 0.14039205\n",
      "Iteration 322, loss = 0.07020359\n",
      "Iteration 323, loss = 0.07010472Iteration 78, loss = 0.13933499\n",
      "\n",
      "Iteration 204, loss = 0.16820254\n",
      "Iteration 324, loss = 0.07000457\n",
      "Iteration 184, loss = 0.08841938\n",
      "Iteration 79, loss = 0.13827334\n",
      "Iteration 80, loss = 0.13726938\n",
      "Iteration 185, loss = 0.08817668\n",
      "Iteration 142, loss = 0.20671892\n",
      "Iteration 186, loss = 0.08791839\n",
      "Iteration 187, loss = 0.08767020\n",
      "Iteration 188, loss = 0.08742731\n",
      "Iteration 189, loss = 0.08718642\n",
      "Iteration 325, loss = 0.06990314\n",
      "Iteration 190, loss = 0.08694510\n",
      "Iteration 191, loss = 0.08670301\n",
      "Iteration 303, loss = 0.21265412\n",
      "Iteration 192, loss = 0.08646402\n",
      "Iteration 193, loss = 0.08623300\n",
      "Iteration 194, loss = 0.08599080\n",
      "Iteration 195, loss = 0.08576404\n",
      "Iteration 326, loss = 0.06980765\n",
      "Iteration 196, loss = 0.08553112\n",
      "Iteration 81, loss = 0.13625335\n",
      "Iteration 327, loss = 0.06970341\n",
      "Iteration 267, loss = 0.15306747\n",
      "Iteration 328, loss = 0.06960755\n",
      "Iteration 205, loss = 0.16793910\n",
      "Iteration 329, loss = 0.06950793\n",
      "Iteration 330, loss = 0.06941159\n",
      "Iteration 331, loss = 0.06931800\n",
      "Iteration 58, loss = 0.33483347\n",
      "Iteration 332, loss = 0.06922450\n",
      "Iteration 333, loss = 0.06912216\n",
      "Iteration 82, loss = 0.13526141\n",
      "Iteration 197, loss = 0.08530371\n",
      "Iteration 143, loss = 0.20638717\n",
      "Iteration 83, loss = 0.13429611\n",
      "Iteration 268, loss = 0.15294428\n",
      "Iteration 198, loss = 0.08509735\n",
      "Iteration 199, loss = 0.08485716\n",
      "Iteration 206, loss = 0.16770468\n",
      "Iteration 200, loss = 0.08463274\n",
      "Iteration 201, loss = 0.08441451\n",
      "Iteration 202, loss = 0.08419496\n",
      "Iteration 203, loss = 0.08396877\n",
      "Iteration 204, loss = 0.08375317\n",
      "Iteration 84, loss = 0.13339016\n",
      "Iteration 85, loss = 0.13247307\n",
      "Iteration 86, loss = 0.13156999\n",
      "Iteration 87, loss = 0.13070929\n",
      "Iteration 88, loss = 0.12986199\n",
      "Iteration 89, loss = 0.12902438\n",
      "Iteration 334, loss = 0.06903287\n",
      "Iteration 90, loss = 0.12821250\n",
      "Iteration 335, loss = 0.06893380\n",
      "Iteration 336, loss = 0.06883915\n",
      "Iteration 337, loss = 0.06874713\n",
      "Iteration 59, loss = 0.33147893\n",
      "Iteration 338, loss = 0.06865104\n",
      "Iteration 144, loss = 0.20607354\n",
      "Iteration 339, loss = 0.06856593\n",
      "Iteration 91, loss = 0.12741681\n",
      "Iteration 304, loss = 0.21258346\n",
      "Iteration 340, loss = 0.06847023\n",
      "Iteration 341, loss = 0.06837794\n",
      "Iteration 92, loss = 0.12664068\n",
      "Iteration 205, loss = 0.08353517\n",
      "Iteration 93, loss = 0.12588566\n",
      "Iteration 342, loss = 0.06828189\n",
      "Iteration 94, loss = 0.12514649\n",
      "Iteration 206, loss = 0.08333414\n",
      "Iteration 95, loss = 0.12440970\n",
      "Iteration 343, loss = 0.06819801Iteration 96, loss = 0.12369720\n",
      "\n",
      "Iteration 207, loss = 0.16746928\n",
      "Iteration 207, loss = 0.08311858\n",
      "Iteration 97, loss = 0.12298022\n",
      "Iteration 208, loss = 0.08291342\n",
      "Iteration 209, loss = 0.08270056\n",
      "Iteration 98, loss = 0.12229660\n",
      "Iteration 210, loss = 0.08249986\n",
      "Iteration 99, loss = 0.12162977\n",
      "Iteration 100, loss = 0.12096898\n",
      "Iteration 211, loss = 0.08228275\n",
      "Iteration 344, loss = 0.06810175\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 208, loss = 0.16725307\n",
      "Iteration 1, loss = 0.74323031\n",
      "Iteration 145, loss = 0.20577315\n",
      "Iteration 269, loss = 0.15281148\n",
      "Iteration 2, loss = 0.70766278\n",
      "Iteration 3, loss = 0.66097068\n",
      "Iteration 4, loss = 0.61359663\n",
      "Iteration 5, loss = 0.57074825\n",
      "Iteration 6, loss = 0.53471833\n",
      "Iteration 7, loss = 0.50254938\n",
      "Iteration 101, loss = 0.12031425Iteration 212, loss = 0.08207873\n",
      "\n",
      "Iteration 213, loss = 0.08187561\n",
      "Iteration 214, loss = 0.08166711\n",
      "Iteration 215, loss = 0.08146146\n",
      "Iteration 270, loss = 0.15268494\n",
      "Iteration 102, loss = 0.11967158\n",
      "Iteration 8, loss = 0.47644772\n",
      "Iteration 103, loss = 0.11903833\n",
      "Iteration 9, loss = 0.45408899\n",
      "Iteration 104, loss = 0.11842162\n",
      "Iteration 60, loss = 0.32817313\n",
      "Iteration 216, loss = 0.08129286\n",
      "Iteration 305, loss = 0.21249681\n",
      "Iteration 209, loss = 0.16703991\n",
      "Iteration 105, loss = 0.11782800\n",
      "Iteration 217, loss = 0.08108132\n",
      "Iteration 106, loss = 0.11720191\n",
      "Iteration 218, loss = 0.08088053\n",
      "Iteration 219, loss = 0.08069583\n",
      "Iteration 210, loss = 0.16680038\n",
      "Iteration 220, loss = 0.08049337\n",
      "Iteration 221, loss = 0.08030323\n",
      "Iteration 107, loss = 0.11664762\n",
      "Iteration 222, loss = 0.08010955\n",
      "Iteration 108, loss = 0.11605618\n",
      "Iteration 223, loss = 0.07991441\n",
      "Iteration 109, loss = 0.11547506\n",
      "Iteration 110, loss = 0.11493260\n",
      "Iteration 10, loss = 0.43411779\n",
      "Iteration 111, loss = 0.11440021\n",
      "Iteration 112, loss = 0.11385582\n",
      "Iteration 11, loss = 0.41744470\n",
      "Iteration 113, loss = 0.11334199\n",
      "Iteration 146, loss = 0.20545455\n",
      "Iteration 114, loss = 0.11283218\n",
      "Iteration 224, loss = 0.07973356\n",
      "Iteration 12, loss = 0.40258848\n",
      "Iteration 225, loss = 0.07954404\n",
      "Iteration 226, loss = 0.07936811\n",
      "Iteration 227, loss = 0.07917716\n",
      "Iteration 271, loss = 0.15257977\n",
      "Iteration 228, loss = 0.07898991\n",
      "Iteration 211, loss = 0.16658597\n",
      "Iteration 229, loss = 0.07882777\n",
      "Iteration 230, loss = 0.07864574\n",
      "Iteration 231, loss = 0.07846473\n",
      "Iteration 115, loss = 0.11231762\n",
      "Iteration 232, loss = 0.07828252\n",
      "Iteration 233, loss = 0.07810665Iteration 306, loss = 0.21244106\n",
      "\n",
      "Iteration 13, loss = 0.38961618\n",
      "Iteration 212, loss = 0.16636986\n",
      "Iteration 234, loss = 0.07793876\n",
      "Iteration 14, loss = 0.37775177\n",
      "Iteration 147, loss = 0.20511749\n",
      "Iteration 61, loss = 0.32509874\n",
      "Iteration 116, loss = 0.11182283\n",
      "Iteration 117, loss = 0.11133855\n",
      "Iteration 235, loss = 0.07776362\n",
      "Iteration 236, loss = 0.07759734\n",
      "Iteration 118, loss = 0.11083530\n",
      "Iteration 237, loss = 0.07741993Iteration 119, loss = 0.11036798\n",
      "\n",
      "Iteration 120, loss = 0.10990152\n",
      "Iteration 121, loss = 0.10943683\n",
      "Iteration 272, loss = 0.15245018\n",
      "Iteration 15, loss = 0.36704673\n",
      "Iteration 122, loss = 0.10899550\n",
      "Iteration 213, loss = 0.16613464\n",
      "Iteration 123, loss = 0.10854441\n",
      "Iteration 16, loss = 0.35706449\n",
      "Iteration 124, loss = 0.10808675\n",
      "Iteration 125, loss = 0.10764850\n",
      "Iteration 238, loss = 0.07725789\n",
      "Iteration 239, loss = 0.07707969\n",
      "Iteration 240, loss = 0.07691917\n",
      "Iteration 273, loss = 0.15236491\n",
      "Iteration 241, loss = 0.07675241\n",
      "Iteration 242, loss = 0.07658792\n",
      "Iteration 214, loss = 0.16595890\n",
      "Iteration 243, loss = 0.07641998\n",
      "Iteration 244, loss = 0.07625565\n",
      "Iteration 126, loss = 0.10723919\n",
      "Iteration 17, loss = 0.34795120\n",
      "Iteration 245, loss = 0.07609516\n",
      "Iteration 127, loss = 0.10678756\n",
      "Iteration 128, loss = 0.10636665\n",
      "Iteration 129, loss = 0.10595469\n",
      "Iteration 18, loss = 0.33938576\n",
      "Iteration 246, loss = 0.07593577\n",
      "Iteration 215, loss = 0.16573664\n",
      "Iteration 247, loss = 0.07579252\n",
      "Iteration 148, loss = 0.20480880\n",
      "Iteration 19, loss = 0.33130095\n",
      "Iteration 248, loss = 0.07561968\n",
      "Iteration 249, loss = 0.07545609\n",
      "Iteration 62, loss = 0.32198420\n",
      "Iteration 250, loss = 0.07531114\n",
      "Iteration 130, loss = 0.10554003\n",
      "Iteration 307, loss = 0.21237969\n",
      "Iteration 20, loss = 0.32378207\n",
      "Iteration 251, loss = 0.07516099\n",
      "Iteration 131, loss = 0.10513787\n",
      "Iteration 252, loss = 0.07499685\n",
      "Iteration 21, loss = 0.31661445\n",
      "Iteration 274, loss = 0.15220694\n",
      "Iteration 22, loss = 0.30976049\n",
      "Iteration 23, loss = 0.30321099\n",
      "Iteration 253, loss = 0.07484301\n",
      "Iteration 254, loss = 0.07469651\n",
      "Iteration 255, loss = 0.07453996\n",
      "Iteration 24, loss = 0.29713957\n",
      "Iteration 256, loss = 0.07439185\n",
      "Iteration 25, loss = 0.29106571\n",
      "Iteration 257, loss = 0.07425039\n",
      "Iteration 26, loss = 0.28532953\n",
      "Iteration 27, loss = 0.27976605\n",
      "Iteration 132, loss = 0.10474172\n",
      "Iteration 28, loss = 0.27435760\n",
      "Iteration 133, loss = 0.10434351\n",
      "Iteration 134, loss = 0.10394890\n",
      "Iteration 149, loss = 0.20455563\n",
      "Iteration 135, loss = 0.10357413\n",
      "Iteration 258, loss = 0.07409232\n",
      "Iteration 29, loss = 0.26915826\n",
      "Iteration 63, loss = 0.31918987\n",
      "Iteration 30, loss = 0.26420871\n",
      "Iteration 31, loss = 0.25930988\n",
      "Iteration 259, loss = 0.07394995\n",
      "Iteration 136, loss = 0.10319780\n",
      "Iteration 260, loss = 0.07381407\n",
      "Iteration 261, loss = 0.07366398\n",
      "Iteration 137, loss = 0.10281410\n",
      "Iteration 262, loss = 0.07351412\n",
      "Iteration 275, loss = 0.15211430\n",
      "Iteration 216, loss = 0.16552759\n",
      "Iteration 308, loss = 0.21231759\n",
      "Iteration 32, loss = 0.25466538\n",
      "Iteration 138, loss = 0.10245300\n",
      "Iteration 139, loss = 0.10209230\n",
      "Iteration 33, loss = 0.25015872\n",
      "Iteration 263, loss = 0.07338056\n",
      "Iteration 140, loss = 0.10172776\n",
      "Iteration 264, loss = 0.07323911\n",
      "Iteration 141, loss = 0.10136985\n",
      "Iteration 265, loss = 0.07308960\n",
      "Iteration 266, loss = 0.07295444\n",
      "Iteration 142, loss = 0.10103219\n",
      "Iteration 276, loss = 0.15200422\n",
      "Iteration 143, loss = 0.10067141\n",
      "Iteration 144, loss = 0.10034498\n",
      "Iteration 145, loss = 0.09998998\n",
      "Iteration 267, loss = 0.07280979\n",
      "Iteration 146, loss = 0.09966872\n",
      "Iteration 34, loss = 0.24568013\n",
      "Iteration 268, loss = 0.07269826\n",
      "Iteration 269, loss = 0.07253544\n",
      "Iteration 270, loss = 0.07240621\n",
      "Iteration 271, loss = 0.07226716\n",
      "Iteration 217, loss = 0.16534914\n",
      "Iteration 272, loss = 0.07213647\n",
      "Iteration 273, loss = 0.07199968\n",
      "Iteration 274, loss = 0.07186152\n",
      "Iteration 275, loss = 0.07172227\n",
      "Iteration 150, loss = 0.20422327\n",
      "Iteration 147, loss = 0.09931421\n",
      "Iteration 148, loss = 0.09899377\n",
      "Iteration 276, loss = 0.07159250\n",
      "Iteration 277, loss = 0.15187184\n",
      "Iteration 64, loss = 0.31640922\n",
      "Iteration 35, loss = 0.24143745\n",
      "Iteration 277, loss = 0.07146052\n",
      "Iteration 218, loss = 0.16516237\n",
      "Iteration 309, loss = 0.21236597\n",
      "Iteration 278, loss = 0.07133589\n",
      "Iteration 149, loss = 0.09867123\n",
      "Iteration 279, loss = 0.07120188\n",
      "Iteration 36, loss = 0.23734811\n",
      "Iteration 219, loss = 0.16493843\n",
      "Iteration 150, loss = 0.09833738\n",
      "Iteration 280, loss = 0.07109085\n",
      "Iteration 281, loss = 0.07094401\n",
      "Iteration 282, loss = 0.07082289\n",
      "Iteration 283, loss = 0.07069751\n",
      "Iteration 284, loss = 0.07056204\n",
      "Iteration 151, loss = 0.09803161\n",
      "Iteration 37, loss = 0.23347468\n",
      "Iteration 285, loss = 0.07043479\n",
      "Iteration 38, loss = 0.22958755\n",
      "Iteration 286, loss = 0.07032535\n",
      "Iteration 39, loss = 0.22588217\n",
      "Iteration 287, loss = 0.07017970\n",
      "Iteration 152, loss = 0.09770782\n",
      "Iteration 288, loss = 0.07006413\n",
      "Iteration 151, loss = 0.20392425\n",
      "Iteration 153, loss = 0.09740158\n",
      "Iteration 154, loss = 0.09709315\n",
      "Iteration 155, loss = 0.09677977\n",
      "Iteration 156, loss = 0.09646970\n",
      "Iteration 157, loss = 0.09618545\n",
      "Iteration 158, loss = 0.09588418\n",
      "Iteration 159, loss = 0.09559162\n",
      "Iteration 160, loss = 0.09530351\n",
      "Iteration 289, loss = 0.06995351\n",
      "Iteration 278, loss = 0.15174871\n",
      "Iteration 65, loss = 0.31371210\n",
      "Iteration 310, loss = 0.21219134\n",
      "Iteration 290, loss = 0.06981228\n",
      "Iteration 291, loss = 0.06969313\n",
      "Iteration 40, loss = 0.22237701\n",
      "Iteration 292, loss = 0.06957778\n",
      "Iteration 41, loss = 0.21894838\n",
      "Iteration 293, loss = 0.06944868\n",
      "Iteration 42, loss = 0.21549319\n",
      "Iteration 220, loss = 0.16477999\n",
      "Iteration 161, loss = 0.09501219\n",
      "Iteration 43, loss = 0.21230695\n",
      "Iteration 162, loss = 0.09472871\n",
      "Iteration 163, loss = 0.09445494\n",
      "Iteration 221, loss = 0.16458659\n",
      "Iteration 164, loss = 0.09419158\n",
      "Iteration 279, loss = 0.15165269\n",
      "Iteration 44, loss = 0.20927278\n",
      "Iteration 45, loss = 0.20612219\n",
      "Iteration 46, loss = 0.20317496\n",
      "Iteration 294, loss = 0.06933619\n",
      "Iteration 295, loss = 0.06922193\n",
      "Iteration 296, loss = 0.06910740\n",
      "Iteration 297, loss = 0.06898377\n",
      "Iteration 298, loss = 0.06888934\n",
      "Iteration 299, loss = 0.06875640\n",
      "Iteration 165, loss = 0.09390396\n",
      "Iteration 300, loss = 0.06863887\n",
      "Iteration 152, loss = 0.20361414\n",
      "Iteration 166, loss = 0.09363007\n",
      "Iteration 167, loss = 0.09336988\n",
      "Iteration 168, loss = 0.09310710\n",
      "Iteration 66, loss = 0.31115686\n",
      "Iteration 47, loss = 0.20046536\n",
      "Iteration 301, loss = 0.06852143\n",
      "Iteration 222, loss = 0.16439128\n",
      "Iteration 302, loss = 0.06841599\n",
      "Iteration 280, loss = 0.15152021\n",
      "Iteration 303, loss = 0.06831088\n",
      "Iteration 311, loss = 0.21209199\n",
      "Iteration 304, loss = 0.06819402\n",
      "Iteration 305, loss = 0.06807752\n",
      "Iteration 223, loss = 0.16427216\n",
      "Iteration 306, loss = 0.06797092\n",
      "Iteration 48, loss = 0.19768436\n",
      "Iteration 307, loss = 0.06785351\n",
      "Iteration 224, loss = 0.16401258\n",
      "Iteration 308, loss = 0.06774539\n",
      "Iteration 309, loss = 0.06764161\n",
      "Iteration 310, loss = 0.06753319Iteration 153, loss = 0.20337217\n",
      "Iteration 169, loss = 0.09283252\n",
      "\n",
      "Iteration 170, loss = 0.09259598\n",
      "Iteration 171, loss = 0.09232789\n",
      "Iteration 172, loss = 0.09208010\n",
      "Iteration 173, loss = 0.09181706\n",
      "Iteration 174, loss = 0.09157402\n",
      "Iteration 49, loss = 0.19500842\n",
      "Iteration 225, loss = 0.16381721\n",
      "Iteration 175, loss = 0.09132562\n",
      "Iteration 67, loss = 0.30876926\n",
      "Iteration 176, loss = 0.09107841\n",
      "Iteration 177, loss = 0.09085225\n",
      "Iteration 281, loss = 0.15141813\n",
      "Iteration 178, loss = 0.09060224\n",
      "Iteration 179, loss = 0.09035991\n",
      "Iteration 311, loss = 0.06742183\n",
      "Iteration 312, loss = 0.06732695\n",
      "Iteration 313, loss = 0.06721546\n",
      "Iteration 314, loss = 0.06710960\n",
      "Iteration 315, loss = 0.06701018\n",
      "Iteration 316, loss = 0.06689590\n",
      "Iteration 317, loss = 0.06678932\n",
      "Iteration 50, loss = 0.19245949\n",
      "Iteration 51, loss = 0.18999432\n",
      "Iteration 226, loss = 0.16367577\n",
      "Iteration 52, loss = 0.18764013\n",
      "Iteration 318, loss = 0.06668594\n",
      "Iteration 319, loss = 0.06659146\n",
      "Iteration 320, loss = 0.06647982Iteration 53, loss = 0.18529357\n",
      "\n",
      "Iteration 227, loss = 0.16344376\n",
      "Iteration 312, loss = 0.21202775\n",
      "Iteration 54, loss = 0.18301235\n",
      "Iteration 321, loss = 0.06637600\n",
      "Iteration 180, loss = 0.09012521\n",
      "Iteration 55, loss = 0.18085340\n",
      "Iteration 181, loss = 0.08988958\n",
      "Iteration 154, loss = 0.20307510\n",
      "Iteration 282, loss = 0.15131337\n",
      "Iteration 56, loss = 0.17878345\n",
      "Iteration 182, loss = 0.08966418\n",
      "Iteration 183, loss = 0.08943904\n",
      "Iteration 68, loss = 0.30651283\n",
      "Iteration 322, loss = 0.06627746\n",
      "Iteration 323, loss = 0.06617482\n",
      "Iteration 324, loss = 0.06607759\n",
      "Iteration 325, loss = 0.06597863\n",
      "Iteration 326, loss = 0.06588408\n",
      "Iteration 327, loss = 0.06576882\n",
      "Iteration 328, loss = 0.06567960\n",
      "Iteration 329, loss = 0.06558224\n",
      "Iteration 184, loss = 0.08921001\n",
      "Iteration 57, loss = 0.17670823\n",
      "Iteration 58, loss = 0.17474592\n",
      "Iteration 330, loss = 0.06548941\n",
      "Iteration 185, loss = 0.08899144\n",
      "Iteration 331, loss = 0.06540402\n",
      "Iteration 332, loss = 0.06529257\n",
      "Iteration 333, loss = 0.06519282\n",
      "Iteration 334, loss = 0.06510445\n",
      "Iteration 335, loss = 0.06500175\n",
      "Iteration 59, loss = 0.17283372\n",
      "Iteration 186, loss = 0.08876916\n",
      "Iteration 336, loss = 0.06490787\n",
      "Iteration 337, loss = 0.06482162\n",
      "Iteration 338, loss = 0.06471957\n",
      "Iteration 60, loss = 0.17099877Iteration 313, loss = 0.21196191\n",
      "Iteration 339, loss = 0.06463324\n",
      "\n",
      "Iteration 187, loss = 0.08854388\n",
      "Iteration 340, loss = 0.06454341\n",
      "Iteration 188, loss = 0.08832126\n",
      "Iteration 341, loss = 0.06444003\n",
      "Iteration 342, loss = 0.06436022\n",
      "Iteration 189, loss = 0.08811169\n",
      "Iteration 343, loss = 0.06426347\n",
      "Iteration 344, loss = 0.06417160\n",
      "Iteration 61, loss = 0.16922993\n",
      "Iteration 190, loss = 0.08789768\n",
      "Iteration 228, loss = 0.16327278\n",
      "Iteration 345, loss = 0.06408572\n",
      "Iteration 62, loss = 0.16752685\n",
      "Iteration 191, loss = 0.08767998\n",
      "Iteration 346, loss = 0.06399306\n",
      "Iteration 347, loss = 0.06391111\n",
      "Iteration 192, loss = 0.08746116\n",
      "Iteration 63, loss = 0.16577651\n",
      "Iteration 348, loss = 0.06381666\n",
      "Iteration 155, loss = 0.20281082\n",
      "Iteration 283, loss = 0.15120372\n",
      "Iteration 64, loss = 0.16417430\n",
      "Iteration 229, loss = 0.16307471\n",
      "Iteration 193, loss = 0.08726504\n",
      "Iteration 194, loss = 0.08704371\n",
      "Iteration 195, loss = 0.08684481\n",
      "Iteration 349, loss = 0.06373448\n",
      "Iteration 65, loss = 0.16266158\n",
      "Iteration 350, loss = 0.06364108\n",
      "Iteration 351, loss = 0.06356393\n",
      "Iteration 352, loss = 0.06346757\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 284, loss = 0.15108233\n",
      "Iteration 196, loss = 0.08664331\n",
      "Iteration 1, loss = 0.79831708\n",
      "Iteration 197, loss = 0.08644658\n",
      "Iteration 2, loss = 0.75954634\n",
      "Iteration 66, loss = 0.16101953\n",
      "Iteration 198, loss = 0.08625083\n",
      "Iteration 67, loss = 0.15954963\n",
      "Iteration 156, loss = 0.20250226\n",
      "Iteration 68, loss = 0.15809187\n",
      "Iteration 69, loss = 0.30420625\n",
      "Iteration 314, loss = 0.21188993\n",
      "Iteration 199, loss = 0.08603908\n",
      "Iteration 69, loss = 0.15667982\n",
      "Iteration 70, loss = 0.15531138\n",
      "Iteration 200, loss = 0.08584769\n",
      "Iteration 230, loss = 0.16290997\n",
      "Iteration 201, loss = 0.08566817\n",
      "Iteration 3, loss = 0.70899439\n",
      "Iteration 285, loss = 0.15098141\n",
      "Iteration 157, loss = 0.20223464\n",
      "Iteration 71, loss = 0.15397473\n",
      "Iteration 202, loss = 0.08545965\n",
      "Iteration 72, loss = 0.15266387\n",
      "Iteration 203, loss = 0.08525813\n",
      "Iteration 73, loss = 0.15139547\n",
      "Iteration 231, loss = 0.16274235\n",
      "Iteration 204, loss = 0.08508055\n",
      "Iteration 205, loss = 0.08488177\n",
      "Iteration 4, loss = 0.65778054\n",
      "Iteration 74, loss = 0.15015798\n",
      "Iteration 5, loss = 0.61089208\n",
      "Iteration 232, loss = 0.16256503\n",
      "Iteration 70, loss = 0.30204999\n",
      "Iteration 206, loss = 0.08469994\n",
      "Iteration 207, loss = 0.08451298\n",
      "Iteration 286, loss = 0.15087017\n",
      "Iteration 208, loss = 0.08431994\n",
      "Iteration 75, loss = 0.14894141\n",
      "Iteration 209, loss = 0.08413281\n",
      "Iteration 210, loss = 0.08395546\n",
      "Iteration 211, loss = 0.08377601\n",
      "Iteration 233, loss = 0.16238794\n",
      "Iteration 76, loss = 0.14778104\n",
      "Iteration 212, loss = 0.08359870\n",
      "Iteration 6, loss = 0.57103921\n",
      "Iteration 213, loss = 0.08341674\n",
      "Iteration 77, loss = 0.14661989\n",
      "Iteration 7, loss = 0.53641878\n",
      "Iteration 315, loss = 0.21189661\n",
      "Iteration 78, loss = 0.14553401\n",
      "Iteration 8, loss = 0.50796083\n",
      "Iteration 214, loss = 0.08324015\n",
      "Iteration 158, loss = 0.20198003\n",
      "Iteration 79, loss = 0.14444549\n",
      "Iteration 9, loss = 0.48242718\n",
      "Iteration 80, loss = 0.14335568\n",
      "Iteration 287, loss = 0.15077202\n",
      "Iteration 81, loss = 0.14235425\n",
      "Iteration 215, loss = 0.08306920\n",
      "Iteration 82, loss = 0.14131237\n",
      "Iteration 216, loss = 0.08289310\n",
      "Iteration 217, loss = 0.08272611\n",
      "Iteration 10, loss = 0.46068376\n",
      "Iteration 218, loss = 0.08254790\n",
      "Iteration 71, loss = 0.30002954\n",
      "Iteration 219, loss = 0.08237385\n",
      "Iteration 159, loss = 0.20171452\n",
      "Iteration 316, loss = 0.21181475\n",
      "Iteration 11, loss = 0.44132482\n",
      "Iteration 234, loss = 0.16223505\n",
      "Iteration 235, loss = 0.16202149\n",
      "Iteration 220, loss = 0.08222157\n",
      "Iteration 12, loss = 0.42432668\n",
      "Iteration 83, loss = 0.14034118\n",
      "Iteration 221, loss = 0.08204017\n",
      "Iteration 84, loss = 0.13935458\n",
      "Iteration 222, loss = 0.08187271\n",
      "Iteration 13, loss = 0.40843830\n",
      "Iteration 160, loss = 0.20142102\n",
      "Iteration 85, loss = 0.13843228\n",
      "Iteration 223, loss = 0.08171016\n",
      "Iteration 224, loss = 0.08154997\n",
      "Iteration 86, loss = 0.13749968\n",
      "Iteration 225, loss = 0.08138548\n",
      "Iteration 317, loss = 0.21168113\n",
      "Iteration 87, loss = 0.13660038\n",
      "Iteration 88, loss = 0.13571347\n",
      "Iteration 288, loss = 0.15065569\n",
      "Iteration 89, loss = 0.13484904\n",
      "Iteration 14, loss = 0.39441280\n",
      "Iteration 72, loss = 0.29804328\n",
      "Iteration 236, loss = 0.16186291\n",
      "Iteration 226, loss = 0.08123091\n",
      "Iteration 289, loss = 0.15055883\n",
      "Iteration 227, loss = 0.08107145\n",
      "Iteration 15, loss = 0.38114481\n",
      "Iteration 161, loss = 0.20117501\n",
      "Iteration 228, loss = 0.08090629\n",
      "Iteration 229, loss = 0.08074561\n",
      "Iteration 90, loss = 0.13404222\n",
      "Iteration 16, loss = 0.36912535\n",
      "Iteration 91, loss = 0.13320241\n",
      "Iteration 237, loss = 0.16167720\n",
      "Iteration 230, loss = 0.08058913\n",
      "Iteration 231, loss = 0.08044308\n",
      "Iteration 92, loss = 0.13239623\n",
      "Iteration 93, loss = 0.13163526\n",
      "Iteration 94, loss = 0.13085532\n",
      "Iteration 95, loss = 0.13010428\n",
      "Iteration 17, loss = 0.35791361\n",
      "Iteration 96, loss = 0.12936121\n",
      "Iteration 18, loss = 0.34718815\n",
      "Iteration 232, loss = 0.08027841\n",
      "Iteration 290, loss = 0.15043559\n",
      "Iteration 97, loss = 0.12861551\n",
      "Iteration 19, loss = 0.33727579\n",
      "Iteration 98, loss = 0.12792539\n",
      "Iteration 233, loss = 0.08012498\n",
      "Iteration 20, loss = 0.32763053\n",
      "Iteration 99, loss = 0.12722070\n",
      "Iteration 100, loss = 0.12654317\n",
      "Iteration 234, loss = 0.07998070\n",
      "Iteration 73, loss = 0.29620000\n",
      "Iteration 238, loss = 0.16153393\n",
      "Iteration 235, loss = 0.07981983\n",
      "Iteration 318, loss = 0.21160786\n",
      "Iteration 236, loss = 0.07968059\n",
      "Iteration 21, loss = 0.31889517\n",
      "Iteration 237, loss = 0.07952447\n",
      "Iteration 162, loss = 0.20096286\n",
      "Iteration 22, loss = 0.31042635\n",
      "Iteration 101, loss = 0.12587919\n",
      "Iteration 238, loss = 0.07938883\n",
      "Iteration 239, loss = 0.16138914\n",
      "Iteration 23, loss = 0.30243171\n",
      "Iteration 291, loss = 0.15034722\n",
      "Iteration 102, loss = 0.12521269\n",
      "Iteration 239, loss = 0.07922950\n",
      "Iteration 240, loss = 0.07909616\n",
      "Iteration 241, loss = 0.07896066\n",
      "Iteration 103, loss = 0.12456380\n",
      "Iteration 24, loss = 0.29481404\n",
      "Iteration 104, loss = 0.12395530\n",
      "Iteration 25, loss = 0.28759870\n",
      "Iteration 240, loss = 0.16118499\n",
      "Iteration 242, loss = 0.07878945\n",
      "Iteration 243, loss = 0.07865776\n",
      "Iteration 319, loss = 0.21154695\n",
      "Iteration 244, loss = 0.07850803\n",
      "Iteration 105, loss = 0.12331037\n",
      "Iteration 245, loss = 0.07838139\n",
      "Iteration 106, loss = 0.12271337\n",
      "Iteration 74, loss = 0.29438290\n",
      "Iteration 292, loss = 0.15023236\n",
      "Iteration 163, loss = 0.20068634\n",
      "Iteration 107, loss = 0.12211486\n",
      "Iteration 26, loss = 0.28070478\n",
      "Iteration 246, loss = 0.07824073\n",
      "Iteration 241, loss = 0.16101437\n",
      "Iteration 27, loss = 0.27418755\n",
      "Iteration 247, loss = 0.07809420\n",
      "Iteration 28, loss = 0.26790160\n",
      "Iteration 248, loss = 0.07795750\n",
      "Iteration 108, loss = 0.12151949\n",
      "Iteration 249, loss = 0.07781137\n",
      "Iteration 250, loss = 0.07768345\n",
      "Iteration 109, loss = 0.12094181Iteration 251, loss = 0.07753747\n",
      "\n",
      "Iteration 252, loss = 0.07740217\n",
      "Iteration 29, loss = 0.26207513\n",
      "Iteration 110, loss = 0.12038990\n",
      "Iteration 293, loss = 0.15015258\n",
      "Iteration 242, loss = 0.16087317\n",
      "Iteration 320, loss = 0.21151923\n",
      "Iteration 253, loss = 0.07727197\n",
      "Iteration 111, loss = 0.11981563\n",
      "Iteration 254, loss = 0.07713239\n",
      "Iteration 112, loss = 0.11926942\n",
      "Iteration 255, loss = 0.07698990\n",
      "Iteration 75, loss = 0.29252389\n",
      "Iteration 256, loss = 0.07686635\n",
      "Iteration 113, loss = 0.11874222\n",
      "Iteration 257, loss = 0.07672197\n",
      "Iteration 258, loss = 0.07660149\n",
      "Iteration 259, loss = 0.07646473\n",
      "Iteration 30, loss = 0.25630145\n",
      "Iteration 260, loss = 0.07633141\n",
      "Iteration 261, loss = 0.07619644\n",
      "Iteration 262, loss = 0.07607603\n",
      "Iteration 114, loss = 0.11821053\n",
      "Iteration 294, loss = 0.15004645\n",
      "Iteration 31, loss = 0.25101012\n",
      "Iteration 164, loss = 0.20041055\n",
      "Iteration 243, loss = 0.16072598\n",
      "Iteration 32, loss = 0.24576223\n",
      "Iteration 115, loss = 0.11768108\n",
      "Iteration 116, loss = 0.11717720\n",
      "Iteration 263, loss = 0.07595366\n",
      "Iteration 244, loss = 0.16052129\n",
      "Iteration 117, loss = 0.11667413\n",
      "Iteration 118, loss = 0.11616802\n",
      "Iteration 33, loss = 0.24088114\n",
      "Iteration 264, loss = 0.07581302\n",
      "Iteration 295, loss = 0.14994431\n",
      "Iteration 321, loss = 0.21141596\n",
      "Iteration 165, loss = 0.20015684\n",
      "Iteration 34, loss = 0.23613212\n",
      "Iteration 265, loss = 0.07568870\n",
      "Iteration 266, loss = 0.07556030\n",
      "Iteration 76, loss = 0.29086521\n",
      "Iteration 267, loss = 0.07543413\n",
      "Iteration 119, loss = 0.11568279\n",
      "Iteration 35, loss = 0.23160048\n",
      "Iteration 268, loss = 0.07530451\n",
      "Iteration 245, loss = 0.16038162\n",
      "Iteration 269, loss = 0.07517465\n",
      "Iteration 36, loss = 0.22725520\n",
      "Iteration 246, loss = 0.16022576\n",
      "Iteration 120, loss = 0.11518465\n",
      "Iteration 37, loss = 0.22308469\n",
      "Iteration 270, loss = 0.07505989\n",
      "Iteration 38, loss = 0.21913270\n",
      "Iteration 271, loss = 0.07493618\n",
      "Iteration 39, loss = 0.21533682\n",
      "Iteration 296, loss = 0.14983772\n",
      "Iteration 272, loss = 0.07480884\n",
      "Iteration 247, loss = 0.16009843\n",
      "Iteration 121, loss = 0.11473177Iteration 166, loss = 0.19991319\n",
      "\n",
      "Iteration 322, loss = 0.21137218\n",
      "Iteration 273, loss = 0.07469225\n",
      "Iteration 274, loss = 0.07456237\n",
      "Iteration 275, loss = 0.07444386\n",
      "Iteration 77, loss = 0.28930023\n",
      "Iteration 297, loss = 0.14972901\n",
      "Iteration 276, loss = 0.07432789\n",
      "Iteration 122, loss = 0.11424715\n",
      "Iteration 277, loss = 0.07422511\n",
      "Iteration 123, loss = 0.11379671\n",
      "Iteration 40, loss = 0.21161453\n",
      "Iteration 124, loss = 0.11332854\n",
      "Iteration 278, loss = 0.07409209\n",
      "Iteration 279, loss = 0.07397944\n",
      "Iteration 125, loss = 0.11288718\n",
      "Iteration 41, loss = 0.20816497\n",
      "Iteration 248, loss = 0.15991433\n",
      "Iteration 126, loss = 0.11244029\n",
      "Iteration 280, loss = 0.07386144\n",
      "Iteration 127, loss = 0.11201827\n",
      "Iteration 281, loss = 0.07374857\n",
      "Iteration 128, loss = 0.11158671\n",
      "Iteration 42, loss = 0.20474365\n",
      "Iteration 323, loss = 0.21130320\n",
      "Iteration 43, loss = 0.20151619\n",
      "Iteration 167, loss = 0.19965115\n",
      "Iteration 44, loss = 0.19843677\n",
      "Iteration 129, loss = 0.11116650\n",
      "Iteration 45, loss = 0.19541495\n",
      "Iteration 130, loss = 0.11073711\n",
      "Iteration 131, loss = 0.11034296\n",
      "Iteration 282, loss = 0.07362460\n",
      "Iteration 168, loss = 0.19939982\n",
      "Iteration 78, loss = 0.28770995\n",
      "Iteration 283, loss = 0.07352005\n",
      "Iteration 298, loss = 0.14966921\n",
      "Iteration 284, loss = 0.07340355\n",
      "Iteration 249, loss = 0.15973791\n",
      "Iteration 285, loss = 0.07330994\n",
      "Iteration 46, loss = 0.19257327\n",
      "Iteration 286, loss = 0.07318705\n",
      "Iteration 287, loss = 0.07307732\n",
      "Iteration 299, loss = 0.14954240\n",
      "Iteration 288, loss = 0.07297562\n",
      "Iteration 132, loss = 0.10993605\n",
      "Iteration 250, loss = 0.15966304\n",
      "Iteration 133, loss = 0.10953838\n",
      "Iteration 47, loss = 0.18976303\n",
      "Iteration 324, loss = 0.21123970\n",
      "Iteration 134, loss = 0.10914039\n",
      "Iteration 48, loss = 0.18708735\n",
      "Iteration 135, loss = 0.10876414\n",
      "Iteration 289, loss = 0.07285886\n",
      "Iteration 290, loss = 0.07275385\n",
      "Iteration 136, loss = 0.10837386\n",
      "Iteration 291, loss = 0.07263701\n",
      "Iteration 251, loss = 0.15947818\n",
      "Iteration 49, loss = 0.18450927\n",
      "Iteration 292, loss = 0.07252896\n",
      "Iteration 293, loss = 0.07242843\n",
      "Iteration 294, loss = 0.07231424\n",
      "Iteration 300, loss = 0.14945583\n",
      "Iteration 79, loss = 0.28619658\n",
      "Iteration 295, loss = 0.07221796\n",
      "Iteration 50, loss = 0.18209345\n",
      "Iteration 137, loss = 0.10799055\n",
      "Iteration 296, loss = 0.07210409\n",
      "Iteration 138, loss = 0.10761517\n",
      "Iteration 297, loss = 0.07199918\n",
      "Iteration 139, loss = 0.10725206\n",
      "Iteration 298, loss = 0.07189755\n",
      "Iteration 51, loss = 0.17972290\n",
      "Iteration 169, loss = 0.19917946\n",
      "Iteration 301, loss = 0.14935115\n",
      "Iteration 252, loss = 0.15932939\n",
      "Iteration 299, loss = 0.07179616\n",
      "Iteration 325, loss = 0.21118484\n",
      "Iteration 300, loss = 0.07169394\n",
      "Iteration 140, loss = 0.10689563\n",
      "Iteration 301, loss = 0.07159375\n",
      "Iteration 302, loss = 0.07148999\n",
      "Iteration 52, loss = 0.17743896\n",
      "Iteration 141, loss = 0.10654095\n",
      "Iteration 142, loss = 0.10617351\n",
      "Iteration 253, loss = 0.15913884\n",
      "Iteration 303, loss = 0.07138185\n",
      "Iteration 143, loss = 0.10582192\n",
      "Iteration 144, loss = 0.10549181\n",
      "Iteration 145, loss = 0.10513721\n",
      "Iteration 304, loss = 0.07128624\n",
      "Iteration 80, loss = 0.28474558\n",
      "Iteration 146, loss = 0.10481104\n",
      "Iteration 53, loss = 0.17518373\n",
      "Iteration 54, loss = 0.17309894\n",
      "Iteration 305, loss = 0.07118910\n",
      "Iteration 254, loss = 0.15901820\n",
      "Iteration 302, loss = 0.14923920\n",
      "Iteration 170, loss = 0.19890914\n",
      "Iteration 306, loss = 0.07109309\n",
      "Iteration 147, loss = 0.10446810\n",
      "Iteration 307, loss = 0.07099428\n",
      "Iteration 55, loss = 0.17103111\n",
      "Iteration 148, loss = 0.10414606\n",
      "Iteration 149, loss = 0.10382933\n",
      "Iteration 56, loss = 0.16904693\n",
      "Iteration 308, loss = 0.07090699\n",
      "Iteration 326, loss = 0.21109489\n",
      "Iteration 309, loss = 0.07079308\n",
      "Iteration 57, loss = 0.16711134\n",
      "Iteration 150, loss = 0.10351038\n",
      "Iteration 58, loss = 0.16524420\n",
      "Iteration 59, loss = 0.16347148\n",
      "Iteration 60, loss = 0.16170979\n",
      "Iteration 61, loss = 0.16001814\n",
      "Iteration 171, loss = 0.19869243\n",
      "Iteration 303, loss = 0.14916466\n",
      "Iteration 151, loss = 0.10318522\n",
      "Iteration 310, loss = 0.07069800\n",
      "Iteration 255, loss = 0.15885560\n",
      "Iteration 152, loss = 0.10287857\n",
      "Iteration 311, loss = 0.07060472\n",
      "Iteration 312, loss = 0.07050374\n",
      "Iteration 313, loss = 0.07041673\n",
      "Iteration 256, loss = 0.15873592\n",
      "Iteration 314, loss = 0.07031828\n",
      "Iteration 315, loss = 0.07022121\n",
      "Iteration 62, loss = 0.15841850\n",
      "Iteration 153, loss = 0.10256255\n",
      "Iteration 172, loss = 0.19845345\n",
      "Iteration 316, loss = 0.07013252\n",
      "Iteration 154, loss = 0.10226691\n",
      "Iteration 81, loss = 0.28340964\n",
      "Iteration 155, loss = 0.10196033\n",
      "Iteration 327, loss = 0.21104915\n",
      "Iteration 304, loss = 0.14906827\n",
      "Iteration 156, loss = 0.10166098\n",
      "Iteration 157, loss = 0.10136837\n",
      "Iteration 317, loss = 0.07003486\n",
      "Iteration 318, loss = 0.06994222\n",
      "Iteration 63, loss = 0.15681360\n",
      "Iteration 319, loss = 0.06985346\n",
      "Iteration 305, loss = 0.14896239\n",
      "Iteration 158, loss = 0.10107881\n",
      "Iteration 64, loss = 0.15531108\n",
      "Iteration 65, loss = 0.15380346\n",
      "Iteration 257, loss = 0.15857109\n",
      "Iteration 66, loss = 0.15239035\n",
      "Iteration 320, loss = 0.06975940\n",
      "Iteration 67, loss = 0.15099177\n",
      "Iteration 159, loss = 0.10080076\n",
      "Iteration 258, loss = 0.15847017\n",
      "Iteration 321, loss = 0.06966486\n",
      "Iteration 322, loss = 0.06956946\n",
      "Iteration 323, loss = 0.06949242\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 160, loss = 0.10050330\n",
      "Iteration 1, loss = 0.68735803\n",
      "Iteration 82, loss = 0.28204598\n",
      "Iteration 68, loss = 0.14965016\n",
      "Iteration 2, loss = 0.66608189\n",
      "Iteration 3, loss = 0.63674727\n",
      "Iteration 4, loss = 0.60735789\n",
      "Iteration 161, loss = 0.10022128\n",
      "Iteration 162, loss = 0.09994366\n",
      "Iteration 328, loss = 0.21099734\n",
      "Iteration 163, loss = 0.09967409\n",
      "Iteration 69, loss = 0.14838429\n",
      "Iteration 173, loss = 0.19820972\n",
      "Iteration 306, loss = 0.14888093\n",
      "Iteration 70, loss = 0.14707198\n",
      "Iteration 259, loss = 0.15829097\n",
      "Iteration 5, loss = 0.57845626\n",
      "Iteration 164, loss = 0.09939521\n",
      "Iteration 71, loss = 0.14584169\n",
      "Iteration 307, loss = 0.14878333\n",
      "Iteration 6, loss = 0.55341793\n",
      "Iteration 7, loss = 0.53098328\n",
      "Iteration 165, loss = 0.09912589\n",
      "Iteration 72, loss = 0.14467308\n",
      "Iteration 166, loss = 0.09886321\n",
      "Iteration 167, loss = 0.09859461\n",
      "Iteration 260, loss = 0.15815446\n",
      "Iteration 8, loss = 0.51115938\n",
      "Iteration 73, loss = 0.14349135\n",
      "Iteration 168, loss = 0.09833833\n",
      "Iteration 308, loss = 0.14868043\n",
      "Iteration 329, loss = 0.21091252\n",
      "Iteration 83, loss = 0.28074315\n",
      "Iteration 169, loss = 0.09807758\n",
      "Iteration 74, loss = 0.14237927\n",
      "Iteration 9, loss = 0.49358691\n",
      "Iteration 170, loss = 0.09782230\n",
      "Iteration 174, loss = 0.19801097\n",
      "Iteration 171, loss = 0.09757558\n",
      "Iteration 10, loss = 0.47818685\n",
      "Iteration 11, loss = 0.46425382\n",
      "Iteration 172, loss = 0.09731695\n",
      "Iteration 75, loss = 0.14127744\n",
      "Iteration 76, loss = 0.14019808\n",
      "Iteration 12, loss = 0.45178840\n",
      "Iteration 77, loss = 0.13918346\n",
      "Iteration 261, loss = 0.15800119\n",
      "Iteration 13, loss = 0.44073421\n",
      "Iteration 78, loss = 0.13816743\n",
      "Iteration 14, loss = 0.43046972\n",
      "Iteration 15, loss = 0.42123182\n",
      "Iteration 79, loss = 0.13718253\n",
      "Iteration 16, loss = 0.41282497\n",
      "Iteration 309, loss = 0.14859103\n",
      "Iteration 17, loss = 0.40491271\n",
      "Iteration 80, loss = 0.13621680\n",
      "Iteration 18, loss = 0.39745841\n",
      "Iteration 19, loss = 0.39059836\n",
      "Iteration 173, loss = 0.09706803\n",
      "Iteration 81, loss = 0.13527992\n",
      "Iteration 82, loss = 0.13440106\n",
      "Iteration 174, loss = 0.09683883\n",
      "Iteration 175, loss = 0.09658541\n",
      "Iteration 175, loss = 0.19776946\n",
      "Iteration 176, loss = 0.09633691\n",
      "Iteration 262, loss = 0.15789467\n",
      "Iteration 84, loss = 0.27948081\n",
      "Iteration 330, loss = 0.21086180\n",
      "Iteration 20, loss = 0.38413248\n",
      "Iteration 310, loss = 0.14852875\n",
      "Iteration 263, loss = 0.15775417\n",
      "Iteration 21, loss = 0.37797288\n",
      "Iteration 264, loss = 0.15759468\n",
      "Iteration 22, loss = 0.37211117\n",
      "Iteration 23, loss = 0.36661781\n",
      "Iteration 177, loss = 0.09610260\n",
      "Iteration 176, loss = 0.19749244\n",
      "Iteration 178, loss = 0.09586163\n",
      "Iteration 265, loss = 0.15747100\n",
      "Iteration 179, loss = 0.09563254\n",
      "Iteration 24, loss = 0.36132298\n",
      "Iteration 25, loss = 0.35632014\n",
      "Iteration 180, loss = 0.09539231\n",
      "Iteration 83, loss = 0.13351621\n",
      "Iteration 331, loss = 0.21084286\n",
      "Iteration 26, loss = 0.35123208\n",
      "Iteration 181, loss = 0.09517261\n",
      "Iteration 266, loss = 0.15735751\n",
      "Iteration 84, loss = 0.13263434\n",
      "Iteration 85, loss = 0.27829514\n",
      "Iteration 27, loss = 0.34647209\n",
      "Iteration 182, loss = 0.09494024\n",
      "Iteration 85, loss = 0.13183008\n",
      "Iteration 28, loss = 0.34185795\n",
      "Iteration 183, loss = 0.09470850\n",
      "Iteration 29, loss = 0.33737093\n",
      "Iteration 30, loss = 0.33300929\n",
      "Iteration 311, loss = 0.14840966\n",
      "Iteration 31, loss = 0.32864110\n",
      "Iteration 32, loss = 0.32452233\n",
      "Iteration 33, loss = 0.32042533\n",
      "Iteration 34, loss = 0.31617474\n",
      "Iteration 35, loss = 0.31207281\n",
      "Iteration 36, loss = 0.30820108\n",
      "Iteration 37, loss = 0.30418315\n",
      "Iteration 38, loss = 0.30042032\n",
      "Iteration 86, loss = 0.13097462\n",
      "Iteration 39, loss = 0.29644306\n",
      "Iteration 177, loss = 0.19729019\n",
      "Iteration 184, loss = 0.09449490\n",
      "Iteration 312, loss = 0.14831852\n",
      "Iteration 185, loss = 0.09427481\n",
      "Iteration 186, loss = 0.09405071\n",
      "Iteration 87, loss = 0.13019512\n",
      "Iteration 88, loss = 0.12938524\n",
      "Iteration 267, loss = 0.15722135\n",
      "Iteration 89, loss = 0.12863088\n",
      "Iteration 90, loss = 0.12788433\n",
      "Iteration 187, loss = 0.09383148\n",
      "Iteration 313, loss = 0.14822342\n",
      "Iteration 91, loss = 0.12714342\n",
      "Iteration 86, loss = 0.27716253\n",
      "Iteration 188, loss = 0.09361937\n",
      "Iteration 40, loss = 0.29271920\n",
      "Iteration 41, loss = 0.28895017\n",
      "Iteration 189, loss = 0.09340710\n",
      "Iteration 42, loss = 0.28506654\n",
      "Iteration 190, loss = 0.09320233\n",
      "Iteration 43, loss = 0.28148735\n",
      "Iteration 191, loss = 0.09299390\n",
      "Iteration 44, loss = 0.27777828\n",
      "Iteration 332, loss = 0.21079885\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 45, loss = 0.27417572\n",
      "Iteration 178, loss = 0.19705542\n",
      "Iteration 92, loss = 0.12643014\n",
      "Iteration 46, loss = 0.27050665\n",
      "Iteration 93, loss = 0.12573212\n",
      "Iteration 314, loss = 0.14814671\n",
      "Iteration 47, loss = 0.26692478\n",
      "Iteration 268, loss = 0.15709813\n",
      "Iteration 48, loss = 0.26350857\n",
      "Iteration 192, loss = 0.09278603\n",
      "Iteration 94, loss = 0.12502903\n",
      "Iteration 193, loss = 0.09259009\n",
      "Iteration 95, loss = 0.12435895\n",
      "Iteration 194, loss = 0.09237952\n",
      "Iteration 315, loss = 0.14805235\n",
      "Iteration 96, loss = 0.12369655\n",
      "Iteration 195, loss = 0.09217725\n",
      "Iteration 196, loss = 0.09198807\n",
      "Iteration 1, loss = 0.87426399\n",
      "Iteration 49, loss = 0.25993746\n",
      "Iteration 197, loss = 0.09179168\n",
      "Iteration 50, loss = 0.25658243\n",
      "Iteration 198, loss = 0.09158328\n",
      "Iteration 51, loss = 0.25327530\n",
      "Iteration 87, loss = 0.27594794\n",
      "Iteration 2, loss = 0.80726001\n",
      "Iteration 269, loss = 0.15695981\n",
      "Iteration 179, loss = 0.19685235\n",
      "Iteration 97, loss = 0.12305374\n",
      "Iteration 270, loss = 0.15684604\n",
      "Iteration 52, loss = 0.24989537\n",
      "Iteration 98, loss = 0.12242664\n",
      "Iteration 199, loss = 0.09139339\n",
      "Iteration 53, loss = 0.24658237\n",
      "Iteration 99, loss = 0.12181121\n",
      "Iteration 54, loss = 0.24342117\n",
      "Iteration 316, loss = 0.14794713\n",
      "Iteration 55, loss = 0.24013962\n",
      "Iteration 200, loss = 0.09120439\n",
      "Iteration 56, loss = 0.23711484\n",
      "Iteration 3, loss = 0.72546579\n",
      "Iteration 100, loss = 0.12117703\n",
      "Iteration 201, loss = 0.09101097\n",
      "Iteration 202, loss = 0.09082386\n",
      "Iteration 101, loss = 0.12061528\n",
      "Iteration 4, loss = 0.64972205\n",
      "Iteration 102, loss = 0.12000911\n",
      "Iteration 5, loss = 0.58826563\n",
      "Iteration 6, loss = 0.54158846\n",
      "Iteration 7, loss = 0.50436369\n",
      "Iteration 271, loss = 0.15669001\n",
      "Iteration 57, loss = 0.23390673\n",
      "Iteration 58, loss = 0.23091826\n",
      "Iteration 203, loss = 0.09062994\n",
      "Iteration 59, loss = 0.22782295\n",
      "Iteration 317, loss = 0.14786871\n",
      "Iteration 180, loss = 0.19663161\n",
      "Iteration 204, loss = 0.09044950\n",
      "Iteration 88, loss = 0.27485890\n",
      "Iteration 60, loss = 0.22490444\n",
      "Iteration 61, loss = 0.22194289\n",
      "Iteration 103, loss = 0.11944174\n",
      "Iteration 8, loss = 0.47654233\n",
      "Iteration 62, loss = 0.21891912\n",
      "Iteration 205, loss = 0.09026596\n",
      "Iteration 206, loss = 0.09008858\n",
      "Iteration 104, loss = 0.11888259\n",
      "Iteration 63, loss = 0.21609052\n",
      "Iteration 272, loss = 0.15657578\n",
      "Iteration 64, loss = 0.21327562\n",
      "Iteration 207, loss = 0.08989479\n",
      "Iteration 105, loss = 0.11831288\n",
      "Iteration 65, loss = 0.21044775\n",
      "Iteration 318, loss = 0.14778044\n",
      "Iteration 9, loss = 0.45482482\n",
      "Iteration 66, loss = 0.20779415\n",
      "Iteration 181, loss = 0.19639672\n",
      "Iteration 106, loss = 0.11777034\n",
      "Iteration 273, loss = 0.15645499\n",
      "Iteration 67, loss = 0.20516540\n",
      "Iteration 208, loss = 0.08972589\n",
      "Iteration 68, loss = 0.20248746\n",
      "Iteration 10, loss = 0.43695591\n",
      "Iteration 209, loss = 0.08953502\n",
      "Iteration 182, loss = 0.19620408\n",
      "Iteration 210, loss = 0.08936231\n",
      "Iteration 69, loss = 0.19985332\n",
      "Iteration 70, loss = 0.19740919\n",
      "Iteration 11, loss = 0.42181169\n",
      "Iteration 107, loss = 0.11725227\n",
      "Iteration 211, loss = 0.08918855\n",
      "Iteration 89, loss = 0.27381734\n",
      "Iteration 212, loss = 0.08901547\n",
      "Iteration 213, loss = 0.08883816\n",
      "Iteration 319, loss = 0.14770484\n",
      "Iteration 214, loss = 0.08866651\n",
      "Iteration 108, loss = 0.11669807\n",
      "Iteration 71, loss = 0.19496618\n",
      "Iteration 109, loss = 0.11618660\n",
      "Iteration 72, loss = 0.19265995\n",
      "Iteration 274, loss = 0.15633320\n",
      "Iteration 12, loss = 0.40894435\n",
      "Iteration 110, loss = 0.11567552\n",
      "Iteration 320, loss = 0.14760030\n",
      "Iteration 215, loss = 0.08850494\n",
      "Iteration 216, loss = 0.08833090\n",
      "Iteration 275, loss = 0.15620871\n",
      "Iteration 73, loss = 0.19026557\n",
      "Iteration 111, loss = 0.11517065\n",
      "Iteration 74, loss = 0.18801966\n",
      "Iteration 112, loss = 0.11470804\n",
      "Iteration 75, loss = 0.18581045\n",
      "Iteration 321, loss = 0.14753108\n",
      "Iteration 13, loss = 0.39791455\n",
      "Iteration 90, loss = 0.27275263\n",
      "Iteration 113, loss = 0.11420706\n",
      "Iteration 114, loss = 0.11373952\n",
      "Iteration 217, loss = 0.08816311\n",
      "Iteration 115, loss = 0.11327337\n",
      "Iteration 76, loss = 0.18366849Iteration 218, loss = 0.08799653\n",
      "Iteration 116, loss = 0.11281995\n",
      "\n",
      "Iteration 14, loss = 0.38739164\n",
      "Iteration 219, loss = 0.08783416\n",
      "Iteration 117, loss = 0.11237426\n",
      "Iteration 77, loss = 0.18156658\n",
      "Iteration 183, loss = 0.19597868\n",
      "Iteration 220, loss = 0.08766621\n",
      "Iteration 78, loss = 0.17949243\n",
      "Iteration 79, loss = 0.17747775\n",
      "Iteration 221, loss = 0.08750808\n",
      "Iteration 80, loss = 0.17554844\n",
      "Iteration 81, loss = 0.17362155\n",
      "Iteration 222, loss = 0.08735189\n",
      "Iteration 82, loss = 0.17177297\n",
      "Iteration 15, loss = 0.37782665\n",
      "Iteration 276, loss = 0.15607825\n",
      "Iteration 83, loss = 0.16995518\n",
      "Iteration 223, loss = 0.08718676\n",
      "Iteration 84, loss = 0.16809655\n",
      "Iteration 85, loss = 0.16635793\n",
      "Iteration 86, loss = 0.16464599\n",
      "Iteration 184, loss = 0.19578127\n",
      "Iteration 322, loss = 0.14743523\n",
      "Iteration 87, loss = 0.16299042\n",
      "Iteration 16, loss = 0.36884484\n",
      "Iteration 88, loss = 0.16133187\n",
      "Iteration 89, loss = 0.15973664\n",
      "Iteration 277, loss = 0.15596263\n",
      "Iteration 90, loss = 0.15820902\n",
      "Iteration 91, loss = 0.15667310\n",
      "Iteration 224, loss = 0.08701983\n",
      "Iteration 92, loss = 0.15515166\n",
      "Iteration 17, loss = 0.36039600\n",
      "Iteration 118, loss = 0.11193376\n",
      "Iteration 93, loss = 0.15373667\n",
      "Iteration 94, loss = 0.15234436\n",
      "Iteration 119, loss = 0.11149514\n",
      "Iteration 225, loss = 0.08687295\n",
      "Iteration 95, loss = 0.15091239\n",
      "Iteration 18, loss = 0.35209388\n",
      "Iteration 96, loss = 0.14954539\n",
      "Iteration 226, loss = 0.08671808\n",
      "Iteration 19, loss = 0.34428652\n",
      "Iteration 97, loss = 0.14824524\n",
      "Iteration 120, loss = 0.11105822\n",
      "Iteration 91, loss = 0.27171718\n",
      "Iteration 98, loss = 0.14698185\n",
      "Iteration 20, loss = 0.33648507\n",
      "Iteration 227, loss = 0.08655689\n",
      "Iteration 323, loss = 0.14734656\n",
      "Iteration 99, loss = 0.14563475\n",
      "Iteration 100, loss = 0.14439461\n",
      "Iteration 101, loss = 0.14318843\n",
      "Iteration 21, loss = 0.32921785\n",
      "Iteration 102, loss = 0.14196453\n",
      "Iteration 278, loss = 0.15584303\n",
      "Iteration 185, loss = 0.19557778\n",
      "Iteration 228, loss = 0.08640238\n",
      "Iteration 121, loss = 0.11064532\n",
      "Iteration 229, loss = 0.08624709\n",
      "Iteration 22, loss = 0.32211385\n",
      "Iteration 279, loss = 0.15573297\n",
      "Iteration 230, loss = 0.08610490\n",
      "Iteration 122, loss = 0.11025642\n",
      "Iteration 324, loss = 0.14726036\n",
      "Iteration 23, loss = 0.31509654\n",
      "Iteration 123, loss = 0.10983419\n",
      "Iteration 124, loss = 0.10944236\n",
      "Iteration 24, loss = 0.30856638\n",
      "Iteration 231, loss = 0.08594523\n",
      "Iteration 125, loss = 0.10904031\n",
      "Iteration 232, loss = 0.08578534\n",
      "Iteration 25, loss = 0.30209968\n",
      "Iteration 126, loss = 0.10866735\n",
      "Iteration 127, loss = 0.10829863\n",
      "Iteration 103, loss = 0.14083120\n",
      "Iteration 233, loss = 0.08564341\n",
      "Iteration 104, loss = 0.13962548\n",
      "Iteration 92, loss = 0.27082029\n",
      "Iteration 105, loss = 0.13854794\n",
      "Iteration 234, loss = 0.08549220\n",
      "Iteration 106, loss = 0.13744114\n",
      "Iteration 235, loss = 0.08534184\n",
      "Iteration 186, loss = 0.19536919\n",
      "Iteration 107, loss = 0.13642064\n",
      "Iteration 236, loss = 0.08520288\n",
      "Iteration 108, loss = 0.13536678\n",
      "Iteration 109, loss = 0.13440576\n",
      "Iteration 237, loss = 0.08505312\n",
      "Iteration 26, loss = 0.29574351\n",
      "Iteration 110, loss = 0.13336052\n",
      "Iteration 238, loss = 0.08491239\n",
      "Iteration 111, loss = 0.13240193\n",
      "Iteration 239, loss = 0.08477894\n",
      "Iteration 112, loss = 0.13147762\n",
      "Iteration 280, loss = 0.15563714\n",
      "Iteration 325, loss = 0.14719361\n",
      "Iteration 281, loss = 0.15547922\n",
      "Iteration 326, loss = 0.14710242\n",
      "Iteration 27, loss = 0.28973847\n",
      "Iteration 240, loss = 0.08463854\n",
      "Iteration 128, loss = 0.10789855\n",
      "Iteration 129, loss = 0.10755372\n",
      "Iteration 113, loss = 0.13055096\n",
      "Iteration 93, loss = 0.26980563\n",
      "Iteration 130, loss = 0.10717068\n",
      "Iteration 131, loss = 0.10683536\n",
      "Iteration 114, loss = 0.12966927\n",
      "Iteration 28, loss = 0.28394602\n",
      "Iteration 115, loss = 0.12876522\n",
      "Iteration 116, loss = 0.12792021\n",
      "Iteration 117, loss = 0.12707312\n",
      "Iteration 118, loss = 0.12626345\n",
      "Iteration 282, loss = 0.15540319\n",
      "Iteration 119, loss = 0.12546776\n",
      "Iteration 120, loss = 0.12468647\n",
      "Iteration 187, loss = 0.19518432\n",
      "Iteration 132, loss = 0.10646709\n",
      "Iteration 29, loss = 0.27829403\n",
      "Iteration 241, loss = 0.08447972\n",
      "Iteration 242, loss = 0.08435051\n",
      "Iteration 30, loss = 0.27268639\n",
      "Iteration 243, loss = 0.08420432\n",
      "Iteration 133, loss = 0.10612227\n",
      "Iteration 244, loss = 0.08407646\n",
      "Iteration 134, loss = 0.10577739\n",
      "Iteration 31, loss = 0.26746373\n",
      "Iteration 245, loss = 0.08392356\n",
      "Iteration 188, loss = 0.19496230\n",
      "Iteration 283, loss = 0.15525794\n",
      "Iteration 121, loss = 0.12391029\n",
      "Iteration 122, loss = 0.12315157\n",
      "Iteration 135, loss = 0.10545556\n",
      "Iteration 327, loss = 0.14703491\n",
      "Iteration 123, loss = 0.12243187\n",
      "Iteration 136, loss = 0.10512131\n",
      "Iteration 124, loss = 0.12169972\n",
      "Iteration 125, loss = 0.12096295\n",
      "Iteration 137, loss = 0.10480629\n",
      "Iteration 126, loss = 0.12026748\n",
      "Iteration 127, loss = 0.11958056\n",
      "Iteration 328, loss = 0.14692737\n",
      "Iteration 138, loss = 0.10448360\n",
      "Iteration 139, loss = 0.10416760\n",
      "Iteration 246, loss = 0.08382094\n",
      "Iteration 32, loss = 0.26230115\n",
      "Iteration 247, loss = 0.08365468\n",
      "Iteration 248, loss = 0.08352203\n",
      "Iteration 249, loss = 0.08337725\n",
      "Iteration 250, loss = 0.08324706\n",
      "Iteration 128, loss = 0.11891422\n",
      "Iteration 33, loss = 0.25731377\n",
      "Iteration 129, loss = 0.11825325\n",
      "Iteration 284, loss = 0.15512652\n",
      "Iteration 34, loss = 0.25256746\n",
      "Iteration 94, loss = 0.26889534\n",
      "Iteration 130, loss = 0.11760871\n",
      "Iteration 131, loss = 0.11697485\n",
      "Iteration 140, loss = 0.10386776\n",
      "Iteration 132, loss = 0.11633548\n",
      "Iteration 35, loss = 0.24787866\n",
      "Iteration 251, loss = 0.08310845\n",
      "Iteration 133, loss = 0.11572414\n",
      "Iteration 252, loss = 0.08298574\n",
      "Iteration 285, loss = 0.15507256\n",
      "Iteration 329, loss = 0.14687132\n",
      "Iteration 253, loss = 0.08285839\n",
      "Iteration 254, loss = 0.08272634\n",
      "Iteration 141, loss = 0.10355524\n",
      "Iteration 255, loss = 0.08258947\n",
      "Iteration 189, loss = 0.19475941\n",
      "Iteration 142, loss = 0.10324888\n",
      "Iteration 256, loss = 0.08247759\n",
      "Iteration 134, loss = 0.11510180\n",
      "Iteration 330, loss = 0.14678260\n",
      "Iteration 135, loss = 0.11451304\n",
      "Iteration 136, loss = 0.11390592\n",
      "Iteration 137, loss = 0.11333614\n",
      "Iteration 257, loss = 0.08232592\n",
      "Iteration 143, loss = 0.10296063\n",
      "Iteration 258, loss = 0.08220238\n",
      "Iteration 138, loss = 0.11277373\n",
      "Iteration 36, loss = 0.24350283\n",
      "Iteration 259, loss = 0.08208199\n",
      "Iteration 139, loss = 0.11220784\n",
      "Iteration 331, loss = 0.14669709\n",
      "Iteration 286, loss = 0.15489912\n",
      "Iteration 190, loss = 0.19460554\n",
      "Iteration 144, loss = 0.10265839\n",
      "Iteration 260, loss = 0.08195190\n",
      "Iteration 140, loss = 0.11168512\n",
      "Iteration 95, loss = 0.26804196\n",
      "Iteration 37, loss = 0.23920548\n",
      "Iteration 141, loss = 0.11112273\n",
      "Iteration 38, loss = 0.23508027\n",
      "Iteration 142, loss = 0.11060213\n",
      "Iteration 145, loss = 0.10238817\n",
      "Iteration 143, loss = 0.11005424\n",
      "Iteration 144, loss = 0.10954818\n",
      "Iteration 39, loss = 0.23096819\n",
      "Iteration 145, loss = 0.10905164\n",
      "Iteration 287, loss = 0.15480965\n",
      "Iteration 146, loss = 0.10208262\n",
      "Iteration 146, loss = 0.10856979\n",
      "Iteration 40, loss = 0.22726250\n",
      "Iteration 261, loss = 0.08182808\n",
      "Iteration 262, loss = 0.08171328\n",
      "Iteration 147, loss = 0.10179405\n",
      "Iteration 263, loss = 0.08158615\n",
      "Iteration 148, loss = 0.10152738\n",
      "Iteration 264, loss = 0.08146465\n",
      "Iteration 332, loss = 0.14662713\n",
      "Iteration 265, loss = 0.08135481\n",
      "Iteration 147, loss = 0.10805840\n",
      "Iteration 149, loss = 0.10125139\n",
      "Iteration 191, loss = 0.19439080\n",
      "Iteration 41, loss = 0.22339606\n",
      "Iteration 148, loss = 0.10756157\n",
      "Iteration 266, loss = 0.08122373\n",
      "Iteration 149, loss = 0.10708370\n",
      "Iteration 288, loss = 0.15467971\n",
      "Iteration 150, loss = 0.10660978\n",
      "Iteration 96, loss = 0.26712026\n",
      "Iteration 151, loss = 0.10613690\n",
      "Iteration 152, loss = 0.10568795\n",
      "Iteration 150, loss = 0.10097549\n",
      "Iteration 42, loss = 0.21983663\n",
      "Iteration 267, loss = 0.08110738\n",
      "Iteration 151, loss = 0.10071277\n",
      "Iteration 152, loss = 0.10043902\n",
      "Iteration 333, loss = 0.14654605\n",
      "Iteration 268, loss = 0.08100315\n",
      "Iteration 269, loss = 0.08087547\n",
      "Iteration 153, loss = 0.10522252\n",
      "Iteration 289, loss = 0.15462136Iteration 43, loss = 0.21633370\n",
      "Iteration 270, loss = 0.08075523\n",
      "Iteration 154, loss = 0.10477000\n",
      "Iteration 155, loss = 0.10434534\n",
      "Iteration 192, loss = 0.19417076\n",
      "\n",
      "Iteration 271, loss = 0.08064665\n",
      "Iteration 156, loss = 0.10391703\n",
      "Iteration 153, loss = 0.10019677\n",
      "Iteration 272, loss = 0.08053419\n",
      "Iteration 154, loss = 0.09991547\n",
      "Iteration 273, loss = 0.08041876\n",
      "Iteration 44, loss = 0.21310495\n",
      "Iteration 157, loss = 0.10346808\n",
      "Iteration 274, loss = 0.08030546\n",
      "Iteration 45, loss = 0.20986915\n",
      "Iteration 158, loss = 0.10304459\n",
      "Iteration 334, loss = 0.14644797\n",
      "Iteration 159, loss = 0.10264111\n",
      "Iteration 155, loss = 0.09967100\n",
      "Iteration 290, loss = 0.15446203\n",
      "Iteration 46, loss = 0.20688068\n",
      "Iteration 160, loss = 0.10221048\n",
      "Iteration 97, loss = 0.26625194\n",
      "Iteration 161, loss = 0.10181132\n",
      "Iteration 47, loss = 0.20371964\n",
      "Iteration 162, loss = 0.10141051\n",
      "Iteration 163, loss = 0.10101811\n",
      "Iteration 156, loss = 0.09941264\n",
      "Iteration 275, loss = 0.08019422\n",
      "Iteration 276, loss = 0.08007868\n",
      "Iteration 277, loss = 0.07997785\n",
      "Iteration 193, loss = 0.19401669\n",
      "Iteration 157, loss = 0.09916088\n",
      "Iteration 278, loss = 0.07986092\n",
      "Iteration 158, loss = 0.09892711\n",
      "Iteration 335, loss = 0.14637846\n",
      "Iteration 48, loss = 0.20098703\n",
      "Iteration 98, loss = 0.26553011\n",
      "Iteration 164, loss = 0.10062547\n",
      "Iteration 49, loss = 0.19814117\n",
      "Iteration 159, loss = 0.09867205\n",
      "Iteration 279, loss = 0.07975123\n",
      "Iteration 165, loss = 0.10023484\n",
      "Iteration 291, loss = 0.15437689\n",
      "Iteration 166, loss = 0.09985194\n",
      "Iteration 280, loss = 0.07963585\n",
      "Iteration 50, loss = 0.19552193\n",
      "Iteration 167, loss = 0.09945699\n",
      "Iteration 160, loss = 0.09844966\n",
      "Iteration 161, loss = 0.09819763\n",
      "Iteration 281, loss = 0.07952695\n",
      "Iteration 168, loss = 0.09909592\n",
      "Iteration 336, loss = 0.14630300\n",
      "Iteration 51, loss = 0.19294408\n",
      "Iteration 169, loss = 0.09873267\n",
      "Iteration 162, loss = 0.09796522\n",
      "Iteration 194, loss = 0.19380432\n",
      "Iteration 282, loss = 0.07941762\n",
      "Iteration 170, loss = 0.09833831\n",
      "Iteration 52, loss = 0.19050340\n",
      "Iteration 283, loss = 0.07931307\n",
      "Iteration 163, loss = 0.09772964\n",
      "Iteration 171, loss = 0.09798451\n",
      "Iteration 284, loss = 0.07920825\n",
      "Iteration 164, loss = 0.09749813\n",
      "Iteration 292, loss = 0.15425067\n",
      "Iteration 165, loss = 0.09727262\n",
      "Iteration 53, loss = 0.18808336\n",
      "Iteration 172, loss = 0.09762707\n",
      "Iteration 173, loss = 0.09724893\n",
      "Iteration 166, loss = 0.09704910\n",
      "Iteration 174, loss = 0.09687950\n",
      "Iteration 99, loss = 0.26464739\n",
      "Iteration 54, loss = 0.18578213\n",
      "Iteration 337, loss = 0.14621330\n",
      "Iteration 175, loss = 0.09655469\n",
      "Iteration 285, loss = 0.07910582\n",
      "Iteration 286, loss = 0.07899446\n",
      "Iteration 176, loss = 0.09619524\n",
      "Iteration 167, loss = 0.09683689\n",
      "Iteration 177, loss = 0.09585161\n",
      "Iteration 195, loss = 0.19364815\n",
      "Iteration 168, loss = 0.09660976\n",
      "Iteration 178, loss = 0.09550239\n",
      "Iteration 179, loss = 0.09517146\n",
      "Iteration 169, loss = 0.09640983\n",
      "Iteration 180, loss = 0.09483650\n",
      "Iteration 181, loss = 0.09449792\n",
      "Iteration 170, loss = 0.09618234\n",
      "Iteration 182, loss = 0.09417158\n",
      "Iteration 287, loss = 0.07889084\n",
      "Iteration 293, loss = 0.15414153\n",
      "Iteration 288, loss = 0.07878383\n",
      "Iteration 55, loss = 0.18363818\n",
      "Iteration 289, loss = 0.07869059\n",
      "Iteration 290, loss = 0.07858087\n",
      "Iteration 338, loss = 0.14614708\n",
      "Iteration 183, loss = 0.09387448\n",
      "Iteration 184, loss = 0.09352763\n",
      "Iteration 185, loss = 0.09320517\n",
      "Iteration 56, loss = 0.18144389\n",
      "Iteration 186, loss = 0.09291934\n",
      "Iteration 187, loss = 0.09257732\n",
      "Iteration 291, loss = 0.07848205\n",
      "Iteration 188, loss = 0.09230607\n",
      "Iteration 339, loss = 0.14605987\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 189, loss = 0.09197807\n",
      "Iteration 292, loss = 0.07838873\n",
      "Iteration 293, loss = 0.07828125\n",
      "Iteration 294, loss = 0.07816287\n",
      "Iteration 196, loss = 0.19344374\n",
      "Iteration 294, loss = 0.15406836\n",
      "Iteration 171, loss = 0.09596712\n",
      "Iteration 295, loss = 0.07807227\n",
      "Iteration 57, loss = 0.17941585\n",
      "Iteration 296, loss = 0.07797683\n",
      "Iteration 58, loss = 0.17743620\n",
      "Iteration 100, loss = 0.26385947\n",
      "Iteration 190, loss = 0.09168617\n",
      "Iteration 297, loss = 0.07786565\n",
      "Iteration 59, loss = 0.17559078\n",
      "Iteration 191, loss = 0.09139180\n",
      "Iteration 298, loss = 0.07777322\n",
      "Iteration 172, loss = 0.09576064\n",
      "Iteration 299, loss = 0.07767672\n",
      "Iteration 192, loss = 0.09108511\n",
      "Iteration 295, loss = 0.15395755\n",
      "Iteration 60, loss = 0.17375024\n",
      "Iteration 193, loss = 0.09079128\n",
      "Iteration 61, loss = 0.17200551\n",
      "Iteration 194, loss = 0.09051861\n",
      "Iteration 173, loss = 0.09555252\n",
      "Iteration 195, loss = 0.09023716\n",
      "Iteration 296, loss = 0.15381924\n",
      "Iteration 62, loss = 0.17032051\n",
      "Iteration 174, loss = 0.09534633\n",
      "Iteration 300, loss = 0.07757374\n",
      "Iteration 1, loss = 0.86079395\n",
      "Iteration 63, loss = 0.16866806\n",
      "Iteration 175, loss = 0.09514985\n",
      "Iteration 64, loss = 0.16711876\n",
      "Iteration 196, loss = 0.08996814\n",
      "Iteration 301, loss = 0.07747938\n",
      "Iteration 197, loss = 0.08969946\n",
      "Iteration 198, loss = 0.08940765\n",
      "Iteration 65, loss = 0.16558499\n",
      "Iteration 176, loss = 0.09493849\n",
      "Iteration 101, loss = 0.26311309\n",
      "Iteration 66, loss = 0.16415442\n",
      "Iteration 199, loss = 0.08915541\n",
      "Iteration 67, loss = 0.16274879\n",
      "Iteration 302, loss = 0.07737879\n",
      "Iteration 297, loss = 0.15371763\n",
      "Iteration 197, loss = 0.19326394\n",
      "Iteration 2, loss = 0.81199201\n",
      "Iteration 177, loss = 0.09473291\n",
      "Iteration 178, loss = 0.09455389\n",
      "Iteration 200, loss = 0.08887990\n",
      "Iteration 201, loss = 0.08860717\n",
      "Iteration 303, loss = 0.07728219\n",
      "Iteration 304, loss = 0.07719627\n",
      "Iteration 202, loss = 0.08834607\n",
      "Iteration 203, loss = 0.08809654\n",
      "Iteration 305, loss = 0.07709886\n",
      "Iteration 204, loss = 0.08782968\n",
      "Iteration 306, loss = 0.07700163\n",
      "Iteration 205, loss = 0.08758094\n",
      "Iteration 102, loss = 0.26237606\n",
      "Iteration 206, loss = 0.08733016\n",
      "Iteration 179, loss = 0.09434921\n",
      "Iteration 3, loss = 0.74981407\n",
      "Iteration 298, loss = 0.15362304\n",
      "Iteration 307, loss = 0.07690211\n",
      "Iteration 180, loss = 0.09416228\n",
      "Iteration 198, loss = 0.19305479\n",
      "Iteration 207, loss = 0.08710707\n",
      "Iteration 181, loss = 0.09395445\n",
      "Iteration 68, loss = 0.16135657\n",
      "Iteration 208, loss = 0.08684066\n",
      "Iteration 182, loss = 0.09377966\n",
      "Iteration 308, loss = 0.07681821\n",
      "Iteration 183, loss = 0.09358146\n",
      "Iteration 69, loss = 0.16007441\n",
      "Iteration 299, loss = 0.15352234\n",
      "Iteration 70, loss = 0.15879403Iteration 209, loss = 0.08660353\n",
      "\n",
      "Iteration 309, loss = 0.07671778\n",
      "Iteration 4, loss = 0.69103111\n",
      "Iteration 310, loss = 0.07662386\n",
      "Iteration 184, loss = 0.09340292\n",
      "Iteration 311, loss = 0.07653244\n",
      "Iteration 199, loss = 0.19289830\n",
      "Iteration 300, loss = 0.15344439\n",
      "Iteration 210, loss = 0.08636841\n",
      "Iteration 312, loss = 0.07644392\n",
      "Iteration 71, loss = 0.15759502\n",
      "Iteration 211, loss = 0.08612667\n",
      "Iteration 212, loss = 0.08588835\n",
      "Iteration 313, loss = 0.07634589\n",
      "Iteration 185, loss = 0.09321389\n",
      "Iteration 213, loss = 0.08567577\n",
      "Iteration 214, loss = 0.08544862\n",
      "Iteration 215, loss = 0.08522378\n",
      "Iteration 216, loss = 0.08500802\n",
      "Iteration 5, loss = 0.63987494\n",
      "Iteration 314, loss = 0.07625990\n",
      "Iteration 72, loss = 0.15640053\n",
      "Iteration 217, loss = 0.08479660\n",
      "Iteration 218, loss = 0.08459461\n",
      "Iteration 73, loss = 0.15526050\n",
      "Iteration 186, loss = 0.09302916\n",
      "Iteration 315, loss = 0.07616562\n",
      "Iteration 187, loss = 0.09285859\n",
      "Iteration 200, loss = 0.19273440\n",
      "Iteration 188, loss = 0.09267192\n",
      "Iteration 316, loss = 0.07606933\n",
      "Iteration 301, loss = 0.15334547\n",
      "Iteration 219, loss = 0.08436749\n",
      "Iteration 6, loss = 0.59768860\n",
      "Iteration 103, loss = 0.26162462\n",
      "Iteration 220, loss = 0.08416164\n",
      "Iteration 74, loss = 0.15415562\n",
      "Iteration 75, loss = 0.15310699\n",
      "Iteration 221, loss = 0.08395935\n",
      "Iteration 222, loss = 0.08375196\n",
      "Iteration 7, loss = 0.56342154\n",
      "Iteration 189, loss = 0.09250374\n",
      "Iteration 223, loss = 0.08354814\n",
      "Iteration 224, loss = 0.08336017\n",
      "Iteration 76, loss = 0.15207067\n",
      "Iteration 190, loss = 0.09233360\n",
      "Iteration 317, loss = 0.07597685\n",
      "Iteration 302, loss = 0.15323745\n",
      "Iteration 191, loss = 0.09215270\n",
      "Iteration 225, loss = 0.08314763\n",
      "Iteration 318, loss = 0.07589534\n",
      "Iteration 201, loss = 0.19258049\n",
      "Iteration 226, loss = 0.08295092\n",
      "Iteration 319, loss = 0.07580229\n",
      "Iteration 227, loss = 0.08275901\n",
      "Iteration 104, loss = 0.26092494\n",
      "Iteration 8, loss = 0.53461822\n",
      "Iteration 320, loss = 0.07571517\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 228, loss = 0.08257381\n",
      "Iteration 77, loss = 0.15105466\n",
      "Iteration 303, loss = 0.15313256\n",
      "Iteration 229, loss = 0.08235973\n",
      "Iteration 1, loss = 0.81425314\n",
      "Iteration 192, loss = 0.09197535\n",
      "Iteration 2, loss = 0.78651810\n",
      "Iteration 3, loss = 0.74963794\n",
      "Iteration 193, loss = 0.09180012\n",
      "Iteration 4, loss = 0.71048833\n",
      "Iteration 194, loss = 0.09165470\n",
      "Iteration 5, loss = 0.67390368\n",
      "Iteration 304, loss = 0.15304188\n",
      "Iteration 195, loss = 0.09146548\n",
      "Iteration 78, loss = 0.15008684\n",
      "Iteration 230, loss = 0.08217200\n",
      "Iteration 231, loss = 0.08197889\n",
      "Iteration 9, loss = 0.51133966\n",
      "Iteration 79, loss = 0.14915126\n",
      "Iteration 6, loss = 0.64181249\n",
      "Iteration 232, loss = 0.08179031\n",
      "Iteration 233, loss = 0.08161189\n",
      "Iteration 7, loss = 0.61222972\n",
      "Iteration 196, loss = 0.09130206\n",
      "Iteration 234, loss = 0.08143083\n",
      "Iteration 235, loss = 0.08124096\n",
      "Iteration 236, loss = 0.08104815\n",
      "Iteration 80, loss = 0.14823868\n",
      "Iteration 8, loss = 0.58601126\n",
      "Iteration 81, loss = 0.14737168\n",
      "Iteration 9, loss = 0.56306316\n",
      "Iteration 197, loss = 0.09113172\n",
      "Iteration 82, loss = 0.14649015\n",
      "Iteration 202, loss = 0.19235626\n",
      "Iteration 237, loss = 0.08087688\n",
      "Iteration 238, loss = 0.08070050\n",
      "Iteration 239, loss = 0.08052402\n",
      "Iteration 10, loss = 0.49155489\n",
      "Iteration 105, loss = 0.26025411\n",
      "Iteration 305, loss = 0.15293258\n",
      "Iteration 11, loss = 0.47471715\n",
      "Iteration 10, loss = 0.54210707\n",
      "Iteration 306, loss = 0.15282864\n",
      "Iteration 198, loss = 0.09097244\n",
      "Iteration 240, loss = 0.08034298\n",
      "Iteration 11, loss = 0.52319268\n",
      "Iteration 83, loss = 0.14563100\n",
      "Iteration 241, loss = 0.08017217\n",
      "Iteration 199, loss = 0.09080984\n",
      "Iteration 12, loss = 0.50651284\n",
      "Iteration 13, loss = 0.49089144\n",
      "Iteration 242, loss = 0.07999418\n",
      "Iteration 243, loss = 0.07982456\n",
      "Iteration 203, loss = 0.19218085\n",
      "Iteration 200, loss = 0.09065535\n",
      "Iteration 84, loss = 0.14482517\n",
      "Iteration 85, loss = 0.14402983\n",
      "Iteration 14, loss = 0.47746441\n",
      "Iteration 201, loss = 0.09049379\n",
      "Iteration 86, loss = 0.14326833\n",
      "Iteration 87, loss = 0.14249513\n",
      "Iteration 307, loss = 0.15276752\n",
      "Iteration 202, loss = 0.09033225\n",
      "Iteration 244, loss = 0.07963800\n",
      "Iteration 15, loss = 0.46480733\n",
      "Iteration 106, loss = 0.25956994\n",
      "Iteration 16, loss = 0.45377217\n",
      "Iteration 245, loss = 0.07947196\n",
      "Iteration 246, loss = 0.07929634\n",
      "Iteration 17, loss = 0.44315327\n",
      "Iteration 203, loss = 0.09019574\n",
      "Iteration 18, loss = 0.43415364\n",
      "Iteration 12, loss = 0.45992201\n",
      "Iteration 247, loss = 0.07915489\n",
      "Iteration 308, loss = 0.15263446\n",
      "Iteration 248, loss = 0.07896028\n",
      "Iteration 13, loss = 0.44657065\n",
      "Iteration 204, loss = 0.19200296\n",
      "Iteration 204, loss = 0.09002590\n",
      "Iteration 249, loss = 0.07878429\n",
      "Iteration 19, loss = 0.42558591\n",
      "Iteration 88, loss = 0.14178083\n",
      "Iteration 14, loss = 0.43461108\n",
      "Iteration 89, loss = 0.14107263\n",
      "Iteration 90, loss = 0.14035694\n",
      "Iteration 20, loss = 0.41793147\n",
      "Iteration 250, loss = 0.07862855\n",
      "Iteration 251, loss = 0.07844944\n",
      "Iteration 252, loss = 0.07829880\n",
      "Iteration 205, loss = 0.08987667\n",
      "Iteration 21, loss = 0.41091205\n",
      "Iteration 309, loss = 0.15254896\n",
      "Iteration 253, loss = 0.07813259\n",
      "Iteration 107, loss = 0.25890368\n",
      "Iteration 310, loss = 0.15250580\n",
      "Iteration 15, loss = 0.42354557\n",
      "Iteration 91, loss = 0.13969876\n",
      "Iteration 206, loss = 0.08972508\n",
      "Iteration 254, loss = 0.07797828\n",
      "Iteration 22, loss = 0.40424647\n",
      "Iteration 207, loss = 0.08957610\n",
      "Iteration 255, loss = 0.07779977\n",
      "Iteration 208, loss = 0.08944480\n",
      "Iteration 205, loss = 0.19183491\n",
      "Iteration 23, loss = 0.39824998\n",
      "Iteration 256, loss = 0.07764290\n",
      "Iteration 24, loss = 0.39276701\n",
      "Iteration 209, loss = 0.08929176\n",
      "Iteration 257, loss = 0.07749159\n",
      "Iteration 25, loss = 0.38737032\n",
      "Iteration 92, loss = 0.13901941\n",
      "Iteration 258, loss = 0.07733567\n",
      "Iteration 259, loss = 0.07716928\n",
      "Iteration 16, loss = 0.41365732\n",
      "Iteration 210, loss = 0.08913462\n",
      "Iteration 26, loss = 0.38235821\n",
      "Iteration 27, loss = 0.37753834\n",
      "Iteration 93, loss = 0.13837842\n",
      "Iteration 28, loss = 0.37313359\n",
      "Iteration 29, loss = 0.36860663\n",
      "Iteration 311, loss = 0.15240743\n",
      "Iteration 260, loss = 0.07701474\n",
      "Iteration 211, loss = 0.08898843\n",
      "Iteration 30, loss = 0.36445369\n",
      "Iteration 31, loss = 0.36037473\n",
      "Iteration 212, loss = 0.08885648\n",
      "Iteration 261, loss = 0.07685019\n",
      "Iteration 32, loss = 0.35641688\n",
      "Iteration 262, loss = 0.07670311\n",
      "Iteration 94, loss = 0.13776249\n",
      "Iteration 213, loss = 0.08870295\n",
      "Iteration 312, loss = 0.15228647\n",
      "Iteration 108, loss = 0.25825332\n",
      "Iteration 263, loss = 0.07654995\n",
      "Iteration 95, loss = 0.13715457\n",
      "Iteration 264, loss = 0.07640225\n",
      "Iteration 96, loss = 0.13654695\n",
      "Iteration 17, loss = 0.40416919\n",
      "Iteration 206, loss = 0.19168179\n",
      "Iteration 33, loss = 0.35246743\n",
      "Iteration 265, loss = 0.07624041\n",
      "Iteration 214, loss = 0.08856551\n",
      "Iteration 266, loss = 0.07610421\n",
      "Iteration 34, loss = 0.34870173\n",
      "Iteration 215, loss = 0.08842674\n",
      "Iteration 18, loss = 0.39514526\n",
      "Iteration 97, loss = 0.13595590\n",
      "Iteration 35, loss = 0.34495536\n",
      "Iteration 216, loss = 0.08828839\n",
      "Iteration 19, loss = 0.38673546\n",
      "Iteration 313, loss = 0.15219562\n",
      "Iteration 267, loss = 0.07594517\n",
      "Iteration 268, loss = 0.07580002\n",
      "Iteration 217, loss = 0.08816137\n",
      "Iteration 20, loss = 0.37852686\n",
      "Iteration 207, loss = 0.19150607\n",
      "Iteration 98, loss = 0.13536577\n",
      "Iteration 218, loss = 0.08801295\n",
      "Iteration 36, loss = 0.34124536\n",
      "Iteration 219, loss = 0.08788952\n",
      "Iteration 314, loss = 0.15209758\n",
      "Iteration 269, loss = 0.07566347\n",
      "Iteration 37, loss = 0.33767758\n",
      "Iteration 270, loss = 0.07549581\n",
      "Iteration 38, loss = 0.33401179\n",
      "Iteration 99, loss = 0.13481743\n",
      "Iteration 39, loss = 0.33046064\n",
      "Iteration 220, loss = 0.08774014\n",
      "Iteration 109, loss = 0.25766460\n",
      "Iteration 271, loss = 0.07535643\n",
      "Iteration 40, loss = 0.32690081\n",
      "Iteration 272, loss = 0.07520459\n",
      "Iteration 221, loss = 0.08761808\n",
      "Iteration 273, loss = 0.07508425\n",
      "Iteration 100, loss = 0.13427291\n",
      "Iteration 41, loss = 0.32337967\n",
      "Iteration 274, loss = 0.07492582\n",
      "Iteration 275, loss = 0.07478943\n",
      "Iteration 101, loss = 0.13370827\n",
      "Iteration 42, loss = 0.31971800\n",
      "Iteration 222, loss = 0.08747753\n",
      "Iteration 21, loss = 0.37065536\n",
      "Iteration 315, loss = 0.15199719\n",
      "Iteration 276, loss = 0.07464721\n",
      "Iteration 43, loss = 0.31626579\n",
      "Iteration 44, loss = 0.31283224\n",
      "Iteration 208, loss = 0.19136011\n",
      "Iteration 277, loss = 0.07451861\n",
      "Iteration 45, loss = 0.30934207\n",
      "Iteration 278, loss = 0.07438132\n",
      "Iteration 223, loss = 0.08735245\n",
      "Iteration 102, loss = 0.13320213\n",
      "Iteration 22, loss = 0.36335918\n",
      "Iteration 46, loss = 0.30586453\n",
      "Iteration 316, loss = 0.15189886\n",
      "Iteration 279, loss = 0.07423209\n",
      "Iteration 224, loss = 0.08721808\n",
      "Iteration 280, loss = 0.07411745\n",
      "Iteration 103, loss = 0.13264384\n",
      "Iteration 281, loss = 0.07396753\n",
      "Iteration 225, loss = 0.08708900\n",
      "Iteration 282, loss = 0.07383239\n",
      "Iteration 47, loss = 0.30242392\n",
      "Iteration 317, loss = 0.15185443\n",
      "Iteration 23, loss = 0.35617823\n",
      "Iteration 283, loss = 0.07370863Iteration 226, loss = 0.08695634\n",
      "\n",
      "Iteration 104, loss = 0.13216875\n",
      "Iteration 110, loss = 0.25704926\n",
      "Iteration 48, loss = 0.29890478\n",
      "Iteration 284, loss = 0.07357966\n",
      "Iteration 227, loss = 0.08683156\n",
      "Iteration 105, loss = 0.13166439\n",
      "Iteration 285, loss = 0.07345952\n",
      "Iteration 49, loss = 0.29554925\n",
      "Iteration 286, loss = 0.07331285\n",
      "Iteration 287, loss = 0.07318992\n",
      "Iteration 209, loss = 0.19118079\n",
      "Iteration 106, loss = 0.13116814\n",
      "Iteration 228, loss = 0.08670110\n",
      "Iteration 50, loss = 0.29207781\n",
      "Iteration 107, loss = 0.13067110\n",
      "Iteration 288, loss = 0.07305490\n",
      "Iteration 24, loss = 0.34931804\n",
      "Iteration 111, loss = 0.25641345\n",
      "Iteration 51, loss = 0.28876221\n",
      "Iteration 289, loss = 0.07291134\n",
      "Iteration 108, loss = 0.13022845\n",
      "Iteration 290, loss = 0.07279636\n",
      "Iteration 229, loss = 0.08658638\n",
      "Iteration 291, loss = 0.07266366\n",
      "Iteration 230, loss = 0.08645751\n",
      "Iteration 318, loss = 0.15173088\n",
      "Iteration 292, loss = 0.07252531\n",
      "Iteration 52, loss = 0.28527245\n",
      "Iteration 293, loss = 0.07242244\n",
      "Iteration 294, loss = 0.07226879\n",
      "Iteration 53, loss = 0.28182866\n",
      "Iteration 295, loss = 0.07216052\n",
      "Iteration 54, loss = 0.27846093\n",
      "Iteration 296, loss = 0.07203668\n",
      "Iteration 55, loss = 0.27506074\n",
      "Iteration 25, loss = 0.34256723\n",
      "Iteration 319, loss = 0.15166220\n",
      "Iteration 297, loss = 0.07190340\n",
      "Iteration 109, loss = 0.12975654\n",
      "Iteration 56, loss = 0.27161788\n",
      "Iteration 57, loss = 0.26828489\n",
      "Iteration 231, loss = 0.08633698\n",
      "Iteration 232, loss = 0.08620232\n",
      "Iteration 298, loss = 0.07178170\n",
      "Iteration 26, loss = 0.33620410\n",
      "Iteration 110, loss = 0.12930891\n",
      "Iteration 58, loss = 0.26488545\n",
      "Iteration 210, loss = 0.19102431\n",
      "Iteration 59, loss = 0.26153623\n",
      "Iteration 60, loss = 0.25826439\n",
      "Iteration 299, loss = 0.07166663\n",
      "Iteration 111, loss = 0.12884612\n",
      "Iteration 300, loss = 0.07152982\n",
      "Iteration 233, loss = 0.08608671\n",
      "Iteration 320, loss = 0.15155221\n",
      "Iteration 61, loss = 0.25502662\n",
      "Iteration 301, loss = 0.07140704\n",
      "Iteration 234, loss = 0.08596397\n",
      "Iteration 302, loss = 0.07128980\n",
      "Iteration 112, loss = 0.12841114\n",
      "Iteration 303, loss = 0.07116880\n",
      "Iteration 235, loss = 0.08583381\n",
      "Iteration 62, loss = 0.25175756\n",
      "Iteration 304, loss = 0.07105890\n",
      "Iteration 305, loss = 0.07093867\n",
      "Iteration 306, loss = 0.07083335\n",
      "Iteration 307, loss = 0.07071342\n",
      "Iteration 236, loss = 0.08571928\n",
      "Iteration 112, loss = 0.25582103\n",
      "Iteration 321, loss = 0.15147645\n",
      "Iteration 308, loss = 0.07060579\n",
      "Iteration 309, loss = 0.07048107\n",
      "Iteration 237, loss = 0.08561452\n",
      "Iteration 238, loss = 0.08547960\n",
      "Iteration 27, loss = 0.33013070\n",
      "Iteration 113, loss = 0.12799207\n",
      "Iteration 114, loss = 0.12755703\n",
      "Iteration 28, loss = 0.32412974\n",
      "Iteration 63, loss = 0.24855543\n",
      "Iteration 64, loss = 0.24546391\n",
      "Iteration 239, loss = 0.08536075\n",
      "Iteration 65, loss = 0.24225424\n",
      "Iteration 211, loss = 0.19085026\n",
      "Iteration 115, loss = 0.12714926\n",
      "Iteration 240, loss = 0.08524618\n",
      "Iteration 310, loss = 0.07037853\n",
      "Iteration 322, loss = 0.15138952\n",
      "Iteration 311, loss = 0.07027331\n",
      "Iteration 312, loss = 0.07015391\n",
      "Iteration 29, loss = 0.31836548Iteration 241, loss = 0.08513747\n",
      "\n",
      "Iteration 313, loss = 0.07004162\n",
      "Iteration 314, loss = 0.06994187\n",
      "Iteration 66, loss = 0.23938325\n",
      "Iteration 315, loss = 0.06981257Iteration 116, loss = 0.12673303\n",
      "\n",
      "Iteration 113, loss = 0.25531313\n",
      "Iteration 67, loss = 0.23620804\n",
      "Iteration 30, loss = 0.31283603\n",
      "Iteration 212, loss = 0.19069360\n",
      "Iteration 242, loss = 0.08501910\n",
      "Iteration 68, loss = 0.23317444\n",
      "Iteration 243, loss = 0.08490144\n",
      "Iteration 323, loss = 0.15130291\n",
      "Iteration 316, loss = 0.06972402\n",
      "Iteration 244, loss = 0.08478875\n",
      "Iteration 69, loss = 0.23022598\n",
      "Iteration 117, loss = 0.12635109\n",
      "Iteration 245, loss = 0.08467734\n",
      "Iteration 317, loss = 0.06960499\n",
      "Iteration 70, loss = 0.22721622\n",
      "Iteration 318, loss = 0.06950251\n",
      "Iteration 118, loss = 0.12595658\n",
      "Iteration 246, loss = 0.08456198\n",
      "Iteration 31, loss = 0.30748291\n",
      "Iteration 114, loss = 0.25477398Iteration 319, loss = 0.06939174\n",
      "Iteration 71, loss = 0.22433086\n",
      "\n",
      "Iteration 320, loss = 0.06928968\n",
      "Iteration 72, loss = 0.22140171\n",
      "Iteration 324, loss = 0.15122823\n",
      "Iteration 119, loss = 0.12554440\n",
      "Iteration 73, loss = 0.21850909\n",
      "Iteration 32, loss = 0.30231575\n",
      "Iteration 213, loss = 0.19054884\n",
      "Iteration 74, loss = 0.21566117\n",
      "Iteration 120, loss = 0.12516286\n",
      "Iteration 75, loss = 0.21278149\n",
      "Iteration 321, loss = 0.06918776\n",
      "Iteration 33, loss = 0.29735677\n",
      "Iteration 322, loss = 0.06910443\n",
      "Iteration 323, loss = 0.06897331\n",
      "Iteration 324, loss = 0.06888693\n",
      "Iteration 325, loss = 0.06877462\n",
      "Iteration 326, loss = 0.06867216\n",
      "Iteration 247, loss = 0.08446020\n",
      "Iteration 34, loss = 0.29259272\n",
      "Iteration 327, loss = 0.06857166\n",
      "Iteration 328, loss = 0.06847417\n",
      "Iteration 325, loss = 0.15113716\n",
      "Iteration 248, loss = 0.08434611\n",
      "Iteration 329, loss = 0.06838002\n",
      "Iteration 249, loss = 0.08423578\n",
      "Iteration 250, loss = 0.08412854\n",
      "Iteration 121, loss = 0.12481875Iteration 115, loss = 0.25419537\n",
      "\n",
      "Iteration 330, loss = 0.06828243\n",
      "Iteration 76, loss = 0.20995267\n",
      "Iteration 331, loss = 0.06817430\n",
      "Iteration 332, loss = 0.06809160\n",
      "Iteration 333, loss = 0.06798155\n",
      "Iteration 77, loss = 0.20719286\n",
      "Iteration 251, loss = 0.08401826\n",
      "Iteration 326, loss = 0.15103530\n",
      "Iteration 252, loss = 0.08391597\n",
      "Iteration 35, loss = 0.28796489\n",
      "Iteration 78, loss = 0.20433612\n",
      "Iteration 214, loss = 0.19039252\n",
      "Iteration 122, loss = 0.12443054\n",
      "Iteration 79, loss = 0.20159243\n",
      "Iteration 334, loss = 0.06788533\n",
      "Iteration 80, loss = 0.19884476\n",
      "Iteration 253, loss = 0.08380182\n",
      "Iteration 335, loss = 0.06778525\n",
      "Iteration 123, loss = 0.12405518\n",
      "Iteration 81, loss = 0.19622121\n",
      "Iteration 254, loss = 0.08369941\n",
      "Iteration 336, loss = 0.06770561\n",
      "Iteration 82, loss = 0.19361795\n",
      "Iteration 337, loss = 0.06762374\n",
      "Iteration 255, loss = 0.08359697\n",
      "Iteration 124, loss = 0.12369177\n",
      "Iteration 338, loss = 0.06751097\n",
      "Iteration 116, loss = 0.25364399\n",
      "Iteration 83, loss = 0.19104087\n",
      "Iteration 256, loss = 0.08349246\n",
      "Iteration 125, loss = 0.12333615\n",
      "Iteration 36, loss = 0.28340937\n",
      "Iteration 126, loss = 0.12299164\n",
      "Iteration 339, loss = 0.06742847\n",
      "Iteration 340, loss = 0.06733839\n",
      "Iteration 84, loss = 0.18852166\n",
      "Iteration 341, loss = 0.06723297\n",
      "Iteration 327, loss = 0.15102570\n",
      "Iteration 342, loss = 0.06714243\n",
      "Iteration 85, loss = 0.18605768\n",
      "Iteration 127, loss = 0.12270099\n",
      "Iteration 257, loss = 0.08338620\n",
      "Iteration 128, loss = 0.12231929\n",
      "Iteration 328, loss = 0.15085750\n",
      "Iteration 37, loss = 0.27911181\n",
      "Iteration 258, loss = 0.08328563\n",
      "Iteration 215, loss = 0.19024686\n",
      "Iteration 343, loss = 0.06705983\n",
      "Iteration 86, loss = 0.18363175\n",
      "Iteration 87, loss = 0.18129769\n",
      "Iteration 38, loss = 0.27486189\n",
      "Iteration 329, loss = 0.15081740\n",
      "Iteration 88, loss = 0.17892582\n",
      "Iteration 259, loss = 0.08317930\n",
      "Iteration 89, loss = 0.17669704\n",
      "Iteration 260, loss = 0.08309161\n",
      "Iteration 344, loss = 0.06696031\n",
      "Iteration 90, loss = 0.17447025\n",
      "Iteration 129, loss = 0.12198930\n",
      "Iteration 261, loss = 0.08298510\n",
      "Iteration 262, loss = 0.08287920\n",
      "Iteration 130, loss = 0.12164466\n",
      "Iteration 345, loss = 0.06687498\n",
      "Iteration 346, loss = 0.06681117\n",
      "Iteration 347, loss = 0.06670595\n",
      "Iteration 348, loss = 0.06661457\n",
      "Iteration 349, loss = 0.06652607\n",
      "Iteration 350, loss = 0.06643835\n",
      "Iteration 117, loss = 0.25317475\n",
      "Iteration 39, loss = 0.27093700\n",
      "Iteration 351, loss = 0.06636061\n",
      "Iteration 91, loss = 0.17229472\n",
      "Iteration 131, loss = 0.12132876\n",
      "Iteration 263, loss = 0.08278137\n",
      "Iteration 92, loss = 0.17023498\n",
      "Iteration 93, loss = 0.16821617\n",
      "Iteration 352, loss = 0.06625770\n",
      "Iteration 264, loss = 0.08268909\n",
      "Iteration 216, loss = 0.19008035\n",
      "Iteration 330, loss = 0.15072178\n",
      "Iteration 132, loss = 0.12099330\n",
      "Iteration 94, loss = 0.16619108\n",
      "Iteration 353, loss = 0.06617977\n",
      "Iteration 265, loss = 0.08258864\n",
      "Iteration 354, loss = 0.06609756\n",
      "Iteration 95, loss = 0.16429630\n",
      "Iteration 133, loss = 0.12067291\n",
      "Iteration 355, loss = 0.06604116\n",
      "Iteration 266, loss = 0.08248303\n",
      "Iteration 356, loss = 0.06592205\n",
      "Iteration 267, loss = 0.08239889\n",
      "Iteration 96, loss = 0.16237164\n",
      "Iteration 357, loss = 0.06584829\n",
      "Iteration 40, loss = 0.26705822\n",
      "Iteration 268, loss = 0.08230669\n",
      "Iteration 97, loss = 0.16062241\n",
      "Iteration 134, loss = 0.12036266\n",
      "Iteration 358, loss = 0.06574192\n",
      "Iteration 98, loss = 0.15883393\n",
      "Iteration 359, loss = 0.06566594\n",
      "Iteration 269, loss = 0.08219351\n",
      "Iteration 99, loss = 0.15713199\n",
      "Iteration 41, loss = 0.26315556\n",
      "Iteration 360, loss = 0.06557578\n",
      "Iteration 100, loss = 0.15540405\n",
      "Iteration 118, loss = 0.25258829\n",
      "Iteration 361, loss = 0.06549191\n",
      "Iteration 362, loss = 0.06540710\n",
      "Iteration 270, loss = 0.08208877\n",
      "Iteration 363, loss = 0.06533239\n",
      "Iteration 364, loss = 0.06525438\n",
      "Iteration 135, loss = 0.12007782\n",
      "Iteration 42, loss = 0.25962717\n",
      "Iteration 271, loss = 0.08199556\n",
      "Iteration 136, loss = 0.11975838\n",
      "Iteration 137, loss = 0.11945620\n",
      "Iteration 217, loss = 0.18992510\n",
      "Iteration 365, loss = 0.06517062\n",
      "Iteration 366, loss = 0.06508943\n",
      "Iteration 331, loss = 0.15066834\n",
      "Iteration 272, loss = 0.08190063\n",
      "Iteration 138, loss = 0.11915970\n",
      "Iteration 332, loss = 0.15054119\n",
      "Iteration 139, loss = 0.11886917\n",
      "Iteration 367, loss = 0.06501130Iteration 101, loss = 0.15379534\n",
      "\n",
      "Iteration 43, loss = 0.25607347\n",
      "Iteration 102, loss = 0.15223411\n",
      "Iteration 103, loss = 0.15072652\n",
      "Iteration 368, loss = 0.06492820\n",
      "Iteration 369, loss = 0.06484116\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 218, loss = 0.18981150\n",
      "Iteration 273, loss = 0.08180974\n",
      "Iteration 274, loss = 0.08172470\n",
      "Iteration 275, loss = 0.08162440\n",
      "Iteration 119, loss = 0.25206169\n",
      "Iteration 140, loss = 0.11856761Iteration 104, loss = 0.14911739\n",
      "Iteration 105, loss = 0.14764314\n",
      "Iteration 333, loss = 0.15050445\n",
      "\n",
      "Iteration 44, loss = 0.25270404\n",
      "Iteration 106, loss = 0.14625716\n",
      "Iteration 107, loss = 0.14482036\n",
      "Iteration 141, loss = 0.11829096\n",
      "Iteration 1, loss = 0.79370526\n",
      "Iteration 2, loss = 0.75367364\n",
      "Iteration 219, loss = 0.18962400\n",
      "Iteration 276, loss = 0.08153403\n",
      "Iteration 108, loss = 0.14343172\n",
      "Iteration 277, loss = 0.08144273\n",
      "Iteration 3, loss = 0.70360752\n",
      "Iteration 334, loss = 0.15040594\n",
      "Iteration 109, loss = 0.14215513\n",
      "Iteration 278, loss = 0.08135042\n",
      "Iteration 279, loss = 0.08125658\n",
      "Iteration 45, loss = 0.24940987\n",
      "Iteration 110, loss = 0.14078145\n",
      "Iteration 120, loss = 0.25156362\n",
      "Iteration 142, loss = 0.11799961\n",
      "Iteration 143, loss = 0.11772095\n",
      "Iteration 280, loss = 0.08116803\n",
      "Iteration 46, loss = 0.24618605\n",
      "Iteration 281, loss = 0.08108822\n",
      "Iteration 111, loss = 0.13953671\n",
      "Iteration 112, loss = 0.13826333\n",
      "Iteration 282, loss = 0.08098663\n",
      "Iteration 144, loss = 0.11744884\n",
      "Iteration 145, loss = 0.11719370\n",
      "Iteration 283, loss = 0.08090369\n",
      "Iteration 47, loss = 0.24310582Iteration 113, loss = 0.13708718\n",
      "\n",
      "Iteration 114, loss = 0.13589752\n",
      "Iteration 48, loss = 0.24015569\n",
      "Iteration 335, loss = 0.15027903\n",
      "Iteration 4, loss = 0.65340986\n",
      "Iteration 284, loss = 0.08082174\n",
      "Iteration 115, loss = 0.13475697\n",
      "Iteration 220, loss = 0.18947738\n",
      "Iteration 285, loss = 0.08072446\n",
      "Iteration 336, loss = 0.15021266\n",
      "Iteration 121, loss = 0.25107105\n",
      "Iteration 146, loss = 0.11691709\n",
      "Iteration 147, loss = 0.11664647\n",
      "Iteration 5, loss = 0.61071908\n",
      "Iteration 116, loss = 0.13364570\n",
      "Iteration 117, loss = 0.13253920\n",
      "Iteration 337, loss = 0.15011901\n",
      "Iteration 118, loss = 0.13148561\n",
      "Iteration 119, loss = 0.13044529\n",
      "Iteration 49, loss = 0.23731594\n",
      "Iteration 286, loss = 0.08062818\n",
      "Iteration 148, loss = 0.11638072\n",
      "Iteration 6, loss = 0.57321032\n",
      "Iteration 120, loss = 0.12940366\n",
      "Iteration 121, loss = 0.12846684\n",
      "Iteration 122, loss = 0.12745120\n",
      "Iteration 122, loss = 0.25069474\n",
      "Iteration 123, loss = 0.12648017\n",
      "Iteration 287, loss = 0.08054245\n",
      "Iteration 50, loss = 0.23447194\n",
      "Iteration 124, loss = 0.12559281\n",
      "Iteration 149, loss = 0.11610917\n",
      "Iteration 7, loss = 0.54234229\n",
      "Iteration 288, loss = 0.08045674\n",
      "Iteration 8, loss = 0.51741878\n",
      "Iteration 289, loss = 0.08037564\n",
      "Iteration 290, loss = 0.08029265\n",
      "Iteration 338, loss = 0.15008884\n",
      "Iteration 221, loss = 0.18936169\n",
      "Iteration 150, loss = 0.11585319\n",
      "Iteration 151, loss = 0.11560075\n",
      "Iteration 125, loss = 0.12468363\n",
      "Iteration 126, loss = 0.12379859\n",
      "Iteration 291, loss = 0.08019649\n",
      "Iteration 292, loss = 0.08011768\n",
      "Iteration 51, loss = 0.23186019\n",
      "Iteration 222, loss = 0.18917681\n",
      "Iteration 127, loss = 0.12294646\n",
      "Iteration 339, loss = 0.14998635\n",
      "Iteration 152, loss = 0.11535101\n",
      "Iteration 340, loss = 0.14989304\n",
      "Iteration 153, loss = 0.11510915\n",
      "Iteration 9, loss = 0.49657002\n",
      "Iteration 123, loss = 0.25016160\n",
      "Iteration 293, loss = 0.08003551\n",
      "Iteration 154, loss = 0.11485442\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 0.47959046\n",
      "Iteration 128, loss = 0.12210895\n",
      "Iteration 155, loss = 0.11461757\n",
      "Iteration 52, loss = 0.22912903\n",
      "Iteration 11, loss = 0.46469305\n",
      "Iteration 129, loss = 0.12127424\n",
      "Iteration 156, loss = 0.11436591\n",
      "Iteration 223, loss = 0.18904307\n",
      "Iteration 130, loss = 0.12046702\n",
      "Iteration 131, loss = 0.11969941\n",
      "Iteration 12, loss = 0.45240796\n",
      "Iteration 132, loss = 0.11893407\n",
      "Iteration 341, loss = 0.14982755\n",
      "Iteration 53, loss = 0.22665503\n",
      "Iteration 1, loss = 0.87608380\n",
      "Iteration 133, loss = 0.11820008\n",
      "Iteration 13, loss = 0.44149620\n",
      "Iteration 157, loss = 0.11412929\n",
      "Iteration 134, loss = 0.11743883\n",
      "Iteration 158, loss = 0.11388521\n",
      "Iteration 135, loss = 0.11671139\n",
      "Iteration 136, loss = 0.11603421\n",
      "Iteration 159, loss = 0.11365504\n",
      "Iteration 124, loss = 0.24969753\n",
      "Iteration 342, loss = 0.14975351\n",
      "Iteration 14, loss = 0.43181426\n",
      "Iteration 224, loss = 0.18890992\n",
      "Iteration 137, loss = 0.11533113\n",
      "Iteration 2, loss = 0.83432410\n",
      "Iteration 15, loss = 0.42308611\n",
      "Iteration 160, loss = 0.11343508\n",
      "Iteration 54, loss = 0.22416195\n",
      "Iteration 161, loss = 0.11319061\n",
      "Iteration 16, loss = 0.41505252\n",
      "Iteration 55, loss = 0.22184765\n",
      "Iteration 56, loss = 0.21961362\n",
      "Iteration 138, loss = 0.11464398\n",
      "Iteration 343, loss = 0.14968778\n",
      "Iteration 3, loss = 0.78152486\n",
      "Iteration 162, loss = 0.11296732\n",
      "Iteration 139, loss = 0.11399267\n",
      "Iteration 125, loss = 0.24926530\n",
      "Iteration 140, loss = 0.11332554\n",
      "Iteration 141, loss = 0.11267663\n",
      "Iteration 163, loss = 0.11272781\n",
      "Iteration 142, loss = 0.11206969\n",
      "Iteration 344, loss = 0.14959173\n",
      "Iteration 143, loss = 0.11145200\n",
      "Iteration 225, loss = 0.18878971Iteration 144, loss = 0.11085457\n",
      "Iteration 57, loss = 0.21725745\n",
      "\n",
      "Iteration 17, loss = 0.40768135\n",
      "Iteration 126, loss = 0.24886432\n",
      "Iteration 164, loss = 0.11251088\n",
      "Iteration 18, loss = 0.40059424\n",
      "Iteration 145, loss = 0.11022681\n",
      "Iteration 4, loss = 0.72961535\n",
      "Iteration 165, loss = 0.11230519\n",
      "Iteration 166, loss = 0.11207091\n",
      "Iteration 345, loss = 0.14951326Iteration 167, loss = 0.11185148\n",
      "\n",
      "Iteration 58, loss = 0.21516030\n",
      "Iteration 5, loss = 0.68611922\n",
      "Iteration 146, loss = 0.10966527\n",
      "Iteration 19, loss = 0.39395873\n",
      "Iteration 147, loss = 0.10908143\n",
      "Iteration 59, loss = 0.21305448\n",
      "Iteration 148, loss = 0.10851513\n",
      "Iteration 149, loss = 0.10796455\n",
      "Iteration 346, loss = 0.14943439\n",
      "Iteration 150, loss = 0.10744391\n",
      "Iteration 168, loss = 0.11165052\n",
      "Iteration 20, loss = 0.38749462\n",
      "Iteration 226, loss = 0.18862256\n",
      "Iteration 151, loss = 0.10693601\n",
      "Iteration 169, loss = 0.11143463\n",
      "Iteration 347, loss = 0.14940023\n",
      "Iteration 6, loss = 0.65004787\n",
      "Iteration 152, loss = 0.10640108\n",
      "Iteration 153, loss = 0.10588612\n",
      "Iteration 170, loss = 0.11122740\n",
      "Iteration 154, loss = 0.10537292\n",
      "Iteration 7, loss = 0.61951288\n",
      "Iteration 127, loss = 0.24841955\n",
      "Iteration 155, loss = 0.10490439\n",
      "Iteration 21, loss = 0.38124168\n",
      "Iteration 227, loss = 0.18847489\n",
      "Iteration 60, loss = 0.21108532\n",
      "Iteration 171, loss = 0.11100877\n",
      "Iteration 22, loss = 0.37500860\n",
      "Iteration 23, loss = 0.36909191\n",
      "Iteration 156, loss = 0.10443526\n",
      "Iteration 172, loss = 0.11082340\n",
      "Iteration 157, loss = 0.10395999\n",
      "Iteration 348, loss = 0.14929304\n",
      "Iteration 158, loss = 0.10353287\n",
      "Iteration 61, loss = 0.20907419\n",
      "Iteration 173, loss = 0.11059638\n",
      "Iteration 159, loss = 0.10305520\n",
      "Iteration 24, loss = 0.36319045\n",
      "Iteration 160, loss = 0.10263253\n",
      "Iteration 349, loss = 0.14920782\n",
      "Iteration 161, loss = 0.10221851\n",
      "Iteration 228, loss = 0.18834481\n",
      "Iteration 128, loss = 0.24795780\n",
      "Iteration 8, loss = 0.59502924\n",
      "Iteration 62, loss = 0.20731517\n",
      "Iteration 174, loss = 0.11037789\n",
      "Iteration 25, loss = 0.35747284\n",
      "Iteration 9, loss = 0.57458268\n",
      "Iteration 175, loss = 0.11018406\n",
      "Iteration 162, loss = 0.10179975\n",
      "Iteration 163, loss = 0.10139066\n",
      "Iteration 350, loss = 0.14916471\n",
      "Iteration 164, loss = 0.10100101\n",
      "Iteration 176, loss = 0.11003651\n",
      "Iteration 63, loss = 0.20538675\n",
      "Iteration 26, loss = 0.35183381\n",
      "Iteration 165, loss = 0.10059801\n",
      "Iteration 64, loss = 0.20355895\n",
      "Iteration 27, loss = 0.34628215\n",
      "Iteration 177, loss = 0.10978356\n",
      "Iteration 166, loss = 0.10021608\n",
      "Iteration 10, loss = 0.55754627\n",
      "Iteration 167, loss = 0.09983411\n",
      "Iteration 168, loss = 0.09949224\n",
      "Iteration 65, loss = 0.20188994\n",
      "Iteration 351, loss = 0.14907874\n",
      "Iteration 229, loss = 0.18823849\n",
      "Iteration 129, loss = 0.24752709\n",
      "Iteration 169, loss = 0.09913589\n",
      "Iteration 170, loss = 0.09875878\n",
      "Iteration 28, loss = 0.34087611\n",
      "Iteration 171, loss = 0.09843678\n",
      "Iteration 352, loss = 0.14899251Iteration 178, loss = 0.10958223\n",
      "\n",
      "Iteration 172, loss = 0.09809539\n",
      "Iteration 11, loss = 0.54280050\n",
      "Iteration 29, loss = 0.33537486\n",
      "Iteration 66, loss = 0.20019232\n",
      "Iteration 179, loss = 0.10938992\n",
      "Iteration 180, loss = 0.10919930\n",
      "Iteration 230, loss = 0.18807334\n",
      "Iteration 173, loss = 0.09774045\n",
      "Iteration 12, loss = 0.53012535\n",
      "Iteration 30, loss = 0.33001964\n",
      "Iteration 31, loss = 0.32473932\n",
      "Iteration 174, loss = 0.09743588\n",
      "Iteration 181, loss = 0.10900447\n",
      "Iteration 130, loss = 0.24716542\n",
      "Iteration 175, loss = 0.09710292\n",
      "Iteration 176, loss = 0.09678526\n",
      "Iteration 353, loss = 0.14891953\n",
      "Iteration 182, loss = 0.10882095\n",
      "Iteration 177, loss = 0.09649335\n",
      "Iteration 178, loss = 0.09617969\n",
      "Iteration 67, loss = 0.19856064\n",
      "Iteration 183, loss = 0.10861414\n",
      "Iteration 32, loss = 0.31956882\n",
      "Iteration 179, loss = 0.09586634\n",
      "Iteration 180, loss = 0.09559831\n",
      "Iteration 181, loss = 0.09529801\n",
      "Iteration 354, loss = 0.14886711\n",
      "Iteration 182, loss = 0.09501219\n",
      "Iteration 33, loss = 0.31447947\n",
      "Iteration 13, loss = 0.51940624\n",
      "Iteration 231, loss = 0.18793487\n",
      "Iteration 68, loss = 0.19703453\n",
      "Iteration 184, loss = 0.10843679\n",
      "Iteration 183, loss = 0.09473001\n",
      "Iteration 34, loss = 0.30937038\n",
      "Iteration 131, loss = 0.24670662\n",
      "Iteration 14, loss = 0.50900345\n",
      "Iteration 69, loss = 0.19546617\n",
      "Iteration 185, loss = 0.10824401\n",
      "Iteration 186, loss = 0.10805972\n",
      "Iteration 35, loss = 0.30440327\n",
      "Iteration 184, loss = 0.09445895\n",
      "Iteration 355, loss = 0.14880498\n",
      "Iteration 70, loss = 0.19401536\n",
      "Iteration 187, loss = 0.10786162\n",
      "Iteration 232, loss = 0.18779403\n",
      "Iteration 185, loss = 0.09419885\n",
      "Iteration 186, loss = 0.09394121\n",
      "Iteration 187, loss = 0.09366249\n",
      "Iteration 188, loss = 0.09341095\n",
      "Iteration 356, loss = 0.14872740\n",
      "Iteration 189, loss = 0.09319200\n",
      "Iteration 188, loss = 0.10768690\n",
      "Iteration 190, loss = 0.09291351\n",
      "Iteration 191, loss = 0.09268526\n",
      "Iteration 189, loss = 0.10751197\n",
      "Iteration 357, loss = 0.14865387\n",
      "Iteration 36, loss = 0.29951033\n",
      "Iteration 192, loss = 0.09243364\n",
      "Iteration 15, loss = 0.49990847\n",
      "Iteration 132, loss = 0.24634330\n",
      "Iteration 71, loss = 0.19257894\n",
      "Iteration 193, loss = 0.09220785\n",
      "Iteration 194, loss = 0.09197159\n",
      "Iteration 37, loss = 0.29468869\n",
      "Iteration 195, loss = 0.09174825\n",
      "Iteration 233, loss = 0.18766739\n",
      "Iteration 190, loss = 0.10733259\n",
      "Iteration 38, loss = 0.28989434\n",
      "Iteration 196, loss = 0.09152920\n",
      "Iteration 358, loss = 0.14858846\n",
      "Iteration 72, loss = 0.19119612\n",
      "Iteration 39, loss = 0.28540481\n",
      "Iteration 191, loss = 0.10712849\n",
      "Iteration 40, loss = 0.28063682\n",
      "Iteration 197, loss = 0.09128895\n",
      "Iteration 41, loss = 0.27603549\n",
      "Iteration 16, loss = 0.49162575\n",
      "Iteration 42, loss = 0.27171370\n",
      "Iteration 192, loss = 0.10696818\n",
      "Iteration 43, loss = 0.26722974\n",
      "Iteration 198, loss = 0.09109784\n",
      "Iteration 193, loss = 0.10678296\n",
      "Iteration 359, loss = 0.14856944\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 44, loss = 0.26307889\n",
      "Iteration 234, loss = 0.18755540\n",
      "Iteration 194, loss = 0.10660172\n",
      "Iteration 195, loss = 0.10646064\n",
      "Iteration 199, loss = 0.09085882\n",
      "Iteration 196, loss = 0.10627314\n",
      "Iteration 1, loss = 0.63076560\n",
      "Iteration 133, loss = 0.24591591\n",
      "Iteration 2, loss = 0.61342545\n",
      "Iteration 200, loss = 0.09066876\n",
      "Iteration 73, loss = 0.18980495\n",
      "Iteration 201, loss = 0.09046126\n",
      "Iteration 197, loss = 0.10609783\n",
      "Iteration 202, loss = 0.09024529\n",
      "Iteration 74, loss = 0.18848784\n",
      "Iteration 203, loss = 0.09003830\n",
      "Iteration 198, loss = 0.10592911\n",
      "Iteration 204, loss = 0.08983113\n",
      "Iteration 17, loss = 0.48384341\n",
      "Iteration 3, loss = 0.58932378\n",
      "Iteration 205, loss = 0.08964304\n",
      "Iteration 206, loss = 0.08943924\n",
      "Iteration 4, loss = 0.56351139\n",
      "Iteration 207, loss = 0.08925633\n",
      "Iteration 5, loss = 0.53816181\n",
      "Iteration 208, loss = 0.08904079\n",
      "Iteration 199, loss = 0.10575065\n",
      "Iteration 209, loss = 0.08886300\n",
      "Iteration 134, loss = 0.24554812\n",
      "Iteration 235, loss = 0.18745475\n",
      "Iteration 210, loss = 0.08867423\n",
      "Iteration 18, loss = 0.47623257\n",
      "Iteration 200, loss = 0.10558769\n",
      "Iteration 211, loss = 0.08851024\n",
      "Iteration 6, loss = 0.51464484\n",
      "Iteration 212, loss = 0.08830204\n",
      "Iteration 213, loss = 0.08813032\n",
      "Iteration 7, loss = 0.49325031\n",
      "Iteration 214, loss = 0.08794492\n",
      "Iteration 45, loss = 0.25891324\n",
      "Iteration 46, loss = 0.25471603\n",
      "Iteration 8, loss = 0.47414127\n",
      "Iteration 201, loss = 0.10541771\n",
      "Iteration 9, loss = 0.45816164\n",
      "Iteration 202, loss = 0.10525982\n",
      "Iteration 10, loss = 0.44369278\n",
      "Iteration 203, loss = 0.10508755\n",
      "Iteration 215, loss = 0.08777938\n",
      "Iteration 19, loss = 0.46917895\n",
      "Iteration 216, loss = 0.08758684\n",
      "Iteration 204, loss = 0.10491616\n",
      "Iteration 47, loss = 0.25081442\n",
      "Iteration 11, loss = 0.43139625\n",
      "Iteration 236, loss = 0.18727752\n",
      "Iteration 217, loss = 0.08740374\n",
      "Iteration 75, loss = 0.18720437\n",
      "Iteration 135, loss = 0.24511585\n",
      "Iteration 48, loss = 0.24685047\n",
      "Iteration 76, loss = 0.18596542\n",
      "Iteration 205, loss = 0.10476395\n",
      "Iteration 12, loss = 0.42065211\n",
      "Iteration 218, loss = 0.08724412\n",
      "Iteration 219, loss = 0.08708026\n",
      "Iteration 220, loss = 0.08691383\n",
      "Iteration 77, loss = 0.18471994\n",
      "Iteration 221, loss = 0.08674357\n",
      "Iteration 20, loss = 0.46228604\n",
      "Iteration 13, loss = 0.41175650\n",
      "Iteration 206, loss = 0.10460588\n",
      "Iteration 49, loss = 0.24317982\n",
      "Iteration 14, loss = 0.40384565\n",
      "Iteration 78, loss = 0.18353879\n",
      "Iteration 50, loss = 0.23940503\n",
      "Iteration 15, loss = 0.39720172\n",
      "Iteration 237, loss = 0.18714674\n",
      "Iteration 16, loss = 0.39104456\n",
      "Iteration 207, loss = 0.10445621\n",
      "Iteration 222, loss = 0.08656803\n",
      "Iteration 223, loss = 0.08641085\n",
      "Iteration 21, loss = 0.45536778\n",
      "Iteration 208, loss = 0.10427247\n",
      "Iteration 51, loss = 0.23590649\n",
      "Iteration 79, loss = 0.18241464\n",
      "Iteration 136, loss = 0.24480232\n",
      "Iteration 17, loss = 0.38590101\n",
      "Iteration 224, loss = 0.08625338\n",
      "Iteration 209, loss = 0.10412796\n",
      "Iteration 52, loss = 0.23254271\n",
      "Iteration 210, loss = 0.10399504\n",
      "Iteration 18, loss = 0.38092041\n",
      "Iteration 53, loss = 0.22922858\n",
      "Iteration 211, loss = 0.10381376\n",
      "Iteration 225, loss = 0.08610207\n",
      "Iteration 19, loss = 0.37640417\n",
      "Iteration 226, loss = 0.08592653\n",
      "Iteration 227, loss = 0.08576366\n",
      "Iteration 228, loss = 0.08563498\n",
      "Iteration 54, loss = 0.22596720\n",
      "Iteration 229, loss = 0.08546526\n",
      "Iteration 20, loss = 0.37224880\n",
      "Iteration 22, loss = 0.44893525\n",
      "Iteration 238, loss = 0.18699918\n",
      "Iteration 80, loss = 0.18131535\n",
      "Iteration 212, loss = 0.10366408\n",
      "Iteration 21, loss = 0.36826676\n",
      "Iteration 213, loss = 0.10351949\n",
      "Iteration 137, loss = 0.24438384\n",
      "Iteration 239, loss = 0.18688840\n",
      "Iteration 230, loss = 0.08531003\n",
      "Iteration 231, loss = 0.08521499\n",
      "Iteration 23, loss = 0.44249434\n",
      "Iteration 232, loss = 0.08507027\n",
      "Iteration 214, loss = 0.10336517\n",
      "Iteration 81, loss = 0.18022652\n",
      "Iteration 22, loss = 0.36442369\n",
      "Iteration 233, loss = 0.08487776\n",
      "Iteration 55, loss = 0.22291510\n",
      "Iteration 234, loss = 0.08471163\n",
      "Iteration 235, loss = 0.08459840\n",
      "Iteration 56, loss = 0.22005962\n",
      "Iteration 236, loss = 0.08443510\n",
      "Iteration 215, loss = 0.10321083\n",
      "Iteration 23, loss = 0.36071549\n",
      "Iteration 24, loss = 0.35701182\n",
      "Iteration 25, loss = 0.35336890\n",
      "Iteration 237, loss = 0.08428982\n",
      "Iteration 238, loss = 0.08415362\n",
      "Iteration 26, loss = 0.34979795\n",
      "Iteration 57, loss = 0.21712855\n",
      "Iteration 239, loss = 0.08401466\n",
      "Iteration 216, loss = 0.10306659\n",
      "Iteration 82, loss = 0.17911420\n",
      "Iteration 240, loss = 0.08383321\n",
      "Iteration 241, loss = 0.08370707\n",
      "Iteration 24, loss = 0.43620940\n",
      "Iteration 58, loss = 0.21438141\n",
      "Iteration 27, loss = 0.34626852\n",
      "Iteration 138, loss = 0.24400030\n",
      "Iteration 242, loss = 0.08357901\n",
      "Iteration 28, loss = 0.34262557\n",
      "Iteration 243, loss = 0.08342958\n",
      "Iteration 217, loss = 0.10290818\n",
      "Iteration 29, loss = 0.33904058\n",
      "Iteration 218, loss = 0.10276656\n",
      "Iteration 83, loss = 0.17811925\n",
      "Iteration 25, loss = 0.42985277Iteration 240, loss = 0.18676058\n",
      "\n",
      "Iteration 244, loss = 0.08329397Iteration 219, loss = 0.10263507\n",
      "\n",
      "Iteration 59, loss = 0.21167051\n",
      "Iteration 245, loss = 0.08314140\n",
      "Iteration 30, loss = 0.33552561\n",
      "Iteration 246, loss = 0.08302927\n",
      "Iteration 84, loss = 0.17708567\n",
      "Iteration 60, loss = 0.20912524\n",
      "Iteration 31, loss = 0.33199901\n",
      "Iteration 220, loss = 0.10249217\n",
      "Iteration 32, loss = 0.32833223\n",
      "Iteration 247, loss = 0.08287039\n",
      "Iteration 221, loss = 0.10235514\n",
      "Iteration 241, loss = 0.18666520\n",
      "Iteration 248, loss = 0.08278032\n",
      "Iteration 26, loss = 0.42392739\n",
      "Iteration 139, loss = 0.24366091\n",
      "Iteration 249, loss = 0.08261697\n",
      "Iteration 85, loss = 0.17610797\n",
      "Iteration 33, loss = 0.32472315\n",
      "Iteration 250, loss = 0.08249770\n",
      "Iteration 251, loss = 0.08234818\n",
      "Iteration 222, loss = 0.10219991\n",
      "Iteration 34, loss = 0.32108554\n",
      "Iteration 252, loss = 0.08222117\n",
      "Iteration 61, loss = 0.20675404\n",
      "Iteration 253, loss = 0.08210137\n",
      "Iteration 223, loss = 0.10205776\n",
      "Iteration 35, loss = 0.31744173\n",
      "Iteration 254, loss = 0.08196753Iteration 36, loss = 0.31383440\n",
      "Iteration 86, loss = 0.17515475\n",
      "Iteration 37, loss = 0.31019149\n",
      "\n",
      "Iteration 62, loss = 0.20438153\n",
      "Iteration 38, loss = 0.30653299\n",
      "Iteration 242, loss = 0.18654981\n",
      "Iteration 255, loss = 0.08183562\n",
      "Iteration 27, loss = 0.41801975\n",
      "Iteration 63, loss = 0.20206366\n",
      "Iteration 87, loss = 0.17421352\n",
      "Iteration 224, loss = 0.10192128\n",
      "Iteration 256, loss = 0.08171344\n",
      "Iteration 64, loss = 0.19985175\n",
      "Iteration 257, loss = 0.08159308\n",
      "Iteration 258, loss = 0.08146265\n",
      "Iteration 140, loss = 0.24332873\n",
      "Iteration 259, loss = 0.08133317\n",
      "Iteration 260, loss = 0.08122485\n",
      "Iteration 39, loss = 0.30297954\n",
      "Iteration 225, loss = 0.10177845\n",
      "Iteration 226, loss = 0.10165662\n",
      "Iteration 227, loss = 0.10149908\n",
      "Iteration 65, loss = 0.19775162\n",
      "Iteration 88, loss = 0.17331425\n",
      "Iteration 40, loss = 0.29929511\n",
      "Iteration 261, loss = 0.08109042\n",
      "Iteration 41, loss = 0.29572223\n",
      "Iteration 262, loss = 0.08097760\n",
      "Iteration 28, loss = 0.41210403\n",
      "Iteration 66, loss = 0.19571001\n",
      "Iteration 263, loss = 0.08084569\n",
      "Iteration 42, loss = 0.29221657\n",
      "Iteration 243, loss = 0.18639743\n",
      "Iteration 43, loss = 0.28870102\n",
      "Iteration 89, loss = 0.17243796\n",
      "Iteration 228, loss = 0.10136602\n",
      "Iteration 67, loss = 0.19378755\n",
      "Iteration 264, loss = 0.08075041\n",
      "Iteration 44, loss = 0.28523018\n",
      "Iteration 229, loss = 0.10122069\n",
      "Iteration 265, loss = 0.08062222\n",
      "Iteration 266, loss = 0.08049623\n",
      "Iteration 267, loss = 0.08038336\n",
      "Iteration 45, loss = 0.28177103\n",
      "Iteration 46, loss = 0.27840397\n",
      "Iteration 29, loss = 0.40639338\n",
      "Iteration 230, loss = 0.10108035\n",
      "Iteration 141, loss = 0.24293766\n",
      "Iteration 68, loss = 0.19191414\n",
      "Iteration 47, loss = 0.27506766\n",
      "Iteration 90, loss = 0.17156580\n",
      "Iteration 48, loss = 0.27194346\n",
      "Iteration 69, loss = 0.19010061\n",
      "Iteration 268, loss = 0.08026983\n",
      "Iteration 30, loss = 0.40040444\n",
      "Iteration 244, loss = 0.18631233\n",
      "Iteration 91, loss = 0.17071281\n",
      "Iteration 269, loss = 0.08014812\n",
      "Iteration 142, loss = 0.24255049\n",
      "Iteration 231, loss = 0.10095830\n",
      "Iteration 49, loss = 0.26883250\n",
      "Iteration 31, loss = 0.39475938\n",
      "Iteration 270, loss = 0.08005879\n",
      "Iteration 50, loss = 0.26572947\n",
      "Iteration 232, loss = 0.10080584\n",
      "Iteration 271, loss = 0.07992535\n",
      "Iteration 70, loss = 0.18838510\n",
      "Iteration 272, loss = 0.07981050\n",
      "Iteration 51, loss = 0.26259390\n",
      "Iteration 273, loss = 0.07969042\n",
      "Iteration 274, loss = 0.07958750\n",
      "Iteration 92, loss = 0.16989526\n",
      "Iteration 52, loss = 0.25967193\n",
      "Iteration 245, loss = 0.18616026\n",
      "Iteration 233, loss = 0.10068470\n",
      "Iteration 71, loss = 0.18671900\n",
      "Iteration 234, loss = 0.10057455\n",
      "Iteration 72, loss = 0.18504440\n",
      "Iteration 275, loss = 0.07946553\n",
      "Iteration 235, loss = 0.10042406\n",
      "Iteration 53, loss = 0.25671308\n",
      "Iteration 276, loss = 0.07938715\n",
      "Iteration 143, loss = 0.24218480\n",
      "Iteration 32, loss = 0.38914307\n",
      "Iteration 54, loss = 0.25385193\n",
      "Iteration 93, loss = 0.16912026\n",
      "Iteration 73, loss = 0.18357239\n",
      "Iteration 277, loss = 0.07927556\n",
      "Iteration 236, loss = 0.10030022\n",
      "Iteration 94, loss = 0.16833031\n",
      "Iteration 278, loss = 0.07915582\n",
      "Iteration 55, loss = 0.25098165\n",
      "Iteration 279, loss = 0.07906202\n",
      "Iteration 56, loss = 0.24809457\n",
      "Iteration 237, loss = 0.10015664\n",
      "Iteration 95, loss = 0.16752368\n",
      "Iteration 57, loss = 0.24529043\n",
      "Iteration 280, loss = 0.07892184\n",
      "Iteration 58, loss = 0.24264273\n",
      "Iteration 246, loss = 0.18603202\n",
      "Iteration 59, loss = 0.23989865Iteration 281, loss = 0.07883369\n",
      "Iteration 238, loss = 0.10002854\n",
      "\n",
      "Iteration 33, loss = 0.38351693\n",
      "Iteration 282, loss = 0.07871105\n",
      "Iteration 74, loss = 0.18209535\n",
      "Iteration 283, loss = 0.07861199\n",
      "Iteration 75, loss = 0.18064317\n",
      "Iteration 239, loss = 0.09990870\n",
      "Iteration 60, loss = 0.23727652\n",
      "Iteration 284, loss = 0.07850249\n",
      "Iteration 144, loss = 0.24191552\n",
      "Iteration 61, loss = 0.23456522\n",
      "Iteration 240, loss = 0.09978346\n",
      "Iteration 76, loss = 0.17931172\n",
      "Iteration 96, loss = 0.16678453\n",
      "Iteration 62, loss = 0.23207202\n",
      "Iteration 285, loss = 0.07843109\n",
      "Iteration 34, loss = 0.37793946\n",
      "Iteration 63, loss = 0.22954104\n",
      "Iteration 286, loss = 0.07829476\n",
      "Iteration 247, loss = 0.18593340\n",
      "Iteration 64, loss = 0.22705525\n",
      "Iteration 77, loss = 0.17794750\n",
      "Iteration 287, loss = 0.07818211\n",
      "Iteration 241, loss = 0.09965220\n",
      "Iteration 288, loss = 0.07805857\n",
      "Iteration 97, loss = 0.16607418\n",
      "Iteration 289, loss = 0.07798115\n",
      "Iteration 290, loss = 0.07785336\n",
      "Iteration 65, loss = 0.22459250\n",
      "Iteration 291, loss = 0.07774399\n",
      "Iteration 78, loss = 0.17674450\n",
      "Iteration 242, loss = 0.09952341\n",
      "Iteration 145, loss = 0.24151856\n",
      "Iteration 35, loss = 0.37235498\n",
      "Iteration 292, loss = 0.07765195\n",
      "Iteration 66, loss = 0.22212173\n",
      "Iteration 243, loss = 0.09940341\n",
      "Iteration 293, loss = 0.07757034\n",
      "Iteration 244, loss = 0.09927750\n",
      "Iteration 248, loss = 0.18578062\n",
      "Iteration 67, loss = 0.21983017\n",
      "Iteration 245, loss = 0.09915052\n",
      "Iteration 68, loss = 0.21746230\n",
      "Iteration 98, loss = 0.16535863\n",
      "Iteration 69, loss = 0.21520148\n",
      "Iteration 79, loss = 0.17550588\n",
      "Iteration 294, loss = 0.07744064\n",
      "Iteration 295, loss = 0.07734332\n",
      "Iteration 80, loss = 0.17435768\n",
      "Iteration 296, loss = 0.07724420\n",
      "Iteration 99, loss = 0.16465877\n",
      "Iteration 246, loss = 0.09900760\n",
      "Iteration 36, loss = 0.36710375\n",
      "Iteration 70, loss = 0.21289459\n",
      "Iteration 297, loss = 0.07715314\n",
      "Iteration 100, loss = 0.16397412Iteration 71, loss = 0.21071313\n",
      "\n",
      "Iteration 298, loss = 0.07704052\n",
      "Iteration 146, loss = 0.24116929\n",
      "Iteration 247, loss = 0.09890148\n",
      "Iteration 81, loss = 0.17319339\n",
      "Iteration 249, loss = 0.18566642\n",
      "Iteration 299, loss = 0.07692909\n",
      "Iteration 72, loss = 0.20853453\n",
      "Iteration 300, loss = 0.07683214\n",
      "Iteration 248, loss = 0.09877595\n",
      "Iteration 249, loss = 0.09865625\n",
      "Iteration 301, loss = 0.07675252\n",
      "Iteration 82, loss = 0.17214221\n",
      "Iteration 73, loss = 0.20645589\n",
      "Iteration 302, loss = 0.07665445\n",
      "Iteration 250, loss = 0.09852861\n",
      "Iteration 74, loss = 0.20425531\n",
      "Iteration 37, loss = 0.36158064\n",
      "Iteration 303, loss = 0.07654516\n",
      "Iteration 304, loss = 0.07644772\n",
      "Iteration 305, loss = 0.07635530\n",
      "Iteration 251, loss = 0.09840170\n",
      "Iteration 75, loss = 0.20231649\n",
      "Iteration 101, loss = 0.16330343\n",
      "Iteration 83, loss = 0.17103744\n",
      "Iteration 147, loss = 0.24081236\n",
      "Iteration 76, loss = 0.20028077\n",
      "Iteration 250, loss = 0.18554553\n",
      "Iteration 306, loss = 0.07626148\n",
      "Iteration 77, loss = 0.19830343\n",
      "Iteration 307, loss = 0.07614357\n",
      "Iteration 102, loss = 0.16267267\n",
      "Iteration 78, loss = 0.19638201\n",
      "Iteration 79, loss = 0.19452005\n",
      "Iteration 84, loss = 0.17005919\n",
      "Iteration 103, loss = 0.16200439\n",
      "Iteration 85, loss = 0.16909459\n",
      "Iteration 38, loss = 0.35626756\n",
      "Iteration 252, loss = 0.09829180\n",
      "Iteration 308, loss = 0.07606416\n",
      "Iteration 309, loss = 0.07596751\n",
      "Iteration 253, loss = 0.09816924\n",
      "Iteration 254, loss = 0.09807018\n",
      "Iteration 310, loss = 0.07587951\n",
      "Iteration 255, loss = 0.09793944\n",
      "Iteration 86, loss = 0.16814941\n",
      "Iteration 251, loss = 0.18545376\n",
      "Iteration 80, loss = 0.19258928\n",
      "Iteration 87, loss = 0.16722392\n",
      "Iteration 311, loss = 0.07578294\n",
      "Iteration 81, loss = 0.19076322\n",
      "Iteration 104, loss = 0.16136277\n",
      "Iteration 82, loss = 0.18893697\n",
      "Iteration 83, loss = 0.18713957\n",
      "Iteration 39, loss = 0.35097339\n",
      "Iteration 312, loss = 0.07568772\n",
      "Iteration 88, loss = 0.16633901\n",
      "Iteration 256, loss = 0.09784057\n",
      "Iteration 148, loss = 0.24051227\n",
      "Iteration 89, loss = 0.16545606\n",
      "Iteration 313, loss = 0.07561955\n",
      "Iteration 314, loss = 0.07549521\n",
      "Iteration 315, loss = 0.07542770\n",
      "Iteration 40, loss = 0.34581615\n",
      "Iteration 90, loss = 0.16461692\n",
      "Iteration 316, loss = 0.07535506\n",
      "Iteration 317, loss = 0.07523265\n",
      "Iteration 105, loss = 0.16075668\n",
      "Iteration 318, loss = 0.07516510\n",
      "Iteration 257, loss = 0.09771084\n",
      "Iteration 84, loss = 0.18535455\n",
      "Iteration 319, loss = 0.07505516\n",
      "Iteration 258, loss = 0.09759862\n",
      "Iteration 85, loss = 0.18358648\n",
      "Iteration 106, loss = 0.16015702\n",
      "Iteration 41, loss = 0.34067777\n",
      "Iteration 259, loss = 0.09747177\n",
      "Iteration 86, loss = 0.18181892\n",
      "Iteration 87, loss = 0.18012028\n",
      "Iteration 149, loss = 0.24016106\n",
      "Iteration 88, loss = 0.17842929\n",
      "Iteration 89, loss = 0.17673810\n",
      "Iteration 320, loss = 0.07495251\n",
      "Iteration 90, loss = 0.17505026\n",
      "Iteration 260, loss = 0.09736316\n",
      "Iteration 91, loss = 0.17343925\n",
      "Iteration 91, loss = 0.16377414\n",
      "Iteration 252, loss = 0.18533247\n",
      "Iteration 92, loss = 0.17191599\n",
      "Iteration 261, loss = 0.09725414\n",
      "Iteration 150, loss = 0.23988204\n",
      "Iteration 262, loss = 0.09713433\n",
      "Iteration 107, loss = 0.15955713\n",
      "Iteration 321, loss = 0.07487095\n",
      "Iteration 42, loss = 0.33563683\n",
      "Iteration 322, loss = 0.07476123\n",
      "Iteration 263, loss = 0.09702706\n",
      "Iteration 92, loss = 0.16299155\n",
      "Iteration 323, loss = 0.07467244\n",
      "Iteration 324, loss = 0.07460957\n",
      "Iteration 108, loss = 0.15898702\n",
      "Iteration 264, loss = 0.09690353\n",
      "Iteration 93, loss = 0.17024207\n",
      "Iteration 109, loss = 0.15840971\n",
      "Iteration 265, loss = 0.09679474\n",
      "Iteration 93, loss = 0.16224519\n",
      "Iteration 253, loss = 0.18519407\n",
      "Iteration 325, loss = 0.07450486\n",
      "Iteration 326, loss = 0.07440152\n",
      "Iteration 94, loss = 0.16148277\n",
      "Iteration 94, loss = 0.16867014\n",
      "Iteration 95, loss = 0.16715563\n",
      "Iteration 95, loss = 0.16075395\n",
      "Iteration 43, loss = 0.33054953\n",
      "Iteration 96, loss = 0.16558769\n",
      "Iteration 327, loss = 0.07431662\n",
      "Iteration 110, loss = 0.15786341\n",
      "Iteration 266, loss = 0.09668042\n",
      "Iteration 328, loss = 0.07422692\n",
      "Iteration 97, loss = 0.16409557\n",
      "Iteration 329, loss = 0.07415070\n",
      "Iteration 267, loss = 0.09657116\n",
      "Iteration 98, loss = 0.16258775\n",
      "Iteration 151, loss = 0.23951719\n",
      "Iteration 99, loss = 0.16104897\n",
      "Iteration 330, loss = 0.07408393\n",
      "Iteration 96, loss = 0.16002434\n",
      "Iteration 268, loss = 0.09648240\n",
      "Iteration 331, loss = 0.07398672\n",
      "Iteration 100, loss = 0.15958986\n",
      "Iteration 111, loss = 0.15731898\n",
      "Iteration 269, loss = 0.09634971\n",
      "Iteration 97, loss = 0.15933064\n",
      "Iteration 270, loss = 0.09624885\n",
      "Iteration 98, loss = 0.15865685\n",
      "Iteration 112, loss = 0.15676943\n",
      "Iteration 101, loss = 0.15812927\n",
      "Iteration 102, loss = 0.15674205\n",
      "Iteration 271, loss = 0.09613447\n",
      "Iteration 103, loss = 0.15534715\n",
      "Iteration 332, loss = 0.07387429\n",
      "Iteration 333, loss = 0.07380077\n",
      "Iteration 104, loss = 0.15397412\n",
      "Iteration 272, loss = 0.09601418\n",
      "Iteration 113, loss = 0.15624739\n",
      "Iteration 254, loss = 0.18510210\n",
      "Iteration 334, loss = 0.07373604\n",
      "Iteration 99, loss = 0.15801009\n",
      "Iteration 44, loss = 0.32584153\n",
      "Iteration 152, loss = 0.23920185\n",
      "Iteration 273, loss = 0.09591770\n",
      "Iteration 105, loss = 0.15256868\n",
      "Iteration 106, loss = 0.15130484\n",
      "Iteration 100, loss = 0.15737078\n",
      "Iteration 335, loss = 0.07365439\n",
      "Iteration 336, loss = 0.07354093\n",
      "Iteration 45, loss = 0.32093231\n",
      "Iteration 255, loss = 0.18496225\n",
      "Iteration 274, loss = 0.09581247\n",
      "Iteration 275, loss = 0.09570094\n",
      "Iteration 337, loss = 0.07347341\n",
      "Iteration 107, loss = 0.14988983\n",
      "Iteration 114, loss = 0.15573067\n",
      "Iteration 276, loss = 0.09558734\n",
      "Iteration 338, loss = 0.07339798\n",
      "Iteration 339, loss = 0.07329995\n",
      "Iteration 340, loss = 0.07323058\n",
      "Iteration 101, loss = 0.15673601\n",
      "Iteration 115, loss = 0.15520991\n",
      "Iteration 46, loss = 0.31622930\n",
      "Iteration 102, loss = 0.15611279\n",
      "Iteration 277, loss = 0.09548450\n",
      "Iteration 278, loss = 0.09539604\n",
      "Iteration 153, loss = 0.23888427\n",
      "Iteration 341, loss = 0.07313792\n",
      "Iteration 108, loss = 0.14863937\n",
      "Iteration 109, loss = 0.14739485\n",
      "Iteration 342, loss = 0.07305488\n",
      "Iteration 343, loss = 0.07297201\n",
      "Iteration 344, loss = 0.07288999\n",
      "Iteration 110, loss = 0.14612379\n",
      "Iteration 345, loss = 0.07280847\n",
      "Iteration 346, loss = 0.07274543\n",
      "Iteration 256, loss = 0.18486911\n",
      "Iteration 111, loss = 0.14493215\n",
      "Iteration 279, loss = 0.09529721\n",
      "Iteration 103, loss = 0.15547562\n",
      "Iteration 280, loss = 0.09516921\n",
      "Iteration 47, loss = 0.31173190\n",
      "Iteration 347, loss = 0.07265218\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 104, loss = 0.15489771\n",
      "Iteration 281, loss = 0.09507095\n",
      "Iteration 154, loss = 0.23859830\n",
      "Iteration 112, loss = 0.14368148\n",
      "Iteration 116, loss = 0.15475442\n",
      "Iteration 113, loss = 0.14253634\n",
      "Iteration 282, loss = 0.09498056\n",
      "Iteration 114, loss = 0.14140999\n",
      "Iteration 117, loss = 0.15425212\n",
      "Iteration 115, loss = 0.14029238\n",
      "Iteration 283, loss = 0.09487053\n",
      "Iteration 257, loss = 0.18475178\n",
      "Iteration 48, loss = 0.30716542\n",
      "Iteration 116, loss = 0.13914671\n",
      "Iteration 105, loss = 0.15430280\n",
      "Iteration 1, loss = 0.82992915\n",
      "Iteration 106, loss = 0.15373764\n",
      "Iteration 155, loss = 0.23826027\n",
      "Iteration 117, loss = 0.13805032\n",
      "Iteration 118, loss = 0.15375537\n",
      "Iteration 49, loss = 0.30282474\n",
      "Iteration 118, loss = 0.13698979\n",
      "Iteration 284, loss = 0.09478056\n",
      "Iteration 2, loss = 0.79100143\n",
      "Iteration 107, loss = 0.15317766\n",
      "Iteration 119, loss = 0.13595143\n",
      "Iteration 285, loss = 0.09465751\n",
      "Iteration 120, loss = 0.13493012\n",
      "Iteration 121, loss = 0.13397051\n",
      "Iteration 119, loss = 0.15330353\n",
      "Iteration 108, loss = 0.15262739\n",
      "Iteration 109, loss = 0.15208647\n",
      "Iteration 3, loss = 0.74014393\n",
      "Iteration 258, loss = 0.18468600\n",
      "Iteration 286, loss = 0.09456060\n",
      "Iteration 122, loss = 0.13300816\n",
      "Iteration 4, loss = 0.68904510\n",
      "Iteration 287, loss = 0.09446836\n",
      "Iteration 123, loss = 0.13201806\n",
      "Iteration 50, loss = 0.29857620\n",
      "Iteration 120, loss = 0.15284567\n",
      "Iteration 124, loss = 0.13108439\n",
      "Iteration 288, loss = 0.09435252\n",
      "Iteration 156, loss = 0.23799296\n",
      "Iteration 125, loss = 0.13017384\n",
      "Iteration 289, loss = 0.09427561\n",
      "Iteration 5, loss = 0.64360343\n",
      "Iteration 110, loss = 0.15154438\n",
      "Iteration 259, loss = 0.18454944\n",
      "Iteration 290, loss = 0.09416308\n",
      "Iteration 111, loss = 0.15104959\n",
      "Iteration 51, loss = 0.29445560\n",
      "Iteration 291, loss = 0.09410397\n",
      "Iteration 126, loss = 0.12926076\n",
      "Iteration 121, loss = 0.15237348\n",
      "Iteration 292, loss = 0.09397469\n",
      "Iteration 127, loss = 0.12838149\n",
      "Iteration 128, loss = 0.12751544\n",
      "Iteration 293, loss = 0.09386695\n",
      "Iteration 122, loss = 0.15194495\n",
      "Iteration 129, loss = 0.12668137\n",
      "Iteration 123, loss = 0.15150685\n",
      "Iteration 6, loss = 0.60368738\n",
      "Iteration 157, loss = 0.23772649\n",
      "Iteration 112, loss = 0.15051276\n",
      "Iteration 130, loss = 0.12587754\n",
      "Iteration 131, loss = 0.12504810\n",
      "Iteration 7, loss = 0.57005065\n",
      "Iteration 132, loss = 0.12431272\n",
      "Iteration 260, loss = 0.18443629\n",
      "Iteration 113, loss = 0.15001854\n",
      "Iteration 8, loss = 0.54369916\n",
      "Iteration 294, loss = 0.09377651\n",
      "Iteration 133, loss = 0.12352486\n",
      "Iteration 52, loss = 0.29029581\n",
      "Iteration 134, loss = 0.12277980\n",
      "Iteration 114, loss = 0.14949087\n",
      "Iteration 295, loss = 0.09367256\n",
      "Iteration 135, loss = 0.12202902\n",
      "Iteration 124, loss = 0.15106361\n",
      "Iteration 9, loss = 0.52058947\n",
      "Iteration 296, loss = 0.09359601\n",
      "Iteration 115, loss = 0.14901277\n",
      "Iteration 125, loss = 0.15065251\n",
      "Iteration 136, loss = 0.12130812\n",
      "Iteration 53, loss = 0.28647895\n",
      "Iteration 137, loss = 0.12060042\n",
      "Iteration 158, loss = 0.23742159\n",
      "Iteration 138, loss = 0.11994908\n",
      "Iteration 54, loss = 0.28257300\n",
      "Iteration 139, loss = 0.11928639\n",
      "Iteration 10, loss = 0.50244028\n",
      "Iteration 297, loss = 0.09349844\n",
      "Iteration 261, loss = 0.18433389\n",
      "Iteration 298, loss = 0.09339536\n",
      "Iteration 116, loss = 0.14853602\n",
      "Iteration 299, loss = 0.09330151\n",
      "Iteration 117, loss = 0.14803141\n",
      "Iteration 140, loss = 0.11863375\n",
      "Iteration 126, loss = 0.15024603\n",
      "Iteration 141, loss = 0.11796306\n",
      "Iteration 262, loss = 0.18419675\n",
      "Iteration 127, loss = 0.14983995\n",
      "Iteration 11, loss = 0.48704241\n",
      "Iteration 300, loss = 0.09320628\n",
      "Iteration 118, loss = 0.14758345\n",
      "Iteration 301, loss = 0.09312201\n",
      "Iteration 12, loss = 0.47409868\n",
      "Iteration 142, loss = 0.11733802\n",
      "Iteration 302, loss = 0.09301678\n",
      "Iteration 143, loss = 0.11674369\n",
      "Iteration 55, loss = 0.27892000\n",
      "Iteration 263, loss = 0.18411093\n",
      "Iteration 56, loss = 0.27530012\n",
      "Iteration 119, loss = 0.14713014\n",
      "Iteration 159, loss = 0.23709008\n",
      "Iteration 144, loss = 0.11612913\n",
      "Iteration 120, loss = 0.14666193\n",
      "Iteration 145, loss = 0.11552652\n",
      "Iteration 128, loss = 0.14942157\n",
      "Iteration 146, loss = 0.11494071\n",
      "Iteration 303, loss = 0.09292939\n",
      "Iteration 129, loss = 0.14901813\n",
      "Iteration 304, loss = 0.09282565\n",
      "Iteration 13, loss = 0.46296402\n",
      "Iteration 121, loss = 0.14620433\n",
      "Iteration 147, loss = 0.11437834\n",
      "Iteration 148, loss = 0.11384122\n",
      "Iteration 149, loss = 0.11325336\n",
      "Iteration 305, loss = 0.09274795\n",
      "Iteration 264, loss = 0.18405839\n",
      "Iteration 122, loss = 0.14575114\n",
      "Iteration 57, loss = 0.27185256\n",
      "Iteration 150, loss = 0.11274092\n",
      "Iteration 151, loss = 0.11226389\n",
      "Iteration 306, loss = 0.09266097\n",
      "Iteration 14, loss = 0.45351223\n",
      "Iteration 152, loss = 0.11169783\n",
      "Iteration 160, loss = 0.23685246\n",
      "Iteration 15, loss = 0.44580000\n",
      "Iteration 130, loss = 0.14865509\n",
      "Iteration 307, loss = 0.09255760\n",
      "Iteration 153, loss = 0.11120759\n",
      "Iteration 123, loss = 0.14529487\n",
      "Iteration 154, loss = 0.11068290\n",
      "Iteration 308, loss = 0.09247392\n",
      "Iteration 155, loss = 0.11021638\n",
      "Iteration 58, loss = 0.26847578\n",
      "Iteration 156, loss = 0.10977176\n",
      "Iteration 16, loss = 0.43839488\n",
      "Iteration 131, loss = 0.14824354\n",
      "Iteration 124, loss = 0.14488045\n",
      "Iteration 309, loss = 0.09237766\n",
      "Iteration 17, loss = 0.43146011\n",
      "Iteration 265, loss = 0.18390703\n",
      "Iteration 161, loss = 0.23656898\n",
      "Iteration 157, loss = 0.10930293\n",
      "Iteration 132, loss = 0.14785767\n",
      "Iteration 310, loss = 0.09228104\n",
      "Iteration 125, loss = 0.14444835\n",
      "Iteration 133, loss = 0.14748834\n",
      "Iteration 158, loss = 0.10888950\n",
      "Iteration 18, loss = 0.42535407\n",
      "Iteration 159, loss = 0.10839992\n",
      "Iteration 126, loss = 0.14402315\n",
      "Iteration 160, loss = 0.10796412\n",
      "Iteration 59, loss = 0.26527627\n",
      "Iteration 266, loss = 0.18379103\n",
      "Iteration 19, loss = 0.41945527\n",
      "Iteration 161, loss = 0.10755997\n",
      "Iteration 311, loss = 0.09219529\n",
      "Iteration 127, loss = 0.14364256\n",
      "Iteration 162, loss = 0.10715016\n",
      "Iteration 163, loss = 0.10672710\n",
      "Iteration 164, loss = 0.10632805\n",
      "Iteration 312, loss = 0.09210919\n",
      "Iteration 162, loss = 0.23629412Iteration 60, loss = 0.26224696\n",
      "Iteration 128, loss = 0.14322213\n",
      "Iteration 134, loss = 0.14712594\n",
      "\n",
      "Iteration 313, loss = 0.09201230\n",
      "Iteration 165, loss = 0.10594150\n",
      "Iteration 129, loss = 0.14279024\n",
      "Iteration 166, loss = 0.10555564\n",
      "Iteration 167, loss = 0.10519191\n",
      "Iteration 267, loss = 0.18369634\n",
      "Iteration 20, loss = 0.41383965\n",
      "Iteration 314, loss = 0.09195172\n",
      "Iteration 168, loss = 0.10479321\n",
      "Iteration 135, loss = 0.14675435\n",
      "Iteration 169, loss = 0.10440993\n",
      "Iteration 170, loss = 0.10405374\n",
      "Iteration 136, loss = 0.14638359\n",
      "Iteration 61, loss = 0.25920064\n",
      "Iteration 130, loss = 0.14243009\n",
      "Iteration 315, loss = 0.09184401\n",
      "Iteration 163, loss = 0.23596311\n",
      "Iteration 131, loss = 0.14202441\n",
      "Iteration 316, loss = 0.09175887\n",
      "Iteration 21, loss = 0.40843577\n",
      "Iteration 171, loss = 0.10372686\n",
      "Iteration 268, loss = 0.18357058\n",
      "Iteration 172, loss = 0.10339474\n",
      "Iteration 22, loss = 0.40314412\n",
      "Iteration 137, loss = 0.14603677\n",
      "Iteration 173, loss = 0.10304602\n",
      "Iteration 317, loss = 0.09166466\n",
      "Iteration 62, loss = 0.25636749\n",
      "Iteration 132, loss = 0.14163604\n",
      "Iteration 23, loss = 0.39792250\n",
      "Iteration 318, loss = 0.09158244\n",
      "Iteration 174, loss = 0.10265471\n",
      "Iteration 138, loss = 0.14567520\n",
      "Iteration 319, loss = 0.09149737\n",
      "Iteration 133, loss = 0.14124981\n",
      "Iteration 164, loss = 0.23566363\n",
      "Iteration 269, loss = 0.18352134\n",
      "Iteration 175, loss = 0.10235001\n",
      "Iteration 63, loss = 0.25356350\n",
      "Iteration 24, loss = 0.39280719\n",
      "Iteration 139, loss = 0.14534699\n",
      "Iteration 320, loss = 0.09140287\n",
      "Iteration 176, loss = 0.10199086\n",
      "Iteration 177, loss = 0.10168605\n",
      "Iteration 178, loss = 0.10137292\n",
      "Iteration 321, loss = 0.09132847\n",
      "Iteration 134, loss = 0.14086271\n",
      "Iteration 322, loss = 0.09124291\n",
      "Iteration 135, loss = 0.14050210\n",
      "Iteration 179, loss = 0.10106431\n",
      "Iteration 180, loss = 0.10076888\n",
      "Iteration 140, loss = 0.14500191\n",
      "Iteration 64, loss = 0.25096022\n",
      "Iteration 181, loss = 0.10044399\n",
      "Iteration 323, loss = 0.09115201\n",
      "Iteration 182, loss = 0.10014831\n",
      "Iteration 136, loss = 0.14011883\n",
      "Iteration 270, loss = 0.18337439\n",
      "Iteration 141, loss = 0.14466133\n",
      "Iteration 25, loss = 0.38766633\n",
      "Iteration 183, loss = 0.09983753\n",
      "Iteration 165, loss = 0.23540776\n",
      "Iteration 137, loss = 0.13977275\n",
      "Iteration 26, loss = 0.38265404\n",
      "Iteration 271, loss = 0.18329350\n",
      "Iteration 27, loss = 0.37762005\n",
      "Iteration 184, loss = 0.09957869\n",
      "Iteration 324, loss = 0.09107135\n",
      "Iteration 65, loss = 0.24835285\n",
      "Iteration 142, loss = 0.14433678\n",
      "Iteration 325, loss = 0.09099299\n",
      "Iteration 326, loss = 0.09090073\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 138, loss = 0.13939171\n",
      "Iteration 185, loss = 0.09928093\n",
      "Iteration 186, loss = 0.09901205\n",
      "Iteration 187, loss = 0.09872261\n",
      "Iteration 28, loss = 0.37259753\n",
      "Iteration 143, loss = 0.14400322\n",
      "Iteration 188, loss = 0.09842125\n",
      "Iteration 66, loss = 0.24586061\n",
      "Iteration 189, loss = 0.09816699\n",
      "Iteration 139, loss = 0.13903925\n",
      "Iteration 29, loss = 0.36757270\n",
      "Iteration 190, loss = 0.09789692\n",
      "Iteration 144, loss = 0.14368356\n",
      "Iteration 140, loss = 0.13868580Iteration 1, loss = 0.93272289\n",
      "\n",
      "Iteration 30, loss = 0.36266794Iteration 191, loss = 0.09762152\n",
      "\n",
      "Iteration 141, loss = 0.13832785\n",
      "Iteration 272, loss = 0.18317971\n",
      "Iteration 166, loss = 0.23516190\n",
      "Iteration 145, loss = 0.14336687\n",
      "Iteration 142, loss = 0.13798684\n",
      "Iteration 67, loss = 0.24348479\n",
      "Iteration 192, loss = 0.09738855\n",
      "Iteration 2, loss = 0.88797264\n",
      "Iteration 193, loss = 0.09712544\n",
      "Iteration 194, loss = 0.09685904\n",
      "Iteration 143, loss = 0.13764091\n",
      "Iteration 31, loss = 0.35797426\n",
      "Iteration 273, loss = 0.18310690\n",
      "Iteration 195, loss = 0.09657061\n",
      "Iteration 167, loss = 0.23492793\n",
      "Iteration 196, loss = 0.09632475\n",
      "Iteration 144, loss = 0.13729500\n",
      "Iteration 68, loss = 0.24119678\n",
      "Iteration 197, loss = 0.09607033\n",
      "Iteration 32, loss = 0.35309399\n",
      "Iteration 146, loss = 0.14303678\n",
      "Iteration 198, loss = 0.09581897\n",
      "Iteration 274, loss = 0.18298251\n",
      "Iteration 199, loss = 0.09556577\n",
      "Iteration 3, loss = 0.83023419\n",
      "Iteration 200, loss = 0.09532020\n",
      "Iteration 147, loss = 0.14274005\n",
      "Iteration 145, loss = 0.13696805\n",
      "Iteration 33, loss = 0.34835464\n",
      "Iteration 201, loss = 0.09508744\n",
      "Iteration 146, loss = 0.13664588\n",
      "Iteration 202, loss = 0.09481478\n",
      "Iteration 69, loss = 0.23901293\n",
      "Iteration 147, loss = 0.13630624\n",
      "Iteration 34, loss = 0.34370972\n",
      "Iteration 148, loss = 0.14241830\n",
      "Iteration 168, loss = 0.23464868\n",
      "Iteration 203, loss = 0.09462325\n",
      "Iteration 149, loss = 0.14211837\n",
      "Iteration 204, loss = 0.09435121\n",
      "Iteration 205, loss = 0.09412269\n",
      "Iteration 206, loss = 0.09391192\n",
      "Iteration 148, loss = 0.13600392\n",
      "Iteration 4, loss = 0.77440595\n",
      "Iteration 275, loss = 0.18288693\n",
      "Iteration 207, loss = 0.09374391\n",
      "Iteration 35, loss = 0.33910568\n",
      "Iteration 70, loss = 0.23690921\n",
      "Iteration 208, loss = 0.09346406\n",
      "Iteration 209, loss = 0.09325234\n",
      "Iteration 149, loss = 0.13566662\n",
      "Iteration 150, loss = 0.13534760\n",
      "Iteration 5, loss = 0.72417290\n",
      "Iteration 169, loss = 0.23442846\n",
      "Iteration 151, loss = 0.13503526\n",
      "Iteration 71, loss = 0.23475451\n",
      "Iteration 210, loss = 0.09301775\n",
      "Iteration 276, loss = 0.18280709\n",
      "Iteration 211, loss = 0.09281535\n",
      "Iteration 150, loss = 0.14183095\n",
      "Iteration 212, loss = 0.09263062\n",
      "Iteration 36, loss = 0.33460979\n",
      "Iteration 151, loss = 0.14150414\n",
      "Iteration 213, loss = 0.09246081\n",
      "Iteration 152, loss = 0.13471424\n",
      "Iteration 37, loss = 0.32998151\n",
      "Iteration 214, loss = 0.09221352\n",
      "Iteration 72, loss = 0.23283365\n",
      "Iteration 215, loss = 0.09202645\n",
      "Iteration 216, loss = 0.09185627\n",
      "Iteration 6, loss = 0.68111019\n",
      "Iteration 170, loss = 0.23410524\n",
      "Iteration 38, loss = 0.32565668\n",
      "Iteration 73, loss = 0.23093102\n",
      "Iteration 217, loss = 0.09161653\n",
      "Iteration 277, loss = 0.18268829\n",
      "Iteration 152, loss = 0.14123786\n",
      "Iteration 218, loss = 0.09143322\n",
      "Iteration 153, loss = 0.13442364\n",
      "Iteration 39, loss = 0.32110300\n",
      "Iteration 154, loss = 0.13411922\n",
      "Iteration 219, loss = 0.09127680\n",
      "Iteration 220, loss = 0.09103710\n",
      "Iteration 221, loss = 0.09085770\n",
      "Iteration 74, loss = 0.22903334\n",
      "Iteration 7, loss = 0.64734338\n",
      "Iteration 153, loss = 0.14094164\n",
      "Iteration 278, loss = 0.18259776\n",
      "Iteration 222, loss = 0.09067136\n",
      "Iteration 171, loss = 0.23386995\n",
      "Iteration 154, loss = 0.14065786\n",
      "Iteration 223, loss = 0.09049401\n",
      "Iteration 40, loss = 0.31657241\n",
      "Iteration 224, loss = 0.09030250\n",
      "Iteration 155, loss = 0.13380601\n",
      "Iteration 156, loss = 0.13351181\n",
      "Iteration 225, loss = 0.09009411\n",
      "Iteration 155, loss = 0.14037466\n",
      "Iteration 157, loss = 0.13322190\n",
      "Iteration 41, loss = 0.31220168\n",
      "Iteration 158, loss = 0.13291879\n",
      "Iteration 226, loss = 0.08993925\n",
      "Iteration 227, loss = 0.08981231\n",
      "Iteration 279, loss = 0.18249846\n",
      "Iteration 172, loss = 0.23358297\n",
      "Iteration 75, loss = 0.22727554\n",
      "Iteration 8, loss = 0.61861921\n",
      "Iteration 156, loss = 0.14011356\n",
      "Iteration 228, loss = 0.08958386\n",
      "Iteration 159, loss = 0.13264466\n",
      "Iteration 42, loss = 0.30791126\n",
      "Iteration 229, loss = 0.08942614\n",
      "Iteration 230, loss = 0.08927067\n",
      "Iteration 160, loss = 0.13234963\n",
      "Iteration 43, loss = 0.30360701\n",
      "Iteration 9, loss = 0.59547022\n",
      "Iteration 231, loss = 0.08911433\n",
      "Iteration 76, loss = 0.22550925\n",
      "Iteration 280, loss = 0.18240845\n",
      "Iteration 173, loss = 0.23336233\n",
      "Iteration 232, loss = 0.08888263\n",
      "Iteration 161, loss = 0.13208624\n",
      "Iteration 157, loss = 0.13983094\n",
      "Iteration 44, loss = 0.29929240\n",
      "Iteration 233, loss = 0.08873802\n",
      "Iteration 234, loss = 0.08857868\n",
      "Iteration 235, loss = 0.08838099\n",
      "Iteration 236, loss = 0.08825922\n",
      "Iteration 162, loss = 0.13179654\n",
      "Iteration 158, loss = 0.13953756\n",
      "Iteration 45, loss = 0.29519721\n",
      "Iteration 77, loss = 0.22393944\n",
      "Iteration 237, loss = 0.08809335\n",
      "Iteration 238, loss = 0.08800254\n",
      "Iteration 10, loss = 0.57586609\n",
      "Iteration 159, loss = 0.13928856\n",
      "Iteration 239, loss = 0.08777415\n",
      "Iteration 281, loss = 0.18230942\n",
      "Iteration 174, loss = 0.23311240\n",
      "Iteration 240, loss = 0.08766007\n",
      "Iteration 46, loss = 0.29115431Iteration 241, loss = 0.08748127\n",
      "\n",
      "Iteration 242, loss = 0.08732573\n",
      "Iteration 163, loss = 0.13152202\n",
      "Iteration 243, loss = 0.08716759\n",
      "Iteration 78, loss = 0.22225837\n",
      "Iteration 282, loss = 0.18231930\n",
      "Iteration 47, loss = 0.28738607\n",
      "Iteration 164, loss = 0.13124785\n",
      "Iteration 165, loss = 0.13097463\n",
      "Iteration 160, loss = 0.13901604\n",
      "Iteration 244, loss = 0.08703007\n",
      "Iteration 175, loss = 0.23295136\n",
      "Iteration 48, loss = 0.28343140\n",
      "Iteration 245, loss = 0.08688464\n",
      "Iteration 79, loss = 0.22074329\n",
      "Iteration 11, loss = 0.55977150\n",
      "Iteration 246, loss = 0.08674884\n",
      "Iteration 247, loss = 0.08658960\n",
      "Iteration 248, loss = 0.08645275\n",
      "Iteration 249, loss = 0.08633402\n",
      "Iteration 12, loss = 0.54611161\n",
      "Iteration 166, loss = 0.13072877\n",
      "Iteration 161, loss = 0.13875950\n",
      "Iteration 49, loss = 0.27966175\n",
      "Iteration 283, loss = 0.18211473\n",
      "Iteration 167, loss = 0.13046226\n",
      "Iteration 162, loss = 0.13849546\n",
      "Iteration 50, loss = 0.27592474\n",
      "Iteration 176, loss = 0.23262880\n",
      "Iteration 250, loss = 0.08619532\n",
      "Iteration 251, loss = 0.08607013\n",
      "Iteration 80, loss = 0.21923984\n",
      "Iteration 168, loss = 0.13019575\n",
      "Iteration 252, loss = 0.08591125\n",
      "Iteration 169, loss = 0.12991957\n",
      "Iteration 163, loss = 0.13825564\n",
      "Iteration 51, loss = 0.27236765\n",
      "Iteration 253, loss = 0.08579322\n",
      "Iteration 81, loss = 0.21779557\n",
      "Iteration 284, loss = 0.18203492\n",
      "Iteration 13, loss = 0.53409969\n",
      "Iteration 254, loss = 0.08565593\n",
      "Iteration 164, loss = 0.13801283\n",
      "Iteration 52, loss = 0.26891216\n",
      "Iteration 170, loss = 0.12967483\n",
      "Iteration 255, loss = 0.08549450\n",
      "Iteration 256, loss = 0.08540645\n",
      "Iteration 171, loss = 0.12942827\n",
      "Iteration 82, loss = 0.21636113\n",
      "Iteration 257, loss = 0.08522394\n",
      "Iteration 177, loss = 0.23239684\n",
      "Iteration 53, loss = 0.26547046\n",
      "Iteration 172, loss = 0.12919133\n",
      "Iteration 258, loss = 0.08510934\n",
      "Iteration 285, loss = 0.18194066\n",
      "Iteration 259, loss = 0.08501069\n",
      "Iteration 54, loss = 0.26208398\n",
      "Iteration 14, loss = 0.52332350\n",
      "Iteration 260, loss = 0.08485497\n",
      "Iteration 83, loss = 0.21500851\n",
      "Iteration 165, loss = 0.13774560\n",
      "Iteration 173, loss = 0.12895865\n",
      "Iteration 261, loss = 0.08474267\n",
      "Iteration 286, loss = 0.18184206\n",
      "Iteration 262, loss = 0.08460085\n",
      "Iteration 55, loss = 0.25874785\n",
      "Iteration 174, loss = 0.12868789\n",
      "Iteration 263, loss = 0.08446632\n",
      "Iteration 264, loss = 0.08433316\n",
      "Iteration 166, loss = 0.13751515\n",
      "Iteration 178, loss = 0.23216075\n",
      "Iteration 175, loss = 0.12844992\n",
      "Iteration 56, loss = 0.25562450\n",
      "Iteration 265, loss = 0.08423847\n",
      "Iteration 84, loss = 0.21372332\n",
      "Iteration 15, loss = 0.51301433\n",
      "Iteration 57, loss = 0.25242285\n",
      "Iteration 266, loss = 0.08410391\n",
      "Iteration 287, loss = 0.18176217\n",
      "Iteration 167, loss = 0.13729061\n",
      "Iteration 267, loss = 0.08400664\n",
      "Iteration 85, loss = 0.21245549\n",
      "Iteration 176, loss = 0.12824085\n",
      "Iteration 58, loss = 0.24937161\n",
      "Iteration 268, loss = 0.08386375\n",
      "Iteration 179, loss = 0.23191862\n",
      "Iteration 16, loss = 0.50359687\n",
      "Iteration 168, loss = 0.13701016\n",
      "Iteration 177, loss = 0.12798451\n",
      "Iteration 59, loss = 0.24641870\n",
      "Iteration 269, loss = 0.08374198\n",
      "Iteration 270, loss = 0.08364405\n",
      "Iteration 271, loss = 0.08356923\n",
      "Iteration 86, loss = 0.21120972\n",
      "Iteration 178, loss = 0.12777058\n",
      "Iteration 288, loss = 0.18168717\n",
      "Iteration 169, loss = 0.13677001\n",
      "Iteration 179, loss = 0.12749732\n",
      "Iteration 180, loss = 0.23171558\n",
      "Iteration 60, loss = 0.24354936\n",
      "Iteration 272, loss = 0.08339180\n",
      "Iteration 170, loss = 0.13654395\n",
      "Iteration 273, loss = 0.08329579\n",
      "Iteration 180, loss = 0.12727894\n",
      "Iteration 61, loss = 0.24063947\n",
      "Iteration 289, loss = 0.18158224\n",
      "Iteration 274, loss = 0.08319837\n",
      "Iteration 17, loss = 0.49464236\n",
      "Iteration 275, loss = 0.08308597\n",
      "Iteration 181, loss = 0.12703735Iteration 87, loss = 0.21003484\n",
      "Iteration 276, loss = 0.08296251\n",
      "\n",
      "Iteration 277, loss = 0.08287113\n",
      "Iteration 62, loss = 0.23800353\n",
      "Iteration 171, loss = 0.13630491\n",
      "Iteration 182, loss = 0.12682995\n",
      "Iteration 183, loss = 0.12659325\n",
      "Iteration 290, loss = 0.18151225\n",
      "Iteration 278, loss = 0.08281274\n",
      "Iteration 279, loss = 0.08269699\n",
      "Iteration 63, loss = 0.23538965\n",
      "Iteration 280, loss = 0.08254612\n",
      "Iteration 281, loss = 0.08241433\n",
      "Iteration 88, loss = 0.20887326\n",
      "Iteration 172, loss = 0.13605749\n",
      "Iteration 184, loss = 0.12638691\n",
      "Iteration 18, loss = 0.48604325\n",
      "Iteration 181, loss = 0.23147838\n",
      "Iteration 19, loss = 0.47769107\n",
      "Iteration 185, loss = 0.12614582\n",
      "Iteration 186, loss = 0.12593533\n",
      "Iteration 173, loss = 0.13585344\n",
      "Iteration 187, loss = 0.12572191\n",
      "Iteration 182, loss = 0.23126346\n",
      "Iteration 282, loss = 0.08231144\n",
      "Iteration 283, loss = 0.08221448\n",
      "Iteration 64, loss = 0.23268227\n",
      "Iteration 284, loss = 0.08211831\n",
      "Iteration 291, loss = 0.18140044\n",
      "Iteration 89, loss = 0.20773486\n",
      "Iteration 285, loss = 0.08201603\n",
      "Iteration 174, loss = 0.13561639\n",
      "Iteration 188, loss = 0.12548904\n",
      "Iteration 65, loss = 0.23015905\n",
      "Iteration 189, loss = 0.12530061\n",
      "Iteration 90, loss = 0.20668787\n",
      "Iteration 66, loss = 0.22772448\n",
      "Iteration 286, loss = 0.08196566\n",
      "Iteration 292, loss = 0.18133600\n",
      "Iteration 175, loss = 0.13537882\n",
      "Iteration 287, loss = 0.08183972\n",
      "Iteration 288, loss = 0.08173237\n",
      "Iteration 20, loss = 0.46968170\n",
      "Iteration 289, loss = 0.08162600\n",
      "Iteration 67, loss = 0.22534945\n",
      "Iteration 190, loss = 0.12508127\n",
      "Iteration 290, loss = 0.08154613\n",
      "Iteration 176, loss = 0.13515416\n",
      "Iteration 91, loss = 0.20562041\n",
      "Iteration 183, loss = 0.23103674\n",
      "Iteration 291, loss = 0.08140970\n",
      "Iteration 292, loss = 0.08134077\n",
      "Iteration 191, loss = 0.12487783\n",
      "Iteration 177, loss = 0.13494986\n",
      "Iteration 293, loss = 0.18122845\n",
      "Iteration 92, loss = 0.20461130\n",
      "Iteration 21, loss = 0.46158436\n",
      "Iteration 68, loss = 0.22310117\n",
      "Iteration 184, loss = 0.23082808\n",
      "Iteration 178, loss = 0.13471842\n",
      "Iteration 69, loss = 0.22086953\n",
      "Iteration 22, loss = 0.45406034\n",
      "Iteration 70, loss = 0.21882291\n",
      "Iteration 293, loss = 0.08124587\n",
      "Iteration 294, loss = 0.08114594\n",
      "Iteration 295, loss = 0.08106303\n",
      "Iteration 192, loss = 0.12467879\n",
      "Iteration 296, loss = 0.08096717\n",
      "Iteration 193, loss = 0.12448741\n",
      "Iteration 294, loss = 0.18114458\n",
      "Iteration 179, loss = 0.13448261\n",
      "Iteration 194, loss = 0.12428268\n",
      "Iteration 297, loss = 0.08088336\n",
      "Iteration 93, loss = 0.20362779\n",
      "Iteration 298, loss = 0.08078828\n",
      "Iteration 180, loss = 0.13427210\n",
      "Iteration 71, loss = 0.21673714\n",
      "Iteration 72, loss = 0.21479751\n",
      "Iteration 94, loss = 0.20265510\n",
      "Iteration 295, loss = 0.18104071\n",
      "Iteration 299, loss = 0.08070810\n",
      "Iteration 195, loss = 0.12407627\n",
      "Iteration 181, loss = 0.13406285\n",
      "Iteration 185, loss = 0.23062162\n",
      "Iteration 23, loss = 0.44648451\n",
      "Iteration 300, loss = 0.08059453\n",
      "Iteration 196, loss = 0.12388216\n",
      "Iteration 301, loss = 0.08049296\n",
      "Iteration 95, loss = 0.20174428\n",
      "Iteration 73, loss = 0.21291502\n",
      "Iteration 302, loss = 0.08041718\n",
      "Iteration 197, loss = 0.12369538\n",
      "Iteration 74, loss = 0.21109196\n",
      "Iteration 182, loss = 0.13384367\n",
      "Iteration 198, loss = 0.12353270\n",
      "Iteration 296, loss = 0.18096898\n",
      "Iteration 303, loss = 0.08031683\n",
      "Iteration 199, loss = 0.12331033\n",
      "Iteration 186, loss = 0.23037744\n",
      "Iteration 304, loss = 0.08025586\n",
      "Iteration 183, loss = 0.13363351\n",
      "Iteration 305, loss = 0.08013900\n",
      "Iteration 306, loss = 0.08007204\n",
      "Iteration 75, loss = 0.20937755\n",
      "Iteration 24, loss = 0.43891610\n",
      "Iteration 200, loss = 0.12312243\n",
      "Iteration 96, loss = 0.20081924\n",
      "Iteration 297, loss = 0.18088540\n",
      "Iteration 76, loss = 0.20769546\n",
      "Iteration 184, loss = 0.13342425\n",
      "Iteration 307, loss = 0.08000480\n",
      "Iteration 308, loss = 0.07991904\n",
      "Iteration 309, loss = 0.07982995\n",
      "Iteration 185, loss = 0.13320722\n",
      "Iteration 310, loss = 0.07970944\n",
      "Iteration 187, loss = 0.23029100\n",
      "Iteration 25, loss = 0.43154169\n",
      "Iteration 201, loss = 0.12291126\n",
      "Iteration 311, loss = 0.07963659\n",
      "Iteration 77, loss = 0.20609733\n",
      "Iteration 202, loss = 0.12273901\n",
      "Iteration 78, loss = 0.20453390\n",
      "Iteration 97, loss = 0.19994765\n",
      "Iteration 312, loss = 0.07956061\n",
      "Iteration 313, loss = 0.07946536\n",
      "Iteration 186, loss = 0.13300693\n",
      "Iteration 314, loss = 0.07936827\n",
      "Iteration 315, loss = 0.07929141\n",
      "Iteration 298, loss = 0.18082449\n",
      "Iteration 316, loss = 0.07921897Iteration 203, loss = 0.12255138\n",
      "\n",
      "Iteration 26, loss = 0.42441560\n",
      "Iteration 204, loss = 0.12236319\n",
      "Iteration 188, loss = 0.22993774\n",
      "Iteration 79, loss = 0.20302747\n",
      "Iteration 98, loss = 0.19909879\n",
      "Iteration 187, loss = 0.13281058\n",
      "Iteration 317, loss = 0.07913302\n",
      "Iteration 318, loss = 0.07903497\n",
      "Iteration 319, loss = 0.07895691\n",
      "Iteration 80, loss = 0.20159748\n",
      "Iteration 205, loss = 0.12219589\n",
      "Iteration 99, loss = 0.19825251\n",
      "Iteration 81, loss = 0.20018368\n",
      "Iteration 299, loss = 0.18071767\n",
      "Iteration 320, loss = 0.07888361\n",
      "Iteration 321, loss = 0.07883606\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 188, loss = 0.13261988\n",
      "Iteration 82, loss = 0.19885043\n",
      "Iteration 206, loss = 0.12198644\n",
      "Iteration 1, loss = 0.74104180\n",
      "Iteration 2, loss = 0.72003548\n",
      "Iteration 189, loss = 0.13239919\n",
      "Iteration 207, loss = 0.12180181\n",
      "Iteration 100, loss = 0.19743819\n",
      "Iteration 3, loss = 0.69195214\n",
      "Iteration 189, loss = 0.22972804\n",
      "Iteration 208, loss = 0.12165244\n",
      "Iteration 27, loss = 0.41719304\n",
      "Iteration 83, loss = 0.19755545\n",
      "Iteration 4, loss = 0.66293790\n",
      "Iteration 190, loss = 0.13222719\n",
      "Iteration 5, loss = 0.63514304\n",
      "Iteration 28, loss = 0.41012039\n",
      "Iteration 6, loss = 0.61035708\n",
      "Iteration 84, loss = 0.19633382\n",
      "Iteration 209, loss = 0.12148325\n",
      "Iteration 300, loss = 0.18064025\n",
      "Iteration 101, loss = 0.19665579\n",
      "Iteration 210, loss = 0.12128484\n",
      "Iteration 191, loss = 0.13202120\n",
      "Iteration 190, loss = 0.22951576\n",
      "Iteration 7, loss = 0.58774821\n",
      "Iteration 85, loss = 0.19510562\n",
      "Iteration 8, loss = 0.56771618\n",
      "Iteration 211, loss = 0.12115364\n",
      "Iteration 192, loss = 0.13182464\n",
      "Iteration 29, loss = 0.40314633\n",
      "Iteration 193, loss = 0.13162926\n",
      "Iteration 86, loss = 0.19386640\n",
      "Iteration 102, loss = 0.19588285\n",
      "Iteration 191, loss = 0.22931133\n",
      "Iteration 212, loss = 0.12095488\n",
      "Iteration 301, loss = 0.18056330Iteration 87, loss = 0.19272789\n",
      "\n",
      "Iteration 9, loss = 0.55013813\n",
      "Iteration 10, loss = 0.53453247\n",
      "Iteration 30, loss = 0.39635846\n",
      "Iteration 103, loss = 0.19516704\n",
      "Iteration 11, loss = 0.52067801\n",
      "Iteration 213, loss = 0.12076802\n",
      "Iteration 12, loss = 0.50797200\n",
      "Iteration 13, loss = 0.49674800\n",
      "Iteration 302, loss = 0.18046992\n",
      "Iteration 194, loss = 0.13144534\n",
      "Iteration 88, loss = 0.19164405\n",
      "Iteration 214, loss = 0.12062554\n",
      "Iteration 192, loss = 0.22910581\n",
      "Iteration 14, loss = 0.48632768\n",
      "Iteration 215, loss = 0.12042422\n",
      "Iteration 195, loss = 0.13125701\n",
      "Iteration 216, loss = 0.12028357\n",
      "Iteration 15, loss = 0.47738765\n",
      "Iteration 16, loss = 0.46876880\n",
      "Iteration 196, loss = 0.13107511\n",
      "Iteration 104, loss = 0.19441971\n",
      "Iteration 217, loss = 0.12012623\n",
      "Iteration 89, loss = 0.19055400\n",
      "Iteration 303, loss = 0.18037814\n",
      "Iteration 17, loss = 0.46111964\n",
      "Iteration 18, loss = 0.45426921\n",
      "Iteration 90, loss = 0.18949656\n",
      "Iteration 105, loss = 0.19372339\n",
      "Iteration 19, loss = 0.44747905\n",
      "Iteration 218, loss = 0.11996233\n",
      "Iteration 91, loss = 0.18847813\n",
      "Iteration 31, loss = 0.38971115\n",
      "Iteration 20, loss = 0.44198783\n",
      "Iteration 197, loss = 0.13087783\n",
      "Iteration 304, loss = 0.18035431\n",
      "Iteration 219, loss = 0.11978014\n",
      "Iteration 220, loss = 0.11961837\n",
      "Iteration 92, loss = 0.18749369\n",
      "Iteration 193, loss = 0.22902119\n",
      "Iteration 21, loss = 0.43632374\n",
      "Iteration 106, loss = 0.19301426\n",
      "Iteration 22, loss = 0.43169217\n",
      "Iteration 23, loss = 0.42706619\n",
      "Iteration 198, loss = 0.13070183\n",
      "Iteration 24, loss = 0.42321283\n",
      "Iteration 93, loss = 0.18651257\n",
      "Iteration 32, loss = 0.38317149\n",
      "Iteration 25, loss = 0.41946097\n",
      "Iteration 221, loss = 0.11949807\n",
      "Iteration 199, loss = 0.13051808\n",
      "Iteration 94, loss = 0.18554289\n",
      "Iteration 107, loss = 0.19234825\n",
      "Iteration 200, loss = 0.13033235\n",
      "Iteration 194, loss = 0.22878827\n",
      "Iteration 26, loss = 0.41621183\n",
      "Iteration 305, loss = 0.18020013\n",
      "Iteration 27, loss = 0.41318428\n",
      "Iteration 222, loss = 0.11933523\n",
      "Iteration 28, loss = 0.41032911\n",
      "Iteration 223, loss = 0.11913983\n",
      "Iteration 306, loss = 0.18012676\n",
      "Iteration 108, loss = 0.19166931\n",
      "Iteration 95, loss = 0.18465025\n",
      "Iteration 33, loss = 0.37699208\n",
      "Iteration 224, loss = 0.11903493\n",
      "Iteration 29, loss = 0.40781509\n",
      "Iteration 96, loss = 0.18375944\n",
      "Iteration 225, loss = 0.11884593\n",
      "Iteration 201, loss = 0.13017217\n",
      "Iteration 30, loss = 0.40544657\n",
      "Iteration 31, loss = 0.40332273\n",
      "Iteration 195, loss = 0.22857449\n",
      "Iteration 109, loss = 0.19104366Iteration 226, loss = 0.11871766\n",
      "\n",
      "Iteration 202, loss = 0.12997612\n",
      "Iteration 32, loss = 0.40129249\n",
      "Iteration 227, loss = 0.11855641\n",
      "Iteration 33, loss = 0.39933941\n",
      "Iteration 228, loss = 0.11838902\n",
      "Iteration 203, loss = 0.12979427\n",
      "Iteration 307, loss = 0.18009565\n",
      "Iteration 34, loss = 0.39766155\n",
      "Iteration 34, loss = 0.37098173\n",
      "Iteration 97, loss = 0.18297597\n",
      "Iteration 204, loss = 0.12961579\n",
      "Iteration 110, loss = 0.19042260\n",
      "Iteration 35, loss = 0.39598310\n",
      "Iteration 229, loss = 0.11824617\n",
      "Iteration 98, loss = 0.18206321\n",
      "Iteration 36, loss = 0.39425646\n",
      "Iteration 308, loss = 0.17996569\n",
      "Iteration 196, loss = 0.22834658\n",
      "Iteration 230, loss = 0.11808697\n",
      "Iteration 111, loss = 0.18977742\n",
      "Iteration 37, loss = 0.39277549\n",
      "Iteration 38, loss = 0.39130013\n",
      "Iteration 39, loss = 0.38988713\n",
      "Iteration 35, loss = 0.36479194\n",
      "Iteration 99, loss = 0.18123453\n",
      "Iteration 231, loss = 0.11793928\n",
      "Iteration 205, loss = 0.12945592\n",
      "Iteration 40, loss = 0.38842896\n",
      "Iteration 41, loss = 0.38708283\n",
      "Iteration 232, loss = 0.11780922\n",
      "Iteration 42, loss = 0.38571910\n",
      "Iteration 309, loss = 0.17987731\n",
      "Iteration 100, loss = 0.18043222\n",
      "Iteration 112, loss = 0.18917988\n",
      "Iteration 206, loss = 0.12926229\n",
      "Iteration 43, loss = 0.38436913\n",
      "Iteration 197, loss = 0.22812023\n",
      "Iteration 44, loss = 0.38302026\n",
      "Iteration 101, loss = 0.17962301\n",
      "Iteration 233, loss = 0.11767157\n",
      "Iteration 36, loss = 0.35927888\n",
      "Iteration 207, loss = 0.12910081\n",
      "Iteration 45, loss = 0.38171469\n",
      "Iteration 234, loss = 0.11751801\n",
      "Iteration 113, loss = 0.18859557\n",
      "Iteration 46, loss = 0.38031835\n",
      "Iteration 102, loss = 0.17887071\n",
      "Iteration 208, loss = 0.12893019\n",
      "Iteration 198, loss = 0.22790939\n",
      "Iteration 235, loss = 0.11736840\n",
      "Iteration 310, loss = 0.17984635\n",
      "Iteration 47, loss = 0.37900120\n",
      "Iteration 209, loss = 0.12877668\n",
      "Iteration 37, loss = 0.35339843\n",
      "Iteration 114, loss = 0.18801416\n",
      "Iteration 48, loss = 0.37763172\n",
      "Iteration 236, loss = 0.11721978\n",
      "Iteration 49, loss = 0.37623669\n",
      "Iteration 237, loss = 0.11707672\n",
      "Iteration 103, loss = 0.17811588\n",
      "Iteration 50, loss = 0.37483510\n",
      "Iteration 311, loss = 0.17977807\n",
      "Iteration 238, loss = 0.11693639\n",
      "Iteration 38, loss = 0.34786115\n",
      "Iteration 104, loss = 0.17737861\n",
      "Iteration 51, loss = 0.37345723\n",
      "Iteration 115, loss = 0.18744734\n",
      "Iteration 52, loss = 0.37203101\n",
      "Iteration 53, loss = 0.37055570\n",
      "Iteration 54, loss = 0.36905718\n",
      "Iteration 55, loss = 0.36755132\n",
      "Iteration 56, loss = 0.36596173\n",
      "Iteration 239, loss = 0.11682122Iteration 116, loss = 0.18690538\n",
      "Iteration 39, loss = 0.34255722\n",
      "Iteration 210, loss = 0.12859753\n",
      "Iteration 199, loss = 0.22779425\n",
      "\n",
      "Iteration 312, loss = 0.17966278\n",
      "Iteration 57, loss = 0.36436410\n",
      "Iteration 211, loss = 0.12842975\n",
      "Iteration 105, loss = 0.17668444\n",
      "Iteration 240, loss = 0.11668213\n",
      "Iteration 241, loss = 0.11655619\n",
      "Iteration 212, loss = 0.12827225\n",
      "Iteration 106, loss = 0.17599533\n",
      "Iteration 242, loss = 0.11639474\n",
      "Iteration 213, loss = 0.12812205\n",
      "Iteration 200, loss = 0.22765943\n",
      "Iteration 107, loss = 0.17529455\n",
      "Iteration 313, loss = 0.17960938\n",
      "Iteration 58, loss = 0.36267145\n",
      "Iteration 59, loss = 0.36099325\n",
      "Iteration 60, loss = 0.35925131\n",
      "Iteration 61, loss = 0.35749617\n",
      "Iteration 214, loss = 0.12794852\n",
      "Iteration 243, loss = 0.11626800\n",
      "Iteration 62, loss = 0.35564614\n",
      "Iteration 40, loss = 0.33748112\n",
      "Iteration 108, loss = 0.17459657\n",
      "Iteration 63, loss = 0.35377194\n",
      "Iteration 117, loss = 0.18636937\n",
      "Iteration 64, loss = 0.35191864\n",
      "Iteration 118, loss = 0.18588929\n",
      "Iteration 201, loss = 0.22740352\n",
      "Iteration 65, loss = 0.35009208\n",
      "Iteration 66, loss = 0.34813946\n",
      "Iteration 215, loss = 0.12780067\n",
      "Iteration 244, loss = 0.11613699\n",
      "Iteration 314, loss = 0.17949876\n",
      "Iteration 245, loss = 0.11603645\n",
      "Iteration 109, loss = 0.17398385\n",
      "Iteration 216, loss = 0.12764012\n",
      "Iteration 41, loss = 0.33248649\n",
      "Iteration 67, loss = 0.34624040\n",
      "Iteration 119, loss = 0.18531273\n",
      "Iteration 68, loss = 0.34418616\n",
      "Iteration 246, loss = 0.11588452\n",
      "Iteration 69, loss = 0.34224576\n",
      "Iteration 70, loss = 0.34013241\n",
      "Iteration 110, loss = 0.17330961\n",
      "Iteration 71, loss = 0.33805235\n",
      "Iteration 202, loss = 0.22720834\n",
      "Iteration 72, loss = 0.33591775\n",
      "Iteration 73, loss = 0.33381035\n",
      "Iteration 42, loss = 0.32762669\n",
      "Iteration 315, loss = 0.17943904\n",
      "Iteration 74, loss = 0.33159102\n",
      "Iteration 247, loss = 0.11574961\n",
      "Iteration 217, loss = 0.12748236\n",
      "Iteration 248, loss = 0.11560910\n",
      "Iteration 120, loss = 0.18480635\n",
      "Iteration 75, loss = 0.32952957\n",
      "Iteration 111, loss = 0.17267337\n",
      "Iteration 218, loss = 0.12731823\n",
      "Iteration 249, loss = 0.11551543\n",
      "Iteration 43, loss = 0.32299976\n",
      "Iteration 112, loss = 0.17210921\n",
      "Iteration 121, loss = 0.18432302\n",
      "Iteration 219, loss = 0.12717140\n",
      "Iteration 250, loss = 0.11537264\n",
      "Iteration 76, loss = 0.32735521\n",
      "Iteration 316, loss = 0.17934909\n",
      "Iteration 203, loss = 0.22703858\n",
      "Iteration 77, loss = 0.32514656\n",
      "Iteration 220, loss = 0.12702251\n",
      "Iteration 113, loss = 0.17150223\n",
      "Iteration 251, loss = 0.11522245\n",
      "Iteration 44, loss = 0.31858702\n",
      "Iteration 114, loss = 0.17084918\n",
      "Iteration 317, loss = 0.17925468\n",
      "Iteration 78, loss = 0.32297084\n",
      "Iteration 252, loss = 0.11511547\n",
      "Iteration 122, loss = 0.18382699\n",
      "Iteration 221, loss = 0.12686388\n",
      "Iteration 115, loss = 0.17029000\n",
      "Iteration 253, loss = 0.11499270\n",
      "Iteration 79, loss = 0.32079479\n",
      "Iteration 80, loss = 0.31857860\n",
      "Iteration 222, loss = 0.12671821\n",
      "Iteration 81, loss = 0.31641941\n",
      "Iteration 123, loss = 0.18336545\n",
      "Iteration 204, loss = 0.22687848\n",
      "Iteration 45, loss = 0.31424885\n",
      "Iteration 254, loss = 0.11486438\n",
      "Iteration 82, loss = 0.31411471\n",
      "Iteration 83, loss = 0.31187387\n",
      "Iteration 255, loss = 0.11473788\n",
      "Iteration 318, loss = 0.17918971\n",
      "Iteration 256, loss = 0.11464370\n",
      "Iteration 116, loss = 0.16972180\n",
      "Iteration 223, loss = 0.12658600\n",
      "Iteration 224, loss = 0.12642550\n",
      "Iteration 84, loss = 0.30958840\n",
      "Iteration 117, loss = 0.16917065\n",
      "Iteration 124, loss = 0.18287088Iteration 225, loss = 0.12625570\n",
      "\n",
      "Iteration 85, loss = 0.30723126\n",
      "Iteration 46, loss = 0.31011785\n",
      "Iteration 118, loss = 0.16862412\n",
      "Iteration 86, loss = 0.30491299\n",
      "Iteration 119, loss = 0.16809095\n",
      "Iteration 319, loss = 0.17912518\n",
      "Iteration 205, loss = 0.22664248\n",
      "Iteration 257, loss = 0.11448754\n",
      "Iteration 125, loss = 0.18245649\n",
      "Iteration 87, loss = 0.30251783\n",
      "Iteration 88, loss = 0.30006852\n",
      "Iteration 258, loss = 0.11438949\n",
      "Iteration 226, loss = 0.12612733\n",
      "Iteration 320, loss = 0.17903037\n",
      "Iteration 47, loss = 0.30609742\n",
      "Iteration 126, loss = 0.18197846\n",
      "Iteration 89, loss = 0.29774254\n",
      "Iteration 259, loss = 0.11426607\n",
      "Iteration 206, loss = 0.22648276\n",
      "Iteration 227, loss = 0.12600502\n",
      "Iteration 90, loss = 0.29529666\n",
      "Iteration 120, loss = 0.16760918\n",
      "Iteration 91, loss = 0.29284331\n",
      "Iteration 92, loss = 0.29033592\n",
      "Iteration 228, loss = 0.12583992\n",
      "Iteration 121, loss = 0.16706775\n",
      "Iteration 93, loss = 0.28784489\n",
      "Iteration 260, loss = 0.11413837\n",
      "Iteration 48, loss = 0.30225128\n",
      "Iteration 94, loss = 0.28520091\n",
      "Iteration 122, loss = 0.16656470\n",
      "Iteration 261, loss = 0.11403694\n",
      "Iteration 229, loss = 0.12570011\n",
      "Iteration 127, loss = 0.18154049\n",
      "Iteration 95, loss = 0.28264805\n",
      "Iteration 262, loss = 0.11390888\n",
      "Iteration 321, loss = 0.17897298\n",
      "Iteration 96, loss = 0.27994093\n",
      "Iteration 97, loss = 0.27739877\n",
      "Iteration 263, loss = 0.11380405\n",
      "Iteration 264, loss = 0.11368119\n",
      "Iteration 128, loss = 0.18111310\n",
      "Iteration 98, loss = 0.27463985\n",
      "Iteration 230, loss = 0.12557258\n",
      "Iteration 207, loss = 0.22626707\n",
      "Iteration 123, loss = 0.16605221\n",
      "Iteration 99, loss = 0.27204654\n",
      "Iteration 100, loss = 0.26913395\n",
      "Iteration 322, loss = 0.17891459\n",
      "Iteration 49, loss = 0.29859982\n",
      "Iteration 129, loss = 0.18068543\n",
      "Iteration 265, loss = 0.11358736\n",
      "Iteration 101, loss = 0.26637093\n",
      "Iteration 124, loss = 0.16555452\n",
      "Iteration 231, loss = 0.12544014\n",
      "Iteration 102, loss = 0.26358235\n",
      "Iteration 266, loss = 0.11346608\n",
      "Iteration 232, loss = 0.12527650\n",
      "Iteration 103, loss = 0.26077714\n",
      "Iteration 323, loss = 0.17881579\n",
      "Iteration 125, loss = 0.16509598\n",
      "Iteration 104, loss = 0.25790423\n",
      "Iteration 105, loss = 0.25497677\n",
      "Iteration 208, loss = 0.22611577\n",
      "Iteration 106, loss = 0.25206058\n",
      "Iteration 267, loss = 0.11338532\n",
      "Iteration 233, loss = 0.12513673\n",
      "Iteration 130, loss = 0.18026886\n",
      "Iteration 50, loss = 0.29503619\n",
      "Iteration 126, loss = 0.16461958\n",
      "Iteration 268, loss = 0.11322880\n",
      "Iteration 127, loss = 0.16415267\n",
      "Iteration 269, loss = 0.11313306Iteration 324, loss = 0.17872851\n",
      "\n",
      "Iteration 107, loss = 0.24912944\n",
      "Iteration 209, loss = 0.22596178\n",
      "Iteration 131, loss = 0.17986860\n",
      "Iteration 108, loss = 0.24630211\n",
      "Iteration 51, loss = 0.29177922\n",
      "Iteration 234, loss = 0.12499885\n",
      "Iteration 128, loss = 0.16373058\n",
      "Iteration 109, loss = 0.24334820\n",
      "Iteration 235, loss = 0.12485520\n",
      "Iteration 270, loss = 0.11300921\n",
      "Iteration 129, loss = 0.16327256\n",
      "Iteration 110, loss = 0.24046365\n",
      "Iteration 111, loss = 0.23755167\n",
      "Iteration 112, loss = 0.23463907\n",
      "Iteration 130, loss = 0.16285249\n",
      "Iteration 132, loss = 0.17947151\n",
      "Iteration 325, loss = 0.17866529\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 236, loss = 0.12473413\n",
      "Iteration 271, loss = 0.11295241\n",
      "Iteration 272, loss = 0.11279866\n",
      "Iteration 113, loss = 0.23177403\n",
      "Iteration 273, loss = 0.11273699\n",
      "Iteration 114, loss = 0.22894798\n",
      "Iteration 115, loss = 0.22614332\n",
      "Iteration 210, loss = 0.22578586\n",
      "Iteration 274, loss = 0.11258627\n",
      "Iteration 1, loss = 0.79666209\n",
      "Iteration 116, loss = 0.22324826\n",
      "Iteration 131, loss = 0.16241271\n",
      "Iteration 52, loss = 0.28848641\n",
      "Iteration 133, loss = 0.17908769\n",
      "Iteration 237, loss = 0.12463389\n",
      "Iteration 132, loss = 0.16196960\n",
      "Iteration 117, loss = 0.22041639\n",
      "Iteration 118, loss = 0.21761457\n",
      "Iteration 134, loss = 0.17866758\n",
      "Iteration 238, loss = 0.12446425\n",
      "Iteration 119, loss = 0.21483534\n",
      "Iteration 120, loss = 0.21212999\n",
      "Iteration 133, loss = 0.16154370\n",
      "Iteration 121, loss = 0.20952870\n",
      "Iteration 275, loss = 0.11247572\n",
      "Iteration 2, loss = 0.77280877\n",
      "Iteration 53, loss = 0.28539009\n",
      "Iteration 239, loss = 0.12432660\n",
      "Iteration 211, loss = 0.22559096\n",
      "Iteration 135, loss = 0.17826788\n",
      "Iteration 122, loss = 0.20678620\n",
      "Iteration 134, loss = 0.16113685\n",
      "Iteration 276, loss = 0.11236807\n",
      "Iteration 54, loss = 0.28246732\n",
      "Iteration 240, loss = 0.12420491\n",
      "Iteration 3, loss = 0.74085184\n",
      "Iteration 277, loss = 0.11226403\n",
      "Iteration 278, loss = 0.11214829\n",
      "Iteration 123, loss = 0.20404593\n",
      "Iteration 4, loss = 0.70705441\n",
      "Iteration 241, loss = 0.12406700\n",
      "Iteration 136, loss = 0.17792498\n",
      "Iteration 124, loss = 0.20157817\n",
      "Iteration 135, loss = 0.16076492\n",
      "Iteration 125, loss = 0.19910112\n",
      "Iteration 126, loss = 0.19674943\n",
      "Iteration 127, loss = 0.19434407\n",
      "Iteration 5, loss = 0.67528894\n",
      "Iteration 212, loss = 0.22543671\n",
      "Iteration 279, loss = 0.11205750\n",
      "Iteration 280, loss = 0.11194797\n",
      "Iteration 136, loss = 0.16033619\n",
      "Iteration 128, loss = 0.19211024\n",
      "Iteration 242, loss = 0.12395459\n",
      "Iteration 6, loss = 0.64594922\n",
      "Iteration 137, loss = 0.15998067\n",
      "Iteration 281, loss = 0.11184661\n",
      "Iteration 55, loss = 0.27961619\n",
      "Iteration 138, loss = 0.15956776\n",
      "Iteration 137, loss = 0.17752724\n",
      "Iteration 282, loss = 0.11174629\n",
      "Iteration 283, loss = 0.11163862\n",
      "Iteration 129, loss = 0.18981094\n",
      "Iteration 243, loss = 0.12380575\n",
      "Iteration 130, loss = 0.18768343\n",
      "Iteration 284, loss = 0.11153201\n",
      "Iteration 285, loss = 0.11144211\n",
      "Iteration 56, loss = 0.27692480\n",
      "Iteration 213, loss = 0.22530629\n",
      "Iteration 138, loss = 0.17718511\n",
      "Iteration 131, loss = 0.18547546\n",
      "Iteration 7, loss = 0.62061371\n",
      "Iteration 132, loss = 0.18345726\n",
      "Iteration 139, loss = 0.15918299\n",
      "Iteration 133, loss = 0.18152658\n",
      "Iteration 134, loss = 0.17952687\n",
      "Iteration 8, loss = 0.59759748\n",
      "Iteration 140, loss = 0.15881078\n",
      "Iteration 135, loss = 0.17779782\n",
      "Iteration 244, loss = 0.12368926\n",
      "Iteration 136, loss = 0.17587069\n",
      "Iteration 245, loss = 0.12356392\n",
      "Iteration 141, loss = 0.15846575\n",
      "Iteration 246, loss = 0.12342963\n",
      "Iteration 286, loss = 0.11135662\n",
      "Iteration 137, loss = 0.17424194\n",
      "Iteration 139, loss = 0.17680048\n",
      "Iteration 138, loss = 0.17239589\n",
      "Iteration 142, loss = 0.15806850\n",
      "Iteration 287, loss = 0.11123767\n",
      "Iteration 214, loss = 0.22509058\n",
      "Iteration 57, loss = 0.27429485\n",
      "Iteration 9, loss = 0.57734668\n",
      "Iteration 288, loss = 0.11115535\n",
      "Iteration 140, loss = 0.17644576Iteration 289, loss = 0.11105778\n",
      "Iteration 10, loss = 0.55961510\n",
      "Iteration 143, loss = 0.15772820\n",
      "\n",
      "Iteration 139, loss = 0.17075750\n",
      "Iteration 247, loss = 0.12330255\n",
      "Iteration 140, loss = 0.16916956\n",
      "Iteration 141, loss = 0.16762790\n",
      "Iteration 290, loss = 0.11097207\n",
      "Iteration 142, loss = 0.16611495\n",
      "Iteration 58, loss = 0.27187126\n",
      "Iteration 215, loss = 0.22493513\n",
      "Iteration 291, loss = 0.11086080\n",
      "Iteration 143, loss = 0.16463264\n",
      "Iteration 144, loss = 0.15737863\n",
      "Iteration 292, loss = 0.11075985\n",
      "Iteration 144, loss = 0.16325197\n",
      "Iteration 11, loss = 0.54416443\n",
      "Iteration 145, loss = 0.15703595\n",
      "Iteration 293, loss = 0.11067468\n",
      "Iteration 141, loss = 0.17610719\n",
      "Iteration 294, loss = 0.11058721\n",
      "Iteration 12, loss = 0.53023726\n",
      "Iteration 248, loss = 0.12318217\n",
      "Iteration 146, loss = 0.15670695\n",
      "Iteration 249, loss = 0.12309173\n",
      "Iteration 13, loss = 0.51797797\n",
      "Iteration 147, loss = 0.15634092\n",
      "Iteration 145, loss = 0.16183661\n",
      "Iteration 295, loss = 0.11047756\n",
      "Iteration 146, loss = 0.16051925\n",
      "Iteration 147, loss = 0.15921742\n",
      "Iteration 216, loss = 0.22476984\n",
      "Iteration 250, loss = 0.12293672\n",
      "Iteration 148, loss = 0.15597531\n",
      "Iteration 296, loss = 0.11039815\n",
      "Iteration 59, loss = 0.26957853\n",
      "Iteration 148, loss = 0.15792101\n",
      "Iteration 142, loss = 0.17576831\n",
      "Iteration 149, loss = 0.15567558\n",
      "Iteration 251, loss = 0.12281325\n",
      "Iteration 14, loss = 0.50707715\n",
      "Iteration 149, loss = 0.15674195\n",
      "Iteration 143, loss = 0.17543772\n",
      "Iteration 217, loss = 0.22465744\n",
      "Iteration 150, loss = 0.15550592\n",
      "Iteration 297, loss = 0.11029313\n",
      "Iteration 15, loss = 0.49742204\n",
      "Iteration 151, loss = 0.15435110\n",
      "Iteration 252, loss = 0.12269430\n",
      "Iteration 60, loss = 0.26728992\n",
      "Iteration 150, loss = 0.15532311\n",
      "Iteration 298, loss = 0.11020674\n",
      "Iteration 151, loss = 0.15503795\n",
      "Iteration 253, loss = 0.12257777\n",
      "Iteration 152, loss = 0.15319907\n",
      "Iteration 299, loss = 0.11011820\n",
      "Iteration 16, loss = 0.48893892\n",
      "Iteration 153, loss = 0.15207827\n",
      "Iteration 144, loss = 0.17506628\n",
      "Iteration 154, loss = 0.15098804\n",
      "Iteration 300, loss = 0.11004117\n",
      "Iteration 254, loss = 0.12245700\n",
      "Iteration 152, loss = 0.15469967\n",
      "Iteration 61, loss = 0.26525154\n",
      "Iteration 17, loss = 0.48111270\n",
      "Iteration 301, loss = 0.10994651\n",
      "Iteration 155, loss = 0.14994778\n",
      "Iteration 153, loss = 0.15436783\n",
      "Iteration 218, loss = 0.22444335\n",
      "Iteration 154, loss = 0.15407714\n",
      "Iteration 155, loss = 0.15371701\n",
      "Iteration 156, loss = 0.14888646\n",
      "Iteration 302, loss = 0.10984735\n",
      "Iteration 145, loss = 0.17473100\n",
      "Iteration 255, loss = 0.12235539\n",
      "Iteration 156, loss = 0.15342934\n",
      "Iteration 18, loss = 0.47423782\n",
      "Iteration 157, loss = 0.14787752\n",
      "Iteration 62, loss = 0.26314155\n",
      "Iteration 19, loss = 0.46816818\n",
      "Iteration 256, loss = 0.12221761\n",
      "Iteration 158, loss = 0.14692015\n",
      "Iteration 159, loss = 0.14592877\n",
      "Iteration 303, loss = 0.10977165\n",
      "Iteration 146, loss = 0.17439335\n",
      "Iteration 63, loss = 0.26119489\n",
      "Iteration 304, loss = 0.10968485\n",
      "Iteration 219, loss = 0.22429828\n",
      "Iteration 160, loss = 0.14506543\n",
      "Iteration 161, loss = 0.14414310\n",
      "Iteration 147, loss = 0.17408629\n",
      "Iteration 305, loss = 0.10959221\n",
      "Iteration 20, loss = 0.46253749\n",
      "Iteration 162, loss = 0.14325586\n",
      "Iteration 257, loss = 0.12210187\n",
      "Iteration 157, loss = 0.15312091\n",
      "Iteration 258, loss = 0.12198942\n",
      "Iteration 259, loss = 0.12185856\n",
      "Iteration 163, loss = 0.14241554Iteration 21, loss = 0.45729574\n",
      "\n",
      "Iteration 148, loss = 0.17378686\n",
      "Iteration 164, loss = 0.14156909\n",
      "Iteration 306, loss = 0.10951775\n",
      "Iteration 165, loss = 0.14075236\n",
      "Iteration 64, loss = 0.25929433\n",
      "Iteration 220, loss = 0.22415264\n",
      "Iteration 166, loss = 0.13994993\n",
      "Iteration 158, loss = 0.15283037\n",
      "Iteration 167, loss = 0.13920808\n",
      "Iteration 307, loss = 0.10943292\n",
      "Iteration 308, loss = 0.10935401\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 260, loss = 0.12175221\n",
      "Iteration 261, loss = 0.12163337\n",
      "Iteration 262, loss = 0.12153134\n",
      "Iteration 159, loss = 0.15253768\n",
      "Iteration 263, loss = 0.12140562\n",
      "Iteration 160, loss = 0.15223256\n",
      "Iteration 168, loss = 0.13841485\n",
      "Iteration 22, loss = 0.45251297\n",
      "Iteration 149, loss = 0.17343803\n",
      "Iteration 169, loss = 0.13766778\n",
      "Iteration 150, loss = 0.17315901\n",
      "Iteration 65, loss = 0.25751091\n",
      "Iteration 221, loss = 0.22397586\n",
      "Iteration 170, loss = 0.13703258\n",
      "Iteration 23, loss = 0.44802697\n",
      "Iteration 161, loss = 0.15197860\n",
      "Iteration 171, loss = 0.13628615\n",
      "Iteration 162, loss = 0.15168000\n",
      "Iteration 151, loss = 0.17281926\n",
      "Iteration 172, loss = 0.13556090\n",
      "Iteration 222, loss = 0.22386517\n",
      "Iteration 66, loss = 0.25576479\n",
      "Iteration 173, loss = 0.13492390\n",
      "Iteration 174, loss = 0.13425998\n",
      "Iteration 175, loss = 0.13364440\n",
      "Iteration 24, loss = 0.44366913\n",
      "Iteration 1, loss = 0.97408302\n",
      "Iteration 264, loss = 0.12131628\n",
      "Iteration 176, loss = 0.13298899\n",
      "Iteration 265, loss = 0.12118801\n",
      "Iteration 25, loss = 0.43965714\n",
      "Iteration 163, loss = 0.15137887\n",
      "Iteration 67, loss = 0.25415341\n",
      "Iteration 152, loss = 0.17253268\n",
      "Iteration 177, loss = 0.13236307\n",
      "Iteration 164, loss = 0.15113202\n",
      "Iteration 178, loss = 0.13174731\n",
      "Iteration 179, loss = 0.13120766\n",
      "Iteration 180, loss = 0.13059530\n",
      "Iteration 153, loss = 0.17222316\n",
      "Iteration 165, loss = 0.15083642\n",
      "Iteration 26, loss = 0.43576939\n",
      "Iteration 223, loss = 0.22368248\n",
      "Iteration 2, loss = 0.93423913\n",
      "Iteration 266, loss = 0.12106916\n",
      "Iteration 181, loss = 0.13001753\n",
      "Iteration 68, loss = 0.25258519\n",
      "Iteration 166, loss = 0.15059714\n",
      "Iteration 27, loss = 0.43194848\n",
      "Iteration 182, loss = 0.12947708\n",
      "Iteration 167, loss = 0.15028235\n",
      "Iteration 183, loss = 0.12893699\n",
      "Iteration 267, loss = 0.12095063\n",
      "Iteration 154, loss = 0.17194080\n",
      "Iteration 184, loss = 0.12840356\n",
      "Iteration 185, loss = 0.12784987\n",
      "Iteration 186, loss = 0.12730590\n",
      "Iteration 3, loss = 0.88393780\n",
      "Iteration 224, loss = 0.22353226\n",
      "Iteration 168, loss = 0.15000759\n",
      "Iteration 187, loss = 0.12686841\n",
      "Iteration 268, loss = 0.12085357\n",
      "Iteration 28, loss = 0.42832709\n",
      "Iteration 155, loss = 0.17163454\n",
      "Iteration 69, loss = 0.25096744\n",
      "Iteration 169, loss = 0.14979424\n",
      "Iteration 269, loss = 0.12073162\n",
      "Iteration 29, loss = 0.42464635\n",
      "Iteration 188, loss = 0.12627127\n",
      "Iteration 189, loss = 0.12582650\n",
      "Iteration 270, loss = 0.12063201\n",
      "Iteration 170, loss = 0.14947839\n",
      "Iteration 225, loss = 0.22345677\n",
      "Iteration 70, loss = 0.24950249\n",
      "Iteration 156, loss = 0.17135617\n",
      "Iteration 190, loss = 0.12530368\n",
      "Iteration 171, loss = 0.14922132\n",
      "Iteration 191, loss = 0.12481531\n",
      "Iteration 4, loss = 0.83444482\n",
      "Iteration 30, loss = 0.42113561\n",
      "Iteration 271, loss = 0.12052828\n",
      "Iteration 192, loss = 0.12433742\n",
      "Iteration 193, loss = 0.12390083\n",
      "Iteration 194, loss = 0.12341675\n",
      "Iteration 172, loss = 0.14896326\n",
      "Iteration 31, loss = 0.41769922\n",
      "Iteration 71, loss = 0.24805486\n",
      "Iteration 272, loss = 0.12040795\n",
      "Iteration 226, loss = 0.22323642\n",
      "Iteration 195, loss = 0.12299604\n",
      "Iteration 273, loss = 0.12031085\n",
      "Iteration 196, loss = 0.12253320\n",
      "Iteration 32, loss = 0.41423594\n",
      "Iteration 173, loss = 0.14872005\n",
      "Iteration 174, loss = 0.14845761\n",
      "Iteration 197, loss = 0.12214901\n",
      "Iteration 227, loss = 0.22308208\n",
      "Iteration 274, loss = 0.12019120\n",
      "Iteration 198, loss = 0.12168829\n",
      "Iteration 157, loss = 0.17107383\n",
      "Iteration 175, loss = 0.14819851\n",
      "Iteration 5, loss = 0.79042978\n",
      "Iteration 72, loss = 0.24671212\n",
      "Iteration 33, loss = 0.41082095\n",
      "Iteration 199, loss = 0.12125757\n",
      "Iteration 200, loss = 0.12090543\n",
      "Iteration 275, loss = 0.12009143\n",
      "Iteration 201, loss = 0.12058271\n",
      "Iteration 158, loss = 0.17079096\n",
      "Iteration 176, loss = 0.14795167\n",
      "Iteration 6, loss = 0.75223547\n",
      "Iteration 73, loss = 0.24543914\n",
      "Iteration 202, loss = 0.12011156\n",
      "Iteration 177, loss = 0.14768694\n",
      "Iteration 203, loss = 0.11970303\n",
      "Iteration 159, loss = 0.17052850\n",
      "Iteration 34, loss = 0.40752162\n",
      "Iteration 204, loss = 0.11931370\n",
      "Iteration 228, loss = 0.22295711\n",
      "Iteration 276, loss = 0.11998616\n",
      "Iteration 205, loss = 0.11898286\n",
      "Iteration 178, loss = 0.14743525\n",
      "Iteration 277, loss = 0.11989486\n",
      "Iteration 206, loss = 0.11858573\n",
      "Iteration 278, loss = 0.11978285\n",
      "Iteration 207, loss = 0.11819310\n",
      "Iteration 74, loss = 0.24411932\n",
      "Iteration 229, loss = 0.22277688\n",
      "Iteration 35, loss = 0.40431532\n",
      "Iteration 36, loss = 0.40092548\n",
      "Iteration 160, loss = 0.17025355\n",
      "Iteration 161, loss = 0.16998480\n",
      "Iteration 208, loss = 0.11787495\n",
      "Iteration 7, loss = 0.71950764\n",
      "Iteration 179, loss = 0.14720085\n",
      "Iteration 279, loss = 0.11968227\n",
      "Iteration 37, loss = 0.39773633\n",
      "Iteration 209, loss = 0.11752156\n",
      "Iteration 180, loss = 0.14697289\n",
      "Iteration 210, loss = 0.11718192\n",
      "Iteration 181, loss = 0.14673848\n",
      "Iteration 162, loss = 0.16971671\n",
      "Iteration 211, loss = 0.11690860\n",
      "Iteration 230, loss = 0.22264775\n",
      "Iteration 280, loss = 0.11957648\n",
      "Iteration 212, loss = 0.11653477\n",
      "Iteration 213, loss = 0.11630575\n",
      "Iteration 38, loss = 0.39456853\n",
      "Iteration 8, loss = 0.69146179\n",
      "Iteration 75, loss = 0.24282852\n",
      "Iteration 281, loss = 0.11947895\n",
      "Iteration 182, loss = 0.14648684\n",
      "Iteration 163, loss = 0.16945453\n",
      "Iteration 282, loss = 0.11937305\n",
      "Iteration 183, loss = 0.14626518\n",
      "Iteration 214, loss = 0.11588289\n",
      "Iteration 39, loss = 0.39134035\n",
      "Iteration 215, loss = 0.11554021\n",
      "Iteration 216, loss = 0.11526727\n",
      "Iteration 217, loss = 0.11494534\n",
      "Iteration 231, loss = 0.22251112\n",
      "Iteration 218, loss = 0.11463568\n",
      "Iteration 164, loss = 0.16919591\n",
      "Iteration 283, loss = 0.11926957\n",
      "Iteration 219, loss = 0.11432609\n",
      "Iteration 184, loss = 0.14604956\n",
      "Iteration 76, loss = 0.24162005\n",
      "Iteration 40, loss = 0.38816024\n",
      "Iteration 9, loss = 0.66679068\n",
      "Iteration 41, loss = 0.38504043\n",
      "Iteration 185, loss = 0.14578405\n",
      "Iteration 77, loss = 0.24050554\n",
      "Iteration 186, loss = 0.14563465\n",
      "Iteration 165, loss = 0.16892700\n",
      "Iteration 284, loss = 0.11915809\n",
      "Iteration 232, loss = 0.22240582\n",
      "Iteration 220, loss = 0.11401717\n",
      "Iteration 221, loss = 0.11374682\n",
      "Iteration 187, loss = 0.14534336\n",
      "Iteration 222, loss = 0.11345105\n",
      "Iteration 223, loss = 0.11319739\n",
      "Iteration 42, loss = 0.38190505\n",
      "Iteration 285, loss = 0.11908026\n",
      "Iteration 224, loss = 0.11295684\n",
      "Iteration 225, loss = 0.11260480\n",
      "Iteration 10, loss = 0.64558653\n",
      "Iteration 43, loss = 0.37865654\n",
      "Iteration 188, loss = 0.14520046\n",
      "Iteration 78, loss = 0.23931735\n",
      "Iteration 286, loss = 0.11897124\n",
      "Iteration 189, loss = 0.14488116\n",
      "Iteration 166, loss = 0.16869006\n",
      "Iteration 233, loss = 0.22228823\n",
      "Iteration 226, loss = 0.11235463\n",
      "Iteration 227, loss = 0.11205948\n",
      "Iteration 287, loss = 0.11885987\n",
      "Iteration 44, loss = 0.37556571\n",
      "Iteration 228, loss = 0.11178571\n",
      "Iteration 229, loss = 0.11158782\n",
      "Iteration 45, loss = 0.37245558\n",
      "Iteration 190, loss = 0.14468735\n",
      "Iteration 167, loss = 0.16843809\n",
      "Iteration 230, loss = 0.11125859\n",
      "Iteration 11, loss = 0.62732161\n",
      "Iteration 231, loss = 0.11100215\n",
      "Iteration 288, loss = 0.11878131\n",
      "Iteration 79, loss = 0.23819824\n",
      "Iteration 46, loss = 0.36929405\n",
      "Iteration 289, loss = 0.11867253\n",
      "Iteration 191, loss = 0.14448143\n",
      "Iteration 192, loss = 0.14426257\n",
      "Iteration 232, loss = 0.11075958\n",
      "Iteration 168, loss = 0.16818005\n",
      "Iteration 233, loss = 0.11052509\n",
      "Iteration 234, loss = 0.22214699\n",
      "Iteration 234, loss = 0.11025397Iteration 47, loss = 0.36623222\n",
      "Iteration 193, loss = 0.14403933\n",
      "Iteration 80, loss = 0.23719522\n",
      "\n",
      "Iteration 48, loss = 0.36316061\n",
      "Iteration 169, loss = 0.16794264\n",
      "Iteration 235, loss = 0.11000170\n",
      "Iteration 12, loss = 0.61199521\n",
      "Iteration 194, loss = 0.14383724\n",
      "Iteration 170, loss = 0.16768383\n",
      "Iteration 290, loss = 0.11857901\n",
      "Iteration 236, loss = 0.10977305\n",
      "Iteration 237, loss = 0.10954976\n",
      "Iteration 81, loss = 0.23610895\n",
      "Iteration 235, loss = 0.22202646\n",
      "Iteration 291, loss = 0.11848254\n",
      "Iteration 238, loss = 0.10929449\n",
      "Iteration 13, loss = 0.59764019\n",
      "Iteration 195, loss = 0.14367793\n",
      "Iteration 49, loss = 0.36012812\n",
      "Iteration 239, loss = 0.10908182\n",
      "Iteration 50, loss = 0.35710521\n",
      "Iteration 240, loss = 0.10881571\n",
      "Iteration 196, loss = 0.14343679\n",
      "Iteration 171, loss = 0.16745339\n",
      "Iteration 241, loss = 0.10865160\n",
      "Iteration 236, loss = 0.22183290\n",
      "Iteration 242, loss = 0.10842813\n",
      "Iteration 197, loss = 0.14321842\n",
      "Iteration 292, loss = 0.11838882\n",
      "Iteration 243, loss = 0.10816676\n",
      "Iteration 244, loss = 0.10796881\n",
      "Iteration 51, loss = 0.35409200\n",
      "Iteration 198, loss = 0.14304454\n",
      "Iteration 82, loss = 0.23512212\n",
      "Iteration 293, loss = 0.11829300\n",
      "Iteration 245, loss = 0.10775120\n",
      "Iteration 246, loss = 0.10754102\n",
      "Iteration 172, loss = 0.16722176\n",
      "Iteration 14, loss = 0.58580558\n",
      "Iteration 247, loss = 0.10728792\n",
      "Iteration 52, loss = 0.35112277\n",
      "Iteration 294, loss = 0.11820610\n",
      "Iteration 199, loss = 0.14282709\n",
      "Iteration 173, loss = 0.16697426\n",
      "Iteration 237, loss = 0.22173604\n",
      "Iteration 248, loss = 0.10710895\n",
      "Iteration 295, loss = 0.11810620\n",
      "Iteration 83, loss = 0.23416609\n",
      "Iteration 249, loss = 0.10693359\n",
      "Iteration 200, loss = 0.14263332\n",
      "Iteration 53, loss = 0.34812719\n",
      "Iteration 250, loss = 0.10667820\n",
      "Iteration 15, loss = 0.57481640\n",
      "Iteration 174, loss = 0.16674550\n",
      "Iteration 251, loss = 0.10653932\n",
      "Iteration 54, loss = 0.34527516\n",
      "Iteration 84, loss = 0.23321081\n",
      "Iteration 201, loss = 0.14242162\n",
      "Iteration 252, loss = 0.10628761\n",
      "Iteration 296, loss = 0.11802994\n",
      "Iteration 253, loss = 0.10609376\n",
      "Iteration 297, loss = 0.11796764\n",
      "Iteration 254, loss = 0.10588259\n",
      "Iteration 238, loss = 0.22155475\n",
      "Iteration 55, loss = 0.34236344\n",
      "Iteration 202, loss = 0.14223929\n",
      "Iteration 175, loss = 0.16650746\n",
      "Iteration 255, loss = 0.10573759\n",
      "Iteration 203, loss = 0.14207901\n",
      "Iteration 298, loss = 0.11782902\n",
      "Iteration 256, loss = 0.10553331Iteration 85, loss = 0.23230259\n",
      "\n",
      "Iteration 16, loss = 0.56580845\n",
      "Iteration 257, loss = 0.10529914\n",
      "Iteration 176, loss = 0.16629756\n",
      "Iteration 299, loss = 0.11772561\n",
      "Iteration 258, loss = 0.10512886\n",
      "Iteration 56, loss = 0.33944130\n",
      "Iteration 259, loss = 0.10493126\n",
      "Iteration 204, loss = 0.14188553\n",
      "Iteration 239, loss = 0.22141671\n",
      "Iteration 57, loss = 0.33661508\n",
      "Iteration 177, loss = 0.16605631\n",
      "Iteration 260, loss = 0.10473206\n",
      "Iteration 205, loss = 0.14168213\n",
      "Iteration 300, loss = 0.11764956\n",
      "Iteration 261, loss = 0.10462349\n",
      "Iteration 262, loss = 0.10438467\n",
      "Iteration 301, loss = 0.11754796\n",
      "Iteration 86, loss = 0.23141862\n",
      "Iteration 263, loss = 0.10421682\n",
      "Iteration 17, loss = 0.55685316\n",
      "Iteration 206, loss = 0.14150300\n",
      "Iteration 264, loss = 0.10401134\n",
      "Iteration 265, loss = 0.10383321\n",
      "Iteration 302, loss = 0.11746455\n",
      "Iteration 58, loss = 0.33377307\n",
      "Iteration 178, loss = 0.16584228\n",
      "Iteration 266, loss = 0.10364501\n",
      "Iteration 303, loss = 0.11738102\n",
      "Iteration 240, loss = 0.22133897\n",
      "Iteration 207, loss = 0.14131321\n",
      "Iteration 267, loss = 0.10346469\n",
      "Iteration 59, loss = 0.33110780\n",
      "Iteration 268, loss = 0.10333756\n",
      "Iteration 87, loss = 0.23054408\n",
      "Iteration 60, loss = 0.32823400\n",
      "Iteration 179, loss = 0.16563089\n",
      "Iteration 269, loss = 0.10311834\n",
      "Iteration 208, loss = 0.14117495\n",
      "Iteration 304, loss = 0.11728068\n",
      "Iteration 61, loss = 0.32551801\n",
      "Iteration 88, loss = 0.22971193\n",
      "Iteration 241, loss = 0.22119123\n",
      "Iteration 209, loss = 0.14095131\n",
      "Iteration 270, loss = 0.10297935\n",
      "Iteration 18, loss = 0.54919948\n",
      "Iteration 271, loss = 0.10278555\n",
      "Iteration 305, loss = 0.11719692\n",
      "Iteration 272, loss = 0.10261492\n",
      "Iteration 180, loss = 0.16540602\n",
      "Iteration 62, loss = 0.32287852\n",
      "Iteration 210, loss = 0.14077516\n",
      "Iteration 63, loss = 0.32015367\n",
      "Iteration 306, loss = 0.11710146\n",
      "Iteration 273, loss = 0.10244397\n",
      "Iteration 181, loss = 0.16520412\n",
      "Iteration 89, loss = 0.22893805\n",
      "Iteration 274, loss = 0.10229415\n",
      "Iteration 19, loss = 0.54179678\n",
      "Iteration 242, loss = 0.22104714\n",
      "Iteration 211, loss = 0.14058956\n",
      "Iteration 275, loss = 0.10212682\n",
      "Iteration 182, loss = 0.16499715\n",
      "Iteration 64, loss = 0.31756149\n",
      "Iteration 276, loss = 0.10195421\n",
      "Iteration 307, loss = 0.11701418\n",
      "Iteration 277, loss = 0.10181417\n",
      "Iteration 212, loss = 0.14043509\n",
      "Iteration 308, loss = 0.11692142\n",
      "Iteration 65, loss = 0.31504393\n",
      "Iteration 278, loss = 0.10163166\n",
      "Iteration 90, loss = 0.22810291\n",
      "Iteration 20, loss = 0.53500139\n",
      "Iteration 183, loss = 0.16477759\n",
      "Iteration 279, loss = 0.10148008\n",
      "Iteration 280, loss = 0.10134283\n",
      "Iteration 309, loss = 0.11683595\n",
      "Iteration 281, loss = 0.10114776\n",
      "Iteration 243, loss = 0.22092917\n",
      "Iteration 282, loss = 0.10106758\n",
      "Iteration 213, loss = 0.14025863\n",
      "Iteration 91, loss = 0.22733930\n",
      "Iteration 66, loss = 0.31248004\n",
      "Iteration 214, loss = 0.14008992\n",
      "Iteration 310, loss = 0.11676995\n",
      "Iteration 215, loss = 0.13990888\n",
      "Iteration 184, loss = 0.16455316\n",
      "Iteration 283, loss = 0.10084773\n",
      "Iteration 67, loss = 0.30998221\n",
      "Iteration 284, loss = 0.10067312\n",
      "Iteration 311, loss = 0.11666648\n",
      "Iteration 21, loss = 0.52830102\n",
      "Iteration 285, loss = 0.10051224\n",
      "Iteration 92, loss = 0.22658800\n",
      "Iteration 216, loss = 0.13973485\n",
      "Iteration 244, loss = 0.22083795\n",
      "Iteration 286, loss = 0.10039017\n",
      "Iteration 287, loss = 0.10021299\n",
      "Iteration 68, loss = 0.30747142\n",
      "Iteration 312, loss = 0.11657956\n",
      "Iteration 185, loss = 0.16436773\n",
      "Iteration 217, loss = 0.13957395\n",
      "Iteration 22, loss = 0.52187801\n",
      "Iteration 313, loss = 0.11649405\n",
      "Iteration 218, loss = 0.13938678\n",
      "Iteration 288, loss = 0.10007762\n",
      "Iteration 93, loss = 0.22584754\n",
      "Iteration 289, loss = 0.09987914\n",
      "Iteration 245, loss = 0.22066291\n",
      "Iteration 69, loss = 0.30499921\n",
      "Iteration 290, loss = 0.09976071\n",
      "Iteration 186, loss = 0.16416188\n",
      "Iteration 219, loss = 0.13925807\n",
      "Iteration 314, loss = 0.11642108\n",
      "Iteration 291, loss = 0.09969950\n",
      "Iteration 315, loss = 0.11633211\n",
      "Iteration 292, loss = 0.09947130\n",
      "Iteration 220, loss = 0.13906164\n",
      "Iteration 70, loss = 0.30254605\n",
      "Iteration 187, loss = 0.16394552\n",
      "Iteration 71, loss = 0.30014884\n",
      "Iteration 246, loss = 0.22061584\n",
      "Iteration 94, loss = 0.22513218\n",
      "Iteration 293, loss = 0.09934466\n",
      "Iteration 294, loss = 0.09918677\n",
      "Iteration 295, loss = 0.09901670\n",
      "Iteration 221, loss = 0.13890029\n",
      "Iteration 296, loss = 0.09890778\n",
      "Iteration 23, loss = 0.51556513\n",
      "Iteration 316, loss = 0.11624373\n",
      "Iteration 188, loss = 0.16373531\n",
      "Iteration 297, loss = 0.09872827\n",
      "Iteration 222, loss = 0.13873581\n",
      "Iteration 72, loss = 0.29770252\n",
      "Iteration 317, loss = 0.11616118\n",
      "Iteration 298, loss = 0.09859768\n",
      "Iteration 95, loss = 0.22442242\n",
      "Iteration 223, loss = 0.13858155\n",
      "Iteration 247, loss = 0.22045556\n",
      "Iteration 318, loss = 0.11609392\n",
      "Iteration 299, loss = 0.09842321\n",
      "Iteration 224, loss = 0.13843648\n",
      "Iteration 225, loss = 0.13827486\n",
      "Iteration 189, loss = 0.16354872\n",
      "Iteration 226, loss = 0.13809462\n",
      "Iteration 73, loss = 0.29538701\n",
      "Iteration 300, loss = 0.09827222\n",
      "Iteration 24, loss = 0.50919340\n",
      "Iteration 301, loss = 0.09818208\n",
      "Iteration 96, loss = 0.22378406\n",
      "Iteration 190, loss = 0.16335309\n",
      "Iteration 97, loss = 0.22307436\n",
      "Iteration 319, loss = 0.11599525\n",
      "Iteration 302, loss = 0.09800751\n",
      "Iteration 74, loss = 0.29301631\n",
      "Iteration 303, loss = 0.09787129\n",
      "Iteration 320, loss = 0.11592121\n",
      "Iteration 248, loss = 0.22034688\n",
      "Iteration 304, loss = 0.09771520\n",
      "Iteration 191, loss = 0.16314814\n",
      "Iteration 75, loss = 0.29073624\n",
      "Iteration 305, loss = 0.09767441\n",
      "Iteration 321, loss = 0.11585479\n",
      "Iteration 25, loss = 0.50295557\n",
      "Iteration 227, loss = 0.13795785\n",
      "Iteration 76, loss = 0.28843426\n",
      "Iteration 228, loss = 0.13778674\n",
      "Iteration 306, loss = 0.09743040\n",
      "Iteration 322, loss = 0.11576087\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 249, loss = 0.22017944\n",
      "Iteration 229, loss = 0.13761451\n",
      "Iteration 77, loss = 0.28611381\n",
      "Iteration 230, loss = 0.13747871\n",
      "Iteration 307, loss = 0.09730862\n",
      "Iteration 98, loss = 0.22246096\n",
      "Iteration 308, loss = 0.09719353\n",
      "Iteration 192, loss = 0.16298402\n",
      "Iteration 309, loss = 0.09705225\n",
      "Iteration 310, loss = 0.09691921\n",
      "Iteration 311, loss = 0.09677727\n",
      "Iteration 312, loss = 0.09665411\n",
      "Iteration 26, loss = 0.49678168\n",
      "Iteration 313, loss = 0.09650118\n",
      "Iteration 78, loss = 0.28389916\n",
      "Iteration 99, loss = 0.22181682\n",
      "Iteration 231, loss = 0.13733286\n",
      "Iteration 314, loss = 0.09638893\n",
      "Iteration 193, loss = 0.16277351\n",
      "Iteration 315, loss = 0.09623825\n",
      "Iteration 250, loss = 0.22008867\n",
      "Iteration 79, loss = 0.28165336\n",
      "Iteration 232, loss = 0.13717133\n",
      "Iteration 27, loss = 0.49055866\n",
      "Iteration 233, loss = 0.13704166\n",
      "Iteration 80, loss = 0.27944106Iteration 194, loss = 0.16256883\n",
      "\n",
      "Iteration 234, loss = 0.13692921\n",
      "Iteration 100, loss = 0.22119564\n",
      "Iteration 316, loss = 0.09613982\n",
      "Iteration 81, loss = 0.27725417\n",
      "Iteration 317, loss = 0.09600517\n",
      "Iteration 318, loss = 0.09585995\n",
      "Iteration 28, loss = 0.48467748\n",
      "Iteration 195, loss = 0.16237101\n",
      "Iteration 251, loss = 0.21995387\n",
      "Iteration 235, loss = 0.13672410\n",
      "Iteration 319, loss = 0.09575927\n",
      "Iteration 320, loss = 0.09561386\n",
      "Iteration 321, loss = 0.09547365\n",
      "Iteration 101, loss = 0.22058569\n",
      "Iteration 82, loss = 0.27508851\n",
      "Iteration 29, loss = 0.47829109\n",
      "Iteration 83, loss = 0.27299112\n",
      "Iteration 322, loss = 0.09538761\n",
      "Iteration 323, loss = 0.09524510\n",
      "Iteration 196, loss = 0.16221096\n",
      "Iteration 84, loss = 0.27085714\n",
      "Iteration 236, loss = 0.13663516\n",
      "Iteration 252, loss = 0.21984426\n",
      "Iteration 237, loss = 0.13642735\n",
      "Iteration 102, loss = 0.21999644\n",
      "Iteration 324, loss = 0.09513164\n",
      "Iteration 197, loss = 0.16201704\n",
      "Iteration 325, loss = 0.09500533\n",
      "Iteration 326, loss = 0.09489295\n",
      "Iteration 327, loss = 0.09474093\n",
      "Iteration 238, loss = 0.13631932\n",
      "Iteration 85, loss = 0.26878068\n",
      "Iteration 239, loss = 0.13615160\n",
      "Iteration 198, loss = 0.16182883\n",
      "Iteration 30, loss = 0.47221456\n",
      "Iteration 103, loss = 0.21940457\n",
      "Iteration 86, loss = 0.26662479\n",
      "Iteration 328, loss = 0.09468453\n",
      "Iteration 329, loss = 0.09451299\n",
      "Iteration 87, loss = 0.26461628\n",
      "Iteration 240, loss = 0.13600003\n",
      "Iteration 330, loss = 0.09441820\n",
      "Iteration 253, loss = 0.21970788\n",
      "Iteration 331, loss = 0.09431318\n",
      "Iteration 241, loss = 0.13591471\n",
      "Iteration 332, loss = 0.09418322\n",
      "Iteration 104, loss = 0.21891325\n",
      "Iteration 199, loss = 0.16165755\n",
      "Iteration 242, loss = 0.13578335\n",
      "Iteration 31, loss = 0.46612410\n",
      "Iteration 88, loss = 0.26256135\n",
      "Iteration 243, loss = 0.13559337\n",
      "Iteration 333, loss = 0.09407567\n",
      "Iteration 334, loss = 0.09398154Iteration 244, loss = 0.13549725\n",
      "\n",
      "Iteration 245, loss = 0.13532218\n",
      "Iteration 200, loss = 0.16147144\n",
      "Iteration 254, loss = 0.21963176\n",
      "Iteration 335, loss = 0.09382351\n",
      "Iteration 89, loss = 0.26043102\n",
      "Iteration 336, loss = 0.09372072\n",
      "Iteration 105, loss = 0.21832531\n",
      "Iteration 337, loss = 0.09359208\n",
      "Iteration 90, loss = 0.25850483\n",
      "Iteration 32, loss = 0.45999440\n",
      "Iteration 246, loss = 0.13527090\n",
      "Iteration 338, loss = 0.09347567\n",
      "Iteration 339, loss = 0.09339906\n",
      "Iteration 340, loss = 0.09328245\n",
      "Iteration 201, loss = 0.16125885\n",
      "Iteration 106, loss = 0.21777833\n",
      "Iteration 255, loss = 0.21947548\n",
      "Iteration 341, loss = 0.09319220\n",
      "Iteration 202, loss = 0.16110452\n",
      "Iteration 256, loss = 0.21934494\n",
      "Iteration 91, loss = 0.25654087\n",
      "Iteration 342, loss = 0.09309785\n",
      "Iteration 247, loss = 0.13506856\n",
      "Iteration 343, loss = 0.09298779\n",
      "Iteration 344, loss = 0.09287567\n",
      "Iteration 33, loss = 0.45394383\n",
      "Iteration 345, loss = 0.09276615\n",
      "Iteration 92, loss = 0.25456516\n",
      "Iteration 107, loss = 0.21723831\n",
      "Iteration 346, loss = 0.09265580\n",
      "Iteration 347, loss = 0.09258425\n",
      "Iteration 348, loss = 0.09248288\n",
      "Iteration 203, loss = 0.16092377\n",
      "Iteration 248, loss = 0.13493545\n",
      "Iteration 93, loss = 0.25260392\n",
      "Iteration 249, loss = 0.13481154\n",
      "Iteration 250, loss = 0.13471420\n",
      "Iteration 349, loss = 0.09237235\n",
      "Iteration 204, loss = 0.16074645\n",
      "Iteration 94, loss = 0.25069379\n",
      "Iteration 108, loss = 0.21674858\n",
      "Iteration 257, loss = 0.21934637\n",
      "Iteration 251, loss = 0.13456838\n",
      "Iteration 350, loss = 0.09226792\n",
      "Iteration 351, loss = 0.09218498\n",
      "Iteration 352, loss = 0.09222597\n",
      "Iteration 205, loss = 0.16057174\n",
      "Iteration 353, loss = 0.09201810\n",
      "Iteration 34, loss = 0.44793175\n",
      "Iteration 206, loss = 0.16038280\n",
      "Iteration 95, loss = 0.24878455\n",
      "Iteration 252, loss = 0.13444224\n",
      "Iteration 258, loss = 0.21913713\n",
      "Iteration 253, loss = 0.13433536\n",
      "Iteration 354, loss = 0.09188436\n",
      "Iteration 96, loss = 0.24695946\n",
      "Iteration 109, loss = 0.21623511\n",
      "Iteration 355, loss = 0.09180849\n",
      "Iteration 356, loss = 0.09170428\n",
      "Iteration 97, loss = 0.24514646\n",
      "Iteration 357, loss = 0.09158782\n",
      "Iteration 358, loss = 0.09150485\n",
      "Iteration 35, loss = 0.44199907\n",
      "Iteration 254, loss = 0.13420137\n",
      "Iteration 110, loss = 0.21580193\n",
      "Iteration 98, loss = 0.24327899\n",
      "Iteration 207, loss = 0.16021494\n",
      "Iteration 359, loss = 0.09140781\n",
      "Iteration 255, loss = 0.13406137\n",
      "Iteration 360, loss = 0.09131168\n",
      "Iteration 259, loss = 0.21909787\n",
      "Iteration 256, loss = 0.13397579\n",
      "Iteration 361, loss = 0.09121975\n",
      "Iteration 99, loss = 0.24154678\n",
      "Iteration 362, loss = 0.09116561\n",
      "Iteration 208, loss = 0.16004854\n",
      "Iteration 111, loss = 0.21525008\n",
      "Iteration 363, loss = 0.09107321\n",
      "Iteration 36, loss = 0.43607460\n",
      "Iteration 257, loss = 0.13387137\n",
      "Iteration 100, loss = 0.23984068\n",
      "Iteration 258, loss = 0.13371370\n",
      "Iteration 364, loss = 0.09098480\n",
      "Iteration 112, loss = 0.21480983\n",
      "Iteration 259, loss = 0.13360449\n",
      "Iteration 209, loss = 0.15987948\n",
      "Iteration 101, loss = 0.23815191\n",
      "Iteration 365, loss = 0.09087585\n",
      "Iteration 260, loss = 0.13352230\n",
      "Iteration 366, loss = 0.09080982\n",
      "Iteration 260, loss = 0.21893195\n",
      "Iteration 367, loss = 0.09070031\n",
      "Iteration 210, loss = 0.15972929\n",
      "Iteration 368, loss = 0.09061654\n",
      "Iteration 261, loss = 0.13343678\n",
      "Iteration 37, loss = 0.43023121\n",
      "Iteration 369, loss = 0.09052490\n",
      "Iteration 370, loss = 0.09044759\n",
      "Iteration 102, loss = 0.23644058\n",
      "Iteration 113, loss = 0.21432428\n",
      "Iteration 261, loss = 0.21883251\n",
      "Iteration 103, loss = 0.23478691\n",
      "Iteration 262, loss = 0.13325025\n",
      "Iteration 371, loss = 0.09036657\n",
      "Iteration 211, loss = 0.15952673\n",
      "Iteration 104, loss = 0.23324691\n",
      "Iteration 263, loss = 0.13314850\n",
      "Iteration 372, loss = 0.09036160\n",
      "Iteration 373, loss = 0.09021241\n",
      "Iteration 114, loss = 0.21392791\n",
      "Iteration 212, loss = 0.15936690\n",
      "Iteration 374, loss = 0.09010740\n",
      "Iteration 262, loss = 0.21873174\n",
      "Iteration 38, loss = 0.42441799\n",
      "Iteration 375, loss = 0.09004136\n",
      "Iteration 105, loss = 0.23167344\n",
      "Iteration 264, loss = 0.13303099\n",
      "Iteration 376, loss = 0.08996441\n",
      "Iteration 106, loss = 0.23013014\n",
      "Iteration 265, loss = 0.13293245\n",
      "Iteration 213, loss = 0.15920231\n",
      "Iteration 377, loss = 0.08986179\n",
      "Iteration 378, loss = 0.08980419\n",
      "Iteration 266, loss = 0.13280766\n",
      "Iteration 263, loss = 0.21860772Iteration 115, loss = 0.21345391\n",
      "Iteration 379, loss = 0.08972434\n",
      "\n",
      "Iteration 214, loss = 0.15903088\n",
      "Iteration 267, loss = 0.13269723\n",
      "Iteration 116, loss = 0.21299934\n",
      "Iteration 107, loss = 0.22864194\n",
      "Iteration 268, loss = 0.13259750\n",
      "Iteration 380, loss = 0.08962115\n",
      "Iteration 381, loss = 0.08969803\n",
      "Iteration 39, loss = 0.41890271\n",
      "Iteration 382, loss = 0.08948227\n",
      "Iteration 383, loss = 0.08942001\n",
      "Iteration 108, loss = 0.22717099\n",
      "Iteration 384, loss = 0.08934569\n",
      "Iteration 385, loss = 0.08924327\n",
      "Iteration 269, loss = 0.13247418\n",
      "Iteration 264, loss = 0.21848907\n",
      "Iteration 109, loss = 0.22574843\n",
      "Iteration 386, loss = 0.08917062\n",
      "Iteration 40, loss = 0.41345535\n",
      "Iteration 387, loss = 0.08908943\n",
      "Iteration 117, loss = 0.21254611\n",
      "Iteration 215, loss = 0.15887178\n",
      "Iteration 270, loss = 0.13237399\n",
      "Iteration 271, loss = 0.13226143\n",
      "Iteration 110, loss = 0.22438395\n",
      "Iteration 118, loss = 0.21212039\n",
      "Iteration 388, loss = 0.08903387\n",
      "Iteration 216, loss = 0.15870171\n",
      "Iteration 389, loss = 0.08893186\n",
      "Iteration 390, loss = 0.08888809\n",
      "Iteration 391, loss = 0.08879839\n",
      "Iteration 272, loss = 0.13215014\n",
      "Iteration 111, loss = 0.22303129\n",
      "Iteration 217, loss = 0.15856977\n",
      "Iteration 41, loss = 0.40801610\n",
      "Iteration 392, loss = 0.08873346\n",
      "Iteration 265, loss = 0.21841360\n",
      "Iteration 273, loss = 0.13205668\n",
      "Iteration 393, loss = 0.08869046\n",
      "Iteration 119, loss = 0.21171212\n",
      "Iteration 218, loss = 0.15839997\n",
      "Iteration 112, loss = 0.22166499\n",
      "Iteration 394, loss = 0.08857281\n",
      "Iteration 274, loss = 0.13196370\n",
      "Iteration 395, loss = 0.08854900\n",
      "Iteration 113, loss = 0.22032233\n",
      "Iteration 396, loss = 0.08847047\n",
      "Iteration 275, loss = 0.13185658\n",
      "Iteration 397, loss = 0.08838544\n",
      "Iteration 42, loss = 0.40265203\n",
      "Iteration 114, loss = 0.21906974\n",
      "Iteration 398, loss = 0.08836356\n",
      "Iteration 266, loss = 0.21839516\n",
      "Iteration 115, loss = 0.21784129\n",
      "Iteration 120, loss = 0.21139745Iteration 399, loss = 0.08826204\n",
      "\n",
      "Iteration 219, loss = 0.15822450\n",
      "Iteration 276, loss = 0.13173293\n",
      "Iteration 400, loss = 0.08817322\n",
      "Iteration 116, loss = 0.21658711\n",
      "Iteration 401, loss = 0.08810368\n",
      "Iteration 277, loss = 0.13164474\n",
      "Iteration 402, loss = 0.08803885\n",
      "Iteration 267, loss = 0.21825713\n",
      "Iteration 403, loss = 0.08799339\n",
      "Iteration 43, loss = 0.39737514\n",
      "Iteration 220, loss = 0.15805518\n",
      "Iteration 121, loss = 0.21092712\n",
      "Iteration 278, loss = 0.13152528\n",
      "Iteration 404, loss = 0.08797682\n",
      "Iteration 117, loss = 0.21543972\n",
      "Iteration 279, loss = 0.13143136\n",
      "Iteration 405, loss = 0.08784277\n",
      "Iteration 122, loss = 0.21050945\n",
      "Iteration 221, loss = 0.15791519\n",
      "Iteration 268, loss = 0.21812139\n",
      "Iteration 406, loss = 0.08778488\n",
      "Iteration 118, loss = 0.21419751\n",
      "Iteration 280, loss = 0.13138497\n",
      "Iteration 44, loss = 0.39231443\n",
      "Iteration 407, loss = 0.08781575\n",
      "Iteration 281, loss = 0.13124081\n",
      "Iteration 408, loss = 0.08766994\n",
      "Iteration 409, loss = 0.08758355\n",
      "Iteration 282, loss = 0.13112985\n",
      "Iteration 269, loss = 0.21801108\n",
      "Iteration 410, loss = 0.08755399\n",
      "Iteration 119, loss = 0.21305039\n",
      "Iteration 222, loss = 0.15776697\n",
      "Iteration 411, loss = 0.08745836\n",
      "Iteration 283, loss = 0.13105360\n",
      "Iteration 123, loss = 0.21013921\n",
      "Iteration 412, loss = 0.08741468\n",
      "Iteration 120, loss = 0.21193275\n",
      "Iteration 223, loss = 0.15759621\n",
      "Iteration 284, loss = 0.13095057\n",
      "Iteration 413, loss = 0.08738378\n",
      "Iteration 45, loss = 0.38726211\n",
      "Iteration 414, loss = 0.08733320\n",
      "Iteration 270, loss = 0.21797238\n",
      "Iteration 285, loss = 0.13085778\n",
      "Iteration 224, loss = 0.15745992Iteration 124, loss = 0.20973823\n",
      "\n",
      "Iteration 415, loss = 0.08721777\n",
      "Iteration 121, loss = 0.21082505\n",
      "Iteration 416, loss = 0.08717209\n",
      "Iteration 417, loss = 0.08709823\n",
      "Iteration 122, loss = 0.20983530\n",
      "Iteration 46, loss = 0.38223647\n",
      "Iteration 286, loss = 0.13074178\n",
      "Iteration 418, loss = 0.08706030\n",
      "Iteration 419, loss = 0.08697687\n",
      "Iteration 125, loss = 0.20942682\n",
      "Iteration 287, loss = 0.13065590\n",
      "Iteration 123, loss = 0.20870214\n",
      "Iteration 271, loss = 0.21777152\n",
      "Iteration 225, loss = 0.15730204\n",
      "Iteration 288, loss = 0.13059093\n",
      "Iteration 420, loss = 0.08689740\n",
      "Iteration 124, loss = 0.20765798\n",
      "Iteration 421, loss = 0.08686536\n",
      "Iteration 226, loss = 0.15713687\n",
      "Iteration 422, loss = 0.08681979\n",
      "Iteration 423, loss = 0.08673419\n",
      "Iteration 126, loss = 0.20902080\n",
      "Iteration 424, loss = 0.08669754\n",
      "Iteration 47, loss = 0.37737266\n",
      "Iteration 289, loss = 0.13052032\n",
      "Iteration 125, loss = 0.20668150\n",
      "Iteration 272, loss = 0.21769158\n",
      "Iteration 425, loss = 0.08662918\n",
      "Iteration 426, loss = 0.08656522\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 126, loss = 0.20569197\n",
      "Iteration 227, loss = 0.15698328\n",
      "Iteration 290, loss = 0.13039593\n",
      "Iteration 127, loss = 0.20471192\n",
      "Iteration 273, loss = 0.21759742\n",
      "Iteration 128, loss = 0.20379480\n",
      "Iteration 48, loss = 0.37263215\n",
      "Iteration 228, loss = 0.15683918\n",
      "Iteration 127, loss = 0.20866635\n",
      "Iteration 291, loss = 0.13029253\n",
      "Iteration 292, loss = 0.13019446\n",
      "Iteration 293, loss = 0.13014260\n",
      "Iteration 229, loss = 0.15670214\n",
      "Iteration 128, loss = 0.20831913\n",
      "Iteration 294, loss = 0.13004875\n",
      "Iteration 129, loss = 0.20286520\n",
      "Iteration 274, loss = 0.21749895\n",
      "Iteration 49, loss = 0.36807912\n",
      "Iteration 295, loss = 0.12992693\n",
      "Iteration 130, loss = 0.20198786\n",
      "Iteration 230, loss = 0.15654825\n",
      "Iteration 296, loss = 0.12984995\n",
      "Iteration 129, loss = 0.20797022\n",
      "Iteration 131, loss = 0.20112240\n",
      "Iteration 50, loss = 0.36336200\n",
      "Iteration 297, loss = 0.12974343\n",
      "Iteration 275, loss = 0.21741531\n",
      "Iteration 298, loss = 0.12965101\n",
      "Iteration 132, loss = 0.20023817\n",
      "Iteration 130, loss = 0.20762485\n",
      "Iteration 231, loss = 0.15640631\n",
      "Iteration 133, loss = 0.19941918\n",
      "Iteration 51, loss = 0.35894468\n",
      "Iteration 276, loss = 0.21730325\n",
      "Iteration 232, loss = 0.15625783\n",
      "Iteration 299, loss = 0.12957404\n",
      "Iteration 300, loss = 0.12952442\n",
      "Iteration 131, loss = 0.20727780\n",
      "Iteration 134, loss = 0.19857384\n",
      "Iteration 301, loss = 0.12941607\n",
      "Iteration 233, loss = 0.15616413\n",
      "Iteration 52, loss = 0.35462941\n",
      "Iteration 135, loss = 0.19779251\n",
      "Iteration 302, loss = 0.12931844\n",
      "Iteration 277, loss = 0.21719971\n",
      "Iteration 132, loss = 0.20696000\n",
      "Iteration 234, loss = 0.15600001\n",
      "Iteration 136, loss = 0.19698948\n",
      "Iteration 303, loss = 0.12926199\n",
      "Iteration 53, loss = 0.35035732\n",
      "Iteration 137, loss = 0.19622164\n",
      "Iteration 304, loss = 0.12915305\n",
      "Iteration 133, loss = 0.20661094\n",
      "Iteration 305, loss = 0.12906739\n",
      "Iteration 235, loss = 0.15583180\n",
      "Iteration 278, loss = 0.21713055\n",
      "Iteration 138, loss = 0.19546544\n",
      "Iteration 134, loss = 0.20629388\n",
      "Iteration 236, loss = 0.15572227\n",
      "Iteration 306, loss = 0.12900689\n",
      "Iteration 54, loss = 0.34623104\n",
      "Iteration 139, loss = 0.19474014\n",
      "Iteration 279, loss = 0.21702769\n",
      "Iteration 140, loss = 0.19402969\n",
      "Iteration 307, loss = 0.12891102\n",
      "Iteration 237, loss = 0.15556324\n",
      "Iteration 135, loss = 0.20598122\n",
      "Iteration 55, loss = 0.34221210\n",
      "Iteration 308, loss = 0.12884007\n",
      "Iteration 141, loss = 0.19330073\n",
      "Iteration 280, loss = 0.21691343\n",
      "Iteration 238, loss = 0.15544909\n",
      "Iteration 309, loss = 0.12874641\n",
      "Iteration 142, loss = 0.19256689\n",
      "Iteration 136, loss = 0.20568275\n",
      "Iteration 239, loss = 0.15528164\n",
      "Iteration 310, loss = 0.12866863\n",
      "Iteration 56, loss = 0.33840440\n",
      "Iteration 143, loss = 0.19194450\n",
      "Iteration 144, loss = 0.19128362\n",
      "Iteration 240, loss = 0.15514815\n",
      "Iteration 281, loss = 0.21688482\n",
      "Iteration 145, loss = 0.19061424\n",
      "Iteration 311, loss = 0.12862259\n",
      "Iteration 137, loss = 0.20536994\n",
      "Iteration 57, loss = 0.33454024\n",
      "Iteration 312, loss = 0.12851305\n",
      "Iteration 241, loss = 0.15501538\n",
      "Iteration 138, loss = 0.20506665\n",
      "Iteration 313, loss = 0.12843729\n",
      "Iteration 242, loss = 0.15487612\n",
      "Iteration 146, loss = 0.19000105\n",
      "Iteration 282, loss = 0.21675083\n",
      "Iteration 314, loss = 0.12835271\n",
      "Iteration 58, loss = 0.33109592\n",
      "Iteration 147, loss = 0.18934347\n",
      "Iteration 139, loss = 0.20477770\n",
      "Iteration 243, loss = 0.15474342\n",
      "Iteration 315, loss = 0.12833755\n",
      "Iteration 283, loss = 0.21668732\n",
      "Iteration 148, loss = 0.18878047\n",
      "Iteration 149, loss = 0.18819406\n",
      "Iteration 316, loss = 0.12819069\n",
      "Iteration 140, loss = 0.20449405\n",
      "Iteration 244, loss = 0.15461911\n",
      "Iteration 150, loss = 0.18763658\n",
      "Iteration 59, loss = 0.32740133\n",
      "Iteration 284, loss = 0.21657350\n",
      "Iteration 317, loss = 0.12812791\n",
      "Iteration 141, loss = 0.20418718\n",
      "Iteration 60, loss = 0.32415893\n",
      "Iteration 245, loss = 0.15449371\n",
      "Iteration 151, loss = 0.18701819\n",
      "Iteration 318, loss = 0.12804906\n",
      "Iteration 152, loss = 0.18647001\n",
      "Iteration 246, loss = 0.15435762\n",
      "Iteration 285, loss = 0.21654516\n",
      "Iteration 153, loss = 0.18595590\n",
      "Iteration 142, loss = 0.20393935\n",
      "Iteration 319, loss = 0.12801168\n",
      "Iteration 320, loss = 0.12788267\n",
      "Iteration 247, loss = 0.15423590\n",
      "Iteration 61, loss = 0.32094288\n",
      "Iteration 154, loss = 0.18539386\n",
      "Iteration 286, loss = 0.21640028\n",
      "Iteration 143, loss = 0.20362713\n",
      "Iteration 321, loss = 0.12783357\n",
      "Iteration 322, loss = 0.12776234\n",
      "Iteration 248, loss = 0.15411534\n",
      "Iteration 155, loss = 0.18487109\n",
      "Iteration 323, loss = 0.12766105\n",
      "Iteration 62, loss = 0.31773430\n",
      "Iteration 324, loss = 0.12759435\n",
      "Iteration 144, loss = 0.20336315\n",
      "Iteration 287, loss = 0.21630481\n",
      "Iteration 249, loss = 0.15397022\n",
      "Iteration 156, loss = 0.18438014\n",
      "Iteration 157, loss = 0.18386930\n",
      "Iteration 250, loss = 0.15383991\n",
      "Iteration 63, loss = 0.31482851\n",
      "Iteration 145, loss = 0.20309908\n",
      "Iteration 325, loss = 0.12749372\n",
      "Iteration 251, loss = 0.15370573\n",
      "Iteration 288, loss = 0.21621141\n",
      "Iteration 326, loss = 0.12744155\n",
      "Iteration 146, loss = 0.20282508\n",
      "Iteration 158, loss = 0.18338574\n",
      "Iteration 327, loss = 0.12736830\n",
      "Iteration 64, loss = 0.31196727\n",
      "Iteration 328, loss = 0.12727900\n",
      "Iteration 159, loss = 0.18294160\n",
      "Iteration 329, loss = 0.12724698\n",
      "Iteration 252, loss = 0.15357517\n",
      "Iteration 289, loss = 0.21613369\n",
      "Iteration 147, loss = 0.20255316\n",
      "Iteration 330, loss = 0.12716917\n",
      "Iteration 65, loss = 0.30923929\n",
      "Iteration 160, loss = 0.18245751\n",
      "Iteration 331, loss = 0.12709279\n",
      "Iteration 161, loss = 0.18193862\n",
      "Iteration 253, loss = 0.15344435\n",
      "Iteration 162, loss = 0.18148512\n",
      "Iteration 332, loss = 0.12700564\n",
      "Iteration 254, loss = 0.15331574\n",
      "Iteration 66, loss = 0.30667601\n",
      "Iteration 148, loss = 0.20235038\n",
      "Iteration 163, loss = 0.18108608\n",
      "Iteration 290, loss = 0.21606974\n",
      "Iteration 333, loss = 0.12693468\n",
      "Iteration 164, loss = 0.18062936\n",
      "Iteration 255, loss = 0.15320683\n",
      "Iteration 334, loss = 0.12687337\n",
      "Iteration 291, loss = 0.21595341\n",
      "Iteration 335, loss = 0.12679913\n",
      "Iteration 149, loss = 0.20203158\n",
      "Iteration 67, loss = 0.30414900\n",
      "Iteration 256, loss = 0.15309957\n",
      "Iteration 165, loss = 0.18019408\n",
      "Iteration 166, loss = 0.17980276\n",
      "Iteration 150, loss = 0.20175912\n",
      "Iteration 167, loss = 0.17938959\n",
      "Iteration 336, loss = 0.12673593\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 68, loss = 0.30178608\n",
      "Iteration 292, loss = 0.21585388\n",
      "Iteration 257, loss = 0.15298614\n",
      "Iteration 151, loss = 0.20153030\n",
      "Iteration 69, loss = 0.29946501\n",
      "Iteration 258, loss = 0.15282794\n",
      "Iteration 259, loss = 0.15272416\n",
      "Iteration 168, loss = 0.17898618\n",
      "Iteration 293, loss = 0.21580247\n",
      "Iteration 169, loss = 0.17857877\n",
      "Iteration 152, loss = 0.20124989\n",
      "Iteration 260, loss = 0.15263180\n",
      "Iteration 170, loss = 0.17819417\n",
      "Iteration 153, loss = 0.20101448\n",
      "Iteration 171, loss = 0.17778205\n",
      "Iteration 261, loss = 0.15247488\n",
      "Iteration 294, loss = 0.21570832\n",
      "Iteration 70, loss = 0.29729934\n",
      "Iteration 154, loss = 0.20076476\n",
      "Iteration 172, loss = 0.17742519\n",
      "Iteration 262, loss = 0.15235615\n",
      "Iteration 173, loss = 0.17704256\n",
      "Iteration 295, loss = 0.21563823\n",
      "Iteration 71, loss = 0.29518622\n",
      "Iteration 263, loss = 0.15223924\n",
      "Iteration 174, loss = 0.17671371\n",
      "Iteration 155, loss = 0.20050678\n",
      "Iteration 264, loss = 0.15213964\n",
      "Iteration 175, loss = 0.17632049\n",
      "Iteration 72, loss = 0.29314278\n",
      "Iteration 156, loss = 0.20025797\n",
      "Iteration 296, loss = 0.21554145\n",
      "Iteration 265, loss = 0.15200828\n",
      "Iteration 176, loss = 0.17601768\n",
      "Iteration 266, loss = 0.15188911\n",
      "Iteration 73, loss = 0.29122817\n",
      "Iteration 177, loss = 0.17564268\n",
      "Iteration 267, loss = 0.15177553\n",
      "Iteration 157, loss = 0.20002084\n",
      "Iteration 297, loss = 0.21544639\n",
      "Iteration 178, loss = 0.17529466\n",
      "Iteration 158, loss = 0.19976249\n",
      "Iteration 74, loss = 0.28945038\n",
      "Iteration 179, loss = 0.17496393\n",
      "Iteration 268, loss = 0.15166563\n",
      "Iteration 180, loss = 0.17461271\n",
      "Iteration 159, loss = 0.19959492\n",
      "Iteration 75, loss = 0.28765178\n",
      "Iteration 298, loss = 0.21542440\n",
      "Iteration 269, loss = 0.15154740\n",
      "Iteration 181, loss = 0.17433101\n",
      "Iteration 270, loss = 0.15142889\n",
      "Iteration 182, loss = 0.17401827\n",
      "Iteration 160, loss = 0.19935055\n",
      "Iteration 76, loss = 0.28605190\n",
      "Iteration 299, loss = 0.21528517\n",
      "Iteration 183, loss = 0.17369486\n",
      "Iteration 271, loss = 0.15131680\n",
      "Iteration 161, loss = 0.19906045\n",
      "Iteration 184, loss = 0.17338691\n",
      "Iteration 272, loss = 0.15124931\n",
      "Iteration 300, loss = 0.21522619\n",
      "Iteration 77, loss = 0.28432346\n",
      "Iteration 162, loss = 0.19885154\n",
      "Iteration 185, loss = 0.17309236\n",
      "Iteration 273, loss = 0.15109530\n",
      "Iteration 78, loss = 0.28281879\n",
      "Iteration 186, loss = 0.17278221\n",
      "Iteration 301, loss = 0.21516431\n",
      "Iteration 274, loss = 0.15098025\n",
      "Iteration 163, loss = 0.19861069\n",
      "Iteration 275, loss = 0.15086936\n",
      "Iteration 187, loss = 0.17252994\n",
      "Iteration 188, loss = 0.17221421\n",
      "Iteration 164, loss = 0.19840493\n",
      "Iteration 79, loss = 0.28130130\n",
      "Iteration 302, loss = 0.21507212\n",
      "Iteration 276, loss = 0.15076347\n",
      "Iteration 189, loss = 0.17199963\n",
      "Iteration 165, loss = 0.19816677\n",
      "Iteration 277, loss = 0.15066597\n",
      "Iteration 190, loss = 0.17169272\n",
      "Iteration 191, loss = 0.17140233\n",
      "Iteration 303, loss = 0.21497547\n",
      "Iteration 80, loss = 0.27982416\n",
      "Iteration 166, loss = 0.19793552\n",
      "Iteration 278, loss = 0.15054024\n",
      "Iteration 304, loss = 0.21487868\n",
      "Iteration 192, loss = 0.17118538\n",
      "Iteration 81, loss = 0.27843344\n",
      "Iteration 279, loss = 0.15046364\n",
      "Iteration 193, loss = 0.17084382\n",
      "Iteration 167, loss = 0.19775971\n",
      "Iteration 280, loss = 0.15033633\n",
      "Iteration 305, loss = 0.21482908\n",
      "Iteration 82, loss = 0.27710848\n",
      "Iteration 194, loss = 0.17066072\n",
      "Iteration 168, loss = 0.19751901\n",
      "Iteration 281, loss = 0.15022462\n",
      "Iteration 195, loss = 0.17036362\n",
      "Iteration 306, loss = 0.21476057\n",
      "Iteration 282, loss = 0.15012350\n",
      "Iteration 196, loss = 0.17009503\n",
      "Iteration 169, loss = 0.19731568\n",
      "Iteration 83, loss = 0.27580038\n",
      "Iteration 283, loss = 0.15000445\n",
      "Iteration 197, loss = 0.16983766\n",
      "Iteration 170, loss = 0.19705762\n",
      "Iteration 198, loss = 0.16962092\n",
      "Iteration 84, loss = 0.27460031\n",
      "Iteration 307, loss = 0.21467237\n",
      "Iteration 171, loss = 0.19693649\n",
      "Iteration 284, loss = 0.14990784\n",
      "Iteration 199, loss = 0.16934606\n",
      "Iteration 85, loss = 0.27336286\n",
      "Iteration 308, loss = 0.21463356\n",
      "Iteration 200, loss = 0.16910522\n",
      "Iteration 172, loss = 0.19666455\n",
      "Iteration 285, loss = 0.14981378\n",
      "Iteration 309, loss = 0.21459884\n",
      "Iteration 86, loss = 0.27219761\n",
      "Iteration 201, loss = 0.16888053\n",
      "Iteration 286, loss = 0.14971916\n",
      "Iteration 173, loss = 0.19649637\n",
      "Iteration 202, loss = 0.16864508\n",
      "Iteration 287, loss = 0.14959582\n",
      "Iteration 203, loss = 0.16842049\n",
      "Iteration 87, loss = 0.27104399\n",
      "Iteration 310, loss = 0.21444063\n",
      "Iteration 174, loss = 0.19624880\n",
      "Iteration 288, loss = 0.14948745\n",
      "Iteration 204, loss = 0.16818068\n",
      "Iteration 175, loss = 0.19605542\n",
      "Iteration 88, loss = 0.27000807\n",
      "Iteration 205, loss = 0.16795915\n",
      "Iteration 311, loss = 0.21434952\n",
      "Iteration 289, loss = 0.14939787\n",
      "Iteration 176, loss = 0.19583502\n",
      "Iteration 290, loss = 0.14928768\n",
      "Iteration 206, loss = 0.16776829\n",
      "Iteration 89, loss = 0.26897362\n",
      "Iteration 312, loss = 0.21427423\n",
      "Iteration 177, loss = 0.19564564\n",
      "Iteration 207, loss = 0.16755708\n",
      "Iteration 291, loss = 0.14921152\n",
      "Iteration 208, loss = 0.16735819\n",
      "Iteration 90, loss = 0.26792432\n",
      "Iteration 313, loss = 0.21421898\n",
      "Iteration 178, loss = 0.19543847\n",
      "Iteration 292, loss = 0.14912681\n",
      "Iteration 209, loss = 0.16712230\n",
      "Iteration 210, loss = 0.16690311\n",
      "Iteration 293, loss = 0.14898934\n",
      "Iteration 179, loss = 0.19529587\n",
      "Iteration 91, loss = 0.26694595\n",
      "Iteration 294, loss = 0.14890637\n",
      "Iteration 314, loss = 0.21413896\n",
      "Iteration 211, loss = 0.16669902\n",
      "Iteration 92, loss = 0.26601382\n",
      "Iteration 180, loss = 0.19513475\n",
      "Iteration 212, loss = 0.16647352\n",
      "Iteration 315, loss = 0.21416677\n",
      "Iteration 295, loss = 0.14879294\n",
      "Iteration 213, loss = 0.16628922\n",
      "Iteration 93, loss = 0.26512779\n",
      "Iteration 181, loss = 0.19494214\n",
      "Iteration 214, loss = 0.16607350\n",
      "Iteration 296, loss = 0.14869647\n",
      "Iteration 215, loss = 0.16591990\n",
      "Iteration 297, loss = 0.14858906\n",
      "Iteration 216, loss = 0.16565853\n",
      "Iteration 94, loss = 0.26419380\n",
      "Iteration 182, loss = 0.19468805\n",
      "Iteration 316, loss = 0.21415218\n",
      "Iteration 298, loss = 0.14851011\n",
      "Iteration 217, loss = 0.16548699\n",
      "Iteration 183, loss = 0.19449369\n",
      "Iteration 218, loss = 0.16531972\n",
      "Iteration 299, loss = 0.14840440\n",
      "Iteration 317, loss = 0.21399508\n",
      "Iteration 95, loss = 0.26329678\n",
      "Iteration 219, loss = 0.16507963\n",
      "Iteration 300, loss = 0.14832863\n",
      "Iteration 184, loss = 0.19430286\n",
      "Iteration 318, loss = 0.21388041\n",
      "Iteration 301, loss = 0.14822423\n",
      "Iteration 220, loss = 0.16489488\n",
      "Iteration 96, loss = 0.26246017\n",
      "Iteration 185, loss = 0.19412794\n",
      "Iteration 221, loss = 0.16472114\n",
      "Iteration 302, loss = 0.14817039\n",
      "Iteration 222, loss = 0.16450707\n",
      "Iteration 319, loss = 0.21377118\n",
      "Iteration 303, loss = 0.14804274\n",
      "Iteration 186, loss = 0.19393385\n",
      "Iteration 223, loss = 0.16433622Iteration 97, loss = 0.26164988\n",
      "\n",
      "Iteration 224, loss = 0.16420387\n",
      "Iteration 304, loss = 0.14793793\n",
      "Iteration 187, loss = 0.19378516\n",
      "Iteration 225, loss = 0.16396854\n",
      "Iteration 305, loss = 0.14787474\n",
      "Iteration 320, loss = 0.21373701\n",
      "Iteration 98, loss = 0.26086748\n",
      "Iteration 226, loss = 0.16382800\n",
      "Iteration 306, loss = 0.14777359\n",
      "Iteration 227, loss = 0.16360323\n",
      "Iteration 321, loss = 0.21365041\n",
      "Iteration 188, loss = 0.19357618\n",
      "Iteration 307, loss = 0.14768983\n",
      "Iteration 308, loss = 0.14756353\n",
      "Iteration 189, loss = 0.19342643\n",
      "Iteration 322, loss = 0.21356973\n",
      "Iteration 99, loss = 0.26008267\n",
      "Iteration 228, loss = 0.16345043\n",
      "Iteration 229, loss = 0.16326196\n",
      "Iteration 100, loss = 0.25934151\n",
      "Iteration 323, loss = 0.21347988\n",
      "Iteration 190, loss = 0.19324148\n",
      "Iteration 309, loss = 0.14749215\n",
      "Iteration 230, loss = 0.16309628\n",
      "Iteration 231, loss = 0.16294301\n",
      "Iteration 310, loss = 0.14739513\n",
      "Iteration 232, loss = 0.16272618\n",
      "Iteration 191, loss = 0.19305863\n",
      "Iteration 101, loss = 0.25860181\n",
      "Iteration 324, loss = 0.21344538\n",
      "Iteration 233, loss = 0.16262299\n",
      "Iteration 311, loss = 0.14729721\n",
      "Iteration 192, loss = 0.19287875\n",
      "Iteration 234, loss = 0.16240814\n",
      "Iteration 102, loss = 0.25792122\n",
      "Iteration 312, loss = 0.14722076\n",
      "Iteration 235, loss = 0.16222818\n",
      "Iteration 236, loss = 0.16209013\n",
      "Iteration 325, loss = 0.21343085\n",
      "Iteration 193, loss = 0.19273710\n",
      "Iteration 103, loss = 0.25724190\n",
      "Iteration 313, loss = 0.14712892\n",
      "Iteration 326, loss = 0.21327391\n",
      "Iteration 237, loss = 0.16192100\n",
      "Iteration 194, loss = 0.19253128\n",
      "Iteration 238, loss = 0.16173068\n",
      "Iteration 314, loss = 0.14703703\n",
      "Iteration 195, loss = 0.19237377\n",
      "Iteration 239, loss = 0.16163253\n",
      "Iteration 104, loss = 0.25660753\n",
      "Iteration 327, loss = 0.21321119\n",
      "Iteration 315, loss = 0.14693447\n",
      "Iteration 316, loss = 0.14686004\n",
      "Iteration 105, loss = 0.25597900\n",
      "Iteration 240, loss = 0.16141029\n",
      "Iteration 196, loss = 0.19220809\n",
      "Iteration 328, loss = 0.21316729\n",
      "Iteration 241, loss = 0.16125897\n",
      "Iteration 317, loss = 0.14676860\n",
      "Iteration 106, loss = 0.25527819\n",
      "Iteration 197, loss = 0.19206223\n",
      "Iteration 242, loss = 0.16110001\n",
      "Iteration 243, loss = 0.16094190\n",
      "Iteration 107, loss = 0.25464506\n",
      "Iteration 318, loss = 0.14668812\n",
      "Iteration 198, loss = 0.19188306\n",
      "Iteration 329, loss = 0.21306963\n",
      "Iteration 319, loss = 0.14659556\n",
      "Iteration 244, loss = 0.16076529\n",
      "Iteration 320, loss = 0.14651554\n",
      "Iteration 321, loss = 0.14641306\n",
      "Iteration 199, loss = 0.19170502\n",
      "Iteration 245, loss = 0.16063632\n",
      "Iteration 246, loss = 0.16044532\n",
      "Iteration 330, loss = 0.21303189\n",
      "Iteration 108, loss = 0.25402971\n",
      "Iteration 331, loss = 0.21293271\n",
      "Iteration 200, loss = 0.19156311\n",
      "Iteration 322, loss = 0.14633810\n",
      "Iteration 247, loss = 0.16030316\n",
      "Iteration 323, loss = 0.14624670\n",
      "Iteration 248, loss = 0.16013917\n",
      "Iteration 109, loss = 0.25343531\n",
      "Iteration 249, loss = 0.15997910\n",
      "Iteration 201, loss = 0.19146772\n",
      "Iteration 110, loss = 0.25292578\n",
      "Iteration 324, loss = 0.14618528\n",
      "Iteration 332, loss = 0.21290018\n",
      "Iteration 111, loss = 0.25226413\n",
      "Iteration 202, loss = 0.19123068\n",
      "Iteration 250, loss = 0.15984686\n",
      "Iteration 251, loss = 0.15970037\n",
      "Iteration 203, loss = 0.19109698\n",
      "Iteration 325, loss = 0.14609076\n",
      "Iteration 333, loss = 0.21286637\n",
      "Iteration 326, loss = 0.14600053\n",
      "Iteration 112, loss = 0.25173549\n",
      "Iteration 327, loss = 0.14591629\n",
      "Iteration 328, loss = 0.14582408\n",
      "Iteration 252, loss = 0.15953377\n",
      "Iteration 334, loss = 0.21280751\n",
      "Iteration 204, loss = 0.19095216\n",
      "Iteration 205, loss = 0.19079540\n",
      "Iteration 253, loss = 0.15941604\n",
      "Iteration 113, loss = 0.25121410\n",
      "Iteration 254, loss = 0.15927010\n",
      "Iteration 329, loss = 0.14578427\n",
      "Iteration 255, loss = 0.15907341\n",
      "Iteration 335, loss = 0.21268553\n",
      "Iteration 330, loss = 0.14568671\n",
      "Iteration 256, loss = 0.15891520\n",
      "Iteration 206, loss = 0.19063498\n",
      "Iteration 336, loss = 0.21263527\n",
      "Iteration 114, loss = 0.25068443\n",
      "Iteration 257, loss = 0.15886202\n",
      "Iteration 331, loss = 0.14560448\n",
      "Iteration 258, loss = 0.15864155\n",
      "Iteration 207, loss = 0.19047357\n",
      "Iteration 259, loss = 0.15851772\n",
      "Iteration 115, loss = 0.25017741\n",
      "Iteration 332, loss = 0.14551420\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 208, loss = 0.19033157\n",
      "Iteration 337, loss = 0.21254321\n",
      "Iteration 260, loss = 0.15835185\n",
      "Iteration 116, loss = 0.24965684\n",
      "Iteration 261, loss = 0.15823498\n",
      "Iteration 262, loss = 0.15812132\n",
      "Iteration 338, loss = 0.21249826\n",
      "Iteration 209, loss = 0.19017147\n",
      "Iteration 210, loss = 0.19002442\n",
      "Iteration 117, loss = 0.24919015\n",
      "Iteration 263, loss = 0.15792361\n",
      "Iteration 339, loss = 0.21251278\n",
      "Iteration 264, loss = 0.15779217\n",
      "Iteration 211, loss = 0.18990214\n",
      "Iteration 118, loss = 0.24871278\n",
      "Iteration 265, loss = 0.15764085\n",
      "Iteration 340, loss = 0.21238451\n",
      "Iteration 212, loss = 0.18973517\n",
      "Iteration 266, loss = 0.15750542\n",
      "Iteration 119, loss = 0.24826913\n",
      "Iteration 341, loss = 0.21231605\n",
      "Iteration 267, loss = 0.15735990\n",
      "Iteration 268, loss = 0.15726430\n",
      "Iteration 213, loss = 0.18959719\n",
      "Iteration 342, loss = 0.21229644\n",
      "Iteration 120, loss = 0.24781763\n",
      "Iteration 269, loss = 0.15711486\n",
      "Iteration 270, loss = 0.15698200\n",
      "Iteration 214, loss = 0.18945609\n",
      "Iteration 121, loss = 0.24731951\n",
      "Iteration 343, loss = 0.21218961\n",
      "Iteration 215, loss = 0.18930813\n",
      "Iteration 271, loss = 0.15684166\n",
      "Iteration 272, loss = 0.15670945\n",
      "Iteration 344, loss = 0.21209351\n",
      "Iteration 273, loss = 0.15662224\n",
      "Iteration 122, loss = 0.24690522\n",
      "Iteration 216, loss = 0.18920311\n",
      "Iteration 123, loss = 0.24646345\n",
      "Iteration 274, loss = 0.15648839\n",
      "Iteration 345, loss = 0.21212117\n",
      "Iteration 217, loss = 0.18903961\n",
      "Iteration 275, loss = 0.15638412\n",
      "Iteration 276, loss = 0.15629139\n",
      "Iteration 218, loss = 0.18892395\n",
      "Iteration 124, loss = 0.24606283\n",
      "Iteration 346, loss = 0.21197358\n",
      "Iteration 277, loss = 0.15609110\n",
      "Iteration 278, loss = 0.15597731\n",
      "Iteration 347, loss = 0.21189706\n",
      "Iteration 219, loss = 0.18875765\n",
      "Iteration 125, loss = 0.24563946\n",
      "Iteration 279, loss = 0.15589233\n",
      "Iteration 280, loss = 0.15573033\n",
      "Iteration 126, loss = 0.24526598\n",
      "Iteration 220, loss = 0.18864331\n",
      "Iteration 348, loss = 0.21186729\n",
      "Iteration 281, loss = 0.15563497\n",
      "Iteration 221, loss = 0.18846828\n",
      "Iteration 282, loss = 0.15552260\n",
      "Iteration 127, loss = 0.24486740\n",
      "Iteration 349, loss = 0.21180665\n",
      "Iteration 222, loss = 0.18834686\n",
      "Iteration 283, loss = 0.15534315\n",
      "Iteration 284, loss = 0.15528300\n",
      "Iteration 223, loss = 0.18826043\n",
      "Iteration 128, loss = 0.24444390\n",
      "Iteration 350, loss = 0.21176367\n",
      "Iteration 285, loss = 0.15514615\n",
      "Iteration 286, loss = 0.15505335\n",
      "Iteration 129, loss = 0.24403633\n",
      "Iteration 351, loss = 0.21170990\n",
      "Iteration 224, loss = 0.18809605\n",
      "Iteration 287, loss = 0.15490272\n",
      "Iteration 225, loss = 0.18795595\n",
      "Iteration 288, loss = 0.15482171\n",
      "Iteration 130, loss = 0.24378654\n",
      "Iteration 352, loss = 0.21161208\n",
      "Iteration 226, loss = 0.18783403\n",
      "Iteration 289, loss = 0.15467614\n",
      "Iteration 131, loss = 0.24330291\n",
      "Iteration 290, loss = 0.15458265\n",
      "Iteration 227, loss = 0.18771096\n",
      "Iteration 353, loss = 0.21159496\n",
      "Iteration 132, loss = 0.24294895\n",
      "Iteration 291, loss = 0.15455479\n",
      "Iteration 354, loss = 0.21153982\n",
      "Iteration 228, loss = 0.18756423\n",
      "Iteration 292, loss = 0.15439007\n",
      "Iteration 293, loss = 0.15425817\n",
      "Iteration 229, loss = 0.18750296\n",
      "Iteration 294, loss = 0.15415016\n",
      "Iteration 133, loss = 0.24256974\n",
      "Iteration 295, loss = 0.15405632\n",
      "Iteration 355, loss = 0.21150457\n",
      "Iteration 296, loss = 0.15395485\n",
      "Iteration 356, loss = 0.21140695\n",
      "Iteration 230, loss = 0.18732531\n",
      "Iteration 134, loss = 0.24223656\n",
      "Iteration 231, loss = 0.18722265\n",
      "Iteration 297, loss = 0.15384484\n",
      "Iteration 298, loss = 0.15374171\n",
      "Iteration 357, loss = 0.21134594\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 232, loss = 0.18707809\n",
      "Iteration 233, loss = 0.18695452\n",
      "Iteration 135, loss = 0.24188611\n",
      "Iteration 299, loss = 0.15365393\n",
      "Iteration 300, loss = 0.15356134\n",
      "Iteration 136, loss = 0.24154007\n",
      "Iteration 234, loss = 0.18682582\n",
      "Iteration 301, loss = 0.15342741\n",
      "Iteration 302, loss = 0.15336742\n",
      "Iteration 235, loss = 0.18674342\n",
      "Iteration 137, loss = 0.24120094\n",
      "Iteration 303, loss = 0.15321676\n",
      "Iteration 138, loss = 0.24088072\n",
      "Iteration 304, loss = 0.15316381\n",
      "Iteration 236, loss = 0.18661512\n",
      "Iteration 305, loss = 0.15301467\n",
      "Iteration 306, loss = 0.15295218\n",
      "Iteration 139, loss = 0.24059607\n",
      "Iteration 237, loss = 0.18648535\n",
      "Iteration 307, loss = 0.15284617\n",
      "Iteration 308, loss = 0.15275045\n",
      "Iteration 238, loss = 0.18636548\n",
      "Iteration 309, loss = 0.15270473\n",
      "Iteration 140, loss = 0.24022267\n",
      "Iteration 310, loss = 0.15254217\n",
      "Iteration 311, loss = 0.15250547\n",
      "Iteration 239, loss = 0.18623917\n",
      "Iteration 312, loss = 0.15236812\n",
      "Iteration 240, loss = 0.18622788\n",
      "Iteration 141, loss = 0.23988866\n",
      "Iteration 313, loss = 0.15228522\n",
      "Iteration 314, loss = 0.15221589\n",
      "Iteration 315, loss = 0.15210751\n",
      "Iteration 241, loss = 0.18603946\n",
      "Iteration 142, loss = 0.23958869\n",
      "Iteration 242, loss = 0.18588461\n",
      "Iteration 316, loss = 0.15201214\n",
      "Iteration 143, loss = 0.23929799\n",
      "Iteration 317, loss = 0.15190654\n",
      "Iteration 243, loss = 0.18581449\n",
      "Iteration 144, loss = 0.23897113\n",
      "Iteration 318, loss = 0.15186308\n",
      "Iteration 244, loss = 0.18569399\n",
      "Iteration 319, loss = 0.15174778\n",
      "Iteration 145, loss = 0.23867797\n",
      "Iteration 320, loss = 0.15167027\n",
      "Iteration 245, loss = 0.18557953\n",
      "Iteration 321, loss = 0.15159308\n",
      "Iteration 146, loss = 0.23837209\n",
      "Iteration 246, loss = 0.18544579\n",
      "Iteration 322, loss = 0.15153589\n",
      "Iteration 323, loss = 0.15141250\n",
      "Iteration 147, loss = 0.23812018\n",
      "Iteration 247, loss = 0.18532629\n",
      "Iteration 324, loss = 0.15132603\n",
      "Iteration 148, loss = 0.23779315\n",
      "Iteration 325, loss = 0.15121740\n",
      "Iteration 248, loss = 0.18523331\n",
      "Iteration 326, loss = 0.15115472\n",
      "Iteration 249, loss = 0.18511521\n",
      "Iteration 327, loss = 0.15106978\n",
      "Iteration 149, loss = 0.23755947\n",
      "Iteration 328, loss = 0.15096158\n",
      "Iteration 250, loss = 0.18501566\n",
      "Iteration 329, loss = 0.15088258\n",
      "Iteration 150, loss = 0.23723352\n",
      "Iteration 330, loss = 0.15084993\n",
      "Iteration 251, loss = 0.18489635\n",
      "Iteration 151, loss = 0.23696110\n",
      "Iteration 331, loss = 0.15072985\n",
      "Iteration 252, loss = 0.18482334\n",
      "Iteration 332, loss = 0.15064825\n",
      "Iteration 152, loss = 0.23668705\n",
      "Iteration 333, loss = 0.15061677\n",
      "Iteration 334, loss = 0.15047051\n",
      "Iteration 253, loss = 0.18469341\n",
      "Iteration 335, loss = 0.15040771\n",
      "Iteration 153, loss = 0.23640889\n",
      "Iteration 254, loss = 0.18457902\n",
      "Iteration 336, loss = 0.15037785\n",
      "Iteration 337, loss = 0.15026511\n",
      "Iteration 255, loss = 0.18448911\n",
      "Iteration 154, loss = 0.23615394\n",
      "Iteration 338, loss = 0.15017841\n",
      "Iteration 339, loss = 0.15008558\n",
      "Iteration 155, loss = 0.23586932\n",
      "Iteration 340, loss = 0.15002440\n",
      "Iteration 256, loss = 0.18438072\n",
      "Iteration 341, loss = 0.14992880\n",
      "Iteration 257, loss = 0.18428872\n",
      "Iteration 342, loss = 0.14987122\n",
      "Iteration 156, loss = 0.23575408\n",
      "Iteration 343, loss = 0.14978245\n",
      "Iteration 344, loss = 0.14972543\n",
      "Iteration 258, loss = 0.18418905\n",
      "Iteration 157, loss = 0.23545324\n",
      "Iteration 345, loss = 0.14965396\n",
      "Iteration 259, loss = 0.18411592\n",
      "Iteration 346, loss = 0.14958818\n",
      "Iteration 260, loss = 0.18402284\n",
      "Iteration 261, loss = 0.18385890\n",
      "Iteration 158, loss = 0.23512916\n",
      "Iteration 347, loss = 0.14947594\n",
      "Iteration 348, loss = 0.14942103\n",
      "Iteration 262, loss = 0.18376007\n",
      "Iteration 159, loss = 0.23485398\n",
      "Iteration 349, loss = 0.14938003\n",
      "Iteration 263, loss = 0.18367638\n",
      "Iteration 350, loss = 0.14927204\n",
      "Iteration 160, loss = 0.23459592\n",
      "Iteration 351, loss = 0.14922093\n",
      "Iteration 264, loss = 0.18356596\n",
      "Iteration 352, loss = 0.14913392\n",
      "Iteration 353, loss = 0.14908743\n",
      "Iteration 161, loss = 0.23433838\n",
      "Iteration 354, loss = 0.14897158\n",
      "Iteration 265, loss = 0.18347935\n",
      "Iteration 355, loss = 0.14892412\n",
      "Iteration 162, loss = 0.23410083\n",
      "Iteration 266, loss = 0.18338496\n",
      "Iteration 356, loss = 0.14882750\n",
      "Iteration 357, loss = 0.14878123\n",
      "Iteration 267, loss = 0.18328373\n",
      "Iteration 163, loss = 0.23384722\n",
      "Iteration 358, loss = 0.14870486\n",
      "Iteration 268, loss = 0.18319626\n",
      "Iteration 359, loss = 0.14863507\n",
      "Iteration 360, loss = 0.14855663\n",
      "Iteration 164, loss = 0.23361254\n",
      "Iteration 269, loss = 0.18309836\n",
      "Iteration 361, loss = 0.14851976\n",
      "Iteration 362, loss = 0.14843577\n",
      "Iteration 270, loss = 0.18307496\n",
      "Iteration 363, loss = 0.14836651\n",
      "Iteration 165, loss = 0.23344799\n",
      "Iteration 271, loss = 0.18289730\n",
      "Iteration 364, loss = 0.14834070\n",
      "Iteration 166, loss = 0.23315850\n",
      "Iteration 365, loss = 0.14823106\n",
      "Iteration 272, loss = 0.18287884\n",
      "Iteration 366, loss = 0.14816362\n",
      "Iteration 167, loss = 0.23304297\n",
      "Iteration 273, loss = 0.18271479\n",
      "Iteration 367, loss = 0.14811041\n",
      "Iteration 168, loss = 0.23272806\n",
      "Iteration 368, loss = 0.14804222\n",
      "Iteration 274, loss = 0.18260085\n",
      "Iteration 369, loss = 0.14798673\n",
      "Iteration 169, loss = 0.23252153\n",
      "Iteration 275, loss = 0.18253602\n",
      "Iteration 370, loss = 0.14792701\n",
      "Iteration 371, loss = 0.14787173\n",
      "Iteration 170, loss = 0.23227848\n",
      "Iteration 276, loss = 0.18244479\n",
      "Iteration 372, loss = 0.14782527\n",
      "Iteration 373, loss = 0.14774251\n",
      "Iteration 277, loss = 0.18235537\n",
      "Iteration 374, loss = 0.14768484\n",
      "Iteration 171, loss = 0.23202070\n",
      "Iteration 375, loss = 0.14760336\n",
      "Iteration 376, loss = 0.14752344\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 278, loss = 0.18225400\n",
      "Iteration 172, loss = 0.23179483\n",
      "Iteration 279, loss = 0.18216623\n",
      "Iteration 173, loss = 0.23157854\n",
      "Iteration 280, loss = 0.18205887\n",
      "Iteration 174, loss = 0.23140736\n",
      "Iteration 281, loss = 0.18198813\n",
      "Iteration 175, loss = 0.23116726\n",
      "Iteration 282, loss = 0.18200371\n",
      "Iteration 283, loss = 0.18179859\n",
      "Iteration 176, loss = 0.23096313\n",
      "Iteration 284, loss = 0.18170604\n",
      "Iteration 177, loss = 0.23074010\n",
      "Iteration 285, loss = 0.18163327\n",
      "Iteration 286, loss = 0.18153840\n",
      "Iteration 178, loss = 0.23052494\n",
      "Iteration 287, loss = 0.18145073\n",
      "Iteration 179, loss = 0.23039176\n",
      "Iteration 288, loss = 0.18137312\n",
      "Iteration 180, loss = 0.23012851\n",
      "Iteration 289, loss = 0.18129147\n",
      "Iteration 181, loss = 0.22993606\n",
      "Iteration 290, loss = 0.18119126\n",
      "Iteration 291, loss = 0.18112818\n",
      "Iteration 292, loss = 0.18105227\n",
      "Iteration 182, loss = 0.22972999\n",
      "Iteration 293, loss = 0.18095546\n",
      "Iteration 183, loss = 0.22957318\n",
      "Iteration 184, loss = 0.22932225\n",
      "Iteration 294, loss = 0.18089980\n",
      "Iteration 295, loss = 0.18077980\n",
      "Iteration 185, loss = 0.22913855\n",
      "Iteration 296, loss = 0.18072827\n",
      "Iteration 186, loss = 0.22894904\n",
      "Iteration 297, loss = 0.18065218\n",
      "Iteration 187, loss = 0.22876861\n",
      "Iteration 298, loss = 0.18055798\n",
      "Iteration 299, loss = 0.18047947\n",
      "Iteration 188, loss = 0.22859795\n",
      "Iteration 300, loss = 0.18039625\n",
      "Iteration 189, loss = 0.22835561\n",
      "Iteration 301, loss = 0.18029392\n",
      "Iteration 190, loss = 0.22824525\n",
      "Iteration 302, loss = 0.18029988\n",
      "Iteration 191, loss = 0.22805255\n",
      "Iteration 303, loss = 0.18015311\n",
      "Iteration 304, loss = 0.18010146\n",
      "Iteration 305, loss = 0.18000867\n",
      "Iteration 192, loss = 0.22783676\n",
      "Iteration 306, loss = 0.17991553\n",
      "Iteration 193, loss = 0.22766635\n",
      "Iteration 307, loss = 0.17986325\n",
      "Iteration 194, loss = 0.22747909\n",
      "Iteration 308, loss = 0.17977967\n",
      "Iteration 309, loss = 0.17970774\n",
      "Iteration 195, loss = 0.22730063\n",
      "Iteration 310, loss = 0.17965705\n",
      "Iteration 196, loss = 0.22715571\n",
      "Iteration 311, loss = 0.17955058\n",
      "Iteration 197, loss = 0.22697305\n",
      "Iteration 312, loss = 0.17946527\n",
      "Iteration 198, loss = 0.22679884\n",
      "Iteration 313, loss = 0.17939037\n",
      "Iteration 314, loss = 0.17933498\n",
      "Iteration 199, loss = 0.22662705\n",
      "Iteration 315, loss = 0.17934179\n",
      "Iteration 200, loss = 0.22651259\n",
      "Iteration 316, loss = 0.17919397\n",
      "Iteration 201, loss = 0.22630870\n",
      "Iteration 317, loss = 0.17910043\n",
      "Iteration 202, loss = 0.22612541\n",
      "Iteration 318, loss = 0.17901781\n",
      "Iteration 203, loss = 0.22595728\n",
      "Iteration 319, loss = 0.17897147\n",
      "Iteration 204, loss = 0.22583104\n",
      "Iteration 320, loss = 0.17892465\n",
      "Iteration 205, loss = 0.22567261\n",
      "Iteration 321, loss = 0.17881349\n",
      "Iteration 206, loss = 0.22550506\n",
      "Iteration 322, loss = 0.17875347\n",
      "Iteration 207, loss = 0.22539439\n",
      "Iteration 323, loss = 0.17869315\n",
      "Iteration 324, loss = 0.17859195\n",
      "Iteration 208, loss = 0.22520334\n",
      "Iteration 325, loss = 0.17853389\n",
      "Iteration 209, loss = 0.22507177\n",
      "Iteration 326, loss = 0.17849436\n",
      "Iteration 210, loss = 0.22487335\n",
      "Iteration 327, loss = 0.17839380\n",
      "Iteration 211, loss = 0.22473678\n",
      "Iteration 328, loss = 0.17835200\n",
      "Iteration 329, loss = 0.17831349\n",
      "Iteration 212, loss = 0.22459515\n",
      "Iteration 330, loss = 0.17821575\n",
      "Iteration 213, loss = 0.22447217\n",
      "Iteration 214, loss = 0.22430202\n",
      "Iteration 331, loss = 0.17811239\n",
      "Iteration 215, loss = 0.22413891\n",
      "Iteration 332, loss = 0.17812568\n",
      "Iteration 216, loss = 0.22402314\n",
      "Iteration 333, loss = 0.17799522\n",
      "Iteration 334, loss = 0.17791642\n",
      "Iteration 217, loss = 0.22387288\n",
      "Iteration 335, loss = 0.17787622\n",
      "Iteration 218, loss = 0.22374894\n",
      "Iteration 336, loss = 0.17778897\n",
      "Iteration 219, loss = 0.22358282\n",
      "Iteration 337, loss = 0.17775865\n",
      "Iteration 220, loss = 0.22343344\n",
      "Iteration 338, loss = 0.17765490\n",
      "Iteration 339, loss = 0.17758401\n",
      "Iteration 221, loss = 0.22331348\n",
      "Iteration 340, loss = 0.17751804\n",
      "Iteration 222, loss = 0.22316540\n",
      "Iteration 341, loss = 0.17746126\n",
      "Iteration 223, loss = 0.22302725\n",
      "Iteration 342, loss = 0.17740451\n",
      "Iteration 343, loss = 0.17733518\n",
      "Iteration 224, loss = 0.22293806\n",
      "Iteration 344, loss = 0.17727209\n",
      "Iteration 225, loss = 0.22277203\n",
      "Iteration 345, loss = 0.17721879\n",
      "Iteration 226, loss = 0.22265030\n",
      "Iteration 346, loss = 0.17715819\n",
      "Iteration 227, loss = 0.22250698\n",
      "Iteration 347, loss = 0.17709214\n",
      "Iteration 348, loss = 0.17702521\n",
      "Iteration 228, loss = 0.22243606\n",
      "Iteration 349, loss = 0.17696224\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 229, loss = 0.22228202\n",
      "Iteration 230, loss = 0.22214659\n",
      "Iteration 231, loss = 0.22197621\n",
      "Iteration 232, loss = 0.22183994\n",
      "Iteration 233, loss = 0.22175811\n",
      "Iteration 234, loss = 0.22162997\n",
      "Iteration 235, loss = 0.22147826\n",
      "Iteration 236, loss = 0.22135849\n",
      "Iteration 237, loss = 0.22125031\n",
      "Iteration 238, loss = 0.22112637\n",
      "Iteration 239, loss = 0.22101553\n",
      "Iteration 240, loss = 0.22091017\n",
      "Iteration 241, loss = 0.22076051\n",
      "Iteration 242, loss = 0.22065811\n",
      "Iteration 243, loss = 0.22053731\n",
      "Iteration 244, loss = 0.22043775\n",
      "Iteration 245, loss = 0.22028944\n",
      "Iteration 246, loss = 0.22019589\n",
      "Iteration 247, loss = 0.22006202\n",
      "Iteration 248, loss = 0.21998974\n",
      "Iteration 249, loss = 0.21985209\n",
      "Iteration 250, loss = 0.21974664\n",
      "Iteration 251, loss = 0.21962844\n",
      "Iteration 252, loss = 0.21953075\n",
      "Iteration 253, loss = 0.21944283\n",
      "Iteration 254, loss = 0.21935658\n",
      "Iteration 255, loss = 0.21924064\n",
      "Iteration 256, loss = 0.21910156\n",
      "Iteration 257, loss = 0.21900192\n",
      "Iteration 258, loss = 0.21890697\n",
      "Iteration 259, loss = 0.21880522\n",
      "Iteration 260, loss = 0.21874574\n",
      "Iteration 261, loss = 0.21857462\n",
      "Iteration 262, loss = 0.21848488\n",
      "Iteration 263, loss = 0.21838862\n",
      "Iteration 264, loss = 0.21829110\n",
      "Iteration 265, loss = 0.21819429\n",
      "Iteration 266, loss = 0.21809878\n",
      "Iteration 267, loss = 0.21802880\n",
      "Iteration 268, loss = 0.21789687\n",
      "Iteration 269, loss = 0.21778720\n",
      "Iteration 270, loss = 0.21770890\n",
      "Iteration 271, loss = 0.21761108\n",
      "Iteration 272, loss = 0.21749415\n",
      "Iteration 273, loss = 0.21742301\n",
      "Iteration 274, loss = 0.21739431\n",
      "Iteration 275, loss = 0.21723003\n",
      "Iteration 276, loss = 0.21714492\n",
      "Iteration 277, loss = 0.21704742\n",
      "Iteration 278, loss = 0.21697875\n",
      "Iteration 279, loss = 0.21688228\n",
      "Iteration 280, loss = 0.21678347\n",
      "Iteration 281, loss = 0.21667148\n",
      "Iteration 282, loss = 0.21658150\n",
      "Iteration 283, loss = 0.21649142\n",
      "Iteration 284, loss = 0.21642968\n",
      "Iteration 285, loss = 0.21633924\n",
      "Iteration 286, loss = 0.21624312\n",
      "Iteration 287, loss = 0.21616814\n",
      "Iteration 288, loss = 0.21606748\n",
      "Iteration 289, loss = 0.21598161\n",
      "Iteration 290, loss = 0.21590910\n",
      "Iteration 291, loss = 0.21585482\n",
      "Iteration 292, loss = 0.21573891\n",
      "Iteration 293, loss = 0.21568833\n",
      "Iteration 294, loss = 0.21560963\n",
      "Iteration 295, loss = 0.21551160\n",
      "Iteration 296, loss = 0.21540397\n",
      "Iteration 297, loss = 0.21536484\n",
      "Iteration 298, loss = 0.21527525\n",
      "Iteration 299, loss = 0.21517992\n",
      "Iteration 300, loss = 0.21513316\n",
      "Iteration 301, loss = 0.21501222\n",
      "Iteration 302, loss = 0.21493875\n",
      "Iteration 303, loss = 0.21484734\n",
      "Iteration 304, loss = 0.21478214\n",
      "Iteration 305, loss = 0.21469594\n",
      "Iteration 306, loss = 0.21462023\n",
      "Iteration 307, loss = 0.21457862\n",
      "Iteration 308, loss = 0.21446938\n",
      "Iteration 309, loss = 0.21439452\n",
      "Iteration 310, loss = 0.21432565\n",
      "Iteration 311, loss = 0.21430705\n",
      "Iteration 312, loss = 0.21418526\n",
      "Iteration 313, loss = 0.21409854\n",
      "Iteration 314, loss = 0.21404353\n",
      "Iteration 315, loss = 0.21395664\n",
      "Iteration 316, loss = 0.21387117\n",
      "Iteration 317, loss = 0.21384802\n",
      "Iteration 318, loss = 0.21374512\n",
      "Iteration 319, loss = 0.21366915\n",
      "Iteration 320, loss = 0.21361625\n",
      "Iteration 321, loss = 0.21354960\n",
      "Iteration 322, loss = 0.21352099\n",
      "Iteration 323, loss = 0.21338881\n",
      "Iteration 324, loss = 0.21331436\n",
      "Iteration 325, loss = 0.21329705\n",
      "Iteration 326, loss = 0.21318367\n",
      "Iteration 327, loss = 0.21317572\n",
      "Iteration 328, loss = 0.21306967\n",
      "Iteration 329, loss = 0.21298474\n",
      "Iteration 330, loss = 0.21292423\n",
      "Iteration 331, loss = 0.21285526\n",
      "Iteration 332, loss = 0.21290508\n",
      "Iteration 333, loss = 0.21274365\n",
      "Iteration 334, loss = 0.21268768\n",
      "Iteration 335, loss = 0.21260514\n",
      "Iteration 336, loss = 0.21252504\n",
      "Iteration 337, loss = 0.21246202\n",
      "Iteration 338, loss = 0.21249011\n",
      "Iteration 339, loss = 0.21229048\n",
      "Iteration 340, loss = 0.21230267\n",
      "Iteration 341, loss = 0.21219847\n",
      "Iteration 342, loss = 0.21217272\n",
      "Iteration 343, loss = 0.21208119\n",
      "Iteration 344, loss = 0.21199644\n",
      "Iteration 345, loss = 0.21196342\n",
      "Iteration 346, loss = 0.21189214\n",
      "Iteration 347, loss = 0.21181668\n",
      "Iteration 348, loss = 0.21175648\n",
      "Iteration 349, loss = 0.21169376\n",
      "Iteration 350, loss = 0.21165625\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Define the parameter range for hidden_layer_sizes\n",
    "hidden_layer_sizes_range = [\n",
    "    (50,), (100,), (150,), \n",
    "    (50, 50), (100, 50), (100, 100), (150, 100), \n",
    "    (50, 50, 50), (100, 100, 100), (150, 150, 150), \n",
    "    (50, 50, 50, 50), (100, 100, 100, 100), (150, 150, 150, 150), \n",
    "    (50, 50, 50, 50, 50), (100, 100, 100, 100, 100), (150, 150, 150, 150, 150),\n",
    "    (50, 50, 50, 50, 50, 50), (100, 100, 100, 100, 100, 100), (150, 150, 150, 150, 150, 150)\n",
    "]\n",
    "hidden_layer_sizes_labels = [str(hls) for hls in hidden_layer_sizes_range]\n",
    "\n",
    "# Create a scorer for Recall\n",
    "recall_scorer = make_scorer(recall_score, average='weighted')\n",
    "\n",
    "# Validation curve for hidden_layer_sizes\n",
    "train_scores, valid_scores = validation_curve(\n",
    "    MLPClassifier(solver='sgd', max_iter=3000, learning_rate='constant', alpha=0.1, activation='relu', random_state=42, verbose=True),\n",
    "    X_train, y_train,\n",
    "    param_name='hidden_layer_sizes',\n",
    "    param_range=hidden_layer_sizes_range,\n",
    "    cv=5,\n",
    "    scoring=recall_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate mean and standard deviation for training and validation scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "valid_mean = np.mean(valid_scores, axis=1)\n",
    "valid_std = np.std(valid_scores, axis=1)\n",
    "\n",
    "# Plot the validation curve\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(range(len(hidden_layer_sizes_range)), train_mean, label='Training score', color='blue')\n",
    "plt.plot(range(len(hidden_layer_sizes_range)), valid_mean, label='Cross-validation score', color='green')\n",
    "plt.fill_between(range(len(hidden_layer_sizes_range)), train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n",
    "plt.fill_between(range(len(hidden_layer_sizes_range)), valid_mean - valid_std, valid_mean + valid_std, alpha=0.2, color='green')\n",
    "plt.xticks(range(len(hidden_layer_sizes_labels)), hidden_layer_sizes_labels, rotation=45, ha='right')\n",
    "plt.title('Validation Curve for MLPClassifier (hidden_layer_sizes)')\n",
    "plt.xlabel('Hidden Layer Sizes')\n",
    "plt.ylabel('Recall Score')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94f7ff96-565a-490e-a76a-70b724e9816e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m recall_scorer \u001b[38;5;241m=\u001b[39m make_scorer(recall_score, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Validation curve for activation functions\u001b[39;00m\n\u001b[1;32m     13\u001b[0m train_scores, valid_scores \u001b[38;5;241m=\u001b[39m validation_curve(\n\u001b[1;32m     14\u001b[0m     MLPClassifier(solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m'\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3000\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, hidden_layer_sizes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m50\u001b[39m), random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m---> 15\u001b[0m     X_train, y_train,\n\u001b[1;32m     16\u001b[0m     param_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m     param_range\u001b[38;5;241m=\u001b[39mactivation_range,\n\u001b[1;32m     18\u001b[0m     cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     19\u001b[0m     scoring\u001b[38;5;241m=\u001b[39mrecall_scorer,\n\u001b[1;32m     20\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Calculate mean and standard deviation for training and validation scores\u001b[39;00m\n\u001b[1;32m     24\u001b[0m train_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(train_scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Define the parameter range for activation functions\n",
    "activation_range = ['logistic', 'tanh', 'relu']\n",
    "\n",
    "# Create a scorer for Recall\n",
    "recall_scorer = make_scorer(recall_score, average='weighted')\n",
    "\n",
    "# Validation curve for activation functions\n",
    "train_scores, valid_scores = validation_curve(\n",
    "    MLPClassifier(solver='sgd', max_iter=3000, learning_rate='constant', alpha=0.1, hidden_layer_sizes=(50, 50), random_state=42, verbose=True),\n",
    "    X_train, y_train,\n",
    "    param_name='activation',\n",
    "    param_range=activation_range,\n",
    "    cv=5,\n",
    "    scoring=recall_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate mean and standard deviation for training and validation scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "valid_mean = np.mean(valid_scores, axis=1)\n",
    "valid_std = np.std(valid_scores, axis=1)\n",
    "\n",
    "# Plot the validation curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(activation_range, train_mean, label='Training score', color='blue')\n",
    "plt.plot(activation_range, valid_mean, label='Cross-validation score', color='green')\n",
    "plt.fill_between(activation_range, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n",
    "plt.fill_between(activation_range, valid_mean - valid_std, valid_mean + valid_std, alpha=0.2, color='green')\n",
    "plt.title('Validation Curve for MLPClassifier (activation)')\n",
    "plt.xlabel('Activation Function')\n",
    "plt.ylabel('Recall Score')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23190674-6e17-44ee-a200-5e32d9a9b2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import recall_score, make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the model with the tuned hyperparameters\n",
    "mlp = MLPClassifier(solver='sgd', max_iter=1, learning_rate='constant', alpha=0.1,\n",
    "                    hidden_layer_sizes=(50, 50), activation='relu', warm_start=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store scores\n",
    "train_scores = []\n",
    "cv_scores = []\n",
    "\n",
    "# Define the number of epochs\n",
    "epochs = np.arange(1, 51)  # Reduced the number of epochs to 50\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=3)  # Reduced the number of folds to 3\n",
    "\n",
    "# Create a scorer for Recall\n",
    "recall_scorer = make_scorer(recall_score, average='weighted')\n",
    "\n",
    "# Loop over the number of epochs\n",
    "for epoch in epochs:\n",
    "    mlp.max_iter = epoch  # Set the current number of epochs\n",
    "    mlp.fit(X_train, y_train)  # Fit the model on the training data\n",
    "    \n",
    "    # Calculate training score\n",
    "    train_score = recall_score(y_train, mlp.predict(X_train), average='weighted')\n",
    "    train_scores.append(train_score)\n",
    "    \n",
    "    # Calculate cross-validation score\n",
    "    cv_score = cross_val_score(mlp, X_train, y_train, cv=cv, scoring=recall_scorer, n_jobs=-1).mean()\n",
    "    cv_scores.append(cv_score)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_scores, label='Training score', color='blue')\n",
    "plt.plot(epochs, cv_scores, label='Cross-validation score', color='green')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Recall Score')\n",
    "plt.title('Learning Curve for MLPClassifier (Recall Score)')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32fbc210-3764-403b-a7fe-fd114651540a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (3) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (3) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (3) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (4) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (4) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (4) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (6) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (6) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (6) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (7) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (7) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (7) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (8) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (8) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (8) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (9) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (9) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (9) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (11) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (11) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (11) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (12) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (12) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (12) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (13) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (13) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (13) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (14) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (14) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (14) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (15) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (16) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (16) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (16) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (17) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (17) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (17) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (18) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (18) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (18) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (19) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (19) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (19) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (21) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (21) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (21) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (22) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (22) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (22) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (23) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (23) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (23) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (24) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (24) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (24) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (25) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (25) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (25) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (26) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (26) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (26) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (27) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (27) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (27) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (28) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (28) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (28) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (29) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (29) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (29) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (31) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (31) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (31) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (32) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (32) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (32) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (33) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (33) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (33) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (34) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (34) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (34) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (35) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (35) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (35) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (36) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (36) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (36) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (37) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (37) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (37) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (38) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (38) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (38) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (39) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (39) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (39) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (40) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (40) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (40) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (41) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (41) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (41) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (42) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (42) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (42) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (43) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (43) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (43) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (44) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (44) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (44) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (45) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (45) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (45) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (46) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (46) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (46) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (47) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (47) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (47) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (48) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (48) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (48) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (49) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (49) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (49) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (51) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (52) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (52) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (52) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (53) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (53) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (53) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (54) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (54) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (54) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (55) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (55) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (55) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (56) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (56) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (56) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (57) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (57) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (57) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (58) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (58) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (58) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (59) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (59) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (59) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (60) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (60) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (60) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (61) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (61) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (61) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (62) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (62) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (62) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (63) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (63) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (63) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (64) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (64) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (64) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (65) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (65) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (65) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (66) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (66) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (66) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (67) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (67) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (67) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (68) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (68) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (68) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (69) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (69) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (69) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (70) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (70) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (70) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (71) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (71) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (71) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (72) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (72) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (72) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (73) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (73) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (73) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (74) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (74) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (74) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (75) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (75) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (75) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (76) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (76) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (76) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (77) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (77) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (77) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (78) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (78) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (78) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (79) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (79) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (79) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (80) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (81) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (81) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (81) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (82) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (82) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (82) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (83) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (83) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (83) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (84) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (84) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (84) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (85) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (85) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (85) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (86) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (86) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (86) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (87) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (87) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (87) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (88) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (88) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (88) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (89) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (89) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (89) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (90) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (90) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (90) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (91) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (91) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (91) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (92) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (92) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (92) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (93) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (93) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (93) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (94) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (94) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (94) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (95) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (95) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (95) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (96) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (96) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (96) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (97) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (97) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (97) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (98) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (98) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (98) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (99) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (99) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (99) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import recall_score, make_scorer\n",
    "\n",
    "# Define the model with the tuned hyperparameters\n",
    "mlp = MLPClassifier(solver='sgd', max_iter=1, learning_rate='constant', alpha=0.1,\n",
    "                    hidden_layer_sizes=(100, 50), activation='relu', warm_start=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store scores\n",
    "train_scores = []\n",
    "cv_scores = []\n",
    "\n",
    "# Define the number of epochs\n",
    "epochs = np.arange(1, 100)\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=3)\n",
    "\n",
    "# Loop over the number of epochs\n",
    "for epoch in epochs:\n",
    "    mlp.max_iter = epoch  # Set the current number of epochs\n",
    "    mlp.fit(X_train, y_train)  # Fit the model on the training data\n",
    "    \n",
    "    # Calculate training score\n",
    "    train_score = recall_score(y_train, mlp.predict(X_train), average='weighted')\n",
    "    train_scores.append(train_score)\n",
    "    \n",
    "    # Calculate cross-validation score\n",
    "    cv_score = cross_val_score(mlp, X_train, y_train, cv=cv, scoring=recall_scorer).mean()\n",
    "    cv_scores.append(cv_score)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_scores, label='Training score', color='blue')\n",
    "plt.plot(epochs, cv_scores, label='Cross-validation score', color='green')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Recall Score')\n",
    "plt.title('Learning Curve for MLPClassifier')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d66732c-5566-4805-8792-1804561cd507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Define the model with the tuned hyperparameters\n",
    "mlp = MLPClassifier(solver='sgd', max_iter=2000, learning_rate='constant', alpha=0.1,\n",
    "                    hidden_layer_sizes=(50, 50), activation='relu', random_state=42)\n",
    "\n",
    "# Create a scorer for Recall\n",
    "recall_scorer = make_scorer(recall_score, average='macro')\n",
    "# Calculate learning curve\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    mlp, X_train, y_train, cv=5, scoring=recall_scorer,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1)\n",
    "\n",
    "# Calculate mean and standard deviation for training and validation scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "valid_mean = np.mean(valid_scores, axis=1)\n",
    "valid_std = np.std(valid_scores, axis=1)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, label='Training score', color='blue')\n",
    "plt.plot(train_sizes, valid_mean, label='Cross-validation score', color='green')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n",
    "plt.fill_between(train_sizes, valid_mean - valid_std, valid_mean + valid_std, alpha=0.2, color='green')\n",
    "plt.xlabel('Number of Training Samples')\n",
    "plt.ylabel('Recall Score')\n",
    "plt.title('Learning Curve for MLPClassifier (Recall Score)')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b221fb70-822b-4ee0-b54f-a352268c508f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Define the model with the tuned hyperparameters\n",
    "mlp = MLPClassifier(solver='sgd', max_iter=3000, learning_rate='constant', alpha=0.1,\n",
    "                    hidden_layer_sizes=(50, 50), activation='relu', random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_pred = mlp.predict(X_val)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for MLPClassifier')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b20e45-57fd-487c-8323-76311e9e56f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
